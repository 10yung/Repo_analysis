Problem bijectioncore version is out of date and there is no release for the current version Solution Update bijectioncore version to Result Bijectioncore dependency can be crossbuilt for Problem No crossbuild for finagleopencensustracing Solution include in finagleopencensustracing cross versions Problem No crossbuild for finaglestats and finaglestatscore Solution include in finaglestats and finaglestatscore cross versions Problem B Format is currently the only propagation method we can use Invalid or missing propagation headers result in a response Solution raised opencensusVersion in buildsbt to added optional TextFormat parameter to the withOpenCensusTracing for both Server and Client that defaults to previously used B Format in StackServerOpsscala previous version seemed to expect SpanContextINVALID as a failure result from TextFormatextract which was incorrect This resulted in missing or invalid propagation headers causing a response TextFormatextract throws an exception on bad or missing data which is now treated as SpanContextINVALID added a test dependency to opencensuscontribhttputil which provides an implementation of TextFormat for stackdriver style xcloudtracecontext headers added copy of existing test that uses the CloudTraceFormat from opencensuscontribhttputil Result You can now use other forms of tracing header propagation ie the CloudTraceFormat format from the opencensuscontribhttputil package When rolls around performance should be considered See Expected behavior The same performance as Actual behavior So far mostly unknown Discussion From Scala to scalaSeq changed from an alias to scalacollectionSeq to scalacollectionimmutableSeq In many places in finagle Seqs are built up and returned through ArrayBuffers which are scalaSeqs but not scalaSeqs Returning these as scalaSeq is cost in scala but carries a significant cost in An attempted solution for this problem by using an ArraySeqnewBuilder and returning the result showed a significant and unacceptable performance degradation on Its suspected that the degradation of ArrayBuffertoSeq is going to be even worse on Some plausible solutions that could be tested on how they affect performance between and are Using a ListBuilder and returning a List This is will not invoke any copying but the resulting collection is not indexed Using a custom builder around an Array that is cool with some unpruned slack space Returning it as a Seq should be as cheap as wrapping it and the slack space problem is also present in the mutable collection Returning a scalacollectionSeq instead of a scalaSeq This makes the mutableimmutable distinction go away on but pushes the burden of dealing with a possibly unexpected possibly mutable Seq to the user Also needs to deal with Create an immutable facade around any Seq so you can return the mutable collection immutably This is a bit of a footgun but if the mutation is localized it shouldnt be too hard to reason about when its safe to wrap and return ServiceClosedException during updatig Var Expected behavior Im no sure if it is proper behavior that something is closed during updating Var Actual behavior comtwitterfinagleServiceClosedException null at comtwitterfinaglepoolWatermarkPoolanonfunclose WatermarkPoolscala at comtwitterfinaglepoolWatermarkPoolanonfunclose adaptedWatermarkPoolscala at comtwitterfinaglepoolWatermarkPoolLambda A C F applyUnknown Source at scalacollectionIndexedSeqOptimizedforeachIndexedSeqOptimizedscala at scalacollectionIndexedSeqOptimizedforeachIndexedSeqOptimizedscala at scalacollectionmutableArrayOpsofRefforeachArrayOpsscala at comtwitterfinaglepoolWatermarkPoolcloseWatermarkPoolscala at comtwitterfinagleServiceFactoryProxycloseServiceFactoryProxyscala at comtwitterfinagleServiceFactoryProxycloseServiceFactoryProxyscala at comtwitterfinaglelivenessFailureAccrualFactorycloseFailureAccrualFactoryscala at comtwitterfinagleServiceFactoryProxycloseServiceFactoryProxyscala at comtwitterfinagleServiceFactoryProxycloseServiceFactoryProxyscala at comtwitterfinagleServiceFactoryProxycloseServiceFactoryProxyscala at comtwitterfinagleServiceFactoryProxycloseServiceFactoryProxyscala at comtwitterfinagleServiceFactoryProxycloseServiceFactoryProxyscala at comtwitterfinagleServiceFactoryProxycloseServiceFactoryProxyscala at comtwitterfinagleServiceFactoryProxycloseServiceFactoryProxyscala at comtwitterfinagleFilteranon closeFilterscala at comtwitterfinagleServiceFactoryProxycloseServiceFactoryProxyscala at comtwitterfinagleServiceFactoryProxycloseServiceFactoryProxyscala at comtwitterfinagleServiceFactoryProxycloseServiceFactoryProxyscala at comtwitterfinagleFilteranon closeFilterscala at comtwitterfinagleServiceFactoryProxycloseServiceFactoryProxyscala at comtwitterfinagleloadbalancerLazyEndpointFactorycloseEndpointFactoryscala at comtwitterutilClosablecloseClosablescala at comtwitterutilClosablecloseClosablescala at comtwitterfinagleServiceFactorycloseServiceFactoryscala at comtwitterfinagleloadbalancerTrafficDistributoranonfunweightEndpoints TrafficDistributorscala at comtwitterfinagleloadbalancerTrafficDistributorLambda C applyUnknown Source at scalacollectionTraversableOnceanonfunfoldLeft TraversableOncescala at scalacollectionTraversableOnceanonfunfoldLeft adaptedTraversableOncescala at scalacollectionTraversableOnceLambda CBAF applyUnknown Source at scalacollectionimmutableSetSet foreachSetscala at scalacollectionTraversableOncefoldLeftTraversableOncescala at scalacollectionTraversableOncefoldLeftTraversableOncescala at scalacollectionAbstractTraversablefoldLeftTraversablescala at comtwitterfinagleloadbalancerTrafficDistributoranonfunweightEndpoints TrafficDistributorscala at comtwitterfinagleloadbalancerTrafficDistributorLambda FB applyUnknown Source at comtwitterfinagleloadbalancerTrafficDistributoranonfunsafelyScanLeft TrafficDistributorscala at comtwitterfinagleloadbalancerTrafficDistributorLambda A D applyUnknown Source at comtwitterutilEventanon anonfunregister Eventscala at comtwitterutilEventanon anonfunregister adaptedEventscala at comtwitterutilEventanon Lambda F B applyUnknown Source at comtwitterutilFunctionanonfunsynchronizeWith Functionscala at comtwitterutilFunctionLambda D applyUnknown Source at comtwitterutilWitnessanon notifyEventscala at comtwitterutilVaranon anonfunregister Varscala at comtwitterutilVaranon anonfunregister adaptedVarscala at comtwitterutilVaranon Lambda applyUnknown Source at comtwitterutilVarObserverpublishVarscala at comtwitterutilVarValueobserveVarscala at comtwitterutilVaranon anonfunobserve Varscala at comtwitterutilVaranon anonfunobserve adaptedVarscala at comtwitterutilVaranon Lambda BE F applyUnknown Source at comtwitterutilVarObserverpublishVarscala at comtwitterutilUpdatableVaranonfunupdate Varscala at comtwitterutilUpdatableVaranonfunupdate adaptedVarscala at comtwitterutilUpdatableVarLambda FDC applyUnknown Source at scalacollectionTraversableLikeWithFilteranonfunforeach TraversableLikescala at scalacollectionTraversableLikeWithFilterLambda C EF applyUnknown Source at scalacollectionimmutableRedBlackTreeforeachKeyRedBlackTreescala at scalacollectionimmutableRedBlackTreeforeachKeyRedBlackTreescala at scalacollectionimmutableTreeSetforeachTreeSetscala at scalacollectionTraversableLikeWithFilterforeachTraversableLikescala at comtwitterutilUpdatableVarupdateVarscala Steps to reproduce the behavior It might be hard to reproduce because it might happens during high workload Support for touch gat and gats memcached commands Expected behavior The memcached client provides methods for touch gat and gats memcached commands Actual behavior The memcached client does not have support for the touch gat and gats memcached commands This is a feature request and not a bug Those commands are not documented in the wiki but are in the protocol description and are indeed supported by at least the latest version of memcached I would be happy to contribute a PR for this but wanted to ask here first if this would be something that could be accepted of if there is some reason why you wouldnt want to add support for those commands Cheers Framed transport client saw out of order responses on the same connection from finagle server Expected behavior As far as I understand framed transport doesnt allow out of order responses on the same connection Actual behavior Framed transport client saw out of order responses on the same connection from finagle server Steps to reproduce the behavior We use fbthfit c client with binary protocol and Framed transport Hi Finagle team The following commit changed the behaviour of the method so that HttpClientTraceInitializer is always present even if the original client stack doesnt have the TraceInitializerFilterrole So now Its not possible to disable the ctfhHttpClientTraceInitializerrole in the clients In our case we remove TraceInitializerFilterrole from the client and server stacks so that the B tracing headers can be preserved in the original format because all of the headers are removed by comtwitterfinaglehttpTraceInforemoveAllHeaders What do you think about making the new logick conditional For instance creating overriding the filter only if the existing stack contains TraceInitializerFilterrole Im happy to send a PR if this sounds reasonable Expected behavior The HttpClientTraceInitializerrole can be removed from the client stack Actual behavior The HttpClientTraceInitializerrole is always added when the client is created Theres a todo in code This is only applied to RequeueFilter but not to RetryFilter Is it valid at all If I use Mux then ThresholdFailureDetector will mark ClientSession as Closed after ping times out seconds by default Hence RequeueFilter will stop retrying I might be missing something but when I pass precondition to always true it works fine and not affected by ThresholdFailureDetector My point is in case of service downtime RequeueFilter might stop retrying before exhausting budget because ThresholdFailureDetector marked ClientSession as Closed If theres a problem when the stack returns nonrestartable service why similar logic is not implemented for RetryFilter