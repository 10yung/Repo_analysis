Apache Hudi provides atomic upserts and incremental data streams on Big Data Hi I am trying to use VerificationSuite with constraints manually defined in a config file ie my constraints are not autogenerated from the ConstraintSuggestionRunner Though for inspiration I have investigated the output from ConstraintSuggestionsRunner dataframe with the columns column constraintDescription scalaCodeForConstraint and based on my manually defined config file I have generated a sequence of strings with scalaCodeForConstraint in the following format isNonNegativecolumnA isCompletecolumnB isContainedIncolumnC ArrayvalidValue validValue SomeIt should be above I used your reply to as inspiration and I would like to replace allConstraints in your code example below with input from suggestionResult with my sequence of scalaCodeForConstraint How do I replace allConstraints below with scalaCodeForConstraint based on config file val allConstraints suggestionResultconstraintSuggestions flatMap case suggestions suggestionsmap constraint toSeq probably also to be replaced val generatedCheck CheckCheckLevelError generated constraints allConstraints val verificationResult VerificationSuite onDatadatasource addChecksSeqgeneratedCheck run Do you have a code example on how to use scalaCodeForConstraint as input to the VerificationSuite Thank you very much in advance your input is highly appreciated When I have a VerificationSuite with a Check like this addCheckCheckCheckLevelError Sample check hasSize whereentity A hasSize whereentity B hasSize whereentity C Run the suite and get the VerificationResult like this VerificationResultsuccessMetricsAsDataFramespark distributionSuiteshow I get back the following result entityinstancename value Dataset Size Dataset Size Dataset Size Its hard to differentiate these metrics Is there any chance to add any description name to the metrics Have seen there is an hint Option String param for the constraints however if understand correctly its not available in Metrics result Adjusting the name column might be an option like this entity instance name value Dataset Size where entity A Dataset Size where entity B Dataset Size where entity C Would it be possible to add this info to make the results more clear I am trying to figure out a case when the real fractional column contans a string value val rows sparksparkContextparallelizeSeq RawDatathingA INTRANSIT true RawDatathingA DELAYED false RawDatathingB Test DELAYED null RawDatathingC null INTRANSIT false RawDatathingD DELAYED true RawDatathingC UNKNOWN null RawDatathingC UNKNOWN null RawDatathingE DELAYED false In the test I have added a Test in second column The inferred type is coming as String Is there a way to to identify the distribution of column value is mostly Fractional so cast it to double Hi Trying to calculate a few quantiles Ive realised that the metric names are not clear If they are calculated individually ApproxQuantilesfieldName Seq ApproxQuantilesfieldName Seq ApproxQuantilesfieldName Seq they end with three metrics with the same name and no clue about the quantile calculated DoubleMetricColumnApproxQuantilefieldNameSuccess DoubleMetricColumnApproxQuantilefieldNameSuccess DoubleMetricColumnApproxQuantilefieldNameSuccess When they are calculated all at once ApproxQuantilesfieldName Seq the KeyedDoubleMetric is generated with all the information KeyedDoubleMetricColumnApproxQuantilesfieldNameSuccessMap Here the problem is when the KeyedDoubleMetric is flattened in order to process the metrics individually It ends with three metrics where the calculated quantile is displayed but there is not clue about the metric itself approxQuantile DoubleMetricColumnname fieldNameSuccess DoubleMetricColumnname fieldNameSuccess DoubleMetricColumnname fieldNameSuccess My Suggestions are At ApproxQuantile when saving the metric add the quantile It is from ApproxQuantile to sapproxQuantilequantile At Metric KeyedDoubleMetric on the flatten method update the instance value from snamekey to snamekey actually I think this is the original expected behaviour Im open to help on the changes if you agree with them Hi Team I have been trying to understand if it is possible to implement your anomlay detection strategy not only on the full dataset but on the multiple subset Example I have warehouse generating data in my dataset wharehouseid being a column and I want to apply BatchNormalStrategy for all these particular warehouses Can I achieve this Hello Everyone I would like to know how we can persist the table profile results as data frame json format Currently the return type is a scala object ColumnProfilerRunner Type Anyone implemented this scenario thank you Hi Team We have a use case where we want to collect some metrics for a grouped DataFrame we want to see the collected metrics eg Completeness ApproxQuantiles Histogram separately for every group We also want to store the metrics in MetricsRepository Any best practices on this I have the following idea filter a group of the DataFrame pass it to deequ for analyis store the metrics in MetricsRepository using the group value as a tag repeat the above steps for every group It might be working but its pretty timeconsuming Any idea to improve this approach Thanks in advance Robert Hi Team We have a use case where we want to collect the following metrics the proportion of rows which contain at least nullmissing value Do you have any suggestion to cover this use case using deequ I have workaround idea add a new boolean column eg hasnull to the input DataFrame that is indicating whether we have any null missing value in the given row use Compliancehasnull true analyzer Is there a better solution to this Maybe I could write a dynamic predicate for Compliance that iterates all the columns and checks if any of them null Thanks in advance Robert