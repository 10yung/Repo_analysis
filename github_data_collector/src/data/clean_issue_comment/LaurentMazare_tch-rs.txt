 I am implementing the AdamW optimizer for a project The implementation can benefit from inplace computations in particular add addcmul and addcdiv The tensor multiplied tensors and divided tensors need to be scaled before addition Unfortunately the corresponding Tensor methods do not take a scalar instead the default in the C API of is used So instead some temporaries are needed It would be nice if the scaling scalar was also exposed as an argument Hello thanks for this awesome lib I love it I could not find a way to define the negative slope of the leaky relu The bindings appear to not generate the version which takes a slope parameter such as Is this indeed missing or am I not looking in the right place Current tchrs IValue which is used as the input to TorchScript models covers the following input TensorcrateTensor Inti Doublef TupleVecIValue Taking a look at TorchScript supported types we still need to support x bool A boolean value x str A string x List T A list of which all members are type T Optional T A value which is either None or type T x Dict K V A dict with key type K and value type V Only str int and float are allowed as key types NamedTuple T T Are there plans to add support for these types anytime soon Im getting this error powershell error process didnt exit successfully target debug neural exe exit code xc STATUSORDINALNOTFOUND From what i understood it might come from my installation of LibTorch so heres the steps ive followed Download LibTorch for c CPU release Unzip the libtorch folder somewhere Added an environment variable called LIBTORCH with the path of the libtorch folder Added the libtorch folder to the PATH Hello thank you a lot for this lib Ive been using it to run inference on the YoloV example However after inference the thread local variables seem to not be cleaned up To reproduce the issue just run Yolo and check that the thread now uses around MB of memory even if it is just sleeping The issue seem to be the same as I wonder if there is something we can do about it Anyway a warning on the README would be nice to warn people about this Could you add benchmark of speed training and interference resnet on your rust library and python library on this dataset Hi Thanks for this great package Im looking into using an existing libtorch C module with the package and havent found an example or info on how to do this Im fairly new to Rust so maybe Im also just not seeing that this is trivial Whats the most idiomatic way of doing this with tchrs Im trying to port over some of my python code however coming across some issues due to the way a Sequential currently works The current way its programmed is doesnt allow to add tensors after its initialization Im wanting to dynamically create a model via loops so I have no idea of the input size or number of layers as its set dynamically A barebones example let mut model tchnnseq modeladdtchnnlinearvarstore layerfinal inputfeaturesize outputsize Defaultdefault modeladdfnxs xssigmoid Comes up with the error error E use of moved value model srcmainrs let mut model tchnnseq move occurs because model has type tchnnsequentialSequential which does not implement the Copy trait modeladdtchnnlinearvarstore layerfinal inputfeaturesize outputsize Defaultdefault value moved here modeladdfnxs xssigmoid value used here after move Looking at the source the reason this happens is that due to Copy not being implemented when it returns Self that moves everything meaning that it is no longer accessible further down the line if youre using it this way My first thoughts are to either Make Sequential copyable probably not a valid option due to variable stores etc Adding separate nonbuilder functions that dont return Self Im happy to work on putting something together and submit a PR but thought it would be worth the discussion first as to the best way forward if its even wanted I would think it would be better to add separate functions but I dont know how one would best differentiate them by name I know some libraries have separate builder classes but that would create quite a breaking change to the API Inspired by Tensor Considered Harmful Im wondering if its possible to build compiletime checked tensor type There prior works to make things possible typenum for typed numbers Its necessary for CUDA device ordinal and dimension size frunk for HList Each dimension component can be named by a unit struct HList acts as a Vec of distinct types that help us build the typelevel dimension The last is const generics Though it still unstable and panics the compiler It would replace typenum in the long run I made a little demo to demonstrate this idea May we push it further Introduce tensor operations like expand multiply flatten and more into type semantics Since tch is backed on libtorch a thorough test is required to verify the type design