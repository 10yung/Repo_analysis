Bumps sparkversion from to Updates sparkcore from to Updates sparkstreaming from to Updates sparkstreamingkafka from to Updates sparksql from to Updates sparkhive from to Dependabot will resolve any conflicts with this PR as long as you dont alter it yourself You can also trigger a rebase manually by commenting dependabot rebase dependabotautomergestart dependabotautomergeend details summaryDependabot commands and optionssummary br You can trigger Dependabot actions by commenting on this PR dependabot rebase will rebase this PR dependabot recreate will recreate this PR overwriting any edits that have been made to it dependabot merge will merge this PR after your CI passes on it dependabot squash and merge will squash and merge this PR after your CI passes on it dependabot cancel merge will cancel a previously requested merge and block automerging dependabot reopen will reopen this PR if it is closed dependabot ignore this patchminormajor version will close this PR and stop Dependabot creating any more for this minormajor version unless you reopen the PR or upgrade to it yourself dependabot ignore this dependency will close this PR and stop Dependabot creating any more for this dependency unless you reopen the PR or upgrade to it yourself dependabot use these labels will set the current labels as the default for future PRs for this repo and language dependabot use these reviewers will set the current reviewers as the default for future PRs for this repo and language dependabot use these assignees will set the current assignees as the default for future PRs for this repo and language dependabot use this milestone will set the current milestone as the default for future PRs for this repo and language You can disable automated security fix PRs for this repo from the Security Alerts page details I have a simple java producer which is pushing message to kafka I can read message using simple java consumer but not using spark streaming Using spark streaming all messages are coming as null I have attached the code snippet I am passing the custom deserializer class in kafka properties Please confirm if we need to pass it in kafkautilscreatedirect stream method as well Somehow the method is throwing invalid argument types if I try to pass as per javdoc documents Please help as I am new to Spark img Please help me Whats wrong I cant call Reduce in Pi Example I did everything as a guide List of my components Windows JDK u Spark spark binhadoop Win Utils Uzip all in SPARKHOMEbin Mobius v All variables been regitred HADOOPHOME C spark binhadoop JAVAHOME C Java SCALAHOME C Program Files x scala SPARKHOME C spark binhadoop SPARKCLRHOME C Mobius runtime TMP C tmp i change defoult path like this PATHs been registrated too Run in VisualStudio I change Java running settings for heap size in sparkclrsubmitcmd file debugmode JAVAHOME bin java Xms m Xmx m cp LAUNCHCLASSPATH orgapachesparkdeploycsharpCSharpRunner debug goto eof call sparkclrsubmit debugg in command line Load VisualStudio Exaples and Run Pi project with params add keyCSharpBackendPortNumber value add keyCSharpWorkerPath valueCMobiusruntimebinCSharpWorkerexe and SparkContext var conf new SparkConfSetsparklocaldir C tmp SparkCLRTemp programm hase down on Reduce part of function CalculatePiUsingAnonymousMethod private static void CalculatePiUsingAnonymousMethodint n RDDint rdd var preCount rdd Mapi var random new Random var x randomNextDouble var y randomNextDouble return x x y y var count preCountReduceab return a b And show this INFO MicrosoftSparkCSharpConfigurationConfigurationService ConfigurationService runMode is DEBUG INFO MicrosoftSparkCSharpConfigurationConfigurationServiceSparkCLRDebugConfiguration CSharpBackend port number read from app config Using it to connect to CSharpBackend INFO MicrosoftSparkCSharpProxyIpcSparkCLRIpcProxy CSharpBackend port number to be used in JvMBridge is INFO MicrosoftSparkCSharpCoreSparkConf sparkmaster not set Assuming debug mode INFO MicrosoftSparkCSharpCoreSparkConf Spark master set to local INFO MicrosoftSparkCSharpCoreSparkConf sparkappname not set Assuming debug mode INFO MicrosoftSparkCSharpCoreSparkConf Spark app name set to debug app INFO MicrosoftSparkCSharpCoreSparkConf Spark configuration keyvalue set to sparklocaldirC tmp SparkCLRTemp INFO MicrosoftSparkCSharpCoreSparkContext Parallelizing items to form RDD in the cluster with partitions INFO MicrosoftSparkCSharpCoreRDD SystemInt mscorlib Version Cultureneutral PublicKeyTokenb a c e Executing Map operation on RDD preservesPartitioningFalse INFO MicrosoftSparkCSharpCoreRDD SystemInt mscorlib Version Cultureneutral PublicKeyTokenb a c e Executing Reduce operation on RDD INFO MicrosoftSparkCSharpConfigurationConfigurationServiceSparkCLRDebugConfiguration Worker path read from setting CSharpWorkerPath in app config ERROR MicrosoftSparkCSharpInteropIpcJvmBridge JVM method execution failed Static method collectAndServe failed for class orgapachesparkapipythonPythonRDD when called with parameters Index TypeJvmObjectReference Value ERROR MicrosoftSparkCSharpInteropIpcJvmBridge orgapachesparkSparkException Job aborted due to stage failure Task in stage failed times most recent failure Lost task in stage TID localhost javanetSocketException Connection reset by peer socket write error at javanetSocketOutputStreamsocketWrite Native Method at javanetSocketOutputStreamsocketWriteSocketOutputStreamjava at javanetSocketOutputStreamwriteSocketOutputStreamjava at javaioBufferedOutputStreamflushBufferBufferedOutputStreamjava at javaioBufferedOutputStreamwriteBufferedOutputStreamjava at javaioDataOutputStreamwriteDataOutputStreamjava at javaioFilterOutputStreamwriteFilterOutputStreamjava at orgapachesparkapipythonPythonRDDorgapachesparkapipythonPythonRDDwrite PythonRDDscala at orgapachesparkapipythonPythonRDDanonfunwriteIteratorToStream applyPythonRDDscala at orgapachesparkapipythonPythonRDDanonfunwriteIteratorToStream applyPythonRDDscala at scalacollectionIteratorclassforeachIteratorscala at orgapachesparkInterruptibleIteratorforeachInterruptibleIteratorscala at orgapachesparkapipythonPythonRDDwriteIteratorToStreamPythonRDDscala at orgapachesparkapipythonPythonRunnerWriterThreadanonfunrun applyPythonRDDscala at orgapachesparkutilUtilslogUncaughtExceptionsUtilsscala at orgapachesparkapipythonPythonRunnerWriterThreadrunPythonRDDscala Driver stacktrace at orgapachesparkschedulerDAGSchedulerorgapachesparkschedulerDAGSchedulerfailJobAndIndependentStagesDAGSchedulerscala at orgapachesparkschedulerDAGScheduleranonfunabortStage applyDAGSchedulerscala at orgapachesparkschedulerDAGScheduleranonfunabortStage applyDAGSchedulerscala at scalacollectionmutableResizableArrayclassforeachResizableArrayscala at scalacollectionmutableArrayBufferforeachArrayBufferscala at orgapachesparkschedulerDAGSchedulerabortStageDAGSchedulerscala at orgapachesparkschedulerDAGScheduleranonfunhandleTaskSetFailed applyDAGSchedulerscala at orgapachesparkschedulerDAGScheduleranonfunhandleTaskSetFailed applyDAGSchedulerscala at scalaOptionforeachOptionscala at orgapachesparkschedulerDAGSchedulerhandleTaskSetFailedDAGSchedulerscala at orgapachesparkschedulerDAGSchedulerEventProcessLoopdoOnReceiveDAGSchedulerscala at orgapachesparkschedulerDAGSchedulerEventProcessLooponReceiveDAGSchedulerscala at orgapachesparkschedulerDAGSchedulerEventProcessLooponReceiveDAGSchedulerscala at orgapachesparkutilEventLoopanon runEventLoopscala at orgapachesparkschedulerDAGSchedulerrunJobDAGSchedulerscala at orgapachesparkSparkContextrunJobSparkContextscala at orgapachesparkSparkContextrunJobSparkContextscala at orgapachesparkSparkContextrunJobSparkContextscala at orgapachesparkSparkContextrunJobSparkContextscala at orgapachesparkrddRDDanonfuncollect applyRDDscala at orgapachesparkrddRDDOperationScopewithScopeRDDOperationScopescala at orgapachesparkrddRDDOperationScopewithScopeRDDOperationScopescala at orgapachesparkrddRDDwithScopeRDDscala at orgapachesparkrddRDDcollectRDDscala at orgapachesparkapipythonPythonRDDcollectAndServePythonRDDscala at orgapachesparkapipythonPythonRDDcollectAndServePythonRDDscala at sunreflectNativeMethodAccessorImplinvoke Native Method at sunreflectNativeMethodAccessorImplinvokeNativeMethodAccessorImpljava at sunreflectDelegatingMethodAccessorImplinvokeDelegatingMethodAccessorImpljava at javalangreflectMethodinvokeMethodjava at orgapachesparkapicsharpCSharpBackendHandlerhandleMethodCallCSharpBackendHandlerscala at orgapachesparkapicsharpCSharpBackendHandlerhandleBackendRequestCSharpBackendHandlerscala at orgapachesparkapicsharpCSharpBackendHandlerchannelRead CSharpBackendHandlerscala at orgapachesparkapicsharpCSharpBackendHandlerchannelRead CSharpBackendHandlerscala at ionettychannelSimpleChannelInboundHandlerchannelReadSimpleChannelInboundHandlerjava at ionettychannelAbstractChannelHandlerContextinvokeChannelReadAbstractChannelHandlerContextjava at ionettychannelAbstractChannelHandlerContextfireChannelReadAbstractChannelHandlerContextjava at ionettyhandlercodecMessageToMessageDecoderchannelReadMessageToMessageDecoderjava at ionettychannelAbstractChannelHandlerContextinvokeChannelReadAbstractChannelHandlerContextjava at ionettychannelAbstractChannelHandlerContextfireChannelReadAbstractChannelHandlerContextjava at ionettyhandlercodecByteToMessageDecoderchannelReadByteToMessageDecoderjava at ionettychannelAbstractChannelHandlerContextinvokeChannelReadAbstractChannelHandlerContextjava at ionettychannelAbstractChannelHandlerContextfireChannelReadAbstractChannelHandlerContextjava at ionettychannelDefaultChannelPipelinefireChannelReadDefaultChannelPipelinejava at ionettychannelnioAbstractNioByteChannelNioByteUnsafereadAbstractNioByteChanneljava at ionettychannelnioNioEventLoopprocessSelectedKeyNioEventLoopjava at ionettychannelnioNioEventLoopprocessSelectedKeysOptimizedNioEventLoopjava at ionettychannelnioNioEventLoopprocessSelectedKeysNioEventLoopjava at ionettychannelnioNioEventLooprunNioEventLoopjava at ionettyutilconcurrentSingleThreadEventExecutor runSingleThreadEventExecutorjava at ionettyutilconcurrentDefaultThreadFactoryDefaultRunnableDecoratorrunDefaultThreadFactoryjava at javalangThreadrunThreadjava Caused by javanetSocketException Connection reset by peer socket write error at javanetSocketOutputStreamsocketWrite Native Method at javanetSocketOutputStreamsocketWriteSocketOutputStreamjava at javanetSocketOutputStreamwriteSocketOutputStreamjava at javaioBufferedOutputStreamflushBufferBufferedOutputStreamjava at javaioBufferedOutputStreamwriteBufferedOutputStreamjava at javaioDataOutputStreamwriteDataOutputStreamjava at javaioFilterOutputStreamwriteFilterOutputStreamjava at orgapachesparkapipythonPythonRDDorgapachesparkapipythonPythonRDDwrite PythonRDDscala at orgapachesparkapipythonPythonRDDanonfunwriteIteratorToStream applyPythonRDDscala at orgapachesparkapipythonPythonRDDanonfunwriteIteratorToStream applyPythonRDDscala at scalacollectionIteratorclassforeachIteratorscala at orgapachesparkInterruptibleIteratorforeachInterruptibleIteratorscala at orgapachesparkapipythonPythonRDDwriteIteratorToStreamPythonRDDscala at orgapachesparkapipythonPythonRunnerWriterThreadanonfunrun applyPythonRDDscala at orgapachesparkutilUtilslogUncaughtExceptionsUtilsscala at orgapachesparkapipythonPythonRunnerWriterThreadrunPythonRDDscala ERROR MicrosoftSparkCSharpInteropIpcJvmBridge JVM method execution failed Static method collectAndServe failed for class orgapachesparkapipythonPythonRDD when called with parameters Index TypeJvmObjectReference Value ERROR MicrosoftSparkCSharpInteropIpcJvmBridge MicrosoftSparkCSharpInteropIpcJvmBridgeCallJavaMethodBoolean isStatic Object classNameOrJvmObjectReference String methodName Object parameters I am trying to run WORDCOUNT example and it works if u run the CLI such as sparkclrsubmitcmd exe SparkClrWordCountexe C Users Downloads Mobiusmaster examples Batch WordCount bin Debug fileCUserseyadminDesktopTESTwordstxt it works HOWEVER if i run using the visual studio such as step sparkclrsubmitcmd debug step running the WORDCOUNT example using visual studio I get the following error Value cannot be null r nParameter name path StackTrace at SystemIOPathCombineString path String path String path r n at MicrosoftSparkCSharpConfigurationConfigurationServiceSparkCLRDebugConfigurationGetSparkCLRArtifactsPathString sparkCLRSubFolderName String fileName r n at Microsoft Downloaded the master yesterday and tried to run buildcmd in the VS command prompt running JDK bit After having to manually download winutils and tartools both failed in the script with file not found from the urls in the script got the following error m Kafka offset checkpoint and replay FAILED m m javaioIOException Cannot run program C Mobius MobiusMaster build to ls winutils bin winutilsexe CreateProcess error is not a valid Win application m m at javalangProcessBuilderstartUnknown Source m m at orgapachehadooputilShellrunCommandShelljava m m at orgapachehadooputilShellrunShelljava m m at orgapachehadooputilShellShellCommandExecutorexecuteShelljava m m at orgapachehadooputilShellexecCommandShelljava m m at orgapachehadooputilShellexecCommandShelljava m m at orgapachehadoopfsRawLocalFileSystemsetPermissionRawLocalFileSys emjava m m at orgapachehadoopfsFilterFileSystemsetPermissionFilterFileSystem ava m m at orgapachehadoopfsChecksumFileSystemcreateChecksumFileSystemjav m m at orgapachehadoopfsChecksumFileSystemcreateChecksumFileSystemjav m m m m Cause javaioIOException CreateProcess error is not a valid W n application m m at javalangProcessImplcreateNative Method m m at javalangProcessImplinitUnknown Source m m at javalangProcessImplstartUnknown Source m m at javalangProcessBuilderstartUnknown Source m m at orgapachehadooputilShellrunCommandShelljava m m at orgapachehadooputilShellrunShelljava m m at orgapachehadooputilShellShellCommandExecutorexecuteShelljava m m at orgapachehadooputilShellexecCommandShelljava m m at orgapachehadooputilShellexecCommandShelljava m m at orgapachehadoopfsRawLocalFileSystemsetPermissionRawLocalFileSys emjava m m m mRun completed in seconds milliseconds m mTotal number of tests run m mSuites completed aborted m mTests succeeded failed canceled ignored pending m m TEST FAILED m INFO ShutdownHookManager Shutdown hook called INFO ShutdownHookManager Deleting directory C Users LUM AppData Local Temp sparkd b dd de bbd f c b INFO INFO BUILD FAILURE INFO INFO Total time s INFO Finished at T INFO Final Memory M M INFO ERROR Failed to execute goal orgscalatestscalatestmavenplugin test t st on project sparkclr There are test failures Help ERROR ERROR To see the full stack trace of the errors rerun Maven with the e swi ch ERROR Rerun Maven using the X switch to enable full debug logging ERROR ERROR For more information about the errors and possible solutions please re d the following articles ERROR Help eption files copied Build Mobius Scala components failed stop building C Mobius MobiusMaster build The documentation file appears to have been generated with no space between the hashes and the header text This is causing the headers to not display correctly and is difficult to read See below for an example of with and without the space centerH font colordarkorchid Mobius API DocumentationfontH center font color BMicrosoftSparkCSharpCoreAccumulatorfont Summary A shared variable that can be accumulated ie has a commutative and associative add operation Worker tasks on a Spark cluster can add values to an Accumulator with br br centerH font colordarkorchid Mobius API DocumentationfontH center font color BMicrosoftSparkCSharpCoreAccumulatorfont Summary A shared variable that can be accumulated ie has a commutative and associative add operation Worker tasks on a Spark cluster can add values to an Accumulator with Im trying to use newAPIHadoopFile to implement custom InputFormat and RecordReader classes instead of using the default ones provided by Spark I can do the same in Java but am unable to find the correct way of achieving this using Mobius For reference these are the classes and interfaces I want to override javalangObject orgapachehadoopmapreduceInputFormatKV orgapachehadoopmapreducelibinputFileInputFormatKV and Interface RecordReaderKV In Mobius trying to use a custom InputFormat class throws a ClassNotFound exception Im guessing it only excepts classes provided by Hadoop Does Mobius support custom partitioning I get a serialization error any time Im passing data at runtime This happens with any calls using a lambda expression with data originating outside of the lambda expression Using broadcast variables also gives the same error Gives serialization error string xpath var results rddMapinput ConsoleWriteLinex but no serialization error here var results rddMapinput ConsoleWriteLinepath This is running Spark Linux Mono built with msbuild Ive also tested Mono with xbuild and get the same error actual error ERROR SystemRuntimeSerializationSerializationException Type MyClassNamecDisplayClass in Assembly MyAssembly Version Cultureneutral PublicKeyTokennull is not marked as serializable at SystemRuntimeSerializationFormatterServicesInternalGetSerializableMembers SystemRuntimeType type x in fbafb c c dad bccfec ae at SystemRuntimeSerializationFormatterServicescDisplayClass GetSerializableMembersb SystemRuntimeSerializationMemberHolder x in fbafb c c dad bccfec ae at SystemCollectionsConcurrentConcurrentDictionary TKeyTValue GetOrAdd TKey key SystemFunc TTResult valueFactory x in fbafb c c dad bccfec ae at SystemRuntimeSerializationFormatterServicesGetSerializableMembers SystemType type SystemRuntimeSerializationStreamingContext context x e in fbafb c c dad bccfec ae 