This issue replaces issue and is a first step towards global tables Today we create a single alternator keyspace and put all Alternator tables inside it Among other things this means that all Alternator tables need to live on the same set of DCs This makes sense today when all Alternator tables are global but if in the future we want to be compatible with DynamoDB and allow different tables to live on different subsets of DCs with the default being that a table only lives on a single DC see issue we cant have multiple tables share the same keyspace we need a separate keyspace for each table so as to allow us later to change the DC choice of each table separately We should switch to a separate keyspace per table now otherwise users will not be able to upgrade Scylla with existing data once is done Currently when all tables are global we should do what is described in issue so that they remain global as the cluster acquires new DCs But beyond that we should already consider how we will support the full global tables vs local tables feature This will affect how we name the keyspace and table inside it we need a way which will support all the possible ways that global and local tables are used and created and keeping in mind that Scylla does not currently allow moving tables between keyspaces and doesnt even allow renaming tables or keyspaces This isnt trivial and I propose the following plan The first time that a table named X is created from DC Y it will get keyspace X and table name X After keyspace X already exists creating a second table with the same name X now in a different DC Z will get keyspace XZ table name X note this can only happen after we add the capability of creating a nonglobal table Since both tables may remain alive and separate reading from table X on a client in DC Z will need to inspect both keyspaces X and XZ and find the one which has DC Z in its replication In the old technique for creating a global table the second table called X in keyspace XZ remains empty and is then joined into a global table This is easy we remove the keyspace XZ and update keyspace X to include the new DC In the new technique of creating a global table the user asks to modify the DCs of table X without creating another empty table first We can do this by modifying keyspace X But theres a snag above we created an invariant that a global table X will always be in a keyspace X and not say XZ But theoretically we need to test this in practice DynamoDB may allow two global tables with the same name Eg we may have one global table X on DCs Y and Z and another one with the same name X in DCs Q and W Only one keyspace can be called X If the other one will be XQ for example how would a read on DC W know to look up a keyspace called XQ I dont have a pretty solution for this Maybe we should just forbid this situation and not allow two global tables with the same name to exist I doubt anybody really wants this confusing situation We should mark Alternator keyspace somehow perhaps with a common prefix but note the name length limit issue or in another way so users cannot access nonAlternator tables with the Alternator API Installation details Scylla version or git commit hash f b c b Cluster size nodes i en xlarge OS RHELCentOSUbuntuAWS AMI ami ec cbb af ab The test run deletes from table with large partitions For large partition creation scyllabench was used I write partitions with K rows in each partition The writes run in parallel to deletions The data from few partitions was deleted using the query delete from scyllabenchtest where pk in After that during trying to read from one of partitions select minck as minck maxck as maxck from scyllabenchtest where pk Got error t fasyncorereactorpy l ccassandraconnection pERROR Error decoding response from Cassandra ver flags stream op offset len buffer b x x x x c x x x x v x x x x x Operation failed for scyllabenchtest received responses a nd failures from CLQUORUM x x x x x x x x x x x x SIMPLE t fasyncorereactorpy l ccassandraconnection pERROR Error decoding response from Cassandra ver flags stream op offset len buffer b x x x x c x x x x v x x x x x Operation failed for scyllabenchtest received responses and failures from CLQUORUM x x x x x x x x x x x x SIMPLE And Cassandra lost connection t fasyncorereactorpy l ccassandraconnection pERROR File cassandraconnectionpy line in cassandraconnectionConnectionprocessmsg t fasyncorereactorpy l ccassandraconnection pERROR File cassandraprotocolpy line in cassandraprotocolProtocolHandlerdecodemessage t fasyncorereactorpy l ccassandraconnection pERROR File cassandraprotocolpy line in cassandraprotocolErrorMessagerecvbody t fasyncorereactorpy l ccassandraconnection pERROR File cassandraprotocolpy line in cassandraprotocolWriteFailureMessagerecverrorinfo t fasyncorereactorpy l ccassandraconnection pERROR KeyError LE Next time when read from the test table operation time out error is received t fnemesispy l csdcmnemesis pDEBUG sdcmnemesisDeleteByPartitionsMonkey Choose partitions for delete query select minck as minck maxck as maxck from scyllabenchtest where pk t fnemesispy l csdcmnemesis pERROR sdcmnemesisDeleteByPartitionsMonkey Unhandled exception in method function DeleteByPartitionsMonkeydisrupt at x f cd b a t fnemesispy l csdcmnemesis pERROR sdcmnemesisDeleteByPartitionsMonkey Unhandled exception in method function DeleteByPartitionsMonkeydisrupt at x f cd b a t fnemesispy l csdcmnemesis pERROR Traceback most recent call last t fnemesispy l csdcmnemesis pERROR File sctsdcmnemesispy line in wrapper t fnemesispy l csdcmnemesis pERROR result methodargs kwargs t fnemesispy l csdcmnemesis pERROR File sctsdcmnemesispy line in disrupt t fnemesispy l csdcmnemesis pERROR selfdistruptdeletebypartitions t fnemesispy l csdcmnemesis pERROR File sctsdcmnemesispy line in distruptdeletebypartitions t fnemesispy l csdcmnemesis pERROR partitionsfordelete selfchoosepartitionsfordelete kscf t fnemesispy l csdcmnemesis pERROR File sctsdcmnemesispy line in choosepartitionsfordelete t fnemesispy l csdcmnemesis pERROR result sessionexecutecmd timeout t fnemesispy l csdcmnemesis pERROR File cassandraclusterpy line in cassandraclusterSessionexecute t fnemesispy l csdcmnemesis pERROR File cassandraclusterpy line in cassandraclusterResponseFutureresult t fnemesispy l csdcmnemesis pERROR cassandraReadTimeout Error from server code Coordinator node timed out waiting for replica nodes responses messageOperation timed out for scyllabenchtest received only responses from CLQUORUM infoconsistency QUORUM requiredresponses receivedresponses and t fnemesispy l csdcmnemesis pDEBUG sdcmnemesisDeleteByPartitionsMonkey Choose partitions for delete query select minck as minck maxck as maxck from scyllabenchtest where pk t fnemesispy l csdcmnemesis pERROR sdcmnemesisDeleteByPartitionsMonkey Unhandled exception in method function DeleteByPartitionsMonkeydisrupt at x f cd b a t fnemesispy l csdcmnemesis pERROR sdcmnemesisDeleteByPartitionsMonkey Unhandled exception in method function DeleteByPartitionsMonkeydisrupt at x f cd b a t fnemesispy l csdcmnemesis pERROR Traceback most recent call last t fnemesispy l csdcmnemesis pERROR File sctsdcmnemesispy line in wrapper t fnemesispy l csdcmnemesis pERROR result methodargs kwargs t fnemesispy l csdcmnemesis pERROR File sctsdcmnemesispy line in disrupt t fnemesispy l csdcmnemesis pERROR selfdistruptdeletebypartitions t fnemesispy l csdcmnemesis pERROR File sctsdcmnemesispy line in distruptdeletebypartitions t fnemesispy l csdcmnemesis pERROR partitionsfordelete selfchoosepartitionsfordelete kscf t fnemesispy l csdcmnemesis pERROR File sctsdcmnemesispy line in choosepartitionsfordelete t fnemesispy l csdcmnemesis pERROR result sessionexecutecmd timeout t fnemesispy l csdcmnemesis pERROR File cassandraclusterpy line in cassandraclusterSessionexecute t fnemesispy l csdcmnemesis pERROR File cassandraclusterpy line in cassandraclusterResponseFutureresult t fnemesispy l csdcmnemesis pERROR cassandraReadTimeout Error from server code Coordinator node timed out waiting for replica nodes responses messageOperation timed out for scyllabenchtest received only responses from CLQUORUM infoconsistency QUORUM requiredresponses receivedresponses All nodes are up status from node that select was run StatusUpDown StateNormalLeavingJoiningMoving Address Load Tokens Owns Host ID Rack UN GB dfd c aea b d b c addce f a UN GB e b b a b fb b a UN GB ab a fc ef f ae a a a UN GB e df b f a dcd ba c f a UN GB c aa e f e d d a But I am not able to run this query manually receive same error it happens almost immediately despite I defined requesttimeout centoslongevitylargepartitions djuliadbnodeaad d cqlsh requesttimeout cqlsh CONSISTENCY QUORUM Consistency level set to QUORUM cqlsh select minck as minck maxck as maxck from scyllabenchtest where pk ReadTimeout Error from server code Coordinator node timed out waiting for replica nodes responses messageOperation timed out for scyllabenchtest received only responses from CLQUORUM inforeceivedresponses requiredresponses consistency QUORUM Load on the nodes is about Screenshot from Test table Test table CREATE TABLE scyllabenchtest pk bigint ck bigint v blob PRIMARY KEY pk ck WITH CLUSTERING ORDER BY ck ASC AND bloomfilterfpchance AND caching keys ALL rowsperpartition ALL AND comment AND compaction class SizeTieredCompactionStrategy AND compression AND crccheckchance AND dclocalreadrepairchance AND defaulttimetolive AND gcgraceseconds AND maxindexinterval AND memtableflushperiodinms AND minindexinterval AND readrepairchance AND speculativeretry NONE Test log sctlogzip Split from issue When adding one million integers adding an identity function causes a slowdown of about seconds The slowdown is about the same in both the best seconds and worst seconds case which means UDF seem to just add a constant overhead for each call This tracks just reducing the UDF overhead not improving scyllas overall aggregation performance Got while running dtests in jenkins Could scylla support function as a filter Eg select id WriteTimecreatedon as time from mytable where time for table CREATE TABLE mykeyspacemytable id text PRIMARY KEY createdon timestamp WITH compaction class orgapachecassandradbcompactionSizeTieredCompactionStrategy maxthreshold minthreshold per PiotrS psarna I think the simplest way to implement this would be to allow aliases in the where clause eg SELECT writetimev as writetimeofv v FROM t where writetimeofv When a node does not have gossip STATUS applicationstate we currently use an empty string to present such state in getgossipstatus It is better to use an explicit UNKNOWN to present it It makes the log easier to understand when the status is unknown Before gossip InetAddress n is now UP status After gossip InetAddress n is now UP status UNKNOWN This patch is safe because the STATUSUNKNOWN is never sent over the cluster So the presentation is only internal to the node Fixes Fedora version of systemd macros does not work correctly on CentOS since CentOS does not support file trigger feature To fix the issue we need to override macros with CentOS version See scylladbscyllajmx As a followup to bde b be a fd d dfe e e This series implements suggestions from avikivity and espindola It simplifies the template definitions for accumulatorfor adds some debug logging for the overflow values and adds unit tests for float and double sum overflow Test unitdev pagingtestTestPagingWithIndexingAndAggregationtestfilterindexednonindexedpkcolumndev Installation details Scylla version or git commit hash unstablebranch Cluster size node OS RHELCentOSUbuntuAWS AMI ubuntu dtest Created cluster with one node Created several keyspaces which contains several tables Filled with data partition contains several rows After that run snapshot operations in parallel create snapshot clear snapshot listsnapshots the list snapshot periodically failed with next error NodetoolError Nodetool command homeabykovccmscyllarepositoryunstablebranch scyllajavatoolsbinnodetool h p listsnapshots failed exit status stdout Snapshot Details stderr error Scylla API server HTTP GET to URL storageservicesnapshotssizetrue failed filesystem error stat failed No such file or directory homeabykovdtestdtestGEXxMttestnode dataks tablecf fc c e ea snapshots mc bigFilterdb StackTrace javalangIllegalStateException Scylla API server HTTP GET to URL storageservicesnapshotssizetrue failed filesystem error stat failed No such file or directory homeabykovdtestdtestGEXxMttestnode dataks tablecf fc c e ea snapshots mc bigFilterdb at comscylladbjmxapiAPIClientgetExceptionAPIClientjava at comscylladbjmxapiAPIClientgetRawValueAPIClientjava at comscylladbjmxapiAPIClientgetRawValueAPIClientjava at comscylladbjmxapiAPIClientgetLongValueAPIClientjava at orgapachecassandraserviceStorageServicetrueSnapshotsSizeStorageServicejava at sunreflectNativeMethodAccessorImplinvoke Native Method at sunreflectNativeMethodAccessorImplinvokeNativeMethodAccessorImpljava at sunreflectDelegatingMethodAccessorImplinvokeDelegatingMethodAccessorImpljava at javalangreflectMethodinvokeMethodjava at sunreflectmiscTrampolineinvokeMethodUtiljava at sunreflectNativeMethodAccessorImplinvoke Native Method at sunreflectNativeMethodAccessorImplinvokeNativeMethodAccessorImpljava at sunreflectDelegatingMethodAccessorImplinvokeDelegatingMethodAccessorImpljava at javalangreflectMethodinvokeMethodjava at sunreflectmiscMethodUtilinvokeMethodUtiljava at comsunjmxmbeanserverStandardMBeanIntrospectorinvokeM StandardMBeanIntrospectorjava at comsunjmxmbeanserverStandardMBeanIntrospectorinvokeM StandardMBeanIntrospectorjava at comsunjmxmbeanserverMBeanIntrospectorinvokeMMBeanIntrospectorjava at comsunjmxmbeanserverPerInterfaceinvokePerInterfacejava at comsunjmxmbeanserverMBeanSupportinvokeMBeanSupportjava at comsunjmxinterceptorDefaultMBeanServerInterceptorinvokeDefaultMBeanServerInterceptorjava at comsunjmxmbeanserverJmxMBeanServerinvokeJmxMBeanServerjava at comscylladbjmxutilsAPIMBeanServerinvokeAPIMBeanServerjava at javaxmanagementremotermiRMIConnectionImpldoOperationRMIConnectionImpljava at javaxmanagementremotermiRMIConnectionImplaccess RMIConnectionImpljava at javaxmanagementremotermiRMIConnectionImplPrivilegedOperationrunRMIConnectionImpljava at javaxmanagementremotermiRMIConnectionImpldoPrivilegedOperationRMIConnectionImpljava at javaxmanagementremotermiRMIConnectionImplinvokeRMIConnectionImpljava at sunreflectNativeMethodAccessorImplinvoke Native Method at sunreflectNativeMethodAccessorImplinvokeNativeMethodAccessorImpljava at sunreflectDelegatingMethodAccessorImplinvokeDelegatingMethodAccessorImpljava at javalangreflectMethodinvokeMethodjava at sunrmiserverUnicastServerRefdispatchUnicastServerRefjava at sunrmitransportTransport runTransportjava at sunrmitransportTransport runTransportjava at javasecurityAccessControllerdoPrivilegedNative Method at sunrmitransportTransportserviceCallTransportjava at sunrmitransporttcpTCPTransporthandleMessagesTCPTransportjava at sunrmitransporttcpTCPTransportConnectionHandlerrun TCPTransportjava at sunrmitransporttcpTCPTransportConnectionHandlerlambdarun TCPTransportjava at javasecurityAccessControllerdoPrivilegedNative Method at sunrmitransporttcpTCPTransportConnectionHandlerrunTCPTransportjava at javautilconcurrentThreadPoolExecutorrunWorkerThreadPoolExecutorjava at javautilconcurrentThreadPoolExecutorWorkerrunThreadPoolExecutorjava at javalangThreadrunThreadjava steps to reproduce create cluster and node create keyspaces and tables fill data better large amount of data run nodetool snapshot to create snapshots for all keyspaces and tables run in parallel next operations nodetool snapshot nodetool cleansnapshots nodetool listsnapshots Hi there The environment Installation details Scylla version Cluster size nodes joining OS Ubuntu LTS Hardware details Platform Azure VM Hardware Standard L sv vcpus GiB memory Disks GB Premium SSD for the OS x TB NVMe SSD for data RAID TB effectively The problem A new node is unable to join the cluster when the average disk usage is high within the cluster The problem does not occur with lower disk usages ie This has also occured before when the cluster was smaller nodes or so The joining process starts as it should but after about disk being filled on the joining node GB it stops It does not do so momentarily the process simply slows until there is no progress The same goes for CPU usage on most of the cores then almost idle By tracing traffic on the network interface one can also see something similar the joining process starts with the new node being connected to all nodes and actively streaming data from about of them And then slows down by the end the transfers are of magnitude of Bps kBps There are no failurerelated logs on the joining node A way to restart the joining process is to look at nodetool netstats on the joining node and then restart scylla processes and all nodes displayed within until nodetool netstats shows no streams Then the joining process runs for another ie and then the story repeats itself The expected behaviour I understand that this particular case is stretching recommended operational conditions in terms of free disk space but I still think one of two outcomes should happen new node joins with no issues new node does not join but fails explicitly instead of going in loops without a clearly logged errors