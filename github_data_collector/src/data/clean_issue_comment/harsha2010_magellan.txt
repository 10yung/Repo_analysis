Add Support for Spark and above The change includes adding support for Apache Spark and Open JDK It also removed backward compatibility for the Spark Version below There is no feature change hence drop support will not impact any existing users because they will be using the version of older Jar I am getting below error when checking if point falls within polygon Here is the error orgapachesparkSparkException Job aborted due to stage failure Task in stage failed times most recent failure Lost task in stage TID localhost executor driver javalangRuntimeException Extra traced not evaluated at magellanWKTParserparseAllWKTParserscala at magellanWKTParserparseAllWKTParserscala at orgapachesparksqlcatalystexpressionsGeneratedClassGeneratedIteratorForCodegenStage processNextUnknown Source at orgapachesparksqlexecutionBufferedRowIteratorhasNextBufferedRowIteratorjava at orgapachesparksqlexecutionWholeStageCodegenExecanonfun anon hasNextWholeStageCodegenExecscala at orgapachesparksqlexecutionSparkPlananonfun applySparkPlanscala at orgapachesparksqlexecutionSparkPlananonfun applySparkPlanscala at orgapachesparkrddRDDanonfunmapPartitionsInternal anonfunapply applyRDDscala at orgapachesparkrddRDDanonfunmapPartitionsInternal anonfunapply applyRDDscala at orgapachesparkrddMapPartitionsRDDcomputeMapPartitionsRDDscala at orgapachesparkrddRDDcomputeOrReadCheckpointRDDscala at orgapachesparkrddRDDiteratorRDDscala at orgapachesparkrddMapPartitionsRDDcomputeMapPartitionsRDDscala at orgapachesparkrddRDDcomputeOrReadCheckpointRDDscala at orgapachesparkrddRDDiteratorRDDscala at orgapachesparkschedulerResultTaskrunTaskResultTaskscala at orgapachesparkschedulerTaskrunTaskscala at orgapachesparkexecutorExecutorTaskRunneranonfun applyExecutorscala at orgapachesparkutilUtilstryWithSafeFinallyUtilsscala at orgapachesparkexecutorExecutorTaskRunnerrunExecutorscala at javautilconcurrentThreadPoolExecutorrunWorkerThreadPoolExecutorjava at javautilconcurrentThreadPoolExecutorWorkerrunThreadPoolExecutorjava at javalangThreadrunThreadjava Driver stacktrace javalangRuntimeException Extra traced not evaluated Dataset that I using is canada provinces and here it is georefpolycazip Hi this is my first ever PR to anything so please bear with me if I have done something terribly illegalbad practice I have updated a range of the dependencies to allow for Magellan compatibility with Spark and Scala In order to make it work with the new versions of the dependencies I had to change a few things regarding classobject creation All tests are passing when I run sbt test Best Mads val spark SparkSessionbuilder appNameTesting Spark DSL masterlocal build a local cluster getOrCreate injectRulesspark import sparkimplicits val data ArrayUS TX ios US PA ios CA null android GB null ios US NC android US CA null A null android US NY ios val df sparksparkContextparallelizedatatoDFcountry state locationat horizontalaccuracy platform appid latitude longitude withColumnlocationat collocationatcastTimestampType df show printlndf printSchema val filterFilePath pathtogeojson val filteringDS sparksqlContextreadformatmagellan optionmagellanindex true optionmagellanindexprecision optiontype geojsonloadfilterFilePath cache filteringDScount filteringDSshowfalse val filtered df withColumnlocationPoint pointcollongitude collatitude joinfilteringDS wherecollocationPoint within colpolygon filteredshow Using the example above if I just injectRules I get results But if I dont use injectRules I get the proper results Also to note Ive tried different levels of precision in the index but the same issue persisted when injecting the rules Geojson file used for testing attached TXgeojsontxt I would like to run the NYC Taxicab analysis notebook with Azure Databricks but the data is in S How do I save the data into Azure Would I save to Azure Data Lake Store and then mount it to Databricks Thanks I need to use a functionality which is part of the current master but is not a part of the latest release More specifically read shapefiles Polygons and Points and convert them to GeoJson I am referring to the asGeoJSON srcmainscalamagellandslpackagescala method for this purpose Could you please share when the next release is going to be made Hi Ram very interesting project so i have a DF with neighborhoods represented as ZCurves in index column var index neighborhoodswithColumnindex polygon index how to obtain the list of geohashes that represent each zone instead same issue as mentioned here he said he could solve it using the toBase function but he did not mention how can you tell me how to use this functionality for example with my index dataframe above thanks a lot and wishing all the best for the project Include package in spark using sparkshell packages harsha magellan s Try import magellancoordNAD Error object coord is not a member of package magellan 