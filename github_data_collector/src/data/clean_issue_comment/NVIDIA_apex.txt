Hi Im wondering is it okay to use distributed training without calling ampinitilize if I dont want to use the mixedprecision ability When training my model which samples from Categorical distribution I every now and then get this stack trace python cvlabdata hometyszkiewPhDpointsalgorithmpy in pointdistributionlogits spatialdist Categoricallogitslogits candidatemarks spatialdistsample spatiallogp spatialdistlogprobcandidatemarks survivallogits torchgather usrlocallibpython distpackagestorchdistributionscategoricalpy in logprobself value value logpmf torchbroadcasttensorsvalue selflogits value value return logpmfgather valuesqueeze def entropyself RuntimeError cuda runtime error deviceside assert triggered at pytorchatensrcTHCgenericTHCTensorScatterGathercu This problem seems to be ameliorated by decreasing the magnitude of logits but is otherwise elusive I was thinking that normalization fails due to numerical precision and samples go outside of the allowed range but it seems to be handled I use apex in the maskrcnnr fpn xpy mmdetection and use pytorch roi layer when I choose optlevel O report error from torchvisionops import roialign as tvroialign File optanaconda libpython sitepackagestorchvisioninitpy line in module from torchvision import models File optanaconda libpython sitepackagestorchvisionmodelsinitpy line in module from import detection File optanaconda libpython sitepackagestorchvisionmodelsdetectioninitpy line in module from fasterrcnn import File optanaconda libpython sitepackagestorchvisionmodelsdetectionfasterrcnnpy line in module from rpn import AnchorGenerator RPNHead RegionProposalNetwork File optanaconda libpython sitepackagestorchvisionmodelsdetectionrpnpy line in module from import utils as detutils File optanaconda libpython sitepackagestorchvisionmodelsdetectionutilspy line in module torchjitscript File optanaconda libpython sitepackagestorchjitinitpy line in script return compilefunctionfnobj qualifiednamequalifiedname framesupframesup rcbrcb File optanaconda libpython sitepackagestorchjitinitpy line in compilefunction scriptfn torchCjitscriptcompilequalifiedname ast rcb getdefaultargsfn File optanaconda libpython sitepackagestorchjitinitpy line in trycompilefn qualifiedname qualifiednamefn File optanaconda libpython sitepackagestorchjitinternalpy line in qualifiedname module cant be Noneformatname RuntimeError Could not get qualified name for class log module cant be None when I choose optlevel O report error ValueError nan or inf found in loss I set lossscale to the program also report this error When I run my own program without using mixed precision training it works well But when I run it with mixed precision training I get this message Traceback most recent call last File trainSelectiveNetGoPropy line in module mainargs File trainSelectiveNetGoPropy line in main scalelossbackward File homegrdminiconda envstorch libpython contextlibpy line in exit nextselfgen File homegrdminiconda envstorch libpython sitepackagesapexamphandlepy line in scaleloss shouldskip False if delayoverflowcheck else lossscalerupdatescale File homegrdminiconda envstorch libpython sitepackagesapexampscalerpy line in updatescale selfhasoverflow selfoverflowbufitem RuntimeError CUDA error an illegal memory access was encountered I have installed apex with pip install v nocachedir globaloptioncppext globaloptioncudaext however the project i want to use is build in python so in that project i am unable to import apex How can i import or how can i install apex with python pip is not working for me it throw many errors My environment ubuntu CUDA Version CuDNN torchversion Python Hello I have a model where in each forward pass layers are randomly skipped with some probability something like this def forwardself x p for i in rangeselfnlayers drop this layer with probability p randomnumber randomuniform if selftraining and randomnumber p continue x selflayers i x return x Theoretically higher p should result in faster training as the number of skipped layers is higher However when training this model using apexparallelDistributedDataParallel I observed a very significant slowdown when p I try timing the forward passes and indeed they were faster when p is higher so I think the issue lies in the backward or the gradient gathering steps Note that this issue does not occur when using torchnnparallelDistributedDataParallel I had to set findunusedparameters True for it to work faster training for higher p Could you please help checking this When all layers are used ie p apex was faster than torchnn especially on multiple nodes so I hope that I can use apex for all my experiments Thank you very much in advance Hi I have this bug following could you help me to fix that File homeshiz frameinterpolationmainpy line in main scaledlossbackward File homeshiz anaconda libpython sitepackagestorchtensorpy line in backward torchautogradbackwardself gradient retaingraph creategraph File homeshiz anaconda libpython sitepackagestorchautogradinitpy line in backward allowunreachableTrue allowunreachable flag RuntimeError cublasOpFromChar input should be t n or c but got This string delayallreduce is an undefined name in this context which will raise NameError at runtime instead of the expected ValueError selfdelayallreduce is set on line and is used three lines below the modified line buckets is an undefined name in this context which will raise NameError at runtime selfbuckets is used on the previous line and bucket best matches the use on this same line flake count selectE F F F showsource statistics apexparalleldistributedpy F undefined name delayallreduce if selfallreducedifferentstreams and delayallreduce apexparalleldistributedpy F undefined name buckets b lenbuckets b selfbucketsizes b 