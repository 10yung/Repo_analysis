In training of generator if dropout is used in discriminator the GAN loss for generator calculated will be affected by the dropout effect of discriminator GAN loss DAGAA selflossGA selfcriterionGANselfnetDAselffakeB True GAN loss DBGBB selflossGB selfcriterionGANselfnetDBselffakeA True So should we use netDAeval and netDBeval before the calculation of GAN loss in order to eliminate the dropout effect E Anaconda pythonexe EPycharmCodeFruitcounttrainpy Traceback most recent call last File EPycharmCodeFruitcounttrainpy line in module torchcudasetdevicegpus File E Anaconda lib sitepackages torch cuda initpy line in setdevice torchCcudasetDevice AttributeError module torchC has no attribute cudasetDevice As already mentioned in this issue the current residual block implementation might not be optimal Currently the residual block is implemented as conv bn relu conv bn add According to this paper by Kaiming He who is the main author of the original Resnet paper a better order would be bn relu conv bn relu conv add I adjusted the ResnetBlock and ResnetGenerator classes in networkspy to use this optimized residual block and trained default pix pix with and without this change on the Cityscapes dataset Here you can find a result comparison Left is current pix pix without the change and right is the optimized version Overall I would say this change slightly improves image quality as the generated images are slightly more detailed probably due to the additional nonlinearities The effect is very minor though Also I neither tested it on CycleGAN nor conducted any quantitative comparisons For net with IN layers it should be fine when batchsize is larger than when testing For net with BN layers can this also be possible if replacing the BN with IN layers by assigning weights from corresponding BN layers to those IN layers Or just Adding a net option to BN layer making it perfoms instance normalization I do current BN does not support it As sometimes the testing set are very large and batchtesting mode should benefit a lot I trained a cyclegan with images from cityscapes A and plant images all default cyclegan options Testing with images from the cityscapes dataset gives the expected results here from visdom but running the testpy script gives the same results However when I take any new image for example a random google street view screenshot it seems as though there are almost no modification to the image Real Fake Is this expected Does the network need to be trained longer Im only at two epochs despite showing interesting results using the test dataset I am working with images x and trained a resnet pix pix with preprocess None The output images are x and it is the left cropped image from the former x image even though I specify preprocess None during testing Any help Hi Lets assume my datasets have samples and samples When I then start training how does the augmentation work Is it only for the smallest dataset to balance the datasets And does it randomly rotate translate etc Or how does it work First thank you for the great work Ive trained a pix pix model using my own dataset Now I want to use the trained model on another dataset to do the imagetoimage translation My initial understanding is that I need to use the testpy script Can you provide a little insight into what input flags should I set for the testpy script Also will the saved results preserve the file name of the input image Thanks Can anybody please help me how to train the models in YIQ LAB HSV colourspaces I am not able to understand that simply reading the files and using BGR HSV wont suffice I guess What changes should be done as I am a beginner in this package Hi I have used your pix pix GAN model with my custom dataset But the process of generating image pairs takes a lot of time I have very less data around image pairs Even when I tried cycle GAN the results were not great I came across the term selfsupervised learning My question is whether can I use selfsupervision to image translation tasks since I have very little data