This PR removes the ioLimitReader that was throwing away server responses beyond an MB threshold The presence of that LimitReader was causing pkmount to fail for me with an unexpected EOF in JSON decoding here on directories with large subtrees Hello all Here is the situation where I am Im looking to keep in Perkeep a note with words in bold underlined words italics words etc so the note is not a txt file but a rtf file So after I uploaded this note on Perkeep I find out that the web UI couldnt display it as Document like the web UI can for txt or pdf format file So I tried to convert the rtf note to the html format hopping that maybe the web UI would display it but the same he didnt So I was wondering if there is a file format for formatted text like rtf html md etc that the web UI could display as Document in sort that we dont need each time to do a right click and then click to View original and then we are out of the webUI like he does for txt file If theres not I was wondering if it could be possible to add a rtf reader as there is for pdf files to the web UI Maybe to display the rtf file the webUI could convert the rtf file to the html format and then display the html page as Document To display the html page the webUI could maybe let the web navigator doing that Or maybe that theres others simpler solutions Thanks Gustav This PR adds some UI improvements to audio content blobs and adds a media predicate that searches through all media tags Related to Some context I am hacking on the audio file handling a bit as I want to use perkeep to play music from my collection on my phone and just wanted to share this early on The next thing I wanted to do is have a central player element instead of a perfile one so that I can navigate while its playing Possible fix for Have been running with this change for a little while havent seen the deadlock again yet pretty sure I would have by now Also go test race reports no new race conditions Mostly this involved adding keys to composite literals which go vet recently started requiring However there are a few legitimate bugfixes in here Some structs were being copied with a b that should not have been because they included syncMutexes Some unreachable code was meant to be retrying indexing failures Some context cancelers were not being called This PR also runs the whole tree through go fmt producing a few additional diffs Ive been encountering ratelimitexceeded errors trying to dump a lot of files into a GCS backend via pkmount I thought what I needed was a rate limiter Thats But the problem persisted and I discovered that the GCS blobserver was repeatedly reuploading the same blob over and over to wit my PGP public key So I wrote this PR to skip uploading a blob if its already present But thats still not good enough to make the ratelimitexceeded errors go away I figure this change is still a good one anyway so here it is while I dig a bit more Fixed in by using an explicit call to Attrs before the call to NewWriter This solves ratelimitexceeded errors when dumping large numbers of blobs into a Google Cloud Storage backend The default limit is request per ms and can be overridden with the new optional setting ratelimit Fixes Maybe overkill This compresses all server responses that issue suggested compressing only JSON responses Run go get t u on direct dependencies in gomod with semver versions v Turns out thats just a few Also rerun go mod vendor Among other things this gets the upsert SQL changes in sqlite needed for This PR is a preliminary sketch for a new blobserver type that uses files uploaded to it as their own storage When you add a file to an fsbackedStorage thats within the directory tree it controls an entry is added to a database that maps between files and blobrefs but the files contents are not copied anywhere When fetching the files content blob later the database directs the Storage to the right local file and the data is served from there Adding files outside the directory tree or adding any other kind of blob fails over to another blobserver nested inside the fsbackedStorage This solves the problem of wanting to add a tree of large files eg videos of my kids growing up to a local Perkeep instance without storing all the data twice This should be used only on directory trees whose files do not change lest the blobrefs in the database become mismatched to their corresponding files A number of other changes throughout Perkeep would be needed to make this truly useful The ioReader presented to a blobservers ReceiveBlob method is usually always some wrapper object like checkHashReader that conceals the underlying osFile without which fsbackedStorage cannot detect that a file within its tree is being uploaded And in any case Perkeep imposes rather a low limit on blob sizes for this purpose Presented for further discussion