typo I believe Thanks for providing the sergeant package My use case is reading a single column of data from a fairly large parquet file The column I want to read from the parquet file is called occurrenceId and the whole column fits into a character vector of length M in R where each value is a string of length characters it is a unique identifier and Id like to check it for uniqueness and presence in R In R the whole column would occupy about M in memory when I inspect it with ls I can do this with sparklyr but the drill sergeants approach is appealing being more lightweight I am struggling with an outofmemory issue though and I have GB available which I think should suffice given the size of the vector in R so now Im wondering if this use case is supported or if Im doing it wrong The dataset Im using is public and can be viewed here and it can be downloaded from here in zip format I first tried using vroom and reading directly from the compressed zip file details here but the promise of being able to read directly from a comparatively smaller parquet file and being able to just read the columns I need made me turn to the sergeant So in my attempt to read the parquet file I have first converted the zip to parquet using sparklyr like so r librarysparklyr librarydplyr first install spark hadoop with sparklyrsparkinstall SyssetenvSPARKMEM g config sparkconfig configsparklyrshelldrivermemory G configsparklyrshellexecutormemory G configsparklyrdefaultPackages comdatastaxsparksparkcassandraconnector M configsparkcassandracassandrahost localhost configsparkdrivermaxResultSize G configsparkexecutorcores is pushdown option TRUE sc sparkconnectmaster local config config for this connection load all records systemtime sparkreadcsvsc memory FALSE name artdata path filehomerogerartdataartdatatsv delimiter t user system elapsed generate a parquet file based on the dataframe above systemtime sparkwriteparquet tblsc artdata filehomerogerartdataartdataparquet user system elapsed the parquetfile is GB on disk smaller than the zip sparktblhandle sparkreadparquetsc memory FALSE artdata filehomerogerartdataartdataparquet hasvalidbor function bor sparktblhandle countbasisOfRecord collect mutateisok basisOfRecord in c humanobservation machineobservation bor pullisok all nrowcount function sparktblhandle summarisen n pulln hasvalidid function ids sparktblhandle countoccurrenceID filtern isnaoccurrenceID collect nrowids systemtime hasvalidbor systemtime hasvalidid systemtime nrowcount sortartdata function sparktblhandle arrangeoccurrenceID head collect systemtime sortartdata sorting in spark takes about minutes user system elapsed This gives me a parquet file on disk I then proceed to attempt to use the sergeant to read the occurrenceId column like so r librarysergeant librarytidyverse if Syswhichdocker stopPlease install docker first see install and run official Apache Drill software systemdocker stop drill docker rm drill docker run i name drill e DRILLHEAP G v homemarkustmpartdatatmp p detach drillapachedrill binbash dc drillconnectionlocalhost df drillquerydc SELECT occurrenceId FROM dfstmpartdataparquet The error message that I get is the following Query SELECT occurrenceId FROM dfstmpartdataparquet RESOURCE ERROR There is not enough heap memory to run this query using the web interface Please try a query with fewer columns or with a filter or limit condition to limit the data returned You can also try an ODBCJDBC client Error Id f e d afb a a baaa c I tried to set the DRILLHEAP to G Is this use case supported with the sergeant Any advice on how I should proceed I am eager to try out what described in the blog post about version but after having installed as instructed r devtoolsinstallgit ref and then running the code below I got a failure librarysergeant librarytidyverse Attaching packages tidyverse ggplot purrr tibble dplyr tidyr stringr readr forcats Conflicts tidyverseconflicts dplyrfilter masks statsfilter dplyrlag masks statslag start Docker in terminal then open dr drillupdatadir hereheredataraw Drill container started Waiting for the service to become active this may take up to s Error in drillupdatadir hereheredataraw Could not connect to Drill container I have been able to have drill running manually and mapping the local file system to my directory of interest On the Docker side I have an old MBP so I have Docker Toolkit That maybe is the reason for the failure If I run drill image from the command line I get the following docker run drillapachedrill master Jan PM orgjlineutilsLog logr WARNING Unable to create a system terminal creating a dumb terminal enable debug logging for more information Apache Drill The only truly happy people are children the creative minority and Drill users jdbcdrillzklocal Closing orgapachedrilljdbcimplDrillConnectionImpl I tried to helloworld Docker example using stevedore package and it worksso somewhat Docker at least the easy bit seems to work I then decided to dig into drillup implementation and executed line by line to debug docker stevedoredockerclient drill dockercontainerrunimage drillapachedrill name drill cmd usrlocalbinbash volumes sprintfsdata hereheredataraw E Jan PM orgjlineutilsLog logr E WARNING Unable to create a system terminal creating a dumb terminal enable debug logging for more information E Apache Drill E Say hello to my little Drill E Closing orgapachedrilljdbcimplDrillConnectionImpl O jdbcdrillzklocal dockerrunoutput container dockercontainer id db d f f d dd b e c fbbe e cf c adb a name drill logs Jan PM orgjlineutilsLog logr WARNING Unable to create a system terminal creating a dumb terminal enable debug logging for more information Apache Drill Say hello to my little Drill Closing orgapachedrilljdbcimplDrillConnectionImpl jdbcdrillzklocal Then the attempts of drillactive failhere I just show one manual try drillcon drillconnectionlocalhost Drill REST API Direct Connection to localhost drillactivedrillcon FALSE I am out of ideas to try to make it workany directions is welcome As of Drill we now know the data types of things returned so we can use this to have proper bit integers as well as proper types all round makes it possible to use nonhardcoded creds so it finally makes sense to add some examples of how to query S data It would be nice if the seargeant package could create new tables as I can do with other database drivers Something like this librarysergeant librarydplyr conn dbConnectDrill Load original data originaldata tblconn dfsdownloadsoriginaldata Do some operation editeddata originaldata filtername John Write the result to a table without loading it into R dbWriteTableconn table dfsdownloadsediteddata values editeddata 