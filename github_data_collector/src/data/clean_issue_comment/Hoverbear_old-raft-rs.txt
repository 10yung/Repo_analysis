I would like to testdrive this raft consensus implementation however at the moment I cannot build master I solved a couple of problems by changing in the Cargotoml the following version bincode because alpha is yanked capnpnonblock the lib isnt mantained but receveid an update to fix a bug now the problem is rustc version rustc nightly b ff cargo build Compiling raft v filerootraftrs error failed to run custom build command for raft v filerootraftrs process didnt exit successfully rootraftrstargetdebugbuildraftbea e c cbuildscriptbuild exit code stderr thread main panicked at called Resultunwrap on an Err value IoError repr Os code message No such file or directory checkoutsrclibcoreresultrs note Run with RUSTBACKTRACE for a backtrace Authors propose capnpfutures to be used instead I want to use Rust and therefore raftrs as a consensus algorithm for some distributed management systems The readme says that is currently not designed for production use and log compaction is not implemented which means the log wil grow without bounds if I understood this correctly Is this library eventually going to be complete I get the following errors test serverteststestclientaccept FAILED test serverteststestinvalidclientmessage FAILED When running the key value storage example I repeatedly get the following error WARNraftserver Server unable to accept connection A request to send or receive data was disallowed because the socket is not connected and when sending on a datagram socket using a sendto call no address was supplied os error Apparently the nodes cannot communicate with each other so they all stay in candidate status Is this a problem of mio I looked at raftserver but couldnt find any obvious bug When a client proposes a command at the leader but the leader steps down before the command commits the client is left waiting Simply telling the waiting client that its proposed command failed isnt a great option The command may still commit if it has been replicated to another node its likely that that node has obtained leadership One convenient way out is the following We already internally store not only the pending command but also the log index i of that command Once a command commits at i its guaranteed that either a the command committed is the one we were waiting for or b the command will not commit at any point in the future So followers can abort any pending commands when their respective index is exceeded Currently the pending proposals are a property of LeaderState so that would have to change Likely makes sense to move them into Consensus proper I noticed that raftrs almost builds on stable only the sockettimeout feature which is used in a single location requires nightly The tests dont pass but I think but havent checked that thats mostly the way we use serde which can be taken care of Is getting to stable sooner rather than later interesting or do we not care As of today the master will immediately satisfy client queries which may result in stale reads during failover scenarios Diegos thesis goes into detail about the cause and possible solutions to this issue in section We should come up with a plan for how to tackle this In chapter on client interaction of the thesis there is a paragraph regarding the requirements for achieving linearizability in Raft To achieve linearizability in Raft servers must filter out duplicate requests The basic idea is that servers save the results of client operations and use them to skip executing the same request multiple times To implement this each client is given a unique identifier and clients assign unique serial numbers to every command Each server s state machine maintains a session for each client The session tracks the latest serial number processed for the client along with the associated response If a server receives a command whose serial number has already been executed it responds immediately without reexecuting the request Given this filtering of duplicate requests Raft provides linearizability Looking through the raftrs implementation I didnt find support for client request serial numbers or session management as described above Is this intentional or an oversight As mentioned here itd be good to be able to fetch low high from the log tschottdorf will likely expand on this as he brought it up on IRC The log currently does not store appliedindex and lastcommitedindex and this will likely be necessary if say a machine is rebooted and it needs to know whats been committed and applied We noted that both are needed because we might be committing say entries but the state machine may take time to apply them 