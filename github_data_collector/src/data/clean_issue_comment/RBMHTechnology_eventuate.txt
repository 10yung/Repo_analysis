After disaster recovery the local sequence is potentially adjusted to ensure that it is larger than the local logs time see also commit for If Cassandra is used as backend and the gap introduced through this adjustment covers empty partitions replays from this event log stop prematurely as they do to proceed after the first empty partition Currently were getting OutOfMemoryErrors in a node thats just replicating events running with leveldb Because the node is only replicating events we assumed that it does not need much memory therefore its running with G max heap The heap dump analysis showed that eventuates LeveldbNumericIdentifierStore consumes GB RAM which is allocated by the internal idMap with M entries sshotsync heapdumpidstore AFAICS the idMap stores aggregate ids right For now well increase the max heap size of the java process which we could also keep as solution assuming that the string ids are aggregate ids It might still be considered to reduce the memory requirements of the leveldb event log Because the LeveldbNumericIdentifierStore stores Int String numeric id to string id in leveldb its currently not possible to resolve a string id to its numeric id via leveldb in def numericIdid String Int Ie we cannot simply get rid of the idMap If we dont want to also store the String Int mapping in leveldb wed have to use a more memory efficient Map implementation for this use case WDYT Currently the number of events that are replicated through a single akkamessage is fix eventuatelogwritebatchsize if enough events wait for replication If the binary representation of the resulting message exceeds the akkaremote frame size akkaremotenettytcpmaximumframesize the message is dropped at the source and the replication essentially stops as each retry results in the same error A problem like this can only be fixed by reconfiguring ether eventuatelogwritebatchsize or akkaremotenettytcpmaximumframesize and restarting the application or at least the actorsystem To avoid a restart in these situations eventuate could either adapt the replication batch size dynamically for example if n n replication requests did not receive a response the batch size is reduced incrementally by factor f f until a replication response is received After that the batchsize can be increased again either incrementally by factor f or at once to its original size Alternatively the batchsize is no longer measured in number of events but rather in number of bytes We want to use eventuate types in our events currently VectorTime It would be great to reuse the protobuf definitions Therefore it would be nice to have them published via a seperate artefact This is a huge PR It arose from my attempts to run the code examples in the User Guide Along the way I realized that most of them in fact did not run so I raised As I mentioned in the comments the code examples needed to be reorganized so they could be run This PR Adds material to the User Guide should be checked for accuracy Moves code examples to the new toplevel examples directory Adjusts every reference to the moved code examples Adds TODO and FIXME comments in broken code examples remember these examples were previously broken All tests pass Commits were squashed There is no easy way to make these changes and I think it makes sense to do all the work in one PR instead of many repetitive and painful PRs I suggest that the embedded FIXME and TODO comments remain that the PR be merged and then followon PRs be created to address related FIXME and TODO issues The fix for introduced an optional field persistOnEventId in PersistOnEventRequest This is optional just for backwards compatibility to existing snapshots Once is released applications can recreate snapshots to make sure all PersistOnEventRequests have this field defined With an release the implementation can be simplified by making this field mandatory ie drop support for reading snapshots written with releases Users who are just getting started with Eventuate like me would find it very helpful to work through the code examples in the User Guide Following are suggestions to make that experience better The User Guide should say that the code exists at It would be better if this code was a series of small projects so the dependencies would be better understood and so the reader would know that each project is independent Following my own suggestions I made a GitHub repo for UserGuideDocscala I refactored the User Guide code into separate Scala files for easier reading One big file for the entire chapter is painful to figure out Updated to Scala and SBT Ascribed types where they were not declared Applied minor reformatting while attempting to not disturb the Sphinx comments Created applicationconf for Akka configuration In doing the above I discovered that the mysterious LevelDB mentioned in the docs was actually a Scala port of the Google LevelDB project originally written in C It would be good to tell readers early on that LevelDB is a fast keyvalue storage originally developed by Google which provides ordered mapping from ByteArray keys to ByteArray values When the sample project is ready it might be a good idea to move the repo over to a subproject under RBMHTechnologyeventuate and replace UserGuideDocscala so others can have a quicker start If a location loses the local event log including backups due to a disaster it can recover events from other locations through replication connections This will recover all events stored at remote locations considering replication filters Depending on the number of events to be recovered this can take a significant amount of time where the location is not available Especially in the case of locations that are using event deletion it can make sense to recover not all but just the most recent events So disaster recovery should allow the location to be recovered to specify a limit for the events to be recovered for example in form of sequence numbers in the remote logs so event recovery begins from this sequence number When the CassandraEventLog gives up retrying a failed write it stops itself As a consequence all eventsourced components connected to this log stop themselves The error logging could be improved in that case the event log could add some information about the events to be written and the components could log why they are stopping 