Hello Lastest debian x context How to install timescaledb as pure postgres extension on already installed postgres db from official postgres sources Can someone help with this question Ubuntu TimescaleDB version Docker In version youve added additional functionality for continuous aggregates that allows us to ignore invalidation and therefore we can keep data in the cagg even if we drop the raw data This is great but only if we have options to limit the amount of data we keep in the continuous aggregate For example if we use a cagg to rollup to a minute interval If we have a lot of items we store data for we could quickly start hitting memory limits from having too much minute data It would be nice to be able to use dropchunks on a continuous aggregate so that we can manage the data retention for rollups of different widths An example if we have stock symbols and we collect raw tick data for each into a hypertable we can then rollup to m intervals using a continuous agg We can then drop the raw data aftersay hours The problem is that after a couple of weeks well have too much minute data weeks days minutes symbols Billion rows in our continuous agg view Relevant system information OS CentOS PostgreSQL version output of postgres version TimescaleDB version output of dx in psql Installation method rpm install from repository Describe the bug A call to dropchunks seems to cause a deadlock occasionally The following locks are present until I kill the dropchunks task Both locks are then cleared PID SELECT typinputarrayinregproc typtype FROM pgcatalogpgtype LEFT JOIN select nsoid as nspoid nsnspname rr from pgnamespace as ns join select sr currentschemasfalse sr as nspname from generateseries arrayuppercurrentschemasfalse as sr as r using nspname as sp ON spnspoid typnamespace WHERE typname ORDER BY spr pgtypeoid DESC LIMIT Drop chunks command run every hour select dropchunksschemaname metricv tablename counterdata olderthan interval d To Reproduce This is difficult to reproduce reliably I have a running system where datapoints are inserted at an average rate of points per second The points are generated by clients A maintenance task runs every hour to drop old chunks select dropchunksschemaname metricv tablename counterdata olderthan interval d Expected behavior Most of the times the dropchunks function returns after seconds I would expect this to always be the case Actual behavior Some time dropchunks locks until the function is cancelled Screenshots The following screenshot shows the rate at which points are written to the database It is clear that the deadlock situation happened a couple of times during the day In each case the access to the database was locked such that no inserts could be completed img width altScreenshot at src Relevant system information OS Ubuntu LTS CPUs Memory GB PostgreSQL version TimescaleDB version Installation method apt Describe the bug Our simple hypertable uuid time value contains billion rows divided over week chunks The size of the chunks varies wildly but weve made sure that the largest one comprises no more than of main memory Apart from an index on time an extra index primary key on uuid time was created to support our queries The database was tuned via timescaledbtune Table statistics were up to date What we observe is that the query planning time does not scale well with the number of chunks that have to be touched In contrast execution time is fast even for the largest interval never exceeds second EXPLAIN ANALYZE SELECT timebucket week time AS bucket avgvalue FROM timeseries WHERE uuid b a d d a dcad caf e d AND time BETWEEN AND GROUP BY bucket ORDER BY bucket and planning time ms and planning time ms and planning time ms and planning time ms and planning time ms and planning time ms No interval uuid filter only planning time ms Expected behavior We expected less query planning overhead but are unsure since were new to timescaledb Planning times in your blog for chunks are much much smaller Optimizing queries on TimescaleDB hypertables with thousands of partitions Hi devs per Slack discussion I have a large table with year of data with a Continuous Aggregates view the view has a missing column so I have to drop the view then recreate it But as Continuous Aggregates always start from day which is quite long time ago it makes the whole physical view useless for quite a while I suggest when creating Continuous Aggregates allow some option to aggregate the data from latest first Then backwards to the oldest This will make the view available sooner Hello everyone We are considering TSDB for supporting events in our production environment and need some help with the same On the official site of TSDB I see installers available for ubuntu and redhat in addition to other platforms However I dont see one for SLES Could some one help NOTE Installation using source code or on docker is not an option for us Any help will be appreciated Thanks Pooja Relevant system information OS CentOS PostgreSQL version TimescaleDB version Installation method Source Describe the bug I am following the documentation to compile timescale from the source but I am getting asked about the C flag which should be there in the makefile Bootstrap the build system bootstrap To build the extension cd build make To install make install Actual behavior My steps are cd contribtimescaledb bootstrap DPGCONFIGusrlocalpgsqlpgsql binpgconfig DREGRESSCHECKSOFF cd contribtimescaledb build make make install The output is postgresql contribtimescaledb tslsrccompressioncompressionc In function createpercompressedcolumn postgresql contribtimescaledb tslsrccompressioncompressionc error for loop initial declarations are only allowed in C mode for int col col indescnatts col postgresql contribtimescaledb tslsrccompressioncompressionc note use option stdc or stdgnu to compile your code postgresql contribtimescaledb tslsrccompressioncompressionc In function populatepercompressedcolumnsfromdata postgresql contribtimescaledb tslsrccompressioncompressionc error for loop initial declarations are only allowed in C mode for int col col numcols col postgresql contribtimescaledb tslsrccompressioncompressionc In function rowdecompressordecompressrow postgresql contribtimescaledb tslsrccompressioncompressionc error for loop initial declarations are only allowed in C mode for int col col rowdecompressornumcompressedcolumns col make tslsrcCMakeFilestimescaledbtsldircompressioncompressionco Error make tslsrcCMakeFilestimescaledbtsldirall Error make all Error So why the makefile does not include the stdc flag Relevant system information OS Linux localhost gentoo PostgreSQL version TimescaleDB version Installation method using docker Describe the functionality Sometimes a system needs to add support for new data that requires some backfilling to add older preexisting data to the database eg adding support to a new external market pair to a financial solution or backfill missing data to an already stored timeseries Another case would be if your system automatically enables the compression job at startup to ensure it is running in case it crashed with it disabled and you were in the middle of a backfilling when the system crashed in this case the system would reenable compression before the new data reached current time data To do that correctly the system will need to disable temporarily compression so it can add old data without TimescaleDB trying to compress it The issue arrives when some of this new data is trying to be added in an already compressed chunk In that case the documentation shows that we need to manually decompress the chunk and then insert the data The problem is that currently there is no way to know what these chunks would be theoretically showchunks would be used for that but this only works with only one dimension the interval timestamp not more For example consider a case that I have a table with deviceid and time as dimensions In this case I have full deviceid data until current time already in the DB and the system is backfilling data from to today for deviceid In the middle that backfilling lets say at the system crashed for some reason and compression was reenabled This means that now the last row at for deviceid is in a compressed chunk but I dont know which block it is The only solution is to run select showchunkstable newerthan and decompress all the chunks returned This works but it will decompress a lot of chunks that contain only data for deviceid especially if I use something like a big numberpartitions number to force different deviceid values to be stored in individual chunks Possible Solutions Allow multiple dimensions in showchunks eg select showchunkstable deviceid newerthan Allow to get a chunk of specific row If I can get some row from the DB with select from table where deviceid order by time desc limit and get this row chunk I can then decompress this chunk only and safely continue the backfilling Return compressed chunk name if the insert fails because it is compressed This already happens in the error message but it is not a structured message so I dont think it is safe and wise to parse it to get the chunk name This was previously discussed in detail in this thread in Slack Thanks for your attention Relevant system information OS Centos PostgreSQL TimescaleDB Installation method yum install Describe the bug Considering this hypertable having hundreds of chunks sql CREATE TABLE publicimpactsoundrisktiles keyid bigint NOT NULL tile publicraster NOT NULL SELECT createhypertablesoundrisktiles keyid chunktimeinterval createdefaultindexes FALSE CREATE INDEX indexsoundrisktilesontile ON publicsoundrisktiles USING brin keyid and this request sql EXPLAIN ANALYZE WITH keys AS SELECT AS id route AS SELECT ARRAY DOUBLE PRECISION AS coordinates STSetSRIDSTMakePoint AS waypoint SELECT STValuetile waypoint FROM soundrisktiles keys route WHERE soundrisktileskeyid id AND soundrisktilestile waypoint then every partition is checked as seen here details summaryEXPLAIN ANALYZEsummary p sql Nested Loop cost rows width actual time rows loops Join Filter hyper chunktilegeometry routewaypoint Rows Removed by Join Filter CTE keys Result cost rows width actual time rows loops CTE route Result cost rows width actual time rows loops CTE Scan on route cost rows width actual time rows loops Nested Loop cost rows width actual time rows loops CTE Scan on keys cost rows width actual time rows loops Append cost rows width actual time rows loops Bitmap Heap Scan on hyper chunk cost rows width actual time rows loops Recheck Cond keyid keysid Bitmap Index Scan on hyper chunkindexsoundrisktilesontile cost rows width actual time rows loops Index Cond keyid keysid Bitmap Heap Scan on hyper chunk cost rows width actual time rows loops Recheck Cond keyid keysid Bitmap Index Scan on hyper chunkindexsoundrisktilesontile cost rows width actual time rows loops Index Cond keyid keysid hundreds of partitions Bitmap Heap Scan on hyper chunk cost rows width actual time rows loops Recheck Cond keyid keysid Bitmap Index Scan on hyper chunkindexsoundrisktilesontile cost rows width actual time rows loops Index Cond keyid keysid Bitmap Heap Scan on hyper chunk cost rows width actual time rows loops Recheck Cond keyid keysid Bitmap Index Scan on hyper chunkindexsoundrisktilesontile cost rows width actual time rows loops Index Cond keyid keysid Planning Time ms Execution Time ms p details Otherwise if the request is against a constant sql EXPLAIN ANALYZE WITH route AS SELECT ARRAY DOUBLE PRECISION AS coordinates STSetSRIDSTMakePoint AS waypoint SELECT STValuetile waypoint FROM soundrisktiles route WHERE soundrisktileskeyid AND soundrisktilestile waypoint then only the desired partition is checked as seen here sql Nested Loop cost rows width actual time rows loops Join Filter hyper chunktilegeometry routewaypoint Rows Removed by Join Filter CTE route Result cost rows width actual time rows loops CTE Scan on route cost rows width actual time rows loops Append cost rows width actual time rows loops Bitmap Heap Scan on hyper chunk cost rows width actual time rows loops Recheck Cond keyid Rows Removed by Index Recheck Heap Blocks lossy Bitmap Index Scan on hyper chunkindexsoundrisktilesontile cost rows width actual time rows loops Index Cond keyid Planning Time ms Execution Time ms Expected behavior Should be optimized like if it was a constant check Actual behavior All partitions are checked regardless of the narrow value of the partition key 