Currently I am using one instance of Db during all execution of my service but every time the process is interrupted my db gets corrupted I understand the motivation but its too long the time for recovery in the next start Is there an option to safe save the current data and on the recovery moment jump to her I have already tried the flusheveryms and snapshotafterops config options but without success sled rust rustc e os raspbian armv bits My related code rust let db sledConfigdefault pathvarlibtapbeerdb flusheverymsSome snapshotafterops open expectErro ao iniciar o banco de dados local infoBanco de dados local size was recovered dbsizeondisk dbwasrecovered let dbrw ArcnewRwLocknewdb threadspawnmove loop beerdbsyncdbrwclone threadsleepDurationfromsecs When I write my data rust let data serdejsontostring self expectError on serialize tostring treeinsertselfuuidasbytes dataasbytestovec expectError on store the serialized row treeflush expectError on flush the stored row This adds batch operations to testtreefailpoints Im aware that batches will be overhauled in light of so its too early to merge this just yet Ive noticed two classes of failures from running this When the batch is applied a FailPoint error bubbles up inside Reservation as Dropdrop and gets unwrapped Should this use globalerror instead of unwrapping Some test runs will report failed to makestable after seconds I havent dug into this much yet Currently importing an exported sled database can panic is many ways and at least one of them I would consider to be surprising and undocumented Here are the ways it can panic Failed to open a tree with the given name Failed to pop a value from the keyvalue pair Vec Failed to pop a key from the keyvalue pair Vec Failed to insert this keyvalue pair into the open tree The value returned by this insert is not None This is in an assert statement The CollectionType value is anything other than btree To be fair every panic on that list except for should never happen if the export was performed correctly Number however is pretty easy to get wrong and it can happen even if the user exported the previous database correctly Its not obvious that you cant import a database into one that has any overlap at all with the keys youre importing At the very least this should probably be mentioned in the documentation Right now the docs just state that import can panic if there are IO problems Im also not sure Id really consider any of this IO so that could be misleading Preferably the import method would return a Result that informs the user why it failed Ideally all of these panics would be errors but I think number is the most important Id be happy to help write the PR if theres interest in making this change Same thing with documentation It would be nice to be able to impl Transactional for wrapper structs around multiple Trees I am not proposing exposing the contents of TransactionalTrees or any of its methods Sample use case rust pub struct Table objects sledTree metadata sledTree pub struct TransactionalTable objects sledTransactionalTree metadata sledTransactionalTree impl sledTransactional for Table type View TransactionalTable fn makeoverlay self sledTransactionalTrees sledTransactionalmakeoverlay selfobjects selfmetadata fn viewoverlayoverlay sledTransactionalTrees SelfView let objects metadata sledTree sledTree as sledTransactionalviewoverlayoverlay TransactionalTable objects metadata Bug reports must include sled version rustc version operating system all logs NA The macro implementation of Transactional on tuples of trees is not generic over the error The implementation for Tree is This means that I cant return an error with the abort helper function when running a transaction over a tuple of Trees like I can for a transaction on an individual Tree This compiles rust use sledTransactionError TransactionResult Config abort deriveDebug PartialEq struct MyBullshitError fn main TransactionResult MyBullshitError let config Confignewtemporarytrue let db configopenunwrapopentreetree unwrap Use writeonly transactions as a writebatch let res dbtransactiondb dbinsertbk bcats dbinsertbk bdogs aborting will cause all writes to rollback if true abortMyBullshitError Ok unwraperr asserteqres TransactionErrorAbortMyBullshitError asserteqdbgetbk None asserteqdbgetbk None Ok This does not compile rust use sledTransactional TransactionError TransactionResult Config abort deriveDebug PartialEq struct MyBullshitError fn main TransactionResult MyBullshitError let config Confignewtemporarytrue let db configopenunwrap let tree dbopentreetree unwrap let tree dbopentreetree unwrap Use writeonly transactions as a writebatch let res tree tree transactiontree tree tree insertbk bcats tree insertbk bdogs aborting will cause all writes to rollback if true abortMyBullshitError Ok unwraperr asserteqres TransactionErrorAbortMyBullshitError asserteqtree getbk None asserteqtree getbk None Ok Heres the output of rustc for that particular piece of code error E couldnt convert the error to sledConflictableTransactionError srctreers abortMyBullshitError the trait stdconvertFromsledConflictableTransactionErrorMyBullshitError is not implemented for sledConflictableTransactionError I tried fixing this by changing line to rust implE TransactionalE for repeattype Tree indices This would fix it but it breaks type inference for closures that do not abort which judging by is undesirable The example showing a transaction on tuple of trees gives a compiler errors that a type annotation is needed after this change is applied error E type annotations needed srctreers transactionunprocessed processed cannot infer type for E I cant say I fully understand why type inference works for TransactableE on a Tree but not on a tuple of Trees iouringsetup syscall returns ENOMEM when code tries to allocate too much iourings Specifically this test cargo test featurestestingiouring logchunkyiterator allocates threads and they start returning ENOMEM on iouringsetup starting from This is very beginning of the iouring setup and even fails before queues mmaps I will trace kernel functions to understand better why it has this limit but the fix anyway is to reduce the test parallelism for iouring setup Its clearly observed by strace f env cargo test featurestestingiouring logchunkyiterator Tracing notes sudo tracecmd record p functiongraph g x sysiouringsetup sudo tracecmd report cpu It is true that execution return ENOMEM at logchunkyiter funcgraphexit us good invocation logchunkyiter funcgraphexit us good invocation logchunkyiter funcgraphexit us bad invocation logchunkyiter funcgraphexit us logchunkyiter funcgraphentry x sysiouringsetup logchunkyiter funcgraphentry iouringsetup logchunkyiter funcgraphentry capable logchunkyiter funcgraphentry nscapablecommon logchunkyiter funcgraphentry securitycapable logchunkyiter funcgraphentry capcapable logchunkyiter funcgraphexit us logchunkyiter funcgraphexit us logchunkyiter funcgraphexit us logchunkyiter funcgraphexit us logchunkyiter funcgraphentry freeuid logchunkyiter funcgraphexit us logchunkyiter funcgraphexit us logchunkyiter funcgraphexit us from kernel C accountmem capableCAPIPCLOCK if accountmem ret ioaccountmemuser ringpagespsqentries pcqentries if ret freeuiduser return ret ctx ioringctxallocp if ctx if accountmem iounaccountmemuser ringpagespsqentries pcqentries freeuiduser return ENOMEM So basically it fails due to the Dont allow more pages than we can safely lock pagelimit rlimitRLIMITMEMLOCK PAGESHIFT Limit of allocated pages per process putting this in one issue because these issues are two sides of the same coin and it helps me keep notes in one place concurrent transactions occurring on nonoverlapping sets of trees can cause nonatomic recovery of transactions we need to make transaction batching fully atomic by ditching the plug approach and just support multiitem logging this also will reduce contention on the log add relative file offset to blob pointers add batch log message that includes the expected batch length add reservebatch method to Log recovery can be parallelized before the last batch header after can also be done but is more complex and not likely to be as useful switch recovery over to new batch style implement cicadalike page concurrency control add page view local write cache and read ts maps calls to SAnext can often avoid coordination as long as we dont need to bump the file tip in which case we might need to wait for some file truncation futures to resolve before extending the file again these operations are fully async and would benefit from cache locality when batching anyway we currently try to claim the SA mutex on many write operations and this flatcombining approach similar to what now is called in Lruaccessed may significantly reduce contention on this mutex for all write operations in order to reduce specialcasing in the PageCache Meta and Counter can be folded into a special Tree