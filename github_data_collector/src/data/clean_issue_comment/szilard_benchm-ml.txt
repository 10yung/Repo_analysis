It seems to be a little bit confused that the evaluation on classification tasks uses the probabilities output directly in calculating the AUC For example in xgboostRL Will it be better to do that with phat Did you consider using more datasets And how about regression problems There is for example this benchmarking suite accessible via the OpenML packages As weve discussed in Slack H O has recently released some very interesting AutoML functionality In this case the leader is the StackedEnsemble generated from a GBM grid a DL grid a DRF and an XRT model On k records it trained for a while on some small cloud hardware and generated a respectable AUC of md An object of class H OAutoML Slot projectname default Slot leader Model Details H OBinomialModel stackedensemble Model ID StackedEnsemblemodel NULL H OBinomialMetrics stackedensemble Reported on training data MSE RMSE LogLoss Mean PerClass Error AUC Gini Confusion Matrix vertical actual across predicted for F optimal threshold N Y Error Rate N Y Totals Maximum Metrics Maximum metrics at their respective thresholds metric threshold value idx max f max f max f point max accuracy max precision max recall max specificity max absolutemcc max minperclassaccuracy max meanperclassaccuracy GainsLift Table Extract with h ogainsLiftmodel data or h ogainsLiftmodel validTF xvalTF H OBinomialMetrics stackedensemble Reported on validation data MSE RMSE LogLoss Mean PerClass Error AUC Gini Confusion Matrix vertical actual across predicted for F optimal threshold N Y Error Rate N Y Totals Maximum Metrics Maximum metrics at their respective thresholds metric threshold value idx max f max f max f point max accuracy max precision max recall max specificity max absolutemcc max minperclassaccuracy max meanperclassaccuracy GainsLift Table Extract with h ogainsLiftmodel data or h ogainsLiftmodel validTF xvalTF Slot leaderboard modelid auc logloss StackedEnsemblemodel GBMgrida cd a f af model GBMgrida cd a f af model GBMgrida cd a f af model DRFmodel XRTmodel DLgrida cd a f af model DLgrida cd a f af model GBMgrida cd a f af model GBMgrida cd a f af model DLgrida cd a f af model GLMgrida cd a f af model GLMgrida cd a f af model GBMgrida cd a f af model GBMgrida cd a f af model GBMgrida cd a f af model DLgrida cd a f af model GBMgrida cd a f af model DLgrida cd a f af model Great initiative thanks for making this public You might be interested in extending your benchmarking to the autosklearn I have created a script that can take in a sparse dataset in the pandas HDFS dataframe h format and run a binary classification on it on multiprocessing cluster with autosklearn Myself I will try to duplicate your benchmark but just in case you are on it you might want to try out yourself Hey Szilard Id like to replicate your code from beginning to end perhaps on Google Compute Engine GCE mainly to test out GCE with Vagrant Do you know have a sense of how long the entire process would take assuming a similar server size as what you used on EC Is there a convenient way to run all your scripts in from folder to That is is there a master script that executes them all I notice that the results are written out to the console Do you have a script that scrapes all the AUCs for your comparison analysis Thanks Thanks for great work We have an open source machine learning library called SMILE We have incorporated your benchmark We found that our system is much faster for this data set For K training data on a core machine we can train a random forest with trees in seconds and gradient boost trees of trees in seconds Projected to cores I think that we will be much faster than all the tools you tested You can try it out by cloning our project Then sbt benchmarkrun This also includes benchmark on USPS data which you may ignore Thanks Motivation I cant run mxnet on the M records airline set because modelmatrix crashes out of RAM on g xlarge with GB or RAM largest available for GPU instances Using Matrixsparsemodelmatrix to encode the categorical data would be great uses GB RAM but I get Error in asMethodobject Cholmod error problem too large at file Corecholmoddensec line Strangely on the M dataset I get another error Error iocc Seems X y was passed in a Row major way MXNetR adopts a column major convention I know from glouppe that RFs in sklearn now support sparse matrices too It would be interesting to see the results with sparse for RF and for logistic regression too We should see lower memory footprint and perhaps faster runs Anyone wants to help w the code PR This is to collaborate on some issues with Spark RF also addressed by jkbradley in comments to this post see comments by Joseph Bradley cc mengxr Please see Absolute Minimal Benchmark for random forests and lets use the M row training set and the test set linked in from there jkbradley says Onehot encoder Spark includes this plus a lot more feature transformers Preprocessing should become evereasier especially using DataFrames Spark Yes indeed Can you please provide code that reads in the original dataset pre hot encoding and does the hot encoding in Spark Also if random forest API can use data frames I guess we should use that for the training Can you please provide code for that too jkbradley says AUCaccuracy The AUC issue appears to be caused by MLlib tree ensembles aggregating votes rather than class probabilities as you suggested I reran your test using class probabilities which can be aggregated by hand and then got the same AUC as other libraries We re planning on including this fix in Spark and thanks for providing some evidence of its importance Fantastic Can you please share code that does that already I would be happy to check it out Factorization machines FM are a generic approach that allows to mimic most factorization models by feature engineering LIBFFM is an open source tool for fieldaware factorization machines FFM For the formulation of FFM please see these slides It has been used to win two recent clickthrough rate prediction competitions Criteos and Avazus They are also very interesting tools 