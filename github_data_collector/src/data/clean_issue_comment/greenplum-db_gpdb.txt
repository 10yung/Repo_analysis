Hi I was research the gpdb code recently and I find the duplicate statement below We need to allow SIGINT etc during the initial transaction PGSETMASK UnBlockSig We need to allow SIGINT etc during the initial transaction Also currently if this is the Master with standby support we need to allow SIGUSR for performing sync replication used by latch PGSETMASK UnBlockSig I was wondering if it is a duplicate statement or feature not implemented I think it is a little ugly maybe you want to code like this Pseudo code ifGprole GPROLEDISPATCH master has standby sigdelset BlockSig SIGUSR PGSETMASK UnBlockSig Please ignore me if its just a typo pc prepared commitabort retry failure is an annoying issue since it causes PANIC in production Weve seen some users complaints about reducing such cases when the cluster is in healthy state or during recovery eg via gprecoverseg There are many reasons that pc retry could not complete eg primary is recovering for this create gang could tolerate and retry or could not create connections to segments eg too many connections etc Ideally pc retry dispatchexecute code should be run as firstclass citizen but it seems to be hard to inspect all cases so this patch simply adds a guc dtxphase retrysecond to set retry time limit we have had a guc dtxphase retrycount default thats the limit of pc retry times also we do sleep ms in each try but this is not enough sometime eg how if dispatch fails soon even fts shows the segments are all ok so we could enforce the retry time limit also With this we stop retry when both control is exhausted ion Statement Insert on conflict do update will invoke update on segments If the on conflict update modifies the distkeys of the table this would lead to wrong data distribution This commit avoid this issue by raising error when transformInsertStmt if it finds that the on conflict update will touch the distribution keys of the table Fixes github issue Here are some reminders before you submit the pull request Add tests for the change Document changes Communicate in the mailing list if needed Pass make installcheck Review a PR in return to support the community After enabling the global deadlock detector we can support concurrent updates When updating one tuple at the same time the conflict and wait are moved from QD to QE We need to make sure the implicated transaction order on the segments is also considered on the master when taking the distributed snapshots This is reported Here are some reminders before you submit the pull request x Add tests for the change Document changes Communicate in the mailing list if needed Pass make installcheck Review a PR in return to support the community This patch performs a little cosmetic refactoring on how groupids are computed Also in test case olapplans I believe we meant to execute analyze olaptestsingle as in this patch Here are some reminders before you submit the pull request Add tests for the change Document changes Communicate in the mailing list if needed Pass make installcheck Review a PR in return to support the community Greenplum version or build master branch since this is introduced by upsert from pg Step to reproduce the behavior sql create table ta int b int distributed by a create unique index mud on ta b insert into t select i i from generateseries i Then insert into t values x x on conflict a b do update set b yyy this kind of statement hold RowExclusiveLock on QDmeans on QD they can run concurrently But they may lock tuples in segment when conflicts happen it invokes ExecUpdate even gdd is turned off Locking tuples using xid in segments without gdd is dangerous it will lead to global deadlock The following code is using isolation framework syntax sql begin insert into t values on conflict a b do update set b begin insert into t values on conflict a b do update set b insert into t values on conflict a b do update set b insert into t values on conflict a b do update set b global deadlock happens but no gdd is there Proposal to fix Hold high lock if insert statement contains conflict of and do update The tmid should be of the same value among the cluster It increases only on gpdb cluster full restart Tmid gpsessionid and gpcommandcount are put together to uniquely identify a single query execution monitoring agents such as gpperfmon relies on this uniqueness In c d eee adeb f bfe d a introduced a different way for gpmongettmid it brings in a problem that on master and segments the tmid may be different This commit fixes the problem Reviewedby Ning Yu nyupivotalio For prepared statements of updatedelete the lock mode is determined by the function CondUpgradeRelLock it forgot taking the GUC gpenableglobaldeadlockdetector into consideration So before this commit QD will first hold RowExclusiveLock and then hold ExclusiveLock on the table if gpenableglobaldeadlockdetector is set off which might lead to local deadlock in QD This commit fixes github issue This pr makes code in x more similar to master It fixes the issue Here are some reminders before you submit the pull request Add tests for the change Document changes Communicate in the mailing list if needed Pass make installcheck Review a PR in return to support the community Greenplum version or build gpdb master has fixed the issue Step to reproduce the behavior sql create table tc int c int c int prepare haha as update t set c begin execute haha use gdb to step into this statement The last statement will first hold rowexclusivelock and then exclusivelock which leads to local deadlock in QD This issue is fixed in master by a series of commits on lockmode The fix is easy for but we need to discuss whether to backport all related commits from master Greenplum version or build Master branch Since upsert is introduced by Postgres Step to reproduce the behavior sql create table ta int b int c int partition by list b partition t values partition t values psqlbsql NOTICE Table doesnt have DISTRIBUTED BY clause Using column named a as the Greenplum Database data distribution key for this table HINT The DISTRIBUTED BY clause determines the distribution of data Make sure columns chosen are the optimal data distribution key to minimize skew psqlbsql NOTICE CREATE TABLE will create partition t prtt for table t psqlbsql NOTICE CREATE TABLE will create partition t prtt for table t CREATE TABLE create unique index mud on ta b CREATE INDEX insert into t values INSERT insert into t values on conflict a b do update set b psqlbsql ERROR unexpected failure to find arbiter index execIndexingc seg pid execIndexingc The errmsg unexpected failure to find arbiter index is quite confusing We should throw an error when we detect it is going to modify a partition key 