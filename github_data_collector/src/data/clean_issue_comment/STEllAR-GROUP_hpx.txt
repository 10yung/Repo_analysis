Fixes again since the previous fixes were not complete Fixes fix a typo Fixes Proposed Changes Any background context you want to provide As documented in and I would like to use apex with user MPI driven jobs I have modified the init of APEX to use MPI with an experimental not cleaned up version looking like ifdef HPXHAVEAPEX if definedHPXHAVENETWORKING definedHPXHAVELIBMPI int rank size isinitialized Check if MPIInit has been called previously MPIInitialized isinitialized ifisinitialized MPICommrankMPICOMMWORLD rank MPICommsizeMPICOMMWORLD size else int required provided required MPITHREADMULTIPLE MPIInitthread nullptr required provided if provided MPITHREADFUNNELED stdcout Your MPI installation doesnt allow multiple threads Exiting n stdterminate MPICommrankMPICOMMWORLD rank MPICommsizeMPICOMMWORLD size stdcout Initializing mpi on rank rank of size stdendl stdcout Initializing instrumentation with rank rank of size stdendl utilexternaltimerinitnullptr rank size else utilexternaltimerinitnullptr hpxgetlocalityid hpxgetinitialnumlocalities endif I have enabled debugging in OTF and I see output from my test Initializing mpi on rank of Initializing instrumentation with rank of Initializing mpi on rank of Initializing instrumentation with rank of Rank of Rank of OTF srcOTF Archivec Entering function OTF ArchiveOpen OTF archiveAPEX OTF Active debug modules ARCHIVE READER ANCHORFILE COLLECTIVES POSIX SION SIONRANKMAP SIONCOLLECTIVES LOCKS OTF Active debug modules ARCHIVE READER ANCHORFILE COLLECTIVES POSIX SION SIONRANKMAP SIONCOLLECTIVES LOCKS OTF srcOTF Archivec Entering function OTF ArchiveOpen OTF archiveAPEX OTF srcotf collectivesc Entering function otf collectivesgetrank OTF srcotf collectivesc Entering function otf collectivesgetrank OTF srcotf collectivesc Entering function otf collectivesbcast nil x ffecb c INT Rank Fudged myOTF Bcast OTF srcotf collectivesc Entering function otf collectivesbcast nil x ffcbbf f INT Rank Fudged myOTF Bcast Elapsed time seconds Cores detected Worker Threads observed Available CPU time seconds Counter samples minimum mean maximum stddev Minute Load average Timer calls mean total total unknown Dataflow Task hpxcomponentsserverruntimesupportshutdow hpxcomponentsserverruntimesupportloadco hpxcomponentsserverruntimesupportcallsh hpxagasserverprimarynamespacecolocateac hpxactionsactionvoid hpxcomponentsse asynclaunchpolicydispatch backtrace hpxparallelexecutionparallelexecutorpost runhelper APEX Idle Total timers Closing OTF event files Writing OTF definition files OTF srcotf fileposixc File to open OTF archiveAPEX def Writing OTF Global definition file OTF srcotf collectivesc Entering function otf collectivesgetrank Writing OTF Node information Writing OTF Communicators OTF srcotf fileposixc File to open OTF archiveAPEX def OTF srcotf fileposixc File to open OTF archiveAPEX def OTF srcotf fileposixc File to open OTF archiveAPEX def OTF srcotf fileposixc File to open OTF archiveAPEX def OTF srcotf fileposixc File to open OTF archiveAPEX def OTF srcotf fileposixc File to open OTF archiveAPEX def OTF srcotf fileposixc File to open OTF archiveAPEX def OTF srcotf fileposixc File to open OTF archiveAPEX def OTF srcotf fileposixc File to open OTF archiveAPEX def OTF srcotf collectivesc Entering function otf collectivesgetrank OTF srcotf fileposixc File to open OTF archiveAPEX evt Closing the archive OTF srcotf collectivesc Entering function otf collectivesgetrank OTF srcotf fileposixc File to open OTF archiveAPEXotf OTF srcotf fileposixc File to open OTF archiveAPEX evt OTF srcotf fileposixc File to open OTF archiveAPEX evt OTF srcotf fileposixc File to open OTF archiveAPEX evt OTF srcotf fileposixc File to open OTF archiveAPEX evt OTF srcotf fileposixc File to open OTF archiveAPEX evt OTF srcotf fileposixc File to open OTF archiveAPEX evt OTF srcotf fileposixc File to open OTF archiveAPEX evt OTF srcotf fileposixc File to open OTF archiveAPEX evt OTF srcotf fileposixc File to open OTF archiveAPEX evt OTF srcotf fileposixc File to open OTF archiveAPEXdef done but when loaded in vampir I do not see Dataflow Task for example there may be other tasks missing but I didnt look for them The trace shows two ranks each with threads but not all tasks are present or labelled incorrectly When I run the same job with mpi ranks then I see the correct task names What other functions must I implement to get this to work This is a screenshot of the output from a mpi rank node run rm rf OTF archive appsmpich binmpiexec n binschedulerprioritychecktest Initializing mpi on rank of Initializing instrumentation with rank of Rank of OTF srcOTF Archivec Entering function OTF ArchiveOpen OTF archiveAPEX OTF Active debug modules ARCHIVE READER ANCHORFILE COLLECTIVES POSIX SION SIONRANKMAP SIONCOLLECTIVES LOCKS OTF srcotf collectivesc Entering function otf collectivesgetrank OTF srcotf collectivesc Entering function otf collectivesbcast nil x ffc bb INT Rank Fudged myOTF Bcast Elapsed time seconds Cores detected Worker Threads observed Available CPU time seconds Timer calls mean total total unknown Dataflow Task hpxcomponentsserverruntimesupportshutdow hpxcomponentsserverruntimesupportloadco hpxcomponentsserverruntimesupportcallsh hpxagasserverprimarynamespacecolocateac hpxactionsactionvoid hpxcomponentsse asynclaunchpolicydispatch backtrace hpxparallelexecutionparallelexecutorpost runhelper APEX Idle Total timers Closing OTF event files Writing OTF definition files Writing OTF Global definition file OTF srcotf collectivesc Entering function otf collectivesgetrank Writing OTF Node information Writing OTF Communicators OTF srcotf fileposixc File to open OTF archiveAPEX def OTF srcotf fileposixc File to open OTF archiveAPEX def OTF srcotf fileposixc File to open OTF archiveAPEX def OTF srcotf fileposixc File to open OTF archiveAPEX def OTF srcotf fileposixc File to open OTF archiveAPEX def Closing the archive OTF srcotf collectivesc Entering function otf collectivesgetrank OTF srcotf fileposixc File to open OTF archiveAPEXotf OTF srcotf fileposixc File to open OTF archiveAPEX evt OTF srcotf fileposixc File to open OTF archiveAPEX evt OTF srcotf fileposixc File to open OTF archiveAPEX evt OTF srcotf fileposixc File to open OTF archiveAPEX evt OTF srcotf fileposixc File to open OTF archiveAPEX evt OTF srcotf fileposixc File to open OTF archiveAPEXdef done and this is the vampir view of the two rank run there is not DataFlow task among others missing Version A user on IRC posted a request to bind tasks to threads but when using the default executor the schedulehint was not being taken from the executor instead the default was being used This PR introduces hpxfunctionaltaginvoke which is an implementation of This is needed to implement Expected Behavior I expected the dstencil example to scale well over threads in a single node When adding workers to a constant size problem I expect the run time to decrease Ideally proportionally Actual Behavior It does not seem to scale well strong scaling Output on a cluster node LocalitiesOSThreadsExecutionTimesecPointsperPartitionPartitionsTimeSteps After a drop in the execution time it even increases In an experiment I ran the parallel efficiency drops to well below Steps to Reproduce the Problem Build release build of HPX with examples Download the attached shell script Edit it for the location of the HPX examples and possibly the number of workers Run the shell script It will print execution times for different numbers of threads Specifications HPX Version upcoming commit d c de Platform compiler OS gcc CentOS node with two AMD EPYC Core Processors and GB memory output of hpxdumpconfig output of hpxinfo Notes and questions I spent quite some time trying to get my own programs to scale well and so far I have not been successful Therefore I turned to the HPX examples to see whether they scale well Is stencil supposed to scale well If so what could be possible reasons when it doesnt yet Fixes Cleans up thread executors Some are rewritten some deprecated and some are simply typedefed to other executors The changes are not backwards compatible ie some APIs that existed using the old interface have been completely removed I think this should be okay but lets gather some feedback until the next release if some of them should be reintroduced Summary Moved all executors to hpxparallelexecutors and left only forwarding headers and typedefs for the hpxthreadsexecutors namespace in hpxruntimethreadsexecutors serviceexecutor rewritten using the the newer executor interface The old serviceexecutor claimed to block until all outstanding work was done but this wasnt the case Ive made it nonblocking now since if someone relied on it they were very lucky or else noone actually relied on that functionality Id add it back only if someone requests the functionality currentexecutor is typedefed to threadpoolexecutor after some constructors were added to it The nullary constructor just grabs the current thread pool and submits tasks to that poolexecutor inherits threadpoolexecutor defaultexecutor is typedefed to parallelexecutor embeddedthreadpoolexecutor are deprecated behind a CMake option thisthreadexecutor deprecated threadpoolattachedexecutor deprecated with a replacement based on threadpoolexecutor computehostblockexecutor is now based on this replacement threadpoolosexecutors deprecated Expected Behavior Given a system with a single clean OpenMPI installation an mpi hello world program finds a given Commsize and Commrank for example and for mpirun np We now compile HPX with MPIParcelportON with this same OpenMPI installation Then an HPX application run with mpirun np should find those same ranks and sizes as hpxgetlocalityid and hpxfindalllocalitiessize Actual Behavior All hpxfindalllocalities method calls return vectors with size no communication between localities is possile Steps to Reproduce the Problem A docker container containing a minimal HPX hello world setup is available at dockerkilianwernerhpxminimal The used docker folder including all source and configuration is available at Running on any host with singularity and openmpi version the following commands sudo singularity build minsif dockerkilianwernerhpxminimal mpirun np singularity exec minsif optmpitest result in output Hello I am rank Hello I am rank However mpirun np singularity exec minsif homemyUserMinimalExamplebuildhpxmin results in output Localities Threads Localities Threads Specifications HPX Version Platform compiler OS ubuntudisco OpenMPI Version Comments Our work group often stumbles into the problem that hpxfindalllocalitiessize regardless of the mpi backend situation Most of the times this is solved by changing to older versions of hpx openmpi use other mpi implementations like intel mpi and just generally swap things around until hpx localities find each other As we are now trying to get a robust common solution working in a docker container we face the problem again and need hpx to behave consistent with raw mpi applications in a robust manner