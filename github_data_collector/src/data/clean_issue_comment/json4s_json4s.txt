 json s version scala version jdk version Hi I have a question about default values in case classes Im having trouble setting a default value for a Map value when the input contains an explicit nullvalue Heres a test to demonstrate import orgjson sFormats import orgjson sNoTypeHints import orgjson snativeSerialization import orgscalatestFlatSpec case class IntClassv Int case class MapClassv Map String String Mapempty class Json sTest extends FlatSpec implicit val formats Formats SerializationformatsNoTypeHints This passes IntClass should deserialize with default when value is missing in assertSerializationread IntClass v This passes it should deserialize with default when value is null in assertSerializationread IntClass v nullv This passes MapClass should deserialize with default when value is missing in assertSerializationread MapClass v Mapempty This Fails it should deserialize with default when value is null in assertSerializationread MapClass v nullv Mapempty Is there any way I can use a default value for the Map in this case json s scala version jdk version In x I could do val extractedOptionalField myJsonObject myOptionalFieldextractOpt String Which worked and produced None when myOptionalField is not present in the object For some reason this does not work in x when the field is missing throws orgjson spackageMappingException Did not find value which can be converted into javalangString As a workaround I found that i can use extract Option String and it works as expected But I do not understand what is the point of extractOpt if it throws an exception in case the field is missing What is the reason the behavior changed For me it seems broken json s version orgjson s json snative M orgjson s json snative orgjson s json sext orgjson s json sext M scala version scalaVersion jdk version java version JavaTM SE Runtime Environment build b Hi I have a case class generated from an Avro schema When I pass a JSON string to be parsed that is missing a field in the case class the parser returns an object with default values for ALL fields The expected result is an object with values for fields where they exist in the JSON and defaults where they do not Below is a trivial example to demonstrate Given an Avro generated case class with fields first and last import scalaannotationswitch case class FullNamevar first String F var last String L extends orgapacheavrospecificSpecificRecordBase def this thisF L def getfield Int AnyRef field switch match case first asInstanceOf AnyRef case last asInstanceOf AnyRef case new orgapacheavroAvroRuntimeExceptionBad index def putfield Int value Any Unit field switch match case thisfirst valuetoString asInstanceOf String case thislast valuetoString asInstanceOf String case new orgapacheavroAvroRuntimeExceptionBad index def getSchema orgapacheavroSchema FullNameSCHEMA object FullName val SCHEMA new orgapacheavroSchemaParserparse type record name FullName namespace testvendorbasic fields name first type string default F name last type string default L when processing a response with only ONE FIELD val httpBody first bob val messageObject responseParserparseList FullName httpBody and def parseList T string Stringimplicit m Manifest List T List T val jsonAst JsonMethodsparsestring implicit val formats Formats DefaultFormats jsonAstextract List T the output is List first F last L I would expect to get ListfirstboblastL where the default is used only for the missing field I have tried a number of the various configs around format and have not found success It is written in the comment of orgjson sExecutable as follows This class is intended as a workaround until we are able to use Java s javalangreflectExecutable class json s depends on paranamer but it seems that the equivalent function has been introduced in Java so it can be removed json s version scala version jdk version json s version scala version jdk version OpenJDK I have a json that contains a field like this sometimestamp other fields And case class like case class SomeDatasometimestamp Date I tried to create a FieldSerializr implicit val formats DefaultFormats FieldSerializer SomeData case sometimestamp JInttimestamp Somesometimestamp new DatetimestamptoLong But it doesnt work as expected It says No usable value for sometimestamp Do not know how to convert JInt into class javautilDate json s version scala version jdk version I have problem when try i to parse json string into scala case class object when i run the code with json s version or previous it work fine but with version and upper i get exception Exception in thread main orgjson spackageMappingException No usable value for header No usable value for senumvalue No usable value for outer Cant find field scalaEnumerationvmap from class scalaEnumeration at orgjson sreflectpackagefailpackagescala the code example scala package iocheck import orgjson sjacksonSerializationread readJson import orgjson sDefaultFormats Formats object App protected implicit val formats Formats DefaultFormats case class headertypesenumvalue senumvaluetypeValue senumvaluetypeeenumval case class SomeMsgheader headertype x Int y Float def mainargs Array String Unit val jsonVal String header senumvalue eenumval x y val res readJson SomeMsg jsonVal printlnres object senumvaluetype extends Enumeration type senumvaluetype Value val eenumval eenumval eenumval Value object senumvaluetype extends Enumeration type senumvaluetype Value val eenumval eenumval eenumval Value json s version scala version jdk version spark version heyguysi run json s on spark is okbut spark has this error javalangNoSuchMethodError orgjson sJsonASTJBoolFalseLorgjson sJsonASTJBool at orgjson sjacksonJValueDeserializerdeserializeJValueDeserializerscala at orgjson sjacksonJValueDeserializerdeserializeJValueDeserializerscala at orgjson sjacksonJValueDeserializerdeserializeJValueDeserializerscala at orgjson sjacksonJValueDeserializerdeserializeJValueDeserializerscala at orgjson sjacksonJValueDeserializerdeserializeJValueDeserializerscala at orgjson sjacksonJValueDeserializerdeserializeJValueDeserializerscala at orgjson sjacksonJValueDeserializerdeserializeJValueDeserializerscala at orgjson sjacksonJValueDeserializerdeserializeJValueDeserializerscala at comfasterxmljacksondatabindObjectReaderbindAndCloseObjectReaderjava at comfasterxmljacksondatabindObjectReaderreadValueObjectReaderjava at orgjson sjacksonJsonMethodsclassparseJsonMethodsscala at orgjson sjacksonJsonMethodsparseJsonMethodsscala Getting NoSuchMethodError in json s with Spark SQL compiled against json s x because this method signature was changed in Adding it back as package private private json s would allow you to keep binary compatibility with x but at the same time not allowing users of x to use it Related to this is would be useful if the contract for binary compatibility between releases was a bit more explicit in the readme Thanks