I note that crust depends on mio directly and doesnt use futures which is a solid choice for a project that started many years ago when futures in its present form didnt exist and there were multiple competing async frameworks that were eventually abandoned Now that asyncawait is hitting stable in I was wondering if there were any plans to use it Benefits would include better integration with other runtimes leveraging executors with workstealing threadpools like tokio and easier to write state machines with asyncawait What do you think On stable using a trait object without a dyn is a warning That along with forbidding warnings in the example causes the example to currently fail A quick fix locally is to either remove that line or run cargo fix all This is an easy fix run cargo fix Hello it would be very useful to provide some minimal usage example to the users of the crate that does not make use of internal structs like PeerId and at least mark examples that are not examples of the API functionality Cheers The docs currently say Crust implements several NAT traversal techniques such as hole punching and use of IGD Since understanding the nat traversal techniques used is critical to understanding whether or not this package will work in a particular use case please document all techniques used ideally also summarizing the cases that are and arent covered by those techniques That kind of documentation is critical for understanding the expected behavior of this code pub fn printconnectednodes self service Service printlnNode count selfnodeslen for id node in selfnodes let ip servicegetpeeripaddrnodeunwrap let status if serviceisconnectednode Connected else Disconnected println id ip status node println On the examples Ive added the following line to obtain a peer IP let ip sesiongetpeeripaddrnodeunwrap This method gives the IP but I cant find how to obtain the port Is there any method that returns the peer port Thanks At any given time Crust will try and limit the max active connectionssessions to peers It will sever a connection if it needs to make another once there are active ones in the pool Upper layer is not informed about any of these details only internal to Crust Connects will happen either on calling send API or indirectly by accepting a connection from peer As usual we should eventually have only one active connection to a peer if we are deciding to maintaininitiate a connection to it Currently the state of the socket in sockcoll crate has associated data When a socket is destroyed because say the pool wanted a new connection established and an existing connection is severed the associated data would be lost by both ends when they realise the connection is dropped This logic should change a Tx side should keep all the full or partial remaining data around to be delivered later We will simplify this in c below b Rx should keep all the partial data yet to be reconstructed and given to upper layer around and wait for Tx side to send more stuff in future by connecting to it It should have a timer to not keep it around for always though ofc This is a bit complex so well try avoiding this via the next point c below c Since session resumption is going to be a bit complex for TCP we will simplify it to sever a connection for the purposes of dynamic connection preserving only full messages which are not yet written to the socket Note the ones written are not guaranteed rxd by the peer but we are accepting the message loss at this point Things could be made better by having Crust ACKs and duplicate detection but we are avoiding complications just now Now the Rx either buffers data until it can reconstruct it at Crust level or discard it if the connection was dropped before it could reconstruct a single Crust message unit d Again to keep things simple once we reach the limit for max connection and we want to send to a new peer we will simply displace the least recently used peer It might happen that this peer was in the process of sending us something in which case it might attempt an immediate reconnection and displace someone else from our active pool We will let this logic be like this for simplicity This is going to be a problem where more than the active connections limit number of peers had something to send us and we keep severing as we get newer incoming connection at the same time maybe wanting ourselves to send something and the ones disconnected immediately attempt to connect back disconnecting yet someone else who attempts to connect and so on slowing things down greatly Assuming this wont happen we will let the logic be simply to displace someone from the pool if theres a fresh send or an incoming connection e At this point there might be a few displaced connections which had something to send Each of these will wait for a global tick of secs before displacing someone from the active pool These pending ones should be in a FIFO queue Send will take a blob which will either help Crust identify which connected peer to send the message to OR will help it make such a connection and then send if not already connected This will likely be the PubConnectionInfo blob Send will not return success or error It will try connecting sending for a max of secs after which it will silently fail Apart from the connection blob itll accept the message itself and the priority as usual Dont expose a connect API anymore Dont expose a disconnect API anymore Multiple sends to the same peer while in process of establishing a connection should not try establishing more connections wait for the one connection being established and then try sending all accumulated data to the peer We say try because the connection could be severed before sending all the messages according to the dynamic connections rule see dynconn pool issue Rename to selfconnectioninfo as we only want PubConnectionInfo now from it Without HolePunching in Crust for Fleming PubConnectionInfo is identical to PrivConnectionInfo Hence remove the latter and refactor all the related tests examples and API accordingly We will no longer be using CrustUserCrustRole Remove from Crust