 Steps to reproduce When using a take on a stream with enableack False by using streamnoack the messages are still automatically acknowledged Actual behavior As take only returns a list of values not the events themselves which are necessary to acknowledge the behavior while unexpected might be unavoidable Expected behavior I would propose to add another stream method eg noacktake which does not acknowledge messages and actually returns events instead of values If the returned items would not be different I would suggest to just add a configuration parameter to the method but since the signature is different a new method while logically mostly identical might be adequate Maybe we could also add a warning when using take with enableack False ask What do you think Is this a bug or totally intended Would a PR in order to introduce such a noacktake be welcome Hello Checklist I have included information about relevant versions I have verified that the issue persists when using the master branch of Faust Steps to reproduce App worker apppy python import faust from faust import currentevent from modeutilsaiter import aiter app faustAppapp brokerkafkalocalhost storememory consumerautooffsetresetearliest streamwaitemptyFalse autodiscoverFalse class CommandSubscribefaustRecord topic str commandtopic apptopiccommandtopic valuetypeCommandSubscribe appagentcommandtopic concurrency async def commandconsumerstream async for record in stream event currentevent printfGot record topic apptopicrecordtopic valuetypebytes valueserializerraw topiciterator aitertopic apptopicsaddtopic newstream faustStreamtopiciterator appapp eventack await dataconsumernewstream printawaited async def dataconsumerstream printconsumer called counter async for record in stream event currentevent if counter printexiting break printfConsumed record counter eventack printexited if name main appmain Data producer worker dataproducerpy python import faust import asyncio app faustAppapp brokerkafkalocalhost storememory webport autodiscoverFalse class CommandSubscribefaustRecord topic str commandtopic apptopiccommandtopic valuetypeCommandSubscribe apptask async def task printsending command datatopicname datatopic datatopic apptopicdatatopicname await commandtopicsendvalueCommandSubscribetopicdatatopicname for idx in range printtask sending data idx idx await datatopicsendvaluefmessage idx await datatopicsendvaluefmessage idx await datatopicsendvaluefmessage idx await datatopicsendvaluefmessage idx await asynciosleep apptask async def task printsending command datatopicname datatopic datatopic apptopicdatatopicname await commandtopicsendvalueCommandSubscribetopicdatatopicname for idx in range printtask sending data idx idx await datatopic sendvaluefmessage idx await datatopic sendvaluefmessage idx await datatopic sendvaluefmessage idx await datatopic sendvaluefmessage idx await asynciosleep if name main appmain Expected behavior When creating stream dynamically it starts to consume in a reasonable time it starts to consume from the earliest offset consumerautooffsetresetearliest Actual behavior When creating the stream dynamically on runtime there are occurring two unexpected behaviours After the creation of a new stream it takes approx seconds until it starts to consume from the stream It ignores configuration consumerautooffsetresetearliest and starts to consume only from that time coming messages Question I wonder if this is the right way to create the topic dynamically or if there is a better approach Full traceback apppy pytb a S v id app transport URLkafkalocalhost store memory web log stderr info pid hostname localhostlocaldomain platform CPython Linux x drivers transport aiokafka web aiohttp datadir homemattPycharmProjectskxapp data appdir homemattPycharmProjectskxapp datav INFO Worker Starting INFO App Starting INFO Monitor Starting INFO Producer Starting INFO ProducerBuffer Starting INFO CacheBackend Starting INFO Web Starting INFO Server Starting INFO Consumer Starting INFO AIOKafkaConsumerThread Starting INFO LeaderAssignor Starting INFO Producer Creating topic app assignorleader INFO Producer Topic app assignorleader created INFO ReplyConsumer Starting INFO AgentManager Starting INFO Agent maincommandconsumer Starting INFO OneForOneSupervisor x f b ee dd Starting INFO Conductor Starting INFO TableManager Starting INFO Conductor Waiting for agents to start INFO Conductor Waiting for tables to be registered INFO Recovery Starting INFO Producer Creating topic app assignorleader INFO Updating subscribed topics to frozensetcommandtopic app assignorleader INFO Subscribed to topics commandtopic app assignorleader INFO Discovered coordinator for group app INFO Revoking previously assigned partitions set for group app WARNING Topic commandtopic is not available during autocreate initialization INFO Rejoining group app INFO Joined group app generation with memberid faust c be c b c b e bd cc f INFO Elected group leader performing partition assignments using faust WARNING Ignoring missing topic commandtopic INFO Successfully synced group app with generation ERROR Rejoining group Need to rejoin Topics not yet created commandtopic INFO Rejoining group app INFO Joined group app generation with memberid faust c be c b c b e bd cc f INFO Elected group leader performing partition assignments using faust INFO Successfully synced group app with generation INFO Setting newly assigned partitions TopicPartitiontopicapp assignorleader partition TopicPartitiontopiccommandtopic partition for group app INFO Recovery Resuming flow INFO Recovery Seek stream partitions to committed offsets INFO Fetcher Starting INFO Recovery Worker ready INFO Worker Ready WARNING Got CommandSubscribe topicdatatopic WARNING consumer called WARNING Got CommandSubscribe topicdatatopic WARNING consumer called INFO Producer Creating topic app assignorleader INFO Updating subscribed topics to frozensetdatatopic commandtopic datatopic app assignorleader INFO Subscribed to topics datatopic commandtopic datatopic app assignorleader INFO Revoking previously assigned partitions frozensetTopicPartitiontopicapp assignorleader partition TopicPartitiontopiccommandtopic partition for group app INFO Rejoining group app INFO Joined group app generation with memberid faust c be c b c b e bd cc f INFO Elected group leader performing partition assignments using faust INFO Successfully synced group app with generation INFO Setting newly assigned partitions TopicPartitiontopicapp assignorleader partition TopicPartitiontopicdatatopic partition TopicPartitiontopiccommandtopic partition TopicPartitiontopicdatatopic partition for group app INFO Recovery Resuming flow INFO Recovery Seek stream partitions to committed offsets INFO Recovery Worker ready WARNING Consumed bmessage WARNING Consumed bmessage WARNING Consumed bmessage WARNING Consumed bmessage WARNING Consumed bmessage WARNING Consumed bmessage WARNING Consumed bmessage WARNING Consumed bmessage WARNING Consumed bmessage WARNING Consumed bmessage Versions Python version Python Faust version Operating system NAMEFedora VERSION Twenty Eight IDfedora VERSIONID VERSIONCODENAME PLATFORMIDplatformf PRETTYNAMEFedora Twenty Eight Kafka version docker image confluentinccpenterprisekafka confluentinccpschemaregistry confluentinccpzookeeper Many thanks for any insides In my faust app I have a blocking background task in my case a dash app that I run in a separate process using multiprocessing python apptask async def startdashboard global dashprocess dashprocess multiprocessingProcesstargetdashapprunserver dashprocessstart How do I ensure that proper cleanup happens when my faust app exits for instance when I hit nobrkbdCtrlkbd kbdCkbdnobr on the console running my app I image application signals are what I am looking for however the user guide does not go into much detail there In the repo I found onbeforeshutdown which sounds like a correct signal name but seems not to be exposed on the faustApp class I imagined this to look something like python apponbeforeshutdownconnect async def cleanupargs kwargs dashprocessterminate Checklist x I have included information about relevant versions x I have verified that the issue persists when using the master branch of Faust Steps to reproduce As soon as I edit the provided window example to use rocksdb the behavior suddenly changes The valuefunction does always return an empty list The code is more or less the same as I just shortened the example for brevity pytb from datetime import datetime timedelta from time import time import random import faust class RawModelfaustRecord date datetime value float TOPIC rawevent TABLE tumblingtable KAFKA kafkalocalhost CLEANUPINTERVAL WINDOW WINDOWEXPIRES PARTITIONS app faustAppwindowedagg brokerKAFKA version topicpartitions storerocksdb appconftablecleanupinterval CLEANUPINTERVAL source apptopicTOPIC valuetypeRawModel def windowprocessorkey events printfwindowprocessor events lenevents tumblingtable appTable TABLE defaultlist partitionsPARTITIONS onwindowclosewindowprocessor tumblingWINDOW expirestimedeltasecondsWINDOWEXPIRES relativetofieldRawModeldate appagentsource async def printwindowedeventsstream async for event in stream valuelist tumblingtable events value printfprintwindowedevents before valuelist printevent valuelistappendevent tumblingtable events valuelist valuelist tumblingtable events value printfprintwindowedevents after valuelist apptimer async def produce await sourcesendvalueRawModelvaluerandomrandom dateinttime if name main appmain Expected behavior I would expect the output like when using the in memory store WARNING printwindowedevents before WARNING RawModel date value WARNING printwindowedevents after RawModel date value WARNING printwindowedevents before RawModel date value WARNING RawModel date value WARNING printwindowedevents after RawModel date value RawModel date value Actual behavior The value method does always return an empty list even when I just added an element Therefore the windowprocessor always gets only one event instead of all events in the window pytb WARNING printwindowedevents before WARNING RawModel date value WARNING printwindowedevents after WARNING printwindowedevents before WARNING RawModel date value WARNING printwindowedevents after Let me know if I miss something I also checked pythonrocksdb separately and it seems to work Versions Python version Faust version Operating system macOS Catalina Kafka version Confluent ccs Commita eb a d f a RocksDB version if applicable pythonrocksdb Comment Thanks a lot for any suggestions on this Checklist x I have included information about relevant versions x I have verified that the issue persists when using the master branch of Faust Steps to reproduce Create simple app with GlobalTable like so python app faustApptest table appGlobalTabletest def main appmain and run it using Faust from master branch Expected behavior App should start Actual behavior App bootstrap hangs on INFO Elected group leader performing partition assignments using faust During our investigation we found that the problem is in def globaltablestandbyassignments method in assignorpartitionassignorpy file numpartitions variable is None during app bootstrap Single GlobalTable works fine on Faust Full traceback pytb a S v id test transport URLkafka URLkafka URLkafka store memory web log stderr info pid hostname makz rd platform CPython Darwin x drivers transport aiokafka web aiohttp datadir cuttestdata appdir cuttestdatav INFO Worker Starting INFO App Starting INFO Monitor Starting INFO Producer Starting INFO ProducerBuffer Starting INFO CacheBackend Starting INFO Web Starting INFO Server Starting INFO Consumer Starting INFO AIOKafkaConsumerThread Starting INFO LeaderAssignor Starting INFO Producer Creating topic testassignorleader INFO ReplyConsumer Starting INFO AgentManager Starting INFO Conductor Starting INFO TableManager Starting INFO Conductor Waiting for agents to start INFO Conductor Waiting for tables to be registered INFO GlobalTable test Starting INFO Store test Starting INFO Producer Creating topic testtestchangelog INFO Recovery Starting INFO Producer Creating topic testtestchangelog INFO Producer Creating topic testassignorleader INFO Updating subscribed topics to frozensettesttestchangelog testassignorleader INFO Subscribed to topics testtestchangelog testassignorleader INFO Discovered coordinator for group test INFO Revoking previously assigned partitions set for group test INFO Rejoining group test INFO Joined group test generation with memberid faust f c c caf b c fe INFO Elected group leader performing partition assignments using faust Versions Python version Faust version master rev fb c bc b ed f ba e b cd e b Operating system macOS Kafka version RocksDB version if applicable Checklist X I have included information about relevant versions X I have verified that the issue persists when using the master branch of Faust Steps to reproduce This is my first time using faust and I wanted to create a small example application with multiple agents My expectation is that each agent would run concurrently and that I would see the print lines from each agent interspersed in some random ordering sh appagentjoblocationtopic async def joblocationsjoblocation async for joblocbatch in joblocationtake within for jobloc in joblocbatch printJOB LOC await asynciosleep printnot sleeping appagentjobtopic async def testjobsjobs async for jobbatch in jobstake within for job in jobbatch printjob However when I observe the output it appears that only one agent can run at any given time This is even true if I use an await asynciosleep which I thought might let other agents process events from the event loop I have a feeling Im just misunderstanding how agents are executed and managed via the agentsupervisor Would really appreciate it if I could get some insight into what I might be doingmisunderstanding Thank you Expected behavior sh not sleeping JOB LOC job not sleeping JOB LOC job job job job not sleeping JOB LOC Something of this variation Actual behavior sh not sleeping JOB LOC not sleeping JOB LOC not sleeping JOB LOC not sleeping JOB LOC not sleeping JOB LOC not sleeping JOB LOC Versions Python version docker slimbuster Faust version Kafka version Is there a standard way to listen to the changelog of a table Seems silly to have a secondary topic just to notify an agent of table changes Thanks We try to connect to a kerberized Kafka Cluster In Java we use a keytab to achieve that the JAAS File looks like this we use SASLSSL and GSSAPI KafkaClient comsunsecurityauthmoduleKrb LoginModule required useKeyTabtrue keyTabetcschemaregistryusernamekeytab debugtrue serviceNamekafka doNotPrompttrue principalusernameMYDOMAINCOM Steps to reproduce Tell us what you did to cause something to happen Expected behavior Is it possible to do that with faust too We try something like this import ssl sslcontext sslcreatedefaultcontext purposesslPurposeSERVERAUTH cafilecapem sslcontextloadcertchainclientcert keyfileclientkey app faustApp brokercredentialsfaustGSSAPICredentials kerberosservicenamefaust kerberosdomainnameexamplecom sslcontextsslcontext We get an Kerberos error that the server is not in the list We wonder how to supply the kerberos principal in this case Should this work Versions Python version Faust version Operating system CentOS Kafka version RocksDB version if applicable Adds the option to yield Events from Streamtake with a yieldevents parameter Checklist x I have included information about relevant versions x I have verified that the issue persists when using the master branch of Faust Steps to reproduce I use confluent kafka only use confluent Connectors streaming mysql user table data to kafka topic user then an agent consumer it save it in rocks db but sometime user data is stream in topic and consumer successed print info but the offset not update single node Kafka topicpartitions is run app in docker Faust Confluent ce Commitdeddb f cd Docker version ce build e fc a app faustApptransporter enablewebTrue webport streamwaitemptyFalse autodiscoverTrue origintransporter consumerautooffsetresetearliest usertopic apptopicuser valuetypeUser def modifysink mtime inttimetime appruntimetable APPRUNTIMEBASEDATAMODIFYTIME mtime appagentusertopic sink modifysink async def userstreamusers async for user in users usertable userid userdumps printfsave user id userid yield user current info WARNING Consumer Possible livelock COMMIT OFFSET NOT ADVANCING FOR TopicPartitiontopicuser partition kafkaconsumergroups bootstrapserver localhost group transporter describe GROUP TOPIC PARTITION CURRENTOFFSET LOGENDOFFSET LAG CONSUMERID transporteruser faust d f c a f f d dab ba f transporter transporteruserchangelog faust d f c a f f d dab ba f change db data will get new date and log print it and rocksdb store it WARNING save user id but the offset not update consumer groups info GROUP TOPIC PARTITION CURRENTOFFSET LOGENDOFFSET LAG CONSUMERID transporteruser faust d f c a f f d dab ba f transporter transporteruserchangelog faust d f c a f f d dab ba f when I restart the app it will consume user again then LAG and hang again Expected behavior not hang offset updated Actual behavior agents hangs consumer success but offset not update Versions Python Faust Operating system docker ce build e fc a Kafka version Confluent ce Commitdeddb f cd RocksDB version 