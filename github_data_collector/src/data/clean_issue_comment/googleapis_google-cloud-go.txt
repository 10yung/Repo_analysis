PROBLEM Spanners Nullable types eg NullString NullDate etc dont have support for Gos native JSON marshalingunmarshaling which results in displaying the guts of the nullable type instead of a reasonable output and makes it hard to accept JSON input without manual surgery and intermittent structs For example golang type Blog struct Post spannerNullString jsonpost blog Blog Post spannerNullString StringVal this is a post Valid true json jsonMarshalIndentblog fmtPrintlnstringjson prints json post StringVal this is a post Valid true PROPOSED SOLUTION I would like to ask for jsonMarshaller and jsonUnmarshaller interfaces to be implemented For example for NullString it could simply be golang NullString copied from spanner type NullString struct StringVal string Valid bool convention to represent null in JSON doc var jsonNull bytenull jsonMarshaller interface func n NullString MarshalJSON byte error if nValid return bytefmtSprintfq nStringVal nil return jsonNull nil jsonUnmarshaller interface func n NullString UnmarshalJSONpayload byte error if payload nil bytesEqualpayload jsonNull return nil nStringVal stringpayload nValid true return nil which then lets json to nicely marshalunmarshal nullable strings into the proper spannerNullString object Then this golang type Blog struct Post NullString jsonpost blog Blog Post NullString StringVal this is a post Valid true change this to false to test the null case marshalled jsonMarshal blog fmtPrintfMarshaled v n stringmarshalled var unmarshalled Blog jsonUnmarshalmarshalled unmarshalled fmtPrintfUnmarshalled v n unmarshalled would print Valid true yaml Marshaled postthis is a post Unmarshalled PostStringValthis is a post Validtrue and for when Valid false yaml Marshaled postnull Unmarshalled PostStringVal Validfalse Go Playground with the above code Client storage Expected behavior For gzip compressed files objNewRangeReaderctx offset length should return a partial reader for that range that starts at offset for the uncompressed file Actual behavior If length it returns a NewReader for the entire file with offset instead of the requested range though it does read it uncompressed as desired If length but offset it returns a partial request not satisfied error Client PubSub Describe Your Environment locally But same behaviour on GKE I have a fairly simple subscriber tried both synchronous and asynchronous that should constantly check for new messages and handle a bulk of messages This is the code you can run to reproduce go package main import context os time cloudgooglecomgopubsub log githubcomsirupsenlogrus func main ctx contextBackground projectID osGetenvGCLOUDPROJECTID subscriptionID osGetenvSUBSCRIPTIONID client err pubsubNewClientctx projectID if err nil panicerr defer clientClose sub clientSubscriptionsubscriptionID subReceiveSettingsSynchronous true subReceiveSettingsMaxOutstandingMessages subReceiveSettingsMaxOutstandingBytes subReceiveSettingsMaxExtension for Receive messages constantly logInfofcheck for new messages in subscription v subscriptionID tctx cancel contextWithTimeoutctx timeSecond defer cancel Create a channel to handle messages to as they come in cm makechan pubsubMessage messages finished false go func for select case msg cm logInfofreceived message v stringmsgData messages make acks conditional defer func if finished msgAck else msgNack case tctxDone if messages logInfohere we do something only if it suceeds we acknowledge the messages finished true logInfofnumber of messages v messages messages return Receive blocks until the context is cancelled or an error occurs err subReceivetctx funcctx contextContext msg pubsubMessage cm msg if err nil statusCodeerr codesCanceled logErrorfReceive v err closecm Expected Behavior this program constantly checks for new messages in the subscription If new messages arrive it picks up as many messages as possible within seconds process them and acknowledge them If a lot of messages arrive in the subscription the program should still be able to handle them by processing bulks of messages one by another Actual Behavior All works fine if there are only few messages in the subscription If I start putting load onto it say produce messages the program gets stuck after printing the log info number of messages X Client Pubsub Describe Your Environment MacOS Mojave Expected Behavior Querying for topics with emulator returns true if exists false otherwise Actual Behavior Function hangs and does not return until timeout es err pubsubNewClientctx dev if err nil logFatalfFailed to create client v err topic esTopichello ok err topicExistsctx if err nil iFatalffailed to check if topic exists v err if ok if err topicDeletectx err nil iFatalffailed to cleanup the topic q v hello err Client firestore Describe Your Environment Using firestore for web services deployed on Google App Engine We have used firestore as our backend to serve static data from our web services However while we batch documents from the firestore in some scenarios we do not require the entire documents we need only fields out of the entire documents But because there is no way you can pass projectionfields to be selected from documents during batch query we saw our API was taking more time to respond In the source code we saw func c Client getAllctx contextContext docRefs DocumentRef tid byte DocumentSnapshot error It creates req pbBatchGetDocumentsRequest Database cpath Documents docNames This BatchGetDocumentsRequest has Mask DocumentMask field which only returns the selected fields from the resultset or query the firestore only for the given data But there is no provision in the GetAll method to pass the fields We have created a wrapper in our code that takes an array of field paths fieldPaths string and create a DocumentMask with this array and pass it to the BatchGetDocumentsRequest instance After doing this we again did a performance test of our application and the result showed us a improvement in total response time I am raising this Issue so that If approved I will like to raise a PR where the GetAll method also takes fieldPaths array and pass it to BatchGetDocumentsRequest or have another method in the API that allows projection This will help others also who are having the same scenario as ours and using the same API to use Projection Expected Behavior Should allow the user to pass FieldsPath to GetAll Method and Only return the requested fields by the user and if fieldPaths is nil then return the entire document Actual Behavior Current API doesnt allow a user to do projects for GetAll method call Following up on Were using Spanner as the backing storage for a resource oriented API were using the Cloud API design guide This means were storing a standard set of createdat updatedat and deletedat timestamps for our resources using TIMESTAMP as the column type Support for TIMESTAMP in spannertest is the one thing blocking us from covering these APIs with spannertestbased unit tests offset support would enable unit testing of list pagination but thats a separate smaller issue Were currently strapped for time and are not able to take on implementation of this ourselves Creating this issue on the offchance you are able to upprioritize it Client StackDriver Describe Your Environment MacOS Expected Behavior monitoredresMonitoredResourceMetadataGetUserLabels returns labels that are defined by users on GCP objects like compute VMs or CloudSQL instances Actual Behavior Always returns empty map whenever object has or does not have userdefined labels To reproduce Create a VM and assign some labels to it like test Create a SA with read permissions on StackDriver API and put it to sajson file next to the maingo Replace YOURPROJECTISHERE and YOURVMISHERE hardcoded in the following snippet with GCP project where your VM is located and name of the VM you just created accordingly Put the code snippet to the maingo Then just go run maingo As output you will always see map instead of real userdefined labels that Id expect as output from fmtPrintlnseriesGetMetadataGetUserLabels in the end of the snippet Code snippet Im sorry for the code quality I did not have much time to prepare anything clean and beautiful go package main import bytes context fmt githubcomgolangprotobufptypestimestamp githubcomgoogleapisgaxgov googlegolangorgapiiterator githubcomprometheuscommonlog googlegolangorgapioption googlegolangorggrpccodes googlegolangorggrpcstatus io time monitoring cloudgooglecomgomonitoringapiv monitoringpb googlegolangorggenprotogoogleapismonitoringv const timeRange timeDuration timeMinute func main var buf bytesBuffer if err printUserLabels buf YOURPROJECTISHERE computegoogleapiscominstancecpuutilization err nil logErrorlnerr result bufString fmtPrintfresult func printUserLabelsw ioWriter projectID metricType string error ctx contextBackground c err monitoringNewMetricClientctx if err nil return fmtErrorfNewMetricClient v err req monitoringpbGetMetricDescriptorRequest Name fmtSprintfprojectssmetricDescriptorss projectID metricType resp err cGetMetricDescriptorctx req if err nil return fmtErrorfcould not get custom metric v err filter metrictype respType AND metriclabelinstancename YOURVMISHERE endTime timeNowUTC startTime endTimeAddtimeRange mClient err monitoringNewMetricClientctx optionWithCredentialsFilesajson if err nil logErrorlnerr it mClientListTimeSeriesctx monitoringpbListTimeSeriesRequest Name projects projectID Filter filter Interval monitoringpbTimeInterval StartTime timestampTimestampSeconds startTimeUnix EndTime timestampTimestampSeconds endTimeUnix View monitoringpbListTimeSeriesRequestFULL gaxWithGRPCOptions for series err itNext if err iteratorDone statusCodeerr codesNotFound statusCodeerr codesCanceled err contextCanceled break if err nil logErrorfError retrieve s s respType err break fmtPrintlnseriesGetMetadataGetUserLabels return nil pstest is great for testing pubsub makes using queues so much easier Could there be an option to simulate a Queue failure Maybe something along the lines of srv pstestNewServer srvNextQueueErrorerrorsNewoops It could either return an error only for the next call to Queue or for every call after this Im using the Spanner Go client in an application Im building Im running this application on GKE using the alpine docker image Every now and then I get an error message like this The transaction contains too many mutations Insert and update operations count with the multiplicity of the number of columns they affect For example inserting values into one key column and four nonkey columns count as five mutations total for the insert Delete and delete range operations count as one mutation regardless of the number of columns affected The total mutation count includes any changes to indexes that the transaction generates Please reduce the number of writes or use fewer indexes Maximum number Im trying to make sense of this error message and I cant find any documentation on it on Googles documentation pages Id like to know how to programmatically calculate the maximum number of objects eg rows that I can insert given this mutation limit For example lets say I have a table with columns and of the columns compose the primary key Additionally there are secondary indexes on the table As I understand from that error statement if R number of rowsobjects being inserted C number of columns in the table I number of secondary indexes on the table M max number of mutations then R floorM I C which means R floor So I should only be able to insert objects into the table in one transaction I know this formula isnt correct though because I regularly still get this error even using this formula Can someone help me understand what the formula is I dont understand how the row and column multiplicity comes into play with secondary indexes and primary keys and also interleaved tables Whats the formula Flakes for multiple spanner integration tests over the past day TestIntegrationDML TestBatchDMLTwoStatements TestIntegrationQueryExpressions multiple tests failed Please fix these andor disable flaky tests for the time being if theres not a quick fix