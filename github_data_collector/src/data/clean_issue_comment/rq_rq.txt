Ive tried djangorq but it cant run worker with custom rq queue name which is not present in Django settingspy With rq it is possible But found unhandled exception Env django rq Steps rqworker was launched in code as proc subprocessPopen executable rqworker struuid burst name uuid Queue configured in code as Queuenameuuid connectionselfredis isasyncTrue Error python Worker rqworker fa d a bd b bd f e started version Listening on fa d a bd b bd f e Cleaning registries for queue fa d a bd b bd f e fa d a bd b bd f e startruntest c b b b bf d d Worker rqworker fa d a bd b bd f e found an unhandled exception quitting Traceback most recent call last File homeuservenvlibpython sitepackagesrqjobpy line in unpickle obj loadspickledstring File homeusersoftwareloadermethodsmyloaderpy line in module from loadermodels import Model Model Model Model File homeusersoftwareloadermodelspy line in module class EnvironmentsmodelsModel File homeuservenvlibpython sitepackagesdjangodbmodelsbasepy line in new appconfig appsgetcontainingappconfigmodule File homeuservenvlibpython sitepackagesdjangoappsregistrypy line in getcontainingappconfig selfcheckappsready File homeuservenvlibpython sitepackagesdjangoappsregistrypy line in checkappsready raise AppRegistryNotReadyApps arent loaded yet djangocoreexceptionsAppRegistryNotReady Apps arent loaded yet During handling of the above exception another exception occurred Traceback most recent call last File homeuservenvlibpython sitepackagesrqworkerpy line in work selfexecutejobjob queue File homeuservenvlibpython sitepackagesrqworkerpy line in executejob selfforkworkhorsejob queue File homeuservenvlibpython sitepackagesrqworkerpy line in forkworkhorse selfmainworkhorsejob queue File homeuservenvlibpython sitepackagesrqworkerpy line in mainworkhorse raise e File homeuservenvlibpython sitepackagesrqworkerpy line in mainworkhorse selfperformjobjob queue File homeuservenvlibpython sitepackagesrqworkerpy line in performjob selfpreparejobexecutionjob heartbeatttl File homeuservenvlibpython sitepackagesrqworkerpy line in preparejobexecution selfproclinemsgformatjobfuncname joborigin timetime File homeuservenvlibpython sitepackagesrqjobpy line in funcname selfunpickledata File homeuservenvlibpython sitepackagesrqjobpy line in unpickledata selffuncname selfinstance selfargs selfkwargs unpickleselfdata File homeuservenvlibpython sitepackagesrqjobpy line in unpickle raise UnpickleErrorCould not unpickle pickledstring e rqexceptionsUnpickleError Could not unpickle AppRegistryNotReadyApps arent loaded yet Looks like when creating a scheduled job the job status is not set to scheduled Think this thing may cause new problems when deleting jobs from scheduled registry The current way the logger is configured in the scheduler impacts other loggers in our project This probably relates to too This PR uses RQs setuploghandlers instead of calling loggingbasicConfig directly There appears to be a change in logging behavior between and that Im having trouble overriding Previously I was able to get the workers to output JSON output by passing in a CustomWorker that used this logging configuration python from pythonjsonlogger import jsonlogger import logging import progressbar import sys class CustomJsonFormatterjsonloggerJsonFormatter def addfieldsself logrecord record messagedict superCustomJsonFormatter selfaddfieldslogrecord record messagedict logrecord timestamp logrecord asctime if logrecordgetlevel logrecord level logrecord level upper else logrecord level recordlevelname logger logginggetLogger logHandler loggingStreamHandlersysstdout loggersetLevelloggingINFO logHandlersetLevelloggingINFO FORMAT asctimes names levelnames messages pathnames excinfos funcNames linenod threadd processd processNames threadNames formatter CustomJsonFormatterFORMAT logHandlersetFormatterformatter loggeraddHandlerlogHandler However now each worker emits two logs one correctly formatted to stdout and another using the default Python formatter to stderr Whats the preferred way to override this and emit one log output again Hi Is there some way to queue a task from PHP directly Or over HTTP Or in another way In addition to queuing there is a need to wait until the workers completion and get the result of his work to PHP Is it possible Used Redis Broker Hi Im trying to unpickle the rq data present inside the Redis key but its failing with invalid load key Heres how it looks like excinfo x x c x x x x x x data x x c x c n x x E x xc x x x l xb x d xa xee x x xaf x dV x x bd x dL xb x l x x bt x e xb xf p xfa x b xcf x xff xf xf xd xfb xf xa KBk xf xc j x xe x a xa xfa xb x x xb xd xb xa xb xd xa x x xce x x x xe rb xa x xff xb xeb x x xd t x x f x Hp xc Q xd xe x c x x x x xa xf x e xe x f xb I xaa x c x xc Y xf xa xd xee xd xba x xe x b xde x x bX xcd x aZ x a x el I x e x E x xaaM Any help Im facing this issues while using rq with tornado app successor to This PR updates Queueenqueuedependents such that only those dependents for which all dependencies are complete will be enqueued It includes New method on Jobgetdependenciesstatuses to retrieve the status of all of a jobs dependencies A modification to Queueenqueuedependents as described above A modification to Workerhandlejobsuccess so that the status of ALL successful jobs is persisted to FINISHED in Redis A small fix to Jobsetstatus so that it uses a pipeline when appropriate Notwithstanding any bugs I believe the next step here is to modify Job such that it accepts more than one dependencyin dependson in case the forked process gets stuck and cannot stop itself As a workaround I changed the value HIGHESTPROTOCOL to in jobpy Line of jobspy dumps partialpickledumps protocol rqexceptionsUnpickleError Could not unpickle ValueErrorunsupported pickle protocol Can the protocol value be set somewhere as a client of rq