I cant start Windows node within same network because Rancher agent cant connect to servers external IP which it resolves It should resolve to local ip to connect One solution would be to use custom DNS server and set DNS starting docker Preferred way is to use docker run addhost flag but it doesnt work Error FATA is not accessible PowerShell NoLogo NonInteractive Command docker run addhost wwwmyservercom v cchost rancherrancheragentv bootstrap server token xxx worker iex None Please search for existing issues first then read to see what we expect in an issue For security issues please email securityranchercom instead of posting a public issue in GitHub You may but are not required to use the GPG key located on Keybase What kind of request is this questionbugenhancementfeature request Bug Steps to reproduce least amount of steps as possible Create a new custom cluster with control plane etcd and worker nodes Shut down one of the etcd nodes To accomplish this I stopped docker on the VM Attempt to add a new etcd node by running the docker run on a new host Result The UI reports Failed to reconcile etcd plane Failed to add etcd member etcdip to etcd cluster Rancher logs show INFO cluster cjpzp provisioning addetcd Adding member etcdip to etcd cluster ERROR ClusterController cjpzp clusterprovisionercontroller failed with Failed to reconcile etcd plane Failed to add etcd member etcdip to etcd cluster Every minutes it attempts to add the etcd member but fails with the same error Other details that may be helpful If the unhealthy etcd node is deleted in the UI the new etcd does get added successfully Environment information Rancher version rancherrancherrancherserver image tag or shown bottom left in the UI v Installation option single installHA single install If the reported issue is regarding a created cluster please provide requested info below Cluster information Cluster type HostedInfrastructure ProviderCustomImported custom Machine type cloudVMmetal and specifications CPUmemory t amedium on AWS Kubernetes version use kubectl version kubectl version Client Version versionInfoMajor Minor GitVersionv GitCommitd ddbd faf a faf ffc GitTreeStateclean BuildDate T Z GoVersiongo Compilergc Platformlinuxamd Server Version versionInfoMajor Minor GitVersionv GitCommit be bdce a dd c fd d b e fe ba GitTreeStateclean BuildDate T Z GoVersiongo Compilergc Platformlinuxamd Docker version use docker version docker version Client Docker Engine Community Version API version Go version go Git commit a ea Built Wed Nov OSArch linuxamd Experimental false Server Docker Engine Community Engine Version API version minimum version Go version go Git commit a ea Built Wed Nov OSArch linuxamd Experimental false containerd Version v GitCommit b a c af e c db c f fa runc Version rc dev GitCommit e f a c f e d a c b d aa dockerinit Version GitCommit fec Right now scale testing framework only measures cluster and project listing Add listing for all v resources to testing frame work Automate scaling tests using Jenkins What kind of request is this questionbugenhancementfeature request bug Steps to reproduce least amount of steps as possible Deploy a cluster Through the API run CIS Scan on the cluster with the config map value set as configjson skiprkecis Actual Result The scan does NOT skip the test Expected Result The Scan should skip the test Other details that may be helpful Environment information Rancher version rancherrancherrancherserver image tag or shown bottom left in the UI masterhead Installation option single installHA single Please search for existing issues first then read to see what we expect in an issue For security issues please email securityranchercom instead of posting a public issue in GitHub You may but are not required to use the GPG key located on Keybase What kind of request is this questionbugenhancementfeature request Bug Steps to reproduce least amount of steps as possible Create an ingress that includes TLS Result The host field is changed from the yaml supplied value to the ingressnamenamespaceipxipio The desired result is to maintain the value provided in the yaml file Other details that may be helpful This issue was visible when trying to install harbor using helm in a cluster on a private network nginx ingress no configuration for letsencrypt Logs of the session First cleanup previous install the change appears to be persistent bmitchrke kubectl delete n harbor f foohttpyaml ingressextensions fooingress deleted Try applying an ingress without a TLS config bmitchrke cat foohttpyaml apiVersion extensionsv beta kind Ingress metadata labels app foo name fooingress spec rules host fooharbor xipio http paths backend serviceName harborharborportal servicePort path bmitchrke kubectl apply n harbor f foohttpyaml ingressextensionsfooingress created bmitchrke kubectl get n harbor o yaml ingressextensionsfooingress apiVersion extensionsv beta kind Ingress metadata annotations kubectlkubernetesiolastappliedconfiguration apiVersionextensionsv beta kindIngressmetadataannotationslabelsappfoonamefooingressnamespaceharborspecrules hostfooharbor xipiohttppaths backendserviceNameharborharborp ortalservicePort path creationTimestamp T Z generation labels app foo name fooingress namespace harbor resourceVersion selfLink apisextensionsv beta namespacesharboringressesfooingress uid dfe afdbf d aedcb d spec rules host fooharbor xipio http paths backend serviceName harborharborportal servicePort path status loadBalancer Add the TLS config from harbor bmitchrke cat foohttpsyaml apiVersion extensionsv beta kind Ingress metadata labels app foo name fooingress spec rules host fooharbor xipio http paths backend serviceName harborharborportal servicePort path tls hosts coreharbor xipio secretName harborharboringress bmitchrke kubectl apply n harbor f foohttpsyaml ingressextensionsfooingress configured On first glance this appears ok the host is still fooharbor bmitchrke kubectl get n harbor o yaml ingressextensionsfooingress apiVersion extensionsv beta kind Ingress metadata annotations kubectlkubernetesiolastappliedconfiguration apiVersionextensionsv beta kindIngressmetadataannotationslabelsappfoonamefooingressnamespaceharborspecrules hostfooharbor xipiohttppaths backendserviceNameharborharborp ortalservicePort path tls hosts coreharbor xipio secretNameharborharboringress creationTimestamp T Z generation labels app foo name fooingress namespace harbor resourceVersion selfLink apisextensionsv beta namespacesharboringressesfooingress uid dfe afdbf d aedcb d spec rules host fooharbor xipio http paths backend serviceName harborharborportal servicePort path tls hosts coreharbor xipio secretName harborharboringress status loadBalancer But after a second the resource is updated to fooingressharbor bmitchrke kubectl get n harbor o yaml ingressextensionsfooingress apiVersion extensionsv beta kind Ingress metadata annotations fieldcattleiopublicEndpoints addresses port protocolHTTPserviceNameharborharborharborportalingressNameharborfooingresshostnamefooingressharbor xipiopathallNodestrue kubectlkubernetesiolastappliedconfiguration apiVersionextensionsv beta kindIngressmetadataannotationslabelsappfoonamefooingressnamespaceharborspecrules hostfooharbor xipiohttppaths backendserviceNameharborharborp ortalservicePort path tls hosts coreharbor xipio secretNameharborharboringress creationTimestamp T Z generation labels app foo name fooingress namespace harbor resourceVersion selfLink apisextensionsv beta namespacesharboringressesfooingress uid dfe afdbf d aedcb d spec rules host fooingressharbor xipio http paths backend serviceName harborharborportal servicePort path tls hosts coreharbor xipio secretName harborharboringress status loadBalancer ingress ip ip ip This is particularly bad when one ingress configuration is used for two different hostnames as the harbor helm install does At this point from the command line and from the GUI Im unable to configure the host in the ingress it keeps getting modified back to the generated value Environment information Rancher version rancherrancherrancherserver image tag or shown bottom left in the UI v Installation option single installHA single rancher instance managing a cluster with managers and workers this is a lab environment If the reported issue is regarding a created cluster please provide requested info below Cluster information Cluster type HostedInfrastructure ProviderCustomImported Custom Machine type cloudVMmetal and specifications CPUmemory VM vCPU G ram each Kubernetes version use kubectl version kubectl version Client Version versionInfoMajor Minor GitVersionv GitCommitd cd baca b e d ed d GitTreeStateclean BuildDate T Z GoVersiongo Compilergc Platformlinuxamd Server Version versionInfoMajor Minor GitVersionv GitCommitb cbbae ec a fc d e d e GitTreeStateclean BuildDate T Z GoVersiongo Compilergc Platformlinuxamd Docker version use docker version rootrke rancher docker version Client Docker Engine Community Version API version Go version go Git commit a ea Built Wed Nov OSArch linuxamd Experimental false Server Docker Engine Community Engine Version API version minimum version Go version go Git commit a ea Built Wed Nov OSArch linuxamd Experimental false containerd Version GitCommit b a c af e c db c f fa runc Version rc dev GitCommit e f a c f e d a c b d aa dockerinit Version GitCommit fec Changes proposed to validate KDM changes In addition to the current onTag job that gets triggered in jenkins we need another jenkins job to call a validation test that should do the following Deploy Rancher server with current RC build Create custom cluster with all K s versions available Currently no K s version is set which results in only the default K s version being deployed Deploy last released version can we rely on latest tag for this Change kdm to point to dev branch Create custom cluster with all K s versions available Record cluster details for each of the cluster that gets created on both the rancher server instances so that it can be used by the jenkins jobs to trigger automation runs 