I have been trying to install leelazero on my arch for the whole afternoon The result is frustrating I kept getting the complaint C exception with description No suitable OpenCL device found At first I thought its because Im missing some dependencies but it does not seem to be like Im using an old laptop manufactured in perhaps it lacks some device GPU too old Detailed complaints yay leelazerogit gives Linking CXX executable tests Built target tests Running main from gtestmaincc Running tests from test cases Global test environment setup BLAS Core builtin Eigen library Detecting residual layersv channels blocks Initializing OpenCL autodetecting precision Detected OpenCL platforms Platform version OpenCL Mesa Platform profile FULLPROFILE Platform name Clover Platform vendor Mesa Error getting devices clGetDeviceIDs unknown file Failure C exception with description No suitable OpenCL device found thrown in auxiliary test code environments or even listeners ERROR A failure occurred in check Aborting Error making leelazerogit clinfo gives Number of platforms Platform Name Clover Platform Vendor Mesa Platform Version OpenCL Mesa Platform Profile FULLPROFILE Platform Extensions clkhricd Platform Extensions function suffix MESA Platform Name Clover Number of devices NULL platform behavior clGetPlatformInfoNULL CLPLATFORMNAME Clover clGetDeviceIDsNULL CLDEVICETYPEALL clCreateContextNULL default No devices found in platform clCreateContextFromTypeNULL CLDEVICETYPEDEFAULT No devices found in platform clCreateContextFromTypeNULL CLDEVICETYPECPU No devices found in platform clCreateContextFromTypeNULL CLDEVICETYPEGPU No devices found in platform clCreateContextFromTypeNULL CLDEVICETYPEACCELERATOR No devices found in platform clCreateContextFromTypeNULL CLDEVICETYPECUSTOM No devices found in platform clCreateContextFromTypeNULL CLDEVICETYPEALL No devices found in platform ICD loader properties ICD loader Name OpenCL ICD Loader ICD loader Vendor OCL Icd free software ICD loader Version ICD loader Profile OpenCL Hello all Firstly I would like to thank developers for great work and open sourcing the project It is an invaluable educational tool Basically I was browsing Leela Zero code to learn how it all works and wanted to clarify that understanding of parallelism model is correct leelazeroserver single server manages whole crowd sourcing effort accumulates games from clients performs neural network training autogtp is a main client single executable run locally spins up mgpusmgames instances of leelaz executable communicates with each leelaz using GTP protocol via OS pipes communication happens every move leelaz executable manages single game receives per move commands from autogtp or other client eg GUI runs by default gpucount GPU threads each thread grabs x GPU OpenCLSchedulercpp for tree search runs multiple MCTS threads on a single tree Would you say above description accurate Thanks in advance I managed to install Leela Zero on RPi Gb with Raspbian Buster and it run pretty well With the following setting for leelaz gtp w bgz noponder p timemanage off it took aprox sec for a move So Install NodeJS curl sL sudo bash sudo apt install nodejs Install LeelaZero cd sudo apt install clinfo clinfo git clone cd leelazero git submodule update init recursive sudo apt install cmake g libboostdev libboostprogramoptionsdev libboostfilesystemdev openclheaders oclicdlibopencl oclicdopencldev zlib gdev mkdir build cd build cmake DUSECPUONLY cmake build Install Sabaki cd git clone cd Sabaki npm install npm run build npm start Install LizGoban cd git clone cd lizgoban npm install npm start Copy to lizgobanexternal directory leelaz executable and networkgz weights file I tried before as gui quarry and kigo but I had issues with them exits unexpectedly Thanks a lot for GCP leading a huge project of training Leelazero and the result is really amazing gcp Since there are new weight still comes out and many fans hope to know if there is any method to keep Leelazero growing Here is one of my suggestion Rent a server for Leelazero and I think the fees could be collected by donation or some other ways First we need GCPs approvement It is important Find volunteers to keep the trainingproduce new network adjust the learning rate blocks or visits etc If there is any new method that is useful for upgrade trainingfor example we can learn some new techniques from SAI or Katago etc the folunteers could try it in the training of Leelazero Just hope that Leelazero could be live longer and become stronger Any suggestion is appreciated I am trying to connect with AutoGTP v for the first time and I get this response AutoGTP v Using game threads per device Starting tuning process please wait Network connection to server failed NetworkException Curl returned nonzero exit code Retrying in s Can anyone help Thanks total match games in past hours in past hour Start Date Network Hashes Wins Losses Games SPRT a e VS d d fail If I want to expand the network using net will make MSE and other data abnormal The situation is very bad Hi I tried to run the tests after compilation but leelazero died with a floating point exception If I try to start the binary directly I get the same problem tests Running main from gtestmaincc Running tests from test cases Global test environment setup BLAS Core builtin Eigen library Detecting residual layersv channels blocks Initializing OpenCL autodetecting precision Detected OpenCL platforms Platform version OpenCL Mesa Platform profile FULLPROFILE Platform name Clover Platform vendor Mesa Device ID Device name AMD VERDE DRM arch LLVM Device type GPU Device vendor AMD Device driver Device speed MHz Device cores CU Device score Selected platform Clover Selected device AMD VERDE DRM arch LLVM with OpenCL capability Half precision compute support Yes Tensor Core support No OpenCL using fp half or tensor core compute support Started OpenCL SGEMM tuner Will try valid configurations floating point exception core dumped tests As hardware details are already listed above I dont know what to add If you need additional information pleas let me know Edit Added stack trace x fea f in from usrlibgalliumpipepiperadeonsiso x fea be in from usrlibgalliumpipepiperadeonsiso x fea in from usrlibgalliumpipepiperadeonsiso x fea d in from usrlibgalliumpipepiperadeonsiso x fea a in from usrlibgalliumpipepiperadeonsiso x fea f in from usrlibgalliumpipepiperadeonsiso x fea d in from usrlibgalliumpipepiperadeonsiso x fea e c in from usrlibgalliumpipepiperadeonsiso x fea a d b in from usrliblibMesaOpenCLso x fea a d in from usrliblibMesaOpenCLso x fea a d bc in from usrliblibMesaOpenCLso x fea a d in from usrliblibMesaOpenCLso x fea a c in from usrliblibMesaOpenCLso x fea b f e in clEnqueueNDRangeKernel from usrliblibOpenCLso x bf f in clCommandQueueenqueueNDRangeKernel offset events x event x ffc c b local global kernelsynthetic pointer thissynthetic pointer at homenerfgitleelasrcCLcl hpp Tunerhalffloathalftunesgemm abicxx int int int int int this x ffc c m n k batchsize runsoptimized out at homenerfgitleelasrcTunercpp x bf d in Tunerhalffloathalfloadsgemmtuners abicxx int int int int this x ffc c m n k batchsize at usrincludec extnewallocatorh x bf in OpenCLhalffloathalfinitialize this x c f f c channels batchsize at homenerfgitleelasrcTunercpp x bf c in OpenCLSchedulerhalffloathalfinitialize this x c f f e channels at usrincludec bitsuniqueptrh x bf aa b in Networkinitnet this x fea aa channels pipe at usrincludec bitsuniqueptrh x bf d a in Networkselectprecision this x fea aa channels at usrincludec bitsmoveh x bf in Networkinitialize this x fea aa playoutsoptimized out weightsfile at homenerfgitleelasrcNetworkcpp x bf a in LeelaEnvSetUp thisoptimized out at homenerfgitleelasrctestsgtestscpp x bf b c c in testinginternalSetUpEnvironmenttestingEnvironment x bf d edd in void stdforeachgnucxxnormaliteratortestingEnvironment const stdvectortestingEnvironment stdallocatortestingEnvironment void testingEnvironmentgnucxxnormaliteratortestingEnvironment const stdvectortestingEnvironment stdallocatortestingEnvironment gnucxxnormaliteratortestingEnvironment const stdvectortestingEnvironment stdallocatortestingEnvironment void testingEnvironmenttestingEnvironment x bf d in void testinginternalForEachstdvectortestingEnvironment stdallocatortestingEnvironment void testingEnvironmentstdvectortestingEnvironment stdallocatortestingEnvironment const void testingEnvironment x bf b ed in testinginternalUnitTestImplRunAllTests x bf d in bool testinginternalHandleSehExceptionsInMethodIfSupportedtestinginternalUnitTestImpl booltestinginternalUnitTestImpl bool testinginternalUnitTestImpl char const x bf cfeb in bool testinginternalHandleExceptionsInMethodIfSupportedtestinginternalUnitTestImpl booltestinginternalUnitTestImpl bool testinginternalUnitTestImpl char const x bf b fa in testingUnitTestRun x bf a f b in RUNALLTESTS x bf a ea in main A big difference between the training methods of MuZero and AlphaZero is Muzero uses K consequential steps samples and AlphaZero uses only one step samples Muzeros paper has shown the advantage of this training approach My suggestion is this Modify chunkparserpy let parse generate K batches data once the samples of this K batches form one batch of consequential K positions of different games Modify tfprocesspy using gradient accumulation method updating network parameters with the averaged gradients of K batches In MuZero K I do not have the resource for training maybe someone here can try The coding can be done within an hour