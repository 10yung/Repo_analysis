Im working on a Spark ML sentiment analysis platform using a mobile feedback APP as a data source The Data Feedback re stored on MongoDB and i wanna read them with spark I tried to connect spark to MongoDB but not lucky i keep getting this error Exception in thread main orgapachesparkSparkException Job aborted due to stage failure Task in stage failed times most recent failure Lost task in stage TID localhost executor driver javalangNoSuchMethodError orgapachesparksqlcatalystanalysisTypeCoercionfindTightestCommonTypeOfTwoLscalaFunction at comstratiodatasourcemongodbschemaMongodbSchemacomstratiodatasourcemongodbschemaMongodbSchemacompatibleTypeMongodbSchemascala at comstratiodatasourcemongodbschemaMongodbSchemaanonfun applyMongodbSchemascala at comstratiodatasourcemongodbschemaMongodbSchemaanonfun applyMongodbSchemascala at orgapachesparkutilcollectionExternalSorteranonfun applyExternalSorterscala at orgapachesparkutilcollectionExternalSorteranonfun applyExternalSorterscala at orgapachesparkutilcollectionAppendOnlyMapchangeValueAppendOnlyMapscala at orgapachesparkutilcollectionSizeTrackingAppendOnlyMapchangeValueSizeTrackingAppendOnlyMapscala at orgapachesparkutilcollectionExternalSorterinsertAllExternalSorterscala at orgapachesparkshufflesortSortShuffleWriterwriteSortShuffleWriterscala at orgapachesparkschedulerShuffleMapTaskrunTaskShuffleMapTaskscala at orgapachesparkschedulerShuffleMapTaskrunTaskShuffleMapTaskscala at orgapachesparkschedulerTaskrunTaskscala at orgapachesparkexecutorExecutorTaskRunnerrunExecutorscala at javautilconcurrentThreadPoolExecutorrunWorkerThreadPoolExecutorjava at javautilconcurrentThreadPoolExecutorWorkerrunThreadPoolExecutorjava at javalangThreadrunThreadjava Driver stacktrace at orgapachesparkschedulerDAGSchedulerorgapachesparkschedulerDAGSchedulerfailJobAndIndependentStagesDAGSchedulerscala at orgapachesparkschedulerDAGScheduleranonfunabortStage applyDAGSchedulerscala at orgapachesparkschedulerDAGScheduleranonfunabortStage applyDAGSchedulerscala at scalacollectionmutableResizableArrayclassforeachResizableArrayscala at scalacollectionmutableArrayBufferforeachArrayBufferscala at orgapachesparkschedulerDAGSchedulerabortStageDAGSchedulerscala at orgapachesparkschedulerDAGScheduleranonfunhandleTaskSetFailed applyDAGSchedulerscala at orgapachesparkschedulerDAGScheduleranonfunhandleTaskSetFailed applyDAGSchedulerscala at scalaOptionforeachOptionscala at orgapachesparkschedulerDAGSchedulerhandleTaskSetFailedDAGSchedulerscala at orgapachesparkschedulerDAGSchedulerEventProcessLoopdoOnReceiveDAGSchedulerscala at orgapachesparkschedulerDAGSchedulerEventProcessLooponReceiveDAGSchedulerscala at orgapachesparkschedulerDAGSchedulerEventProcessLooponReceiveDAGSchedulerscala at orgapachesparkutilEventLoopanon runEventLoopscala at orgapachesparkschedulerDAGSchedulerrunJobDAGSchedulerscala at orgapachesparkSparkContextrunJobSparkContextscala at orgapachesparkSparkContextrunJobSparkContextscala at orgapachesparkrddRDDanonfunaggregate applyRDDscala at orgapachesparkrddRDDOperationScopewithScopeRDDOperationScopescala at orgapachesparkrddRDDOperationScopewithScopeRDDOperationScopescala at orgapachesparkrddRDDwithScopeRDDscala at orgapachesparkrddRDDaggregateRDDscala at comstratiodatasourcemongodbschemaMongodbSchemaschemaMongodbSchemascala at comstratiodatasourcemongodbMongodbRelationcomstratiodatasourcemongodbMongodbRelationlazySchemalzycomputeMongodbRelationscala at comstratiodatasourcemongodbMongodbRelationcomstratiodatasourcemongodbMongodbRelationlazySchemaMongodbRelationscala at comstratiodatasourcemongodbMongodbRelationanonfun applyMongodbRelationscala at comstratiodatasourcemongodbMongodbRelationanonfun applyMongodbRelationscala at scalaOptiongetOrElseOptionscala at comstratiodatasourcemongodbMongodbRelationinitMongodbRelationscala at comstratiodatasourcemongodbDefaultSourcecreateRelationDefaultSourcescala at orgapachesparksqlexecutiondatasourcesDataSourceresolveRelationDataSourcescala at orgapachesparksqlDataFrameReaderloadV SourceDataFrameReaderscala at orgapachesparksqlDataFrameReaderloadDataFrameReaderscala at orgapachesparksqlDataFrameReaderloadDataFrameReaderscala at comspmlMongoDbSparkConmainMongoDbSparkConjava Caused by javalangNoSuchMethodError orgapachesparksqlcatalystanalysisTypeCoercionfindTightestCommonTypeOfTwoLscalaFunction at comstratiodatasourcemongodbschemaMongodbSchemacomstratiodatasourcemongodbschemaMongodbSchemacompatibleTypeMongodbSchemascala at comstratiodatasourcemongodbschemaMongodbSchemaanonfun applyMongodbSchemascala at comstratiodatasourcemongodbschemaMongodbSchemaanonfun applyMongodbSchemascala at orgapachesparkutilcollectionExternalSorteranonfun applyExternalSorterscala at orgapachesparkutilcollectionExternalSorteranonfun applyExternalSorterscala at orgapachesparkutilcollectionAppendOnlyMapchangeValueAppendOnlyMapscala at orgapachesparkutilcollectionSizeTrackingAppendOnlyMapchangeValueSizeTrackingAppendOnlyMapscala at orgapachesparkutilcollectionExternalSorterinsertAllExternalSorterscala at orgapachesparkshufflesortSortShuffleWriterwriteSortShuffleWriterscala at orgapachesparkschedulerShuffleMapTaskrunTaskShuffleMapTaskscala at orgapachesparkschedulerShuffleMapTaskrunTaskShuffleMapTaskscala at orgapachesparkschedulerTaskrunTaskscala at orgapachesparkexecutorExecutorTaskRunnerrunExecutorscala at javautilconcurrentThreadPoolExecutorrunWorkerThreadPoolExecutorjava INFO DAGScheduler Job failed aggregate at MongodbSchemascala took s at javautilconcurrentThreadPoolExecutorWorkerrunThreadPoolExecutorjava at javalangThreadrunThreadjava My spark code Im using Java public class MongoDbSparkCon public static void mainString args JavaSparkContext sc new JavaSparkContextlocal test sparkmongodb java SQLContext sqlContext new orgapachesparksqlSQLContextsc Map options new HashMap optionsputhost localhost optionsputdatabase feedbackuib optionsputcollection Feedback DatasetRow df sqlContextreadformatcomstratiodatasourcemongodboptionsoptionsload dfregisterTempTableFeedback sqlContextsqlSELECT FROM Feedback dfshow Pomxml dependencies dependency groupIdorgapachesparkgroupId artifactIdsparkcore artifactId version version dependency dependency groupIdorgapachesparkgroupId artifactIdsparkmllib artifactId version version dependency dependency groupIdorgapachesparkgroupId artifactIdsparksql artifactId version version dependency dependency groupIdorgapachesparkgroupId artifactIdsparkstreaming artifactId version version scopeprovidedscope dependency dependency groupIdorgmongodbsparkgroupId artifactIdmongosparkconnector artifactId version version dependency dependency groupIdcomstratiodatasourcegroupId artifactIdsparkmongodb artifactId version version dependency dependency groupIdorgmongodbgroupId artifactIdmongojavadriverartifactId version version dependency dependency groupIdorgmongodbgroupId artifactIdcasbahcore artifactId version version dependency dependency groupIdorgmongodbgroupId artifactIdcasbahcommons artifactId version version dependency dependency groupIdorgmongodbgroupId artifactIdcasbahquery artifactId version version dependency dependency groupIdorgscalalanggroupId artifactIdscalalibraryartifactId version version dependency dependency groupIdcomtypesafeakkagroupId artifactIdakkaactor artifactId version version dependency dependencies Please anny help MongodbRelation extends PrunedFilteredScan it can not push down nested struct filter I think it can use CatalystScan to push down nested struct query Hello everyone last I use mongodb spark but when writen data to mongodb db the connection socket error error tips come out commongodbMongoTimeoutException Timed out after ms while waiting to connect I find the source but not find how to make the connectTimeout to take effect thank you hello how to make index use column to read data faster thank you My cloud server is using SCRAMSHA authMechanism I dont see how to config this authMechanism in the docs Can I change the authMechanism in the python version The latest compatible version does not have SPARK do not support it I want to read collection from mongodb which is secured I am writing the following query dfraweventdata sqlContextreadformatcomstratiodatasourcemongodb optionshostserveripport databasedb collectioncollec credentials usernamedb passwordload But I am getting this error py jprotocolPy JJavaError An error occurred while calling o load commongodbCommandFailureException serverUsed myserver ok errmsg Authentication failed code codeName AuthenticationFailed at commongodbCommandResultgetExceptionCommandResultjava at commongodbCommandResultthrowOnErrorCommandResultjava at commongodbDBPortSaslAuthenticatorauthenticateDBPortjava at commongodbDBPortauthenticateDBPortjava at commongodbDBPortcheckAuthDBPortjava at commongodbDBTCPConnectorinnerCallDBTCPConnectorjava at commongodbDBTCPConnectorcallDBTCPConnectorjava at commongodbDBCollectionImplfindDBCollectionImpljava at commongodbDBcommandDBjava at commongodbDBcommandDBjava at commongodbDBCollectiongetStatsDBCollectionjava at commongodbcasbahMongoCollectionBaseclassgetStatsMongoCollectionscala at commongodbcasbahMongoCollectiongetStatsMongoCollectionscala at commongodbcasbahMongoCollectionBaseclassstatsMongoCollectionscala at commongodbcasbahMongoCollectionstatsMongoCollectionscala at comstratiodatasourcemongodbpartitionerMongodbPartitionerisShardedCollectionMongodbPartitionerscala at comstratiodatasourcemongodbpartitionerMongodbPartitioneranonfuncomputePartitions applyMongodbPartitionerscala at comstratiodatasourcemongodbpartitionerMongodbPartitioneranonfuncomputePartitions applyMongodbPartitionerscala at comstratiodatasourcemongodbutilusingMongoClientapplyusingMongoClientscala at comstratiodatasourcemongodbpartitionerMongodbPartitionercomputePartitionsMongodbPartitionerscala at comstratiodatasourcemongodbrddMongodbRDDgetPartitionsMongodbRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at scalaOptiongetOrElseOptionscala at orgapachesparkrddRDDpartitionsRDDscala at orgapachesparkrddMapPartitionsRDDgetPartitionsMapPartitionsRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at scalaOptiongetOrElseOptionscala at orgapachesparkrddRDDpartitionsRDDscala at orgapachesparkPartitionerdefaultPartitionerPartitionerscala at orgapachesparkrddPairRDDFunctionsanonfunreduceByKey applyPairRDDFunctionsscala at orgapachesparkrddPairRDDFunctionsanonfunreduceByKey applyPairRDDFunctionsscala at orgapachesparkrddRDDOperationScopewithScopeRDDOperationScopescala at orgapachesparkrddRDDOperationScopewithScopeRDDOperationScopescala at orgapachesparkrddRDDwithScopeRDDscala at orgapachesparkrddPairRDDFunctionsreduceByKeyPairRDDFunctionsscala at comstratiodatasourcemongodbschemaMongodbSchemaschemaMongodbSchemascala at comstratiodatasourcemongodbMongodbRelationcomstratiodatasourcemongodbMongodbRelationlazySchemalzycomputeMongodbRelationscala at comstratiodatasourcemongodbMongodbRelationcomstratiodatasourcemongodbMongodbRelationlazySchemaMongodbRelationscala at comstratiodatasourcemongodbMongodbRelationanonfun applyMongodbRelationscala at comstratiodatasourcemongodbMongodbRelationanonfun applyMongodbRelationscala at scalaOptiongetOrElseOptionscala at comstratiodatasourcemongodbMongodbRelationinitMongodbRelationscala at comstratiodatasourcemongodbDefaultSourcecreateRelationDefaultSourcescala at orgapachesparksqlexecutiondatasourcesDataSourceresolveRelationDataSourcescala at orgapachesparksqlDataFrameReaderloadDataFrameReaderscala at orgapachesparksqlDataFrameReaderloadDataFrameReaderscala at sunreflectNativeMethodAccessorImplinvoke Native Method at sunreflectNativeMethodAccessorImplinvokeNativeMethodAccessorImpljava at sunreflectDelegatingMethodAccessorImplinvokeDelegatingMethodAccessorImpljava at javalangreflectMethodinvokeMethodjava at py jreflectionMethodInvokerinvokeMethodInvokerjava at py jreflectionReflectionEngineinvokeReflectionEnginejava at py jGatewayinvokeGatewayjava at py jcommandsAbstractCommandinvokeMethodAbstractCommandjava at py jcommandsCallCommandexecuteCallCommandjava at py jGatewayConnectionrunGatewayConnectionjava at javalangThreadrunThreadjava Any help is appreciated Despite the multiple improvements to prevent connection leakage I have observed a sharp spike in leaked connections moving to from the migration is required by our move to Spark x Here is what happened after the upgrade The steps happen when we run a scheduled Spark job that pulls collections from Mongo The drops are cluster restarts for library upgrades Zooming out here what the situation looked like under before the upgrade to Since is the only release compatible with Spark x there is nothing to downgrade to Hi I installed spark and run the spark shell bin sparkshell jars sparkmongodb jar packages orgmongodbcasbahcore Ivy Default Cache set to homecentosivy cache The jars for the packages stored in homecentosivy jars loading settings url jarfilehomecentossparkspark binhadoop jarsivy jarorgapacheivycoresettingsivysettingsxml orgmongodbcasbahcore added as a dependency resolving dependencies orgapachesparksparksubmitparent confs default found orgmongodbcasbahcore in central found orgmongodbcasbahcommons in central found comgithubnscalatimenscalatime in central found jodatimejodatime in central found orgjodajodaconvert in central found orgmongodbmongojavadriver in central found orgslf jslf japi in central found orgmongodbcasbahquery in central resolution report resolve ms artifacts dl ms modules in use comgithubnscalatimenscalatime from central in default jodatimejodatime from central in default orgjodajodaconvert from central in default orgmongodbcasbahcommons from central in default orgmongodbcasbahcore from central in default orgmongodbcasbahquery from central in default orgmongodbmongojavadriver from central in default orgslf jslf japi from central in default modules artifacts conf number searchdwnldedevicted numberdwnlded default retrieving orgapachesparksparksubmitparent confs default artifacts copied already retrieved kB ms Using Sparks default log j profile orgapachesparklog jdefaultsproperties Setting default log level to WARN To adjust logging level use scsetLogLevelnewLevel WARN NativeCodeLoader Unable to load nativehadoop library for your platform using builtinjava classes where applicable WARN SparkContext Use an existing SparkContext some configuration may not take effect Spark context Web UI available at Spark context available as sc master local app id local Spark session available as spark Welcome to version Using Scala version Java HotSpotTM Bit Server VM Java Type in expressions to have them evaluated Type help for more information scala import orgapachesparksql import orgapachesparksql scala import commongodbcasbahWriteConcern MongodbWriteConcern import commongodbcasbahWriteConcernMongodbWriteConcern scala import comstratiodatasourcemongodb import comstratiodatasourcemongodb scala import comstratiodatasourcemongodbconfig import comstratiodatasourcemongodbconfig scala import comstratiodatasourcemongodbconfigMongodbConfig import comstratiodatasourcemongodbconfigMongodbConfig scala val builder MongodbConfigBuilderMapHost Listlocalhost Database db Collection coll SamplingRatio WriteConcern normal builder comstratiodatasourcemongodbconfigMongodbConfigBuilder MongodbConfigBuilderMapdatabase db writeConcern normal schemasamplingRatio collection coll host Listlocalhost scala scala val readConfig builderbuild readConfig comstratiodatasourceutilConfig comstratiodatasourceutilConfigBuilderanon f cee fa scala val mongoRDD sparksqlContextfromMongoDBreadConfig javalangNoSuchMethodError comstratiodatasourcemongodbMongodbContextfromMongoDBLcomstratiodatasourceutilConfigLscalaOptionLorgapachesparksqlDataset elided 