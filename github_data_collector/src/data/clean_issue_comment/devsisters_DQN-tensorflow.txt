 I dont know why this error shows up Please help me out here GPU I tensorflowcoreplatformcpufeatureguardcc Your CPU supports instructions that this TensorFlow binary was not compiled to use AVX FMA savestep teststep actionrepeat backend tf batchsize cnnformat NHWC discount display True doubleq False dueling False envname Breakoutv envtype detail epend ependt epstart historylength learnstart learningrate learningratedecay learningratedecaystep learningrateminimum maxdelta maxreward maxstep memorysize mindelta minreward model m randomstart scale screenheight screenwidth targetqupdatestep trainfrequency WARNINGtensorflowFrom hometejask DesktopDQNtensorflowdqnagentpy calling argmax from tensorflowpythonopsmathops with dimension is deprecated and will be removed in a future version Instructions for updating Use the axis argument instead WARNINGtensorflowFrom usrlocallibpython distpackagestensorflowpythonutiltfshouldusepy initializeallvariables from tensorflowpythonopsvariables is deprecated and will be removed after Instructions for updating Use tfglobalvariablesinitializer instead Loading checkpoints Load FAILED checkpointsBreakoutv mindelta maxdelta historylength trainfrequency targetqupdatestep doubleqFalsememorysize actionrepeat ependt duelingFalseminreward backendtfrandomstart scale envtypedetaillearningratedecaystep epstart screenwidth learnstart cnnformatNHWClearningrate batchsize discount maxstep maxreward learningratedecay learningrateminimum envnameBreakoutv epend modelm screenheight Traceback most recent call last File mainpy line in module tfapprun File usrlocallibpython distpackagestensorflowpythonplatformapppy line in run sysexitmainargv File mainpy line in main agenttrain File hometejask DesktopDQNtensorflowdqnagentpy line in train screen reward action terminal selfenvnewrandomgame File hometejask DesktopDQNtensorflowdqnenvironmentpy line in newrandomgame selfnewgameTrue File hometejask DesktopDQNtensorflowdqnenvironmentpy line in newgame selfstep File hometejask DesktopDQNtensorflowdqnenvironmentpy line in step selfscreen selfreward selfterminal selfenvstepaction File usrlocallibpython distpackagesgymwrapperstimelimitpy line in step assert selfepisodestartedat is not None Cannot call envstep before calling reset AssertionError Cannot call envstep before calling reset class M DQNConfig backend tf envtype detail actionrepeat class M DQNConfig backend tf envtype detail actionrepeat I use python mainpy envnameBreakoutv istrainTrue displayFalse usegpuTrue modelm and python mainpy envnameBreakoutv istrainTrue displayFalse usegpuTrue modelm The avgepr in both models reaches at around million iterations But when it comes to even million iterations the avgepr still fluctuates between and Just like the result they have shown I guess that is the result of Actionrepeat frameskip of without learning rate decay I didnt change any parameters The strange thing is even when I use model m Actionrepeat frameskip of my result is similar to model m The avgepr fluctuates between and from around million to million iterations The maxepr fluctuates between and from around million to million iterations class M DQNConfig backend tf envtype detail actionrepeat Do I need to change some parameters to reach the best result they have shown Thank you very much for selfstep in tqdmrangestartstep selfmaxstep ncols initialstartstep if selfstep selflearnstart numgame selfupdatecount epreward totalreward selftotalloss selftotalq eprewards actions predict action selfpredictselfhistoryget act screen reward terminal selfenvactaction istrainingTrue observe selfobservescreen reward action terminal if terminal screen reward action terminal selfenvnewrandomgame numgame eprewardsappendepreward epreward Function train in agentpy may not handle properly when the game is terminated As the game is terminated the new screen didnt add into history and memory selfhistory isnt get updated And in the next iteration action selfpredictselfhistoryget will be the same ie terminated in dqnagentpy line if terminal screen reward action terminal selfenvnewrandomgame when starting a new game due to a terminal state why we dont need to reset the selfhistory because it would affect the next iteration predict action selfpredictselfhistoryget act screen reward terminal selfenvactaction istrainingTrue observe selfobservescreen reward action terminal the predicted action for selfhistoryget is not depending on the current game screens it will predict action for the previous game screen which is ended instead Do I miss anything Thank you very much whats the difference between simple game and detail game I tried to run from the windows command line using the command in Readme but it says Import Error No module named tensorflow How do I run the program on Windows I am using Anaconda where is the m and m Fixed minor screen size bug in environment This suggests that the maximum of the minimum learning rate and exponentially decayed rate is calculated But in the configurations file both the learning rate and the minimum learning rate are supplied the same values This will result in no updates to the learning rate with more training steps OR Is this specifically for the case with no updates in the learning rate Thanks