Hi As stated in the README Im using MLeap because I run Spark My pipeline is as follows code simplified val str new StringIndexer val ohe new OneHotEncoder val vecAssembler new VectorAssembler val rf new RandomForestClassifier new PipelinesetStagesArraystr ohe vecAssembler rf The saving works fine val filePrefix jarfile val filePath tmpmodelzip val sbc SparkBundleContextwithDatasetmodeltransformdata for bf managedBundleFilesfilePrefixfilePath modelwriteBundleformatSerializationFormatJsonsavebfsbcget I checked the model I saved here is what OneHotEncoder looks like op onehotencoder attributes size long droplast boolean true The problem happens when Im trying to load the pipeline to use it When I do import mlcombustmleapruntimeMleapSupport import javanetURI val bundleUri new URIjarfiletmpmodelzip val model bundleUriloadMleapBundlegetroot I get the following error javautilNoSuchElementException key not found categorysizes This attribute comes from the new OneHotEncoder estimator from Spark but Im running Spark When retrieving the list of ops from the referenceconf file of mleapruntime we get two keys for the OneHotEncoder here they are mlcombustmleapbundleopsfeatureNormalizerOp mlcombustmleapbundleopsfeatureOneHotEncoderOp mlcombustmleapbundleopsfeatureOneHotEncoderOpV mlcombustmleapbundleopsfeaturePcaOp And then in BundleRegistry class line we loop through the ops name instantiate them and put them in a Map with their name as the key The thing is OneHotEncoderOp and OneHotEncoderOpV have the same key and thus OneHotEncoderOpV overrides OneHotEncoderOp in the Map you can clearly see that during debug Afterwards when trying to load my Spark model MLeap sees the OneHotEncoderOpV has the correct transformer to load and looks for an attribute categorysizes that does not exist in the model I saved Is something wrong on my end Maybe Im missing some conf or something Thanks for your help This builds on top of so the additional commits are the point of this one On Travis stringmaptest testserializetobundle fails with this error javautilNoSuchElementException key not found orgapachesparkmlmleapfeatureStringMap This is thrown when trying opAliasorgapachesparkmlmleapfeatureStringMap in BundleRegistryscala this line So apparently the opAlias map hasnt been initialized properly Normally its initialized here Which is called by this code The registryConfiggetStringListops here expects to read from this resource file but if referenceconf is not in classpath then nothing is read Locally I was able to trace this problem down to this sbt mleapsparkextensioncompile find mleapsparkextensiontarget name referenceconf finds only mleapsparkextensiontargetscala classesreferenceconf sbt mleapsparkextensioncompile find mleapsparkextensiontarget name referenceconf finds only mleapsparkextensiontargetscala classesreferenceconf With this I can also reproduce the Travis failure by trying to run tox c pythontoxini v when the conf doesnt exist in the classes folder So somehow building sbt with ie multiversion builds first then removes the referenceconf from under scala classes even though it doesnt delete the class files in that folder Does it make any sense that sbt behaves like this I posted this as an sbt question on stack overflow Full trace of the failed test on Travis ERROR testserializetobundle testspysparkstringmaptestStringMapTest Traceback most recent call last File hometravisbuildcombustmleappythontestspysparkstringmaptestpy line in testserializetobundle serializetofilepipelinefile selfinput pipeline File hometravisbuildcombustmleappythontestspysparkstringmaptestpy line in serializetofile SimpleSparkSerializerserializeToBundlemodel tofilepathpath dfforserializing File hometravisbuildcombustmleappythonmleappysparksparksupportpy line in serializeToBundle selfjavaobjserializeToBundletransformertojava path datasetjdf File hometravisbuildcombustmleappythontoxpylocallibpython sitepackagespy jjavagatewaypy line in call answer selfgatewayclient selftargetid selfname File hometravisbuildcombustmleappythontoxpylocallibpython sitepackagespysparksqlutilspy line in deco return fa kw File hometravisbuildcombustmleappythontoxpylocallibpython sitepackagespy jprotocolpy line in getreturnvalue formattargetid name value Py JJavaError An error occurred while calling o serializeToBundle javautilNoSuchElementException key not found orgapachesparkmlmleapfeatureStringMap at scalacollectionMapLikeclassdefaultMapLikescala at scalacollectionAbstractMapdefaultMapscala at scalacollectionMapLikeclassapplyMapLikescala at scalacollectionAbstractMapapplyMapscala at mlcombustbundleBundleRegistryopForObjBundleRegistryscala at mlcombustbundleserializerGraphSerializeranonfunwriteNode applyGraphSerializerscala at mlcombustbundleserializerGraphSerializeranonfunwriteNode applyGraphSerializerscala at scalautilTryapplyTryscala at mlcombustbundleserializerGraphSerializerwriteNodeGraphSerializerscala at mlcombustbundleserializerGraphSerializeranonfunwrite applyGraphSerializerscala at mlcombustbundleserializerGraphSerializeranonfunwrite applyGraphSerializerscala at scalacollectionIndexedSeqOptimizedclassfoldlIndexedSeqOptimizedscala at scalacollectionIndexedSeqOptimizedclassfoldLeftIndexedSeqOptimizedscala at scalacollectionmutableWrappedArrayfoldLeftWrappedArrayscala at mlcombustbundleserializerGraphSerializerwriteGraphSerializerscala at orgapachesparkmlbundleopsPipelineOpanon storePipelineOpscala at orgapachesparkmlbundleopsPipelineOpanon storePipelineOpscala at mlcombustbundleserializerModelSerializeranonfunwrite applyModelSerializerscala at mlcombustbundleserializerModelSerializeranonfunwrite applyModelSerializerscala at scalautilTryapplyTryscala at mlcombustbundleserializerModelSerializerwriteModelSerializerscala at mlcombustbundleserializerNodeSerializeranonfunwrite applyNodeSerializerscala at mlcombustbundleserializerNodeSerializeranonfunwrite applyNodeSerializerscala at scalautilTryapplyTryscala at mlcombustbundleserializerNodeSerializerwriteNodeSerializerscala at mlcombustbundleserializerBundleSerializeranonfunwrite applyBundleSerializerscala at mlcombustbundleserializerBundleSerializeranonfunwrite applyBundleSerializerscala at scalautilTryapplyTryscala at mlcombustbundleserializerBundleSerializerwriteBundleSerializerscala at mlcombustbundleBundleWritersaveBundleWriterscala at mlcombustmleapsparkSimpleSparkSerializeranonfunserializeToBundleWithFormat applySimpleSparkSerializerscala at mlcombustmleapsparkSimpleSparkSerializeranonfunserializeToBundleWithFormat applySimpleSparkSerializerscala at resourceAbstractManagedResourceanonfun applyAbstractManagedResourcescala at scalautilcontrolExceptionCatchanonfuneither applyExceptionscala at scalautilcontrolExceptionCatchanonfuneither applyExceptionscala at scalautilcontrolExceptionCatchapplyExceptionscala at scalautilcontrolExceptionCatcheitherExceptionscala at resourceAbstractManagedResourceacquireForAbstractManagedResourcescala at resourceManagedResourceOperationsclassapplyManagedResourceOperationsscala at resourceAbstractManagedResourceapplyAbstractManagedResourcescala at resourceDeferredExtractableManagedResourceanonfuntried applyAbstractManagedResourcescala at scalautilTryapplyTryscala at resourceDeferredExtractableManagedResourcetriedAbstractManagedResourcescala at mlcombustmleapsparkSimpleSparkSerializerserializeToBundleWithFormatSimpleSparkSerializerscala at mlcombustmleapsparkSimpleSparkSerializerserializeToBundleSimpleSparkSerializerscala at sunreflectNativeMethodAccessorImplinvoke Native Method at sunreflectNativeMethodAccessorImplinvokeNativeMethodAccessorImpljava at sunreflectDelegatingMethodAccessorImplinvokeDelegatingMethodAccessorImpljava at javalangreflectMethodinvokeMethodjava at py jreflectionMethodInvokerinvokeMethodInvokerjava at py jreflectionReflectionEngineinvokeReflectionEnginejava at py jGatewayinvokeGatewayjava at py jcommandsAbstractCommandinvokeMethodAbstractCommandjava at py jcommandsCallCommandexecuteCallCommandjava at py jGatewayConnectionrunGatewayConnectionjava at javalangThreadrunThreadjava it seems that this bug not fixed for version with this case i write two test cases first is a spark test case which generate a mleap zip model to local second test case is a java test case which load the zip model and predict on sampe data from test s dataframe but the probability is not same here is my code test package comzenmenwkreadpmmlModelctrModel import comzenmenwkreadSparkFunSuite import orgapachesparkmlclassificationLogisticRegression LogisticRegressionModel import orgapachesparkmlfeature import orgapachesparkmllinalgVector import orgapachesparkmlPipeline PipelineStage import orgapachesparksqlfunctions import orgscalatestFunSuite import scalacollectionmutableArrayBuffer import scalautilTry class ChapterLrManModelTest extends FunSuite with SparkFunSuite private val getKeys Map String Double Seq String input Map String Double inputkeySettoSeq private val getProb functionsudfv Vector vtoArray val keyUdf functionsudfgetKeys testtestTrainModel val data sparkcreateDataFrameSeq Array Mapa b c Array Mapd e c Array Mapx a b Array Mapc b w Array Mapc b w Array Mapc b w Array Mapa b d Array Mapc b w toDFbookid pv myInputCol label val data data withColumnmyInputCol keyUdffunctionscolmyInputCol dropmyInputCol datashow dataprintSchema val pipelineStage new ArrayBuffer PipelineStage val bookFiter new CountVectorizer setInputColbookid setOutputColbookidvec setMinDF setMinTF setBinarytrue pipelineStage bookFiter val doubleDiscretizer new QuantileDiscretizer setInputColpv setOutputColpvbucket setNumBuckets pipelineStage doubleDiscretizer val myFiter new CountVectorizer setInputColmyInputCol setOutputColmyInputCol vec setMinDF setMinTF setBinarytrue pipelineStage myFiter val vectorAsCols Arraypvbucket bookidvec myInputCol vec val vectorAssembler new VectorAssemblersetInputColsvectorAsColssetOutputColvectorFeature pipelineStage vectorAssembler val scaler new MinMaxScalersetInputColvectorFeaturesetOutputColscaledFeatures pipelineStage scaler val lr LogisticRegression new LogisticRegression setFitIntercepttrue setMaxIter max iteration setFeaturesColscaledFeatures setLabelCollabel pipelineStage lr val featurePipeline new PipelinesetStagesArraymyFiter labelQuant vectorAssembler scaler val featurePipeline new PipelinesetStagespipelineStagetoArray val fitor featurePipelinefitdata val data fitortransformdata val lrm fitorstageslastasInstanceOf LogisticRegressionModel val vecModels fitor stages mapx TryxasInstanceOf CountVectorizerModel filterx xisSuccess val quantModels fitor stages mapx TryxasInstanceOf Bucketizer filterx xisSuccess val featureIndex data schemafieldIndexvectorFeature val vecMap data schemafieldsfeatureIndexmetadatagetMetadatamlattrgetMetadataattrs val featureMapping Array TryvecMapgetMetadataArraynumeric TryvecMapgetMetadataArraybinary TryvecMapgetMetadataArraynominal filterx xisSuccess flatMapx xget mapx xgetLongidx xgetStringname val featureCof featureMapping mapx x x mapx x lrmcoefficientsx toInt toList sortByx x reverse mapx if x containsvec val featureOrgIndex x lastIndexOftoInt val vecModelOutCol x substring featureOrgIndex val vecIndex x substringfeatureOrgIndex toInt val vecModels fitor stages mapx TryxasInstanceOf CountVectorizerModel filterx xisSuccess xgetgetOutputCol vecModelOutCol mapx xget val feature vecModelsheadvocabularyvecIndex svecModelOutColfeature x else x x mapx sx tx mkString n val dataFinal data select Seqfunctionsconcatws functionscolbookid functionscolpv functionsconcatws functionscolmyInputCol getProb functionscolprobabilityaliasprob dataFinalshow printlndataFinalschema dataFinal write modeoverwrite csvsdataOutPathlrbasecsv val mleapModel new comzenmenwkreadpmmlModelservingmleapserializationModelSerializer mleapModelserializeModelfitor sjarfiledataOutPathlrbasemodellrmodelmleapzip data split line test package comzenmenwkreadpmmlModelservingmleapload import comgooglecommoncollectLists import mlcombustmleapcoretypesStructField import mlcombustmleapcoretypesStructType import mlcombustmleapruntimeframeRow import mlcombustmleapruntimejavadslLeapFrameBuilder import orgjunitTest import javaioFile import javautilArrayList import javautilList program wkrec description mleap java serving author XuChao create public class JavaModelServerTest String testResourcePath Stringformatssrctestresource new FilegetAbsolutePath String testDataInPath StringformatssrctestdataIn new FilegetAbsolutePath String testDataOutPath StringformatssrctestdataOut new FilegetAbsolutePath Test public void testLoadModel LeapFrameBuilder builder new LeapFrameBuilder ListStructField fields new ArrayList fieldsaddbuildercreateFieldbookid buildercreateListbuildercreateBasicString fieldsaddbuildercreateFieldpv buildercreateDouble fieldsaddbuildercreateFieldmyInputCol buildercreateListbuildercreateBasicString true StructType schema buildercreateSchemafields JavaModelServer javaModelServer new JavaModelServerStringformatslrbasemodellrmodelmleapzip testDataOutPath schema javaModelServerloadModel ListListString books ListsnewArrayList booksaddListsnewArrayList booksaddListsnewArrayList booksaddListsnewArrayList booksaddListsnewArrayList booksaddListsnewArrayList booksaddListsnewArrayList booksaddListsnewArrayList booksaddListsnewArrayList ListDouble pv ListsnewArrayList pvadd pvadd pvadd pvadd pvadd pvadd pvadd pvadd ListListString myInputCol ListsnewArrayList myInputColaddListsnewArrayLista b c myInputColaddListsnewArrayListd e c myInputColaddListsnewArrayListx a b myInputColaddListsnewArrayListc b w myInputColaddListsnewArrayListc b w myInputColaddListsnewArrayListc b w myInputColaddListsnewArrayLista b d myInputColaddListsnewArrayListc b w ListRow rows ListsnewArrayList forint i i pvsize i ListObject rowValues ListsnewArrayList rowValuesaddbooksgeti rowValuesaddpvgeti rowValuesaddmyInputColgeti Row features buildercreateRowFromIterablerowValues ArrayListObject result javaModelServerforecastfeatures SystemoutprintlnStringformat f double resultget test s predict is concatws bookid pvconcatws myInputCol prob abc dec xab cbw cbw cbw abd cbw test s predict probability is the first row probability is same others are different and order changed Anyone else frustrated like me at the lack of mleap support Im desperate enough that Im willing to pay the creators to help get my issues resolved After creating a PySpark model and serializing it to a bundle I try to read in the MLeap transformer and make a prediction but the prediction is wrong Upon investigation Ive found that the inputSchema of the model has been modified so the features are in the wrong order If you simply print out the PipelineModel it shows the features in the correct order but calling inputSchema gives an incorrect order abaveja Code to reproduce Model from sklearndatasets import loadbreastcancer from pyspark import SparkConf SparkContext SQLContext from pysparksql import Row from pysparkmlfeature import VectorAssemblerStringIndexer from pysparkmlclassification import LogisticRegressionRandomForestClassifier from pysparkml import Pipeline data loadbreastcancer X y data data data target cols stri for i in data featurenames label sample Rowcols dataframe for Xsample ysample in zipX y Xdata floati for i in Xsample label floatysample sampledata Xdata label dataframeappendsamplesampledata df sqlContextcreateDataFramedataframe features dfcolumns featuresremovelabel assembler VectorAssemblerinputColsfeatures outputColfeatures model RandomForestClassifier pipeline Pipelinestages assembler model train test dfrandomSplit fittedPipeline pipelinefittrain predictions fittedPipelinetransformtest printpredictionsselectpredictionlimit collect serialization and movement to HDFS fittedPipelineserializeToBundlejarfiletmpmleaprftestzip predictions bash hdfs dfs copyFromLocal f tmpmleaprftestzip tmpmleaprftestzip Reading in the model scala import javanetURI import mlbundlehdfsHadoopBundleFileSystem import mlcombustmleapruntimeMleapContext import mlcombustmleapruntimeframeTransformer import mlcombustmleapruntimeMleapSupport import orgapachehadoopconfConfiguration import orgapachehadoopfsFileSystem object HDFSRetriever val config new Configuration Create the hadoop file system val fs FileSystem FileSystemgetconfig Create the hadoop bundle file system val bundleFs new HadoopBundleFileSystemfs Create an implicit custom mleap context for savingloading implicit val customMleapContext MleapContext MleapContextdefaultContextcopy registry MleapContextdefaultContextbundleRegistryregisterFileSystembundleFs Load a given model from HDFS using the configuration specified in the MLeapContext param path hdfs path to load from def loadBundleFromHDFSpath String Transformer new URIpathloadMleapBundlegetroot val model HDFSRetrieverloadBundleFromHDFShdfstmpmleaprftestzip printmodel out PipelinePipelineModel b a NodeShapeMapMap PipelineModelListVectorAssemblerVectorAssemblerb fd d NodeShapeMap input Socketinput mean radius input Socketinput mean texture input Socketinput mean perimeter input Socketinput mean area input Socketinput mean smoothness input Socketinput mean compactness input Socketinput mean concavity input Socketinput mean concave points input Socketinput mean symmetry input Socketinput mean fractal dimension input Socketinput radius error input Socketinput texture error input Socketinput perimeter error input Socketinput area error input Socketinput smoothness error input Socketinput compactness error input Socketinput concavity error input Socketinput concave points error input Socketinput symmetry error input Socketinput fractal dimension error input Socketinput worst radius input Socketinput worst texture input Socketinput worst perimeter input Socketinput worst area input Socketinput worst smoothness input Socketinput worst compactness input Socketinput worst concavity input Socketinput worst concave points input Socketinput worst symmetry input Socketinput worst fractal dimensionMapoutput SocketoutputfeaturesVectorAssemblerModelListScalarShapetrue printing the inputSchema scala modelinputSchemafieldszipWithIndexforeach case field idx printlnsidx field out StructFieldmean textureScalarTypedoubletrue StructFieldconcavity errorScalarTypedoubletrue StructFieldmean compactnessScalarTypedoubletrue StructFieldmean radiusScalarTypedoubletrue StructFieldtexture errorScalarTypedoubletrue StructFieldmean smoothnessScalarTypedoubletrue StructFieldconcave points errorScalarTypedoubletrue StructFieldworst concavityScalarTypedoubletrue StructFieldmean concavityScalarTypedoubletrue StructFieldcompactness errorScalarTypedoubletrue StructFieldmean areaScalarTypedoubletrue StructFieldworst fractal dimensionScalarTypedoubletrue StructFieldworst concave pointsScalarTypedoubletrue StructFieldworst perimeterScalarTypedoubletrue StructFieldarea errorScalarTypedoubletrue StructFieldworst compactnessScalarTypedoubletrue StructFieldworst textureScalarTypedoubletrue StructFieldmean concave pointsScalarTypedoubletrue StructFieldmean symmetryScalarTypedoubletrue StructFieldworst areaScalarTypedoubletrue StructFieldsymmetry errorScalarTypedoubletrue StructFieldfractal dimension errorScalarTypedoubletrue StructFieldworst radiusScalarTypedoubletrue StructFieldworst smoothnessScalarTypedoubletrue StructFieldmean fractal dimensionScalarTypedoubletrue StructFieldradius errorScalarTypedoubletrue StructFieldsmoothness errorScalarTypedoubletrue StructFieldmean perimeterScalarTypedoubletrue StructFieldworst symmetryScalarTypedoubletrue StructFieldperimeter errorScalarTypedoubletrue As you can see the inputSchema is wrong causing all predictions to be wrong Ive reproduced the same with LogisticRegression models as well Im stuck here because without being able to generate the schema I have to specify it each time which creates nonreproducible code Is there something Im doing wrong here or missing Help would be greatly appreciated The Imputer class imported here Was removed in scikitlearn version as mentioned here I ran into this exception and dont seem to be able to identify the root cause Am I missing any dependencies Project Structure buildsbt project Dependenciesscala xyzProjectscala buildproperties pluginssbt xyzcore buildsbt src main scala com abc xyz core features Timestampscala utils DateUtilscala xyzexamples buildsbt src main scala com abc xyz examples TimestampFeatureExtractorscala xyzmleap buildsbt src main resources referenceconf scala com abc xyz mleap ops TimestampFeaturizerOpscala transformer TimestampFeaturizerscala xyzspark buildsbt src main resources referenceconf scala com abc xyz spark ops TimestampFeaturizerOpscala transformer TimestampFeaturizerscala versionsbt Dependencies for xyzspark libraryDependencies Seq orgapachespark sparkcore orgapachespark sparksql orgapachespark sparkmllib mlcombustmleap mleapspark mlcombustmleap mleapsparkextension Dependencies for xyzmleap libraryDependencies Seq mlcombustmleap mleapruntime Dependencies for xyzexamples libraryDependencies Seq orgapachespark sparkcore orgapachespark sparksql Content for xyzsparksrcmainresourcesreferenceconf comabcxyzsparkops comabcxyzsparkopsTimestampFeaturizerOp mlcombustmleapsparkregistrydefault comabcxyzsparkops Content for xyzmleapsrcmainresourcesreferenceconf comabcxyzmleapops comabcxyzmleapopsTimestampFeaturizerOp mlcombustmleapregistrydefaultops comabcxyzmleapops Content for xyzexamplessrcmainscalacomabcxyzTimestampFeatureExtractorscala package comabcxyzexamples import resource import mlcombustbundleBundleFile import orgapachesparkmlmleapSparkUtil import mlcombustmleapsparkSparkSupport import orgapachesparkmlbundleSparkBundleContext import orgapachesparkSparkConf import orgapachesparkmlPipeline Transformer import orgapachesparksqlDataFrame SparkSession import comabcxyzsparktransformerTimestampFeaturizer import mlcombustbundleserializerSerializationFormat object TimestampFeatureExtractor def mainargs Array String Unit initialize spark val conf new SparkConfsetAppNameDateTransformersetMasterlocal val spark SparkSessionbuilderconfigconfgetOrCreate import sparkimplicits val training Seq T T T T T T toDFuid timestampColumn trainingshow val tsFeaturizer Transformer new TimestampFeaturizersetTimestampColtimestampColumn val pipeline new PipelinesetStagesArraytsFeaturizer val model pipelinefittraining val test training modeltransformtestshow val mleapPipeline SparkUtilcreatePipelineModeluid pipeline Arraymodel val sbc SparkBundleContext for bundle managedBundleFilejarfileUsersketankumartmpmodelsTimestampFeatureExtractorzip mleapPipelinewriteBundleformatSerializationFormatJsonsavebundlesbcget Exception in thread main javautilNoSuchElementException key not found orgapachesparkmlPipelineModel at scalacollectionMapLikeclassdefaultMapLikescala at scalacollectionAbstractMapdefaultMapscala at scalacollectionMapLikeclassapplyMapLikescala at scalacollectionAbstractMapapplyMapscala at mlcombustbundleBundleRegistryopForObjBundleRegistryscala at mlcombustbundleBundleWriteranonfun applyBundleWriterscala at mlcombustbundleBundleWriteranonfun applyBundleWriterscala at scalaOptiongetOrElseOptionscala at mlcombustbundleBundleWritersaveBundleWriterscala at comabcxyzexamplesTimestampFeatureExtractoranonfunmain applyTimestampFeatureExtractorscala at comabcxyzexamplesTimestampFeatureExtractoranonfunmain applyTimestampFeatureExtractorscala at resourceAbstractManagedResourceanonfun applyAbstractManagedResourcescala at scalautilcontrolExceptionCatchanonfuneither applyExceptionscala at scalautilcontrolExceptionCatchanonfuneither applyExceptionscala at scalautilcontrolExceptionCatchapplyExceptionscala at scalautilcontrolExceptionCatcheitherExceptionscala at resourceAbstractManagedResourceacquireForAbstractManagedResourcescala at resourceManagedResourceOperationsclassapplyManagedResourceOperationsscala at resourceAbstractManagedResourceapplyAbstractManagedResourcescala at resourceManagedResourceOperationsclassacquireAndGetManagedResourceOperationsscala at resourceAbstractManagedResourceacquireAndGetAbstractManagedResourcescala at resourceManagedResourceOperationsclassforeachManagedResourceOperationsscala at resourceAbstractManagedResourceforeachAbstractManagedResourcescala at comabcxyzexamplesTimestampFeatureExtractormainTimestampFeatureExtractorscala at comabcxyzexamplesTimestampFeatureExtractormainTimestampFeatureExtractorscala I am training and serializing a scikitlearn Pipeline that contains a FeatureExtractor and a LogisticRegression and I am using Spark to deserialize it in Python But when I tried to run predict function on the deserialized pipeline model I encountered an error saying javautilNoSuchElementException Failed to find a default value for threshold Below is the full error log answer xro gatewayclient py jjavagatewayGatewayClient object at x efc eb f targetid o name collectToPython def getreturnvalueanswer gatewayclient targetidNone nameNone Converts an answer received from the Java gateway into a Python object For example string representation of integers are converted to Python integer string representation of objects are converted to JavaObject instances etc param answer the string returned by the Java gateway param gatewayclient the gateway client used to communicate with the Java Gateway Only necessary if the answer is a reference eg object list map param targetid the name of the object from which the answer comes from eg object in object hello Optional param name the name of the member from which the answer comes from eg hello in object hello Optional if iserroranswer if lenanswer type answer value OUTPUTCONVERTER type answer gatewayclient if answer REFERENCETYPE raise Py JJavaError An error occurred while calling n formattargetid name value E py jprotocolPy JJavaError An error occurred while calling o collectToPython E orgapachesparkSparkException Job aborted due to stage failure Task in stage failed times most recent failure Lost task in stage TID localhost executor driver orgapachesparkSparkException Failed to execute user defined functionanonfun structtypetinyintsizeintindicesarrayintvaluesarraydouble double E at orgapachesparksqlcatalystexpressionsGeneratedClassGeneratedIteratorForCodegenStage processNextUnknown Source E at orgapachesparksqlexecutionBufferedRowIteratorhasNextBufferedRowIteratorjava E at orgapachesparksqlexecutionWholeStageCodegenExecanonfun anon hasNextWholeStageCodegenExecscala E at orgapachesparksqlexecutionSparkPlananonfun applySparkPlanscala E at orgapachesparksqlexecutionSparkPlananonfun applySparkPlanscala E at orgapachesparkrddRDDanonfunmapPartitionsInternal anonfunapply applyRDDscala E at orgapachesparkrddRDDanonfunmapPartitionsInternal anonfunapply applyRDDscala E at orgapachesparkrddMapPartitionsRDDcomputeMapPartitionsRDDscala E at orgapachesparkrddRDDcomputeOrReadCheckpointRDDscala E at orgapachesparkrddRDDiteratorRDDscala E at orgapachesparkschedulerResultTaskrunTaskResultTaskscala E at orgapachesparkschedulerTaskrunTaskscala E at orgapachesparkexecutorExecutorTaskRunneranonfun applyExecutorscala E at orgapachesparkutilUtilstryWithSafeFinallyUtilsscala E at orgapachesparkexecutorExecutorTaskRunnerrunExecutorscala E at javautilconcurrentThreadPoolExecutorrunWorkerThreadPoolExecutorjava E at javautilconcurrentThreadPoolExecutorWorkerrunThreadPoolExecutorjava E at javalangThreadrunThreadjava E Caused by javautilNoSuchElementException Failed to find a default value for threshold E at orgapachesparkmlparamParamsanonfungetOrDefault applyparamsscala E at orgapachesparkmlparamParamsanonfungetOrDefault applyparamsscala E at scalaOptiongetOrElseOptionscala E at orgapachesparkmlparamParamsclassgetOrDefaultparamsscala E at orgapachesparkmlPipelineStagegetOrDefaultPipelinescala E at orgapachesparkmlparamParamsclassparamsscala E at orgapachesparkmlPipelineStagePipelinescala E at orgapachesparkmlclassificationLogisticRegressionParamsclassgetThresholdLogisticRegressionscala E at orgapachesparkmlclassificationLogisticRegressionModelgetThresholdLogisticRegressionscala E at orgapachesparkmlclassificationLogisticRegressionModelraw predictionLogisticRegressionscala E at orgapachesparkmlclassificationProbabilisticClassificationModelanonfun applyProbabilisticClassifierscala E at orgapachesparkmlclassificationProbabilisticClassificationModelanonfun applyProbabilisticClassifierscala E more E E Driver stacktrace E at orgapachesparkschedulerDAGSchedulerorgapachesparkschedulerDAGSchedulerfailJobAndIndependentStagesDAGSchedulerscala E at orgapachesparkschedulerDAGScheduleranonfunabortStage applyDAGSchedulerscala E at orgapachesparkschedulerDAGScheduleranonfunabortStage applyDAGSchedulerscala E at scalacollectionmutableResizableArrayclassforeachResizableArrayscala E at scalacollectionmutableArrayBufferforeachArrayBufferscala E at orgapachesparkschedulerDAGSchedulerabortStageDAGSchedulerscala E at orgapachesparkschedulerDAGScheduleranonfunhandleTaskSetFailed applyDAGSchedulerscala E at orgapachesparkschedulerDAGScheduleranonfunhandleTaskSetFailed applyDAGSchedulerscala E at scalaOptionforeachOptionscala E at orgapachesparkschedulerDAGSchedulerhandleTaskSetFailedDAGSchedulerscala E at orgapachesparkschedulerDAGSchedulerEventProcessLoopdoOnReceiveDAGSchedulerscala E at orgapachesparkschedulerDAGSchedulerEventProcessLooponReceiveDAGSchedulerscala E at orgapachesparkschedulerDAGSchedulerEventProcessLooponReceiveDAGSchedulerscala E at orgapachesparkutilEventLoopanon runEventLoopscala E at orgapachesparkschedulerDAGSchedulerrunJobDAGSchedulerscala E at orgapachesparkSparkContextrunJobSparkContextscala E at orgapachesparkSparkContextrunJobSparkContextscala E at orgapachesparkSparkContextrunJobSparkContextscala E at orgapachesparkSparkContextrunJobSparkContextscala E at orgapachesparkrddRDDanonfuncollect applyRDDscala E at orgapachesparkrddRDDOperationScopewithScopeRDDOperationScopescala E at orgapachesparkrddRDDOperationScopewithScopeRDDOperationScopescala E at orgapachesparkrddRDDwithScopeRDDscala E at orgapachesparkrddRDDcollectRDDscala E at orgapachesparksqlexecutionSparkPlanexecuteCollectSparkPlanscala E at orgapachesparksqlDatasetanonfuncollectToPython applyDatasetscala E at orgapachesparksqlDatasetanonfuncollectToPython applyDatasetscala E at orgapachesparksqlDatasetanonfun applyDatasetscala E at orgapachesparksqlexecutionSQLExecutionanonfunwithNewExecutionId applySQLExecutionscala E at orgapachesparksqlexecutionSQLExecutionwithSQLConfPropagatedSQLExecutionscala E at orgapachesparksqlexecutionSQLExecutionwithNewExecutionIdSQLExecutionscala E at orgapachesparksqlDatasetwithActionDatasetscala E at orgapachesparksqlDatasetcollectToPythonDatasetscala E at sunreflectNativeMethodAccessorImplinvoke Native Method E at sunreflectNativeMethodAccessorImplinvokeNativeMethodAccessorImpljava E at sunreflectDelegatingMethodAccessorImplinvokeDelegatingMethodAccessorImpljava E at javalangreflectMethodinvokeMethodjava E at py jreflectionMethodInvokerinvokeMethodInvokerjava E at py jreflectionReflectionEngineinvokeReflectionEnginejava E at py jGatewayinvokeGatewayjava E at py jcommandsAbstractCommandinvokeMethodAbstractCommandjava E at py jcommandsCallCommandexecuteCallCommandjava E at py jGatewayConnectionrunGatewayConnectionjava E at javalangThreadrunThreadjava E Caused by orgapachesparkSparkException Failed to execute user defined functionanonfun structtypetinyintsizeintindicesarrayintvaluesarraydouble double E at orgapachesparksqlcatalystexpressionsGeneratedClassGeneratedIteratorForCodegenStage processNextUnknown Source E at orgapachesparksqlexecutionBufferedRowIteratorhasNextBufferedRowIteratorjava E at orgapachesparksqlexecutionWholeStageCodegenExecanonfun anon hasNextWholeStageCodegenExecscala E at orgapachesparksqlexecutionSparkPlananonfun applySparkPlanscala E at orgapachesparksqlexecutionSparkPlananonfun applySparkPlanscala E at orgapachesparkrddRDDanonfunmapPartitionsInternal anonfunapply applyRDDscala E at orgapachesparkrddRDDanonfunmapPartitionsInternal anonfunapply applyRDDscala E at orgapachesparkrddMapPartitionsRDDcomputeMapPartitionsRDDscala E at orgapachesparkrddRDDcomputeOrReadCheckpointRDDscala E at orgapachesparkrddRDDiteratorRDDscala E at orgapachesparkschedulerResultTaskrunTaskResultTaskscala E at orgapachesparkschedulerTaskrunTaskscala E at orgapachesparkexecutorExecutorTaskRunneranonfun applyExecutorscala E at orgapachesparkutilUtilstryWithSafeFinallyUtilsscala E at orgapachesparkexecutorExecutorTaskRunnerrunExecutorscala E at javautilconcurrentThreadPoolExecutorrunWorkerThreadPoolExecutorjava E at javautilconcurrentThreadPoolExecutorWorkerrunThreadPoolExecutorjava E more E Caused by javautilNoSuchElementException Failed to find a default value for threshold E at orgapachesparkmlparamParamsanonfungetOrDefault applyparamsscala E at orgapachesparkmlparamParamsanonfungetOrDefault applyparamsscala E at scalaOptiongetOrElseOptionscala E at orgapachesparkmlparamParamsclassgetOrDefaultparamsscala E at orgapachesparkmlPipelineStagegetOrDefaultPipelinescala E at orgapachesparkmlparamParamsclassparamsscala E at orgapachesparkmlPipelineStagePipelinescala E at orgapachesparkmlclassificationLogisticRegressionParamsclassgetThresholdLogisticRegressionscala E at orgapachesparkmlclassificationLogisticRegressionModelgetThresholdLogisticRegressionscala E at orgapachesparkmlclassificationLogisticRegressionModelraw predictionLogisticRegressionscala E at orgapachesparkmlclassificationProbabilisticClassificationModelanonfun applyProbabilisticClassifierscala E at orgapachesparkmlclassificationProbabilisticClassificationModelanonfun applyProbabilisticClassifierscala E more venvlibpython sitepackagespy jprotocolpy Py JJavaError If I understand correctly deserializing a LogisticRegressionModel with Spark is using LogisticRegressionOp from mleapspark It will try to look for threshold parameter first if it is not there nothing will be set Since scikitlearn LogisticRegressionModel doesnt use threshold to make predictions while spark LogisticRegressionModel does this error will be thrown when we try to run prediction on a spark deserialized LogisticRegressionModel I saw that LogisticRegressionOp from mleapruntime does set a default value for threshold if it is not found can we do the same thing in mleapspark to set a default value for threshold Im trying to deserialize the airbnb sample model file and getting an error scala val bundlePath jarfiletmpmodelsairbnbmodellrzip bundlePath String jarfiletmpmodelsairbnbmodellrzip scala val zipBundle forbundle managedBundleFilebundlePath yield bundleloadMleapBundleget optget javautilNoSuchElementException Noneget at scalaNonegetOptionscala at scalaNonegetOptionscala elided Any pointers 