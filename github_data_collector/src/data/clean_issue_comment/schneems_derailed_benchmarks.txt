Turns out that reducing a whole bunch of numbers to a single value average for example or median means that were getting rid of a huge amount of information One way to add context back in without drowning users in raw data is to include a histogram in the output With histograms in the output the user can see the distributions of their two runs to make better informed decisions about the validity of the data Heres an example histogram Histogram Frequency Heres how the output looks in a test run NOT Statistically Significant e d Merge pull request from kamipogroupbywithorderbyvirtualcountattribute seconds SLOWER by x oldernewer older newer older f aece Remove duplicated attribute alias resolution in select seconds Iterations per sample Samples Test type Kolmogorov Smirnov Confidence level Is significant max critical false D critical D max Histogram e d Merge pull request from kamipogroupbywithorderbyvirtualcountattribute Frequency Histogram f aece Remove duplicated attribute alias resolution in select Frequency Output tmpcomparebranches resultstxt Note its not that interesting with only two samples Caveats The histograms that are presented do not have identicalcorrect axis So while the shape of the histograms are correct in comparison to each other you cannot place them side by side for an accurate representation because they have different bin sizes and start from a different value In the short term I do think that the visual data being present with this caveat is better than nothing In the long term support would need to be added to unicodeplot to support this behavior Blockers There is a bug that raises an error when values are given to generate a histogram if the values have some special undetermined relationship This is tracked here Need a parser that can accommodate this html I think this happens when im running a benchmark and my computer goes to sleep and then I wake it up again but im not sure I guess if thats the case im not sure if we should abort or plow through The goal of perflibrary was to be able to provide a pretty solid single point where benchmarks could be run and a single trustworthy output could be generated Ive made progress on that goal but were not quite there yet For example I ran the same test different times last night and heres what I saw on code triage env SHASTOTEST aab c bd DERAILEDPATHTOLIBRARY bundle exec derailed exec perflibrary Executed times gave me NOT Statistically Significant aab Fstring gem seconds SLOWER by x oldernewer older newer older c bd Merge pull request from codetriageschneemsupsertdocmethods seconds Iterations per sample Samples Test type Kolmogorov Smirnov Is significant max critical false D critical D max NOT Statistically Significant aab Fstring gem seconds FASTER by x oldernewer older newer older c bd Merge pull request from codetriageschneemsupsertdocmethods seconds Iterations per sample Samples Test type Kolmogorov Smirnov Is significant max critical false D critical D max NOT Statistically Significant aab Fstring gem seconds SLOWER by x oldernewer older newer older c bd Merge pull request from codetriageschneemsupsertdocmethods seconds Iterations per sample Samples Test type Kolmogorov Smirnov Is significant max critical false D critical D max NOT Statistically Significant aab Fstring gem seconds FASTER by x oldernewer older newer older c bd Merge pull request from codetriageschneemsupsertdocmethods seconds Iterations per sample Samples Test type Kolmogorov Smirnov Is significant max critical false D critical D max Statistically Significant aab Fstring gem seconds FASTER by x oldernewer older newer older c bd Merge pull request from codetriageschneemsupsertdocmethods seconds Iterations per sample Samples Test type Kolmogorov Smirnov Is significant max critical true D critical D max NOT Statistically Significant aab Fstring gem seconds FASTER by x oldernewer older newer older c bd Merge pull request from codetriageschneemsupsertdocmethods seconds Iterations per sample Samples Test type Kolmogorov Smirnov Is significant max critical false D critical D max NOT Statistically Significant aab Fstring gem seconds FASTER by x oldernewer older newer older c bd Merge pull request from codetriageschneemsupsertdocmethods seconds Iterations per sample Samples Test type Kolmogorov Smirnov Is significant max critical false D critical D max NOT Statistically Significant aab Fstring gem seconds SLOWER by x oldernewer older newer older c bd Merge pull request from codetriageschneemsupsertdocmethods seconds Iterations per sample Samples Test type Kolmogorov Smirnov Is significant max critical false D critical D max Statistically Significant aab Fstring gem seconds FASTER by x oldernewer older newer older c bd Merge pull request from codetriageschneemsupsertdocmethods seconds Iterations per sample Samples Test type Kolmogorov Smirnov Is significant max critical true D critical D max Statistically Significant aab Fstring gem seconds FASTER by x oldernewer older newer older c bd Merge pull request from codetriageschneemsupsertdocmethods seconds Iterations per sample Samples Test type Kolmogorov Smirnov Is significant max critical true D critical D max Results of runs not significant Faster Slower significant Faster Slower None So these results arent too bad We are consistent in that it seems the runs that say statistically significant are all pointing in the right direction saying the code got faster Based off of this set you could say if its not significant then discard the results and you would end up with tests showing the same thing which is good However heres an older case that I had with the exact same code commits but it was showing statistically significant and the results did not agree with the other tests Statistically Significant aab Fstring gem seconds SLOWER by x oldernewer older newer older c bd Merge pull request from codetriageschneemsupsertdocmethods seconds Iterations per sample Samples Test type Kolmogorov Smirnov Is significant max critical true D critical D max Yikes in the prior runs even the statistically significant results are saying that its only around a difference while this is claiming things got slower This was before I switched from average to median Doing this does make the results not quite so dramatic and slower becomes slower But the problem still persists This is very far from the consensus picture that running the tests times shows I think its statistically significant because it is definitely two different curves however the outliers on the new data set are massive here is a comparison of tail numbers Old New Sorted values these are not the actual comparison runs here is raw data While we could run this perflibrary test multiple times to see if results agree I want to be able to only have to run this task once and get a fairly confident or My question is essentially Could we massage all of this data in a way so that the result is discredited or known to be suspicious Or flagged somehow One thing that stood out to me here is that old has a variance of while new has a variance of While its okay to say that newer is slower we cannot conclusively say that it is slower which is a lot Heres the good result set to compare to with the bad data also annotated Heres a spreadsheet showing distribution of the bad data here Hey schneems I was wondering if you would be interested in adding a dependency to bundlerleak and using it to check for known leaky dependencies at load time That way we could use that tool to save people time They might be using a dependency which is known to be leaky If you are interested bronzdoc or me could send a PR to address this idea Thanks Any exec task that that I run throws this error Using ruby on a Mac bundle exec derailed exec help Usersstephenturleyrvmgemsruby gemsderailedbenchmarks binderailed in require Usersstephenturleyrvmgemsruby gemsderailedbenchmarks libderailedbenchmarkstasksrb syntax error unexpected keywordensure expecting keywordend SyntaxError ensure Usersstephenturleyrvmgemsruby gemsderailedbenchmarks libderailedbenchmarkstasksrb syntax error unexpected keywordend expecting endofinput from Usersstephenturleyrvmgemsruby gemsderailedbenchmarks binderailed in exec from Usersstephenturleyrvmgemsruby gemsthor libthorcommandrb in run from Usersstephenturleyrvmgemsruby gemsthor libthorinvocationrb in invokecommand from Usersstephenturleyrvmgemsruby gemsthor libthorrb in dispatch from Usersstephenturleyrvmgemsruby gemsthor libthorbaserb in start from Usersstephenturleyrvmgemsruby gemsderailedbenchmarks binderailed in top required from Usersstephenturleyrvmgemsruby binderailed in load from Usersstephenturleyrvmgemsruby binderailed in main from Usersstephenturleyrvmgemsruby binrubyexecutablehooks in eval from Usersstephenturleyrvmgemsruby binrubyexecutablehooks in main It looks like this code causes the require to not get fired in our core extension That is applied to Object You can see a failure in on The high level require does not get triggered Even if I replace my core extension with this it does not fire ruby module Kernel modulefunction rubocopdisable StyleModuleFunction aliasmethodrequirewithoutderailed require def requirepath raise require end aliasmethodrequirerelativewithoutderailed requirerelative def requirerelativepath raise requirerelative end aliasmethodloadwithoutderailed load def loadpath wrap false raise load end end class Module aliasmethodautoloadwithoutderailed autoload def autoloadconst path raise autoload end end No idea how bootsnap is working around this in Either theyre not and are silently getting this error or theyve mitigated it somewhere The requirements on this project say Ruby but when trying to install this on our Ruby system we are getting the following error GemRuntimeRequirementNotMetError heapy requires Ruby version The current ruby version is An error occurred while installing heapy and Bundler cannot continue Make sure that gem install heapy v source succeeds before bundling In Gemfile derailedbenchmarks was resolved to which depends on heapy Is there a work around for this Or Does the documentation need to be updated Is there a way currently to boot a Rails app but not hit any endpoints instead opting for running specific code paths like a Sidekiq background job derailedbenchmarks detects railsrack application by railtie gem It isnt completely good option because some gem which can be used in rack application require railtie For example standalonemigrations gem requires railtie So whenever it is used in rack nonRails app then rails setup is invoked instead of rack setup I think it will be good to prepare a mechanism which will force in this moment rack setup somehow