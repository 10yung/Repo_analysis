 As discussed in the JIRA ticket revisited version of HDFS persistence of MQTT messages which should ease recovery in case of device failure The design discussion may be continued in JIRA but I decided to open WIP PR for yanlinLynn to review and comment JIRA ticket Sample output mergeprsh Commits to be merged b bd BAHIR Corner fix f BAHIR Important feature Maybe ask contributor to squash commits Continue as is yn y Close pull request yn y Commits pushed to originmaster Approver JIRA user root Approver JIRA password Specify assignee of JIRA ticket BAHIR current root root other Chosen option Enter JIRA account name lukasz Specify fixed release version Chosen option JIRA ticket successfully updated If you comment out lines pushing changes to remote repository line cleanup of working directory line and two REST requests updating JIRA ticket lines you can perform a dry run to make sure the script works well Comments welcome Improvement ideas This component implements Hadoop File System orgapachehadoopfsFileSystem to provide an alternate mechanism instead of using webhdfs or swebhdfs file uri for Spark to access readwrite files fromto a remote Hadoop cluster using webhdfs protocol This component takes care of the following requirements related to accessing files readwrite fromto a remote enterprise Hadoop cluster from a remote Spark cluster Support for Apache Knox Support for passing user idpassword different from the user who has started the sparkshellsparksubmit process Support for SSL in three modes Ignoring certificate validation certificate validation through user supplied trust store path and password and automatic creation of certificate using openssl and keytool Optimized way of getting data from remote HDFS where each connection will get only its part of data This component is not a full fledged implementation of Hadoop File System It implements only those interfaces those are needed by Spark for reading data form remote HDFS and writing back the data to remote HDFS Example Usage Step Set Hadoop configuration to define a custom uri of your choice and specify the class name BahirWebHdfsFileSystem For example schadoopConfigurationsetfsremoteHdfsimplorgapachebahirdatasourcewebhdfsBahirWebHdfsFileSystem You can use any name apart form the standard uris like hdfs webhdfs file etc already used by Spark instead of remoteHdfs However subsequently while loading the file or writing a file the same should be used Step Set the user name and password as below val userid biadmin val password password val userCred userid password schadoopConfigurationsetusrCredStruserCred Step Now you are ready to load any file from the remote Hadoop cluster using Sparks standard DataframeDataSet APIs For example val filePath biginsightssparkenablementdatasetsNewYorkCity Service ServiceRequestsfrom toPresentcsv val srvr ehaasp mastermanagerbiservicesbluemixnet gatewaydefaultwebhdfsv val knoxPath gatewaydefault val webhdfsPath webhdfsv val prtcl remoteHdfs val fullPath sprtclsrvrknoxPathwebhdfsPathfilePath val df sparkreadformatcsvoptionheader trueloadfullPath Please not the use of gatewaydefault and webhdfsv used for specifying the server specific information in the path The first one is specific to Apache Knox and the second one is specific for webhdfs protocol Step To write data back to remote HDFS following steps can be used using standard Dataframe writer of spark val filePathWrite biginsightssparkenablementdatasetsNewYorkCity ServiceResultcsv val srvr ehaasp mastermanagerbiservicesbluemixnet val knoxPath gatewaydefault val webhdfsPath webhdfsv val prtcl remoteHdfs val fullPath sprtclsrvrknoxPathwebhdfsPathfilePathWrite dfwriteformatcsvoptionheader truesavefilePathw We are still working on followings Unit Testing Code cleanup Examples showcasing various configuration parameters API documentation 