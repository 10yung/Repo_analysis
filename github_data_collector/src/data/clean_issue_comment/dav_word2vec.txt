The if statement if word continue will never evaluate to true because when filling the sen array words with id are already being discarded by this check a few lines above I suggest to remove the ifstatement because it is misleading and if it every evaluated to true it would do so in the next iteration again because sentenceposition would not change and the program would be in an infinite loop I thought this was the Macsavvy version of the code but its failing right out of the box for me on the first allocation around line of word vecc because the conditional compilation doesnt include a Mac case ifdef MSCVER syn alignedmalloclong longvocabsize layer size sizeofreal elif defined linux a posixmemalignvoid syn long longvocabsize layer size sizeofreal endif Neither MSCVER nor linux is defined so we simply dont allocate anything Just wondering if I have missed some obvious solution or whether I need to add a Mac case or maybe a generic case simply using calloc to each such block I have my own training set of k wordswritten in Devanagari scriptIndic language how should I approach Do I need to change the file path in demowordsh or help me with the stepwise execution Memory leaks detected Im running program word vec with command line like word vec train questionswordstxt output outtxt ERROR LeakSanitizer detected memory leaks Direct leak of bytes in objects allocated from x fdddc f in malloc usrlibx linuxgnulibasanso x x ff in TrainModel homemfcfuzznewprogramword vecsrcword vecc SUMMARY AddressSanitizer bytes leaked in allocations In the skipgram part when computing propagate hidden output use this code for c c layer size c f syn c l syn c l while l l lastword layer size l vocab word point d layer size which means the syn is input word syn is output word the codes show syn is the target word syn is contexttarget word The skipgram is using w to predict contextw but this code is use contextw to predit w is that right If a readvocab file is specified then all their counts are it seems SortVocab will remove all words in such case Hi Im working on a paper after reading the word vecc code it looks like the CBOW gradient of loss is calculated in f expTable intf MAXEXP EXPTABLESIZE MAXEXP g vocab word code d f alpha g is the gradient multiplied by the learning rate I want to be able to get the loss of an input and not the gradient of it is there anyway to get that is there any function that does so thanks for sharing Could anyone please point me to where the embedding matrix is initialized in the code I would like to know how it is initialized If random what random distribution are weights sampled from Thank you Hi Were using word vec for hypernymy discovering In order to design a more efficient version of word vec we need to know what is exactly the semantics of the variable c in function ReadVocab within the file word vecc Thanks in advance void ReadVocab long long a i char c char word MAXSTRING FILE fin fopenreadvocabfile rb if fin NULL printfVocabulary file not found n exit for a a vocabhashsize a vocabhash a vocabsize while ReadWordword fin if feoffin break a AddWordToVocabword fscanffin lldc vocab a cn c semantics of c i SortVocab if debugmode printfVocab size lld n vocabsize printfWords in train file lld n trainwords fin fopentrainfile rb if fin NULL printfERROR training data file not found n exit fseekfin SEEKEND filesize ftellfin fclosefin Hi Im using word vec for translations between languages I know if youre using the same vector it is possible to calculate a similarity score between words but is the same true between words on different vectors Thanks in advance I love the package