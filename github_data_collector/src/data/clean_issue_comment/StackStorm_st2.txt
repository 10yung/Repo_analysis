Despite Ubuntus popularity Debian still remains present in production environments It would be useful to support this platform to expand the potential StackStorm user base by having native packaging and installation process for this distribution The current stable version is Debian Buster which provides Python only no Python packages are provided by Debian Since Debian and Ubuntu are very similar I was able to build docker images and build system packages for StackStorm See the links below for details Build package docker images Build system packages For the current experience of running StackStorm dev on Debian Buster a large portion of the code base functions out of the box There is an issue with rule processing which prevents StackStorm from being using in production on Buster This feature request is to officially integrate Debian Buster support into the packaging process and test against Python and as part of the commit process to StackStorm Without this change I could not successfully run make requirements or make lint on python Something is pulling in moreitertools but the metadata for moreitertools is confusing pip so it is selecting a version that is python only when it should be installing moreitertools or less on python This takes the shotgun approach to fixing that It includes the dep for all st packages If anyone cares to evaluate the tree of deps to only include this where needed be my guest Note that this includes a fix for the fixaterequirements script to allow including the pythonversion marker Hello Stackstorm team Were noticing an issue with st pack install filePWD command with HTTPS endpoints For eg if we set the below environment variables and try to install a pack it works without any issues export ST APIURL export ST STREAMURL export ST AUTHURL export ST BASEURL But when we try to install a pack with the below environment variables wherein we route every request via Nginx reverse proxyload balancer the st client fails to show the progress export ST BASEURLhttps export ST AUTHURLhttps auth export ST APIURLhttps api export ST STREAMURLhttps stream export ST CACERTetcsslst st crt Basically the pack install will show the progress like but the client wont exit and it would simply hang forever But the pack install action chain would have really executed and completed without any issues Appreciate if any pointers to resolved this issue Regards Harish This is an example for an Orquesta workflow using the coreask pack to query multiple parameters I will also create a PR in the st docs repository to include this into the documentation Summary If a local pack has the same ref from a remote pack the remote pack repository URL is set to the local pack meta data in the interface ST version ST on Python Environment Docker Installed via Docker Compose Steps to reproduce the problem Create a local pack that has the same name from a remote pack eg elasticsearch create the container and open the interface pack see the meta data Expected Results A local pack should not get the metadata from a remote pack that has the same name This gives the impression that the person set in the local pack email name etc is the mantainer from the remote pack Actual results The local pack has the repository URL from a remote pack The packyaml ref elasticsearch name Elasticsearch description Elasticsearch CRUD version author X email Y The interface I have edited the HTML to not show the sensitive information name email Hey all wanted to get this change in front of some eyeballs here to get some feedback Weve punkrokk had the need in many of our stackstorm packs to configure them such that they can be run against multiple instances or tenants of the serviceAPI theyre built against Our sensors have typically be written to iterate over all of the tenant configurations in our pack configs but actions tend to be run against only one tenant and thus take a tenant name as a parameter in the action at runtime Typically our pack config schema looks similar to this yaml service type object required true patternProperties w ref propertiescredentials additionalProperties false credentials type object properties username type string password type string secret true Which results in a config file like yaml service instanceone username st kvserviceinstanceoneusername password st kvserviceinstanceonepassword decryptkv instancetwo username password Weve been taking credential names or tenant names as parameters into our action so that if an action should be run against instanceone the parameter passed into the action would be tenantinstanceone This has however resulted in a lot of reused boilerplate across all of our packs that support this kind of multitenancy for fetching all tenants ensuring the provided tenant exists and constructing the client object in our lib code for the requested tenant We wanted to provide a simpler way to fetch entire objects of configuration details from the pack config so we started first looking at the PackConfigDict The simple implementation here was to just run a YAQL expression against the pack config to fetch the instance configuration if present This seemed pretty intuitive to us since the query string could be parameterized to take in the instance name since we have control over the pack schema against which the query is run were not worried about things moving around without us knowing and wed fetch the entire config object in a couple of lines plus some error handling and send the config parameters to our client constructor in far less duplicated code across packs than before We wanted to get this up to get some feedback on it to see if were onbase with where a change like this should exist if its enough of a change or if there would be some consideration for changing pack schema to support the idea of a tenant object more natively We posted some discussion about this in the community slack but wanted to get the change more formalized here We also have a variant on this that avoids the performance overhead of YAQL but is a very naive solution the idea could be fleshed out to be a more proper search on the dictionary for a key without a YAQL query though see Background Details There is a known issue with some mongoengine operations especially retrievals and to lesser extend also insertions being very slow when working with large documents The primary reason for that is massive overhead mongoengine adds when converting native pymongo representation to the document class instance and vice versa There are multiple approaches on how to tackle that including switching to something like pymodm which is much more efficient than mongoengine Most of those libraries claim very similar and compatible API to the mongoengine but this doesnt help us at all Our DB later abstraction is based heavily around mongoengine and we utilize a lot of low level mongoengine functionality and primitives and also extend it in various places This means that switching the ORM library we use is pretty much impossible at the moment It would simply require too much time and resources and it would likely also result in a lot of hard to catch edge case bugs Proposed Solution While profiling and measuring hot code paths and trying to see which places we could optimize switch directly to pymongo and avoid mongoengine layer all together I found out that calling aspymongo on mongoengine QuerySet object results in massive performance improvements The nice thing is that its much easier to make that change compatible with out existing code and DB layer compared to fully switching an ORM layer which would require rewriting very big chunks of the code The reason for that is that this method avoids very expensive mongoengine conversion and dereferrencing layer which we dont need anyway It simply returns raw pymongo result aka dictionary In this proposed implementation I updated our get and query method to utilize aspymongo method and manually convert those raw dictionaries to our database model class instances It turns out that approach is much more efficient As part of that changes I also need to update various code to utilize query in a manner that it expects that method to return actual DB model class instances and not a QuerySet object In fact we already did that for a lot of the code in the past If you check of the code which calls query already expects it to return actual DB model objects and not a query set so the change shouldnt be that big In places where there is not possible and where we need access to raw QuerySet there should be very few of those we can use rawquery method directly Keep in mind that this approach is still not ideal In ideal world we would avoid instantiating DB class all together and work directly with raw pymongo dictionaries at least in critical code paths but that change would be much harder and more involved Profiling Data Here is some profiling data when retrieving a single large object with and without utilizing aspymongo bash Retrieve large execution with aspymongoFalse percentile s percentile s percentile s percentile s Retrieve large execution with aspymongoTrue percentile s percentile s percentile s percentile s As you can see the difference is substantial aka aspymongo manual model object instantiation around faster TODO Overall Im quite optimistic I can get all that change finished and call the tests passing compared to many other much more involved and complex approach I already have a huge chunk of tests passing locally Update affected code and make sure all the tests pass This is just a quick experiment to see if we could use a more efficient fork of mongoengine which reduces a lot of conversion overhead which is involved in regular mongoengine which results in very slow operations with large documents The problem is that even though a lot of similar projects claim almost full mongoengine API compatibility that doesnt really help us much since our DB abstraction is based heavily around mongoengine and also utilizes various low level APIs and functionality In addition to that this fork is also based around older version of mongoengine So this means there will likely be no quick and easy wins for us If that doesnt work out probably the most reasonable approach is to try to incrementally optimize hot code paths especially the ones which workflow results are saved and retrieved one by one and use pymongo directly in those code paths SUMMARY Unable to connect Stackstorm to Mongo DB over SSL Stackstorm cant function correctly when connection to MongoDB is encrypted by SSL There is exception of aximum recursion depth exceeded while calling a Python object in st apilog and st authlog In this conditions stackstorm is in non functional state at all STACKSTORM VERSION st on Python OS environment install method OS Both the stackstorm and moongodb on Ubuntu LTS but installed in diffrent VMs in the same subnet All Stackstorm components are installed on one VM but Mongo DB RAbitMQ on the second VM Steps to reproduce the problem Without SSL all works fine but after moving a connection to SSL Stackstorm cant connect to Mongo Put ssltrue in st conf Other parameters related to ssl dont really matter Tested in all combinations of these parameters with and without client certificate authentication and so on mongo client can connect to monog over ssl without any problem Simple python script using mongoengine module can connect to mongo over ssl So it seems as not a problem of underlying infrastructure because all other tools and components can connect to mongo wih SSL but staskstorm cant Expected Results Staskstoem should work with mongo over SSL That a basic requirements Actual Results Stackstorm starting up but in the st api log and st authlog we can see every few seconds error INFO unknown file Connecting to database somedatabase mongodbhost as user someuser WARNING unknown file Retry on ConnectionError Cannot connect to database default maximum recursion depth exceeded while calling a Python object it seems like some bug in initpy file in dbconnect function in line number where the connection attempt goes to endless recursion or something likes this ISSUE TYPE Bug Report STACKSTORM VERSION st dev e on Python OS ENVIRONMENT INSTALL METHOD stackstormha helm chart SUMMARY When trying to filter all the actions based on the tags field an empty list is returned For example with the following action definition yaml name restart runnertype localshellscript description Restart Gameworld enabled true entrypoint webistranopy tags name portalvisible value yes parameters gameworld type string description Enter Gameworld required true position I get this response GET HTTP OK Server nginx Date Tue Dec GMT ContentType applicationjson ContentLength Connection keepalive XTotalCount Response code OK Time ms Content length bytes 