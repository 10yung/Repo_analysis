Short Version I was expecting the data structure used in im to be provide a large performance boost when merging to similar HashMap coming from the same ancestors HashMap It does not I wonder is there are low hanging fruit to perform such optimisation with the data structure used Long Version We are writing Rust for the Mercurial DVCS We started investigate the use of IM for an algorithm that travel a DAG agregating information along the way For the record The information is copiesrenames data They exist as a sourcedestination mapping Not all DAG nodes carries information and the information is usually but not always small The agregation of these small piece can become larges or we sometime encounter node with a lot of renames The DAG can contains a lots of branchpoint merge point up to merge point for some users Each time there is a branchpoint we basically have to duplicated the map to update it independently and each time there is a merge we have to merge the two maps together again one of the side has priority In many case the two maps are initially copied from the same one and only have a small faction of changes or None at all on each side This looks like a job for persistent data structure Switching to rust im had gave the intended result in regards with copies Branch point is mostly cheap now However the extendunions call does not benefit from the data structure It seems like the implementation is mostly calling insert in a loop Do you think there would be a reasonably easy way to leverage the underlying data structure to quickly recognise similar data section during unionsextends Examples Here is a basic rust source showing a simplied example of the pattern we have to deal with mainrstxt And here is the rust code we used to try this out on real data In particular provided implementations of len count nth and nthback for Iter IterMut Chunks ChunksMut that make use of frontindex and backindex to improve performance Similar work might also be useful for ConsumingIter but I couldnt easily trace that code to see whereif such improvements would make sense Hi I have a hopefully small feature request Could OrdSetT expose a method findlesser that takes a T and returns a reference to the greatest value in the set that is less than or equal to T or None if no such element exists Analogously findgreater returns least value that is greater or equal to the argument and then the same for OrdMap operating on the keys From a quick look at the code it seems like the lookup implementation could support this but it would take me a while to fully understand what exactly is going on in order to implement this myself It might be interesting to have some benchmark results in readme comparing im imrc and stdcollections to get a rough estimate of the overhead of immutability and atomic reference counting Context I actually want to know about rc vs Arc specifically and though that im might be the ideal testcase Hello From the docs in O means amortised O which means that an operation usually runs in constant time but will occasionally be more expensive for instance Vectorpushback if called in sequence will be O most of the time but every th time it will be Olog n as it fills up its tail chunk and needs to insert it into the tree Please note that the O with the asterisk attached is not a common notation its just a convention Ive used in these docs to save myself from having to type amortised everywhere This is AFAIK wrong Amortized means that while one operation may be higher than the advertised complexity n operations in a sequence must be bounded by n the advertised complexity For example the classical stdvectorVec has O amortized push while a single push can take On to reallocate and grow to twice the size it happens on every n pushes so it sums to something like the trick there is that the bigger the reallocations the rarer they happen On the other hand Olog n every constant number of elements is still amortized to Olog n In the standard library the only collection that allocates when you call new is VecDeque which will allocate space for elements By comparison the collections in this library allocate a lot more Using an element type of u for testing HashSet and HashMap will allocate a small amount for the hasher which is typically a size or Copy type and bytes of space OrdSet allocates bytes Vector allocates bytes Im not sure if this is easily fixable without a performance penalty but it would be nice to not allocate at all I havent seen too much prior art in the persistent sphere but finger trees seem to be the way they go for it in haskell Loving your work here so far btw thanks ever so much Hi First of all this is amazing work I was long waiting for someone to go ahead and do this The code looks very good and you also seem to be very fast in making progress smile I myself have been doing some work on bringing Clojurelike datastructures to system languages but in the ugly land of C which resulted in the Immer library and this paper One of the things that took most effort there and brought most complexity is to try achieve the most cache locality possible In particular vector elements are not boxed individually but live in the same array block Studying your code it seems to me that you are boxing the elements individually and it seems to me that it would be impossible to do otherwise without adding ridiculous amounts of unsafe code Do you have some thoughts on that Also do you plan to implement catenable vectors Relaxed Radix Balanced trees Once again congrats I would be interested also in trying to do some benchmarking between the libraries Let me know eventually if you would like to work together on that to try to find useful and fair benchmarks Cheers This and this is what I mean Maybe also performance comparisons of data structures with stdcollections equivalents If youre interested Ill write something up in the next couple of days 