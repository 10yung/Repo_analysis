Would it be possible to add a notice at the beginning of the readme to warn that this datasource is merged into spark and all users of spark should not use it When I have a schema which has a reference to itself it causes an infinite recursion and thus a StackOverflowError SchemaConverters should have some sort of bailout if it reads a class more than once We are using Spark in our clusters and want to use this library to read avro files As part of reading the avro files we want to able to supply a different read schema so that we can handle schema evolutions The version suggested for Spark version does not support this Is there a version which works with Spark and also supports this feature Ive created a PR to support few logical types and was very surprised that Java is still being targeted by the tests I think it is time leave Java legacy behind For databricks with spark While Reading a Complex AVRO with below Code SparkConf conf new SparkConfsetAppNameSearchAVROsetMasterlocal JavaSparkContext sc new JavaSparkContextconf SQLContext sqlContext new SQLContextsc Creates a DataFrame from a specified file DataFrame df sqlContextreadformatcomdatabrickssparkavroloadinputPath We get an exception Exception in thread main javalangUnsupportedOperationException This mix of union types is not supported see README ArrayBufferRECORD at comdatabrickssparkavroSchemaConverterstoSqlTypeSchemaConvertersscala at comdatabrickssparkavroSchemaConverterstoSqlTypeSchemaConvertersscala at comdatabrickssparkavroSchemaConverterstoSqlTypeSchemaConvertersscala at comdatabrickssparkavroSchemaConvertersanonfun applySchemaConvertersscala Similar code works properly for databricks with Spark Can you please add support for the same in databricks When I read simple avro file with all fileds nonnullable resulted dataframe schema has all fields nullable avro file schema type record name RobotDetection namespace czsearchrobotdetection fields name sessionId type string name robotDetectionResult type int dataframe schema val a sparkreadformatcomdatabrickssparkavroloadavrofileavro aschema res orgapachesparksqltypesStructType StructTypeStructFieldsessionIdStringTypetrue StructFieldrobotDetectionResultIntegerTypetrue I was taking a quick look through DefaultSourcescala and it seems there is only support for snappy and deflate I believe Avro has support for xz and bzip as well Hello Caused by orgapacheavroAvroRuntimeException Unknown datum type LjavalangObject LjavalangObject f b at orgapacheavrogenericGenericDatagetSchemaNameGenericDatajava at orgapacheavrospecificSpecificDatagetSchemaNameSpecificDatajava at orgapacheavrogenericGenericDataresolveUnionGenericDatajava at orgapacheavrogenericGenericDatumWriterresolveUnionGenericDatumWriterjava at orgapacheavrogenericGenericDatumWriterwriteGenericDatumWriterjava at orgapacheavrogenericGenericDatumWriterwriteFieldGenericDatumWriterjava at orgapacheavrogenericGenericDatumWriterwriteRecordGenericDatumWriterjava at orgapacheavrogenericGenericDatumWriterwriteGenericDatumWriterjava at orgapacheavrogenericGenericDatumWriterwriteGenericDatumWriterjava The above exception is triggered in the following scenario case class Modelparams Option List String The above case class generates the following schema typerecordnameModelnamespaceTestfields nameparamstype typearrayitems stringnull null Now when I create my converterToAvro val structType StructType Encodersproduct Model schema val converter createConverterToAvrostructType recordName recordNamespace and try to generate my genericRecord val record GenericRecord converteritemasInstanceOf GenericRecord I get the above exception This happens because in the implementation of AvroOutputWritercreateConverterToAvro in the case ArrayType we have the following val targetArray new Array Any sourceArraySize and GenericDatagetSchemaName does this check if isArraydatum return TypeARRAYgetName protected boolean isArrayObject datum return datum instanceof Collection Now scalaArray is not an instance of Collection and it will fail gracefully In order to fix this we can use javautilArrayList This is not an issue It is a request to improve README documentation I am not sure where do I put it sparkcsv Features says API accepts several options A list of accepted options is followed with brief description This is very intuitive to understand supported options Can we please have such a list for sparkavro as well I know it is mentioned in README but it looks scattered and is not like a list This pr saves on string conversions by explicitly making the generic datum reader read strings instead of utf Also the converter copies every object from the record read by the reader but the iterator never actually reused any record instance Now record is reused per iterator for efficient memory reuse