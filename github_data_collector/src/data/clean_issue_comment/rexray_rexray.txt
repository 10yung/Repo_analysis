 Summary As I dont see any activity since January on this repository Im wandering if the project is still maintained My use case is the integration with opensource DCOS external persistent storage Will new features be added Will new bugs be fixed The question is mainly for the the vmware RexRay project maintainers codenrhoden akutz clintkitson Since a few weeks the Website Rexrayio wont load any more Summary In our production environment we connect several CoreOS instances to each other with one of the scripts installing rexrays fs as a docker plugin to enable accessing several AWS S buckets This functions exactly as desired most of the time but sometimes randomly stops working Once we find that one of the buckets does not mount properly anymore and we mount one of the other ones which hasnt been used so far docker container run it rm v s backupbucketmnts busybox we get journalctl logging detailing a successful mount but showing a failure upon container exiting due to failure to dismount As expected succeeding mounts fail This leads to ultimately all s mounts becoming unreachable for that particular worker node Dis Enabling the plugin does not solve the issue removing reinstalling the plugin fixes each bucket for a single time showing failure during dismounting on exit again Rebooting the docker daemon which is a nogo for production environments fixes the issue until at some point it fails again The above scenario is detailed demonstrated in the steps to reproduce Since usually we find the plugin broken through thresholds of minimum number of backups being broken by the time we check the logs have been rotated I can not find the exact moment of the plugin breaking Version shell coreprod worker uname a Linux prod worker coreos SMP Mon Apr x IntelR XeonR Platinum M CPU GHz GenuineIntel GNULinux coreprod worker docker info Server Version ce Kernel Version coreos Operating System Container Linux by CoreOS Rhyolite coreprod worker docker plugin inspect rexrays fs plugin id PluginReference dockeriorexrays fs In short AWS EC instances with CoreOS docker ce rexrays fs Expected Behavior Not to break andor to restore itself in time Actual Behavior Stays broken requires a docker daemon restart for a proper fix meaning services that do not use S are affected as well Steps To Reproduce Im still working on a from working properly to broken guide which Ill add if I find it but once broken I can rebreak it through Cancelkillstoprm any job involving S usage docker volume ls should now show the S buckets that fail to mount DRIVER VOLUME NAME local ad d cdc d ee f f a c d d aba ffb be f eb local d e bdf cc f d c b d b fd af fc ac c bffd c aa local a fb bf d c d f b eed e ce a d cfecb cbceb ada s fslatest remotes bucket s fslatest remotes bucket s fslatest remotes bucket docker plugin disable f s fs docker plugin rm f s fs f are required due to I think as Ive killed literally every job that had something to do with S Without I get blocked by still in use docker volume ls should now no longer show the S buckets shell docker plugin install disable grantallpermissions rexrays fs alias s fs REXRAYLOGLEVELdebug S FSACCESSKEYaccesskey S FSSECRETKEYsecretkey S FSREGIONeuwest LINUXVOLUMEFILEMODE S FSOPTIONSallowotheriamroleautoumask docker plugin enable timeout s fs Logs docker volume ls should now show the present S buckets again DRIVER VOLUME NAME local ad d cdc d ee f f a c d d aba ffb be f eb local d e bdf cc f d c b d b fd af fc ac c bffd c aa local a fb bf d c d f b eed e ce a d cfecb cbceb ada s fslatest remotes bucket s fslatest remotes bucket s fslatest remotes bucket Logs docker container run it rm v remotes bucket s fs busybox should now work sh coreprod worker varlibdocker docker container run it rm v remotes bucket s fs busybox Logs Inside the container exit Logs docker container run it rm v remotes bucket s fs busybox should now be broken again sh coreprod worker varlibdocker docker container run it rm v remotes bucket s fs busybox runtorcxbindocker Error response from daemon error while mounting volume VolumeDriverMount dockerlegacy Mount remotes bucket failed error mounting s fs bucket Logs Configuration Files docker plugin inspect s fslatest json Config Args Description Name Settable null Value null Description REXRay FUSE Driver for Amazon Simple Storage Service S FS DockerVersion ce Documentation Entrypoint rexraysh rexray start f nopid Env Description Name REXRAYLOGLEVEL Settable value Value warn Description Name LIBSTORAGEINTEGRATIONVOLUMEOPERATIONSMOUNTROOTPATH Settable value Value Description Name LINUXVOLUMEROOTPATH Settable value Value Description Name LINUXVOLUMEFILEMODE Settable value Value Description Name HTTPPROXY Settable value Value Description Name S FSENDPOINT Settable value Value Description Name S FSACCESSKEY Settable value Value Description Name S FSDISABLEPATHSTYLE Settable value Value Description Name S FSMAXRETRIES Settable value Value Description Name S FSOPTIONS Settable value Value Description Name S FSREGION Settable value Value Description Name S FSSECRETKEY Settable value Value Description Name CSIENDPOINT Settable value Value Description The name of the CSI plugin used by the CSI module Name XCSIDRIVER Settable value Value Description A flag that disables the CSI to libStorage bridge Name XCSINATIVE Settable value Value false Interface Socket rexraysock Types dockervolumedriver IpcHost false Linux AllowAllDevices true Capabilities CAPSYSADMIN Devices null Mounts Description Destination dev Name Options rbind Settable null Source dev Type bind Network Type host PidHost false PropagatedMount varlibrexray User WorkDir rootfs diffids sha bbb bdd c b fe acab d ea b bd ae a be b f type layers Enabled true Id e a ff f f da e bc c af b fa f Name s fslatest PluginReference dockeriorexrays fs Settings Args Devices Env REXRAYLOGLEVELdebug LIBSTORAGEINTEGRATIONVOLUMEOPERATIONSMOUNTROOTPATH LINUXVOLUMEROOTPATH LINUXVOLUMEFILEMODE HTTPPROXY S FSENDPOINT S FSACCESSKEYaccess key S FSDISABLEPATHSTYLE S FSMAXRETRIES S FSOPTIONSallowotheriamroleautoumask S FSREGIONeuwest S FSSECRETKEYsecret key CSIENDPOINT XCSIDRIVER XCSINATIVEfalse Mounts Description Destination dev Name Options rbind Settable null Source dev Type bind Logs Instead of pasting the full logs here Ive opted to paste them as gistssnippets in between the steps to reproduce Let me know if youd still prefer a full log Summary Often after reboot of a docker host Rexray managed docker volume plugins will fail to automatically enable This results in disabled volume plugins containers unable to access those volumes and probably a cascade of catastrophic failures The cause I isolated was a default watchdog timer in Docker The fix is to change the length of that timer See below Bug Reports Version dockeriorexrayebs dockeriorexrays fs perhaps all of them Steps To Reproduce Use AWS However other hosts cloud services and your own private servers may also reproduce this issue Create three S buckets Create two EBS volumes Create an IAM role that is granted permissions to access the S and EBS resources Follow docs like Create and start a t nano instance with the Amazon Linux ECSoptimized AMI that uses the IAM role you created This should provide you docker SSH to that instance Install the Rexray EBS and S FS managed volume plugins Replace useast with your AWS region Change the s url and endpoint options if your region is not useast bash docker plugin install grantallpermissions rexrayebs REXRAYLOGLEVELdebug LIBSTORAGEINTEGRATIONVOLUMEOPERATIONSMOUNTROOTPATH LINUXVOLUMEROOTPATH LINUXVOLUMEFILEMODE EBSREGIONuseast docker plugin install grantallpermissions rexrays fs REXRAYLOGLEVELdebug LIBSTORAGEINTEGRATIONVOLUMEOPERATIONSMOUNTROOTPATH LINUXVOLUMEROOTPATH LINUXVOLUMEFILEMODE S FSREGIONuseast S FSOPTIONSallowotherumask noexecmpumask uid gid noatimeusepathrequeststyleiamroleautourl Verify both plugins report they installed Fix any issues before continuing to the next step Verify you can see the names of all your S and EBS volumes using docker volume ls Fix any issues before continuing to the next step Reboot your instance using something like sudo reboot now ssh to your instance View the status of volume plugins with docker plugin ls Actual Behavior After reboot some or all of the rexray docker volume plugins are disabled If they are all enabled keep rebootingsome will eventually be disabled varlogmessages or other logfiles if you didnt use the AMI described above you find entries like the following The most important entry for this scenario is the received signal shutting down entry Sep ip dockerd time T Z levelerror msgtime T Z levelinfo msg received signal shutting down signalterminated time pluginedbc aa bad f c d cfea eb e e b f af b dd f bfc Sep ip dockerd time T Z levelerror msgtime T Z levelinfo msg received exit signal signalterminated time pluginedbc aa bad f c d cfea eb e e b f af b dd f bfc Sep ip dockerd time T Z levelerror msgerror service startup failed agent mod init failed context canceled plugin bd c f c b fdb e d c c bd e d d ccc c Sep ip dockerd time T Z levelerror msgfailed to enable plugin errordial unix rundockerplugins bd c f c b fdb e d c c bd e d d ccc crexraysock connect no such file or directory id bd c f c b fdb e d c c bd e d d ccc c Expected Behavior After reboot all rexray docker volume plugins should be enabled Cause After rebooting the OS starts dockerd dockerd has code to also start any plugins that were previously enabled dockerd iterates through each of them and calls a function enablep v Plugin c controller force bool This is the code in Docker The code then calls pluginsNewClientWithTimeout with a default timeout of s This timeout is an overarching watchdog timeout that will kill the plugin if that timeout is exceeded during the NewClient startup This is mentioned in passing at The Rexray managed plugins like EBS and S do a lot of work during startup This is a significant difference from some other plugins that do very little work during startup and instead do all their work on attachmount During busy times on the instance like reboots the time needed for the services within the managed volume plugins to startup have a fully working network the API calls to EBSS to complete and results to be compiled might exceed this default s watchdog timer If that occurs then the watchdog calls shutdownPlugin which then uses SignalpluginID intunixSIGTERM to send a kill SIGTERM to the volume plugin While isolating this I saw in the logs entries where the S https APIs returned results that looked ok Yet I also saw the received signal entries which the caused the S managed plugin to shutdown Naturally this is because this watchdog timer naturally runs in parallel The S plugin was running correctly but it wasnt getting the result back to the watchdog code within the seconds Fix The easiest fix that worked for me is to make this watchdog timeout longer Of course this is a race condition and there are more robust solutions possible if the core Docker team were to make code changes This timeout can only be changed using the docker plugin enable command There is no possibility with Docker to use the docker plugin install command To change this watchdog timer from its default s to a longer timeout use the following This example is for the s fs plugin and changing it to s It applies also to other plugins and other timeouts might better meet your needs Notice the use of the disable and timeout options docker plugin install disable rexrays fs options docker plugin enable timeout rexrays fs Related issues Summary Travis CI always fails in job docker driverscaleio due to failure of apk add numactl This job error is in all PRs for the past months See for them They all fail on the same job docker driverscaleio with an error due to apk add numactl Expected Behavior No Travis error with apk add numactl Actual Behavior Step RUN echo etcapkrepositories apk update apk add numactl Running in b dd fetch fetch fetch v gda e v gf ba b d v g bd f e OK distinct packages available WARNING This apktools is OLD Some packages might not function properly WARNING This apktools is OLD Some packages might not function properly numactl missing ERROR unsatisfiable constraints required by world numactl The command binsh c echo etcapkrepositories apk update apk add numactl returned a nonzero code make dockerpluginsscaleiorootfsrexraysh Error The command make builddockerplugin exited with Steps To Reproduce Please list the steps to reproduce the issue in this section Submit a PR Wait for Travis to fail on the job docker driverscaleio In Makefile the parameters to find cause warningserrors of deprecated options and expression precedence This is due to an invalid use of the d parameter d is deprecated and d doesnt take a numeric value after it This two actions cause the warning and error seen I think the intention was to use the maxdepth parameter In addition there is the possibility of an error sometime in the future by using a wildcard in the following name It needs to be single quoted so the shell glob doesnt expand it Setup Ubuntu Docker is installed Go is not installed Repo clone the rexray repo change pwd into the root of the repo DRIVERebs make Result find warning the d option is deprecated please use depth instead because the latter is a POSIXcompliant feature find paths must precede expression Expected No warnings or errors This continues Were starting to hit the digital ocean rate limits during our provisioning Digital Ocean limits requests to per hour and provides feedback on how many you have left Would it be possible to scale back any nonessential requests when the threshold is near Example if RateLimitRemaining total rexray volumes skip status checks etc Hi I am using macOS and I tried to install rexray And it shows tar Unrecognized archive format It was quite confusing and I had to do some research to find out what happened It was not until then that I realized macOS is not supported So this commit will make the script show the warning message Unable to download the package Please make sure your platform is supported when curl cant download the tarball I think it may make the error message a little bit more friendly Summary Create s fs volume then inspect this volume and found filed Mountpoint is New Feature install plugin docker plugin install rexrays fslatest grantallpermissions S FSREGIONcnsouth S FSENDPOINT S FSACCESSKEYak S FSSECRETKEYsk create volume docker volume create driver rexrays fs name rexrays check volume rootqxgdev docker volume ls DRIVER VOLUME NAME rexrays fslatest rexrays inspect volume rootqxgdev volumes docker volume inspect rexrays CreatedAt T Z Driver rexrays fslatest Labels Mountpoint Name rexrays Options Scope global Status availabilityZone fields null iops name rexrays server s fs service s fs size type Why filed Mountpoint is null run image with this volume rootqxgdev volumes docker run it v rexrays opt centos docker Error response from daemon error while mounting volume VolumeDriverMount dockerlegacy Mount rexrays failed error mounting s fs bucket Run a centos image fail which step is wrong 