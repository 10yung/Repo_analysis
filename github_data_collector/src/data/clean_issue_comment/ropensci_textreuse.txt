I am encountering an issue using the TextReuseCorpus function where I feed in a vector of texts using the text option in the function and receive a warning of skipped texts due to insufficient length on character strings that should be long enough and get a different number of skip warnings each time I am reading in a large vector of texts ranging from to characters and usually k to k are skipped for being too short I can take these same skipped strings run TextReuseCorpus on them and theyll be fine this time around Perhaps Im simply doing something wrong When using Windows readLines will expect text files to be encoded in Windows This would add an optional encoding argument to TextReuseCorpus as well as TextReuseTextDocument which can be used to explicitly specify the encoding of the input files mostly UTF As it defaults to unknown which is the default for readLines this should maintain backward compatability Edit I forgot to mention that this specific issue can be worked around by setting optionsencoding UTF before creating the corpus however this has some side effects so I still think having an encoding argument is the better way to deal with this As I am reading it the TextReuseCorpus function has some safety check in order not to run tokenizers on documents that are too short too short being documents too small to generate two ngrams of the requested size In addition to that the tokenizers seem to have their own assertions to prevent running with too short documents However I have run into problems with skipgrams First the safety check in TextReuseCorpus lets documents pass that the assertion in tokenizeskipngrams then bails out on because the latter assertion assumes a larger minimum document length Second I dont quite understand why the assertion would require this in the first place IIUC its n n k k lengthwords but why would I not be able to generate skipgrams with the same document length as that of the ngram tokenizer n lengthwords FWIW I am trying to build large skipgrams say with n and k Thanks for any pointers or insights Hello from rOpenSci The official rOpenSci docs server which we announced in June is fully ready for production now Our server automatically builds and hosts pkgdown sites for all ropensci packages Official documentation URL Website build logs click last build console output If all seems good we strongly suggest to add the URL to the package DESCRIPTION file and include this in the next CRAN update This has two major benefits Pkgdown does automatic crosslinking to other pkgdown sites that can be found via CRAN This means that if another package refers to your package in an example or vignette their pkgdown site automatically hyperlinks those functions to your pkgdown site if your pkgdown URL has been published on CRAN Because all our documentation is hosted under docsropensciorg this will accumulate a higher pagerank than when a site are hosted on various custom domains This should make it easier to find these documentation sites on Google and other search engines We hope that this service will make it easy to maintain high quality and visible documentation for your packages If something is unclear or not working feel free to ask questions here or on slack FAQ What do I need to do to maintain documentation at docsropensciorg Absolutely nothing everything is done automatically How can I customize my docsropensciorg site You can use all standard pkgdown options in your pkgdownyml file except for the template we use the rotemplate pkgdown theme to render Can I help to improve the template Of course You can send pull requests to ropenscirotemplate Why are the images from the readmemd not showing in my pkgdown site pkgdown only supports local images in a few locations since only a few locations also work with CRANs rendering of readmes The recommended path for static images is manfigures I already had a site How to create a redirect from my old pkgdown site Simply push an indexhtml file to your ghpages branch which redirects to the new site see for example here Background In order to trace Shakespearean Intertextuality I am tokenizing Shakespeare texts hypotexts as grams and align each ngram alignlocal with other texts eg by Terry Pratchett or Charles Dickens hypertexts I loop through all the ngrams and only return alignments that are above a certain alignment score To speed the process up a little bit I only use every third ngram which should still be sufficient overlap to not miss any potential quotes WyrdSistersMacbethminimalRzip Problem However I am occasionally getting the following error message Error in bout outi borig rowi replacement has length zero Here is some more context via a screenshot from my console screenshoterror I cannot really reproduce the error but it seems to depend on how I set the countvariable which has an effect on the ngram I start with I assume the error has something to do with how the SmithWaterman algorithm builds up its matrix of values or looking into the TextReuse code more concretely with the output vector construction Place our first known values in the output vectors bout outi borig rowi aout outi aorig coli outi outi L Advance the out vector position I assume a related problem is described in StackOverflow but with no real solution Since the overall approach seems to work pretty well when it comes to discovering verbatim or near verbatim Shakespeare text reuse in other hypertexts I would be really happy to understand what is happening here and how I can possibly fix it For very large corpora it would be good to have a database backend so things dont have to stay in memory Create an index of ngrams and documents containing those ngrams with ways to remove rare unique or common ngrams and to extract pairs of matches from that index Implement earth mover distances as described in this paper There is a gensim implementation If possible do this using the word vec packages GloVe implementation from dselivanov 