Hi when utilising the faiss lib for KNN search you choose faissIndexFlatIP which has a relatively low performance compared with other indexes via faiss Could you please share if you select IndexFlatIP based on some reasons or its worth encouraging to try the more timeeffective index algorithms eg HNSWx Thanks ValueError could not convert string to float encodingutf MUSEmastersrcevaluationwordsimpy line in getwordpairs getwordpairspath lowerTrue ENWS ALLtxt xml version encodingUTF floatline float encodingUTF Hi What would be the best approach to align multiple more than monolingual word embeddings into a single vector space I come from fastTextmultilingual which is unfortunately outdated With this repo you could get words embeddings aligned into a single vector space I also read this article from Facebook talking about how they merged lot of different word embeddings I would like to be able to do the same not necessary for that much of languages but at least a dozen Any tip Thanks as per June the python module is named fastext Since I dont want to change the fastText function I just created an import alias wikienesesvec HELLO it seems that do not have wikienesesvec file In the Multilingual word Embeddings section the Arabic file is not correct It is only MB and there is no n at the end of the file like other languages FYI there are some issues downloading data with getevaluationsh First its pretty easy to be ratelimited from Furthermore I get Access Denied when trying to download the wordsim data not rate limit Second is down so you cant get the semeval data that way Third the two ways of getting data mentioned on the README arent equivalent for instance wget give you different dictionaries well more than running getevaluationsh Not currently causing me problems because Ive now figured it out but did trip me up for a little while so thought you should know Hi I have a question when I try to do unsupervised mapping between the source and target embeddings which are all subwordshave BPE procedures Since the SRC and TGT embeddings are subwords and all the dictionaries are words how can I get the evaluation dictionary that are subwords And the question also exists when I try to do unsupervised mapping between subword embeddings Hello I want to mapping some languages in a single vector space just you have described in the README But the code seems to adapt only two languages If I want to mapping more languages what should I do Hi Python module for fastText is now called fasttext see The loadfasttextmodel function should be updated 