Im not sure if this is a big deal or not but clippy is reporting that a number of variants have large size differences which can result in lots of wasted space or excessive memcpys warning large size difference between variants srcadminmessagesrs RpyAdminRpy note warnlargeenumvariant on by default help for further information visit help consider boxing the large fields to reduce the total size of the enum RpyBoxAdminRpy warning large size difference between variants srcadminmessagesrs ReplicaStateVrState help for further information visit help consider boxing the large fields to reduce the total size of the enum ReplicaStateBoxVrState warning large size difference between variants srcvrvrfsmrs RecoveryRecovery help for further information visit help consider boxing the large fields to reduce the total size of the enum RecoveryBoxRecovery warning large size difference between variants srcmsgrs AdminRpyAdminRpy help for further information visit help consider boxing the large fields to reduce the total size of the enum AdminRpyBoxAdminRpy Are there any downsides on introducing boxes in these structures This ended up being a big win for the compilers AST Right now the log is only GCd during commits when the globalminaccept and commitnum align The other opportunity to do this is when globalminaccept is incremented to reach the commitnum This occurs when a node has been down and then recovers If operations were committed while this node was down they wont be garbage collected because they dont exist on all nodes When the down node recovers those commits will be reflected before the primary receives the prepareOk messages and sends the updated globalminaccept It is now possible to garbage collect although garbage collection doesnt happen until the next commit We can optimize this to free memory earlier by doing the garbage collection when the globalminaccept count changes also This isnt crucial right now since reads and writes are in the log and commits are frequent However if writes are few and far between and reads dont cause commits as is planned then this will become a necessary optimization The cli command metrics Pid causes the admin server to crash ajsshelbsharet thread unnamed panicked at internal error entered unreachable code haretsrcadminconnectionhandlerrs Oddly enough this only brings down that thread The rest of the system including clients continues to work That seems to be an error in the thread join code that also could use fixing This root consensus group is used for administrative commands that need to be consistent across the cluster Things its used for are Creating and Deleting namespaces Global Configuration such as changing the idle timeouts of replicas Bulk key loading CrashRecovery Instead of running each operation in quickcheck serially and then sending all the VR messages corresponding to that operation allow running multiple operations in parallel and interleaving the VR messages until a desired state is reached or an error or timeout occurs Right now when an OpViewChange is run it results in a Tick message being send to a backup node to trigger a view change It then plays all resulting VR messages StartViewChange DoViewChange and StartView until there are no more messages left to send What Id like to do is be able to run multiple ops and interleave these view change messages with recovery messages If a view change is going on a recovery cannot successfully complete and must retry When the view change then completes retry to recover and ensure it does or the test times out and fails or some invariant is violated This should lead to more interesting histories The goal is not to do this per operation but to allow writing tests to run operations in parallel and interleave the resulting VR messages Postconditions can be waited on to result in retries where necessary to bring the system back to a healthy state Note that it would be handy to also allow dropping messages and simulating partitions during these tests The key challenge is how to enforce the determinism of VR msg interleavings so rerunning the same test results in the same order of messages being sent from and arriving at each node Since all replicas in these tests run in a single thread it should be possible to maintain the same exact linear order of messages processed given the same random seed value The reason for trying to maintain this order is to be able to reliably reproduce failures and replay the failing history with debugging messages turned on Hello As part of cleaning up HaretClientexec the first thing Im seeing is its a bit hard to tell here which operation needs which optional message Since things are pretty early I was wondering what you thought about restructuring TreeOp into a series of operationspecific request and response messages similar to etcd The advantage here we wouldnt have the ambiguity of doing a blobput and having the possibility the response containing namespaces In the whymd document there is a discussion about how haret was designed to isolate off the protocol from the clientfacing and datastorage parts of the system Would there be any interest in formalizing this into multiple libraries Im personally interested in exploring a Zookeeper client wirecompatible frontend a la zetcd so having a looser coupling between the subsystems would make this a bit easier to do Reads dont need to go into the log They just need to receive quorum in order to be linearizable When a read occurs and there are no outstanding prepare messages the data is read at the primary and a commit message is broadcast with a Read flag set If the Read flag is set the backups respond with a CommitOk message containing the epoch view and commitnum If the Read flag is not set backups dont respond at all normal VRR protocol When quorum is received the already read data is returned to the user If there are multiple pending prepare requests due to retries the read is tracked with the last outstanding prepare at the primary which has not yet gone out When the last prepare is sent its roundtrip counts as the read quorum as well When a quorum of prepareOk messages is received the commit for the latest write is made and the data is read from the primary and returned to the client Note however that if there is only one outstanding prepare request already in flight the primary must wait an extra round trip before returning the read It either waits for quorum on the retry of the prepare if it times out or the next commit or prepare request that was received after the read request This change also requires checking client requests to see if they are reads or writes Right now this isnt indicated in the vr specific client request itself It should be however as we want to keep the VR protocol itself agnostic to substance of state machine operations It only has to know if they are reads or writes Note that this information doesnt need to be added to the client protocol as that would be redundant for users It can be added when the the messages are received by the api server and converted to VR client requests 