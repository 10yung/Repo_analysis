In order to make a service shareable a locking middleware can provide mutually exclusive access without buffering requests This middleware is currently unused but will be used in followup changes This updates only the proxy not the testsupport code as suggested by olix r Im opening this as a draft since we likely will want to wait on merging it until the performancetesting harness is in place to compare performance with tokio This is essentially pothos PR but rebased updated to work with the current master In addition Ive changed the profiling proxy to be run as a separate bin target run with cargo run bin profile rather than a test case that does nothing on most test runs and runs the proxy in profiling mode when an env var is set Description from the original PR A benchmarkprofiling script for local development or a CI helps to catch performance regressions early and find spots for optimization The benchmark setup consists of a cargo test that reuses the test infrastructure to forward localhost connections This test is skipped by default unless an env var is set The benchmark load comes from a fortio server and client for HTTPgRPC reqs latency measurement and from an iperf server and client for TCP throughput measurement In addition to the fortio UI to inspect the benchmark data the results are also stored to a summary text file which can be used to plot the difference of the summary results of eg two git branches The profiling setup is the same as above but also runs perf or memoryprofilier to sample the call stacks at either runtime or on heap allocation calls This requires a special debug build with optimizations that can be generated with a build script The results can be inspected as interactive flamegraph SVGs in the browser Please follow the instructions in the profilingREADMEmd file on how the scripts are used Signedoffby Kai L ke kaikinvolkio Closes Signedoffby Eliza Weisman elizabuoyantio Coauthoredby Kai L ke kaikinvolkio Overview At the outset of the proxys life we introduced a Router The router has a few jobs To identify a target for each request To provision a service for that target and To maintain a cache of services to be reused across connections To evict idle services that have not been used ie so that the cache does not leak defunct services Because the router cannot know which inner service will be used before the request is received the routers pollready always returns ready meaning that it cannot exert backpressure to its callers So in order to ensure that the routers inner services can be shared by an arbitrary number of callersand to ensure that the inner service is driven to readinesswe had to add a buffering layer within the router The buffer holds requests while the inner service is driven to readiness on a dedicated task But requests could remain queued indefinitely so we introduced a deadline feature to the buffer so that the request could be stolen from the queue and failed if the request was not processed in a given amount of time And then we added Service Profiles with destination overrides and with these features a slew of new routers and buffers As weve diagnosed recent issue reports its become apparent that the system as its grown organically does not properly handle backpressure This most frequently manifests as s when requests are timed out of buffers though it is undoubtedly related to a plethora of other userreported issues like Service Discovery staleness Buffers Routers Backpressure Oh my Buffers Buffers are deployed to solve two problems Clonability and Readiness Clonability Without getting too into the weeds of Rusts memory model or Tokios execution model clonability basically refers to the ability to share a single service across multiple tasks For example if an application initiates multiple connections to an outbound proxy we dont want to create new routerscachesload balancers for each connection Instead we want to share the cached load balancers across all of these connections so we need to clone the cache into each connections task The buffer allows multiple tasks to send messages to a single service via an Multiproducer Single Consumer queue Readiness In Tower a services readiness indicates its ability to process requests Before a request can be dispatched the caller must invoke Servicepollready to ensure that the inner service is able to accept a request This is how backpressure works If an HTTP servers inner service is not ready it wont attempt to read a request from the socket and so the remote clients write calls will block once the kernel buffers are full Backpressure magic baby When we use buffers to ensure readiness we are effectively disabling backpressure We are signing up to handle requests and have to deal with timeouts etc We should only do this in rare and exceptional circumstances But as discussed above our current routing strategy explicitly requires that we do not exert backpressure routers must always be ready and so must each inner service Otherwise requests are dropped on the floor So not great Cancelation If an inner service does not become ready in a timely fashion requests can get stucK in the buffer If a caller cancels the request ie by dropping the response future we dont have any means to eagerly evict the request from the buffer Weve taken pains to be able to steal requests from the buffer after a timeout but this behavior has proven to be complex imperfect and difficult to diagnoseexplain Routers As discussed above routers do a few jobs and we currently use routers for a few different things Destination routing The primary use is when we receive a request and want to send it through a service that is configured by the control plane We dont want to query the control plane for each request and so we want to cache a service that holds the proper configuration for the target service Also as mentioned above these services need to be garbage collected once they are no longer in use otherwise the proxy is prone to consume memory without bounds Service Profile routing Routers are used somewhat differently in the context of Service profiles though Service profile routing is substantially different from destination routing All routes are known a priori and hence do not need to be discovered for each request Because all routes are provided by the control plane garbage collection is unnecessary and undesirable Because all routes in a service profile operate over the same inner services and because none of the layers in service profile routes can actually implement backpressure theres no reason a service profile router really needs to guarantee readiness The concrete destination router similarly abuses routers and therefore busts our ability to exert backpressure Summary We only care about clonability We should never synthesize readiness in the data path We need a simpler mechanism for bounding the time a request remains in the proxy before being dispatched to a client The routing layer is not a onesizefitsall solution Be wary the siren song of code reuse Solution Armed with this fresh knowledge Ive refactored the proxy stacks to eliminate all buffer layers from the data path and enforce a single service acquisition timeout that bounds the amount of time a request waits in the proxy to be dispatched to a client Clonability without Buffer Assuming that we can relax the readiness constraints what size should the buffers be to support clonability If we used a buffer with only a single slot for instance we could limit the how many requests can get stuck in a buffer to but the problem would remain We cant create a buffer with a capacity of zero but if we could we would have a Mutex So why not just use one of those My proposed change replaces use of a Buffer with a new clonable Lock layer Lockpollready first acquires a lock guarnteeing exclusive access to the inner service before the service is polled to ready Then the locked state is consumed and dropped as a request is dispatched on this service permitting other calls to obtain exclusive access to the service The one important subtlety here is that we need to be careful about services that invoke pollready before polling another source like the socket for a request In these cases the service can hold the lock on the inner service preventing other services from being stalled Services that are in this situation may either clone the service to drop the locked state or a Oneshot layer can be used to push backpressure into the services response ie so that pollready is not invoked until the request is materialized This is all to say that using a lock comes with some complexity We have to be careful about how we use services that may contain a lock otherwise tasks may be starved This seems to be an acceptable tradeoff though well need to find ways to detecttest improper access patterns Breaking the Readiness Requirement Fundamentally we need to do routing without requiring that all routes are ready and we need to be able to limit the amount of time a request spends waiting for a ready service In order to accomplish this we need to formalize two distinct phases of proxying The first phase is service acquisition which can be loosely described by the towerMakeService API we first asynchronously build a service for a target type so that the request can be dispatched to the target service We can set a service acquisition timeout on the future that acquires the service ie without setting a response timeout This is all accomplished by decoupling the routing from caching Routing selects which target to use The cache implements towerMakeService returning a service for each target With this decoupling we can insert timeout layers within the toplevel logical router to ensure that service acquisition is bounded With this new strategy the Tokio runtime becomes the buffer We can bound the total number of requests in flight to constrain this buffer but this means that backpressure cancelation etc can be achieved more naturally Areas of Focus Look this change is huge A significant number of superficial changes are comingled here On the upside compile time has been reduced substantially and some organically diverging idioms have been consolidated The proxy stacks The lock layer The caching layer The routing layer The profile router Beyond this Ive endeavored to limit PhantomData uses especially in layers They make it so the stack cannot be reused and I believe negatively impact compile times In general Ive tried to remove unnecesscary type constraints They make the system more resistant to change Such changes include The routerMake type has been renamed to stackNewService Make and MakeService were too close for comfort Many layers now implement both MakeService via Service and NewService Stack types are now generally named MakeFoo or NewFoo Types in the retry module have been renamed so that the Service could be called Retry basically The profile router is just totally new Target types especially DstAddr are not reused Purposespecific target types are used Probably other things too Its been a trip This is going to be difficult to review My advice to you the reviewer is to first focus on the above description Please ask questions poke at implementation until most of the above makes some sense Id like to schedule a longer inperson review for later this week so that we can get enough confidence to move forward with this approach The routers Make trait is generally useful when we have to build middlewares In preparation for improvements to the profile router this change moves the Make trait into the linkerd stack crate with some API improvements Furthermore the Router is changed to avoid requiring that its services implement Clone In general we expect targets to be cheaply cloneable but we should only require that services are cloneable when absolutely necessary This PR makes the proxy changes which are necessary for We revamp the transport metrics to provide the following labels direction one of incoming or outgoing srcaddr the IP address of the initiator of the connection does not include port omitted when directionoutgoing since the local address is provided by Prometheus and the ephemeral client port is not useful dstaddr the IP address and port of the accepter of the connection always present even when directionincoming since the local address provided by Prometheus does not include the relevant server port tls boolean indicating if the connection has TLS notlsreason if tlsfalse this gives the reason remoteidentity the TLS identity of the remote peer if tlstrue localidentity the local TLS identity if tlstrue protocol the detected protocol of this connection one of http or tcp Here is an example of what these metrics look like in Prometheus tcpopentotalcontrolplanenslinkerddeploymentauthorsdirectioninbounddstaddr instance joblinkerdproxylocalidentitydefaultdefaultserviceaccountidentitylinkerdclusterlocalnamespacedefaultpodauthorsf b f s jzhprotocolhttpremoteidentitydefaultdefaultserviceaccountidentitylinkerdclusterlocalsrcaddr tlstrue tcpopentotalcontrolplanenslinkerddeploymentauthorsdirectioninbounddstaddr instance joblinkerdproxylocalidentitydefaultdefaultserviceaccountidentitylinkerdclusterlocalnamespacedefaultpodauthorsf b f s jzhprotocolhttpremoteidentitydefaultdefaultserviceaccountidentitylinkerdclusterlocalsrcaddr tlstrue tcpopentotalcontrolplanenslinkerddeploymentauthorsdirectioninbounddstaddr instance joblinkerdproxylocalidentitydefaultdefaultserviceaccountidentitylinkerdclusterlocalnamespacedefaultpodauthorsf b f s jzhprotocolhttpremoteidentitydefaultdefaultserviceaccountidentitylinkerdclusterlocalsrcaddr tlstrue tcpopentotalcontrolplanenslinkerddeploymentauthorsdirectioninbounddstaddr instance joblinkerdproxylocalidentitydefaultdefaultserviceaccountidentitylinkerdclusterlocalnamespacedefaultpodauthorsf b f s jzhprotocolhttpremoteidentitydefaultdefaultserviceaccountidentitylinkerdclusterlocalsrcaddr tlstrue tcpopentotalcontrolplanenslinkerddeploymentauthorsdirectioninbounddstaddr instance joblinkerdproxynamespacedefaultnotlsreasonnotprovidedbyremotepodauthorsf b f s jzhprotocolhttpsrcaddr tlsfalse tcpopentotalcontrolplanenslinkerddeploymentauthorsdirectionoutbounddstaddr instance joblinkerdproxylocalidentitydefaultdefaultserviceaccountidentitylinkerdclusterlocalnamespacedefaultpodauthorsf b f s jzhprotocolhttpremoteidentitydefaultdefaultserviceaccountidentitylinkerdclusterlocaltlstrue Currently all crates in the proxy repo are configured to always deny all Rust compiler warnings This means that if the compiler reports any warnings the build will fail This can be an issue when changes are made locally that introduce warnings ie unused code is added imports are no longer needed etc and the developer wishes to test an incomplete state to ensure their changes are on the right path With warnings denied tests will not run if the project contains any warnings so developers must either fix all warnings or disable the deny attribute Disabling the deny attribute when making changes locally has become a fairly common practice but its errorprone sometimes the deny attribute is commented out and then accidentally committed This branch is a proposal for a new way to handle Rust warnings Rather than always failing the build if any warnings are reported we now export an env var on CI that configures the compiler to deny warnings Local indevelopment builds will report warnings but not fail the build making it easier to run tests for incomplete changes but code with warnings wont be allowed to merge to master Hopefully this obsoletes the practice of commenting out denywarnings when making changes Signedoffby Eliza Weisman elizabuoyantio This PR is a draft and for discussion purpose only I added Kafka as a new traffic protocol and called the external lib kafkacodec to decode it More details are given in the inline comments Also the header of Kafka response can be decoded A benchmarkprofiling script for local development or a CI helps to catch performance regressions early and find spots for optimization The benchmark setup consists of a cargo test that reuses the test infrastructure to forward localhost connections This test is skipped by default unless an env var is set The benchmark load comes from a fortio server and client for HTTPgRPC reqs latency measurement and from an iperf server and client for TCP throughput measurement In addition to the fortio UI to inspect the benchmark data the results are also stored to a summary text file which can be used to plot the difference of the summary results of eg two git branches The profiling setup is the same as above but also runs perf or memoryprofilier to sample the call stacks at either runtime or on heap allocation calls This requires a special debug build with optimizations that can be generated with a build script The results can be inspected as interactive flamegraph SVGs in the browser Please follow the instructions in the profilingREADMEmd file on how the scripts are used Signedoffby Kai L ke kaikinvolkio Discussion This commit is based on our work in the branch Here we have removed the wsspl memory usage summary and the wrk actixweb and strestgrpc test loads but you can find them in the linked branch We replaced wrk and strestgrpc with fortio because it turned out to give more consistent results and uses the same software stack for both HTTP and gRPC simplify the setup and include a UI to inspect and compare the benchmark results The plotpy script is still useful to visualize the difference in the results of two branches specially in a CI report where the fortio web UI is of less use We would like to get feedback Is is better to stay with wrk and strestgrpc The current default reqs values come from my system and you might need to adjust them to have one scenario with medium load and one with almost maximum load through fortio Do you get only small variations between different runs Maybe the runtime of the benchmark should be longer increase duration or iterations The scripts for heap and perf profiling and benchmarking only differ in a few lines We can unify them with if branches We didnt rename them yet to ease comparison with the above linked branch but eg the fortio suffix can go We hope you find the local benchmark data and the perfheap flamegraphs useful 