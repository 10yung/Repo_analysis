When I do dfwriteformattfrecordsoptionwriteLocalitylocalsavepath or dfwriteformattfrecordssavepath the file is not getting created on the main Driver node I dont see the folder as well I am unable to overwrite as that mode is not working I am unable to delete those files as they are in worker node Is there a simple API that can delete the files created through this spark job Are there examples compatible with TF similar to Aligning with orgtensorflowhadoopioTFRecordFileOutputFormat enable compression in orgtensorflowhadoopioTFRecordFileOutputFormatV as well To activate compression in old MapReduce APIs simply specify options as below bash Dmapredoutputcompresstrue DmapredoutputcompressioncodecorgapachehadoopiocompressGzipCodec This pull request can be considered as supplementary to case VectorType val field rowgetindex field match case v SparseVector FloatListFeatureEncoderencodevtoDensetoArraymaptoFloat case v DenseVector FloatListFeatureEncoderencodevtoArraymaptoFloat case throw new RuntimeExceptionsCannot convert field to vector I found this code in your DefaultTfRecordRowEncoderscala explicitly converse a SparseVector to a DenseVector I have a dimentional feature vector in my DataFrame which has about nonzero values So this conversion make the size of tfrecord dataset very much larger than snappyparquet in Spark Im a little confused about the conversion When I ran mvn clean install Dsparkversion The build fails because of the following error appears error scalareflectinternalMissingRequirementError object javalangObject in compiler mirror not found at scalareflectinternalMissingRequirementErrorsignalMissingRequirementErrorscala at scalareflectinternalMissingRequirementErrornotFoundMissingRequirementErrorscala at scalareflectinternalMirrorsRootsBasegetModuleOrClassMirrorsscala at scalareflectinternalMirrorsRootsBasegetModuleOrClassMirrorsscala at scalareflectinternalMirrorsRootsBasegetModuleOrClassMirrorsscala at scalareflectinternalMirrorsRootsBasegetModuleOrClassMirrorsscala at scalareflectinternalMirrorsRootsBasegetClassByNameMirrorsscala at scalareflectinternalMirrorsRootsBasegetRequiredClassMirrorsscala at scalareflectinternalDefinitionsDefinitionsClassObjectClasslzycomputeDefinitionsscala at scalareflectinternalDefinitionsDefinitionsClassObjectClassDefinitionsscala at scalareflectinternalDefinitionsDefinitionsClassinitDefinitionsscala at scalatoolsnscGlobalRuninitGlobalscala at scalatoolsnscDriverdoCompileDriverscala at scalatoolsnscMainClassdoCompileMainscala at scalatoolsnscDriverprocessDriverscala at scalatoolsnscMainprocessMainscala at javabasejdkinternalreflectNativeMethodAccessorImplinvoke Native Method at javabasejdkinternalreflectNativeMethodAccessorImplinvokeNativeMethodAccessorImpljava at javabasejdkinternalreflectDelegatingMethodAccessorImplinvokeDelegatingMethodAccessorImpljava at javabasejavalangreflectMethodinvokeMethodjava at sbtcompilerRawCompilerapplyRawCompilerscala at sbtcompilerAnalyzingCompileranonfuncompileSources anonfunapply applyAnalyzingCompilerscala at sbtcompilerAnalyzingCompileranonfuncompileSources anonfunapply applyAnalyzingCompilerscala at sbtIOwithTemporaryDirectoryIOscala at sbtcompilerAnalyzingCompileranonfuncompileSources applyAnalyzingCompilerscala at sbtcompilerAnalyzingCompileranonfuncompileSources applyAnalyzingCompilerscala at sbtIOwithTemporaryDirectoryIOscala at sbtcompilerAnalyzingCompilercompileSourcesAnalyzingCompilerscala at sbtcompilerICcompileInterfaceJarIncrementalCompilerscala at comtypesafezincCompilercompilerInterfaceCompilerscala at comtypesafezincCompilercreateCompilerscala at comtypesafezincCompilercreateCompilerscala at sbtincSbtIncrementalCompilerinitSbtIncrementalCompilerjava at scalamavenScalaCompilerSupportincrementalCompileScalaCompilerSupportjava at scalamavenScalaCompilerSupportcompileScalaCompilerSupportjava at scalamavenScalaCompilerSupportdoExecuteScalaCompilerSupportjava at scalamavenScalaMojoSupportexecuteScalaMojoSupportjava at orgapachemavenpluginDefaultBuildPluginManagerexecuteMojoDefaultBuildPluginManagerjava at orgapachemavenlifecycleinternalMojoExecutorexecuteMojoExecutorjava at orgapachemavenlifecycleinternalMojoExecutorexecuteMojoExecutorjava at orgapachemavenlifecycleinternalMojoExecutorexecuteMojoExecutorjava at orgapachemavenlifecycleinternalLifecycleModuleBuilderbuildProjectLifecycleModuleBuilderjava at orgapachemavenlifecycleinternalLifecycleModuleBuilderbuildProjectLifecycleModuleBuilderjava at orgapachemavenlifecycleinternalbuildersinglethreadedSingleThreadedBuilderbuildSingleThreadedBuilderjava at orgapachemavenlifecycleinternalLifecycleStarterexecuteLifecycleStarterjava at orgapachemavenDefaultMavendoExecuteDefaultMavenjava at orgapachemavenDefaultMavendoExecuteDefaultMavenjava at orgapachemavenDefaultMavenexecuteDefaultMavenjava at orgapachemavencliMavenCliexecuteMavenClijava at orgapachemavencliMavenClidoMainMavenClijava at orgapachemavencliMavenClimainMavenClijava at javabasejdkinternalreflectNativeMethodAccessorImplinvoke Native Method at javabasejdkinternalreflectNativeMethodAccessorImplinvokeNativeMethodAccessorImpljava at javabasejdkinternalreflectDelegatingMethodAccessorImplinvokeDelegatingMethodAccessorImpljava at javabasejavalangreflectMethodinvokeMethodjava at orgcodehausplexusclassworldslauncherLauncherlaunchEnhancedLauncherjava at orgcodehausplexusclassworldslauncherLauncherlaunchLauncherjava at orgcodehausplexusclassworldslauncherLaunchermainWithExitCodeLauncherjava at orgcodehausplexusclassworldslauncherLaunchermainLauncherjava Change sparktensorflowconnector to be spark preview Hi Im testing Distribution Strategies Example on my own K S cluster there is an unexpected error before the end of the task However when I run kerasmodeltoestimatorpy locally it ends normally docker HADOOPVERSION APACHESPARKVERSION PYTHONVERSION codes spark None try printcreating spark cluster spark SparkSessionbuilder appNamesparkappname configsparkkubernetesnamespace sparkk snamespace configsparkjarspackages orgtensorflowsparktensorflowconnector configmapreducefileoutputcommitteralgorithmversion configsparkexecutorinstances strsparknexecutors configsparksqlwindowExecbufferspillthreshold configsparksqlwindowExecbufferinmemorythreshold enableHiveSupport getOrCreate logic df sparkreadparquetdatalakelocation dfrepartitionnumpartition write formattfrecords modeoverwrite savedatasetlocation finally terminate spark context if spark printstopping spark cluster sparkstop After all jobs has been finished it will write to temporary under target directory first and then write to the target directory sequentially which cost a lot of time as following how to read the tfrecord use python its always wrongwhen i read FixedSequnceFeatures 