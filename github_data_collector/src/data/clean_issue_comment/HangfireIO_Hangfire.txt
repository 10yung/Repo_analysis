So for example a user left a tab open on the Hangfire Recurring Jobs page when the sql server was rebooted overnight this usually takes minutes or so the server obviously spews out errors during the time that SQL is offline however once SQL comes back online its like hangfire has saturated all the available connections in the process I end up with the error SystemInvalidOperationException Timeout expired The timeout period elapsed prior to obtaining a connection from the pool This may have occurred because all pooled connections were in use and max pool size was reached at SystemDataProviderBaseDbConnectionFactoryTryGetConnectionDbConnection owningConnection TaskCompletionSource retry DbConnectionOptions userOptions DbConnectionInternal oldConnection DbConnectionInternal connection at SystemDataProviderBaseDbConnectionInternalTryOpenConnectionInternalDbConnection outerConnection DbConnectionFactory connectionFactory TaskCompletionSource retry DbConnectionOptions userOptions at SystemDataProviderBaseDbConnectionClosedTryOpenConnectionDbConnection outerConnection DbConnectionFactory connectionFactory TaskCompletionSource retry DbConnectionOptions userOptions at SystemDataSqlClientSqlConnectionTryOpenTaskCompletionSource retry at SystemDataSqlClientSqlConnectionOpen at HangfireSqlServerSqlServerStorageCreateAndOpenConnection at HangfireSqlServerSqlServerStorageUseConnection T DbConnection dedicatedConnection Func func at HangfireSqlServerSqlServerMonitoringApiGetStatistics at SystemLazy ViaFactoryLazyThreadSafetyMode mode at SystemLazy ExecutionAndPublicationLazyHelper executionAndPublication Boolean useDefaultConstructor at SystemLazy CreateValue at HangfireDashboardDashboardMetricsccctorb RazorPage page at HangfireDashboardJsonStatsDispatchDashboardContext context at HangfireDashboardAspNetCoreDashboardMiddlewareInvokeHttpContext httpContext at MicrosoftAspNetCoreBuilderExtensionsMapMiddlewareInvokeHttpContext context at MicrosoftAspNetCoreBuilderRouterMiddlewareInvokeHttpContext httpContext at MicrosoftAspNetCoreAuthenticationAuthenticationMiddlewareInvokeHttpContext context at MicrosoftAspNetCoreStaticFilesStaticFileMiddlewareInvokeHttpContext context at MicrosoftAspNetCoreStaticFilesStaticFileMiddlewareInvokeHttpContext context at MicrosoftAspNetCoreServerKestrelCoreInternalHttpHttpProtocolProcessRequests TContext IHttpApplication application And it is only fixed by stopping and restarting the dotnet process I am using hangfire v with the following SqlServerStorageOptions new SqlServerStorageOptions CommandBatchMaxTimeout TimeSpanFromMinutes SlidingInvisibilityTimeout TimeSpanFromMinutes QueuePollInterval TimeSpanZero UseRecommendedIsolationLevel true UsePageLocksOnDequeue true DisableGlobalLocks true i was add job by RecurringJobAddOrUpdate TaskMangerrun it should be seconds but it ran by what can i do We have multiple instances of a windows service running that starts Hangfire with TopShelf We re using version in production with Sql storage on Azure though I ve been able to reproduce this issue on the latest release as well We typically have recurring jobs a couple that run every minutes others that run once per day and those recurring jobs spawn several hundred or thousand individual jobs when they run In general this works just fine Periodically though varies between weekly and every couple weeks we ll see a situation where jobs will be enqueued and some processing up to the count of workers setup in the instance but jobs actually stop running and new jobs submitted stack up in the enqueued state A restart of the windows service will typically correct this and get things moving again but the jobs are timesensitive dealing with closing financial transactions so money moves from one account to another and if we do not catch it right away it can create delays in funding Our production environments will have more than one server each server having its own windows service pointing at the same SQL Server sharing the processing and when this happens we will see it happen on each of the servers at the same time We ve enabled trace logging trying to find any issues that might set this off We haven t logged anything absolutely conclusive but we did have several occurrences where the log showed that a connection could not be obtained from the pool Error occurred during execution of Worker f b b process Execution will be retried attempt in seconds SystemInvalidOperationException Timeout expired The timeout period elapsed prior to obtaining a connection from the pool This may have occurred because all pooled connections were in use and max pool size was reached at SystemDataProviderBaseDbConnectionFactoryTryGetConnectionDbConnection owningConnection TaskCompletionSource retry DbConnectionOptions userOptions DbConnectionInternal oldConnection DbConnectionInternal connection at SystemDataProviderBaseDbConnectionInternalTryOpenConnectionInternalDbConnection outerConnection DbConnectionFactory connectionFactory TaskCompletionSource retry DbConnectionOptions userOptions at SystemDataProviderBaseDbConnectionClosedTryOpenConnectionDbConnection outerConnection DbConnectionFactory connectionFactory TaskCompletionSource retry DbConnectionOptions userOptions at SystemDataSqlClientSqlConnectionTryOpenInnerTaskCompletionSource retry at SystemDataSqlClientSqlConnectionTryOpenTaskCompletionSource retry at SystemDataSqlClientSqlConnectionOpen at HangfireSqlServerSqlServerStorageCreateAndOpenConnection at HangfireSqlServerSqlServerJobQueueDequeueUsingTransactionString queues CancellationToken cancellationToken at HangfireSqlServerSqlServerJobQueueDequeueString queues CancellationToken cancellationToken at HangfireSqlServerSqlServerConnectionFetchNextJobString queues CancellationToken cancellationToken at HangfireServerWorkerExecuteBackgroundProcessContext context at HangfireServerServerProcessExtensionsExecuteIServerProcess process BackgroundProcessContext context at HangfireServerAutomaticRetryProcessExecuteBackgroundProcessContext context This was logged for the HangfireServerWorker HangfireServerRecurringJobScheduler and HangfireServerDelayedJobScheduler It didn t seem to be immediately after this that the issue above occurred but close enough to be suspicious To attempt to reproduce this locally I setup a vanilla console application with the latest release pointing to a local sql database containing only the Hangfire tables I created a simple class that would create a sql connection open it wait seconds and close it and when the application starts up I queued up of those jobs Once the application starts I give it just a moment then go into SSMS and take the local database offline having it kill any active sessions This starts generating errors in the Hangfire logs of course I leave that offline for several minutes then bring the database back online The dashboard recovers at this point and occasionally jobs resume processing at this point and everything completes OK However other times this will recreate what I described above When I ve recreated this locally I also received an error in the log Unable to signal the stopped event for BackgroundDispatcher it was already disposed Though this may be unrelated When I got it in this state locally I noticed if I try to do a select against the JobQueue table in sql the query doesn t return any rows and just continues to run Digging into that a little more I saw transactions left open Once I killed those sessions the JobQueue table was queryable again I used stdump to get stack traces when the issue occurred and after I killed the sql processes that had the open transactions traces attached HangfireTestlog stdump after killtxt stdump pre kill txt stdump pre kill txt HangfireTest log Hi This PR adds support for job descriptions which are longform blocks of HTML prose which offer more information on a particular job than the name field can allow If present the description field is displayed on the Job Details page of the dashboard The attribute itself is symmetrical to the JobDisplayNameAttribute Likewise I have added DashboardOptionsDescriptionFunc to override the description provider if desired Thanks I have implemented IDashboardAuthorizationFilter it works fine locally I am running the site in a virtual directory When I push to the server via Azure Devops I get a error when accessing the dashboard I added this to webconfig per a Stack Overflow post add namehangfireDashboard pathhangfire typeSystemWebDefaultHttpHandler verb now I am getting HTTP Error Internal Server Error Handler hangfireDashboard has a bad module ManagedPipelineHandler in its module list Server is Windows Server R IIS Is it possible to use BackgroundJobContinueJobWith with the extension HttpJobs I have developed an api website where I have methods and these must be executed synchronously Thank you very much for your help regards Not sure if this is simply a documentation concern but the LocalRequestsOnlyAuthorizationFilter being applied automatically for the following use case may cause confusion for some folks csharp appUseEndpointsendpoints endpoints MapHangfireDashboard RequireAuthorizationmyPolicy When this is what the user was probably wanting to do csharp appUseEndpointsendpoints var dashboardOptions new DashboardOptions Authorization new IDashboardAuthorizationFilter endpoints MapHangfireDashboarddashboardOptions RequireAuthorizationmyPolicy Potential resolution question Should calling RequireAuthorizationmyPolicy in this manner also unhook the default local only authorization Im working on a Binance bot project and I decided to use Hangfire for each running bot thread How do I cancel a specific BackgroundJob RecurringJobs allow me to specify a job name while BackgroundJob doesnt private UpdateSubscription subscription private readonly CancellationTokenSource cts new CancellationTokenSource public void RunBot bot BackgroundJobEnqueue Startbot ctsToken public void StartBot bot CancellationToken token heavy logic subscription socketClientSubscribeToKlineUpdatesbotCryptoPairSymbol botTimeIntervalInterval KlineIntervalOneHour async data logic if tokenIsCancellationRequested await socketClientUnsubscribesubscription Stop specific bot public void Stopstring botName ctsCancel Hi we have a recurring job that runs every minutes AutomaticRetryAttempts OnAttemptsExceeded AttemptsExceededActionDelete JobDisplayNameUpdate status from public async Task UpdateStatusAsyncint providerId string providerName PerformContext context If the server is disabled for hours when it starts up Hangfire it trying to run this job almost times right away I cant imagine this is the expected way its supposed to work but if it is how can we configure Hangfire to NOT do that and just run the next scheduled recurring task since restarting back up I know theres some hacky ways to fix this but it really seems there should be a built in way to deal with it Is there some documentation I am missing If so I apologize up front 