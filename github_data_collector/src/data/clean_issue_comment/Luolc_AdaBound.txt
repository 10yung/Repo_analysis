Greetings Thanks for your great paper I am wondering about the hyperparameters you used for language modeling experiments Could you provide information about that Thank you I strongly believe that AdaBound would be better if it used RAdam instead of Adam It could merge with Lookahead too and LAMB Then we would have the best of both worlds and a beautiful example of scientific collaboration correct grammar would be as well as adam not sure if you care camp I tested three methods in a very simple problem and got the result as above Code are printed here import torch import torchnn as nn import matplotlibpyplot as plt import adabound class NetnnModule def initself dim superNet selfinit selffc nnLineardim dim selfrelu nnReLUinplaceTrue selffc nnLinear dim dim def forwardself x x selffc x x selfrelux x selffc x return x DIM epochs xini torchones DIM opti torchzeros DIM lr net NetDIM objfun nnMSELoss lossadab lossadam losssgd for epoch in rangeepochs if epoch lr optimizer adaboundAdaBoundnetparameters lr out netxini los objfunout opti lossadabappendlosdetachnumpy optimizerzerograd losbackward optimizerstep lr net NetDIM objfun nnMSELoss for epoch in rangeepochs if epoch lr optimizer torchoptimAdamnetparameters lr out netxini los objfunout opti lossadamappendlosdetachnumpy optimizerzerograd losbackward optimizerstep lr net NetDIM objfun nnMSELoss for epoch in rangeepochs if epoch lr optimizer torchoptimSGDnetparameters lr momentum out netxini los objfunout opti losssgdappendlosdetachnumpy optimizerzerograd losbackward optimizerstep pltfigure pltplotlossadab labeladabound pltplotlossadam labeladam pltplotlosssgd labelSGD pltyscalelog pltxlabelepochs pltylabelLogloss pltlegend pltsavefigcamppng dpi pltshow Thank you very much for sharing this impressive work I am somehow receiving the following error for group baselr in zipselfparamgroups selfbaselrs AttributeError AdaBound object has no attribute baselrs Just change optimizer torchoptimAdammodelparameters lr e amsgradFalse to optimizer adaboundAdaBoundmodelparameters lr e finallr Nan loss in RCAN model but Adam work fine The provided new optimizer is sensitive on tiny batchsize I am testing on the very simply linear regression while others performance looks like nice currently Path Loss curve Zoomed Loss curve I dont see any reason why this code would not run in a lower version of python Could you explain why is there such a requirement Applies bounds on actual learning rate lrscheduler cannot affect finallr this is a workaround to apply lr decay finallr group finallr group lr baselr However lrscheduler may change paramgroup lr during training therefore the finallr lowerbound upperbound will also be affected Should I not use lrscheduler and let AbaBound adapts the params to transform from Adam to SGD Thank you very much