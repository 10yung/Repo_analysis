Threenode cluster had been upgraded all nodes at once due to proto change and stopped functioning Ive seen another GH issue with cant start until leader is available couple of days ago but option sync true doesnt help much Same upgrade procedure works well on a cluster with no data Logs and config are pasted below please suggest Much thanks in advance BEFORE UPGRADE tail varlognatsnsslog envnats INF STREAM Channels Limits INF STREAM Subscriptions INF STREAM Messages INF STREAM Bytes MB INF STREAM Age unlimited INF STREAM Inactivity unlimited INF STREAM INF rid Route connection created INF rid Route connection created envnats INF STREAM Messages INF STREAM Bytes MB INF STREAM Age unlimited INF STREAM Inactivity unlimited INF STREAM INF rid Route connection created INF rid Route connection created INF STREAM Deleting raft logs from to INF STREAM Deletion took s envnats INF STREAM Channels Limits INF STREAM Subscriptions INF STREAM Messages INF STREAM Bytes MB INF STREAM Age unlimited INF STREAM Inactivity unlimited INF STREAM INF STREAM server became leader performing leader promotion actions INF STREAM finished leader promotion actions curl s localhost streamingserverz grep state envnats state CLUSTERED envnats state CLUSTERED envnats state CLUSTERED natsstreamingserver version envnats natsstreamingserver version natsserver version envnats natsstreamingserver version natsserver version envnats natsstreamingserver version natsserver version AFTER UPGRADE natsstreamingserver version envnats natsstreamingserver version natsserver v envnats natsstreamingserver version natsserver v envnats natsstreamingserver version natsserver v curl s localhost streamingserverz grep state envnats state CLUSTERED envnats curl connection timeout envnats curl connection timeout tail varlognatsnsslog envnats INF rid Router connection closed ERR STREAM channel xxxxxxxxfailure unable to restore messages cant start until leader is available INF rid Router connection closed INF rid Route connection created INF rid Route connection created INF rid Router connection closed ERR STREAM channel xxxxxxxxfailure unable to restore messages cant start until leader is available ERR STREAM channel xxxxxxxxfailure unable to restore messages cant start until leader is available ERR STREAM channel xxxxxxxxfailure unable to restore messages cant start until leader is available ERR STREAM channel xxxxxxxxfailure unable to restore messages cant start until leader is available envnats INF STREAM Cluster Node ID envnats INF STREAM Cluster Log Path envnatsstreamingenvnats INF rid Route connection created INF rid Router connection closed INF rid Route connection created INF rid Router connection closed ERR STREAM channel xxxxxxxxfailure unable to restore messages cant start until leader is available ERR STREAM channel xxxxxxxxfailure unable to restore messages cant start until leader is available ERR STREAM channel xxxxxxxxfailure unable to restore messages cant start until leader is available ERR STREAM channel xxxxxxxxfailure unable to restore messages cant start until leader is available envnats INF rid Route connection created INF rid Router connection closed INF rid Router connection closed INF rid Route connection created INF rid Route connection created INF rid Router connection closed INF rid Router connection closed INF rid Route connection created INF rid Route connection created INF rid Router connection closed CONFIGS cat etcnatsstreamingserverconf port httpport pid file pidfile homenatsrunnsspid logging options debug false trace false logtime true logfile varlognatsnsslog cluster listen routes natsenvnats natsenvnats natsenvnats NATS Streaming specific configuration streaming id envnatsstreaming store file dir store cluster nodeid envnats peers envnats envnats envnats sync true Background Weve noticed a behaviour we would like to suggest a change for We are running a number of subscribers in both node and python these are subscribed with a durable name a queue group and manual acks When running multiple instances of these subscribers messages are distributed across the instances and subscribers can drop out and come back fine durable name Not all messages that are sent will be acknowledged if a piece of work fails its message is not acknowledged and natsstreaming will retry later Request We have noticed that if all the subscribers close their connections the first one back gets a huge influx of unacknowledged messages it can be a lot and we suspect it maybe the entire backlog Can natsstreaming therefore honour the maxInFlight setting for that channel when attempting to redeliver all the unacknowledged messages on resumption We understand this is related to and We dont have any problem with unacknowledged messages being redelivered first on resume this is actually fine for our use case But having all messages redelivered at once is a problem as there could be a LOT of them This could send the first listening subscription into a sudden heavy load situation To replicate If we have a running natsstreamingserver and we have a subscriber below in Node that only acknowledges messages that are strings that equal bob subscriberjs js const stan requirenodenatsstreaming const os requireos Client Id const clientId oshostnameprocesspidreplace g Transport const transport stanconnecttestcluster clientId transportonconnect function Generic function const caller msg consolelogFunction msggetSequence msggetData if msggetData bob consolelogGot a bob message msgack Generic function const wrapper msg callermsg const exit transportclose Set options const options transportsubscriptionOptions optionssetAckWait optionssetMaxInFlight optionssetManualAckModetrue optionssetDurableNamenlpa Subscribe const subscription transportsubscribenlpa qnlpa options subscriptiononmessage wrapper processonSIGTERM exit processonSIGINT exit processonSIGQUIT exit Done consolelogReady If we run multiple instances of this messages are shared between the instances queue name if one instance is stopped and started it will resume and join the group again If we send bob messages and non bob messages via callerjs js const stan requirenodenatsstreaming const os requireos Client Id const clientId oshostnameprocesspidreplace g Transport const transport stanconnecttestcluster clientId transportonconnect function for let i i i transportpublishnlpa JSONstringify a a function err uid consolelogMessage recieved uid for let i i i transportpublishnlpa bob function err uid consolelogBob message recieved uid You can see bob messages appearing and being acknowledged and after sometime you can observe all the retries happening If you then close both instances of the of the subscriberjs and start just one instance of subsriberjs again it receives more that the maxInFlight messages in this example This number could get very large very quickly in our situation Environment natsstreamingserver version natsserver v Ive node nats streaming cluster running and was running fine Due to some issues node goes down Once my nodes have recovered Ive tried restarting nats streaming server in all the three nodes but it never comes up logs node log node log node log config files conftxt conftxt conftxt I had to redeploy the cluster with same config file I have a tiny Go application single main package which uses NATS Streaming I have a very small system test that starts an inprocess STAN server using githubcomnatsionatsstreamingserverserverRunServer Problem is that my application is using the flag package which conflicts with githubcomnatsionatsstreamingserverserver You can find a condensed test that breaks here The output is sh go test flag provided but not defined testtestlogfile Usage of varfoldersjkzh vp qj n bpzrgb t p gnTgobuild b yotest FAIL githubcomJensRantilyo s FAIL I think the possibility of starting an inprocess STAN in a Go test is really really nice and Im sure my test passed a little less than a year ago Is it too much to ask that you dont rely on the global CommandLine variable but instead instantiate a FlagSet In case anyone else is hitting this there are two workarounds for this issue Move the test to a separate package which isnt relying on flagCommandLine I and everyone else depending on githubcomnatsionatsstreamingserverserver start using flagFlagSet myself i have setup a node nats cluster on kubernetes using the following manifasts txt txt txt the cluster is initiated and the clients can connect but after deleting a pod it starts to print the following logs frequently INF rid Router connection closed ERR rid attempted to connect to route port INF rid Route connection created ERR rid attempted to connect to route port INF rid Router connection closed ERR rid attempted to connect to route port INF rid Route connection created ERR rid attempted to connect to route port INF rid Router connection closed ERR rid attempted to connect to route port INF rid Route connection created ERR rid attempted to connect to route port INF rid Router connection closed ERR rid attempted to connect to route port INF rid Route connection created ERR rid attempted to connect to route port INF rid Router connection closed ERR rid attempted to connect to route port INF rid Route connection created ERR rid attempted to connect to route port INF rid Router connection closed ERR rid attempted to connect to route port INF rid Route connection created ERR rid attempted to connect to route port INF rid Router connection closed ERR rid attempted to connect to route port INF rid Route connection created ERR rid attempted to connect to route port INF rid Router connection closed ERR rid attempted to connect to route port it seems that the client attempts to reconnect and the nats cluster seems to think it is already connected this behavior causes the node containing the pod to become unstable too much logs and iops any idea on how to solve this I started the server by natsstreamingserver sc c conf natsstreamingserver sc c conf natsstreamingserver sc c conf But there is some error info below looks like election is failed WRN STREAM raft Election timeout reached restarting election INF STREAM raft Node at stancluster stancluster Candidate entering Candidate state in term DBG STREAM raft Votes needed DBG STREAM raft Vote granted from in term Tally INF STREAM Shutting down ERR STREAM raft Failed to make RequestVote RPC to Voter stancluster stancluster nats connection closed DBG STREAM connection NSSstanclusterraft has been closed ERR STREAM raft Failed to make RequestVote RPC to Voter stancluster stancluster nats connection closed INF Initiating Shutdown INF rid Router connection closed INF rid Router connection closed Here is the config file the server is running on a same os c txt c txt c txt and the stan client cant work like this root da f bworlddatagosrcstangoexamplesstanpub stanpub cluster stancluster s nats svr msg Cant connect stan connect request timeout possibly wrong cluster ID Make sure a NATS Streaming Server is running at nats Weve the requirement to scale the cluster upto upto node but currently Ive benchmarked performance of the NATS streaming server for node and nodes and it seems node nats streaming server outperforms node cluster Is the NATS streaming server not horizontally scalable I have a node NATS cluster fairly basic configuration The nodes seem to communicate as the routing detail for all nodes is visible when connecting to an individual nodes web server However I have enabled clusterraftlogging at startup and it clearly shows that the raft election is not happening what am I missing I have compared these configurations to some other devtest clusters and I can see no obvious differences appropriate firewall ports are opened on all nodes nssnodeatxt nssnodebtxt nssnodectxt Hello We have Setup Single instance of natsstreaming in k s on c xlarge InMemory store writeDeadline s maxPayload maxConnections maxSubscriptions maxPending maxControlLine storelimits maxchannels maxmsgs maxbytes maxsubs maxage m maxinactivity m fdslimit Setup natsoperator x nodes natsstreamingoperator x nodes in k s on c xlarge Filestore mounted to emptyDirmedium Memory tmpfs writeDeadline s maxPayload maxConnections maxSubscriptions maxPending maxControlLine streaming storelimits maxchannels maxmsgs maxbytes maxsubs maxage m maxinactivity m fdslimit cluster sync false fileoptions synconflush false autosync in top it was cpu usage Our usual load looks like this k s nodes have antiAffinity and podAntiAffinity too Questions As you can see performance in cluster mode is ok for some period after start of cluster But over time it starts eating cpu and as result ack time increasing Do you have any ideas why it happens Our setup with single node in that environment works like a charm for months Can you please advise some configuration options to prevent cluster performance degradation Why difference between single node and x cluster so big It is because of raft Thnx The return value error of enforceLimits is always nil I feel like here should an error returned instead of logging it if err msremoveFirstMsgnil lockFile err nil We are not going to fail the publish just report the error removing the first message TODO Is this the right thing to do mslogErrorfUnable to remove first message v err return nil storesfilestorego func ms FileMsgStore enforceLimitsreportHitLimit lockFile bool error Check if we need to remove any but leave at least the last added Note that we may have to remove more than one msg if we are here after a restart with smaller limits than originally set or if message is quite big etc maxMsgs mslimitsMaxMsgs maxBytes mslimitsMaxBytes for mstotalCount maxMsgs mstotalCount maxMsgs maxBytes mstotalBytes uint maxBytes Remove first message from first slice potentially removing the slice etc if err msremoveFirstMsgnil lockFile err nil We are not going to fail the publish just report the error removing the first message TODO Is this the right thing to do mslogErrorfUnable to remove first message v err return nil if reportHitLimit mshitLimit mshitLimit true mslogWarnfdroppingMsgsFmt mssubject mstotalCount mslimitsMaxMsgs utilFriendlyBytesint mstotalBytes utilFriendlyBytesmslimitsMaxBytes return nil 