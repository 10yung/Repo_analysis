Hello I have a lot of http requests to make in my django project that uses celery for task processing I tried using eventlet and gevent as a pool executor and I immediately ran into problems when using eventlet Gevent seems to work I will give it more data to work on in the near future to see how it behaves Ive read in the source code that they both should patch themselves on the celery init The error that I got with eventlet is python DatabaseWrapper objects created in a thread can only be used in that same thread The object with alias default was created in thread id x and this is thread id y these are the most important requirements of the project aioredis amqp asgiref Babel billiard celery channels channelsredis daphne decorator Django djangoredis dnspython eventlet flower gevent greenlet hiredis kombu psycopg binary redis vine These are my celery settings CELERYBROKERURL redislocalhost CELERYRESULTBACKEND redislocalhost CELERYRESULTEXPIRES timedeltahours CELERYACCEPTCONTENT applicationjson CELERYTASKSERIALIZER json CELERYRESULTSERIALIZER json CELERYTIMEZONE TIMEZONE CELERYWORKERPREFETCHMULTIPLIER Ive read on stack overflow and such that it is mandatory to monkeypatch eventlet before anything else but that seem like its taken care of here If you need further information just let me know Im open on helping in coding and testing eventlet pool worker Hello I met a problem in the use of celery need help the problem is as follows our celery after receiving the task have been waiting for but does not perform task and we are executing task is empty this should not have to doAbove I use a flower monitoring data waiting for more tasks our celery is blocked I restart the service these services are performed again and normal Note Before submitting this pull request please review our contributing guidelines Description This fixes by adding reconnection logic to the Redis result backend which restores the pubsub context and deals with tasks that changed state while the connection was down Please describe your pull request NOTE All patches should be made against master not a maintenance branch like etc That is unless the bug is already fixed in master but not in that version series If it fixes a bug or resolves a feature request be sure to link to that issue via Fixes for example Please fill this template entirely and do not erase parts of it We reserve the right to close without a response bug reports which are incomplete Checklist To check an item on the list replace with x I have verified that the issue exists against the master branch of Celery This has already been asked to the discussion group first x I have read the relevant section in the contribution guide on reporting bugs x I have checked the issues list for similar or identical bug reports x I have checked the pull requests list for existing proposed fixes x I have checked the commit log to find out if the bug was already fixed in the master branch x I have included all related issues and possible duplicate issues in this issue If there are none check this box anyway Mandatory Debugging Information x I have included the output of celery A proj report in the issue if you are not able to do this then at least specify the Celery version affected I have verified that the issue exists against the master branch of Celery XXX Ive checked the commits since and I couldnt see anything related x I have included the contents of pip freeze in the issue x I have included all the versions of all the external dependencies required to reproduce this bug Optional Debugging Information Try some of the below if you think they are relevant It will help us figure out the scope of the bug and how many users it affects I have tried reproducing the issue on more than one Python version andor implementation I have tried reproducing the issue on more than one message broker andor result backend I have tried reproducing the issue on more than one version of the message broker andor result backend I have tried reproducing the issue on more than one operating system I have tried reproducing the issue on more than one workers pool I have tried reproducing the issue with autoscaling retries ETACountdown rate limits disabled I have tried reproducing the issue after downgrading andor upgrading Celery and its dependencies Related Issues and Possible Duplicates Please make sure to search and mention any related issues or possible duplicates to this issue as requested by the checklist above This may or may not include issues in other repositories that the Celery project maintains or other repositories that are dependencies of Celery If you dont know how to mention issues please refer to Githubs documentation on the subject Related Issues Couldnt find any This might very well be a kombu or pyamqp bug but its hard to tell from here Possible Duplicates None Environment Settings Include the contents of celery version below Celery version cliffs Include the output of celery A proj report below details summarybcodecelery reportcode Outputbsummary p software celery cliffs kombu py billiard pyamqp platform systemLinux arch bit kernel version amd impCPython loader celeryloadersappAppLoader settings transportamqp resultsrpc taskdefaultexchange konditoreiqueuedlifecycle workerpoolrestarts True taskdefaultqueue konditoreiqueuedlifecycle acceptcontent json timezone EuropeVienna resultserializer json resultbackend rpc brokerurl amqpcelerydb celery workerredirectstdoutslevel info taskserializer json taskqueues unbound Queue konditoreiqueuedlifecycle unbound Exchange konditoreiqueuedlifecycledirect konditoreiqueuedlifecycle taskdefaultroutingkey workerlogformat asctimes PIDprocessd processNamesnames levelnames messages resultexpires datetimetimedelta p details Steps to Reproduce Required Dependencies Please fill the required dependencies to reproduce this issue Minimal Python Version Unknown we use Minimal Celery Version Unknown we use Minimal Kombu Version Unknown we use Minimal Broker Version Unknown we use RabbitMQ Minimal Result Backend Version NA or Unknown Minimal OS andor Kernel Version We use kernel amd on Debian stretch Minimal Broker Client Version Unknown we use amqp Minimal Result Backend Client Version NA or Unknown Python Packages Please fill the contents of pip freeze below details summarybcodepip freezecode Outputbsummary p amqp appdirs attrs Babel backcall bcrypt billiard cachedproperty celery certifi cffi chardet cryptography debtcollector decorator defusedxml etxmlfile idna importlibmetadata ipython ipythongenutils iso isodate jdcal jedi Jinja jsonschema keystoneauth kombu lxml MarkupSafe moreitertools msgpack munch netaddr netifaces openpyxl osservicetypes osloconfig osloi n osloserialization osloutils paramiko parso pbr pexpect pickleshare prompttoolkit psycopg ptyprocess pycparser Pygments PyNaCl pyparsing pythondateutil pythonkeystoneclient pythonredislock pythonstdnum pythonswiftclient pytz PyYAML redis requests requeststoolbelt rfc six SQLAlchemy stevedore traitlets urllib vine wcwidth Werkzeug wrapt xmltodict zeep zipp local packages omitted p details Other Dependencies Please provide system dependencies configuration files and other dependency information if applicable details p NA p details Minimally Reproducible Test Case Please provide a reproducible test case Refer to the Reporting Bugs section in our contribution guide We prefer submitting test cases in the form of a PR to our integration test suite If you can provide one please mention the PR number below If not please attach the most minimal code example required to reproduce the issue below If the test case is too large please include a link to a gist or a repository below details Unclear Might not be related to our code in the first place details Expected Behavior Describe in detail what you expect to happen Startstop should not show any WARNINGs Actual Behavior Describe in detail what actually happened Please include a backtrace and surround it with triple backticks In addition include the Celery daemon logs the broker logs the result backend logs and system logs below if they will help us debug the issue We automatically reload our workers on deploys and this tends to happen in parallel systemctl reload celery which in turn calls binkill TERM MAINPID We also run multiple workers for different queues on the same machine they all have a dedicated n set This all works pretty nicely but out of reloads it logs something like this amqp WARNING Received method during closing channel This method will be ignored appears to be CancelOk In the log below you can also see weird Closed channel None stuff happening For this log Ive hacked in logging of the Channel and Connection object IDs so its a bit clearer which objects were involved Workers are started like this python m celery worker A konditoreiqueuedlifecycle Q konditoreiqueuedlifecycle n konditoreiqueuedlifecycledevworkers autoscale f srvlogscelerycelerykonditoreiqueuedlifecyclelog O fair loglevelINFO PID MainProcesscelerybootsteps DEBUG Worker Preparing bootsteps PID MainProcesscelerybootsteps DEBUG Worker Building graph PID MainProcesscelerybootsteps DEBUG Worker New boot order Timer Hub Pool Autoscaler StateDB Beat Consumer PID MainProcesscelerybootsteps DEBUG Consumer Preparing bootsteps PID MainProcesscelerybootsteps DEBUG Consumer Building graph PID MainProcesscelerybootsteps DEBUG Consumer New boot order Connection Events Mingle Tasks Control Agent Gossip Heart event loop PID MainProcesscelerybootsteps DEBUG Worker Starting Hub PID MainProcesscelerybootsteps DEBUG substep ok PID MainProcesscelerybootsteps DEBUG Worker Starting Pool PID MainProcesscelerybootsteps DEBUG substep ok PID MainProcesscelerybootsteps DEBUG Worker Starting Autoscaler PID MainProcesscelerybootsteps DEBUG substep ok PID MainProcesscelerybootsteps DEBUG Worker Starting Consumer PID MainProcesscelerybootsteps DEBUG Consumer Starting Connection PID MainProcessamqp DEBUG Start from server version properties clustername node db version copyright Copyright C Pivotal Software Inc informat ion Licensed under the MPL See platform ErlangOTP product RabbitMQ capabilities connectionblocked True exchangeexchangebindings True consumerpriorities True perconsumerqos True publi sherconfirms True directreplyto True basicnack True authenticationfailureclose True consumercancelnotify True mechanisms bPLAIN bAMQPLAIN locales enUS PID MainProcessceleryworkerconsumerconnection INFO Connected to amqpcelerydb celery PID MainProcesscelerybootsteps DEBUG substep ok PID MainProcesscelerybootsteps DEBUG Consumer Starting Events PID MainProcessamqp DEBUG Start from server version properties clustername node db version copyright Copyright C Pivotal Software Inc informat ion Licensed under the MPL See platform ErlangOTP product RabbitMQ capabilities connectionblocked True exchangeexchangebindings True consumerpriorities True perconsumerqos True publi sherconfirms True directreplyto True basicnack True authenticationfailureclose True consumercancelnotify True mechanisms bPLAIN bAMQPLAIN locales enUS PID MainProcesscelerybootsteps DEBUG substep ok PID MainProcesscelerybootsteps DEBUG Consumer Starting Mingle PID MainProcessceleryworkerconsumermingle INFO mingle searching for neighbors PID MainProcessamqp DEBUG kombutransportpyamqpChannel object at x fea a cac using channelid of connection kombutransportpyamqpConnection object at x fea a c f PID MainProcessamqp DEBUG Channel open PID MainProcessamqp DEBUG Start from server version properties clustername node db version copyright Copyright C Pivotal Software Inc informat ion Licensed under the MPL See platform ErlangOTP product RabbitMQ capabilities connectionblocked True exchangeexchangebindings True consumerpriorities True perconsumerqos True publi sherconfirms True directreplyto True basicnack True authenticationfailureclose True consumercancelnotify True mechanisms bPLAIN bAMQPLAIN locales enUS PID MainProcessamqp DEBUG kombutransportpyamqpChannel object at x fea a e using channelid of connection kombutransportpyamqpConnection object at x fea a a PID MainProcessamqp DEBUG Channel open PID MainProcessceleryworkerconsumermingle INFO mingle all alone PID MainProcesscelerybootsteps DEBUG substep ok PID MainProcesscelerybootsteps DEBUG Consumer Starting Tasks PID MainProcesscelerybootsteps DEBUG substep ok PID MainProcesscelerybootsteps DEBUG Consumer Starting Control PID MainProcessamqp DEBUG kombutransportpyamqpChannel object at x fea a ac using channelid of connection kombutransportpyamqpConnection object at x fea a c f PID MainProcessamqp DEBUG Channel open PID MainProcesscelerybootsteps DEBUG substep ok PID MainProcesscelerybootsteps DEBUG Consumer Starting Gossip PID MainProcessamqp DEBUG kombutransportpyamqpChannel object at x fea a d using channelid of connection kombutransportpyamqpConnection object at x fea a c f PID MainProcessamqp DEBUG Channel open PID MainProcesscelerybootsteps DEBUG substep ok PID MainProcesscelerybootsteps DEBUG Consumer Starting Heart PID MainProcessamqp DEBUG kombutransportpyamqpChannel object at x fea a e using channelid of connection kombutransportpyamqpConnection object at x fea a c d PID MainProcessamqp DEBUG Channel open PID MainProcesscelerybootsteps DEBUG substep ok PID MainProcesscelerybootsteps DEBUG Consumer Starting event loop PID MainProcesscelerybootsteps DEBUG Worker Hubregister Autoscaler PID MainProcesscelerybootsteps DEBUG Worker Hubregister Pool ready heartbeat but often no tasks on these development workers PID MainProcesscelerybootsteps DEBUG Worker Closing Hub PID MainProcesscelerybootsteps DEBUG Worker Closing Pool PID MainProcesscelerybootsteps DEBUG Worker Closing Autoscaler PID MainProcesscelerybootsteps DEBUG Worker Closing Consumer PID MainProcesscelerybootsteps DEBUG Worker Stopping Consumer PID MainProcesscelerybootsteps DEBUG Consumer Closing Connection PID MainProcesscelerybootsteps DEBUG Consumer Closing Events PID MainProcesscelerybootsteps DEBUG Consumer Closing Mingle PID MainProcesscelerybootsteps DEBUG Consumer Closing Tasks PID MainProcesscelerybootsteps DEBUG Consumer Closing Control PID MainProcesscelerybootsteps DEBUG Consumer Closing Gossip PID MainProcesscelerybootsteps DEBUG Consumer Closing Heart PID MainProcesscelerybootsteps DEBUG Consumer Closing event loop PID MainProcesscelerybootsteps DEBUG Consumer Stopping event loop PID MainProcesscelerybootsteps DEBUG Consumer Stopping Heart PID MainProcesscelerybootsteps DEBUG Consumer Stopping Gossip PID MainProcesscelerybootsteps DEBUG Consumer Stopping Control PID MainProcessamqp DEBUG Closed channel obj kombutransportpyamqpChannel object at x fea a d PID MainProcesscelerybootsteps DEBUG Consumer Stopping Tasks PID MainProcesscelerybootsteps DEBUG Consumer Stopping Mingle PID MainProcesscelerybootsteps DEBUG Consumer Stopping Events PID MainProcesscelerybootsteps DEBUG Consumer Stopping Connection PID MainProcesscelerybootsteps DEBUG Worker Stopping Autoscaler PID MainProcesscelerybootsteps DEBUG Worker Stopping Pool PID MainProcesscelerybootsteps DEBUG Worker Stopping Hub PID MainProcesscelerybootsteps DEBUG Consumer Shutdown Heart PID MainProcesscelerybootsteps DEBUG Consumer Shutdown Gossip PID MainProcesscelerybootsteps DEBUG Consumer Shutdown Control PID MainProcesscelerybootsteps DEBUG Consumer Shutdown Tasks PID MainProcesscelerybootsteps DEBUG Consumer Shutdown Events PID MainProcessamqp DEBUG Closed channel obj kombutransportpyamqpChannel object at x fea a e PID MainProcesscelerybootsteps DEBUG Consumer Shutdown Connection PID MainProcessamqp DEBUG Closed channel obj kombutransportpyamqpChannel object at x fea a ac PID MainProcessamqp WARNING Received method during closing channel This method will be ignored PID MainProcessamqp DEBUG Closed channel obj kombutransportpyamqpChannel object at x fea a cac PID MainProcessamqp DEBUG Closed channel None obj kombutransportpyamqpChannel object at x fea a ac PID MainProcessamqp DEBUG Closed channel None obj kombutransportpyamqpChannel object at x fea a d Please fill this template entirely and do not erase parts of it We reserve the right to close without a response enhancement requests which are incomplete Checklist To check an item on the list replace with x x I have checked the issues list for similar or identical enhancement to an existing feature x I have checked the pull requests list for existing proposed enhancements x I have checked the commit log to find out if the if the same enhancement was already implemented in the master branch x I have included all related issues and possible duplicate issues in this issue If there are none check this box anyway Related Issues and Possible Duplicates Please make sure to search and mention any related issues or possible duplicates to this issue as requested by the checklist above This may or may not include issues in other repositories that the Celery project maintains or other repositories that are dependencies of Celery If you dont know how to mention issues please refer to Githubs documentation on the subject Related Issues None Possible Duplicates None Brief Summary Please include a brief summary of what the enhancement is and why it is needed When there is a connection error with Redis while executing a command in most cases the redis client will discard the connection causing the next command sent to Redis to open a new connection This allows applications to recover from connection errors by simply retrying a property that is used in Celery for example when setting keys in the Redis result backend This is not the case however when the connection to Redis is in a pubsub state The reason for that is that some state associated with the connection namely the list of keys subscibed to The Redis client doesnt keep track of this state so it cant possibly restore it when creating a new connection and leaves the connection handling to the application code The Celery Redis result consumer uses pubsub in order to be notified when results are available but doesnt handle connection errors at all causing a result consumer to end up in a state where it cant connect to the result backend any more after a single connection error as any further attempt will reuse the same faulty connection The solution would be to add error handling logic to the result consumer so it will recreate the connection on connection errors and initialize it to the proper state Design Architectural Considerations If more components other than Celery are involved describe them here and the effect it would have on Celery None Proposed Behavior Please describe in detail how this enhancement is going to change the behavior of an existing feature Describe what happens in case of failures as well if applicable Add error handling in all places the Redis result consumer sends a Redis command in a pubsub context We should catch all Redis connection errors and call a new method that will reinitialize a pubsub connection in the proper state discard the current connection from the pool start the pubsub context subscribe to all keys in ResultConsumersubscribedto using the retry policy If in drainevents we should try to get new messages again This will take care of most issues with connection errors I see two remaining issues Some message might have been lost sent between losing the connection and reconnecting We could read all keys subscribed to right after reconnecting and before starting the pubsub context and call onstatechange for each existing key but this might cause some messages to be delivered twice and I dont know how Celery will react to that If the connection cant be reestablished despite the retries and reaches maxretries the result consumer will end up with a faulty connection that cant be recovered from This should be communicated somehow to the user documentation logging an explicit error message custom exception Proposed UIUX Please provide your ideas for the API CLI options configuration key names etc that will be adjusted for this enhancement None Diagrams Please include any diagrams that might be relevant to the implementation of this enhancement such as Class Diagrams Sequence Diagrams Activity Diagrams You can drag and drop images into the text box to attach them to this issue NA Alternatives If you have considered any alternative implementations describe them in detail below None Note Before submitting this pull request please review our contributing guidelines Description Please describe your pull request NOTE All patches should be made against master not a maintenance branch like etc That is unless the bug is already fixed in master but not in that version series If it fixes a bug or resolves a feature request be sure to link to that issue via Fixes for example Please fill this template entirely and do not erase parts of it We reserve the right to close without a response bug reports which are incomplete Checklist To check an item on the list replace with x x I have verified that the issue exists against the master branch of Celery This has already been asked to the discussion group first x I have read the relevant section in the contribution guide on reporting bugs x I have checked the issues list for similar or identical bug reports x I have checked the pull requests list for existing proposed fixes I have checked the commit log to find out if the bug was already fixed in the master branch I have included all related issues and possible duplicate issues in this issue If there are none check this box anyway Mandatory Debugging Information I have included the output of celery A proj report in the issue if you are not able to do this then at least specify the Celery version affected x I have verified that the issue exists against the master branch of Celery I have included the contents of pip freeze in the issue I have included all the versions of all the external dependencies required to reproduce this bug Optional Debugging Information Try some of the below if you think they are relevant It will help us figure out the scope of the bug and how many users it affects x I have tried reproducing the issue on more than one Python version andor implementation x I have tried reproducing the issue on more than one message broker andor result backend x I have tried reproducing the issue on more than one version of the message broker andor result backend x I have tried reproducing the issue on more than one operating system x I have tried reproducing the issue on more than one workers pool x I have tried reproducing the issue with autoscaling retries ETACountdown rate limits disabled I have tried reproducing the issue after downgrading andor upgrading Celery and its dependencies Related Issues and Possible Duplicates Please make sure to search and mention any related issues or possible duplicates to this issue as requested by the checklist above This may or may not include issues in other repositories that the Celery project maintains or other repositories that are dependencies of Celery If you dont know how to mention issues please refer to Githubs documentation on the subject Related Issues I use mongo broker and mongo backend There is a decoding error while try to get task result with AsyncResult File taskresultpy line in module printresresult python Lib json initpy line in loads raise TypeErrorfthe JSON object must be str bytes or bytearray kombuexceptionsDecodeError the JSON object must be str bytes or bytearray not dict taskspy celerytasknamewebadd bindTrue def addtestself x timesleep message IN WORKER selfupdatestatestatePROGRESS meta current total status message timesleep message END return current total status message result videourl videoplayerurl taskresultpy res addAsyncResultd f d b f printres printresresult Possible Duplicates None Environment Settings Include the contents of celery version below Celery version Include the output of celery A proj report below details summarybcodecelery reportcode Outputbsummary p p details Steps to Reproduce Required Dependencies Please fill the required dependencies to reproduce this issue Minimal Python Version NA or Unknown Minimal Celery Version NA or Unknown Minimal Kombu Version NA or Unknown Minimal Broker Version NA or Unknown Minimal Result Backend Version NA or Unknown Minimal OS andor Kernel Version NA or Unknown Minimal Broker Client Version NA or Unknown Minimal Result Backend Client Version NA or Unknown Python Packages Please fill the contents of pip freeze below details summarybcodepip freezecode Outputbsummary p p details Other Dependencies Please provide system dependencies configuration files and other dependency information if applicable details p NA p details Minimally Reproducible Test Case Please provide a reproducible test case Refer to the Reporting Bugs section in our contribution guide We prefer submitting test cases in the form of a PR to our integration test suite If you can provide one please mention the PR number below If not please attach the most minimal code example required to reproduce the issue below If the test case is too large please include a link to a gist or a repository below details p python p details Expected Behavior result current total status IN WORKER Actual Behavior Describe in detail what actually happened Please include a backtrace and surround it with triple backticks In addition include the Celery daemon logs the broker logs the result backend logs and system logs below if they will help us debug the issue Please fill this template entirely and do not erase parts of it We reserve the right to close without a response bug reports which are incomplete Checklist To check an item on the list replace with x x I have verified that the issue exists against the master branch of Celery This has already been asked to the discussion group first x I have read the relevant section in the contribution guide on reporting bugs x I have checked the issues list for similar or identical bug reports I have checked the pull requests list for existing proposed fixes I have checked the commit log to find out if the bug was already fixed in the master branch I have included all related issues and possible duplicate issues in this issue If there are none check this box anyway Mandatory Debugging Information I have included the output of celery A proj report in the issue if you are not able to do this then at least specify the Celery version affected x I have verified that the issue exists against the master branch of Celery I have included the contents of pip freeze in the issue I have included all the versions of all the external dependencies required to reproduce this bug Optional Debugging Information Try some of the below if you think they are relevant It will help us figure out the scope of the bug and how many users it affects I have tried reproducing the issue on more than one Python version andor implementation I have tried reproducing the issue on more than one message broker andor result backend I have tried reproducing the issue on more than one version of the message broker andor result backend I have tried reproducing the issue on more than one operating system I have tried reproducing the issue on more than one workers pool I have tried reproducing the issue with autoscaling retries ETACountdown rate limits disabled I have tried reproducing the issue after downgrading andor upgrading Celery and its dependencies Related Issues and Possible Duplicates Please make sure to search and mention any related issues or possible duplicates to this issue as requested by the checklist above This may or may not include issues in other repositories that the Celery project maintains or other repositories that are dependencies of Celery If you dont know how to mention issues please refer to Githubs documentation on the subject Related Issues None Possible Duplicates None Environment Settings Include the contents of celery version below Celery version Include the output of celery A proj report below details summarybcodecelery reportcode Outputbsummary p p details Steps to Reproduce Required Dependencies Please fill the required dependencies to reproduce this issue Minimal Python Version Minimal Celery Version Minimal Kombu Version NA or Unknown Minimal Broker Version NA or Unknown Minimal Result Backend Version NA or Unknown Minimal OS andor Kernel Version NA or Unknown Minimal Broker Client Version NA or Unknown Minimal Result Backend Client Version NA or Unknown Python Packages Please fill the contents of pip freeze below details summarybcodepip freezecode Outputbsummary p p details Other Dependencies Please provide system dependencies configuration files and other dependency information if applicable details p NA p details Minimally Reproducible Test Case Please provide a reproducible test case Refer to the Reporting Bugs section in our contribution guide We prefer submitting test cases in the form of a PR to our integration test suite If you can provide one please mention the PR number below If not please attach the most minimal code example required to reproduce the issue below If the test case is too large please include a link to a gist or a repository below details p python start timetime printcelerycontrolpingtimeout end timetime printend start p details Expected Behavior I would expect ping to return as soon as it gets a response from the celery workers Actual Behavior Whatever value the timeout is set to it seems to add that value in time to the check running the above is returning values such as if the timeout is set to I get values such as The docs state timeout float Timeout in seconds to wait for the reply It appears to be waiting for at least the timeout amount the wait for the reply Is my interpretation of timeout here wrong This will show current retry progress so it will clear confusion about how many retries will be tried for connecting to broker I still leave the default value in because If I change it Im afraid it will affect many celery apps that was okay or used to with the current default value in brokerconnectionmaxretries May fixes python version package version celery flower kombu redis tornado when I started flower wirh celery like this celery flower brokerredis I command Visit me at I command Broker redis I command Registered tasks celeryaccumulate celerybackendcleanup celerychain celerychord celerychordunlock celerychunks celerygroup celerymap celerystarmap Fatal Python error Cannot recover from stack overflow Thread x ce most recent call first File Usersyeyuqiupyenvversions libpython socketpy line in realclose File Usersyeyuqiupyenvversions libpython socketpy line in close File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in disconnect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackageskombutransportredispy line in disconnect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in readresponse File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in checkhealth File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in onconnect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in connect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in checkhealth File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in onconnect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in connect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in checkhealth File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in onconnect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in connect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in checkhealth File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in onconnect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in connect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in checkhealth File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in onconnect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in connect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in checkhealth File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in onconnect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in connect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in checkhealth File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in onconnect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in connect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in checkhealth File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in onconnect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in connect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in checkhealth File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in onconnect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in connect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in checkhealth File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in onconnect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in connect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in checkhealth File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in onconnect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in connect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in checkhealth File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in onconnect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in connect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in checkhealth File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in onconnect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in connect File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in checkhealth File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendpackedcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in sendcommand File Usersyeyuqiupyenvversions envsschedulelibpython sitepackagesredisconnectionpy line in onconnect 