The pretrained model provided from you here is in tsv format However you mention in documentation that tsv format is mainly for small dataset and the size of that pretrained model after decompressing is around GB making it very slow to convert that into a numpy array as also noted in the documentation Is there any way to convert this tsv file in hdf or is there any pretrained model already in hdf format Types of changes Docs change refactoring dependency upgrade Bug fix nonbreaking change which fixes an issue x New feature nonbreaking change which adds functionality Breaking change fix or feature that would cause existing functionality to change Motivation and Context Related issue Add an interface so its easy to support different edgelist file formats Add untested support for Parquet in resposne to How Has This Been Tested if it applies Unit tests pass python torchbiggraphexamplesfb kpy I havent tested Parquet yet Checklist Go over all the following points and put an x in all the boxes that apply If youre unsure about any of these dont hesitate to ask Were here to help x The documentation is uptodate with the changes I made x I have read the CONTRIBUTING document and completed the CLA see CONTRIBUTING x All tests passed and additional code has been covered with new tests Types of changes What types of changes does your code introduce Put an x in all the boxes that apply Docs change refactoring dependency upgrade x Bug fix nonbreaking change which fixes an issue New feature nonbreaking change which adds functionality Breaking change fix or feature that would cause existing functionality to change Motivation and Context Related issue Why is this change required What problem does it solve The train failed if compute time or io time is zero Please link to an existing issue here if one exists we recommend to have an existing issue for each pull request How Has This Been Tested if it applies It was tested on my local machine with different values of the computetime and iotime variables Checklist Go over all the following points and put an x in all the boxes that apply If youre unsure about any of these dont hesitate to ask Were here to help The documentation is uptodate with the changes I made I have read the CONTRIBUTING document and completed the CLA see CONTRIBUTING All tests passed and additional code has been covered with new tests Steps to reproduce Run traintrainandreportstats with small quantity of data on a fast machine Observed Results Script failed with a stack trace File d a s ETL trainpy line in train for i stats in tbgtraintrainandreportstatsconfig File C hostedtoolcache windows Python x lib sitepackages torchbiggraph trainpy line in trainandreportstats fbucket totalbuckets remaining totalbuckets ZeroDivisionError float division by zero error C hostedtoolcache windows Python x pythonexe failed with return code It looks that iotime or computetime is zero Expected Results Do not divide on zero time Relevant Code traintrainandreportstatsconfig This comes from me wanting to try BigGraph on a dataset of moderate size mb but its a little too large to be exported into tsv files In particular I saw that in your converter you iterate over all lines of tsv individually in python which is going to be really slow when over million edges are involved Have you considered adding parquet as a supported input file format The following table contains the MRR and Hits values I got for the live journal dataset when it was trained with learning rate and epochs Generally these evaluation metrics results have increased when the number of partitions increases I observed similar results for two other edge lists However the results given in the paper for FB k freebase datasets have negligible changes when the number of partitions increases Is there any specific reason to clarify this behaviour No of partitions MRR Hits Hits Hits Hi Can I know the values used as learning rate batch size etc when training Freebase knowledge graph with different number of partitions If you have performed distributed training for other graph datasets too can I know the hyperparameters with the best performance for each number of partitions I am trying to train a few models with PBG and I am wondering how I could get a random baseline of the model Also is there any standardconvention on interpreting the performance eg what is a reasonable pos rankMRR Thanks Currently FilteredRankingEvaluator can be used only when dynamicrel is used Id like to improve the class to support also the non dynamic mode Before I start doing it there are any caveats you are aware of that I should consider 