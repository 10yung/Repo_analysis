Is your feature request related to a problem Please describe The TcpConnection retries times until it gives up If the service the plugin tries to connect is not available within those retries the plugin just gives up The only way to resolve this is to restart the hm job on the BOSH vm Describe the solution youd like Remove the limit and let plugins retry forever Since it has been configured the plugin should successfully connect as soon as the service is available even days later I dont see a scenario where an operator actually want the plugin to give up Describe alternatives youve considered Work around this by monitoring for the error and restarting the health monitor job Is your feature request related to a problem Please describe In this guide you explain how to rotate Nats CA certificates In step one you add the new CA certificates and in step you set everything to use the new CA The issue is if you forget to remove this ops file after running the step renaming and removing the new certs to the name of the old CA certificate you will get a new set of certificates and all agents will be unresponsive In the same fashion you can use the Step Ops file in the first step resulting in unresponsive agents as bosh expects the new certs Those issues can be avoided by removing the variables block from the second ops file If you ran it with ops file first the vars are already present If you ran the ops file that replaces the old Certificate with the new ones the script will error as it will do if you use the second ops file on the first run This makes sure you do not do the Rotation in the wrong order Describe the solution youd like Change the second ops file to yaml type replace path instancegroupsnameboshpropertiesnatstlsca value natsservertls ca type replace path instancegroupsnameboshpropertiesnatstlsserver value certificate natsservertls certificate privatekey natsservertls privatekey type replace path instancegroupsnameboshpropertiesnatstlsclientca value certificate natsca certificate privatekey natsca privatekey type replace path instancegroupsnameboshpropertiesnatstlsdirector value certificate natsclientsdirectortls certificate privatekey natsclientsdirectortls privatekey type replace path instancegroupsnameboshpropertiesnatstlshealthmonitor value certificate natsclientshealthmonitortls certificate privatekey natsclientshealthmonitortls privatekey aka remove all the variables addition This will make sure that you get an error if you try to run the ops files out of order by accident also the variables should already be there at this point Describe alternatives youve considered Changing the documentation so first you move the old certificate to old and change all scripts so no variable block is needed at any point and the ops for the first step uses old for most things and later ops files get shorter Problem Rotating the NATS and blobstore CAs through all of the director and deployed VMs takes an immense amount of time due to the need for deployed VMs to be recreated to update the certificates The certificates have essentially no meaning to operators as theyre created during createenv and distributed as VMs are deployed The only way to transfer new NATSBlobstore CAscerts to VMs is via a recreate of the entire VM This is an optimization that can be performed when needing to rotate the CAs The threelegged deploy should still be performed if operators are concerned about leaked certificates Proposal Given a healthy director an operator can execute some CLI command that executes a task that will update director and deployed VM NATSblobstore certificates without the operator recreating all deployed VM or the director VM Director Given a director execute a bosh cli command that Starts a task that can be tracked Shows each instance being updated during task execution Handles failing VMs gracefully allowing idempotent retries Distributes the new CA and leaves to deployed VMs Accepts the varsstore option in order to update the credsyml file after completion Director reloads gnatsd when appropriate Agents A deployed VM with an agent that has successfully checked in recently will Receive a message over NATS to begin the process Accept a new NATS CA and leaf certificate via message from director Install the new CA concatenated with the old CA Respond to ping over NATS using the new leaf certificate Clean up unused artifacts from the process Problems Security of transferring new credentials over a potentially compromised channel A VM may fail to respond to a ping with the new leaf certificate for reasons other than TLS The task could be cancelled by operators There could be other tasks going on at once that use NATS everything Locking the entire director and deployed VMs may be unreasonable Potential Solutions to above An agent could generate a keypair and send the public key to the director to encrypt the new CA and generated leaf If an attacker can respond as the director with a compromised CA then using the public key could send a replacement certificate to an agent Operators should default to the recreationbased approach in the event of a compromised NATS CA Process should be idempotent and write enough state so the process can be resumed at any time Old and new CAs are concatenated as the certificate authority Retry a couple times quickly If the task is cancelled then if the process is idempotent in all steps then the task could be resumed VMs should still be able to health check Deployment locks work on one deployment at a time Director may need to have a manual start and finish command in order for operators to update deployments at their leisure Expecting all deployments to be rotated at once is unreasonable SecurityRisks What if the NATS CA is compromised The DH key exchange would help here but if the attacker has access to the director VM they could also generate the keypair At the moment the recreate is helpful as the channel for credentials is the cloud providers metadata service This isnt necessarily secure at all but at least doesnt use the same channel for its own credential update Operators concerned about the proposed optimization can still perform the threestep approach Health monitor and updating the VMs may cause some chaos if the check intervals and updates are timed in a certain way The time it takes for gnatsd on the director to reload and the reloadreestablish connection to NATS server from a VM could result in timeouts and necessitate more retries Performance Clearly there will be a gain in performance as operators are not required to recreate VMs and not reinstallstart jobs back up again Any downtime should also be eradicated The key exchange shouldnt be that intensive Concurrency rules like maxinflight can still be honoured Waiting for gnatsd on the director to reload and agent reconnects could take some time but much less than a whole recreate would Accompanying Picture Screen Shot at PM Is your feature request related to a problem Please describe Currently instance group updating order is supported not individual instances in a group But one of my customer believes Its desirable to have newly added diegocells to be updated in the first place among all other diegocells because theyd prefer those new cells to be ready first so that redundancy to other cells could be increasing during upgrade Describe the solution youd like Its desirable to have newly added diegocells to be updated in the first place among all other diegocells If possible wed like bosh can support updating order of specific instances in an instance group Additional context Slack conversation Is your feature request related to a problem Please describe We ship our functionality APM monitoring almost exclusively over BOSH runtimeconfigurations Currently it is pretty hard for us to know if our jobs are going to be purged from a machine due to the removal of the runtime configuration and the update of the deployment We need to look up the deployment manifest Describe the solution youd like Get a clear indication of This job wont be here again after being stopped which is pretty much the semantics I would expect from a BOSHJOBNEXTSTATE delete env var in prestop Describe alternatives youve considered Over the current prestop we look up the runtime configurations listed in the BOSH director over the REST API and if the one that deploys the current job is no longer listed we do longterm cleanup Unfortunately it requires IO to the BOSH Rest API and a boshread token accessible from the VM wed rather not have there Additional context NA Describe the bug After upgrade from x to x line of boshawsxenhvmubuntuxenialgoagent stemcell we noticed issues with resolving hostnames that are not FQDN It looks that the newest stemcell versions missing search ec internal search domain entry in etcresolvconf To Reproduce On instance based on boshawsxenhvmubuntuxenialgoagent cat etcresolvconf nameserver search ec internal On instance based on boshawsxenhvmubuntuxenialgoagent cat etcresolvconf nameserver Expected behavior hostnames pointing to other internal EC instances will be resolved properly We have a cluster with instances VM and VM in AZ and need to distribute to AZs AZ and AZ together with some updates each AZ has an instance to make better HA The current BOSH update strategy on this task is Delete unneeded VM in AZ Create a new VM with no jobs installed in AZ The above two steps may be in different order which has the same result now we have only one working instance VM Update VM in AZ During the update there is no working instances and the outage happens Update VM in AZ Is it possible for BOSH to make better strategy for example put the delete unneeded instance at the end of the task So that we always has a working instance to avoid outage Describe the bug When a bosh release gets uploaded it does appear in the existing releases before the last upload steps are executed and the task is finished Subsequent deploys assume that the release exists even if the upload is not yet finalized and fail with job is not found in the template table The timeframe in which this can happen increases with the size of the bosh release and may only be problematic with releases of a certain size To Reproduce Create a huge bosh release GB Create two manifests which download and use the bosh release Run the following script bosh deploy firstmanifestyml n while true do releasebosh curl releases jq r selectname releasename releaseversions selectversion releaseversion if n release then break fi echo release upload pending sleep done bosh deploy secondmanifestyml n Expected behavior A release is only returned by the releases endpoint after the upload has completely finished and the release is ready to use Versions please complete the following information Infrastructure does not matter BOSH version xx BOSH CLI version x Stemcell version xenial x Describe the bug During stemcell upgrade we noticed that Cloud Controller is failing to talk to one of the doppler instances We have doppler instances One of the instances was being updated and Cloud Controller was trying to talk to that IP address We confirmed that issue by running dig dopplerservicecfinternal and it was returning all IPs even for VMs that were being updated After bringing the issue with the Bosh team we were suggested to update the doppler host URL to include the healthiness filter But it seems like they dont work With instances of doppler running qs dopplerdefaultcfbosh returns instances But both dig qs dopplerservicecfinternal and dig qs dopplerservicecfinternal return instances Documentation states that the qs should return unhealthy and qs healthy instances We ran bosh recreate dopplersomeguid and watched the above commands The output has not changed Cloud controller is configured with doppler service URL as dopplerservicecfinternal and we use cfdeployment Slack conversation with bosh team To Reproduce Steps to reproduce the behavior Deploy a bosh director on GCP with bbl Deploy cfdeployment and opsfile bosh ssh to a api Run watch dig qs dopplerservicecfinternal watch dig qs dopplerservicecfinternal watch dig qs dopplerservicecfinternal Run bosh recreate doppler Expected behavior at some point dig qs dopplerservicecfinternal should return instances instead of dig qs dopplerservicecfinternal should return instance instead of dig qs dopplerservicecfinternal should return instances instead of Versions please complete the following information Infrastructure GCP BOSH version Stemcell version eg ubuntuxenial cfdeployment version dfd ded ea f b e c Deployment info Deployment was deployed with concourse Task config Is your feature request related to a problem Please describe As a bosh operator in order to troubleshoot failing communications with credhub config server I need to access https wire traces for bosh to credhub communications Describe the solution youd like bosh debug logs to include config server api calls wire traces when directorloglevel is set to debug or more verbose trace level Describe alternatives youve considered Manually patching the bosh director package ruby calls to turn on http client verbose logging Caveats ephemeral the patch may not survive bosh director stemcell upgrades no clear whether logs will end up in bosh logs debug output or stdout and require sshing into director box and be rotated soon by log rotation whereas bosh logs debug might be more durable in director db Additional context Some bugs such as produce the following stack trace Credhub HTTPS response seem useful to troubleshoot the root cause T task DEBUG DirectorJobRunner s conn COMMIT E T task ERROR DirectorJobRunner wrong chunk size line varvcapdatapackagesruby r a ea cae dc b cdf c cddelibruby nethttpresponserb in readchunked varvcapdatapackagesruby r a ea cae dc b cdf c cddelibruby nethttpresponserb in block in readbody varvcapdatapackagesruby r a ea cae dc b cdf c cddelibruby nethttpresponserb in inflater varvcapdatapackagesruby r a ea cae dc b cdf c cddelibruby nethttpresponserb in readbody varvcapdatapackagesruby r a ea cae dc b cdf c cddelibruby nethttpresponserb in readbody varvcapdatapackagesruby r a ea cae dc b cdf c cddelibruby nethttprb in block in get varvcapdatapackagesruby r a ea cae dc b cdf c cddelibruby nethttprb in block in transportrequest varvcapdatapackagesruby r a ea cae dc b cdf c cddelibruby nethttpresponserb in readingbody varvcapdatapackagesruby r a ea cae dc b cdf c cddelibruby nethttprb in transportrequest varvcapdatapackagesruby r a ea cae dc b cdf c cddelibruby nethttprb in request varvcapdatapackagesruby r a ea cae dc b cdf c cddelibruby nethttprb in block in request varvcapdatapackagesruby r a ea cae dc b cdf c cddelibruby nethttprb in start varvcapdatapackagesruby r a ea cae dc b cdf c cddelibruby nethttprb in request varvcapdatapackagesruby r a ea cae dc b cdf c cddelibruby nethttprb in get varvcapdatapackagesdirector f d d cb e a f ee bd f fefgemhomeruby gemsboshdirector libboshdirectorconfigserverauthhttpclientrb in block in get varvcapdatapackagesdirector f d d cb e a f ee bd f fefgemhomeruby gemsboshcommon libcommonretryablerb in block in retryer varvcapdatapackagesdirector f d d cb e a f ee bd f fefgemhomeruby gemsboshcommon libcommonretryablerb in loop varvcapdatapackagesdirector f d d cb e a f ee bd f fefgemhomeruby gemsboshcommon libcommonretryablerb in retryer varvcapdatapackagesdirector f d d cb e a f ee bd f fefgemhomeruby gemsboshdirector libboshdirectorconfigserverauthhttpclientrb in get varvcapdatapackagesdirector f d d cb e a f ee bd f fefgemhomeruby gemsboshdirector libboshdirectorconfigserverretryablehttpclientrb in block in get varvcapdatapackagesdirector f d d cb e a f ee bd f fefgemhomeruby gemsboshcommon libcommonretryablerb in block in retryer varvcapdatapackagesdirector f d d cb e a f ee bd f fefgemhomeruby gemsboshcommon libcommonretryablerb in loop varvcapdatapackagesdirector f d d cb e a f ee bd f fefgemhomeruby gemsboshcommon libcommonretryablerb in retryer varvcapdatapackagesdirector f d d cb e a f ee bd f fefgemhomeruby gemsboshdirector libboshdirectorconfigserverretryablehttpclientrb in get varvcapdatapackagesdirector f d d cb e a f ee bd f fefgemhomeruby gemsboshdirector libboshdirectorconfigserverconfigserverhttpclientrb in get varvcapdatapackagesdirector f d d cb e a f ee bd f fefgemhomeruby gemsboshdirector libboshdirectorconfigserverclientrb in getvariableidandvaluebyname varvcapdatapackagesdirector f d d cb e a f ee bd f fefgemhomeruby gemsboshdirector libboshdirectorconfigserverclientrb in block in fetchvalueswithdeployment varvcapdatapackagesdirector f d d cb e a f ee bd f fefgemhomeruby gemsboshdirector libboshdirectorconfigserverclientrb in each varvcapdatapackagesdirector f d d cb e a f ee bd f fefgemhomeruby gemsboshdirector libboshdirectorconfigserverclientrb in fetchvalueswithdeployment varvcapdatapackagesdirector f d d cb e a f ee bd f fefgemhomeruby gemsboshdirector libboshdirectorconfigserverclientrb in interpolatewithversioning 