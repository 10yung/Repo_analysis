A natural extension of AlphaZero is MuZero The pseudocode implementation is very similar to that of AlphaZero so it would be interesting to extend the AlphaZero implementation to also support MuZero Is there any interest in having a MuZero implementation in OpenSpiel This AlphaZero implementation has many design issues that need to be resolved before finalizing the PR eg there are no tests and input validation is not productionready yet It trains well on tictactoe see examplestictactoealphazeropy giving confidence that the implementation is correct The philosophy of this implementation is to follow the pseudocode of the AlphaZero Science paper as closely as possible and also fit it into the current OpenSpiel framework with minimal framework changes But here are some differences with the pseudocode It uses very large batch sizes and uses gradient accumulation to allow this to fit into memory However the AlphaZero ResNet architecture uses batchnorm which doesnt play well with gradient accumulation So Ive avoided this Ive used ADAM as the default optimizer whilst the pseudocode uses SGD with momentum and effectively a tftrainpiecewiseconstant learning rate scheduler Ive found this very annoying to use for adapting to new games as there are now some extra fiddly hyperparameters that need tweaking The loss was not scaled by the batch size in the pseudocode This makes it harder to change the batch size as other hyperparameters are now coupled to it eg learning rate l regularization So this implementation does normalize by batch size It doesnt support nonvector actions representations eg what they did for Chess But the paper does also mention that We also tried using a flat distribution over moves for chess and shogi the final result was almost identical although training was slightly slower So left this feature out for now Missing support for maxmoves this seems like a feature to add to the Game class but it currently seems hardcoded No clean way of handling almostMarkovian environments eg Chess and Go by training on some context window of previous observations This might be best handled at the gamelevel or am I missing some library feature Some design explanations and issues The main frameworklevel change is to introduce a TrainableEvaluator class that acts like the Evaluator class but also has an update method with training logic doesnt have to be neural net training It also builds caching into the design due to how common using nets with both value and policy heads is so subclasses would only implement the valueandprior method More on caching in mctsTrainableEvaluator The current design of mctsEvaluator is that evaluate and prior are called separately during mcts search In the case where these are expensively computed jointly as in AlphaZero this is inefficient I am wellaware of the general injunction that OpenSpiel provides reference implementations that are used to learn from and prototype with rather than fullyoptimized highperformance code So I am aware that this caching mechanism might be contentious but selfplay is the bottleneck and makes AlphaZero a lot less pleasant to use The AlphaZero training loop this implementation ties the number of training iterations to the current buffer size via numtrainingepochs in AlphaZeroupdate This has some virtues the epoch number to train over is a much more gameindependent hyperparameter and makes more sense than the AlphaZero pseudocode approach do a huge number of training iterations even when the buffer hasnt been filled up yet One option I considered first is to make AlphaZeroupdate do a single training iteration which gives most flexibility to users But the optimizer needs to be reset after all training iterations are done The user could call a reset method which is ugly and easy to forget to do Otherwise the optimizer reset could be done in AlphaZeroselfplay But this side effect of AlphaZeroselfplay also feels a bit hacky Im definitely not confident Ive chosen the right design here Device flags for NN I didnt see any other examples of GPU support in OpenSpiel So went ahead and allowed the strings gpu cpu or a tfdevice object useful to specify more exotic things like a particular GPU on a multiGPU machine Or maybe some other device type like TPU Neural nets In all other algorithms that use neural nets there is a lack of flexibility in specifying architectures This could well be intentional makes it easier to write backend independent implementations as there is a nice backend agnostic interface with only knobs like hiddenlayerssizes that can be tweaked Ive gone against this and allowed any Keras model to be supplied to AlphaZeroKerasEvaluator along with allowing flexible feature extraction Ive also focused on Keras as its official highlevel TF API I assume OpenSpiel will move to TF at some stage AlphaZero can probably be extended to more than agents and the MCTS implementation already supports that But that can be a future extension as this implementation only supports player games As a devoted gin player I d like to submit this gin rummy implementation to OpenSpiel s game collection Gin is the best representative of the rummy family of games It s quite a large game with over info states and states per info state off the deal Gin also has two qualities which distinguish it from hold em poker hidden information is regularly added throughout the game not just on the deal and it has a much larger disambiguation factor I ve made every effort to ensure the correctness of the code The test file is extensive and should be instructive to a non gin rummy player I ve carefully played through many games myself And further I wrote up a simple bot that employs a reasonably effective strategy which successfully played two million games Hi there dont know if this is the right place to ask this but yet I dont have any other Contact Details so Im trying it this way Yesterday I got an idea about an algorithm for solving imperfect games Basically it it a cross between deepctr and exploitability decent The main idea behind it is to train a deep Policy network directly with exploitability decent I made the implementation yesterday and well here it is This works very well for Kuhn poker and the exploitability reaches near nash equilibrum But for leduc poker convergence is very slow and gets stuck at approx Do you have any tips how to improve my algorithm that I can contribute it to openspiel Best regards Dennis I am interested in applying and contributing to this framework and have some questions and remarks My background I am a data scientist app developer and a boardgame enthusiast As a hobby project last year I have implemented a webapp an online version of a multiplayer strategic boardgame and now I am think about developing an AI for it so players can choose to play against AI bots instead of against other players I might implement more games in the future as well I guess this framework is a great starting point for me Some information on the game The game is named Ponzi Scheme more info on Some key characteristics economic trading game hidden actions a few chance events three or more players The implementation of the game is in Python on the the server side I think it wont be too difficult to adapt the implementation to the openspiel API Most important change will be handling the chance events ie adding a chance player instead of handling chance events as game state changes as a result of player actions First question is there an interest that I add this game to this github If so I am happy to do so I have understood the games here are implemented both in Python in C Is a C implementation mandatory Also happy to do that but it will take time I am planning to try different algorithms but maybe someone can give some advice I am familiar with some algorithms like MCTS minmax DQN But several others in the list here are rather new to me Which algorithms are most suited for my type of game ie players hidden info The feature I am most concerned about is the hidden information I see there are several games with hidden information implemented here but as far as I can tell most of them deal with information which is hidden at the start of a playthrough like dealing cards face down and is possibly revealed during the playthrough In my game some actions are hidden to some players players may make secret trades with each other the amount of money involved is hidden to players not involved in the trade So during the whole playthrough there is basically more and more information hidden In the games listed here I think only phantom tictactoe has hidden actions but maybe I have overlooked one or the other game with that characteristic Anyway which algorithms are suitable for games with hidden actions Unfortunately we cannot import a fix to the current NeuRD see see implementation see for details Im adding this issiue a reminder to implement a samplebased NeuRD along with the rest of the policy gradient variants in policygradientspy If anybody wants to do this please let us know Otherwise Ill add it eventually Hello When I try to run the Buildandruntestssh file it could Link and built all the targetbut there is a Failed in Performing Test CMAKEHAVELIBCPTHREAD Looking for pthreadcreate in pthreads not foundAnd when Test all the target tests some test occor the massage ImportError XXXDeepmindopenspielopenspielbuildpythonpyspielso undefined symbol PyThreadtssset Below is the building message CXXg NPROCnproc linuxgnu d a r w i n nproc MAKENUMPROCS let TESTNUMPROCS python c import sys printsysversionsplit PYVERSION BUILDDIRbuild mkdir p build cd build echo Building and testing in XXXDeepmindopenspielopenspielbuild using python version Building and testing in XXXDeepmindopenspielopenspielbuild using python version cmake DPythonTARGETVERSION DCMAKECXXCOMPILERg The C compiler identification is GNU The CXX compiler identification is GNU Check for working C compiler usrbincc Check for working C compiler usrbincc works Detecting C compiler ABI info Detecting C compiler ABI info done Detecting C compile features Detecting C compile features done Check for working CXX compiler publicsoftwaregcc bing Check for working CXX compiler publicsoftwaregcc bing works Detecting CXX compiler ABI info Detecting CXX compiler ABI info done Detecting CXX compile features Detecting CXX compile features done Looking for pthreadh Looking for pthreadh found Performing Test CMAKEHAVELIBCPTHREAD Performing Test CMAKEHAVELIBCPTHREAD Failed Looking for pthreadcreate in pthreads Looking for pthreadcreate in pthreads not found Looking for pthreadcreate in pthread Looking for pthreadcreate in pthread found Found Threads TRUE Found Python softwareanconda liblibpython mso found version found components Development Configuring done Generating done What should I do for this problem I saw that a games wrapper for the Ludii General Game System was something that you were looking to add in contributingmd It looks like Ludii is written in Java Was the idea to be able to call the Ludii jar from C in order to interact with it First of all thanks for making this framework open source I m investigating the possibility of making a simplified alphaZero implementation using openspiel and I was looking for some implementation ideas especially since you already mention this in the contributors guide Please note I am not sure if I will have the time to make the code up to openspiel standards Also I might not very closely follow the alphazero pseudocode Thus I am uncertain sure whether this effort will eventually result in a pull request I still think some pointers would be very helpful since others might be going to work on similar algorithms Implementation wise it seems most logical to me to create an rlagent implementation called alphaZero When taking a step however the agent will perform a MCTS To do this a complete game state will have to be reconstructed The easiest way to do this would be to pass the current environment state as an argument to the step function of the rlagent and then creating a game with this state internally in the rlagent This feels hacky to me instead of using the timestep argument which was seemingly designed to provide all available information to the agent you are feeding it extra with the full game state of course in a perfect information game this would be available already anyways What would be your perspective on this topic Of course any other design advice on the implementation would be very welcome as well tldr How to do state reconstruction in rlagent Do you have design advice on the implementation of an alphaZerolike algorithm Thanks for open sourcing openspiel I love the aim for completeness in this space One thing that I noticed is missing is games with continuous actionspaces and related algorithms such as Deterministic Policy Gradient Are there any plans to add support for those Im especially interested in MARL and equilibrium learning in auction games As an example I looked at the included implementation of the discretized firstprice sealedbid auction with integer valuations and actions On the one hand continuousactions are necessary when studying emergent MARLbehavior While a discrete implementation is pretty straightforward for symmetricuniform valuationdistributions it become a lot less meaningful in settings with asymmetric or nonuniform eg Gaussian priors where equilibria are nonlinear On the other hand more involved auction games that are being studied in economics simply become all but intractable in a discrete implementation Take eg combinatorial FPSB auctions where bundles of multiple items are sold to bidders at the same time Even in a continuousaction implementation the representation size of the action space already grows exponentially in the number of items but in a discrete implementation it grows doubleexponentially Im sure theres many other use cases eg in optimal control