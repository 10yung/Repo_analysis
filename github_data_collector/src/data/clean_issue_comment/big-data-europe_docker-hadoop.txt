in the dockerhadoop project i can not build the dockerfile with debian but i can use debian with the Dockerfile to build the hadoop x version you can upgrade the hadoop x Dockerfile ps in the dockerfile you can add a code line as follow RUN ln s opthadoopHADOOPVERSION opthadoop then i can see the version in the docker containner As title I am trying to deploy hadoop on kubernetes using the images build by you the problem I am facing is that the values of hadoopenv that I am passing using configmap not working properly Means the variable are reflected in hdfs and core sites but the value part are missing can you please help me to sort this out I have paste screen dumps below Screendumps propertynamedfswebhdfsenablednamevaluevalueproperty propertynamedfspermissionsenablednamevaluevalueproperty propertynamedfsnamenodedatanoderegistrationiphostnamechecknamevaluevalueproperty propertynamehadoopproxyusersdchostsnamevaluevalueproperty propertynamehadoopproxyusersdcgroupsnamevaluevalueproperty Thanks in advance Hi I want to scale the hadoop datanode to I run the command dockercompose up scale datanode But the resulting cluster still has only one datanode Could you please give me some suggestions on how to run the cluster with multiple datanodes Thanks Im completely new to Hadoop and I found this repo because I had the thought Wow installing Hadoop is hard and all I want is HDFS Surely theres got to be an easier way to do this Maybe someone made a Docker container Indeed this repo does an amazing job of getting all the complicated details out of the way But theres a number of questions that are left unanswered after getting this running Thought itd be useful to list them here Why is only the namenode port forwarded in dockercomposeyml I opened up the other ports and in the other services and can access all the UIs now How do I connect other hosts to the one I spun up with dockercompose up Itd be amazing to be able to do something like dockercompose up and maybe another command on another host and have them connect How do I get started with uploading data to HDFS I tried using Upload in the file system and it failed These questions will probably be answered just by working with Hadoop more but I thought they could help you guys if youre looking to address the new crowd Lots of university students especially those doing data scienceengineering are starting to feel the need to get familiar with tools like this Can someone help me troubleshoot the issue Im having with make wordcount Running that command yields this error docker run network dockerhadoopdefault envfile hadoopenv bde hadoopbase hdfs dfs mkdir p input docker invalid reference format See docker run help make wordcount Error make wordcount failed in my environment with UnknownHostException error How can I fix it Any help would be appreciated Thanks javalangIllegalArgumentException javanetUnknownHostException historyserver at orgapachehadoopsecuritySecurityUtilbuildTokenServiceSecurityUtiljava at orgapachehadoopyarnutiltimelineTimelineUtilsbuildTimelineTokenServiceTimelineUtilsjava at orgapachehadoopyarnclientapiimplYarnClientImplserviceInitYarnClientImpljava at orgapachehadoopserviceAbstractServiceinitAbstractServicejava at orgapachehadoopmapredResourceMgrDelegateserviceInitResourceMgrDelegatejava at orgapachehadoopserviceAbstractServiceinitAbstractServicejava at orgapachehadoopmapredResourceMgrDelegateinitResourceMgrDelegatejava at orgapachehadoopmapredYARNRunnerinitYARNRunnerjava at orgapachehadoopmapredYarnClientProtocolProvidercreateYarnClientProtocolProviderjava at orgapachehadoopmapreduceClusterinitializeClusterjava at orgapachehadoopmapreduceClusterinitClusterjava at orgapachehadoopmapreduceClusterinitClusterjava at orgapachehadoopmapreduceJob runJobjava at orgapachehadoopmapreduceJob runJobjava at javasecurityAccessControllerdoPrivilegedNative Method at javaxsecurityauthSubjectdoAsSubjectjava at orgapachehadoopsecurityUserGroupInformationdoAsUserGroupInformationjava at orgapachehadoopmapreduceJobconnectJobjava at orgapachehadoopmapreduceJobsubmitJobjava at orgapachehadoopmapreduceJobwaitForCompletionJobjava at WordCountmainWordCountjava at sunreflectNativeMethodAccessorImplinvoke Native Method at sunreflectNativeMethodAccessorImplinvokeNativeMethodAccessorImpljava at sunreflectDelegatingMethodAccessorImplinvokeDelegatingMethodAccessorImpljava at javalangreflectMethodinvokeMethodjava at orgapachehadooputilRunJarrunRunJarjava at orgapachehadooputilRunJarmainRunJarjava Caused by javanetUnknownHostException historyserver more I have set up docker swarm cluster use the following configuration file to deploy hdfs cluster on the overlay network named test in my swarm cluster version services namenode image bde hadoopnamenode hadoop java volumes namenodehadoopdfsname environment CLUSTERNAMEtest envfile hadoopenv networks hbase ports deploy mode replicated replicas restartpolicy condition onfailure placement constraints nodehostname labels traefikdockernetwork hbase traefikport datanode image bde hadoopdatanode hadoop java volumes datanodehadoopdfsdata envfile hadoopenv networks hbase environment SERVICEPRECONDITION namenode deploy restartpolicy condition onfailure labels traefikdockernetwork hbase traefikport placement constraints nodehostname volumes datanode namenode networks hbase external name test I can see registed datanode from namenode web ui after successful deployment but the datanode hostname and ip dont matchAnd i cant access datanode port through the datanode ipbut i can access it via datanode hostname Can you tell me why and give me a handThank you According to below doc this port should be exposed 