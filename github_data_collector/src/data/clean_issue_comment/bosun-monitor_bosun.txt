 Description Nodes with nochecks flag are returning RuleCheck in apihealth as true main c node bosuntoml n info migratego checking migrations info searchgo Loading last datapoints from redis info searchgo Done info webgo tsdb host info webgo bosun web listening http on curl s apihealth jq RuleCheck false curl XPOST apireload d Reload true reloaded curl s apihealth jq RuleCheck true If node has nochecks flags RuleCheck should be false all time Type of change x Bug fix nonbreaking change which fixes an issue New feature nonbreaking change which adds functionality Breaking change fix or feature that would cause existing functionality to not work as expected This change requires a documentation update How has this been tested x Integration test nochecks was set main c node bosuntoml n info migratego checking migrations info searchgo Loading last datapoints from redis info searchgo Done info webgo tsdb host info webgo bosun web listening http on curl s apihealth jq RuleCheck false curl XPOST apireload d Reload true reloaded curl s apihealth jq RuleCheck false nochecks is false main c node bosuntoml info migratego checking migrations info searchgo Loading last datapoints from redis info searchgo Done info checkgo check alert bosunuptime start with now set to info webgo tsdb host info webgo bosun web listening http on info checkgo check alert bosunuptime done ms crits warns unevaluated unknown info alertRunnergo runHistory on bosunuptime took ms error notifygo type alert name default transport httpPOST dst body htmlheadheadbody bodyhtml error Post dial tcp connect connection refused info alertRunnergo Stopping alert routine for bosunuptime info maingo schedule shutdown loading new schedule info maingo config reload complete info maingo running new schedule info checkgo check alert bosunuptime start with now set to info checkgo check alert bosunuptime done s crits warns unevaluated unknown info alertRunnergo runHistory on bosunuptime took ms curl s apihealth jq RuleCheck true curl XPOST apireload d Reload true reloaded curl s apihealth jq RuleCheck true Checklist x This contribution follows the projects code of conduct x This contribution follows the projects contributing guidelines x My code follows the style guidelines of this project x I have performed a selfreview of my own code I have commented my code particularly in hardtounderstand areas I have made corresponding changes to the documentation I have added tests that prove my fix is effective or that my feature works x New and existing unit tests pass locally with my changes x Any dependent changes have been merged and published in downstream modules Improve some errors logs Description While Redis is down sometimes we are crashing panic runtime error invalid memory address or nil pointer dereference signal SIGSEGV segmentation violation code x addr x pc x f f goroutine running bosunorgcmdbosunschedScheduleCheckNotifications xc fc x x x builddirbuildBUILDbosun GOsrcbosunorgcmdbosunschednotifygo x e bosunorgcmdbosunschedScheduledispatchNotifications xc fc builddirbuildBUILDbosun GOsrcbosunorgcmdbosunschednotifygo x ee created by bosunorgcmdbosunschedScheduleRun builddirbuildBUILDbosun GOsrcbosunorgcmdbosunschedalertRunnergo xa Type of change x Bug fix nonbreaking change which fixes an issue New feature nonbreaking change which adds functionality Breaking change fix or feature that would cause existing functionality to not work as expected This change requires a documentation update How has this been tested x Interaction test with Redis crashing emulation Checklist x This contribution follows the projects code of conduct x This contribution follows the projects contributing guidelines x My code follows the style guidelines of this project x I have performed a selfreview of my own code I have commented my code particularly in hardtounderstand areas I have made corresponding changes to the documentation I have added tests that prove my fix is effective or that my feature works x New and existing unit tests pass locally with my changes Any dependent changes have been merged and published in downstream modules Looks like after ed e d e fff e d global unknownTemplate configuration option is not working anymore Unknown template validated and assigned to but it seems not used anywhere during the notification As a result default notification is sent instead of the one specified in the global unknownTemplate Either docsconfigurationmd should be updated and reference to unknownTemplate should be removed or global unknownTemplate should be reintroduced Description Fixes Type of change From the following please check the options that are relevant x Bug fix nonbreaking change which fixes an issue New feature nonbreaking change which adds functionality Breaking change fix or feature that would cause existing functionality to not work as expected This change requires a documentation update How has this been tested The test is described in issue Checklist x This contribution follows the projects code of conduct x This contribution follows the projects contributing guidelines x My code follows the style guidelines of this project x I have performed a selfreview of my own code I have commented my code particularly in hardtounderstand areas I have made corresponding changes to the documentation I have added tests that prove my fix is effective or that my feature works New and existing unit tests pass locally with my changes Any dependent changes have been merged and published in downstream modules Expected behavior API POST apireload d Reload true should reread config and make full restart of scheduler Current behaviour When we run bosun first time we have dispatchNotifications routine goroutine select bosunorgcmdbosunschedScheduledispatchNotifications x d bosunorgcmdbosunschednotifygo x created by bosunorgcmdbosunschedScheduleRun bosunorgcmdbosunschedalertRunnergo xa After reload through api POST apireload d Reload true goroutine select bosunorgcmdbosunschedScheduledispatchNotifications x d bosunorgcmdbosunschednotifygo x created by bosunorgcmdbosunschedScheduleRun bosunorgcmdbosunschedalertRunnergo xa goroutine select bosunorgcmdbosunschedScheduledispatchNotifications xc c bosunorgcmdbosunschednotifygo x created by bosunorgcmdbosunschedScheduleRun bosunorgcmdbosunschedalertRunnergo xa As result we have duplicate chained notifications Steps to reproduce Please provide detailed steps for reproducing the issue Run bosun daemon with system config HTTPListen EnableReload true And rules definition alert test template defaulttemplate flapper epoch crit flapper warn flapper runEvery critNotification default warnNotification default template defaulttemplate subject Test body postBody body Subject notification defaultnotification post bodyTemplate body next defaultnotification timeout m Run notification service usrbinenv python from httpserver import HTTPServer BaseHTTPRequestHandler class SBaseHTTPRequestHandler def setheadersself selfsendresponse selfendheaders def doPOSTself Doesnt do anything with posted data selfsetheaders selfwfilewriteok def runserverclassHTTPServer handlerclassS addrlocalhost port serveraddress addr port httpd serverclassserveraddress handlerclass printfStarting httpd server on addrport httpdserveforever if name main run Wait for first notification After that reload config curl XPOST apireload d Reload true Check duplicate call in notification service output Context Running commit e e efb cf b bca db b dff OS linuxx Linux fc x Logs Nothing interesting in logs Short description Currently bosun doesnt support any ha and load distribution We should provide something that will allow us to provide bosun as a high available and scalable service How this feature will help youyour organisation Automatic failover then server with bosun became unavailable Avoid splitbrain problem Distribute check execution between multiple servers Possible solution or implementation details One of working implementation I offer to use raft clustering implementation from hashicorp Possible roundmap x Create cluster for improving availability Have a simple masterslave configuration We can use silencenochecks flags to make node as standby This step without ant snapshots etc Just simple standby Add support for snapshot cluster state rotate snapshots recover the cluster state Host leader can distribute tasks for checks by check name as instance between nodes using consistent hashing distribution In that step we can stop to use flags as the main instrument for manage nodes within the cluster Just fix merge from If you approve this MR please close JFYI harudark Description As solution for issue Same as with solving conflicts and some new api endpoints Were added new api endpoints POST apiclusterrecovercluster Api endpoint Is used to manually force a new configuration in order to recover from a loss of quorum where the current configuration cannot be restored such as when several servers die at the same time This works by reading all the current state for this server creating a snapshot with the supplied configuration and then truncating the Raft log This is the only safe way to force a given configuration without actually altering the log to insert any new entries which could cause conflicts with other servers with a different state WARNING This operation implicitly commits all entries in the Raft log so in general this is an extremely unsafe operation If youve lost your other servers and are performing a manual recovery then youve also lost the commit information so this is likely the best you can do but you should be aware that calling this can cause Raft log entries that were in the process of being replicated but not yet be committed to be committed Example curl s apiclusterstatus jq State Candidate Nodes Address State Follower Address State Follower Stats curl XPOST apiclusterrecovercluster d members address State Leader Nodes Address State Leader Stats POST apiclusterchangemaster move leadership to another node in cluster Example curl s apiclusterstatus jq State Leader Nodes Address State Leader Address State Follower Stats curl XPOST apiclusterchangemaster d id address statuserrorerrorcannot transfer leadership to itself curl XPOST apiclusterchangemaster d id address statuserrorerrornode is not the leader curl XPOST apiclusterchangemaster d id address statusok curl apiclusterstatus jq State Follower Nodes Address State Follower Address State Leader Stats Type of change From the following please check the options that are relevant Bug fix nonbreaking change which fixes an issue x New feature nonbreaking change which adds functionality Breaking change fix or feature that would cause existing functionality to not work as expected x This change requires a documentation update How has this been tested Checklist x This contribution follows the projects code of conduct x This contribution follows the projects contributing guidelines x My code follows the style guidelines of this project x I have performed a selfreview of my own code I have commented my code particularly in hardtounderstand areas I have made corresponding changes to the documentation I have added tests that prove my fix is effective or that my feature works x New and existing unit tests pass locally with my changes Any dependent changes have been merged and published in downstream modules Description This pull request adds ES x support in Bosun I submitted before but that is based on my own go modules migration pull request which is different from what upstream maintainers did This one is based on upstream go modules migration version client is used Most code are copypasted from ES version support The one difference is in ES mapping types is removed TypedocType are removed in ES Type of change From the following please check the options that are relevant Bug fix nonbreaking change which fixes an issue x New feature nonbreaking change which adds functionality Breaking change fix or feature that would cause existing functionality to not work as expected This change requires a documentation update How has this been tested Tested with ES and also ES to make sure nothing breaks update the configuration toml file and create some alerts to query ES verify alerts can be triggered and no errors in ES query verify annotate can work Checklist x This contribution follows the projects code of conduct x This contribution follows the projects contributing guidelines x My code follows the style guidelines of this project x I have performed a selfreview of my own code x I have commented my code particularly in hardtounderstand areas I have made corresponding changes to the documentation I have added tests that prove my fix is effective or that my feature works x New and existing unit tests pass locally with my changes x Any dependent changes have been merged and published in downstream modules Description Resolves Type of change From the following please check the options that are relevant Bug fix nonbreaking change which fixes an issue x New feature nonbreaking change which adds functionality x Breaking change fix or feature that would cause existing functionality to not work as expected x This change requires a documentation update See linked ticket for details on breaking change How has this been tested x Unit tests x Manual testing of Bosun and SCollector with OpenTSDB and Redis Checklist x This contribution follows the projects code of conduct x This contribution follows the projects contributing guidelines x My code follows the style guidelines of this project x I have performed a selfreview of my own code x I have commented my code particularly in hardtounderstand areas x I have made corresponding changes to the documentation x I have added tests that prove my fix is effective or that my feature works x New and existing unit tests pass locally with my changes x Any dependent changes have been merged and published in downstream modules 