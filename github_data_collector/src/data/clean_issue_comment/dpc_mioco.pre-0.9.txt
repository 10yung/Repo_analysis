 This is the followup bug for I was unable to create a minimal repo case but I will try to give you access to the environment I am seeing this bug Background I am writing a replacement for Widelands meta server Think of it as a sort of mini battlenet only for Widelands The current version is in go the one before it was in python I use this to learn new languages and paradigms and I was playing around with mioco My current implementation that shows this bug on Mac OS X with latest nightlies is When running it and connecting widelands to it the last lines I see in the console of the sever are sirver ALIVE srcmainrs sirver ALIVE srcmainrs sirver done sirver ALIVE srcprotocolrs sirver selfbuf selfunconsumed sirver ALIVE srcprotocolrs here is a very long pause without output or so seconds sirver Read size sirver ALIVE srcprotocolrs The last lines are printed here They signify that readertryread blocks when there a few bytes to read but less than the buffers size can be read which I am surprised about If you want to reproduce you can get yourself a download of widelands On Linux you will find build likely in your package tree aptget install widelands will likely work There are also PPAs available otherwise Once you have the game start it once and exit immediately Then edit widelandsconfig and add under global the line metaserverlocalhost Restart the game choose multiplayer internet game at which point the game should try to connect Not sure if this should be considered a performance issue or a bug Consider the following test case rust extern crate mioco use miocosyncmpscsyncchannel Receiver SyncSender fn producertx SyncSenderusize for i in printlnsending i txsendiexpectfailed to send to consumer fn consumerrx Receiverusize while let Oki rxrecv printlnreceived i fn main let size let tx rx syncchannelsize miocostart let producer miocospawn producertx let consumer miocospawn consumerrx producerjoinunwrap consumerjoinunwrap unwrap This example shows a normal pattern where there is a producer that is decoupled from a consumer by a bounded queue the syncchannel It is expected that when the queue is full either because consumer is slow or hasnt been scheduled as often that the producer blocks Later when there is enough space the producer gets resumed However running the above code you can see that the throughput is somewhat surprisingly just per second Most likely when the producer coroutine gets yielded srcsyncmpscrs the event loop thinks there is nothing left to do and starts blocking on IO for some ms In this case the consumer coroutine is ready to be executed and the event loop should have either skipped the iopoll entirelly or done it with a timeout If you are wondering increasing the queue size to does increase the throughput But the result is a lot higher than just x If the scheduling gods make it so that the two courotines are alternating requests to the queue then the limit is never hit and they are able to proceed at full CPU speed This is what you get if you dont wait for travis to complete failures testssimplemutexsupportsinsideandoutside stdout thread testssimplemutexsupportsinsideandoutside panicked at assertion failed left right left right testsrs stack backtrace x b df stdsysbacktracetracingimpwriteh f fdb fc a x b b stdpanickingdefaulthookclosureh cc f x b ac stdpanickingdefaulthookhbbe fa a aca x b d e stdpanickingrustpanicwithhookh c d fcd fb e x b bb stdpanickingbeginpanichbf ea a ff f de x b ada stdpanickingbeginpanicfmth f e d x b f f f miocotestssimplemutexsupportsinsideandoutsidehf be fc x b ef F as allocboxedFnBoxAcallboxh c b f a f cd x b f stdpanickingtrycallh d a e c ac b e x b e rustmaybecatchpanic x b e e F as allocboxedFnBoxAcallboxh f d x b stdsysthreadThreadnewthreadstarth f bd e f ea x b c fe startthread x b e c unknown failures Reported originally in rust let mut remote TcpStream let mut origin tryTcpStreamconnect addr let mut remote tryremotetryclone let mut origin tryorigintryclone miocospawnmove ioResult let mut buf loop let size tryremote read mut buf if size break origin writeall buf size Ok miocospawnmove ioResult let mut buf loop let size tryoriginread mut buf if size break remotewriteall buf size Ok tryclone must return a deep clone new fd or not work at all Also make sure mioco types cant be used from multiple threadscoroutines at the same time It is generally needed When using select in a loop the events dont have to be reregistered every time which for channels at least is expensive To test I made miocothreadtlcurrentcoroutine public and added a flag to miococoroutineCoroutine to disable the deregisterall call in unblock Heres a stress test using channels looprs The test using mioco master rust macrouse extern crate mioco fn main let miocostartmove let sxrx miocosyncmpscchannel let sx rx miocosyncmpscchannel let sx rx miocosyncmpscchannel let mut counter sxsend sx send sx send loopselect rrx let rx tryrecv rrx let rx tryrecv rrx if let Okx rxtryrecv counter sxsend if counter break loop rs With tlcurrentcoroutine public and autoderegister added to coroutine rust macrouse extern crate mioco use miocothreadtlcurrentcoroutine use miocoEvented fn main let miocostartmove let sxrx miocosyncmpscchannel let sx rx miocosyncmpscchannel let sx rx miocosyncmpscchannel let mut counter sxsend sx send sx send unsafe tlcurrentcoroutineautoderegister false rxselectaddmiocoRWread rx selectaddmiocoRWread rx selectaddmiocoRWread loop let ret miocoselectwait if retid rxid if let Ok rxtryrecv counter sxsend if counter break if retid rx id let rx tryrecv if retid rx id let rx tryrecv and the results miocotimer gitmaster time targetreleaseloop targetreleaseloop s user s system cpu total miocotimer gitmaster time targetreleaseloop targetreleaseloop s user s system cpu total One idea for a proper implementation of this would be a selectloop macro which would select in a loop without reregistering 