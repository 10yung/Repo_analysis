 The example has a comment get the threadlocal RNG This makes it sound like init is called once per thread which is what would be ideal in my usecase but actually this is far from true I found by benchmarking that my code spends a lot of time in my init function I am processing items and init gets called about times on average varies per run That means only about items processed per init call which results in poor scaling it spends too much time in init and not enough in processing Possible questions should I be using something other than mapinit am I supposed to do something with TLS or using a syncpool to manually cache items is this by design or canshould it be improved I dont understand why it needs to create so many instances should documentation be updated to avoid confusion Using the Result impl for FromParallelIterator one can already collect the result of many fallible operations into a ResultVec However AFAICT it is currently impossible to do this multiple times while reusing the same results buffer which is possible for infallible operations using collectintovec It would be nice if this was supported Id expect the semantics around errors to be the same as those for tryforeach with the backing Vec additionally being cleared if an Err is encountered Is this just something that hasnt been implemented yet or have I missed something that makes this impossible to implement Would a PR for this be accepted if it can be done I havent previously contributed to rayon and dont know whether Id be able to do it but I might try at least Using the checked out crate tests behave normally when run in debug profile but when run with any kind of optimization this test fails RUSTFLAGSCoptlevel cargo test test stackoverflowcrash Finished test unoptimized debuginfo targets in s Running targetdebugdepsstackoverflowcrashaf c bd e thread main panicked at assertion failed left right left Some right None testsstackoverflowcrashrs note run with RUSTBACKTRACE environment variable to display a backtrace error test failed to rerun pass test stackoverflowcrash echo RUSTFLAGSCoptlevel cargo test test stackoverflowcrash Finished test unoptimized debuginfo targets in s Running targetdebugdepsstackoverflowcrashaf c bd e thread unknown has overflowed its stack fatal runtime error stack overflow echo Backtrace with Coptlevel is as follows details summarythough it doesnt look very helpful to mesummary backtracebacktracelibunwindtrace at cargoregistrysrcgithubcom ecc db ec backtrace srcbacktracelibunwindrs backtracebacktracetraceunsynchronized at cargoregistrysrcgithubcom ecc db ec backtrace srcbacktracemodrs stdsyscommonbacktraceprintfmt at srclibstdsyscommonbacktracers stdsyscommonbacktraceprintDisplayBacktrace as corefmtDisplayfmt at srclibstdsyscommonbacktracers corefmtwrite at srclibcorefmtmodrs stdioWritewritefmt at srclibstdiomodrs stdsyscommonbacktraceprint at srclibstdsyscommonbacktracers stdsyscommonbacktraceprint at srclibstdsyscommonbacktracers stdpanickingdefaulthookclosure at srclibstdpanickingrs stdpanickingdefaulthook at srclibstdpanickingrs stdpanickingrustpanicwithhook at srclibstdpanickingrs stdpanickingcontinuepanicfmt at srclibstdpanickingrs stdpanickingbeginpanicfmt at srclibstdpanickingrs stackoverflowcrashmain at testsstackoverflowcrashrs stdrtlangstartclosure at rustc c b afd e fa d cb c d srclibstdrtrs stdrtlangstartinternalclosure at srclibstdrtrs stdpanickingtrydocall at srclibstdpanickingrs rustmaybecatchpanic at srclibpanicunwindlibrs stdpanickingtry at srclibstdpanickingrs stdpaniccatchunwind at srclibstdpanicrs stdrtlangstartinternal at srclibstdrtrs stdrtlangstart at rustc c b afd e fa d cb c d srclibstdrtrs libcstartmain at csulibcstartc start details I tried singlestepping though the program with lldb but nothing obvious happens I am implementing a parallel Lsystem algorithm This means that threads turn a small vector of symbols into a larger vector of symbols by looking up each symbols result symbols in a hashmap and finally split the larger vector in chunks which are then reoffered as tasks for later consumptionstealing until a desired depth is reached However when this desired depth is reached I want to sort the resulting vectors which are all chunksslices of the desired output and finally end up with these vectors in order So I have a VecId VecSymbol that needs sorting Now I could Use a datastructure like crossbeamskiplist to handle this although it is currently still experimental Use a shared VecId VecSymbol with either a lock around readingwriting it or maybe an atomic variant or another MPSC collection type and when all threads are done start Rayons parsort or parsortunstable on it The first thing parsort then does is split the collection into subslices again and divide the work across threads I am therefore wondering if these two steps writing the final work to a global collection and waiting until this is done and then splitting this work back up across threads might not be combined in some way What is possible here Hi all Just feedback from a realworld workload nonpublic system upgraded from to and got a free performance boost Great work Please feel free to close I am trying to create a local ThreadPool let pool ThreadPoolBuildernew build expectEntityContainerloadentitiesinternal cannot create thread pool Also in context This compiles fine and runs fine on MacOS It compiles on Linux but crashes thread unnamed panicked at EntityContainerloadentitiesinternal cannot create thread pool ThreadPoolBuildError kind IOErrorOs code kind WouldBlock message Resource temporarily unavailable srclibcoreresultrs Earlier errors also included the global thread pool not being initialized but after some fiddling that went away Tried the crate and current git master no joy Tried Rust stable beta and nightly no joy Ive been benching a program of mine that uses rayon and I noticed I get much more clean results if I set the number of cpus to be the physical number instead of the logical number Here is a comparison y axis is actually time taken I mislabeled it I think it is related to hyperthreading Is it possible hyperthreading doesnt actually improve anything when used with rayon or maybe my operating system just isnt optimized to use hyperthreading im using a dell precision preinstalled with linux As a start to improving our sleep system this PR does a refactoring to how tickle works We used to tickle the registry after each job executed the idea was that executing a job would set some latch and we needed to wake up any threads that might be blocked waiting on that latch However this was a bit overapproximated for example the latch mightve been a LockLatch which indicates a thread from outside the pool in which case there is no need to tickle threads at all In this PR the latches themselves track the registry that they must tickle when they are set In the case of a Countdown latch they are given it from the outside this is because countdown latches are sometimes owned by the registry itself so it would be impossible for them to have a reference to the registry Id like to work towards a scenario where we know not only the registry that must be awoken but the exact helper thread This could avoid needless wakeups But this refactoring is as far as I got for now Im using rayon on Windows and rustc stable Im not sure how to best report this bug and I dont even know if it is a bug in rayon at all but I thought its better when someone has a look at this I can provide additional information as requested Ive written the following code rust use jwalkWalkDir let entries WalkDirnewpathintoiterparbridgemapentry entryunwrappathdisplaytostring collectVec Im using jwalk it uses rayon internally If I run this code on a big folder C Users Home or C it sometimes hangs indefinitely in the parbridgers It tries to aquire the lock on line but always continues with the ErrTryLockErrorWouldBlock match arm Is there a way to get chunkslike functionality but with stripingstepping instead of contiguous memory Ie if you have some data in a slice and every nth element of this slice is part of the same independent workload you could split up the buffer for processing on multiple threads in such a way that the data for each thread would interleaved with the other threads data This doesnt seem to be the semantics of interleave windows stepby or any of the other functions Ive found so far in the documentation I am told that such an access pattern is common in image processing