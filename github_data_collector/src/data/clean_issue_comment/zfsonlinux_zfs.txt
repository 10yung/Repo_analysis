 System information Type VersionName Distribution Name Ubuntu Distribution Version Linux Kernel Architecture amd ZFS Version SPL Version The kernel is self compiled using GCC based on Ubuntus stock config mostly unchanged except disabling RETPOLINE Describe the problem youre observing I was copying a bunch of media files size varies from MB to GB from a ZFS filesystem to a XFS filesystem using cp av Suddenly it freezed and unable to be interrupted by CtrlC Describe how to reproduce the problem Not reproduced so far rebooted and continue by cp auv Include any warningerrorsbacktraces from the system logs INFO task cp blocked for more than seconds Tainted P W OEL reimu echo procsyskernelhungtasktimeoutsecs disables this message cp D x Call Trace schedule x b x schedule x f xa scheduletimeout x a x dbufreleandunlock x x zfs nexttimerinterrupt xc xc ioscheduletimeout x x cvtimedwaitcommon x d x spl waitwoken x x ziowait x x zfs dmubufholdarraybydnode x x zfs dmureaduiodnode x x zfs rangelockenter x x zfs dmureaduiodbuf x f x zfs zfsread x x zfs zplreadcommoniovec xaf x zfs zpliterread x x a zfs newsyncread x x b vfsread xa x ksysread x e x dosyscall x f x entrySYSCALL afterhwframe x xa RIP x fd fb d Code Bad RIP value RSP b fff ef f a EFLAGS ORIGRAX RAX ffffffffffffffda RBX RCX fd fb d RDX RSI cc b e RDI RBP R R R R R ef f R R cc b e R ef f INFO task cp blocked for more than seconds Tainted P W OEL reimu echo procsyskernelhungtasktimeoutsecs disables this message cp D x Call Trace schedule x b x schedule x f xa scheduletimeout x a x dbufreleandunlock x x zfs nexttimerinterrupt xc xc ioscheduletimeout x x cvtimedwaitcommon x d x spl waitwoken x x ziowait x x zfs dmubufholdarraybydnode x x zfs dmureaduiodnode x x zfs rangelockenter x x zfs dmureaduiodbuf x f x zfs zfsread x x zfs zplreadcommoniovec xaf x zfs zpliterread x x a zfs newsyncread x x b vfsread xa x ksysread x e x dosyscall x f x entrySYSCALL afterhwframe x xa RIP x fd fb d Code Bad RIP value RSP b fff ef f a EFLAGS ORIGRAX RAX ffffffffffffffda RBX RCX fd fb d RDX RSI cc b e RDI RBP R R R R R ef f R R cc b e R ef f Please fill out the following template which will help other contributors address your issue Thank you for reporting an issue IMPORTANT Please search our issue tracker before making a new issue If you cannot find a similar issue then create a new issue IMPORTANT This issue tracker is for bugs and issues only Please search the wiki and the mailing list archives before asking questions on the mailing list Please fill in as much of the template as possible System information add version after character Type VersionName Distribution Name CentOS Distribution Version Linux Kernel Architecture x ZFS Version SPL Version Commands to find ZFSSPL versions modinfo zfs grep iw version modinfo spl grep iw version Describe the problem youre observing Installing zfskmod package fails on CentOS Furthermore working Centos zfs systems fail to work after update to Describe how to reproduce the problem Install fresh Centos install zfs repo switch to zfskmod repo run dnf install zfs or Update working CentOS system with zfs dnf update reboot Include any warningerrorsbacktraces from the system logs Fresh install dnf install zfs Last metadata expiration check ago on Fri Jan PM CET Error Problem package zfs el x requires zfskmod but none of the providers can be installed conflicting requests nothing provides kernelremoveinodehash x b a ec needed by kmodzfs el x nothing provides kernelclearinode x ef a needed by kmodzfs el x nothing provides kernelclearnlink x b needed by kmodzfs el x nothing provides kernelcurrenttime x e fb needed by kmodzfs el x nothing provides kerneldaddci xee c e d needed by kmodzfs el x nothing provides kerneldinstantiate xa b f db needed by kmodzfs el x nothing provides kerneldinvalidate x needed by kmodzfs el x nothing provides kerneldmakeroot x e needed by kmodzfs el x nothing provides kerneldobtainalias xa bdac needed by kmodzfs el x nothing provides kerneldprunealiases x edc b needed by kmodzfs el x nothing provides kerneldsetdop xd ad needed by kmodzfs el x Update from CentOS sbinmodprobe zfs modprobe FATAL Module zfs not found in directory libmodules el x IMPORTANT Please mark logs and text output from terminal commands or else Github will not display them correctly An example is provided below Example this is an example how log text should be marked wrap it with zdb b filesystem OST txt Problem description File Size on disk size currently unexplained size on disk is x file size Observations A potential ZFS filesystem corruption across RaidInc Storage in London zdb check for leaks it walks the entire block tree constructing the space maps in memory and then compares them to the ones stored on disk If they differ it reports the leak a Presuming from the below investigation that the space leaks mean the pool is corrupted somehow zdb ZFS debug has detected tons of corruptions zdb did not report space leaks on ZFS Houston SI s Does zdb leaked space means trouble with the pool and could explain the file size disk size discrepancy Is it possible that errors got injected due to failover or hardware errors It seems to be at least inconsistent which is supposed to never happen with ZFS Is this indicative of a larger problem Numerous lockups etc Investigation For the troubleshooting the following file located in WEY was selected There are no snapshotsreservationsquotas involved here server usersuser du h apparentsize lusfilesystem projectfilename K lusfilesystem projectfilenameauxdata K lusfilesystem projectfilenamedescriptoryaml G lusfilesystem projectfilenametracedatabin G lusfilesystem projectfilenametraceheaderbin server usersuser du h lusfilesystem projectfilename K lusfilesystem projectfilenameauxdata K lusfilesystem projectfilenamedescriptoryaml G lusfilesystem projectfilenametracedatabin G lusfilesystem projectfilenametraceheaderbin Copy of the dataset onto the same storage o Disk size is different o Checksum matches server usersuser cp rp lusfilesystem projectfilename lusfilesystem projectp j SRME A JC server usersuser md sum lusfilesystem projectfilename md sum lusfilesystem projectfilenameauxdata Is a directory f b d b b e ae aa lusfilesystem projectfilenamedescriptoryaml e ac c e b b e e b lusfilesystem projectfilenametracedatabin bc e d aabcb cd lusfilesystem projectfilenametraceheaderbin server usersuser md sum lusfilesystem projectp j SRME A JC md sum lusfilesystem projectp j SRME A JCauxdata Is a directory f b d b b e ae aa lusfilesystem projectp j SRME A JCdescriptoryaml e ac c e b b e e b lusfilesystem projectp j SRME A JCtracedatabin bc e d aabcb cd lusfilesystem projectp j SRME A JCtraceheaderbin server usersuser du h lusfilesystem projectp j SRME A JC K lusfilesystem projectp j SRME A JCauxdata K lusfilesystem projectp j SRME A JCdescriptoryaml G lusfilesystem projectp j SRME A JCtracedatabin G lusfilesystem projectp j SRME A JCtraceheaderbin server usersuser du h apparentsize lusfilesystem projectp j SRME A JC K lusfilesystem projectp j SRME A JCauxdata K lusfilesystem projectp j SRME A JCdescriptoryaml G lusfilesystem projectp j SRME A JCtracedatabin G lusfilesystem projectp j SRME A JCtraceheaderbin Printing the OST name hosting the given file server usersuser lustrefindostforfile lusfilesystem projectfilenametracedatabin lusfilesystem projectfilenametracedatabin filesystem OST f filesystem oss Run zdb to check for leaks rootfilesystem oss zfs list NAME USED AVAIL REFER MOUNTPOINT filesystem OST T T K none filesystem OST filesystem OST T T T none filesystem OST T T K none filesystem OST filesystem OST T T T none filesystem OST T T K none filesystem OST filesystem OST f T T T none filesystem OST T T K none filesystem OST filesystem OST T T T none rootfilesystem oss zdb b filesystem OST Traversing all blocks to verify nothing leaked loading space map for vdev of metaslab of T completed MBs estimated time remaining hr min sec leaked space vdev offset x d de size See attachment Please would someone be able to advise Thanks Nick Please fill out the following template which will help other contributors review your Pull Request Provide a general summary of your changes in the Title above Documentation on ZFS Buildbot options can be found at Motivation and Context Why is this change required What problem does it solve If it fixes an open issue please link to the issue here Presently native encryption of a ZFS root filesystem requires a passphrase or keyfile to be available every boot A TPM chip has the ability to store and release data if certain criteria are met This PR provides an optional method which allows TPM to be used to automatically unlock the filesystem without user intervention This is a DRAFT PR to foster discussion surrounding the method THIS MAY NOT CONFORM TO STYLE GUIDES OR EVEN GOOD CODING PRACTICES Code nit comments are not unwelcome but methodology is the initial goal Description Describe your changes in detail This PR does the following Within zfss initramfs hooks detects and copies needed tpm tools x to the initrd Provides zfs initramfss script a method of detecting and unlocking using TPM tools If TPM unlock is not successful allows Initramfs to fallback to other manual passphrase methods Stores required and user definable unlocking variables in the encryption root using orgopenzfstpm index and orgopenzfstpm pcrs Provides a utility that maintains the TPM and the above zfs properties The zfs passphrase is locked within the TPMs nvram and only releasable with an expected PCR state and password GUID used to confirm encrypted filesystem match The nvram region of the TPM is locked from further reading following the retrieval of the password within the initramfs script preventing even root users access to the password postboot Quick and probably correctish PCR usage PCR Use BiosPlatform Code BiosPlatform Config Option Rom UEFI Driver Code Option Rom UEFI Config Data Bootloader code GPT boot attempts Bootloader data Manufacturer use wake events Manufacturer use Secureboot policy Grub Grub Kernel and Module Commands Grub ANY file read by Grub includes kernel and initrd Linux Integrity Measurement Architecture IMA Hardware Operating System How Has This Been Tested Please describe in detail how you tested your changes Include details of your testing environment and the tests you ran to see how your change affects other areas of the code etc If your change is a performance enhancement please provide benchmarks here Please think about using the draft PR feature if appropriate Tpmtools x revamped its arguments Grub is secure boot compatible and now reports additional boot time measurements that could harden the boot protections Therefore this has been developed and tested solely on Ubuntu Focal prerelease to be LTS on Dell TPM hardware using UEFI boot Testing different distros kernels and hardware TPM devices need to be conducted Types of changes What types of changes does your code introduce Put an x in all the boxes that apply Bug fix nonbreaking change which fixes an issue X New feature nonbreaking change which adds functionality Performance enhancement nonbreaking change which improves efficiency Code cleanup nonbreaking change which makes code smaller or more readable Breaking change fix or feature that would cause existing functionality to change Documentation a change to man pages or other documentation Checklist Go over all the following points and put an x in all the boxes that apply If youre unsure about any of these dont hesitate to ask Were here to help My code follows the ZFS on Linux code style requirements I have updated the documentation accordingly X I have read the contributing document I have added tests to cover my changes I have run the ZFS Test Suite with this change applied All commit messages are properly formatted and contain Signedoffby Things to Explore Current Status will be edited over time Dracut integration Include kernel commandline panic monitoringinjection to prevent snooping from the initrd prompt Consider updating TPM within the initrd following autounlock failure but successful manual entry ie after kernelinitrd updates Documentation of TPM PCRs Consider the ability to use keyfiles recovery method must match what is in the TPM Prove functionality on other platforms Resources Tpm tools man pages Grub manual specifically Measured Boot Section UEFI PCR explanation on Pg Another PCR description on th page This adds a lazytime dataset property that controls the lazytime mount option The handling of the relatime property was used as a template for how to implement this This addresses Signedoffby Clint Armstrong clintclintarmstrongnet Please fill out the following template which will help other contributors review your Pull Request Provide a general summary of your changes in the Title above Documentation on ZFS Buildbot options can be found at Motivation and Context Fixes Why is this change required What problem does it solve If it fixes an open issue please link to the issue here Description Adds a lazytime property to influence lazytime on mount basically done by copying how relatime is handled Describe your changes in detail How Has This Been Tested Built and manually tested on Ubuntu Table below shows the mount options when creating a dataset with the options listed atime relatime lazytime mount options off off off noatime off on off noatime off off on noatimelazytime off on on noatimelazytime on off off strictatime on on off relatime on off on lazytime on on on relatimelazytime Tests have been written using relatime tests as a guide but none of the atime tests pass and did not pass on the last tagged release zfs Please describe in detail how you tested your changes Include details of your testing environment and the tests you ran to see how your change affects other areas of the code etc If your change is a performance enhancement please provide benchmarks here Please think about using the draft PR feature if appropriate Types of changes What types of changes does your code introduce Put an x in all the boxes that apply Bug fix nonbreaking change which fixes an issue x New feature nonbreaking change which adds functionality Performance enhancement nonbreaking change which improves efficiency Code cleanup nonbreaking change which makes code smaller or more readable Breaking change fix or feature that would cause existing functionality to change Documentation a change to man pages or other documentation Checklist Go over all the following points and put an x in all the boxes that apply If youre unsure about any of these dont hesitate to ask Were here to help x My code follows the ZFS on Linux code style requirements x I have updated the documentation accordingly x I have read the contributing document x I have added tests to cover my changes I have run the ZFS Test Suite with this change applied x All commit messages are properly formatted and contain Signedoffby Please fill out the following template which will help other contributors address your issue Thank you for reporting an issue IMPORTANT Please search our issue tracker before making a new issue If you cannot find a similar issue then create a new issue IMPORTANT This issue tracker is for bugs and issues only Please search the wiki and the mailing list archives before asking questions on the mailing list Please fill in as much of the template as possible System information add version after character Type VersionName Distribution Name Proxmox Distribution Version Linux Kernel Linux longmox pve SMP PVE Thu Dec x GNULinux ZFS Version pve SPL Version pve Commands to find ZFSSPL versions modinfo zfs grep iw version modinfo spl grep iw version Describe the problem youre observing So I have a zpool thats degraded I then tried to create snapshots to do a backup I snapshotted the whole pool sudo zfs snapshot r rpooldump I now have a pool thats wedged and I cannot do anything Im not sure how to reproduce this and I currently have a server thats basically totally hosed NAME SIZE ALLOC FREE CKPOINT EXPANDSZ FRAG CAP DEDUP HEALTH ALTROOT rpool G G G x DEGRADED rootlongmox sudo zfs list NAME USED AVAIL REFER MOUNTPOINT rpool G B K rpool rpoolROOT G B K rpoolROOT rpoolROOTpve G B G rpoolbase disk G G G rpoolbasevol disk M B M rpoolbasevol disk rpooldata K B K rpooldata rpoolsubvol disk G B G rpoolsubvol disk rpoolsubvol disk M B M rpoolsubvol disk rpoolsubvol disk G B G rpoolsubvol disk rpoolsubvol disk G B G rpoolsubvol disk rpoolsubvol disk G B G rpoolsubvol disk rpoolsubvol disk G B G rpoolsubvol disk rpoolsubvol disk G B G rpoolsubvol disk rpoolsubvol disk G B G rpoolsubvol disk rpoolsubvol disk G B G rpoolsubvol disk rpoolsubvol disk G B G rpoolsubvol disk rpoolsubvol disk G B G rpoolsubvol disk rpoolsubvol disk G B G rpoolsubvol disk rpoolsubvol disk M B M rpoolsubvol disk rpoolsubvol disk G B G rpoolsubvol disk rpoolsubvol disk M B M rpoolsubvol disk rpoolvm disk G G G rpoolvm disk G G G rpoolvm disk G B G none rpoolvm disk G G G rpoolvm disk G G G rpoolvm disk G G G rpoolvm disk G G G rpoolvm disk G G G rpoolvm disk G G G rpoolvm disk G G G rootlongmox sudo zfs list t snapshot NAME USED AVAIL REFER MOUNTPOINT rpooldump B K rpoolROOTdump B K rpoolROOTpve dump B G rpoolbase disk base K G rpoolbase disk dump B G rpoolbasevol disk base K M rpoolbasevol disk dump B M rpooldatadump B K rpoolsubvol disk dump B G rpoolsubvol disk dump B M rpoolsubvol disk dump B G rpoolsubvol disk dump B G rpoolsubvol disk dump B G rpoolsubvol disk dump B G rpoolsubvol disk dump B G rpoolsubvol disk dump B G rpoolsubvol disk dump B G rpoolsubvol disk dump B G rpoolsubvol disk dump B G rpoolsubvol disk dump B G rpoolsubvol disk dump B M rpoolsubvol disk dump B G rpoolsubvol disk dump B M rpoolvm disk dump B G rpoolvm disk dump M G rpoolvm disk snap B G rpoolvm disk dump B G rpoolvm disk dump M G rpoolvm disk dump M G rpoolvm disk dump M G rpoolvm disk dump B G rpoolvm disk dump B G rpoolvm disk dump M G rpoolvm disk dump B G I cannot do anything rootlongmox zfs destroy rpoolvm disk dump internal error Channel number out of range Aborted rootlongmox zfs destroy rpooldatadump internal error Channel number out of range Aborted rootlongmox rm vzdumpqemu vma rm cannot remove vzdumpqemu vma No space left on device rootlongmox echo vzdumpqemu vma bash vzdumpqemu vma No space left on device rootlongmox truncate size vzdumpqemu vma truncate failed to truncate vzdumpqemu vma at bytes No space left on device rm vzdumpqemu vma is a large file I was hoping to delete to clear up space Im not sure whats going on The underlying zpool apparently has utilization but all the zvols are full Additionally there are no quotas that seem to be causing this rootlongmox zfs get quota NAME PROPERTY VALUE SOURCE rpool quota none default rpooldump quota rpoolROOT quota none default rpoolROOTdump quota rpoolROOTpve quota none default rpoolROOTpve dump quota rpoolbase disk quota rpoolbase disk base quota rpoolbase disk dump quota rpoolbasevol disk quota none default rpoolbasevol disk base quota rpoolbasevol disk dump quota rpooldata quota none default rpooldatadump quota rpoolsubvol disk quota none default rpoolsubvol disk dump quota rpoolsubvol disk quota none default rpoolsubvol disk dump quota rpoolsubvol disk quota none default rpoolsubvol disk dump quota rpoolsubvol disk quota none default rpoolsubvol disk dump quota rpoolsubvol disk quota none default rpoolsubvol disk dump quota rpoolsubvol disk quota none default rpoolsubvol disk dump quota rpoolsubvol disk quota none default rpoolsubvol disk dump quota rpoolsubvol disk quota none default rpoolsubvol disk dump quota rpoolsubvol disk quota none default rpoolsubvol disk dump quota rpoolsubvol disk quota none default rpoolsubvol disk dump quota rpoolsubvol disk quota none default rpoolsubvol disk dump quota rpoolsubvol disk quota none default rpoolsubvol disk dump quota rpoolsubvol disk quota none default rpoolsubvol disk dump quota rpoolsubvol disk quota none default rpoolsubvol disk dump quota rpoolsubvol disk quota none default rpoolsubvol disk dump quota rpoolvm disk quota rpoolvm disk dump quota rpoolvm disk quota rpoolvm disk dump quota rpoolvm disk quota none default rpoolvm disk snap quota rpoolvm disk dump quota rpoolvm disk quota rpoolvm disk dump quota rpoolvm disk quota rpoolvm disk dump quota rpoolvm disk quota rpoolvm disk dump quota rpoolvm disk quota rpoolvm disk dump quota rpoolvm disk quota rpoolvm disk dump quota rpoolvm disk quota rpoolvm disk dump quota rpoolvm disk quota rpoolvm disk dump quota rootlongmox zfs get refquota NAME PROPERTY VALUE SOURCE rpool refquota none default rpooldump refquota rpoolROOT refquota none default rpoolROOTdump refquota rpoolROOTpve refquota none default rpoolROOTpve dump refquota rpoolbase disk refquota rpoolbase disk base refquota rpoolbase disk dump refquota rpoolbasevol disk refquota G local rpoolbasevol disk base refquota rpoolbasevol disk dump refquota rpooldata refquota none default rpooldatadump refquota rpoolsubvol disk refquota G local rpoolsubvol disk dump refquota rpoolsubvol disk refquota G local rpoolsubvol disk dump refquota rpoolsubvol disk refquota G local rpoolsubvol disk dump refquota rpoolsubvol disk refquota G local rpoolsubvol disk dump refquota rpoolsubvol disk refquota G local rpoolsubvol disk dump refquota rpoolsubvol disk refquota G local rpoolsubvol disk dump refquota rpoolsubvol disk refquota G local rpoolsubvol disk dump refquota rpoolsubvol disk refquota G local rpoolsubvol disk dump refquota rpoolsubvol disk refquota G local rpoolsubvol disk dump refquota rpoolsubvol disk refquota G local rpoolsubvol disk dump refquota rpoolsubvol disk refquota G local rpoolsubvol disk dump refquota rpoolsubvol disk refquota G local rpoolsubvol disk dump refquota rpoolsubvol disk refquota G local rpoolsubvol disk dump refquota rpoolsubvol disk refquota G local rpoolsubvol disk dump refquota rpoolsubvol disk refquota G local rpoolsubvol disk dump refquota rpoolvm disk refquota rpoolvm disk dump refquota rpoolvm disk refquota rpoolvm disk dump refquota rpoolvm disk refquota none default rpoolvm disk snap refquota rpoolvm disk dump refquota rpoolvm disk refquota rpoolvm disk dump refquota rpoolvm disk refquota rpoolvm disk dump refquota rpoolvm disk refquota rpoolvm disk dump refquota rpoolvm disk refquota rpoolvm disk dump refquota rpoolvm disk refquota rpoolvm disk dump refquota rpoolvm disk refquota rpoolvm disk dump refquota rpoolvm disk refquota rpoolvm disk dump refquota Additionally I have no idea where internal error Channel number out of range is coming from It doesnt appear to exist in the current source System information add version after character Type Linux x CentOS Distribution Name CentOS Distribution Version Linux Kernel el x Architecture x ZFS Version SPL Version Commands to find ZFSSPL versions modinfo zfs grep iw version modinfo spl grep iw version Describe the problem youre observing Over the recent vacations I decided to replace three drives in our ZFS pool which had been showing occasional read errors This should be a fairly fast procedure because the pool would be pretty much completely idle The pool consists of raidz vdevs with TB SAS drives in each Coincidentally the three drives I wanted to replace were all in different vdevs I manually faulted all three drives with zpool offline f olddrive and then replaced each drive physically in the enclosure and did a zpool replace olddrive newdrive Time between the three replace commands was in the range of minutes basically the time it took to remove the drive get the new one in the sled and replace it After I had done this zpool status showed the pool as resilvering and each of the three devices had resilvering after them in the list of devices After a while the time estimate stabilized at about hours total which seemed fine I let this run until completion logging in remotely about when it was scheduled to finish to check that everything was ok To my surprise when the resilver finished it immediately started again but now zpool status showed the first drive Id replaced as ONLINE while the other two were again marked as resilvering The time estimate again was about hours after stabilizing At this point I prepared myself for this resilver to finish and then to start again with only the last drive but surprisingly after another two and a half days the second resilver finished and all drives show up as ONLINE There seems to be several suboptimal behaviors here One is that obviously ZFS is capable of concurrently resilvering several drives in a pool at least when theyre in different vdevs as was the case here Ive never tried doing it in the same vdev and probably wouldnt unless the drives were totally dead but it sometimes chooses to do it and other times not The other problem is that if ZFS is really just resilvering one drive out of several it should probably say so somehow Theres no way I could find out to tell if one or several drives were being resilvered they all showed up as resilvering in zpool status Please fill out the following template which will help other contributors address your issue Thank you for reporting an issue IMPORTANT Please search our issue tracker before making a new issue If you cannot find a similar issue then create a new issue IMPORTANT This issue tracker is for bugs and issues only Please search the wiki and the mailing list archives before asking questions on the mailing list Please fill in as much of the template as possible System information add version after character Type VersionName Distribution Name Arch Linux Linux Kernel Architecture x ZFS Version SPL Version Commands to find ZFSSPL versions modinfo zfs grep iw version modinfo spl grep iw version ZFS supports lazytime and datasets can be mounted with lazytime by running mount o remountlazytime tankfoo but there is no property in the dataset to influence lazytime so that zfs mount tankfoo will set lazytime on Please fill out the following template which will help other contributors address your issue Thank you for reporting an issue IMPORTANT Please search our issue tracker before making a new issue If you cannot find a similar issue then create a new issue IMPORTANT This issue tracker is for bugs and issues only Please search the wiki and the mailing list archives before asking questions on the mailing list Please fill in as much of the template as possible System information add version after character Type VersionName Distribution Name Ubuntu Eoan Distribution Version Linux Kernel Architecture aarch ZFS Version SPL Version Commands to find ZFSSPL versions modinfo zfs grep iw version modinfo spl grep iw version Describe the problem youre observing zfsdkms package fails to build due to kernel changes introduced in v rc commit ce a a c aee dcd f d b d Author Al Viro virozenivlinuxorguk Date Sun Jul kill the last users of userpathlpathpathdir old wrappers with few callers remaining put them out of their misery Signedoffby Al Viro virozenivlinuxorguk Removed userpathdir from includelinuxnameih This breaks building of spl as splvnodec uses this function Describe how to reproduce the problem Attempt to install zfsdkms on Ubuntu Eoan running a v or above kernel Include any warningerrorsbacktraces from the system logs IMPORTANT Please mark logs and text output from terminal commands or else Github will not display them correctly An example is provided below Example this is an example how log text should be marked wrap it with Unpacking zfsdkms ubuntu Setting up zfsdkms ubuntu Loading new zfs DKMS files Building for g d c Building initial module for g d c Error Bad return status for module build on kernel g d c aarch Consult varlibdkmszfs buildmakelog for more information dpkg error processing package zfsdkms configure installed zfsdkms package postinstallation script subprocess returned error exit status Errors were encountered while processing zfsdkms E Subprocess usrbindpkg returned an error code CC M varlibdkmszfs buildmodulezfscityhasho varlibdkmszfs buildmodulesplsplvnodec In function vnsetpwd varlibdkmszfs buildmodulesplsplvnodec error implicit declaration of function userpathdir did you mean userpathat Werrorimplicitfunctiondeclaration rc userpathdirfilename path userpathat CC M varlibdkmszfs buildmodulezfsdbufo cc some warnings being treated as errors CC M varlibdkmszfs buildmodulezfsdbufstatso make scriptsMakefilebuild varlibdkmszfs buildmodulesplsplvnodeo Error make scriptsMakefilebuild varlibdkmszfs buildmodulespl Error make Waiting for unfinished jobs Reference Please fill out the following template which will help other contributors review your Pull Request Provide a general summary of your changes in the Title above dmubufholdarraybydnode can create zio on demand saving on few allocations Documentation on ZFS Buildbot options can be found at Motivation and Context Why is this change required What problem does it solve If it fixes an open issue please link to the issue here Description Describe your changes in detail How Has This Been Tested Please describe in detail how you tested your changes tested with Lustre using number of different tests including wellknown like dbench iozone Include details of your testing environment and the tests you ran to mostly in KVM see how your change affects other areas of the code etc If your change is a performance enhancement please provide benchmarks here Lustres sanity a takes s intead of s wo the patch Please think about using the draft PR feature if appropriate Types of changes What types of changes does your code introduce Put an x in all the boxes that apply Bug fix nonbreaking change which fixes an issue New feature nonbreaking change which adds functionality X Performance enhancement nonbreaking change which improves efficiency Code cleanup nonbreaking change which makes code smaller or more readable Breaking change fix or feature that would cause existing functionality to change Documentation a change to man pages or other documentation Checklist Go over all the following points and put an x in all the boxes that apply If youre unsure about any of these dont hesitate to ask Were here to help My code follows the ZFS on Linux code style requirements I have updated the documentation accordingly I have read the contributing document I have added tests to cover my changes I have run the ZFS Test Suite with this change applied All commit messages are properly formatted and contain Signedoffby 