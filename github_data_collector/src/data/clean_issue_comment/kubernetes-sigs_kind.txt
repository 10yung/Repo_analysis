implements Hi I recently try to build node image myself and found an interesting thing I preloaded images like etcd coredns that out of bazel build range But kind does not use preloaded images instead it pulls from upstream ignoring loaded images according to this line it calls dockerPull method But I also see a PullIfNotPresent method I want to know that why dont use PullIfNotPresent while building node image Minor change to the flag description all for kind delete clusters Just being a bit more explicit about what it does at the moment See for more information What would you like to be added The current implementation of kind delete clusters all only takes into account one kubeconfig path at a time It would be nice to take into account if clusters created by kind have different kubeconfig paths Why is this needed The purpose of kind delete clusters all is to delete all clusters not just clusters associated with a particular kubeconfig Its by no means a major issue to only delete all clusters by a particular kubeconfig and I believe this should satisfy most users of kind However it would be nice to support both options of deleting all by a kubeconfig and actually deleting all clusters regardless of kubeconfig path What happened configyaml kind Cluster apiVersion kindxk siov alpha networking ipFamily ipv exactly as in kind create cluster configconfigyaml Creating cluster kind Ensuring node image kindestnodev Preparing nodes Writing configuration Starting controlplane ERROR failed to create cluster failed to init node with kubeadm command docker exec privileged kindcontrolplane kubeadm init ignorepreflighterrorsall configkindkubeadmconf skiptokenprint v failed with error exit status Environment kind version kind v go linuxamd kubectl version Client Version versionInfoMajor Minor GitVersionv beta GitCommit c d bb d e bc b c b bfdffc bd GitTreeStatearchive BuildDate T Z GoVersiongo Compilergc Platformlinuxamd docker info Containers Running Paused Stopped Images Server Version Storage Driver overlay Backing Filesystem xfs Supports dtype true Native Overlay Diff true Logging Driver journald Cgroup Driver systemd Plugins Volume local Network bridge host macvlan null overlay Log awslogs fluentd gcplogs gelf journald jsonfile local logentries splunk syslog Swarm inactive Runtimes runc Default Runtime runc Init Binary usrlibexecdockerdockerinit containerd version runc version ce e cd a ce ef f f d add a ac init version v expected fec b d c ef f f c b Security Options seccomp Profile default selinux Kernel Version fc x Operating System Fedora Workstation Edition OSType linux Architecture x CPUs Total Memory GiB Docker Root Dir varlibdocker Debug Mode client false Debug Mode server false Registry Labels Experimental false Insecure Registries Live Restore Enabled true What happened Whe configuring a pod subnet a subnet smaller than will cause the kubecontrollermanagerkindcontrolplane to enter a CrashLoop due to this error k n kubesystem logs kubecontrollermanagerkindcontrolplane grep cidr F nodeipamcontrollergo Controller Invalid clustercidr mask size of cluster CIDR must be less than nodecidrmasksize What you expected to happen If there is a minimum size of a podSubnet the config should validate that the subnet given meets the criteria Otherwise it should function normally perhaps by adjusting the CIDR mask How to reproduce it shell cat EOF clusteryaml apiVersion kindsigsk siov alpha kind Cluster networking podSubnet EOF kind create cluster configclusteryaml Creating cluster kind Ensuring node image kindestnodev Preparing nodes Creating kubeadm config Starting controlplane Installing CNI Installing StorageClass Waiting m s for controlplane Ready WARNING Timed out waiting for Ready Cluster creation complete You can now use the cluster with export KUBECONFIGkind get kubeconfigpath namekind kubectl clusterinfo docker exec it kindcontrolplane kubectl kubeconfigetckubernetesadminconf n kubesystem get pod kubecontrollermanagerkindcontrolplane NAME READY STATUS RESTARTS AGE kubecontrollermanagerkindcontrolplane CrashLoopBackOff m Anything else we need to know sh host docker exec it kindcontrolplane binsh docker export KUBECONFIGetckubernetesadminconf docker kubectl n kubesystem get pod kubecontrollermanagerkindcontrolplane NAME READY STATUS RESTARTS AGE kubecontrollermanagerkindcontrolplane CrashLoopBackOff m s docker kubectl n kubesystem logs kubecontrollermanagerkindcontrolplane grep cidr F nodeipamcontrollergo Controller Invalid clustercidr mask size of cluster CIDR must be less than nodecidrmasksize docker kubectl n kubesystem get pod kubecontrollermanagerkindcontrolplane o yaml apiVersion v kind Pod metadata annotations kubernetesioconfighash ad fb d e e f d dc cc kubernetesioconfigmirror ad fb d e e f d dc cc kubernetesioconfigseen T Z kubernetesioconfigsource file creationTimestamp T Z labels component kubecontrollermanager tier controlplane name kubecontrollermanagerkindcontrolplane namespace kubesystem resourceVersion selfLink apiv namespaceskubesystempodskubecontrollermanagerkindcontrolplane uid d a fd ed b e acb ebca d spec containers command kubecontrollermanager allocatenodecidrstrue authenticationkubeconfigetckubernetescontrollermanagerconf authorizationkubeconfigetckubernetescontrollermanagerconf bindaddress clientcafileetckubernetespkicacrt clustercidr clustersigningcertfileetckubernetespkicacrt clustersigningkeyfileetckubernetespkicakey controllersbootstrapsignertokencleaner enablehostpathprovisionertrue kubeconfigetckubernetescontrollermanagerconf leaderelecttrue nodecidrmasksize requestheaderclientcafileetckubernetespkifrontproxycacrt rootcafileetckubernetespkicacrt serviceaccountprivatekeyfileetckubernetespkisakey useserviceaccountcredentialstrue snip Environment sh kind version v kubectl version Client Version versionInfoMajor Minor GitVersionv GitCommit c f cee a d c b e bd b b GitTreeStateclean BuildDate T Z GoVersiongo Compilergc Platformlinuxamd Server Version versionInfoMajor Minor GitVersionv GitCommit c d bb d e bc b c b bfdffc bd GitTreeStateclean BuildDate T Z GoVersiongo Compilergc Platformlinuxamd docker info grep i version Server Version containerd version b a c af e c db c f fa runc version e f a c f e d a c b d aa init version fec Kernel Version linuxkit cat etcosrelease NAMEUbuntu VERSION LTS Bionic Beaver IDubuntu IDLIKEdebian PRETTYNAMEUbuntu LTS VERSIONID HOMEURL SUPPORTURL BUGREPORTURL PRIVACYPOLICYURL VERSIONCODENAMEbionic UBUNTUCODENAMEbionic just the Dual Stack PR with an additional commit that forces the IP family to dual stack so we can test it in the CI What happened Block device tests running inside the KinD cluster are flaky It looks like this is because the Docker container in which kubelet runs has a static copy of the hosts dev In devfs new loop devices are added by the kernel as needed But inside the container only those devloop devices are available which existed already on the host when the container was created causing losetup to fail losetup tmptestfile failed to set up loop device No such file or directory What you expected to happen dev needs to be shared with the host This isnt nice from a security perspective but I dont see another solution How to reproduce it as minimally and precisely as possible Create a KinD cluster pick one container with docker ps then run losetup repeatedly docker exec f e f a sh c ls devloop devloopcontrol devloop devloop devloop devloop devloop devloop devloop devloop docker exec f e f a sh c truncate s G tmptestfile losetup f show tmptestfile devloop docker exec f e f a sh c truncate s G tmptestfile losetup f show tmptestfile devloop docker exec f e f a sh c truncate s G tmptestfile losetup f show tmptestfile losetup tmptestfile failed to set up loop device No such file or directory Environment kind version use kind version v alpha Hi Why do I get this error Also this is after configuring servicekubernetes to listen on port rather than Is it possible in kind to have a pod that lives on controlplanenode to send http POST requests to the kubernetes code If yes am I meant to wrap this pod with the servicekubernetes for this purpose How do I make this work networkwise in kind Thanks in advance Please only use this template for submitting enhancement requests What would you like to be added Add a special node to the Kind cluster with registry role This node can run docker registry without the user having to run any extra scripts to configure Kind cluster Why is this needed This can be an alternative to kind load dockerimage and naturally integrates with docker push CI workflows Also the registry storage can be mounted from the host and kind load can be avoided for easier setup 