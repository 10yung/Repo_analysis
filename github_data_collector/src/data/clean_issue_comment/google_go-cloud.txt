 Is your feature request related to a problem Please describe I am uploading public files to buckets using blob api I would like to get the https url not signed url for the object from api call Fixes As described in the bug a batched set of actions with multiple Create actions would drop all but one of the Creates This is because an internal grouping operation used to order actions so that Gets and Writes for the same key are in an appropriate order uses Key as a map key Create actions may have a nil Key so only one survived To fix this keep a separate slice of actions with nil Keys during the grouping and append them to the write group at the end Describe the bug Multiple documents cannot be created within single ActionList only one of the documents would be created To Reproduce Steps to reproduce the behavior golang package docstorecreate import context testing githubcomstretchrtestifyassert goclouddevdocstorememdocstore type Doc struct ID string Payload string func TestMemDocstoret testingT ctx contextBackground coll err memdocstoreOpenCollectionID nil assertNoErrort err in DocPayload one in DocPayload two err collActionsCreate in Create in Doctx assertNoErrort err assertNotEmptyt in ID assertNotEmptyt in ID Expected behavior All documents would be created Version v Additional context The root cause is implementation of GroupActions function that maps write ops under a key When documents are provided for Create operation all keys are empty and therefore only one of the documents would be created This is generic bug that happens for all docstore drivers Please use a title starting with the name of the affected package or all followed by a colon followed by a short summary of the feature request Example blobgcsblob add support for more blobbing Is your feature request related to a problem Please describe We would like to use inmemory blob store but something shared across all the Kubernetes replicas of my microservice Describe the solution youd like A blob store interface which stores data in Redis with LRU or time based eviction Describe alternatives youve considered Write my own caching interface examplezip Is your feature request related to a problem Please describe The docstore API cannot be used with Google Cloud Firestore in Datastore mode The docstoregcpfirestore driver is not sufficient for this trying to results in an error could not save foo docstore codeFailedPrecondition rpc error code FailedPrecondition desc The Cloud Firestore API is not available for Datastore Mode projects See the attached example program It uses GCP application default credentials It expects a GCP project name in the GCPPROJECTNAME environment variable Describe the solution youd like A driver implementation for Google Cloud Firestore in Datastore mode Describe alternatives youve considered Using a different backing service or cloud provider Additional context The GCP Firestore documentation mentions Native Mode has a limit of k writess This makes it unsuitable for writeheavy workloads at certain scales While Docstore Mode does not have an exact figure for its upper writess limit it indicates the limit isnt as low Describe the bug When attempting to perform a Copy operation on a blob in an S bucket which is larger than the maximum size of GB for a single copy operation see the following error message is displayed The specified copy source is larger than the maximum allowable size for a copy source To Reproduce Place a large file GB into a bucket and attempt to Copy it Expected behavior The object should copy without issue regardless of size Version commit a e e ad d e e d c aa Is your feature request related to a problem Please describe Im using the blob List functionality but it returns an iterator that is a little awkward to use In the standard library we see two examples of iterators and To use them you structure a loop like so pseudocode go for scanNext scanGetNextItem if scanErr handle error The ListIterator on the other hand forces you to write a loop with breaks and extra confusing error handling go for item err iterNextctx contextContext switch err case ioEOF break case nil use the item default in the default case handle the error weird With this pattern you have to handle ioEOF even though we arent doing IO at least not directly and it isnt an actual error and we have to write this sort of deeply nested code in a switch or if else if else statement which is awkward to write and read Describe the solution youd like Switch to use the more standard iterator pattern The main disadvantage as far as I can see is you cant pass a separate context for each call but Im not sure why you would want to do that You dont even know if it is making a network call so it doesnt really make sense to have a separate context each time Not specifying a context for each call would also make this match up better with other blob functionality Writers and Readers in this package use one context for their whole lifetimes for example Describe alternatives youve considered It looks like this project is not at yet and I believe you can make this change without breaking by just adding the functionality But if you dont want to make the change it would be nice to give people an example of how to write an appropriate for loop Additional context Make sure this is working to set the HTTP header correctly Secrets version is here Is your feature request related to a problem Please describe We hit a limitation when processing messages from an AWS SQS FIFO queue with message grouping A single batch can contain messages for multiple groups and if a message for one group has a failure there is no way to rejectnack all the messages from that group for the batch The Subscription interface only speaks in messages not batches This meant we were unable to use the pubsub package without resorting to limiting batch sizes to message Describe the solution youd like Given the description of the pubsub package only really mentions interact ing with publishsubscribe systems maybe a queue package for interacting with message queuing systems is more appropriate The difference being that messages would be persisted maybe delivered in sequence andor via streams This might also apply to natsstreaming kafka and others Describe alternatives youve considered Alternatively some additional options or interface methods for greater control of how messages are consumed would help but on the surface that feels like it edges towards a mixing of responsibilities Additional context This issue might be loosely related in that it talks about streaming topics and queues EDIT grammar formatting pubsubnatspubsub add nats streaming support Fixes Adds support for NATS Streaming Server 