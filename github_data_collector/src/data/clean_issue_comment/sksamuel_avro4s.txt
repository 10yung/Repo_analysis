We are using Avro s to build Avro schemas After migrating the Avro version from to and avro s we faced an NotSerializableException orgapacheavroConversionsDecimalConversion The following code reproduces the issue import javaioByteArrayOutputStream ObjectOutputStream import comsksamuelavro sEncoder val oos new ObjectOutputStreamnew ByteArrayOutputStream ooswriteObjectEncoderbigDecimalEncoder oosclose We assume Encoders should be serializable The problem is that avroDecimalConverion is not serializable which is out of scope of avro s The following is a potential BugFix for avro s in line transient private lazy val converter new ConversionsDecimalConversion Is that bugfix in line with the rest of the project Should we open a PR or will you take it from the issue In avro s I dont find the option to generate Schemas from a Scala case class that generate this type for a field json type type string avrojavastring String So if I have scala case class Accountid String it will generate for id field json fields name id type string This additional avrojavastring String is necessary since apparently is checked when comparing schemas Am I stuck having to define JSON schemas and not using the feature of defining case classes Cheers DO NOT MERGE for discussion purposes only Hello Weve been using Avro s to build avro schemas based on case classes and its been super helpful when readingwriting Parquet data via Scio Spotifys Scala API for Apache Beam I wanted to open an issue because up until this point weve been able to use Avro s x alongside Scio beta even though ScioBeam have not migrated past Avro Scio just released and were now seeing issues due to the Avro version mismatch I understand that Avro x is pretty outdated but from what I can tell both Spark and Beam have not been able to migrate to x yet because of the breaking datetime changes Heres the Beam PR that trailed out late last year attempting the upgrade for some context Ive been able to make some minimal modifications to Avro s to depend on Avro and utilizing this avro score avro dependency resolves the issues Im seeing with Scio Im interested in getting your input on whether maintaining an avro branch in avro s is worthwhile Otherwise Ill likely maintain this implementation locally for the forseeable future as its unclear how soon SparkBeam will be able to make the leap forward This sketch is intended to show how schema computations and name mappings could be moved out of the critical encode decode path I tried to find the most challenging parts and address them namely how to handle sealed traits and case classes how to handle annotations on fields how to propagate annotations to primitive codecs Not sure though these actually are the most challenging parts For the design I decided to factor on the shape of the data that is encoded decoded rather than to split schema generation encoding and decoding I tried this since I feel the coupling is stronger this way I see that having a SchemaFor T will be necessary in order to support custom encoding decoding and have build eg RecordCodec so that a SchemaFor could reuse the same code RecordCodecbuildSchema A key piece on how to deal with annotationsbased schema customization is the method CodecwithSchema It allows to enrich a schema that has been derived bottomup by Magnolia with topdown information available through field annotations I must say Im not sure I fully understood the namespaceconversions that happen in the schema helpers encoding decoding of sealed traits and case classes ie GenericEncoderencode and SchemaHelperextractTraitSubschema and the like so while I think it could work it still needs a test to actually prove it is working properly On a second thought I think TypeUnionCodec isnt working fully as before as it doesnt propagate annotations of the sealed trait to the subtypes this can be fixed however I moved the FieldMapper to an implicit of the Magnoliabased codec derivation That has the downside that it makes an implicitNotFound annotation on the Codec really necessary as it is virtually impossible for a newcomer to understand whats going on if there is no implicit FieldMapper in scope Let me know what you think and if you think it is worthwhile to proceed with this I have been working with avro s since years and I must first say thank you for providing this very convenient library It is really well documented all around and nice to use thank you for the effort you put into it Recently however I have experienced a significant performance impact of avro s on the overall CPU utilization of a codebase I managed to narrow it down to the performance impact of using sealed traits with one subtype having a type parameter Ive provided you a benchmark for encoding decoding that shows the impact of sealed traits and type parameters and compares as well with the performance of partially handrolling encoders and decoders I will share the benchmarks as PR soon The numbers are roughly as follows Benchmark avro s simple field decoding ms Benchmark avro s type union decoding ms Benchmark avro s type parameter decoding ms Benchmark avro s union type with type param decoding ms Benchmark Avro specific record union type field decoding ms Benchmark avro s union type with type param handrolled decoding ms Benchmark avro s simple field encoding ms Benchmark avro s type union encoding ms Benchmark avro s type parameter encoding ms Benchmark avro s union type with type param encoding ms Benchmark Avro specific record union type field encoding ms Benchmark avro s union type with type param handrolled encoding ms So introducing a sealed trait with subtypes makes encoding x slower and decoding x slower Introducing a type parameter on its own has little impact the combination however makes encoding x slower and decoding x slower in the benchmark Comparing this with a partially handrolled encoding decoding is also interesting encoding is x slower than handrolled and x slower than specific avro record and decoding is x slower than handrolled and x slower than specific avro record In my real usecase the improvement of partially handrolling the encoding gives a x speedup I have been looking around the code without deep knowledge of it and pointed a profiler at it without deep knowledge of it and gained the impression that there is a lot of potential to move significant amount of logic from encoding decoding paths to encoder decoder generation I will first now provide you the benchmarks so that you can evaluate what I have been measuring and then we can maybe proceed with identifying areas of potential improvement I might be possible to gain significant speedup by moving the Avro Schema parameter from the encode decode methods to a class parameter of encoder decoders As far as I looked this seems possible but there might be corner cases that make it infeasible fix the exception when use avro s in Spark Caused by javaioNotSerializableException comsksamuelavro sSafeFromanon Serialization stack object not serializable class comsksamuelavro sSafeFromanon value comsksamuelavro sSafeFromanon e ea field class comsksamuelavro sDecoderanon name safeFromS type class comsksamuelavro sSafeFrom object class comsksamuelavro sDecoderanon comsksamuelavro sDecoderanon dfd c b field class comsksamuelavro sDecoderanon name decoder type interface comsksamuelavro sDecoder object class comsksamuelavro sDecoderanon comsksamuelavro sDecoderanon cad very appreciate that the SafeFrom class can extends Serializable Thanks Error javaioNotSerializableException scalaSerialVersionUID at javaioObjectOutputStreamwriteObject ObjectOutputStreamjava at javaioObjectOutputStreamwriteArrayObjectOutputStreamjava at javaioObjectOutputStreamwriteObject ObjectOutputStreamjava at javaioObjectOutputStreamdefaultWriteFieldsObjectOutputStreamjava at javaioObjectOutputStreamwriteSerialDataObjectOutputStreamjava at javaioObjectOutputStreamwriteOrdinaryObjectObjectOutputStreamjava at javaioObjectOutputStreamwriteObject ObjectOutputStreamjava at javaioObjectOutputStreamdefaultWriteFieldsObjectOutputStreamjava at javaioObjectOutputStreamwriteSerialDataObjectOutputStreamjava at javaioObjectOutputStreamwriteOrdinaryObjectObjectOutputStreamjava at javaioObjectOutputStreamwriteObject ObjectOutputStreamjava at javaioObjectOutputStreamdefaultWriteFieldsObjectOutputStreamjava at javaioObjectOutputStreamwriteSerialDataObjectOutputStreamjava at javaioObjectOutputStreamwriteOrdinaryObjectObjectOutputStreamjava at javaioObjectOutputStreamwriteObject ObjectOutputStreamjava at javaioObjectOutputStreamdefaultWriteFieldsObjectOutputStreamjava at javaioObjectOutputStreamwriteSerialDataObjectOutputStreamjava at javaioObjectOutputStreamwriteOrdinaryObjectObjectOutputStreamjava at javaioObjectOutputStreamwriteObject ObjectOutputStreamjava at javaioObjectOutputStreamdefaultWriteFieldsObjectOutputStreamjava at javaioObjectOutputStreamwriteSerialDataObjectOutputStreamjava Version RC because Im still on and also transitive deps on Avro Reproduction Scala worksheet import javaioFileOutputStream ObjectOutputStream import comsksamuelavro sEncoder Namespaced object Rebuffers case class Metricscount Int case class EarlyLateearly Metrics case class Statssession Option EarlyLate case class Rebuffersnetwork Option RebuffersStats case class PlaybackSessionrebuffers Option Rebuffers val fileOut new FileOutputStreamremoveme val out new ObjectOutputStreamfileOut outwriteObjectEncoder PlaybackSession printlnDone But if I unnamespace the subclasses then it works import javaioFileOutputStream ObjectOutputStream import comsksamuelavro sEncoder Not namespaced case class Metricscount Int case class EarlyLateearly Metrics case class Statssession Option EarlyLate case class Rebuffersnetwork Option Stats case class PlaybackSession rebuffers Option Rebuffers val fileOut new FileOutputStreamremoveme val out new ObjectOutputStreamfileOut outwriteObjectEncoder PlaybackSession printlnDone Thanks The new schema enum default code is putting the default field in the wrong location in the schema For example the EnumSchemaTestsupport default scala enum test case generates the following schema where the default field is placed outside of the enum JSON object type record name ScalaEnumsWithDefault namespace comsksamuelavro sschema fields name colours type type enum name Colours namespace comsksamuelavro sschema symbols Red Amber Green default Red According to the default field should be at the same level as the symbols field type record name ScalaEnumsWithDefault namespace comsksamuelavro sschema fields name colours type type enum name Colours namespace comsksamuelavro sschema symbols Red Amber Green default Red Below are two tests that check the compatibility of the above schema when adding a new enum value Orange The first test puts the default field in the same location as avro s and fails the compatibility test The second test puts the default field next to the symbols field and succeeds import comsksamuelavro sAvroSchema import orgapacheavroSchemaCompatibility import orgapacheavroSchemaCompatibilitySchemaCompatibilityType import orgjunitjupiterapiAssertions import orgjunitjupiterapiTest class Avro sEnumDefaultTest object Colours extends Enumeration val Red Amber Green Value case class ScalaEnumsWithDefaultcolours ColoursValue ColoursRed This test uses the AVRO SchemaCompatibilitycheckReaderWriterCompatibility method to check the compatibility of two versions of an enum schema with a default enum field in the location produced by avro s In the first version of the schema the enum values are Red Amber and Green In the second version Orange is added to the enum values The test shows that the two schemas versions are not actually compatible because the default enum value field default Red is output in the wrong location in the schema Test def avro sDefaultEnumTest Unit this is the schema generated by avro s val schemaVersion new orgapacheavroSchemaParserparse type record name ScalaEnumsWithDefault namespace Avro sEnumDefaultTest fields name colours type type enum name Colours symbols Red Amber Green default Red stripMargin just to be sure actually generate the schema and check against the one above val avro sSchema AvroSchema ScalaEnumsWithDefault assertEqualsavro sSchema schemaVersion this schema adds a new enum value Orange It should be compatible the the prior version of the schema due to the default value but its not because the default value is in the wrong location val schemaVersion new orgapacheavroSchemaParserparse type record name ScalaEnumsWithDefault namespace Avro sEnumDefaultTest fields name colours type type enum name Colours symbols Red Amber Green Orange default Red stripMargin val schemaPairCompatibility SchemaCompatibilitySchemaPairCompatibility SchemaCompatibilitycheckReaderWriterCompatibility schemaVersion schemaVersion this assert fails assertEqualsSchemaCompatibilityTypeCOMPATIBLE schemaPairCompatibilitygetType This test uses the AVRO SchemaCompatibilitycheckReaderWriterCompatibility method to check the compatibility of two versions of an enum schema with a default enum field placed next to the symbols field as specified in In the first version of the schema the enum values are Red Amber and Green In the second version Orange is added to the enum values The test shows that the two schemas versions are compatible Test def avro sDefaultEnumTest Unit this is the schema generated by avro s val schema WithDefaultOutsideEnumType new orgapacheavroSchemaParserparse type record name ScalaEnumsWithDefault namespace Avro sEnumDefaultTest fields name colours type type enum name Colours symbols Red Amber Green default Red stripMargin this schema adds a new enum value Orange It should be compatible the the prior version of the schema due to the default value val schema ModifiedWithDefaultOutsideEnumType new orgapacheavroSchemaParserparse type record name ScalaEnumsWithDefault namespace Avro sEnumDefaultTest fields name colours type type enum name Colours symbols Red Amber Green Orange default Red stripMargin val schemaPairCompatibility SchemaCompatibilitySchemaPairCompatibility SchemaCompatibilitycheckReaderWriterCompatibility schema WithDefaultOutsideEnumType schema ModifiedWithDefaultOutsideEnumType succeeds assertEqualsSchemaCompatibilityTypeCOMPATIBLE schemaPairCompatibilitygetType 