The file normalizenndatam and normalizecnndatam have the code such as for l numelnnsize Find the max and rescale weightmax maxmaxmax nnWl activationmax maxmaxmax nnal scalefactor maxweightmax activationmax appliedinvfactor scalefactor previousfactor nnWl nnWl appliedinvfactor factorlogl appliedinvfactor previousfactor appliedinvfactor end or for ii numelnetlayersl a for j numelnetlayersla netlayerslkiij netlayerslkiij currentfactor end end factorlogl currentfactor previousfactor currentfactor Sothe previousfactor equals to the appliedinvfactor or currentfactor rather than the scalefactor as in the paper Hi When I ran this code on the network cnn I see that the last layer weight scaling factor is different from that in the paper Could you please let me know if the scaling technique used for last layer was the same as everywhere else Thanks dannyneil Hi Dan Recently I want to use the cifar datasets to training exampleconv network When I change part of the code for the cifar but there is a error I donnot know how to solve it Load data randstate load mnistuint trainx doublereshapetrainx testx doublereshapetestx trainy doubletrainy testy doubletesty cifar load data load labels trainx doublereshapedata for i index labelsi trainyiindex end trainy doubletrainy load testbatch testx doublereshapedata testy zeros for i index labelsi testyiindex end testy doubletesty cnnffm function net cnnffnet x n numelnetlayers netlayers a x dispsizex unusedinputs randsizex netfirstlayerdropout netlayers a unusedinputs inputmaps for l n for each layer if strcmpnetlayersltype c for j netlayersloutputmaps for each output map if netlayerslusedmapsj create temp output map z zerossizenetlayersl a netlayerslkernelsize netlayerslkernelsize dispsizez for i inputmaps for each input map convolve with corresponding kernel and add to temp output map xxnetlayers a yynetlayers k aa convnnetlayersl ai netlayerslkij valid dispsizeaa z z convnnetlayersl ai netlayerslkij valid end add bias pass through nonlinearity netlayerslaj netactfunz netlayerslbj else netlayerslaj zerossizenetlayersl a netlayerslkernelsize netlayerslkernelsize end end set number of input maps to this layers number of outputmaps inputmaps netlayersloutputmaps elseif strcmpnetlayersltype s downsample for j inputmaps z convnnetlayersl aj onesnetlayerslscale netlayerslscale valid replace with variable netlayerslaj z netlayerslscale end netlayerslscale end end end end The error is exampleconvnet epoch Index exceeds matrix dimensions Error in cnnff line z z convnnetlayersl ai netlayerslkij valid Error in cnntrain line net cnnffnet batchx Error in exampleconvnet line cnn cnntraincnn trainx trainy opts So could you give me some ideas to solve this problem Thank you very much Bin Hi Dan Thanks for sharing your code I was trying to add ReLU support to DeepLearningToolbox when I found this repository Would you be so kind to help me with my questions Im looking at the backprop realization for ReLU and cant understand one thing about handling ReLU at output layer see line at nnbpm Looks like you decided to set the derivative of Lossfunction wrt total input of ReLUunit in output layer to the same formula as expected for simple linear unit ignoring the fact that ReLUs derivative is zero for negative input Shouldnt the ReLU in output layer have a special handling as it does in hidden layers Something like matlab case relu dn nne singlennan instead of matlab case softmaxlinear relu dn nne Anyway with the help of your code I added ReLU support to clean DeepLearningToolbox instance Then I tried to test gradients for numerical correction by updating testnngradientsarenumericallycorrect to include relu in tests But then the test started to throw assertion violation in nnchecknumgrad with relu activation and output linear Mere raising error threshold in times more didnt help so I wonder why is that I checked the relu realization multiple times but it look good BTW thats when I found the reason for the first question but it doesnt matter in this question because output is linear unit Did you tried to update your version of testnngradientsarenumericallycorrect to test ReLU Just curious why did you remove bias units from NN code Biases significantly enhances neuron ability to discriminate various patterns Whats the point to remove them Thanks 