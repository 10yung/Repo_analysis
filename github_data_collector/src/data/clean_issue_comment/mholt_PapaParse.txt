Ive been studying PapaParses code from days It is difficult to understand for a programmer without proper architecture defined in the docs I have to read to code and interpret the concept which could be wrong also When I parse remote file file variable in complete callback is just a string with filename instead of File javascript complete functionresults file consolelogParsing complete results file How can I pass File Hello how are you Ive been using PapaParse in Nodejs to parse big remote files with streams The library is great thank you very much for your dedication One thing Im trying to achieve is to be able to retrieve the errors field of an output in PapaParses stream When you process the data in one go that is when not using a stream the output you get is of the following structure data errors meta However when piping PapaParses read stream to a writable stream I dont receive an object with errors and meta fields but a parsed row only I would like to know if there is any way of receiving this whole object for each parsed row as Im very interested in the TooFewFields and TooManyFields errors This is important for me because I would like to maintain the same error checking interface when parsing with and without PapaParses stream even if it means having an impact in the parsing performance Happy holidays Full error Uncaught DOMException Failed to execute open on XMLHttpRequest Invalid URL at lreadChunk blob at lnextChunk blob at lstream blob at Objectparse blob at fonmessage blob The code which works fine when I dont include worker true function downloadWeatherlocation string weather location let rowNumber PapaparsedataWeatherRawlocationcsv download true dynamicTyping true header true worker true steprow any weather location pushrowdata as RawWeatherType complete consolelogWeather downloaded for location The only two things I can think of are a bug in the library unlikely or that I need to set up something special in webpack to work with webworkers Heres my webpack file Is there any way to use a promise instead of a callback Ive gotten to where I hate callbacks Issue If I pass a CSV stream to Papaparse that contains special characters it sometimes breaks the special characters so they show up as eg How to reproduce See example at Press Run at the top of the page What should happen There should only be output of character What happened Theres random occurrences of Root cause These two lines are responsible for this issue So when papaparse reads a chunk it directly calls toString of that chunk However a chunk consists of bytes and some utf characters are two bytes long consists of two bytes and a and other regular characters is just one byte Now if the chunk splits right between a multibyte character like papaparse calls toString to both parts of the character distinctly and produces two weird characters from end of first chunk transforms to from start of second chunk transforms to How to fix this issue If received chunks of bytes the concatenation should be done in bytes too eg using Bufferconcat Papaparse should not call toString before it has split the stream to lines so the partialLine remains as a buffer rather than a string type Hi When I am handling csv file which is originally encoded with Windows cp via eg demo website and choose ISO as encoding in options the special chars are decoded wrongly I tested it for euro sign which has value in Windows and in ISO Parsing cp encoded file with euro signs as ISO should end up displaying some other character in place of euro However euro sign is visible in results What is more it doesnt work like that if I upload ISO encoded file and choose cp as encoding Euro sign is that case replaced with some other character Would love to get your feedback on my wishlist for Papaparse Were going to dedicate some time for the team Flatfile before the end of year to move anything on this list were agreed on to a next release Migrate to ES Typescript or Flow This is something that after a deep audit of the source code we feel is necessary to ensure long term reliability In a data oriented library like this type strength will allow for easier reasoning about the code add stability and prevent unseen bugs ES will allow for more readable code Since not everybody understands TS or Flow deeply enough to have confidence to contribute My recommendation is to keep the config permissive to allow for vanillaJS contributions and have core contributors add stronger typing before merging to master Separate NodeJS build from Browser build This will allow for a lighter package when using in the browser vast majority based on cursory analysis as well as open more freedom to invest in optimizations for each stack independently These could either be distributed as different packages eg papaparsecore and papaparsenode or a second build in the same package eg import PapaParse from papaparsenode Reduce core sugar and add plugin framework Weve noticed a lot of the open issues relate to desired support of edge cases or unique data scenarios that shouldnt be treated as part of the core csv parser but are entirely legitimate use cases The goal here is to distribute a core package with common functionality and allow users to choose additional use cases as needed Candidates papaparsehttp adapter for downloading or streaming data from web can be optimized separately for nodejs and browser as well as opens up for other adapters for things like S with plenty of optimizations papaparsetypes split out the typecasting logic theres a lot of room for improvement here wbetter understanding of boolean types dates etc But it doesnt make sense to invest that into a core csv parser library papaparseunparse theres been a decent amount of confusion with users about how different configurations relate to parse vs unparse These are also distinctly different problems to solve for Future Candidates Things like detectencoding to auto detect file encoding generousescaping for the common unescaped quotes situation and many other user requests Additionally framework specific components like an HoC for React could be awesome Improved docs Would love to see updated searchable docs with both auto generated API references as well as guides fiddles and improved demo Im a fan of docusaurus for this Wed be happy to contribute content design here Reorganized source code With almost lines in papaparsejs its time to tackle deconstructing that a bit into components that are easier to reason about Since b aa da c e ef years ago lines when most of the tools we have at our disposal today werent available we havent changed much Time for a src folder Lets follow as a guide Tests coverage crossbrowser testing CI based distribution We should take advantage of setting up the Sauce testing matrix so we dont break things in old browsers as we go Also itd be great if we could use Github Actions to auto deploy master and release candidates to npm bower etc In addition we should improve unit test coverage in addition to the mainly acceptance testing we have now Other Pipes Promises etc Piping for easier composition of logic also relatively required for plugin framework Promises because its shim for legacy browsers Allow for some dependencies and ensure fossacom scans are run on them lets not reinvent all the wheels Functional first no classes unless necessary Package decomposition As seen in the above example switching to an npm org wmultiple packages and likely using something like lerna to manage the packages Backwards compatibility adapter Because this would be a pretty robust overhaul we should publish an adapter thats fully backwards compatible with recomposed elements Possibly papaparselegacy allowing people to move forwards without a complete overhaul It could also identify the things they arent using and give them a custom migration checklist Were happy to take on the work of this overhaul here at Flatfile so keep in mind were not asking for a lot of work from the community But do please provide feedback on all of this we want to chart a path forwards that makes sense to everybody Also what do you want to see Comment with new ideas or criticisms approval of the above I am trying to download a csvgz file and then be able to treat the data rows by rows on a node server I dont mind the method request https but somehow I do not manage to get clean data that Papa Parse would parse normally Now I am getting some encoded strangeness My last try is the following try var streamHttp await new Promiseresolve reject var buffer httpsgeturl res ressetEncodingbinary let gunzip zlibcreateGzip respipegunzip gunzip ondata data bufferpushdatatoString onend resolvebufferjoin catch e consoleloge PapaparsestreamHttp delimiter step row consolelogrow complete results consolelogcomplete but the result I am getting in the console log is full of g x so I guess I have a problem with encoding The url I use is not directly to a csvgz file but to an automatically generated address ending in where the file is I dont know if it matters or not I do not mind a totally different approach than mine just trying to find a solution Thanks in advance and keep up the great work with papa parse If I get the solution Id be glad to propose a mini example to be added to the doc