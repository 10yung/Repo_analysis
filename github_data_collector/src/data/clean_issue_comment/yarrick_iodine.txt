None None Throughput is increased in legacy mode by dropping packets if the client is not ready to send immediately Upstream packets must conform to the pingpong traffic pattern or be dropped and TCP recovers gracefully from intentional packet loss by sending some segments out of order This is tremendously beneficial on WiFi where latency is variable if the only available DNS server will only tolerate legacy mode Personally I have seen this simple change double my download speed Dear iodine maintainers I implement the n auto to get external ip through STUN protocol Please review this patch and see if it is good This patch isnt important now because I just noticed that you fix the problem I met in Debian In Debian n auto doesnt work because externalipnet is off But youve already fix that But I still think using STUN might be good because it is quite common and standard Many Thanks Paul Overview This fork of iodine was intended primarily to improve performance by using a TCPlike sliding window protocol for having multiple in flight fragments both upstream and downstream This allows greatly increased performance on highlatency connections In order to do so the whole dataping structure has been changed details available in docproto txt Some limited testing has been conducted the results of which can be found in the updated man page This has been almost fully tested on Linux amd and compiles without warnings however no other platforms have been tested yet Due to some hacks to get millisecond timer precision on Windows see windowsh for gettimeofday and struct timeval macros various functionality may not work as expected Unit tests have been updated to suit changes to the main code base and a basic sliding window test was created which tests some of the essential functions Issues This fork is still in development and I plan to keep it up to date with the main iodine repository as much as possible There are probably lots of currently undiscovered bugs and certainly lots of problems with intolerant DNS servers which cause performance and connectivity issues To help diagnose these problems I strongly recommend that you try V to print connection statistics such as the number of queries per second fragments lost failures timeouts roundtrip time etc High query rates such as sec will probably result in DNS servers dropping queries or responding with errors or invalid replies Try changing the encoding to something which uses less strange characters such as base base or the DNS query type if you have total connection failures Reduce the upstream and downstream window sizes using w and W options from the default values to something more suitable to your connection lower roundtrip time means the window size does not have to be so large to get the same throughput If connection succeeds but data stops flowing and DNS queries are still being answered correctly check the stats printout for this information rebuild iodine and iodined with make debug Turn on more debugging with DDDDDD use less Ds if you experience graphic lag in your terminal due to excessive output and copy the debug output on both iodined and iodine corresponding to the time when the problem started Features Most of the important feature additions are listed here Guaranteed data arrival no protection from corruption however if DNS query fails iodine d will resend fragments as required Command line options have been to adjust timeouts and sliding window behaviour Multiple nameservers can be specified to reduce load on a single DNS server Lazy mode now supports any number within reasonable limits of pending queries waiting at the server adjusted using the downstream window timeout option Clientside statistics report every number of seconds specified with V option More finegrained client control over data compression server query timeout and other important connection parameters Client side minimum send interval as an attempt to ratelimit connections if using DNS servers which drop queries under high volume Server timeout is adjusted automatically based on target timeout and the roundtrip time More finegrained automatic adjustment of target timeout and immediate mode switching I may have forgotten to mention some features here but this should cover most of them Protocol Overview Due to the nature of the sliding window protocol the entire data transfer protocol needed to be rewritten The new protocol is detailed in the docs and although the basic DNS encapsulation is the same the headers have been moreorless completely rearranged Upstream and downstream are functionally equivalent at the sliding window layer where new data packets ie from tun device on either client or server are treated as follows Data is optionally compressed depending on userspecific upstreamdownstream compression flags Raw or compressed data is then split into a number of fragments depending on the users maximum fragsize calculated beforehand during the handshake process Each fragment is added to the outgoing window buffer same for both downstream and upstream and assigned a unique sequence ID from to The window buffer maintains a pointer to the current fragment which is the start of the sending window and while sending fragments only the windowsize number of fragments are sent in order from the fragment at the start of the window The fragments are sent in order from the start of the window as described above When the fragments are received at the other end they are placed in the receiving window buffer at an offset determined by their sequence ID This way outoforder fragments very common with loadbalanced DNS servers can be easily handled without dropping them The receiving end will check if it has received both the starting fragment the final fragment and all the inbetween fragments and if it has the full data packet is retrieved and the pointer to the start of the next received chunk is moved forwards by the number of fragments The received full packet is optionally uncompressed and sent to the tun device The receiving end immediately ACKs the fragment using its sequence ID using either a ping or a data packet both have space for an ACK When the ACK is received at the sending side for a fragment it is marked off in the sending buffer as having been successfully received by the other end and based on this the window can be moved forward and the next few fragments sent Other Information Any other information is available in the code Ive put in a reasonable amount of hopefully helpful comments so it shouldnt be too hard to understand Feel free to ask any questions or make comments on any of the changes Ive done quite a lot of refactoring to clarify various parts of the code or make things simpler Thanks for all the great work in making something like iodine and thanks again for making it open source Its truly been a pleasure working with it and I hope to be able to contribute something to this project Complete rewrite of the build system using autotools 