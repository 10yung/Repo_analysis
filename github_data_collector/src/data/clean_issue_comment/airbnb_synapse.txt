This PR adds the following capability for Synapse to run a custom command to validate HAProxy config prior to writing the config file config is held in a staging file before that Note this does not affect reloading just writing the config HAProxy will still get restarted but it will continue to use good config The goal is that the production config file ie that used by the HAProxy process will always be valid configuration for HAProxy validation unit tests for those additions Because of state file caching it is still possible that certain bad state will remain in Synapse inmemory and in cache file However that state will not propagate to HAProxy HAProxy will remain with the last good state Tests Local Testing of bad haproxy config in ZK Synapse does not write bad HAProxy config I T INFO SynapseServiceWatcherZookeeperWatcher synapse no configforgenerator data from mangotest for service mangotest keep existing configforgenerator I T INFO SynapseSynapse synapse configuring haproxy I T INFO SynapseConfigGeneratorHaproxy watcher config mangotestfrontend mode http backend mode http option httpchk health httpcheck expect string OK I T INFO SynapseConfigGeneratorHaproxy frontendscache mangotest nfrontend mangotest tmode http tbind localhost tdefaultbackend mangotest I T INFO SynapseConfigGeneratorHaproxy backendscache mangotest nbackend mangotest tmode http toption httpchk health thttpcheck expect string OK tserver iHOST IP cookie iHOST check inter s rise fall id tserver iHOST IP cookie iHOST check inter s rise fall id I T INFO SynapseConfigGeneratorHaproxy watcherrevisions mangotest E T ERROR SynapseConfigGeneratorHaproxy synapse invalid generated HAProxy config checked via haproxy c f usrlocaletchaproxyhaproxystagingcfg ALERT parsing usrlocaletchaproxyhaproxystagingcfg server iHOST id custom id already used at usrlocaletchaproxyhaproxystagingcfg server iHOST ALERT Errors found in configuration file usrlocaletchaproxyhaproxystagingcfg ALERT Fatal errors found in configuration I T INFO SynapseConfigGeneratorHaproxy synapse checked HAProxy config located at usrlocaletchaproxyhaproxystagingcfg status false I T INFO SynapseConfigGeneratorHaproxy synapse at time waiting until to restart Running HAProxy config is still valid while staging is invalid haproxy c f usrlocaletchaproxyhaproxycfg Configuration file is valid haproxy c f usrlocaletchaproxyhaproxystagingcfg ALERT parsing usrlocaletchaproxyhaproxystagingcfg server iHOST id custom id already used at usrlocaletchaproxyhaproxystagingcfg server iHOST ALERT Errors found in configuration file usrlocaletchaproxyhaproxystagingcfg ALERT Fatal errors found in configuration Normal behavior HAProxy configuration is valid mangotest I T INFO SynapseServiceWatcherZookeeperWatcher synapse zk exists at productionsecureservicesmangocanaryservices for times I T INFO SynapseServiceWatcherZookeeperWatcher synapse discovering backends for service mangocanary I T INFO SynapseServiceWatcherZookeeperWatcher synapse zk list children at productionsecureservicesmangocanaryservices for times I T INFO SynapseServiceWatcherZookeeperWatcher synapse discovered backends for service mangocanary I T INFO SynapseServiceWatcherZookeeperWatcher synapse no configforgenerator data from mangocanary for service mangocanary keep existing configforgenerator I T INFO SynapseSynapse synapse configuring haproxy I T INFO SynapseConfigGeneratorHaproxy synapse restart required because we have a new backend mangocanaryiHOST I T INFO SynapseConfigGeneratorHaproxy synapse reconfigured haproxy via varhaproxystats sock I T INFO SynapseConfigGeneratorHaproxy synapse checked HAProxy config located at etchaproxyhaproxystagingcfg status true I T INFO SynapseConfigGeneratorHaproxy synapse restarted haproxy Behavior with invalid haproxy config mangotest I T INFO SynapseConfigGeneratorHaproxy synapse restart required because we have a new backend mangocanaryibadconfigrandomip I T INFO SynapseConfigGeneratorHaproxy synapse reconfigured haproxy via varhaproxystats sock I T INFO SynapseConfigGeneratorHaproxy synapse restart required because haproxyserveroptions changed for iHOST E T ERROR SynapseConfigGeneratorHaproxy synapse invalid generated HAProxy config checked via sudo haproxy c f etchaproxyhaproxystagingcfg ALERT parsing etchaproxyhaproxystagingcfg server ibadconfigrandomip invalid address randomip in randomip ALERT parsing etchaproxyhaproxystagingcfg server iHOST id custom id already used at etchaproxyhaproxystagingcfg server iHOST ALERT Errors found in configuration file etchaproxyhaproxystagingcfg ALERT Fatal errors found in configuration I T INFO SynapseConfigGeneratorHaproxy synapse checked HAProxy config located at etchaproxyhaproxystagingcfg status false I T INFO SynapseConfigGeneratorHaproxy synapse restarted haproxy Running HAProxy config is valid because it is unchanged sudo haproxy c f etchaproxyhaproxycfg Configuration file is valid sudo haproxy c f etchaproxyhaproxystagingcfg ALERT parsing etchaproxyhaproxystagingcfg server ibadconfigrandomip invalid address randomip in randomip ALERT parsing etchaproxyhaproxystagingcfg server iHOST id custom id already used at etchaproxyhaproxystagingcfg server iHOST ALERT Errors found in configuration file etchaproxyhaproxystagingcfg ALERT Fatal errors found in configuration sudo service haproxy status haproxy is running dochecks false behavior with valid haproxy config No check is performed I T INFO SynapseServiceWatcherZookeeperWatcher synapse zk list children at productionsecureservicesmangocanaryservices for times I T INFO SynapseServiceWatcherZookeeperWatcher synapse discovered backends for service mangocanary I T INFO SynapseServiceWatcherZookeeperWatcher synapse no configforgenerator data from mangocanary for service mangocanary keep existing configforgenerator I T INFO SynapseSynapse synapse configuring haproxy I T INFO SynapseConfigGeneratorHaproxy synapse restart required because we added new section mangocanary I T INFO SynapseConfigGeneratorHaproxy synapse reconfigured haproxy via varhaproxystats sock I T INFO SynapseConfigGeneratorHaproxy synapse restarted haproxy dochecks False behavior with invalid haproxy config No check is performed but HAProxy will fail to restart I T INFO SynapseServiceWatcherZookeeperWatcher synapse no configforgenerator data from mangocanary for service mangocanary keep existing configforgenerator I T INFO SynapseSynapse synapse configuring haproxy I T INFO SynapseConfigGeneratorHaproxy synapse restart required because we have a new backend mangocanaryimyhost I T INFO SynapseConfigGeneratorHaproxy synapse reconfigured haproxy via varhaproxystats sock ALERT parsing etchaproxyhaproxycfg server imyhost id custom id already used at etchaproxyhaproxycfg server imyhost ALERT Errors found in configuration file etchaproxyhaproxycfg ALERT Fatal errors found in configuration E T ERROR SynapseConfigGeneratorHaproxy failed to reload haproxy via sudo service haproxy reload Reloading haproxy haproxy fail And the production config is invalid sudo haproxy c f etchaproxyhaproxycfg ALERT parsing etchaproxyhaproxycfg server imyhost id custom id already used at etchaproxyhaproxycfg server imyhost ALERT Errors found in configuration file etchaproxyhaproxycfg ALERT Fatal errors found in configuration Reviewers anson austinzhu JasonJian cc Ramyak internal ticket TR to avoid security issue Having upgraded to the most recent version of synaps my logs are flooded with messages from collectd collectd statsd plugin Unable to parse line synapsewatcherpingcount cwatchernamestorereaderrawca Initial investigation suggests that the statsd implementation introduced recently is formatted specifically for DataDog so will not work with standard collectd apologies if Ive got this wrong its Friday afternoon and I want to go home Need a way to turn off the datadogging If no keys are provided in discovery ENV synapse is susceptible to entering an unrecoverable failure if it tries to get keys from metadata service when it is unavailable Note that this upgrade changes minimum required ruby version from p to bundle audit check Name nokogiri Version Advisory CVE Criticality Unknown URL Title Nokogiri gem contains several vulnerabilities in libxml and libxslt Solution upgrade to Name nokogiri Version Advisory CVE Criticality Unknown URL Title Nokogiri gem contains two upstream vulnerabilities in libxslt Solution upgrade to Name nokogiri Version Advisory CVE Criticality Unknown URL Title Nokogiri gem contains several vulnerabilities in libxml and libxslt Solution upgrade to Name nokogiri Version Advisory CVE Criticality Unknown URL Title Nokogiri gem contains two upstream vulnerabilities in libxslt Solution upgrade to Vulnerabilities found Reviewers chasechilders lap Hi guys did you have try synapse to replace RDS I have tried it and the result I T INFO SynapseSynapse synapse starting I T INFO SynapseServiceWatcherEc tagWatcher Connecting to EC region apsoutheast I T INFO SynapseServiceWatcherEc tagWatcher synapse ec tag watcher looking for instances tagged with Namestagingsynapse I T INFO SynapseSynapse synapse configuring haproxy W T WARN SynapseConfigGeneratorHaproxy synapse restart required because socket command show stat failed with error ErrnoECONNREFUSED Connection refused connect for runhaproxyadminsock I T INFO SynapseConfigGeneratorHaproxy synapse restarted haproxy W T WARN SynapseServiceWatcherEc tagWatcher synapse no backends for service proddb using default servers namestagingsynapse hoststagingdatabaseblablardsamazonawscom port thanks haproxy supports relative port addresses prefixed by a or If this is set the server port is determined by adding the value to the clients port In the base servicewatcher there is currently a check to verify that the override port is an integer which breaks existing configurations I assume nginx doesnt support a similar behaviour would you accept a patch or should we just maintain our own version The DefaultProvider takes aws keys from environment variables config if available otherwise falling back to ec metadata IAM role If you are using keys from the latter they refresh automatically when they get near to expiry If the metadata service is down at this point Synapse enters a broken state where it has no credentials and cannot recover We have seen this in our production environment Im currently looking at a patch whereby I specifically select the EC Provider if no keys are provided by the environment config inserting the following into ec tagrb before the call to AWSEC new unless discovery awsaccesskeyid ENV awsaccesskeyid discovery awssecretaccesskey ENV awssecretaccesskey AWSconfigcredentialprovider AWSCoreCredentialProvidersEC Providernewretries end Does this seem like a reasonable approach Happy to submit a PR Migrated to v of AWSSDK Updated rspec tests Updated READMEmd to cover new functionality This adds the ability to define a taghash key in the options block for the ec tag watcher I had the use case of wanting to target only the intersection of two tags such as taghash environment staging someservice installed And the existing watcher functionality did not suffice This is backwards compatible with existing synapseyaml configuration files and the tagnametagvalue keys for ec tag Cheers Alex Really useful for local dev where an image might not have a tag yet The fix here was to use split instead of rpartition because rpartition has some unexpected behavior when there is no match foobarrpartition foo bar foorpartition foo 