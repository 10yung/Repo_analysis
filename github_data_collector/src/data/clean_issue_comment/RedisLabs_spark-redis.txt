Hi I am trying to load redis data of the form the hash keys dont have a fixed value hset id feature value hset id feature value hset id feature value hset id feature value to a dataframe with the schema id featurevalue but the method fromRedisHash returns only the hash Is there a way to retrieve the context assuming I dont want to use the dataframe approach because it will require more memory ie hset id feature feature value value I initialized spark session like this scala val spark SparkSession builder appNameExtractRawTablesFromMySQL configsparkredishost myredisurlcom configsparkredisport getOrCreate Later when I want to save rdd to redis scala sctoRedisSETrdd tags But it says Failed connecting to host localhost Looks like it is not honoring this configuration scala configsparkredishost myredisurlcom Error stack redisclientsjedisexceptionsJedisConnectionException Could not get a resource from the pool at redisclientsjedisutilPoolgetResourcePooljava at redisclientsjedisJedisPoolgetResourceJedisPooljava at comredislabsproviderredisConnectionPoolconnectConnectionPoolscala at comredislabsproviderredisRedisEndpointconnectRedisConfigscala at comredislabsproviderredisRedisConfigclusterEnabledRedisConfigscala at comredislabsproviderredisRedisConfiggetNodesRedisConfigscala at comredislabsproviderredisRedisConfiggetHostsRedisConfigscala at comredislabsproviderredisRedisConfiginitRedisConfigscala at comredislabsproviderredisRedisConfigfromSparkConfRedisConfigscala at comredislabsproviderredisRedisContexttoRedisSETdefault redisFunctionsscala elided Caused by redisclientsjedisexceptionsJedisConnectionException Failed connecting to host localhost at redisclientsjedisConnectionconnectConnectionjava at redisclientsjedisBinaryClientconnectBinaryClientjava at redisclientsjedisBinaryJedisconnectBinaryJedisjava at redisclientsjedisJedisFactorymakeObjectJedisFactoryjava at orgapachecommonspool implGenericObjectPoolcreateGenericObjectPooljava at orgapachecommonspool implGenericObjectPoolborrowObjectGenericObjectPooljava at orgapachecommonspool implGenericObjectPoolborrowObjectGenericObjectPooljava at redisclientsjedisutilPoolgetResourcePooljava more Caused by javanetConnectException Connection refused Connection refused at javanetPlainSocketImplsocketConnectNative Method at javanetAbstractPlainSocketImpldoConnectAbstractPlainSocketImpljava at javanetAbstractPlainSocketImplconnectToAddressAbstractPlainSocketImpljava at javanetAbstractPlainSocketImplconnectAbstractPlainSocketImpljava at javanetSocksSocketImplconnectSocksSocketImpljava at javanetSocketconnectSocketjava at redisclientsjedisConnectionconnectConnectionjava more In the doc val listRDD scfromRedisListkeyPattern can get Lists in redis but it get RDD string which contains lists only How to get the corresponding listname of every row of the listRDD I cannot understand from issues that were raised if sparkredis supports Sentinel I couldnt manage to write Dataframe to Redis running with Sentinel Please confirm that currently lib doesnt provide that functionality Hi Instead of passing the Redis credential in the spark session creation moment Is it possible to pass the info at reading or writing time Something like conf dfwrite optionurl selfurl optionuser selfuser optionpassword selfpasswordsave I have a PySpark DataFrame with columns key bz in which key is string and bz is bytes BinaryType I want to write this dataframe into Redis then my teammates will read them out using Node API I tried ways like dfwriteformatorgapachesparksqlredisoptiontable PPoptionkeycolumn keyoptionhost REDISHOSToptionport REDISPORToptionauth REDISPWoptiontimeout modeappendsave dfwriteformatorgapachesparksqlredisoptiontable PPoptionkeycolumn keyoptionhost REDISHOSToptionport REDISPORToptionauth REDISPWoptiontimeout modeappendoptionmodel binarysave The st method will write only a short and incorrect sequence into Redis will lose a lot of information For the nd method we dont know how to decode the persisted bytes with other languages like Node The binary persistence mode seems only work with SparkRedis library So is it possible to write a byte array DataFrame column to Redis How to do that Please advise Thanks a lot Spark version Spark Redis version Spark Streaming scalaVersion I have a requirement to use Redis SETNX command in my Spark job But in RedisContext there is no such a method to support SETNX hi I am writing a PoC to test sparkredis integration Poc Setup sparkcore sparksql sparkredis redis standalone My poc read data sets from csv files and write to redis Later read data set back through sparkredis library There are data sets write to spark size varies from to k In total million entries are written to redis Write dswriteformatorgapachesparksqlredisoptiontable tableoptionkeycolumn keyColumnmodeSaveModeOverwritesave Read sparkreadformatorgapachesparksqlredisoptiontable FXoptionkeycolumn CurrencyIDload I found reading from redis is very slow even for a small data sets Rewrite above same data sets is also slow The library spent a lot of time Computing partition eg it took mins to calculate partitions Can you please suggest if there is way to tune the performance thanks I am using the example provided in the Java docs and running this on a local spark cluster public void run throws Exception SparkSession sparkSession SparkSessionbuilder masterlocal configsparkredishost redisHost configsparkredisport redisPort configsparkredisdb dbName getOrCreate DatasetRow df sparkSessioncreateDataFrameArraysasList new PersonJohn new PersonPeter Personclass dfwrite formatorgapachesparksqlredis optiontable person optionkeycolumn name modeSaveModeOverwrite save sparkSessionstop This writes to redis successfully but does not actually complete the spark job and call the shutdown hooks This stops at INFO oasSparkContext Successfully stopped SparkContext I am ideally expecting this job to complete so that I can proceed with my DAG with INFO oasuShutdownHookManager Shutdown hook called INFO oasuShutdownHookManager Deleting directory privatevarfolderskkgtt h mx s vywsfdv r gpTsparke fe c a a fdf ed da Using Sparkredis version Hello I am using Spark Redis to populate a Redis cluster as follows dfsamplesampleRate write modeSaveModeAppend formatorgapachesparksqlredis optiontable table optionkeycolumn column optionttl ttl save The Redis cluster itself is hosted on the Kubernetes Engine with a load balancer service and the individual nodes can down and come back during the load process If one of the nodes goes down I see an error like Caused by redisclientsjedisexceptionsJedisConnectionException Failed connecting to host When this happens the whole job terminates Instead I would like to ignore this exception and try to continue loading Is there a way to achieve that Regards 