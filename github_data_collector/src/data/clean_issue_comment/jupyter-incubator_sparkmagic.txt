Creates a PR template to make expectations clear when contributing Description FILL IN Checklist Wrote description of my changes above Added or modified unit tests to reflect my changes Manually tested with a notebook If adding a feature there is an example notebook andor documentation in the README As addressed in issue this PR proposes the following changes Using http clients maps on the SparkController class to reuse clients since it is not needed a new client per cell Using requestsSession instead of plain requests to ensure we store the generated Kerberos ticket cookie instead of negotiating the ticket in every request Add a configuration option to override the default HTTPKerberosAuth constructor Due to the following line selfauth HTTPKerberosAuthmutualauthenticationREQUIRED in this class the Kerberos auth constructor is not able to be configured which causes the sparkmagic to always sends requests to Livy the first sending and the second since the cookie was issued but since we already have the cookie due to the first why we are getting a new one everytime An ideal approach would be to be able to pass custom args to the kerberos auth constructor through the configjson and to change the reliablehttpclientpy to use the requests session instead of plain requests Here is the kerberos auth constructor for reference to a possible solution python class HTTPKerberosAuthAuthBase Attaches HTTP GSSAPIKerberos Authentication to the given Request object def init self mutualauthenticationREQUIRED serviceHTTP delegateFalse forcepreemptiveFalse principalNone hostnameoverrideNone sanitizemutualerrorresponseTrue sendcbtTrue I am submitting this issue to see if anyone is already working on it if not my plan is to submit a PR for this problem I was getting the error Missing Required Header for CSRF protection when trying to add an endpoint I added the XRequestedBy Header in livyreliablehttpclient headers variable and in reliablehttpclient too for good measure I am able to successfully add the endpoint now but when I try to create session is throwing the same CSRF error I imagine its something Im doing wrong but not sure where to go from here locallibpython sitepackagessparkmagiclivyclientlibreliablehttpclientpy in postself relativeurl acceptedstatuscodes data return selfsendrequestrelativeurl acceptedstatuscodes requestsget def postself relativeurl acceptedstatuscodes data Sends a post request Returns a response return selfsendrequestrelativeurl acceptedstatuscodes requestspost data locallibpython sitepackagessparkmagiclivyclientlibreliablehttpclientpy in sendrequestself relativeurl acceptedstatuscodes function data Sends a delete request Returns a response return selfsendrequestrelativeurl acceptedstatuscodes requestsdelete def sendrequestself relativeurl acceptedstatuscodes function dataNone printselfcomposeurlrelativeurl locallibpython sitepackagessparkmagiclivyclientlibreliablehttpclientpy in sendrequesthelperself url acceptedstatuscodes function data retrycount if error raise HttpClientExceptionuError sending http request and maximum retry encountered else raise HttpClientExceptionuInvalid status code from with error payload formatstatus url text HttpClientException Invalid status code from with error payload html head meta httpequivContentType contenttexthtmlcharsetISO titleError title head body h HTTP ERROR h pProblem accessing sessions Reason pre Missing Required Header for CSRF protectionprep hr ismallPowered by Jettysmalli body html Describe the bug In AWS EMR cluster I am using sparkmagic to run a spark job on the cluster that I want to display as a bqplot widget in jupyterhub that is running in Docker container Instead of displaying as a widget it shows the definition of the widget in code I have verified that nodejs is installed on both the container and the cluster and that the environment variables are set correctly But I havent been able to get the results to display properly To Reproduce Create an EMR cluster with Hadoop Spark JupyterHub Hive Hue Zookeeper In your bootstrap install bqplot numpy ipywidgets jupytercontribnbextensions jupyternbextensionsconfigurator widgetsnbextension nodejsv Then run jupyter nbextension enable system py widgetsnbextension jupyter nbextension enable system py bqplot In a step or in the docker image on the cluster install the same packages as above and run the enables Create a pyspark notebook in jupyterhub and run this code python from bqplot import pyplot as plt import numpy as np pltfigure titleline chart nprandomseed n x nplinspace n y npcumsumnprandomrandnn pltplotx y pltshow Instead of a graph like the one in the Screenshots section you will get a definition of your spark job and output like python VBoxchildrenFigurefigmargintop bottom left right layoutLayoutminwidth px scalexLinearScaleallowpaddingFalse max min scaleyLinearScaleallowpaddingFalse max min titleline chart ToolbarfigureFigurefigmargintop bottom left right layoutLayoutminwidth px scalexLinearScaleallowpaddingFalse max min scaleyLinearScaleallowpaddingFalse max min titleline chart Expected behavior A graph looking like the one in the attached Screenshot Screenshots bqplot chart Versions SparkMagic Livy if you know it Spark Python Additional context This is an extremely dumbed down version of what I am trying to do but this demonstrates my issue we are also wanting to use qgrid widgets as well If a column has null value in every rowrecord sql will not drop that entire column To reproduce create a table where a column has only null values eg sql insert into table values null null null I have attached screenshots using results from sql and sparksql Screen Shot at pmpdf Versions SparkMagic Livy Kernel Spark Additional context I believe the problem comes from the fact that since JSON doesnt pick up null values when the data got converted into dict and then converted into dataframe it couldnt have known that there was a missing column We need a way to pick up the schema before populating all the data Right now documentation is a brief README and a bunch of examples in Jupyter notebooks that also double as a manual test suite We should really have real documentation As per this would need Tutorial Your first SparkMagic query Howtos various realworld use cases Reference Config options APIs Explanation How it works devstein juhoautio suhsteve assuming youre not already getting notified of new issues Heres my current checklist for accepting PRs should probably add it to docs somewhere Has unit tests for change If its adding feature has examples in the example notebooks Documented in README where relevant Manual testing on notebook See also the milestone in general Python is no longer being maintained as of Jan The first PySpark release in will drop Python Users on Python can continue to use old versions of Sparkmagic but we should be strongly discouraging Python usage Important add metadata to setuppy to say this only supports Python or later and then pip will automatically pick the right version of SparkMagic Specifically pythonrequires or something along those lines We should run flake and perhaps pylint on the code as part of CI