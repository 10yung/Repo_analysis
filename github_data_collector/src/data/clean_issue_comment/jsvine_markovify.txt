Thank you for this beautiful Markov chain generator Sorry to overwhelm you with yet another PR Im playing around with trying to make markovify models out of huge corpus and I keep finding ways to eke out a little more saved memory This is mainly useful for incrementally building one Text from separate chunks of a corpus too large to store in memory all at once This is slightly better than using combine which requires you to have the overhead of at least two models in memory at once Chain now accepts basemodel which the new Text update method uses I also tweaked how TextChain behave when you initialize them with None or so that you can initialize an empty Text without it throwing an exception and then later call update on it Not sure why that is in there but it looks like it is unneeded Feel free to merge or reject as you see fit