Currently the following network stanza will generate an error network mode bridge port http This is because there is no to field in the port stanza We should change this to map the to field to what ever the host port is assigned This means for a dynamic port if to is not specified Nomad will map the port to the same value that is allocated for the dynamic port For static ports this means the port will get mapped to the same port specified in static The panic described in was caused by a missing length check in gopsutil which weve contributed upstream as Once thats been merged and released we should update gopsutil For reporting security vulnerabilities please refer to the website If you have a question prepend your issue with question or preferably use the nomad mailing list If filing a bug please include the following Nomad version Output from nomad version rootdc consulserver nomad version Nomad v d e d fe c a ea bfef a c f rootdc consulserver consul version Consul v Protocol spoken by default understands to agent will automatically use protocol when speaking to compatible agents Operating system and Environment details Linux Issue I am unable to create more than container with dynamic ports when using the consul connect stanza this only allows me to create container only and obviously in prod I wil be running more than container so this is an issue or I may mis configuring the hcl file Reproduction steps nomad plan jobhcl nomad run jobhcl Job file if appropriate job cinemas datacenters dc ncv region dc region type service group paymentapi count task paymentapi driver docker config image crizstianpaymentservicegov portmap http env DBSERVERS mongodb serviceconsul mongodb serviceconsul mongodb serviceconsul SERVICEPORT CONSULIP resources cpu memory network mode bridge port http service name paymentapi port http connect sidecarservice Nomad Client logs if appropriate If possible please post relevant logs in the issue rootdc consulserver nomad status cinemas ID cinemas Name cinemas Submit Date T Z Type service Priority Datacenters dc ncv Status running Periodic false Parameterized false Summary Task Group Queued Starting Running Failed Complete Lost bookingapi notificationapi paymentapi Future Rescheduling Attempts Task Group Eval ID Eval Time paymentapi cb a m s from now Latest Deployment ID ee Status running Description Deployment is running Deployed Task Group Desired Placed Healthy Unhealthy Progress Deadline bookingapi T Z notificationapi T Z paymentapi T Z Allocations ID Node ID Task Group Version Desired Status Created Modified ce a c a paymentapi run failed s ago s ago a b f a c a paymentapi run failed s ago s ago b c d a c a bookingapi run running s ago s ago af a c a notificationapi run running s ago s ago rootdc consulserver nomad alloc status ce ID ce Eval ID fa Name cinemaspaymentapi Node ID a c a Node Name dc consulserver Job ID cinemas Job Version Client Status failed Client Description Failed tasks Desired Status run Desired Description none Created s ago Modified s ago Deployment ID ee Deployment Health unhealthy Reschedule Eligibility m s from now Allocation Addresses mode bridge Label Dynamic Address http yes connectproxypaymentapi yes Task connectproxypaymentapi is dead Task Resources CPU Memory Disk Addresses MHz MiB MiB MiB Task Events Started At T Z Finished At T Z Total Restarts Last Restart NA Recent Events Time Type Description T Z Killing Sent interrupt Waiting s before force killing T Z Killed Task successfully killed T Z Terminated Exit Code T Z Killing Sent interrupt Waiting s before force killing T Z Sibling Task Failed Tasks sibling paymentapi failed T Z Started Task started by client T Z Task Setup Building Task Directory T Z Received Task received by client Task paymentapi is dead Task Resources CPU Memory Disk Addresses MHz MiB MiB Task Events Started At NA Finished At T Z Total Restarts Last Restart NA Recent Events Time Type Description T Z Killing Sent interrupt Waiting s before force killing T Z Killing Sent interrupt Waiting s before force killing T Z Not Restarting Error was unrecoverable T Z Driver Failure Failed to create container configuration for image crizstianpaymentservicegov sha c d e fdbec aeea dda abb f dafa ff d ab f cb faaa Trying to map ports but no network interface is available T Z Driver Downloading image T Z Task Setup Building Task Directory T Z Received Task received by client Logs and other artifacts may also be sent to nomadossdebughashicorpcom Please link to your Github issue in the email and reference it in the subject line To nomadossdebughashicorpcom Subject GH Errors garbage collecting allocs Emails sent to that address are readable by all HashiCorp employees but are not publicly visible Nomad Server logs if appropriate T Z ERROR clientdrivermgrdocker failed to create container configuration driverdocker imagenamecrizstianpaymentservicegov imageidsha c d e fdbec aeea dda abb f dafa ff d ab f cb faaa errorTrying to map ports but no network interface is available T Z ERROR clientallocrunnertaskrunner running driver failed allocid ce bb b ffb a b abfa taskpaymentapi errorFailed to create container configuration for image crizstianpaymentservicegov sha c d e fdbec aeea dda abb f dafa ff d ab f cb faaa Trying to map ports but no network interface is available T Z INFO clientallocrunnertaskrunner not restarting task allocid ce bb b ffb a b abfa taskpaymentapi reasonError was unrecoverable T Z ERROR clientdrivermgrdocker failed to create container configuration driverdocker imagenamecrizstianpaymentservicegov imageidsha c d e fdbec aeea dda abb f dafa ff d ab f cb faaa errorTrying to map ports but no network interface is available T Z ERROR clientallocrunnertaskrunner running driver failed allocid a b f b f bf ff fe c d taskpaymentapi errorFailed to create container configuration for image crizstianpaymentservicegov sha c d e fdbec aeea dda abb f dafa ff d ab f cb faaa Trying to map ports but no network interface is available ECS Elastic Container Service as per Amazons documentation Amazon Elastic Container Service Amazon ECS is a highly scalable fast container management service This PR attempts to pick up some issues found in Issue has some other issues but this addresses one of the problems associated with leadership flapping In particular this PR addresses two issues First if a nonleader is promoted but then immediately loses leadership dont bother establishing it and attempting Barrier raft transactions By using a buffered channel before establishing leadership we peek into rest of channel to see lost it before attempting to establish leadership This should only occur when leadership is flapping while leaderLoop is runningshutting down Second this fixes a condition where we may reattempt to reconcile and establish leadership even after we lost raft leadership Consider the case where a step down occurs during the leaderLoop Barrier call andor it times out The Barrier call times out after m by default but reconcile interval defaults to m Thus both stopCh and interval clauses are ready in the WAIT select statement Golang may arbitrary chose one resulting into potentially unnecessary Barrier call Here we prioritize honoring stopCh and ensure we return early if Barrier or reconciliation fails Also add a script to ease updating the golang version we use Its a bit hacky but better than manually chasing down version references The changes are mostly bug fixes and seem safe go released includes a fix to the runtime See the Go milestone on our issue tracker for details go released includes fixes to the runtime and the nethttp package See the Go milestone on our issue tracker for details From The e e framework instantiates clients for NomadConsul but the provisioning of the actual Nomad cluster is left to Terraform The Terraform provisioning process uses remoteexec to deploy specific versions of Nomad so that we dont have to bake an AMI every time we want to test a new version But Terraform treats the resulting instances as immutable so we cant use the same tooling to update the version of Nomad inplace This is a prerequisite for upgrade testing This changeset extends the e e framework to provide the option of deploying Nomad and in the future ConsulVault with specific versions to running infrastructure This initial implementation is focused on deploying to a single cluster via ssh because thats our current need but provides interfaces to hook the test run at the start of the run the start of each suite or the start of a given test case The input JSON for configuring provisioning targets can be generated by Terraform or written by hand That work is The e e framework instantiates clients for NomadConsul but the provisioning of the actual Nomad cluster is left to Terraform The Terraform provisioning process uses remoteexec to deploy specific versions of Nomad so that we dont have to bake an AMI every time we want to test a new version But Terraform treats the resulting instances as immutable so we cant use the same tooling to update the version of Nomad inplace This is a prerequisite for upgrade testing This changeset provides Terraform output that written to JSON used by the framework to configure provisioning via terraform output provisioning consumed by provides Terraform output that can be used by test operators to configure their shell via terraform output environment drops remoteexec provisioning steps from Terraform makes changes to the deployment scripts to ensure they can be run multiple times w different versions against the same host For reporting security vulnerabilities please refer to the website If you have a question prepend your issue with question or preferably use the nomad mailing list If filing a bug please include the following Nomad version dev Output from nomad version Operating system and Environment details Archlinux docker image FROM archlinuxlatest Issue make devui fails with no clear resolution bash make devui warning shajs Invalid bin entry for shajs in shajs warning babelpluginproposalobjectrestspread has unmet peer dependency babelcore warning babelpluginproposalobjectrestspread babelpluginsyntaxobjectrestspread has unmet peer dependency babelcore warning emberdata emberdatabuildinfra babelplugintransformblockscoping has unmet peer dependency babelcore warning emberdata emberdatabuildinfra babelplugindebugmacros has unmet peer dependency babelcore warning emberdata emberclitypescript babelplugintransformtypescript has unmet peer dependency babelcore warning emberdata emberclitypescript babelplugintransformtypescript babelpluginsyntaxtypescript has unmet peer dependency babelcore Reproduction steps bash git clone cd nomad make bootstrap make dev make devui Full docker image FROM archlinuxlatest RUN pacman Syu noconfirm RUN pacman S noconfirm go python python pip python pythonpip git make gcc docker neovim yarn ENV PATHrootgobinPATH Setup Hashicorp ENV HCrootgosrcgithubcomhashicorp RUN mkdir p HC Nomad RUN cd HC git clone cd nomad make bootstrap make dev make devui nomad v Job file if appropriate Nomad Client logs if appropriate If possible please post relevant logs in the issue Logs and other artifacts may also be sent to nomadossdebughashicorpcom Please link to your Github issue in the email and reference it in the subject line To nomadossdebughashicorpcom Subject GH Errors garbage collecting allocs Emails sent to that address are readable by all HashiCorp employees but are not publicly visible Nomad Server logs if appropriate 