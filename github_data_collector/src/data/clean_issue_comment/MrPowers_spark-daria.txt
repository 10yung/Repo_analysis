ParquetCompactor is not deleting old files and the inputfilenameparts directory on S We are using the spark databricks platform spark pyspark and mrpowerssparkdaria s After running ParquetCompactor we have a new big parquet file but the old files and the inputfilenameparts directory still exists Is it not possible to use the ParquetCompactor on S When I add one more level of nested structure it fails to flatten uses the StackOverflow answer format val data Seq Row Row this is something cool val schema StructType Seq StructField foo StructType Seq StructField bar StructType Seq StructField zoo StringType true StructField baz StringType true true StructField x StringType true StructField y StringType true StructField z StringType true val df spark createDataFrame sparksparkContextparallelizedata StructTypeschema flattenSchema val expectedDF sparkcreateDF Listthis is something cool List foobarzoo StringType true foobaz StringType true x StringType true y StringType true z StringType true assertSmallDataFrameEquality df expectedDF MrPowers nvander do you think its a good idea to add thie project to to keep the dependencies up to date Apache Spark itself is using these settings matrix java hadoop hadoop hadoop exclude java hadoop hadoop I think sparkdaria can simply be tested with Java and without any Hadoop specified correct I dont think we need to be testing multiple different Java Hadoop versions I just learned that Java and Java are the same thing what Im not even going to ask why Java and Java arent included in this discussion So confusing This reverts commit e d da dd f a b b e d This was merged upstream in spark Im getting a strange error Im not a regular Scala user so I may be doing something silly First I start a Spark shell as follows sh sparkshell packages orgapachehadoophadoopaws mrpowerssparkdaria s Then I run this code scala scala val df sparkreadparquets a Stage df orgapachesparksqlDataFrame more fields scala import comgithubmrpowerssparkdariasqlDataFrameHelpers import comgithubmrpowerssparkdariasqlDataFrameHelpers scala DataFrameHelpersprintAthenaCreateTable df mytable s a javaioFileNotFoundException UserspowersDocumentscodemyappssparkdariatargetscala scoveragedatascoveragemeasurements No such file or directory at javaioFileOutputStreamopen Native Method at javaioFileOutputStreamopenFileOutputStreamjava at javaioFileOutputStreaminitFileOutputStreamjava at javaioFileWriterinitFileWriterjava at scoverageInvokeranonfun applyInvokerscala at scoverageInvokeranonfun applyInvokerscala at scalacollectionconcurrentTrieMapgetOrElseUpdateTrieMapscala at scoverageInvokerinvokedInvokerscala at comgithubmrpowerssparkdariasqlDataFrameHelpersprintAthenaCreate TableDataFrameHelpersscala elided The reference to Userspowers seems strange and suggests some path from the project authors workstation got mistakenly baked into the package somehow There isnt a regexpextractall function in Spark yet Pretty surprising that this hasnt been added yet There is a native implementation of regexpextractall that hasnt been merged with master yet I need this so Ill add a quick dirty UDF approach nvander feel free to add the Spark native implementation if youd like Ill start thinking about Spark native functions once my Spark Summit talk is over sparkdaria follows the standard Scala Java deep nesting package convention thats annoying when importing code Users currently need to import code like this import comgithubmrpowerssparkdariasqlColumnExt I noticed that some libraries are deviating from these Scala conventions and offering imports like this import utest Maybe we can change the package structure so users can import code like import mrpowersdariasqlColumnExt Thoughts nvander manuzhang 