This fills in some missing freebsd diskstats Tested on freebsd stable This requires root so it shouldnt be used This closes etcsystemdsystemnodeexporterservice Unit DescriptionNode exporter Afternetworkonlinetarget Service Userroot Restartonfailure Restartalways RestartSec Change this line if you download the Prometheus on different path user ExecStartnodeexporternodeexporter Install WantedBymultiusertarget systemctl status nodeexporterservice nodeexporterservice Node exporter Loaded loaded etcsystemdsystemnodeexporterservice disabled vendor preset enabled Active activating autorestart Result exitcode since Wed IST s ago Process ExecStartnodeexporternodeexporter codeexited status EXEC Main PID codeexited status EXEC Host operating system output of uname a leserver srcutilcompose uname a Linux server arch SMP PREEMPT Sat Jan x GNULinux nodeexporter version output of nodeexporter version nodeexporter version nodeexporter version branch HEAD revision db e c f d a c b ece e build user rootb a acba build date go version go nodeexporter command line flags nodeexporter image promnodeexporter volumes prochostprocro syshostsysro rootfsro mntdatamntdataro mntdataappdatamntdataappdataro tmptextcollectortmptextcollector command pathprocfshostproc pathsysfshostsys collectorfilesystemignoredmountpoints sysprocdevhostetcvarlibdocker collectorfilesystemignoredfstypes autofsbinfmtmisccgroupconfigfsdebugfsdevptsdevtmpfsfusectlhugetlbfsmqueueoverlayprocprocfspstorerpcpipefssecurityfssysfstracefstmpfsnsfs collectortextfiledirectorytmptextcollector collectorsystemd restart always expose Are you running nodeexporter in Docker Yes What did you do that produced an error Check exported metrics What did you expect to see Proper reporting on root filesystem bytes What did you see instead same values for than for mntdataappdata nodefilesystemavailbytesdevicedevmapperae c a d dea e e f ffstypeext mountpointmntdataappdata e nodefilesystemavailbytesdevicedevmappervg datafstypeext mountpointmntdata e nodefilesystemavailbytesdevicedevsda fstypeext mountpoint e I remember I used to have a rootfs mountpoint instead of but that vanished long time ago Can reproduce this on machines Reuse the Goonly implementation already in place for FreeBSD on Darwin DragonflyBSD NetBSD and OpenBSD Tested on all affected platforms Please note GitHub issues should only be used for feature requests and bug reports For general usagehelpdiscussions please refer to one of prometheus on freenode the Prometheus Users list Before filing a bug report note that running nodeexporter in Docker is not recommended for the reasons detailed in the README Finally also note that nodeexporter is focused on NIX kernels and the WMI exporter should be used instead on Windows For bug reports please fill out the below fields and provide as much detail as possible about your issue For feature requests you may omit the following template Host operating system output of uname a Linux hostname generic Ubuntu SMP Mon Feb UTC x x x GNULinux nodeexporter version output of nodeexporter version nodeexporter version ds branch debiansid revision ds build user pkggomaintainerslistsaliothdebianorg build date go version go nodeexporter command line flags collectormountstats Are you running nodeexporter in Docker No What did you do that produced an error Nothing special just scrape node exporter What did you expect to see Mountstats metrics What did you see instead No mountstats metrics In logs Jan hostname prometheusnodeexporter time T Z levelerror msgERROR mountstats collector failed after s failed to parse mountstats not enough information for NFS stats obfuscated sourcecollectorgo This is caused by following line because my mountstats contain following line sorry I had to obfuscate it this is not real output but it is multiline implid nameXXX STABLE r d sf dfHEAD Fri May EDT userhostnamepathtosomething domainsomethingcomdate caps I believe that this is caused because there are no fields in some line and procfs parser just fails If I remove replace the return nil fmtErrorfnot enough information for NFS stats v ss by continue everything works as expected Thanks for any help This closes Only textfile and time collector but thats a start Currently Node Exporter has a metric called nodeunameinfo which of course exposes uname info While this is nice it does not help if you are running different OSes which could have similar uname info Therefore it would be nice to include a similar nodeosreleaseinfo metric which would provide information regarding the OS releaseversion of the node This information can be easily gotten from files like etcrelease etcosrelease etcversion While this functionality could be added through a textfile collector script I think it would make sense to add it to the base exporter as uname is already supported here Please note GitHub issues should only be used for feature requests and bug reports For general usagehelpdiscussions please refer to one of prometheus on freenode the Prometheus Users list Before filing a bug report note that running nodeexporter in Docker is not recommended for the reasons detailed in the README Finally also note that nodeexporter is focused on NIX kernels and the WMI exporter should be used instead on Windows For bug reports please fill out the below fields and provide as much detail as possible about your issue For feature requests you may omit the following template Host operating system output of uname a Linux lgakubnode el elrepox SMP Fri Nov EST x x x GNULinux nodeexporter version output of nodeexporter version If building from source run make first nodeexporter version branch HEAD revision f f c cfde ff b b f c b build user root cb c b build date go version go nodeexporter command line flags Please list all of the command line flags pathprocfshostproc pathsysfshostsys collectortextfiledirectoryvarlogpulsepointprometheus Are you running nodeexporter in Docker Please note the warning above Node exporter is running in docker in kubernetes cluster image promnodeexporterv Node exporter daemonset manifest was created from stable helm chart There are volumes name proc hostPath proc container readonly hostproc hostPath sys container readonly hostsys hostPath varlogpulsepointprometheus container varlogpulsepointprometheus used for text files metrics No kubernetes security context hostNetwork true hostPID true What did you do that produced an error We are running k s cluster with kuberouter based networking So we are heavily using ipvs At some point we started to observe significant number of errors from all nodecollectors about duplicate ipvs metrics starting and heaving max value at AM and then fading within an hour time T Z levelerror msg error gathering metrics errors occurred from Gatherer collected metric nodeipvsbackendconnectionsactive labelnamelocaladdress value labelnamelocalport value labelnameproto valueTCP labelnameremoteaddress value labelnameremoteport value gaugevalue was collected before with the same name and label values sourceloggo And we have many of that type for nodeipvsbackendweight nodeipvsbackendconnectionsactive nodeipvsbackendconnectionsinactive We have scrape interval seconds and nodes cluster What did you expect to see No errors What did you see instead Bunch of errors starting from some specific time with the same pattern start at AM and then fading within an hour The degree to which a disk has extra work it cant service This value should ideally not exceed Beyond this value would indicate that work is getting queued up In cloud environments this is also likely to be amplified by providers throttling operations if IO exceeds allocated IOPs diskAlertSelector added to support adjustment of alert scope Signedoffby trotttrotttrott trottodaacabeefcom By default this is only applied to sda devices as theyre typically the root volume of a Linux machine Root volumes are targeted in particular because often the cause for saturation is a single workload In a k s context this will likely cause issues for colocated workloads A common solution is to assign the saturating workload to a PVC so its disk usage is isolated and the PVC disk can be tuned accordingly Signedoffby trotttrotttrott christrottgrafanacom SuperQ discordianfish wanted to put this out there as its something Im experimenting with alerting on Seemed like it would be useful for other folks using the mixin