 Description of Issue Today stale bot closed two of my issues after just one month of inactivity This is WAY to short All in all I needed to reopen issues in the last few weeks because of stale bot going rampage on them Please reconfigure that stupid bot so that its less annoying or even disable it altogether Thanks a lot I am ttrying to set up VMs in Opennebula using saltcloud two of them are Windows servers and is centos I am having issues while deploying saltminion on one of the windows machine I get this error sudo usrbinsaltcloud m etcsaltcloudmapsd setconf P y WARNING Failed auth for mech NEGOEX Mech Type is not yet supported WARNING Failed auth for mech NEGOEX Mech Type is not yet supported WARNING Failed auth for mech NEGOEX Mech Type is not yet supported ERROR Port connection timed out ERROR Failed to start Salt on host exchangevgautam WARNING Failed auth for mech NEGOEX Mech Type is not yet supported WARNING Failed auth for mech NEGOEX Mech Type is not yet supported appliance deployed True id image zzAppliancewin k name appliance privateips publicips size state backend deployed True id image zzBackendCentOS raw name backend privateips publicips size state exchange Error Not Deployed Failed to start Salt on host exchange id image zzExchangewin k raw name exchange privateips publicips size state This whole process takes almost minutes which is very slow Most of the time is spent while installing saltminions on Windows VM and has a different behavior everytime If i log on to the failed machine i dont see any salt folder or service Salt Version Salt Dependency Versions cffi cherrypy unknown dateutil Not Installed dockerpy Not Installed gitdb gitpython Not Installed ioflo Not Installed Jinja libgit Not Installed libnacl Not Installed M Crypto Not Installed Mako msgpackpure Not Installed msgpackpython mysqlpython Not Installed pycparser Not Installed pycrypto pycryptodome Not Installed pygit Not Installed Python default Aug pythongnupg Not Installed PyYAML PyZMQ RAET Not Installed smmap timelib Not Installed Tornado ZMQ System Versions dist centos Core locale UTF machine x release el x system Linux version CentOS Linux Core Description of Issue With the rc dropping the deprecation of jsondecodedict in the next release was announced The new way to do it is to use tojson In prepare my states and pillars before upgrading from to rc I noticed issues when replacing jsondecodedict with tojson jsondecodedict would output an in the order it was received tojson will output the keys sorted alphabetically Setup loadyaml as x z some y text x here endload tmptojsonout filemanaged contents xtojson tmpjsondecodedictout filemanaged contents xjsondecodedict Steps to Reproduce Issue I provided a basic test state to see it in action notice the different order between tosjon and jsondecodedict Versions Report Salt Version Salt Dependency Versions cffi cherrypy dateutil dockerpy Not Installed gitdb gitpython ioflo Not Installed Jinja libgit Not Installed libnacl M Crypto Mako Not Installed msgpackpure Not Installed msgpackpython mysqlpython Not Installed pycparser pycrypto pycryptodome Not Installed pygit Not Installed Python default Oct pythongnupg PyYAML PyZMQ RAET Not Installed smmap timelib Not Installed Tornado ZMQ System Versions dist locale UTF machine i pc release system SunOS version Not Installed What does this PR do Enables dnf support for CentOS systems What issues does this PR fix or reference Fixes Previous Behavior If you tried to use pkginstalled this error appears ID timezone Function pkginstalled Result False Comment Error occurred installing packages Additional info follows errors Failed to find executable yum No such file or directory Tests written No Commits signed with GPG Yes Description of Issue in saltreturn table in Postgresql the success column is always false for runners the fullret structure is different for runners there is a return dict in return dict in fullret the real success status is inside that I think the outer return must be updated with inner return values and must not include it Versions Report Salt Version Salt Dependency Versions cffi cherrypy unknown dateutil dockerpy Not Installed gitdb gitpython RC ioflo Not Installed Jinja libgit Not Installed libnacl Not Installed M Crypto Not Installed Mako Not Installed msgpackpure Not Installed msgpackpython mysqlpython pycparser pycrypto pycryptodome Not Installed pygit Not Installed Python default Sep pythongnupg Not Installed PyYAML PyZMQ RAET Not Installed smmap timelib Not Installed Tornado ZMQ What does this PR do Only return cached pillar data for minion that is passed with tgt What issues does this PR fix or reference Fixes Previous Behavior If you run saltrun outjson cachepillar tgtdoesnotexist with a target that does not exist it still returns all the cached pilllar data for ALL minions New Behavior If you run saltrun outjson cachepillar tgtdoesnotexist with a target that does not exist will not return anything Also as you can see I updated the docs to state If tgt is not set will return cached pillars for all minions The reason I did this is this is the current behavior that a user can run cachepillar without a tgt set and all pillar data would be returnedand the other reason is tgt is an not required as its currently a kwarg For these reasons I kept this behavior as its expected If we want to eventually require the tgt argument for some reason then we would need to set it on a deprecation path but I believe this change is best path for now Tests written Yes Commits signed with GPG Yes What does this PR do This PR introduces a new minion configuration option masterreturnstrategy Can be source or any The default of source preserves the current behavior namely the minion will only attempt to return the job results to the master that sent the job If set to any then the minion will first attempt to return the job results to the master that sent the job If that fails then the minion will attempt to return the job results to the other configured masters one by one until successful or the master list has been exhausted masteraliveinterval must also be set This is used to keep track of which masters are currently connected What issues does this PR fix or reference Previous Behavior When a master becomes unavailable after sending a job but before receiving the return information that return info will never make it back to the masters If relying on masterjobcache or eventreturns on the master side then the job return information is lost forever New Behavior When the initiating master becomes unavailable after firing a job to a minion then the minion will attempt to return the job information to another configured master Tests written Not yet Commits signed with GPG No Please review Salts Contributing Guide for best practices See GitHubs page on GPG signing for more information about signing commits with GPG What does this PR do SmartOS will automatically calculate and override the mac property of any nics that have vrrpvrid property set This causes a lot of issues we keep trying to add a new nic if the mac that was provided is incorrect we keep trying to delete the existing nic with the mac based on the vrid we keep trying to incorrect update a nic if the vrid was change cannot be update remove and add new nic What issues does this PR fix or reference na Previous Behavior We would try to addremoveupdate the incorrect nic when vrrpvrid property was present making the state module fail New Behavior We now account for this in parsevmconfig this function is used to map unique identifiers for various smartos resource types nics disks We will use the provided mac address from the resource key unless the vrrpvrid property is present then generate the mac based on the provided vrid This results in the nic being correctly added when the vrid is changes this results in a remove of the old nic and adding of the new nic as is expected by vmadm Tests written Yes Ive added tests to test the mac handling of parsevmconfig Commits signed with GPG No Description of Issue Epic or container of issues related to Sodium release deprecation warnings Description of Issue Having defined a service in salt that is a configuration file thats a managed Jinja template and a package install thats sourced in a yum repository which installs an init script I routinely get a Service failed to die message for the servicedead state However the command issued to stop the service appears to be completing successfully Setup Im using Vagrant to test this I stand up a host on AWS and run a highstate on the machine Here are the relevant SLS files related to this particular state failing pganalyzecollector pkgrepomanaged name pganalyzecollector enabled True humanname pganalyzecollector baseurl gpgkey repogpgcheck sslverify sslcacert etcpkitlscertscabundlecrt metadataexpire pkginstalled require pkgrepo pganalyzecollector service salt pillarget pganalyzestate enable salt pillarget pganalyzeenable require pkg pganalyzecollector file etcpganalyzecollectorconf service postgres etcpganalyzecollectorconf filemanaged source saltpostboottemplatespganalyzecollectorconfj template jinja user root group pganalyze mode Suffice it to say that for the environment in question the pillars here are being rendered correctly the service state is dead and the enable flag is False A little more context This service is a metrics collector for a third party Postgres performance analyzer and it depends upon a running postgres server to collect that data This is why there is a requirement for the service postgres state We want that to be available before we decide to launch pganalyzer which will only happen in higher environments which is why we want it dead at the time salt is applied That postgres service is just fine as shown here ID postgres Function servicerunning Name postgresql Result True Comment Service reloaded Started Duration ms Changes postgresql True The managed template is also rendering correctly Steps to Reproduce Issue I can reproduce this by running a highstate saltcall local statehighstate The output excluding the dozens of other unrelated states looks like this ID pganalyzecollector Function servicedead Result False Comment Service pganalyzecollector failed to die Started Duration ms Changes Versions Report saltcall versionsreport Salt Version Salt Dependency Versions cffi Not Installed cherrypy Not Installed dateutil dockerpy Not Installed gitdb Not Installed gitpython Not Installed ioflo Not Installed Jinja libgit Not Installed libnacl Not Installed M Crypto Not Installed Mako Not Installed msgpackpure Not Installed msgpackpython mysqlpython Not Installed pycparser Not Installed pycrypto pycryptodome Not Installed pygit Not Installed Python default Oct pythongnupg Not Installed PyYAML PyZMQ RAET Not Installed smmap Not Installed timelib Not Installed Tornado ZMQ System Versions dist locale UTF machine x release amzn x system Linux version Not Installed Some Other Details This is an older InitV system so I did some testing around various service stop commands All of the following commands succeed throw no errors and return an exit code of etcinitdpganalyzecollector stop service pganalyzecollector stop saltcall local servicestop pganalyzecollector The last one actually outputs the following in addition to the zero exit code local True I looked at the code as well On my system the service states are defined in usrlibpython distpackagessaltstatesservicepy The logic goes that it will use the inbuilt salt servicestop function and respond to the return code I added a print statement so that this segment of code looks like this funcret salt servicestop name stopkwargs printstop returned formatfuncret if not funcret ret result False ret comment Service failed to dieformatname if enable is True retupdateenablename True resultFalse kwargs elif enable is False retupdatedisablename True resultFalse kwargs return ret And shortly beneath that this code printaftertogglestatus formataftertogglestatus if aftertogglestatus ret result False ret comment Service failed to dieformatname else ret comment Service was killedformatname if enable is True retupdateenablename aftertogglestatus resultnot aftertogglestatus kwargs elif enable is False retupdatedisablename aftertogglestatus resultnot aftertogglestatus kwargs When I run the highstate if I scroll up past the state output I see this stop returned True aftertogglestatus True ERROR Service pganalyzecollector failed to die aftertogglestatus gets set to True based on the servicestatus function I can run that manually saltcall local servicestatus pganalyzecollector local True And I can confirm from the init script that the service is offline etcinitdpganalyzecollector status pganalyzecollector is stopped service pganalyzecollector status pganalyzecollector is stopped So as far as I can tell this service should be considered successfully dead and I cant figure out why salt disagrees with that assessment