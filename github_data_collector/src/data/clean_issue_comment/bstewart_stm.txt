Many package developers now include tidy and glance methods to improve user experience This allows users to interact with standardized model summaries in a programmatic way The broom package supplies tidy and glance functions for a lot of model types The estimatr package from DeclareDesign is an example of a package that supplies its own glance and tidy methods This PR adds tidy and glance methods for the estimateEffect object type It works as follows librarystm datagadarian fx estimateEffect treatment gadarianFit gadarian tidyfx topic term estimate stderror statistic pvalue Intercept e treatment e Intercept e treatment e Intercept e treatment e glancefx uncertainty nobs ntopics Global There is a package called tidystm on github but it has not been updated for years and it seems to calculate quantities differently The benefit of this PR is that the returned quantities are extracted directly from summaryestimateEffect and that they are thus consistent with printed output It is also much more convenient for users to have this as part of the package rather than have to find and install a distinct package Hey first of all thank you for this great package It works like a charm and finds frequent use in my projects Lately Im using it on Twitter data and I am currently trying to find out whether the models can be improved by using hashtags as metadata The problem I am running into now is that the stm functions dont like their metadata as character vectors but rather expect single characters or other variables per row While collapsing the vectors is not a problem it does beg some methodological questions For one chances are that a rather large amount of tweets will have unique combinations of hashtags basically eliminating any prevalence effects It also makes effect estimations rather tedious since the unique hashtag combinations make for a very large number of observations in the variable Right now Im dealing with these issues by reducing the amount of hashtags used in the model by frequency and sorting the hashtags for each document This however still does not seem optimal to me So my question is Is there a way to feed the model character vectors as metadata that I am not seeing And if not are there any plans to implement this functionality in the near future add a check on the mode location and throw an error when out of bounds When the covariate values are misspecified eg covvalue instead of covvalue TRUE there isnt an error message it just errors There should be The wordLengths argument is length two but most people probably just want to mess with the lower bound Should probably throw an error or append Inf and give a warning when passed a scalar There is a strange error that pops up in textProcessor when copying very long metadata fields Not entirely sure why or how to stop it Looking into it it might be possible to simplify how metadata is handled by only maintaining a document index in the metadata The case we need to test against is one where the entire document is dropped before creating the document term matrix Good afternoon more than an issue this is a question I have tried looking for the optimal K value using the Lee and Mimno algorithm and the initialization type set to Spectral However when I try to use the K value obtained from the resulting model to generate a new model by explicitly setting up that value as initialization parameter the outcome is a completely different model despite the fact that I am still using Spectral as inittype Therefore I was wondering if the difference is directly related to the fact that the search K method Lee and Mimno algorithm also affects the training stage Thanks in advance for the clarification and regards C Bordet pointed out a problem that should be addressed tm is actually not using SMART stopwords by default Need to figure out how to resolve as you have to pass SMART as the stopword list and we dont have a direct way to do this Probably just need to update the directions and give an example For anyone who wants to use the SMART stopword list you can do the following txt textProcessordocuments customstopwords tmstopwordsSMART I am currently running stm on k documents from customer chat data without covariates but may be running it on times that row count with categorical and time covariates in the near future This seems like a good opportunity for an ngroups benchmark Did you have anything in mind that you would want me to include for this makeheldout has some pretty rough inefficiencies that arise from large vocabularies which and tabulate can take a long time because they populate a lot of zeroes unnecessarily Need to figure out a work around that only populates the nonzero elements of the table This would make which unnecessary and tabulates equivalent way quicker