When tested on a larger number of images level has a lower average compression ratio than level This really shouldnt happen but I am not quite sure why it does perhaps something wrong with the evaluation code or alpha reduction itself In general it might be a good idea to collect numbers on how each level performs to make it easy to compare performance and compression ratio numbers Given that my free time is limited and split between many things and that oxipng has grown quite popular Im looking for volunteers to help maintain the repositorypreferably individuals who have been involved in oxipng development This would primarily involve triaging issues and reviewing PRs and may also involve publishing releases This improves runtime by on my benchmark Surprisingly it only causes output files to be a fraction of a percent larger I feel like this compression loss definitely should be larger It might just be an issue with the images I use for testing but it seems to me that something is going wrong in the evaluation code I also think that evaluation is not that impactful on many images since several reductions are rarely not worth doing Note that I have not refactored this code to fix warnings since it is a significant change and I want to hear your opinion first The filter strategy is evaluated heuristically by trying all filters at compression level and only using full compression with the best result In this patch it is enabled by default on level While compression is a little bit worse since the heuristic is sometimes wrong performance improves by around a third in my tests Alternatively all filters could be tested heuristically which is only slightly slower and produces better filtering than the current approach for level This represents a notable deviation from what optipng does If you think that oxipngs behaviour should mirror optipng its probably best to only enable this if requested through the command line The same approach can be used to determine the best strategy heuristically possibly with a higher compression level than which helped with choosing the right one in my tests There are a number of different filter heuristics that perform better than the one used in libpng documented here I have implemented two additional strategies on the betterfilters branch There are a number of caveats with these that need to be discussed so I am opening this as an issue instead of a pull request The bigrams strategy is more expensive than minsum the current heuristic but on average delivers better results than minsum Trying both strategies yields the smallest size The distinct bytes strategy rarely delivers the best result in my tests and on average performs worse than minsum so its questionable whether it is worth including Enabling these strategies by default inevitably increases runtime so I am not sure what the best way to integrate them is As a possible solution I have written some code that will add the option to use heuristics to choose the color type zlib strategy and filter strategy at a smaller compression level to decrease the time spent deflating I plan to open a seperate pull request for those changes For more aggressive optimizations there are some additional filter strategies based on the size of the deflated scanline or the shannon entropy as implemented in zopflipng I also added an experimental bias factor to the bigrams strategy which slightly improves compression on average but needs more work The recursive option doesnt work and says failed to open file for reading Version is oxipng This is the output konackDESKTOPmntcusersaccounting picturestest ls Screenshots desktopini spng konackDESKTOPmntcusersaccounting picturestest oxipng png opt recursive Processing png Failed to open file for reading Oxipng only ever uses of my CPU power or about threads My command line is optoxipngtargetreleaseoxipng opt zw k zopfli threads png Ive tried with and without the threads option it doesnt seem to change anything Are there only parameter combinations at those settings is it using a limited thread pool or misdetecting my cpu count default is supposedly x cpu cores so if it somehow couldnt detect and fell back to a fixed value of This is on Gentoo Linux Kernel system with two Xeon E CPUs Id like to start off by saying I highly appreciate what this software offers but there is this one feature I miss In jpegoptim you can specify preserve to not change the modification time of the file being optimized Could a similar feature be implemented into oxipng Would make at least one user happy Im working on implementing The problem is that the main loop which tries different compression modesfilters assumes theres a single PNG file and only IDAT differs However palette sorting makes palette IDAT dependent so the compression trials cant just pick a smaller IDAT they have to also set the right palette to match The alpha filtering trials get away with this because they dont change the image mode but Im still not entirely happy with alpha trials running compression trials internally as part of reduction rather than being integrated with the main loop of trials eg the side effect is that verbose mode doesnt show alpha mode compressions tested and the winning IDAT is recompressed later with the same settings So I think some refactoring is needed Thats also related to I could just compress with default settings as part of the palette reduction same as alpha reductions do it but that feels like copying pasting a lot of code and will perform redundant compressions So Im thinking about introducing an object that will explicitly manage compression trials which could be given to alpha and palette reductions to add their trials to the queue so that the progress is properly displayed and the best IDAT is saved WDYT PngData has both idatdata and rawdata which are related but used in different contexts and handled as independent fields This is problematic because Its possible to modify rawdata without invalidating idatdata Currently the code has to keep track of that manually through a flag passed around That feels fragile Copying of PngData copies both of them even when its just for trialsreductions which are going to use only one of them Alpha filter trials get filteredimage version of the idat that needs to be recompressed better later but theres no way to store the filtered version so later recompression does filtering again thats very minor though Im not entirely sure how this should be handled but there must be a better way Some ideas Have separate CompressedPng and UncompressedPng objects with only either raw or idat and try to use them throughout the code accordingly Have one body as enum CompressedUncompressedBoth Have getterssetters on PngData that clear idatdata when rawdata is set or borrowed mutably 