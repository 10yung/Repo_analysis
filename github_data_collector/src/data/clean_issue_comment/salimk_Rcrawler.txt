Is it in general possible to load a list of urls from which Rcrawler collects urls Sometimes when using LinkExtractor function doesnot collect external URLS from the footer of a webpage while the HTML is visible Bug or wrong use For instance urlsLinkExtractor ExternalLInks TRUE Useragent Chrome or urlsLinkExtractor ExternalLInks TRUE Useragent Chrome ps awesome package makes scraping and parsing so much easier Hi first of all thank you for developing and maintaining this great package I ran into a problem yesterday when crawling a large website The site contains round about k pages Before analyzing the crawled data I did a few crosschecks manually by comparing the content of an url with eg Id in the browser and the stored htmlfile with the name html Doing that i realized that the content doesnt match Somehow the Ids of the htmlfiles are shuffled and when i open the file get the content of some other page of the site I would really appreciate any help with this Best cspersonal hi and good day as i need to extract comments from a page as long texts are hided and need to push read more link at first it done by javascript code so i need to run a scripts in each page before extracting data kindly advice how its possible warm regards I would like to extract emails from several domains Lets say I have only two domains c Now I want to extract only emails from this domains I know I can save htmls and than extract emails but is it possible to extract it in one step If this is email regex emailRegex azAZ azAZ azAZ azAZ azAZ azAZ azAZ How can I extract only that emails but anything else It seems KeyWords argument dont accept regex Hi I tried the following codes which only returns links RcrawlerWebsite nocores noconn ExtractXpathPat h classmediaheading textxstrong ahref ManyPerPattern TRUE MaxDepth I even changed nocores noconn the results are the same If I open and click next I can find more than results Could you please help me solve this problem Thanks Hi Salim thank you for this wonderful library Its really powerful I am trying to understand how if there is a way to skip websites that are too big to be crawled In the following example I am trying to crawl a list of websites and extract the pages with the email address but I obviously dont want the last website The Sydney Morning Herald newspaper to be scraped Is there a way to avoid this or to make the function skip if its taking too long or the number of pages are above a given threshold thank you Ahmed libraryRcrawler samplewebcwwwparentskillscomau wwwhuggiescomau wwwbabies infinitycomau wwwsmhcomau resultslapplysampleweb functionx RcrawlerWebsite xMaxDepth saveOnDiskFALSE KeywordsFiltercmailcontactabout KeywordsAccuracy INDEXUrlINDEXUrl strextracthttp s azAZ afAF afAF myINDEXnaomitINDEXUrl Not always I use the crawler actually more often a content scraper following a predefined link path so I can be sure the complete site is scraped The content scraper includes a syssleep which slows him down Removed it myself to make it faster Also the multithreaded scraping as in rcrawler should be available with the contentscraper as well I think Hi a nice feature would be an integration with ip shuffeling services There are numerous i was thinking about something like It is not hard to implement it yourself but I feel this would be a nice feature for a crawler ie think of using crawlera with scrapy When utilizing the network analysis functionality only the internal HTML pages identified in the Index file are stored as copies This should store a copy of all HTML pages crawled including those in NetwIndex correct RcrawlerWebsite MaxDepth nocores noconn NetworkData TRUE NetwExtLinks TRUE statslinks TRUE