Hello Ive deployed linkerd as well as a couple sample applications with linkerdtcp containers Weve successfully applied cfssl and openssl certificates and keys but are having trouble with keys generated with Vault Our pk encoded key will throw the error WrongNumberOfKeysInPrivateKeyFile and our regular key file will throw FailedToConstructPrivateKey errors There does not appear to be any errors with the certificates on the client side We have similar vaultgenerated certs that work with our main linkerd service mesh client configuration routers client kind iol dstatic configs prefix svcserver connectTimeoutMs tls dnsName serverdefaultsvcclusterlocal trustCerts iobuoyantlinkerdcertstlschain server configuration routers servers ip port dstName inet tls defaultIdentity privateKey iobuoyantlinkerdcertstlskey certs iobuoyantlinkerdcertstlscrt Is there any reason that this may be happening to Vault keys and not other private keys Is there any timeline for Alpha release Or this beta version is ready for production build Ive been trying to build linkerdtcp from source since the docker image doesnt include the latest version see but I had to make the some changes in order to make it work Changing the rustls commit ID as instructed in Changing the rust image from jimmycuadrarust to rust in the dockerize script otherwise I get Rust compilations errors Would you like a PR to fix that This change hides Namerd behind a Namer interface so that the Namer can be dynamically configurable Im looking for feedback on the general approach taken here as well as code style and idioms The main challenge I ran into was making the builder pattern polymorphic An executor contains something on which I can call withhandle which returns something on which I can call resolve which returns something that implements Stream Furthermore I think dynamic dispatch is necessary since the actual choice of implementation will be dynamically determined by a config file I do not plan to merge this as is After getting feedback on this I will remove the namerd implementation and replace it with a static namer Currently k s ingress only supports HTTPS SNI I need k s support for TLS SNI such that I can dynamically create TCP services with virtual server names and have a dynamically created TCP SNI reverse proxy dispatch connections to the correct k s service I see that the linkedtcp beta is available and supports SNI I see that linkedtcp integrates with the k s API via namerd I see some info on configuring namerd for k s Since I m hosting k s on AWS I m assuming that the I would be using a loadbalancer service that creates an ELB instance as the internet entry point for TCP connections This would load balance connections across instances of linkedtcp that have been plumbedinto k s via namerd What I don t see is the full set of k s resources that are required to get this to work Has anyone done this What is the best way to get this configured curl X POST vvvv Trying TCPNODELAY set Connected to port POST shutdown HTTP Host UserAgent curl Accept HTTP OK ContentLength Date Tue Jul GMT Curlhttpdone called premature Connection to host left intact A few minutes later the process is still running testing linkerdtcp with the stock exampleyml and slowcooker at k qps is resulting in large waves of EOFs Get EOF Get EOF Get EOF Get EOF T Z s Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get read tcp read connection reset by peer T Z s Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF T Z s Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF T Z s Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF Get EOF T Z s Interestingly they seem to come in groups of at a time about every seconds It would be nice to have linkerdtcp run fully standalone and load all of the configuration settings routing etc from files It would be usefull in environments where TLS is terminated on different machienes than the loadbalancer and client information like the source IP needs to be preserved See for a description of the protocol I can try to implement it as I have some rust experience but I never worked with tokio directly so I have to see Prototype The initial implementation is basically a prototype It proves the concept but it has severe deficiencies that cause performance and probably correctness problems Specifically it implements its own polling poorly At startup the configuration is parsed For each proxy the namerd and serving configurations are split and connectd by an async channel so that namerd updates are processed outside of the serving thread All of the namerd watchers are collected to be run together with the admin server Once all of the proxy configurations are processed the application is run The admin thread is started initiating all namerd polling and starting the admin server Simultaneously all of the proxies are run in the main thread For each of these a connector is created to determine how all downstream connections are established for the proxy A balancer is created with the connector and a stream of namerd updates An acceptor is created for each listening interface which manifests as a stream of connections connections The balancer is made shareable across servers by creating an async channel and each servers connections are streamed into a sink clone The balancer is driven to process all of these connections The balancer implements a Sink that manages all IO and connection management Each time Balancerstartsend or Balancerpollcomplete is called the following work is done all conneciton streams are checked for IO and data is transfered closed connections are reaped service discovery is checked for updates new connections are established stats are recorded LessonsProblems Inflexible This model doesnt really reflect that of linkerd We have no mechanism to route connections All connections are simply forwarded We cannot for instance route based on client credentials or SNI destination Inefficient Currently each balancer is effectively a scheduler and a pretty poor one at that IO processing should be far more granular and we shouldnt update load balancer endpoints in the IO path unless absolutely necessary Timeouts We need several types of timeouts that are not currently implemented Connection timeout time from incoming connection to outbound established Stream lifetime maximum time a stream may stay open Idle timeout maximum time a connection may stay open without transmitting data Proposal linkerdtcp should become a stream router In the same way that linkerd routes requests linkerdtcp should route connections The following is a rough evolving sketch of how linkerdtcp should be refactored to accomodate this The linkerdtcp configuration should support one or more routers Each router is configured with one or more servers A server which may or may not terminate TLS produces a stream of incoming connections comprising an envelopea source identity an address but maybe more and a destination nameand a bidirectional data stream The server may choose the destination by static configuration or as some function of the connection eg client credentials SNI etc Each connection envelope may be annotated with a standard set of metadata including for example an optional connect deadline stream deadline etc The streams of all incoming connections for a router are merged into a single stream of enveloped connections This stream is forwarded to a binder A binder is responsible for maintaining a cache of balancers by destination name When a balancer does not exist in the cache a new namerd lookup is initiated and its result stream and value is cached so that future connections may resolve quickly The binder obtains a balancer for each destination name that maintains a list of endpoints and their load in terms of connections throughput etc If the inbound connection has not expired ie due to a timeout it is dispatched to the balancer for processing The balancer maintains a reactor handle and initiates IO and balancer state management on the reactor srv srvN Envelope IoStream V binder interpreter V balancer V endpoint V duplex 