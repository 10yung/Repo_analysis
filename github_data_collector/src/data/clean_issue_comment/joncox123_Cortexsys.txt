 What is the content of the function gatherm I have been trying to run the example MNISTDeepClassifier but it produces the following message error Undefined function or variable endfunction Error in gather line endfunction When I checked the function this is empty the only text after the definition of the function is endfunction What is the purpouse of the function gatherm Thanks in advance Hi all I had a problem compiling the mmx files with buildmmxm Script Error Code Cortexsys nncore mmx mmxcpp error expected initializer before teval DWORD stdcall tevalvoid pn Fix In File mmxcpp Line add a double underscore to stdcall L DWORD stdcall tevalvoid pn Hope this helps Hello I have been trying to create a convolutional autoencoder for use with D data I can define the network and I can run data through the untrained network but attempting to train the network produces the following error Output argument gp and maybe others not assigned during call to LinUograd Error in nnCostFunctionCNN line dk nnAnnNlv YvnnlafnnNlogradnnAnnNlv Error in nnrnewRandGennnCostFunctionCNNnnrnewRandGen Error in gradientDescentAdaDelta line J dJdW dJdB fevalf nn r true Any suggestions for a workaround would be appreciated The network defined as follows input size output size layersaf layerssz inputsize layerstyp defsTYPESINPUT layersafend ReLUdefs layersszend inputsize layerstypend defsTYPESFULLYCONNECTED layersafend ReLUdefs layersszend outputsize layerstypend defsTYPESFULLYCONNECTED if defsplotOn nnShow layers defs end Error in Error using Matrix dimensions must agree Error in squaredErrorCostFun line J YvtAvt Error in ReLUcost line J squaredErrorCostFunY A m t Error in nnCostFunctionCNN line J nnlafnnNlcostY nnAnnNl m Js Error in TrainproposalnnrnewRandGennnCostFunctionCNNnnrnewRandGen Error in gradientDescentAdaDelta line J dJdW dJdB fevalf nn r true Error in Trainproposal line nn gradientDescentAdaDeltacostFunc nn defs Training Entire Network Thanks for the great toolbox I would like to build a encoderLSTMdecoder network using your toolbox What would be best practice to train such a network as to my understanding the input format for the encoder and decoder would differ from the LSTM which would make it impossible to train the network in one go correct Is it therefore possible to train the encoder and decoder networks separately and use the encoder output as input to the LSTM network for training And after training stack the layers Hi Jon When I use the normal gradientdescent function and I set alpha to zero as your comment in that line implies that this will lead to a variable learning rate the learning rate will actually stay at the entire time and my error for the MNIST stays at If I set alpha to a value it will use this as the first value and then decay it according to the equation in alphatau So it works but the comment is misleading There seems to be only the option of using a variable learning rate A constant seems not implemented Anyways great code I want to use LSTM for mnsit classification i have created two cell arrays for input data and labelled data both f x X nad y respectively X have each cell of x which is the x image Is it the right approach but the problem is that i have to set T and its taking too long to train the network Any suggestions I want to use LSTM neural network for classification of images the input of Lstm must be cell array 