 Bug A clear and concise description of what the bug is imagenetexample cannot be executed To Reproduce Steps to reproduce the behavior Download imagenet cd plexamplesfullexamplesimagenet python imagenetexamplepy datapath imagenetpath If you have a code sample error messages stack traces please provide it here as well Error Traceback most recent call last File imagenetexamplepy line in module maingetargs File imagenetexamplepy line in main model ImageNetLightningModelhparams TypeError Cant instantiate abstract class ImageNetLightningModel with abstract methods forward Expected behavior A clear and concise description of what you expected to happen The example should run Environment PyTorch version Is debug build No CUDA used to build PyTorch OS Ubuntu LTS GCC version Ubuntu ubuntu CMake version Could not collect Python version Is CUDA available Yes CUDA runtime version Could not collect GPU models and configuration GPU GeForce RTX Ti Nvidia driver version cuDNN version Could not collect Versions of relevant libraries pip numpy pip pytorchlightning pip torch pip torchvision conda blas mkl conda mkl conda mklservice py he b f conda mklfft py ha d b conda mklrandom py hd b f Additional context Add any other context about the problem here I suspect that there was breaking changes in the API and that the example is obsolete as the method forward is now expected Bug PyTorch LR schedulers now shouldnt get any arguments in step function see here and here Looks like the calls in PytorchLightning are not in line with the new interface see here This results in unexpected LR changes Removing the epoch argument from step call solves the issue for me Environment PyTorch PyTorchLightning Feature Experiment with novel Github CI action Motivation having builds for all main os in single CI so we can drop Travis and Appveyo We would keep CircleCI for future GPU testing Bug Fitting with loggpumemoryTrue in the Trainer fails in python version To Reproduce Use python version Create any trainer with loggpumemoryTrue option Then fit it See error apytorchlightningpytorchlightningcorememorypy in getgpumemorymap encodingutf captureoutputTrue checkTrue Convert lines into a dictionary gpumemory intx for x in resultstdoutstripsplitoslinesep usrlibpython subprocesspy in runinput timeout check popenargs kwargs kwargs stdin PIPE with Popenpopenargs kwargs as process try stdout stderr processcommunicateinput timeouttimeout TypeError init got an unexpected keyword argument captureoutput Code sample trainer Trainer loggpumemoryTrue trainerfit Expected behavior For the same code there is no errors for python Environment pytorch Ubuntu pytorchlightning installed to pip environment commit a df f e fca python setuppy develop version python cuda V cudnn GPU RTX TI Additional context In the setuppy pythonrequires But captureoutput is used in subprocessrun calling which is valid only for python See also workaround to maintain python Id like to understand how to use ddp properly with multiple GPUs on a single machine as Im unsure of how to bring results together using this method Im using TensorBoard for logging The problem seems to be that my code below runs on each of the three GPUs with a third of the data each but the variables like overallcorrect only exist for each of the three processes so only a third of the data gets logged For example my overall performance on a single GPU is but with the above process on GPUs it is a third of that I know this is a kindof silly thing but can someone explain how I should bring together the required validationtraining statistics from the subprocesses using pytorch lightning My process is roughly model MyModelhparams ttlogger TestTubeLoggersavedirpathnameexpname trainer Trainerlogger ttlogger gpus distributedbackendddp trainerfitmodel class MyModelLightningModule def initself hparams superMyModel selfinit selfhparams hparams selfresnet ResNetEncoderselfhparams selflossmetertraining averageMeter selfoverallcorrect def trainingstepself batch batchi selflossmetertrainingupdatefloattotalloss return loss totalloss def validationstepself batch batchnb if something selfoverallcorrect return valloss totalloss def validationendself outputs selfloggerexperimentaddscalarepoch lossestrainingtotal selflossmetertrainingavg selfepochnb selfloggerexperimentaddscalarmetricsvalidation performance selfoverallcorrect selfepochnb selflossmetervalidationreset selfoverallcorrect def configureoptimizersself optimizer optimAdamselfparameters lrselfhparamslr return optimizer pldataloader def tngdataloaderself tloader PairLoaderdummy selfhparams splittraining distsampler torchutilsdatadistributedDistributedSamplertloader trainloader dataDataLoadertloaderbatchsizeselfhparamsbatchsize samplerdistsampler numworkers return trainloader pldataloader def valdataloaderself vloader PairLoaderdummy selfhparams splitvalidation distsampler torchutilsdatadistributedDistributedSamplervloader trainloader dataDataLoadervloaderbatchsizeselfhparamsbatchsize samplerdistsampler numworkers return trainloader Bug Currently the numtrainingbatches is set to inf when the dataset is in iterablestyle which may lead to this error Traceback most recent call last File scriptsmsmacropy line in module main File scriptsmsmacropy line in main trainerfitmodel File homezhaohaoanaconda envspytorchlibpython sitepackagespytorchlightningtrainertrainerpy line in fit selfrunpretrainroutinemodel File homezhaohaoanaconda envspytorchlibpython sitepackagespytorchlightningtrainertrainerpy line in runpretrainroutine selfgetdataloadersrefmodel File homezhaohaoanaconda envspytorchlibpython sitepackagespytorchlightningtrainerdataloadingpy line in getdataloaders selfinittraindataloadermodel File homezhaohaoanaconda envspytorchlibpython sitepackagespytorchlightningtrainerdataloadingpy line in inittraindataloader selfvalcheckbatch intselfnumtrainingbatches selfvalcheckinterval OverflowError cannot convert float infinity to integer workaround set valcheckinterval to an integer However if the validation dataset is also in iterable style then the following error will be raised as there is no dataset type check in loading validation dataset Traceback most recent call last File scriptsmsmacropy line in module main File scriptsmsmacropy line in main trainerfitmodel File homezhaohaoDocumentspytorchlightningpytorchlightningtrainertrainerpy line in fit selfrunpretrainroutinemodel File homezhaohaoDocumentspytorchlightningpytorchlightningtrainertrainerpy line in runpretrainroutine selfgetdataloadersrefmodel File homezhaohaoDocumentspytorchlightningpytorchlightningtrainerdataloadingpy line in getdataloaders selfinitvaldataloadermodel File homezhaohaoDocumentspytorchlightningpytorchlightningtrainerdataloadingpy line in initvaldataloader selfnumvalbatches sumlendataloader for dataloader in selfgetvaldataloaders File homezhaohaoDocumentspytorchlightningpytorchlightningtrainerdataloadingpy line in genexpr selfnumvalbatches sumlendataloader for dataloader in selfgetvaldataloaders File homezhaohaoanaconda envspytorchlibpython sitepackagestorchutilsdatadataloaderpy line in len return lenselfindexsampler with iterablestyle dataset this will error File homezhaohaoanaconda envspytorchlibpython sitepackagestorchutilsdatasamplerpy line in len return lenselfsampler selfbatchsize selfbatchsize File homezhaohaoanaconda envspytorchlibpython sitepackagestorchutilsdatadataloaderpy line in len raise TypeErrorCannot determine the DataLoader length of a IterableDataset TypeError Cannot determine the DataLoader length of a IterableDataset I am running on a core gpu machine Ubuntu LTS python lightning no virtual environment no SLURM I have moved a tried and true model to ddp It works great in all scenarios including ddp as a single invocation I cannot succesfully start a second one unfortunately I get the following failure File homesethlocallibpython sitepackagestorchdistributedrendezvouspy line in envrendezvoushandler store TCPStoremasteraddr masterport worldsize startdaemon RuntimeError Address already in use This second job is running on different gpus and has a different log path After a brief investigation it seems to me that the second job is trying to use the same master address as the first I did not see any way to alter this with pytorchlightning though it seems straightforward in pytorch My questions are Can I run multiple simultaneous ddp jobs on the same node with different GPUs If so how Thanks Bug There are some JIT problems with newly released torchvision in we freeze version to but in future we want to support all torchvisions Maybe it is just a temporal bug in torchvision and they will handle it To Reproduce Environment Feature replace the few Pandas usage by native CSV package Motivation I have searched through the docs Google as well as looked through the source code It seems like testend returns nothing it has no return in the function I was wondering if I was missing something really obvious I would simply like to return the metrics of the test end