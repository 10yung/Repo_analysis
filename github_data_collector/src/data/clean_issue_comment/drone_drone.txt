Hi With this PR we can fix the behaviour described in With current drone version With this patch Hi to all drone followers First I would like to be grateful with drone owners and collaborators for this great software As previously discused in we have detected some accuracy problems with builds triggered from cron with expressions every XX continuous period without no important where it exact begins The main discussion issues was Accumulated delay on jobs needed to have a stable cadence with acceptance of some jitter it could be a problem as there is lack of data With current drone version En the previous image cron jobs should be triggered each min but all cronjobs has been accumulating time until bypass one period Expected This is the expected behaviour where there is one execution on each time bucket job alignment Right now drone changes the triggering time depending on the DRONECRONINTERVAL if droneserver for maintenance or for incidence has been stopped more than one XX period on restart all cron jobs will be triggered first at the same time This fact could have impact on not much efficient resource consumption distributi n and could overload runner system Could be interesting if Drone could maintain the cron deploy initial time by the end user this is like delegate resource load distribucion on the end user instead of changing it with this dangerous behaviour There is currently a feature request associated with being able to run some sort of pipeline at the termination of some git event specifically in this case a closure of a pull request This allows the consumers of drone to use drone to spin up ephemeral environments for the purpose of testing and then later have them torn down automatically by Drone once the pull request has been closed This commit implements such a solution after reading the guidance on It attempts to use the actions component of the spec rather than create new first level event primitive but stubs out that actions primitive in the case that the user has not supplied any so as to preserve BC with the existing pipelines Design Notes COMPLETELY UNTESTED This compiles but thats the chunk of testing done on it so far Its late and Im tired so this is a discussion patch General Design The design follows the patn of other drone events as much as possible Because this is the first event that would likely break users unlike the sync event on pull requests for example BC preservation is a little more difficult If this comes up frequently in future it might be reasonable to extend a default pipeline object rather than deserialize the pipeline only from Yaml We should have a global option to provide an allowed list of platforms os arch kinds of pipelines docker kubernetes etc and the node labels If a manifest tries and uses values not in the allowedlist the pipeline can fail This prevents a situation where a user defines criteria in a manifest that we know will never have a matching runner and will therefore sit in a pending state indefinitely We recently upgraded to Drone running in Kubernetes and just had an issue where all builds were failing with the following error Post dial tcp connect cannot assign requested address which is the DRONECONVERTPLUGINENDPOINT and in our case is the droneconvertstarlark extension running as a sidecar in the pod spec Restarting the droneci server pod fixed our issue But having seen that error before usually indicating exhaustion of ephemeral port range I started looking into metrics and found what looks like a resource leak related to how the server calls the converter processopenfdskubernetesnamespacedrone grew to over days gogoroutineskubernetesnamespacedrone grew to indicating a potential issue with goroutines not returningfinishing Investigating the open connections inside the new pod we saw the connections from localhost to the converter service on were often but not always staying in the ESTABLISHED state Established connections to the secret endpoint dronevault in our case also may be leaking but at a much slower pace Here you can see samples each taken minutes apart showing src IPs and the local listening port they are connected to In this data all the sockets are in the ESTABLISHED state and you can also see the gogoroutines gauge metric is steadily increasing for run in seq do uptimenetstat nat devshmnetstat echo Totals curl s metricsgrep gogoroutines Established grep ESTABLISHED devshmnetstat wc l gawk BEGIN OFS t tcp LISTEN ndstsplit dstnsrcsplit srcgroups src nsrc dst ndst states src nsrc dst ndst ENDPROCINFO sortedin valnumdescfor g in groups if groups g statelist for s in states g stateliststatelist s states g s print groups g gstatelist devshmnetstat sleep done up days load average Totals gogoroutines Established ESTABLISHED ESTABLISHED ESTABLISHED ESTABLISHED ESTABLISHED ESTABLISHED ESTABLISHED ESTABLISHED ESTABLISHED up days load average Totals gogoroutines Established ESTABLISHED ESTABLISHED ESTABLISHED ESTABLISHED ESTABLISHED ESTABLISHED ESTABLISHED ESTABLISHED ESTABLISHED up days load average Totals gogoroutines Established ESTABLISHED ESTABLISHED ESTABLISHED ESTABLISHED ESTABLISHED ESTABLISHED ESTABLISHED ESTABLISHED ESTABLISHED up days load average Totals gogoroutines Established ESTABLISHED ESTABLISHED ESTABLISHED ESTABLISHED ESTABLISHED ESTABLISHED ESTABLISHED ESTABLISHED ESTABLISHED Checking the code I do see a second timeout in a few places remote converter method http client Do method so Id expect things to get cleaned up rather quickly not continued growth like were seeing I dont see any pproff endpoints in the server but I hope to be able to get some stack traces via triggering a core dump using pkill SIGABRT pidof droneserver Hopefully that will help track down what all the extra goroutines and sockets are doing Graph Execution is awesome What about add limit for service execution For example i need database and other infrastructure only for testing but testing is not end of pipeline I want to free resource immediately as it no more necessary yaml kind pipeline type docker name default steps name database image mariadb detach until unit e e name install image node commands npm install name unit image node commands npm test name e e image node commands npm e e name build push deploy notify something else I have added the closepullrequest event trigger to make it possible to cleanup a PR environment with the github hooks Hi I will give the context of this PR We have been working on a configuration plugin to handle our monorepo and the solution we came up with does the following steps clones the repo based on the build event in the same way does it looks for diff files affected by the current build for affected each files it traverses the repo upward until it finds a drone config file parses these drone config files and passes them to a template engine to compute the root drone file of the repository and send back the final config Leveraging the multipipeline feature we have been able to come up with a solution which handles each microservice individually erasing the drawbacks of monorepos in classic CI environments A root drone file using this can look like the following yaml kind pipeline type docker name lint steps name lint image golang commands lint if Pipelines Pipelines toYaml kind pipeline type docker name deploy steps name deploy image debian commands deploy dependson PipelinesName toYaml end What we have done works great and we are planning to make it available to the community Yet there is one problem our monorepo is pretty big and even though we created a cache mechanism using a mirror repo as reference it still takes between secs to fetch the repository in the configuration plugin and it leads to timeouts in the Github webhooks as the timeout is set at sec Following the good practices advised by github here weve decided to come up with this PR which makes the handling of a webhook more asynchronous With this PR the triggerer starts a goroutine to do most of the heavy lifting to create the build if an error occurs it creates the build with an error message Thanks for the feedbacks Hi Our code coverage tool Codeclimate requires to access the commit timestamp which is not accessible right now from the Runner environment This is a tiny addition just to expose it as an environment variable to the runners PS I couldnt find a contributing guide so I just made sure to not break the format and that all unit tests are passing locally by running go test but didnt add one as these variables dont have unit tests I just made sure that buildTimestamp existed and was accessible