clearbuffercachepy assumes the slaves file is located under rootsparkec slaves This is not consistent with the Spark setup guide for custom deployments I added the ability to find the slaves file relative to the SPARKHOME environmental variable if set If the slaves file cannot be found in either location an error is raised Support reload the cachedRDD upon the start Support the CLI switch for HiveCatalyst binshark catalyst show tables Execution Mode catalyst OK sharktest sharktest cached Time taken seconds catalyst explain select from sharktest Execution Mode catalyst Logical Plan Project key val MetastoreRelation default sharktest None Optimized Logical Plan MetastoreRelation default sharktest None Physical Plan HiveTableScan key val MetastoreRelation default sharktest None None Time taken seconds catalyst set sharkexecmodehive hive explain select from sharktest Execution Mode hive OK ABSTRACT SYNTAX TREE TOKQUERY TOKFROM TOKTABREF TOKTABNAME sharktest TOKINSERT TOKDESTINATION TOKDIR TOKTMPFILE TOKSELECT TOKSELEXPR TOKALLCOLREF STAGE DEPENDENCIES Stage is a root stage STAGE PLANS Stage Stage Fetch Operator limit Processor Tree TableScan alias sharktest Select Operator expressions expr key type int expr val type string outputColumnNames col col ListSink Time taken seconds This change is necessary for compatibility with Spark commit eefc d b f ebc ecb da f e afbb in Spark added ClassTags on Serializer and all dependent types If partition schema does not match table schema the row formed by deserializing through partition serde is converted to match the table schema If conversion was performed convertedRow will be a standard Object but if conversion wasnt necessary it will still be lazy We cant have both standard and lazy objects across partitions so we serialize and deserialize again to make it lazy This extra serializedeserialize is being performed irrespective of the fact that whether conversion was done or not There are two effects of this serialization deserialization Extra serialization deserilization cost for cases where no conversion happened If a table is created using ThriftDeserializer the nonavailability of serialize function in it makes it unusable in this context The fix done is that in case conversion was not done when partition and table serde match then this serialization and deserialization step should be skipped since it is not required as the object would still be lazy This shall also allow users to be able to use ThriftDeserializer in such a case Tachyon supports kinds of WriteType like CACHETHROUGH MUSTCACHE TRYCACHE and etc And currently Shark only supports CACHETHROUGH Here we make the write type of TachyonOffheapTableWriter configurable so that the end user can choose different types by set sharktachyonwritetypexxx to avoid some WRITETHROUGH overhead somehow Scenario The select privilege is not working when we set metastore pression Scenario The yarn user will be executed when Shark submit JOB and write the scratch doc and it will try eliminating the utilized account However it will report waring exception due to Reloading the close to ensure ordinary operation That lead Queries cant return the correct verification code cache tablename directive overwrites existing memory tables and can cause memory leaks when the cache command is used on an already cached table caching new partition also causes a memory leak if the parition already exists using cache directive on a partitioned table causes table to be corrupt Only the last cached partition is accessable with the rest being leaked 