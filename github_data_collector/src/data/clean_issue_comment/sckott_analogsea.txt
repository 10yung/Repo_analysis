Ive been having some inconsistent behavior when creating destroying a droplet I have a nightly script that simply spins up a machine from a previously saved snapshot runs a script on that machine and then destroys the machine Essentially the simplest possible script I can have Id say that of the time this runs just fine However of the time I get some odd behavior Below is my scriptI added the Syssleep just in case it was a problem where I wasnt giving the droplet enough time to create but not sure if they really are needed with a dropletwait r libraryplumber libraryanalogsea dooauth d dropletcreatenamerecmachine sizeg vcpu gb regionnyc imagexxxxxxx dropletwait Syssleep dropletsshd varscriptsrectrainuploadsh Syssleep dropletdeleted The errors Ill get are the following and they always happen after the machine has been created but before the I make the dropletssh call otherwise Id see information from my R script in the log file that I create r Error Failed to load data from database Another one I will often see is r The resource you were accessing could not be found Lastly very few times the machine is created the script is run which I can verify in my logs and the fact that files are updated in the repo where I deposit files into but the machine never deletes despite the explicit call Any help or thoughts would be appreciated It just seems like something in the script loses track of the droplet details summarystrongSession Infostrongsummary r sessionInfo R version Platform x pclinuxgnu bit Running under Ubuntu LTS Matrix products default BLAS usrliblibblaslibblasso LAPACK usrliblapackliblapackso locale LCCTYPEenUSUTF LCNUMERICC LCTIMEenUSUTF LCCOLLATEenUSUTF LCMONETARYenUSUTF LCMESSAGESenUSUTF LCPAPERenUSUTF LCNAMEC LCADDRESSC LCTELEPHONEC LCMEASUREMENTenUSUTF LCIDENTIFICATIONC attached base packages stats graphics grDevices utils datasets methods base other attached packages plumber analogsea loaded via a namespace and not attached Rcpp later digest crayon awssignature R jsonlite magrittr httr stringi promises xml tools awss httpuv yaml compiler base enc details This is less an issue and more a code review sort of discussion Im working with OReilly Paul Teeter to update the R Cookbook for a nd edition One recipe Id really like to have is execute code in parallel in the cloud It seems to me that future with furrr is a really good fit as it makes the purrr approach parallel Towards that end Im trying to figure out the most drop dead simple recipe for spinning up a cluster for use with future Thanks to some good ideas from Andrew Heiss I have cooked up the following r libraryanalogsea librarytidyverse libraryfurrr clustertag rcluster clusterprefix node numberofnodes mydroplets dockletscreatenames paste clusterprefix ascharacter numberofnodes region sfo size gb tags clustertag pull the ip addresses for the droplets ips dropletstagclustertag mapfunctionx xnetworksv ipaddress unlist Path to private SSH key that matches key uploaded to DigitalOcean it looks like does not work here not sure why sshprivatekeyfile Usersjalsshidrsa Connect and create a cluster cl makeClusterPSOCK vector of IPs in our cluster workers ips DigitalOcean droplets use root for user user root use the key connected to Digital Ocean rshopts c o StrictHostKeyCheckingno o IdentitiesOnlyyes i sshprivatekeyfile run Rscript in the tidyverse docker rscript csudodockerrunnethostrockertidyverseRscript rscriptargs c Install furrr future too e shQuoteinstallpackagesfurrr homogeneousFALSE verboseTRUE My question for you all is simply Can I make this more simple This work It makes a parallel backend I can explain each step But can it be even more simple Thanks On July DigitalOcean added support for Projects Currently the API doesnt support anything with projects but they say it will eventually in When the API eventually supports them itd be cool to have a new argument in dropletcreate to create new images in a specific project which would allow users to better separate the temporary images they might create with R from more important production images in their account using old sshoptions function Theres just a few tests right now only those that dont test against actual interactions with Digitalocean Heres what I think could be the approach for each function Right now we can add tests of failure behavior and any other thigns that dont test against requests to Digitalocean see current tests for examples Start on testing real functionality of functions but with mocked data This may have to take different approaches with different functions For spaces functions that wrap aws those internally use httr For other functions I can convert internal use of httr to crul and then we can use vcr package Im working on for caching requests for tests should be ready to go soon Did we talk yet about making spaces functions follow droplet functions pattern of facilitating piping them together do we not want to do that or do we added support for part of the Spaces API and we decided in to start a fresh issue to help divvy up the tasks of completing the API Below is a checklist with items transcribed from the Spaces API docs Bucket Operations x Create a Bucket x List All Buckets x List a Buckets Contents x Delete a Bucket Get a Buckets Location Get a Buckets ACL Set a Buckets ACL Get a Buckets CORS Set a Buckets CORS Delete a Buckets CORS x Object Operations x Get an Object x Get Information About an Object x Upload an Object PUT x Copy an Object x Get an Objects ACLs x Set an Object ACLs x Delete an Object x Begin a Multipart Upload x Upload a Part x List Parts x Complete a Multipart Upload x Cancel a Multipart Upload Let me know if the organization of this list doesnt make sense or isnt useful for tackling this 