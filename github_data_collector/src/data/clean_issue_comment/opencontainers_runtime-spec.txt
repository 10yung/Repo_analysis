Its been a couple of years and I have no idea what I am doing Some of this could be automated but for now Ive included some of the shell that gets close to the process Signedoffby Vincent Batts vbattshashbangbashcom Fixes VOTE PRs since v e c c Merge pull request from RenaudWasTakenhooks fda Merge pull request from giuseppeaddpersonalityschema bab e Merge pull request from x fixweighttype d f Merge pull request from wkingstateversionsource de f Merge pull request from wkingunknownpropertiesonemust f f Merge pull request from justincormackpersonality b f Merge pull request from mastersofcatsmaster cd d d Merge pull request from cypharconfigumaskoption e ca Merge pull request from pjbgfaddactlog e Merge pull request from giuseppeseccompflags f ab Merge pull request from giuseppefixcrunrepository c a f Merge pull request from estespupdatemeetings a Merge pull request from odinugehugetlbpatternfix a e Merge pull request from odinugehugetlbpattern ffda Merge pull request from KentaTadaFixsampleociVersion b a Merge pull request from KentaTadaFixNamespacestouseLinuxNamespaceType a b f Merge pull request from lifubangpidnamespace ad dcd Merge pull request from jterry layerfolderorder d Merge pull request from vsochupdateCoClink db Merge pull request from jhowardmsftjjhcommandline abf Merge pull request from xiaochenshenrdtmbasoftwarecontroller e d Merge pull request from q vmparametersfix f Merge pull request from HaraldNordgrengoversions c Merge pull request from kinvolkalbansourcemount b a Merge pull request from xiaochenshenrdtmba cc cb Merge pull request from linericyangmaster eba d Merge pull request from dineshgovindasamywindowsnamespace d dbc Merge pull request from cwilhitmaster fd Merge pull request from wking meetingbump c c f Merge pull request from jodhinteladdkatacontainerstoimplementations b Merge pull request from wkingconsolemanpage d c Merge pull request from opencontainersaddgvisor c f Merge pull request from wkingbumpgo a ec Merge pull request from kinvolkalbanhookpaths b a c Merge pull request from ntrrgpatch dc Merge pull request from kinvolkalban deviceowners c d f Merge pull request from wkingjsonfromrfc ad e Merge pull request from wking janmeeting d ed Merge pull request from sameovmsection be e Merge pull request from kinvolkalbanuidmapping fa b a Merge pull request from paravmellanoxmaster a c c Merge pull request from q fixmakefiletest c Merge pull request from wkingjsonschemaminimalids b d e Merge pull request from q defaultActionfix c a Merge pull request from q hooklink a b Merge pull request from wkingrootdedent Signedoffby zhouhao zhouhaocnfujitsucom Carrying over a rebased In working with the runtimespec and implementing a system that uses OCI hooks in the runtime spec a number of performance issues came to bear that required the development of a system model for conditionally filtering which hooks are setpresent in a runtime spec One such system is specified in the libpod hooks spec Additionally there exists a desire by the kubernetes SIGNode team and associated device teams to have a common means for administering enabling conditional runtime spec hooks This desire comes from a need to use hooks for certain types of devices in a common way for CRI integrations Eg it would be good for have a common device hook injection pattern for CRIO containerd The model proposed as defined by the CRIO team in building the above libpod hooks package involves having oci hooks specs configured for being conditionally injected when prespecified conditions occur This proposed and already in production extension to the runtimespec provides a way for users to configure the intended hooks for Open Container Initiative containers so they will only be executed for containers that need their functionality and then only for the stages where theyre needed Stages meaning prestart poststop etc see hooks type When struct Always bool jsonalwaysomitempty Annotations map string string jsonannotationsomitempty Commands string jsoncommandsomitempty HasBindMounts bool jsonhasBindMountsomitempty As shown the conditions may be based on podcontainer annotations that are passed down through the CRI that act as a flag to enabledisable predefined hook specs It is suggested that libpods conditional hooks package be moved into this repo in some form at least for use as common tooling for conditionally enabling oci hooks Without having a common means for enabling ocihooks the effort to users and vendors of devices needing said hooks would be prohibitive The systemd cgroup path convention implemented in runccrun should be added to the spec This convention is becoming important for cgroup v because rootless containers depends on systemd for cgroup delegation in most environments There are cases where it would be necessary to skip the setgroups syscall so that the original additional groups can be maintained It can be used for example by rootless containers to keep access to a storage directory that is accessible only by a secondary group runc already skips the setgroups in some cases either if the user had euid or if procselfsetgroups is set to deny Id like to add a third condition where the setgroups is skipped also if explicitly requested Do we need a new field under processuser eg keepOriginalGroups Would be enough to reuse additionalGids to have some special value eg to keep current groups weight leafWeight and weightDevice are removed in kernel Hi there Do you think it would be possible to tag a new release according to semver Some go module based dependencies have to rewrite their gomod to get the latest features in innocent Thanks in advance This is a feature request within the scope of I decided to open a separate issue so we can have a discussion on one specific controller and I found the memory controller to be a very interesting one for this discussion The point of this discussion is to figure out how to support the new features that are coming in cgroupv including a closer look at the motivations to adopt these features and what to do about the controls that are going away and being removed in cgroupv together with a look at why these are going away and what was wrong with them So lets start looking at specific parts of the standard Memory Limits The memory controller in cgroupv exposes these controls of which the following are available to control memory limits memorylow Besteffort memory protection This is similar to cgroupv s memorysoftlimitinbytes but better Its reasonable to map OCIs memoryreservation to this control since its close in meaning memoryhigh Memory usage throttle limit Once memory usage goes above this limit the container will start getting memory pressure even if the host itself is not under memory pressure In practical terms when processes try to allocate memory the kernel will first go into reclaim and try to free memory from this container before giving them memory But note that going over the high limit never invokes the OOM killer which means under extreme conditions the limit may be breached but will not cause OOMs This limit has no counterpart in cgroupv memorymax Memory usage hard limit Hard limit once this one is reached if reclaim cant shrink memory usage of this cgroup then a container OOM will happen here OCIs memorylimit typically wouold map to this control since its the one with the closest semantics including OOMing if necessary memorymin Hard memory protection This is a fairly new control and it sets a limit under which pages from the container will not be reclaimed even if the host itself is under memory pressure This is useful for guaranteed reservations for some workloads that should not be affected or have reduced impact in an oversubscribed host High Limit The current high limit in cgroupv and current OCI spec is undesirable in that it allows OOMs to happen As a consequence some container managers avoid using it For example Kubernetes in many cases will not not set a high limit instead it will monitor memory usage and decide to evict containers to prevent host OOMs Thats not really great since container managers would like to control memory pressure on containers and tell the kernel to try to shrink them just the side effect of OOMing it is undesirable in most cases the container manager still would prefer to make that kind of decision The new control memoryhigh is a much more useful than cgroupv s memorylimitinbytes which is actually still available as memorymax in cgroupv There would be a great advantage to exposing it as part of the OCI Reservation Low Limit While cgroupv offers a new tunable memorymin for a hard guarantee it seems memorylow is pretty close to the behavior that container managers want to control So it seems that cgroupv through memorylow and cgroupv through memorysoftlimitinbytes are offering the same control here But thats really not the case since cgroupv s memorysoftlimitinbytes has a number of issues that have been fixed in cgroupv The biggest issue with memorysoftlimitinbytes is that its not really hierarchical doesnt borrow from a reservation of the parent cgroup which makes subtree delegation not really viable since a manager for a subtree could request an arbitrarily high soft limit if they wanted to memorylow fixes that See issues with memory cgroupv for more details and more of these issues OOMs The memory controller in cgroupv also exposes this control for controlling container OOMs memoryoomgroup Determines whether the cgroup should be treated as an indivisible workload by the OOM killer If set all tasks belonging to the cgroup are killed together or not at all This can be used to avoid partial kills to guarantee workload integrity This is a great new control thats unavailable in cgroupv When OOMs happen killing a whole container eviction is often what the container manager wants so such a setting from the cgroup subsystem is quite useful Disabling OOMs cgroupv has memoryoomcontrol which allows disabling OOMs for a container and is no longer present in cgroupv This will be addressed in the next section about removed controls Removed Controls These three controls which are encoded in the OCI are no longer available in cgroupv memorykmemlimitinbytes and memorykmemtcplimitinbytes Hard limit for kernel and tcp buffer memory respectively These can be used to set a separate limit for kernel memory usage only memoryswappiness Swappiness parameter of vmscan Similar to the global vmswappiness sysctl but per container memoryoomcontrol Disable OOMs This control has some other uses in notifications but writing it is used to disable OOMs for a specific cgroup I asked htejun about these and here is some rationale on why they were removed and ought to have been removed Below are some comments on each of those Kernel Memory Limits kmem and kmemtcp Accounting and limiting kernel memory separately has a number of issues mostly arising from different memory usages siloed into separate buckets Tejun In cgroup all significant memory usages are accounted and controlled together by default There s no distinction between user kernel or network memories Memories are memories and they re handled the same way I guess another way to look at this is considering the history of memory controller in cgroupv First only user memory accounting was available because it was the easiest to measure and was quite useful The kernel limits required deeper changes and were introduced later For quite a few kernel versions kernel memory accounting was buggy andor problematic so it was even possible to not enable accounting by not setting any limit even an arbitrary one Thats all behind us at this point so we should just look at a single memory limit per container and not worry about whether thats used in userspace or kernel The two OCI configurations memorykernel and memorykernelTCP set those limits I would propose that a reasonable way to handle these on a cgroupv system is to simply ignore them Since the other counters will include kernel memory limits for that memory usage will be implicitly set so using the new controls only makes sense here Swappiness Tejun It s not very clear what swappiness encodes A lot of it is compared to filebacked IOs how un favorable IOs for anonymous memory are considering their inherently higher randomness As such it s more a function of the underlying hardware than workloads Also the implementation wasn t quite right either iirc the behavior would differ depending on who s reclaiming I guess the only setting that makes sense here is disabling swap altogether and that setting can be better achieved by setting memoryswapmax The current OCI specification includes a memoryswappiness setting for this control For backwards compatibility we can look into possibly translating or more specifically reconcyling it with memoryswap on the corner cases that try to disable swap for a specific container using this setting Disabling OOMs Tejun In cgroup userspace could block kernel oom actions This puts the victims in a completely new blocked state and can and often did lead to deadlock conditions as it was extending the dependency chain for the forward progress guarantee out to userspace Tejun In cgroup userspace can t block kernel from making forward progress Instead kernel provides metrics to measure resource pressures PSI allowing userspace agent to detect and remediate memory contention issues way before kernel OOM condition is reached Instead of blocking cgroup which is running out of memory completely the kernel slows them down which is reflected in PSI so that userspace can handle the situation This way userspace has way earlier warnings and kernel isn t blocked on guaranteeing forward progress So clearly the corner cases of this setting are pretty problematic There is a separate way to control OOM behavior in the kernel oomscoreadj which can be set to to disable OOM killer on a specific process That has slightly different behavior than the cgroupv option furthermore its set by process and not by cgroup but it could potentially be leveraged for the same purpose OCI includes a memorydisableOOMKiller setting to disable the OOM killer Perhaps translating that setting into using oomscoreadj instead could be a path towards deprecating this option Final Thoughts The memory controller is a very interesting case study into migration to cgroupv perhaps because of how useful the new controls are that exist only in v In particular memoryhigh would be immediately useful to container managers such as Kubernetes There is the problem of the settings that were removed in the sense that these were exposed in the OCI and we need to figure out how to best deprecate or adapt them Hopefully this issue makes a good case of why these are gone why their being gone is justified as they were not well enough defined not properly hierarchical or mainly there for historic reasons and moving forward with deprecating the OCI fields is a good idea Lets use this issue for a discussion on how to best adopt cgroupv I believe a decision on how to handle the memory controller will quite possibly easily end up applying to other controllers too I talked to vbatts about bringing this up on a OCI call planing to do so on the upcoming meeting on Wednesday March th Thanks Filipe Obsoletes Obsoletes Signedoffby Vincent Batts vbattshashbangbashcom