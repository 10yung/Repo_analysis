Hi Ive installed the prereqs and the scons build completes successfully scons Reading SConscript files Detected compiler gcc scons done reading SConscript files scons Building targets scons buildliblogcabina is up to date scons buildClientServerControl is up to date scons buildExamplesBenchmark is up to date scons buildExamplesFailoverTest is up to date scons buildExamplesHelloWorld is up to date scons buildExamplesReconfigure is up to date scons buildExamplesReconfigureTest is up to date scons buildExamplesSmokeTest is up to date scons buildExamplesTreeOps is up to date scons buildtesttest is up to date scons buildLogCabin is up to date scons buildStorageTool is up to date scons done building targets However when I run the buildtesttest I get the following errors Running tests from test cases tests from CoreConditionVariableTest RUN CoreConditionVariableTestwaituntilstdmutexrealTimingSensitive buildCoreConditionVariableTestcc Failure Expected timedWaitUntillockGuard TimeSteadyClocknow ms ms actual ms vs ms buildCoreConditionVariableTestcc Failure Expected timedWaitUntillockGuard TimeSystemClocknow ms ms actual ms vs ms RUN CoreConditionVariableTestwaituntilCoreMutexrealTimingSensitive buildCoreConditionVariableTestcc Failure Expected timedWaitUntillockGuard TimeSteadyClocknow ms ms actual ms vs ms buildCoreConditionVariableTestcc Failure Expected timedWaitUntillockGuard TimeSystemClocknow ms ms actual ms vs ms tests from RPCMessageSocketTest RUN RPCMessageSocketTestreadableEmptyPayload buildRPCMessageSocketTestcc Failure Value of handlerdisconnected Actual true Expected false If it helps Im running in WSL Windows Linux Microsoft Microsoft Fri Nov PST x x x GNULinux libprotoc latest from master ee c ae b b becd d c c b bbfb Im not sure how to interpret the error Thanks for your time There had been multiple fixes on the master branch but no formal release for a few years When are we going to cut a formal release scons Reading SConscript files Detected compiler gcc scons done reading SConscript files scons Building targets protoc I cppout pythonout buildProtocolServerControlproto sh protoc not found protoc I cppout pythonout buildProtocolClientproto sh protoc not found Compiling buildClientBackoffcc Compiling buildClientUtilcc protoc I cppout pythonout buildProtocolServerStatsproto sh protoc not found protoc I cppout pythonout buildTreeSnapshotproto sh protoc not found protoc I cppout pythonout buildProtocolRaftproto sh protoc not found protoc I cppout pythonout buildProtocolRaftLogMetadataproto sh protoc not found Compiling buildRPCAddresscc Compiling buildRPCClientRPCcc Compiling buildRPCClientSessioncc Compiling buildRPCMessageSocketcc Compiling buildRPCOpaqueClientRPCcc Compiling buildRPCOpaqueServercc Compiling buildRPCOpaqueServerRPCcc scons buildProtocolServerControlpbcc Error Compiling buildRPCProtocolcc scons buildProtocolClientpbcc Error scons buildProtocolServerStatspbcc Error scons buildTreeSnapshotpbcc Error scons buildProtocolRaftpbcc Error scons buildProtocolRaftLogMetadatapbcc Error scons building terminated because of errors when i build with scons command as a result it throw some error didnt pass Please help me thanks so much I think i is unnecessary Proposed fix for logcabin has a philosophy that since you have to be able to recovery quickly from a failure you should fail early and often in accordance with this philosophy if the logcabin process is out of file descriptors and an open fails it will abort the shared code that the client uses has the same behavior i dont know if the logcabin failure philosophy should also apply to clients the particular crash is here x f ce f in raise from lib libcso x f ce ce in abort from lib libcso x aac bb in LogCabinCoreRandomanonymous namespaceRandomStatereset this x fea LogCabinCoreRandomanonymous namespacerandomState at buildCoreRandomcc x aac a in LogCabinCoreRandomanonymous namespaceresetRandomState at buildCoreRandomcc x f ce a e in fork from lib libcso void reset stdlockguardstdmutex lockGuardmutex int fd opendevurandom ORDONLY if fd too early to call PANIC in here fprintfstderr Couldnt open devurandom s n strerrorerrno abort in this case a client side fork is also required possible fixes up the fd limit for the client fix the log cabin client code to fall back to a random number generator that does not require a fd ensure the client isnt doing anything that is needlessly eating up file descriptors im ok to close this as wont fix if the behavior is in line with that is expected or to implement a fix for this particular abortonfork when out of FDs issues by falling back to a different RNG This is like a prestaging PR I am not so sure about your code style so please do some review and let me know what needs to be changed My dev env cmake version v gcc version UbuntuLinaro ubuntu os Ubuntu compile command cmake CMakeListstxt make Supported build of daemon build of static lib build of Examples build of test exec file build of tags build of docs build of check and lint build of storage tool auto checkout the submodule of gtest no source code is changed Something changed the test exec file is changed buildtesttest buildTestRunner the daemon exec file is changed buildLogCabin buildLogCabinServer all example exec files are moved to build cmake generates the o files into CMakeFilesexecutablenamedirsrcdiro so it will do some extra compiling for each executable files TODO currently I am using stdc x to compile I will add support to stdc in the future lib detecting rpm packing testing action Separate the flags for protoc generated files gtest files and logcabin project files You will see tons of warning when compiling those protoc generated files now update travisyml Let me know if I still have something missed im seeing a hang in the logcabin client while the cluster is being initialized thread gdb bt x fd bf d in llllockwait from lib libpthreadso x fd d in Llock from lib libpthreadso x fd c in pthreadmutexlock from lib libpthreadso x a c c in gthreadmutexlock mutex x fd at usrincludec x redhatlinuxbitsgthrdefaulth x a ed e in stdmutexlock this x fd at usrincludec mutex x a f in stdlockguardstdmutexlockguard this x fd ffeb m at usrincludec mutex x aa c c in LogCabinEventFileMonitordisableForever this x fd at buildEventFilecc x a ed in LogCabinRPCMessageSocketdisconnect this x fd at buildRPCMessageSocketcc x a ee e in LogCabinRPCMessageSocketreadable this x fd at buildRPCMessageSocketcc x a e f in LogCabinRPCMessageSocketReceiveSockethandleFileEvent this x fd events at buildRPCMessageSocketcc x aa in LogCabinEventLooprunForever this x c at buildEventLoopcc x a ed in stdMemfnvoid LogCabinEventLoopoperator voidLogCabinEventLoop const this x c object x c at usrincludec functional x a in stdBindsimplestdMemfnvoid LogCabinEventLoop LogCabinEventLoopMinvoke ulstdIndextuple ul this x c at usrincludec functional x a e in stdBindsimplestdMemfnvoid LogCabinEventLoop LogCabinEventLoopoperator this x c at usrincludec functional x a in stdthreadImplstdBindsimplestdMemfnvoid LogCabinEventLoop LogCabinEventLoop Mrun this x c at usrincludec thread x fd ce in from lib libstdcso x fd dc in startthread from lib libpthreadso x fd ced in clone from lib libcso gdb fr x a f in stdlockguardstdmutexlockguard this x fd ffeb m at usrincludec mutex usrincludec mutex No such file or directory gdb print m stdlockguardstdmutexmutextype x fd stdmutexbase Mmutex data lock count owner nusers kind spins list prev x next x size repeats times align No data fields mutex held by thread gdb thread Switching to thread Thread x fd b fe LWP x fd d in pthreadcondwaitGLIBC from lib libpthreadso gdb bt x fd d in pthreadcondwaitGLIBC from lib libpthreadso x aa a in LogCabinCoreConditionVariablewait this x c lockGuard at buildCoreConditionVariablecc x aa in LogCabinEventLoopLockLock this x fd b fcc eventLoop at buildEventLoopcc x aa c a in LogCabinEventFileMonitordisableForever this x fd at buildEventFilecc x aa bca in LogCabinEventFileMonitorMonitor this x fd inchrgoptimized out at buildEventFilecc x a eaf in LogCabinRPCMessageSocketMessageSocket this x fd inchrgoptimized out at buildRPCMessageSocketcc x a c in stddefaultdeleteLogCabinRPCMessageSocketoperator this x fd b ptr x fd at usrincludec bitsuniqueptrh x a bdfa in stduniqueptrLogCabinRPCMessageSocket stddefaultdeleteLogCabinRPCMessageSocket reset this x fd b p x fd at usrincludec bitsuniqueptrh x a aa e in LogCabinRPCClientSessionClientSession this x fd inchrgoptimized out at buildRPCClientSessioncc x a e in stdSpcountedptrLogCabinRPCClientSession gnucxxLockpolicy Mdispose this x fd at usrincludec bitssharedptrbaseh x df e in stdSpcountedbasegnucxxLockpolicy Mrelease this x fd at usrincludec bitssharedptrbaseh x a f b in stdsharedcountgnucxxLockpolicy sharedcount this x fd b fcf inchrgoptimized out at usrincludec bitssharedptrbaseh x a d e in stdsharedptrLogCabinRPCClientSession gnucxxLockpolicy sharedptr this x fd b fcf inchrgoptimized out at usrincludec bitssharedptrbaseh x a d in stdsharedptrLogCabinRPCClientSessionsharedptr this x fd b fcf inchrgoptimized out at usrincludec bitssharedptrh x a e in LogCabinClientSessionManagercreateSession this x c b address timeout clusterUUID x c serverId x at buildClientSessionManagercc x a f in LogCabinClientLeaderRPCgetSession this x c e timeout at buildClientLeaderRPCcc x a d f in LogCabinClientLeaderRPCCallstart this x fd b fd d opCodeLogCabinProtocolClientGETCONFIGURATION request timeout at buildClientLeaderRPCcc x a in LogCabinClientLeaderRPCcall this x c e opCodeLogCabinProtocolClientGETCONFIGURATION request response timeout at buildClientLeaderRPCcc x a e in LogCabinClientClientImplgetConfiguration this x c c timeout at buildClientClientImplcc x a e in LogCabinClientClustergetConfiguration this x c d timeoutNanoseconds at buildClientClientcc x a e e in LogCabinClientClustergetConfiguration Ex this x c d timeoutNanoseconds at buildClientClientcc x d d in LogCabinClusterGetConfiguration thisthisentry x bf f timeoutNanosecondstimeoutNanosecondsentry at LogCabinClustercpp x b in LogCabinCollectorcollectLocked this x at LogCabinCollectorcpp x fb in ForteCollectorCollect thisthisentry x at Collectorcpp x f in CollectorReqHandlerHandler thisthisentry x ef eventevententry x fd c a at CollectorManagerImplcpp x e e in ForteThreadPoolDispatcherWorkerrun this x fd at ThreadPoolDispatchercpp x e a in ForteThreadstartThread obj x fd at Threadcpp x fd dc in startthread from lib libpthreadso x fd ced in clone from lib libcso gdb fr x aa a in LogCabinCoreConditionVariablewait this x c lockGuard at buildCoreConditionVariablecc in buildCoreConditionVariablecc gdb print mutex pthreadmutext x c gdb print mutex data lock count owner nusers kind spins list prev x next x size repeats times repeats times align havent dug in fully but i believe the logcabin instance it is attempting to connect to is not yet running so GetConfiguration is timing out havent had a chance to fully dig in but from the context it seems that maybe when we dont get a session we dont get a mutex and the following code is blocked trying to wait on a condition variable with an empty mutex else pthreadmutext mutex lockGuardmutexnativehandle int r pthreadcondwait cv mutex if r PANICpthreadcondwait failed s strerrorr a potential fix would be to check the mutex before waiting but im not sure if this indicates a deeper problem potentially related to This commit adds a new option snapshotIndexWatermark to logcabin server The option expresses a ratio of allowed difference between the latest index of the log and the index of latest snapshot Current implementation uses the fixed value This commit also fixes a bug in the condition IIUC the original condition of the last branch in StateMachineshouldTakeSnapshot would be always true the upstream provided etcinitdfunctions file has a function status used by the service script status checks that the there is pid file in varrunlogcabinpid and that the pid exists a race exists where logcabin can be killed or otherwise abort and a newly started process can take the pid as long as that pid file and pid exists logcabin will show as running 