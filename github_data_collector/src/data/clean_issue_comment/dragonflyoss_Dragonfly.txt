 Weekly Report of Dragonfly This is a weekly report of Dragonfly It summarizes what have changed in the project during the passed week including pr merged new contributors and more things in the future It is all done by AliGHRobot which is an AI robot See Repo Update Watch Star Fork Contributors New Issues Closed Issues PR Update Thanks to contributions from community Dragonfly team merged pull requests in the repository last week All these pull requests could be divided into feature bugfix doc test and others feature feat generate task per http range bugfix bugfix retry multi times if failed to report pieces bugfix update cdn fail when the content is being read and the source server is down doc docs auto generate Dragonfly cliapicontributors docs via code docs auto generate Dragonfly cliapicontributors docs via code docs auto generate Dragonfly cliapicontributors docs via code docs add more detail steps in workspace preparation docs update docs about deploy with Physical Machines docs add a doc about using dragonfly with harbor registry docs optimize the config documents test test add unit test case for func GetAsBitset test add unit test case for func GetAsMap test add util test for atomiccount test add unit test case for func GetAsBool test add test case in unit test TestParseFilter test add unit test case for func GetAsString test add unit test case for func GetAsInt others code clean remove tmp file dfget tmp add unit test for GetMsgByCode Securing many http links to https links Configure dfclientdfdaemondfget log file path via logConfigpath property Code Review Statistics This project encourages everyone to participant in code review in order to improve software quality Every week pouchrobot would automatically help to count pull request reviews of single github user as the following So try to help review code in this project Contributor ID Pull Request Reviews lowzj Starnop xujihui YanzheL New Contributors We have no new contributors in this project this week Dragonfly team encourages everything about contribution from community For more details please refer to Thank all of you Signedoffby Hu Shuai husfnstcnfujitsucom Please make sure you have read and understood the contributing guidelines Describe what this PR did Does this pull request fix one issue If that add fixes xxxx below in the next line for example fixes Otherwise add NONE Why dont you add test cases unit testintegration test Describe how to verify it Special notes for reviews Signedoffby chuxianmjjantfincom Please make sure you have read and understood the contributing guidelines Describe what this PR did Add ClientStreamWriter to implement Streaming Does this pull request fix one issue If that add fixes xxxx below in the next line for example fixes Otherwise add NONE This PR is precommit for Proposal Why dont you add test cases unit testintegration test Describe how to verify it Special notes for reviews Signedoffby YanzheL leeyanzheyanzheorg Please make sure you have read and understood the contributing guidelines Why do you propose this PR The current implementation of HTTPS hijacking is simple but not correct It uses user provided TLS server cert dfcrt and dfkey to decrypt HTTPS connection However this cert cannot be a CA which means every TLS connection is encrypted by the same cert from user point of view This is not a standard behavior of a HTTPS ManInTheMiddle proxy and will cause various issues The cert used by HTTPS hijacking cannot be automatically verified by applications because the Common Name or ServerAlternativeNames is always same and it doesnt match the host of every connection So user have to configure their applications manually to forceignore the TLS verification This prevents dfclient be used as a general systemlevel HTTPS proxy without affecting user applications Some applications cannot be configured to trust a specific TLS cert or ignore TLS verification error eg Google Chrome Instead the only way to achieve it is to configure them to trust the CA or add the CA to system trust store II Describe what this PR did If dfcrt and dfkey is a CA keypair then dfclient use it to issue leaf TLS certs for every connection whose host matches preconfigured hijacking rules User can either add the CA to system trust store or configure individual application to trust it Since the common name of leaf cert is set as the target host the connection will be verified by user application automatically as normal If dfcrt and dfkey is not a CA keypair the behavior of dfclient is same as the old way this cert is used in hijacking instead of generating new certs per connection So this PR is fully backwardcompatible and will NOT break user applications I Does this pull request fix one issue If that add fixes xxxx below in the next line for example fixes Otherwise add NONE Potential issues are stated above IV Potential use cases Maybe now we can cache HTTPS docker registries Probably fixes The dfdaemon acts as a decrypting MITM HTTPS proxy so it can see the request body of docker image pull requests to remote private HTTPS registries If we can see the body then we can cache it as well Generic HTTPS caching proxy just like squid Cache anything in a distributed way and not just for HTTP contents We can also use dfdaemon to speed up normal webpage loading in browser V Why dont you add test cases unit testintegration test Im working on unit tests but currently I dont have much time For now I just tested this feature in container and everything seems good VI Describe how to verify it Prepare a selfsigned CA with private key Configure dfdaemon to use this key pair and also configure the hijack rules proxy rules yaml proxies regx blobssha Caching docker images regx png Caching png files hijackhttps cert cacrt key cakey hosts regx Decrypt all sites for test Setup dfdaemon as your systems https proxy shell export HTTPPROXY export HTTPSPROXY Check the TLS cert return by target site shell curl vkL o devnull This command will report that the TLS cert of alibabacomis signed by your CA The dfdaemon log will indicate that it is downloading png files of alibabacom II Special notes for reviews This PR reuses CAs private key and signature algorithm to generate perconnection certs I think it can save keygeneration overhead and there is no need to use new TLS private key for every connection since the generated cert is temporal Default valid time is hours As for security if the private key of CA is leaked someway generating new private key for every connection will not improve security maybe So reusing CA private key does not bring much security issues If this is not appropriate I can change it Question You can ask any question about this project showbar command is supported in python why not supprted in go Weekly Report of Dragonfly This is a weekly report of Dragonfly It summarizes what have changed in the project during the passed week including pr merged new contributors and more things in the future It is all done by AliGHRobot which is an AI robot See Repo Update Watch Star Fork Contributors New Issues Closed Issues PR Update Thanks to contributions from community Dragonfly team merged pull requests in the repository last week All these pull requests could be divided into feature bugfix doc test and others bugfix fix use StringArrayVar to parse http header doc docs auto generate Dragonfly cliapicontributors docs via code test test add unit test case for func GetAsInt test add unit test case for func Remove test add unit test case for func ParsePieceIndex others fix typo fix comment for func Print fix typo fix comment for func ParseNodesString Refactor make version subcommand as a public subcommand Code Review Statistics This project encourages everyone to participant in code review in order to improve software quality Every week pouchrobot would automatically help to count pull request reviews of single github user as the following So try to help review code in this project Contributor ID Pull Request Reviews lowzj xujihui truongnh Starnop New Contributors We have no new contributors in this project this week Dragonfly team encourages everything about contribution from community For more details please refer to Thank all of you Question FAQ dfdaemon registry dfdaemon dfdaemon registry node xxxxx xxxxx docker pull xxx dfdaemon dfdaemonlog Backgrounds Now the client of Dragonfly will random read and write disk multiple times during the downloading process For directly using dfget to download a file dfget random writes a piece into disk after downloading it dfget server random reads the piece from disk to share it dfget sequential reads the file from disk after downloading to do checksum And for using dfdaemon to pull images therere extra disk IO by Dragonfly dfdaemon sequential reads the file from disk to send it to dockerd Its not a problem when the host has a local disk But it will be a potential bottleneck when the Dragonfly client runs on a virtual machine with a cloud disk all the disk IO will become network IO which has a bad performance when readwrite at the same time So a solution is needed to reduce the IO times generated by Dragonfly Idea P P Streaming is a P P based on Streaming which sends the data downloaded by using p p pattern to the user directly in order to achieve the purpose of reading and writing to disk as few as possible P P Streaming Data Flow This diagram describes the p p streaming data flow Piece Data Cache stores the pieces data in memory that can be shared to the other peers A pieces data should be putted into this cache after downloading and be evicted according to the LRU strategy when the cache is full StreamIO sends pieces data to callers in ascending order of pieces number In the scenario of using dfdaemon to pulling images and others the dfdaemon and dfget should be merged into one process that can reduce the time of starting dfget process Also dfget can be as an individual process to download files directly P P Streaming Sliding Window The P P Streaming Sliding Window is designed to control the number of pieces of a file that can be scheduled and downloaded to avoid unlimited memory usage This idea comes from tcp sliding window but its minimal transmission unit is a piece not a byte Memory Cache is the Piece Data Cache to share pieces in the p p network The larger the cache the higher the p p transmission efficiency Issue Description SuperNode UTC date Mon Jan CST supernode WARN sign Describe what happened Describe what you expected to happen How to reproduce it as minimally and precisely as possible Anything else we need to know Environment dragonfly version OS eg from etcosrelease Kernel eg uname a Install tools Others Question docker pull docker pull s supernode log INFO sign gc peer start to deal with peer tst xxxcomxxxxx WARN sign failed to get dfget task by dstCIDxxxxx and taskIDd d ce d a c d b a c e f c b f and the srcCID is xxxxx err failed to get key xxxxx d d ce d a c d b a c e f c b f from map Code Msgdata not found dfclient log INFO sign pull piece tasktaskIDd d ce d a c d b a c e f c b fsuperNodexxxxx dstCidxxxxx range result status pieceSize pieceNum resultcode msgtaskIDd d ce d a c d b a c e f c b f clientID Code Msg peer should wait and sleep s xxxxx supernode xxxxx xxxxx piece supernode gc xxxxx 