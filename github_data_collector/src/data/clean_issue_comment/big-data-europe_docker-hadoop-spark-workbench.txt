In makefile traefik version is not specified and the syntax is incompatible with the current version Problem see stacktrace below My solution for now Changed the dockercomposeyml file and added priviledged true setting to the nodes configured there namenode orgapachehadoophdfsservercommonInconsistentFSStateException Directory hadoopdfsname is in an inconsistent state storage directory does not exist or is not accessible namenode at orgapachehadoophdfsservernamenodeFSImagerecoverStorageDirsFSImagejava namenode at orgapachehadoophdfsservernamenodeFSImagerecoverTransitionReadFSImagejava namenode at orgapachehadoophdfsservernamenodeFSNamesystemloadFSImageFSNamesystemjava namenode at orgapachehadoophdfsservernamenodeFSNamesystemloadFromDiskFSNamesystemjava namenode at orgapachehadoophdfsservernamenodeNameNodeloadNamesystemNameNodejava namenode at orgapachehadoophdfsservernamenodeNameNodeinitializeNameNodejava namenode at orgapachehadoophdfsservernamenodeNameNodeinitNameNodejava namenode at orgapachehadoophdfsservernamenodeNameNodeinitNameNodejava namenode at orgapachehadoophdfsservernamenodeNameNodecreateNameNodeNameNodejava namenode at orgapachehadoophdfsservernamenodeNameNodemainNameNodejava I copied files from local filesystem to namenode container and then copy it to hdfs on userrootdata path Now i have a problem to load data from hdfs into local spark application sparkreadformatjsonloadhdfs userrootdatafilenamejson Problem is url to data i tried hdfs hdfslocalhost hdfsnamenode hdfsnamenode and none of this is valid Is someone having similar problem when i cloned this project i run starthadoopsparkworkbenchwithHivesh then there are some CONTAINERs running but when i docker exec it sparkmaster binbash and cd sparkbin then run sparkshell some thing happened this is my cmd scala val textFile sctextFilesparkbinwordstxt textFile orgapachesparkrddRDD String sparkbinwordstxt MapPartitionsRDD at textFile at console scala text text textFile scala textFilecount javalangIllegalArgumentException javanetUnknownHostException namenode at orgapachehadoopsecuritySecurityUtilbuildTokenServiceSecurityUtiljava at orgapachehadoophdfsNameNodeProxiescreateNonHAProxyNameNodeProxiesjava at orgapachehadoophdfsNameNodeProxiescreateProxyNameNodeProxiesjava at orgapachehadoophdfsDFSClientinitDFSClientjava at orgapachehadoophdfsDFSClientinitDFSClientjava at orgapachehadoophdfsDistributedFileSysteminitializeDistributedFileSystemjava at orgapachehadoopfsFileSystemcreateFileSystemFileSystemjava at orgapachehadoopfsFileSystemaccess FileSystemjava at orgapachehadoopfsFileSystemCachegetInternalFileSystemjava at orgapachehadoopfsFileSystemCachegetFileSystemjava at orgapachehadoopfsFileSystemgetFileSystemjava at orgapachehadoopfsFileSystemgetFileSystemjava at orgapachehadoopmapredJobConfgetWorkingDirectoryJobConfjava at orgapachehadoopmapredFileInputFormatsetInputPathsFileInputFormatjava at orgapachehadoopmapredFileInputFormatsetInputPathsFileInputFormatjava at orgapachesparkSparkContextanonfunhadoopFile anonfun applySparkContextscala at orgapachesparkSparkContextanonfunhadoopFile anonfun applySparkContextscala at orgapachesparkrddHadoopRDDanonfungetJobConf applyHadoopRDDscala at orgapachesparkrddHadoopRDDanonfungetJobConf applyHadoopRDDscala at scalaOptionforeachOptionscala at orgapachesparkrddHadoopRDDgetJobConfHadoopRDDscala at orgapachesparkrddHadoopRDDgetPartitionsHadoopRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at scalaOptiongetOrElseOptionscala at orgapachesparkrddRDDpartitionsRDDscala at orgapachesparkrddMapPartitionsRDDgetPartitionsMapPartitionsRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at scalaOptiongetOrElseOptionscala at orgapachesparkrddRDDpartitionsRDDscala at orgapachesparkSparkContextrunJobSparkContextscala at orgapachesparkrddRDDcountRDDscala elided Caused by javanetUnknownHostException namenode more i want to use sparksql or hive to exec some sql and connect to hive use python or use pyspark can i achieve my aim Added kafka image zookeeper to compose with web console UI over REST interface i have namenode x in machine spark master x in machine networkendpoint in machine spark worker x in machine datanode x in machine my code run in spark masteruse pyspark text sctextFilehdfsnamenode pathfile textcollect I create swarm with sparkgettyimage with your hadoop and I cannot read data in hadoop I read log in worker and it say Failed to connect to for block BP blk add to deadNodes and continue is network endpoint why namenode connect with endpoint ip but I can access data in namenode and why it endpoint ip dockercomposehadoopyml version services namenode image bde hadoopnamenode hadoop java ports volumes namenodehadoopdfsname hadoopdatahadoopdata environment CLUSTERNAMEtest envfile hadoopenv deploy mode replicated replicas placement constraints noderole manager restartpolicy condition onfailure datanode image bde hadoopdatanode hadoop java volumes datanodehadoopdfsdata envfile hadoopenv environment SERVICEPRECONDITION namenode deploy mode global placement constraints noderole worker restartpolicy condition onfailure networks default external name hadoopsparkswarmnetwork dockercomposesparkyml version services master image gettyimagesspark command binsparkclass orgapachesparkdeploymasterMaster hostname master environment MASTER sparkmaster SPARKCONFDIR conf SPARKPUBLICDNS localhost SPARKMASTERHOST envfile hadoopenv ports volumes datatmpdata deploy replicas restartpolicy condition onfailure placement constraints noderole manager worker image gettyimagesspark command binsparkclass orgapachesparkdeployworkerWorker sparkmaster hostname worker environment SPARKCONFDIR conf SPARKWORKERCORES SPARKWORKERMEMORY g SPARKWORKERPORT SPARKWORKERWEBUIPORT SPARKPUBLICDNS localhost envfile hadoopenv dependson master links master ports volumes datatmpdata deploy replicas restartpolicy condition onfailure placement constraints noderole worker networks default external name hadoopsparkswarmnetwork I create my own network with docker network create d overlay hadoopsparkswarmnetwork print user I got the error below console error not found value globalScope globalScopesparkContextsetJobGroupcellA F CF B DD B E E run print user incompatible clusterID Hadoop Hi anytime I rebooted the swarm I have this problem javaioIOException Incompatible clusterIDs in hadoopdfsdata namenode clusterID CIDb a c a cbd c f datanode clusterID CIDf ca d b fb ca d c at orgapachehadoophdfsserverdatanodeDataStoragedoTransitionDataStoragejava FATAL datanodeDataNode Initialization failed for Block pool registering Datanode Uuid unassigned service to namenode Exiting javaioIOException All specified directories are failed to load at orgapachehadoophdfsserverdatanodeDataStoragerecoverTransitionReadDataStoragejava I solved this problem deleting this docker volume sudo docker volume inspect hadoopdatanode CreatedAt T Z Driver local Labels comdockerstacknamespace hadoop Mountpoint data dockervarvolumeshadoopdatanodedata Name hadoopdatanode Options Scope local but in this volume are present the files which I put in hdfs so in this way I have to to put again the files into hdfs when I deploy the swarm Im not sure this is the right way to solve this problem Googling I found one solution but I dont know how to applicate it before the swarm reboot this is the solution The problem is with the property name dfsdatanodedatadir it is misspelt as dfsdataodedatadir This invalidates the property from being recognised and as a result the default location of hadooptmpdirhadoopUSERdfsdata is used as data directory hadooptmpdir is tmp by default on every reboot the contents of this directory will be deleted and forces datanode to recreate the folder on startup And thus Incompatible clusterIDs Edit this property name in hdfssitexml before formatting the namenode and starting the services thanks I tried docker exec it namenode hadoop fs put READMEmd tmpREADMEmd and got put READMEmd No such file or directory