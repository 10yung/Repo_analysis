 Provide a general summary of your changes in the Title above Description Describe your changes in detail Improved LRU by switching order and storing disk reads increased default cache sizes Add developers section to the documentation Description Add IDEA set up guide and tests run instructions We are getting javaioUTFDataFormatException encoded string too long while running modeltransform when the job is submitted as deploymode cluster orgapachesparkdeployyarnApplicationMasteranon runApplicationMasterscala Caused by javaioUTFDataFormatException encoded string too long bytes at javaioDataOutputStreamwriteUTFDataOutputStreamjava at javaioDataOutputStreamwriteUTFDataOutputStreamjava at Spark version Spark nlp Scla When we run the spark job as deploymode client it works fine Detailed stack trace User class threw exception orgapachesparkSparkException Task not serializable at orgapachesparkutilClosureCleanerensureSerializableClosureCleanerscala at orgapachesparkutilClosureCleanerorgapachesparkutilClosureCleanercleanClosureCleanerscala at orgapachesparkutilClosureCleanercleanClosureCleanerscala at orgapachesparkSparkContextcleanSparkContextscala at orgapachesparkrddRDDanonfunmapPartitionsWithIndex applyRDDscala at orgapachesparkrddRDDanonfunmapPartitionsWithIndex applyRDDscala at orgapachesparkrddRDDOperationScopewithScopeRDDOperationScopescala at orgapachesparkrddRDDOperationScopewithScopeRDDOperationScopescala at orgapachesparkrddRDDwithScopeRDDscala at orgapachesparkrddRDDmapPartitionsWithIndexRDDscala at orgapachesparksqlexecutionWholeStageCodegenExecdoExecuteWholeStageCodegenExecscala at orgapachesparksqlexecutionSparkPlananonfunexecute applySparkPlanscala at orgapachesparksqlexecutionSparkPlananonfunexecute applySparkPlanscala at orgapachesparksqlexecutionSparkPlananonfunexecuteQuery applySparkPlanscala at orgapachesparkrddRDDOperationScopewithScopeRDDOperationScopescala at orgapachesparksqlexecutionSparkPlanexecuteQuerySparkPlanscala at orgapachesparksqlexecutionSparkPlanexecuteSparkPlanscala at orgapachesparksqlexecutionSparkPlangetByteArrayRddSparkPlanscala at orgapachesparksqlexecutionSparkPlanexecuteTakeSparkPlanscala at orgapachesparksqlexecutionCollectLimitExecexecuteCollectlimitscala at orgapachesparksqlDatasetorgapachesparksqlDatasetcollectFromPlanDatasetscala at orgapachesparksqlDatasetanonfunhead applyDatasetscala at orgapachesparksqlDatasetanonfunhead applyDatasetscala at orgapachesparksqlDatasetanonfun applyDatasetscala at orgapachesparksqlexecutionSQLExecutionwithNewExecutionIdSQLExecutionscala at orgapachesparksqlDatasetwithActionDatasetscala at orgapachesparksqlDatasetheadDatasetscala at orgapachesparksqlDatasetheadDatasetscala at orgapachesparksqlDatasetfirstDatasetscala at orgapachesparkmlfeatureVectorAssemblerfirstlzycompute VectorAssemblerscala at orgapachesparkmlfeatureVectorAssemblerorgapachesparkmlfeatureVectorAssemblerfirst VectorAssemblerscala at orgapachesparkmlfeatureVectorAssembleranonfun anonfun applymcIspVectorAssemblerscala at orgapachesparkmlfeatureVectorAssembleranonfun anonfun applyVectorAssemblerscala at orgapachesparkmlfeatureVectorAssembleranonfun anonfun applyVectorAssemblerscala at scalaOptiongetOrElseOptionscala at orgapachesparkmlfeatureVectorAssembleranonfun applyVectorAssemblerscala at orgapachesparkmlfeatureVectorAssembleranonfun applyVectorAssemblerscala at scalacollectionTraversableLikeanonfunflatMap applyTraversableLikescala at scalacollectionTraversableLikeanonfunflatMap applyTraversableLikescala at scalacollectionIndexedSeqOptimizedclassforeachIndexedSeqOptimizedscala at scalacollectionmutableArrayOpsofRefforeachArrayOpsscala at scalacollectionTraversableLikeclassflatMapTraversableLikescala at scalacollectionmutableArrayOpsofRefflatMapArrayOpsscala at orgapachesparkmlfeatureVectorAssemblertransformVectorAssemblerscala at orgapachesparkmlPipelineModelanonfuntransform applyPipelinescala at orgapachesparkmlPipelineModelanonfuntransform applyPipelinescala at scalacollectionIndexedSeqOptimizedclassfoldlIndexedSeqOptimizedscala at scalacollectionIndexedSeqOptimizedclassfoldLeftIndexedSeqOptimizedscala at scalacollectionmutableArrayOpsofReffoldLeftArrayOpsscala at orgapachesparkmlPipelineModeltransformPipelinescala at comtrcccladocketsdeinferencefunctionsEntryPipelineFunctionsprepanddiscovermotionordersPipelineFunctionsscala at comtrcccladocketsdeinferenceSparkAppmainSparkAppscala at comtrcccladocketsdeinferenceSparkAppmainSparkAppscala at sunreflectNativeMethodAccessorImplinvoke Native Method at sunreflectNativeMethodAccessorImplinvokeNativeMethodAccessorImpljava at sunreflectDelegatingMethodAccessorImplinvokeDelegatingMethodAccessorImpljava at javalangreflectMethodinvokeMethodjava at orgapachesparkdeployyarnApplicationMasteranon runApplicationMasterscala Caused by javaioUTFDataFormatException encoded string too long bytes at javaioDataOutputStreamwriteUTFDataOutputStreamjava at javaioDataOutputStreamwriteUTFDataOutputStreamjava at comtypesafeconfigimplSerializedConfigValuewriteValueDataSerializedConfigValuejava at comtypesafeconfigimplSerializedConfigValuewriteValueSerializedConfigValuejava at comtypesafeconfigimplSerializedConfigValuewriteValueDataSerializedConfigValuejava at comtypesafeconfigimplSerializedConfigValuewriteValueSerializedConfigValuejava at comtypesafeconfigimplSerializedConfigValuewriteValueDataSerializedConfigValuejava at comtypesafeconfigimplSerializedConfigValuewriteValueSerializedConfigValuejava at comtypesafeconfigimplSerializedConfigValuewriteValueDataSerializedConfigValuejava at comtypesafeconfigimplSerializedConfigValuewriteValueSerializedConfigValuejava at comtypesafeconfigimplSerializedConfigValuewriteExternalSerializedConfigValuejava at javaioObjectOutputStreamwriteExternalDataObjectOutputStreamjava at javaioObjectOutputStreamwriteOrdinaryObjectObjectOutputStreamjava at javaioObjectOutputStreamwriteObject ObjectOutputStreamjava at javaioObjectOutputStreamdefaultWriteFieldsObjectOutputStreamjava at javaioObjectOutputStreamwriteSerialDataObjectOutputStreamjava at javaioObjectOutputStreamwriteOrdinaryObjectObjectOutputStreamjava at javaioObjectOutputStreamwriteObject ObjectOutputStreamjava at javaioObjectOutputStreamwriteArrayObjectOutputStreamjava at javaioObjectOutputStreamwriteObject ObjectOutputStreamjava at javaioObjectOutputStreamdefaultWriteFieldsObjectOutputStreamjava at javaioObjectOutputStreamwriteSerialDataObjectOutputStreamjava at javaioObjectOutputStreamwriteOrdinaryObjectObjectOutputStreamjava at javaioObjectOutputStreamwriteObject ObjectOutputStreamjava at javaioObjectOutputStreamdefaultWriteFieldsObjectOutputStreamjava at javaioObjectOutputStreamwriteSerialDataObjectOutputStreamjava at javaioObjectOutputStreamwriteOrdinaryObjectObjectOutputStreamjava at javaioObjectOutputStreamwriteObject ObjectOutputStreamjava at javaioObjectOutputStreamdefaultWriteFieldsObjectOutputStreamjava at javaioObjectOutputStreamwriteSerialDataObjectOutputStreamjava at javaioObjectOutputStreamwriteOrdinaryObjectObjectOutputStreamjava at javaioObjectOutputStreamwriteObject ObjectOutputStreamjava at javaioObjectOutputStreamwriteArrayObjectOutputStreamjava at javaioObjectOutputStreamwriteObject ObjectOutputStreamjava at javaioObjectOutputStreamdefaultWriteFieldsObjectOutputStreamjava at javaioObjectOutputStreamwriteSerialDataObjectOutputStreamjava at javaioObjectOutputStreamwriteOrdinaryObjectObjectOutputStreamjava at javaioObjectOutputStreamwriteObject ObjectOutputStreamjava at javaioObjectOutputStreamwriteObjectObjectOutputStreamjava at orgapachesparkserializerJavaSerializationStreamwriteObjectJavaSerializerscala at orgapachesparkserializerJavaSerializerInstanceserializeJavaSerializerscala at orgapachesparkutilClosureCleanerensureSerializableClosureCleanerscala Cannot import sparknlpocr sbt compile is failing because of a third party library that cannot find on maven javaxmediajaicomspringsourcejavaxmediajaicore Description Im tryin to use sparknlpocr so Ive added it to my project buildsbt but the compile is failing as follows info Resolving javaxmediajaicomspringsourcejavaxmediajaicore warn module not found javaxmediajaicomspringsourcejavaxmediajaicore warn local tried warn Usersdzlabivy localjavaxmediajaicomspringsourcejavaxmediajaicore ivysivyxml warn public tried warn warn localpreloadedivy tried warn Usersdzlabsbtpreloadedjavaxmediajaicomspringsourcejavaxmediajaicore ivysivyxml warn localpreloaded tried warn fileUsersdzlabsbtpreloadedjavaxmediajaicomspringsourcejavaxmediajaicore comspringsourcejavaxmediajaicore pom warn Maven Central tried warn warn osgeo tried warn warn geotoolkit tried warn warn Akka repository tried warn warn Typesafe repository tried warn warn Spark Packages Repo tried warn info Resolving jlinejline warn warn UNRESOLVED DEPENDENCIES warn warn javaxmediajaicomspringsourcejavaxmediajaicore not found warn warn warn Note Unresolved dependencies path warn javaxmediajaicomspringsourcejavaxmediajaicore warn comjohnsnowlabsnlpsparknlpocr UsersdzlabworkspacetestnlpbuildsbtL warn comdzlabtestnlp beta g d c dirtySNAPSHOT sbtResolveException unresolved dependency javaxmediajaicomspringsourcejavaxmediajaicore not found Expected Behavior I expect the project to compile when adding a dependency to sparknlpocr Current Behavior It seems that one of sparknlpocr dependencies javaxmediajaicomspringsourcejavaxmediajaicore cannot be found on Maven Possible Solution I tried to add other repos to my resolvers osgeo and geotoolkit like the following but still the issue persist lazy val commonSettings Seq resolvers Seq Maven Central at osgeo at geotoolkit at Akka repository at Typesafe repository at Spark Packages Repo at Here is how Im trying to import my sparknlp in my buildsbt val sparknlpVersion lazy val nlpLibs Seq comjohnsnowlabsnlp sparknlp sparknlpVersion comjohnsnowlabsnlp sparknlpocr sparknlpVersion comjohnsnowlabsnlp sparknlpeval sparknlpVersion But the issue is not solved I keep getting the same problem Steps to Reproduce Start an empty scala project Add sparknlpocr as dependcy in buildsbt Compile project with sbt compile Context Im trying to use Spark NLP OCR feature to parse text from PDF files if I cannot get this done I will swicth to use Apache TIKA which worked for me like a charm Your Environment Include as many relevant details about the environment you experienced the bug in Spark NLP version Apache NLP version not sure about this one Setup and installation Pypi Conda Maven etc buildsbt scala project Operating System and version OSX C Link to your project if any Using offline pretrained pipelines or models on EMR cluster results in Wrong FS error Description Offline pretrained pipeline doesnt get loaded successfully The pretrained pipeline was manually downloaded and then uploaded to a s bucket xxx Heres a code that was tested import orgapachesparkSparkConf import orgapachesparkmlPipelineModel import orgapachesparksqlSparkSession object SimpleApp def mainargs Array String initialise spark context val conf new SparkConfsetAppNameSimpleAppgetClassgetName val spark SparkSession SparkSessionbuilderconfigconfgetOrCreate printlnloading pretrained pipeline offline from s PipelineModelload s axxxpipelinesontorecognizeentitiessmen printlnloaded pretrained pipeline offline from s sparkstop Expected Behavior The pipeline should be loaded successfully Current Behavior An exception is thrown Exception in thread main javalangIllegalArgumentException Wrong FS hdfsipxxxxxxxxuswest computeinternal userhadoopcachepretrainedembeddingstmp expected file at orgapachehadoopfsFileSystemcheckPathFileSystemjava at orgapachehadoopfsRawLocalFileSystempathToFileRawLocalFileSystemjava at orgapachehadoopfsRawLocalFileSystemdeprecatedGetFileStatusRawLocalFileSystemjava at orgapachehadoopfsRawLocalFileSystemgetFileLinkStatusInternalRawLocalFileSystemjava at orgapachehadoopfsRawLocalFileSystemgetFileStatusRawLocalFileSystemjava at orgapachehadoopfsFilterFileSystemgetFileStatusFilterFileSystemjava at orgapachehadoopfsFileSystemexistsFileSystemjava at orgapachehadoopfsFileUtilcheckDestFileUtiljava at orgapachehadoopfsFileUtilcopyFileUtiljava at orgapachehadoopfsFileUtilcopyFileUtiljava at orgapachehadoopfsFileUtilcopyFileUtiljava at orgapachehadoopfsFileSystemcopyToLocalFileFileSystemjava at orgapachehadoopfsFileSystemcopyToLocalFileFileSystemjava at orgapachehadoopfsFileSystemcopyToLocalFileFileSystemjava at comjohnsnowlabsnlpembeddingsEmbeddingsHelperloadEmbeddingsHelperscala Possible Solution If embeddings are on S they are currently copied to the local file system even when code is running on an EMR cluster The code should be modified to handle copying to HDFS Steps to Reproduce Create an EMR cluster version Run SimpleApp see above using client mode on the EMR cluster Context Load pretrained pipelines and models to perform NER and sentiment analysis for huge volume of data on a distributed cluster EMR Your Environment EMR version Spark version Spark NLP version Provide a general summary of the issue in the Title above Description Provide a more detailed introduction to the issue itself and why you consider it to be a bug A Zeppelin or Jupyter notebook connected to a Spark cluster Spark on YARN in client mode was created to train a model for NER The Spark on YARN cluster is a service provided by Qubole that we are evaluating We also created a simple Spark on YARN cluster with one node ourselves solely for the purpose of writing this ticket Our simple pipeline consists of only WordEmbeddings NerDLApproach NerConverter and Finisher For WordEmbeddings we use the public GloVe file glove B dtxt For NerDLApproach we defined our custom tags so we have a combination of tag count embedding dimension character count and LSTM size different from that of the graphs included in the package We followed the instruction to generate a graph with our customized sizes In order for any executor to be able to access the embeddings file for WordEmbeddings and the Tensorflow graph folder for NerDLApproach at training time we put both the embeddings file and the graph on Hadoop When fit is called by the pipeline WordEmbeddings has no problem with reading the embeddings file on HDFS NerDLApproach finds the graph file on HDFS but throws an exception asking for a local path instead Naively I would assume both of them would accept HDFS paths or both of them would reject HDFS paths When one accepts HDFS and the other rejects HDFS either there is a bug or we missed something in our pipeline definition Expected Behavior Tell us what should happen When a Jupyter notebook is connected to Spark cluster on YARN NerDLApproachsetGraphFolder should accept an HDFS path as the location of the graph directory Current Behavior Tell us what happens instead of the expected behavior The Jupyter notebook shows an error that looks like the following Py JJavaError An error occurred while calling o fit javalangIllegalArgumentException Wrong FS hdfslocalhost userhadoopappsupplementdatatfgraphsblstmnoncontrib pb expected file We see the exact same behavior both with Qubole s Spark cluster and with our own Spark cluster Possible Solution Not obligatory but suggest a fixreason for the bug Steps to Reproduce Provide a link to a live example or an unambiguous set of steps to reproduce this bug Include code to reproduce if relevant As the user hadoop activate Python virtualenv source jupyterenterprisegatewayenvbinactivate Start Hadoop cluster startdfssh startyarnsh Start Jupyter Enterprise Gateway using the script from the attachment startjupyterenterprisegatewaysh Start Jupyter Notebook jupyter notebook gatewayurl GatewayClienthttpuserguest GatewayClienthttppwdguestpassword Establish an SSH tunnel from the local machine to the server running Jupyter Notebook In our case we do something like gcloud compute project GCP project ID ssh zone useast b sshflagL localhost C N hadoopGCP VM name TCP ports andor may also need to be open for Jupyter Enterprise Gateway Open the Jupyter notebook from the attachment in a web browser Context How has this bug affected you What were you trying to accomplish We need to establish an NER system in our production in Q We have been evaluating Qubole s Spark cluster service for the purpose However we are blocked from being able to make a decision because of this issue Qubole s engineers have been very generously trying to figure out if there is a workaround for us even though we re past our freetrial period already We should not keep dragging this on Your Environment Include as many relevant details about the environment you experienced the bug in This describes our own Spark cluster not Qubole s Spark cluster We are able to reproduce the same issue using our own Spark cluster Spark NLP version Apache Spark version We downloaded and unzipped it to optspark export SPARKHOMEoptspark export PATHPATHSPARKHOMEbinSPARKHOMEsbin Hadoop version We downloaded and unzipped it to homehadoophadoop We followed this instruction to configure Hadoop export JAVAHOMEusrlibjvmjava openjdkamd export HADOOPHOMEhomehadoophadoop export HADOOPINSTALLHADOOPHOME export HADOOPMAPREDHOMEHADOOPHOME export HADOOPCOMMONHOMEHADOOPHOME export HADOOPHDFSHOMEHADOOPHOME export YARNHOMEHADOOPHOME export HADOOPCOMMONLIBNATIVEDIRHADOOPHOMElibnative export PATHPATHHADOOPHOMEsbinHADOOPHOMEbin export HADOOPOPTSDjavalibrarypathHADOOPHOMElibnative export HADOOPCONFDIRHADOOPHOMEetchadoop Setup and installation Pypi Conda Maven etc hadoop user account and homehadoop need to exist Python because it is the highest version of Python that still allows pip install tensorflow We followed this instruction to install Python instead of Virtualenv eg python m venv jupyterenterprisegatewayenv pip install r requirementstxt requirementstxt is attached to the ticket Copy the sparkpythonyarnclient folder from the attachment to jupyterenterprisegatewayenvsharejupyterkernels Our version is a modified copy of the example from Copy the data files from the attachment to Hadoop hdfs dfs copyFromLocal sampletrainingdataparquet userhadoop hdfs dfs mkdir userhadoopappsupplementdata hdfs dfs copyFromLocal tfgraphs userhadoopappsupplementdata Get a copy of glove B dtxt from somewhere and copy it to Hadoop We cannot attach it to the ticket because it is too large for Github hdfs dfs mkdir userhadoopappsupplementdatawordembeddings hdfs dfs copyFromLocal path to glove B dtxt userhadoopappsupplementdatawordembeddings Operating System and version Ubuntu LTS GNULinux gcp x Link to your project if any requirementstxt jupyterenterprisegatewaycustomizationtargz supplementdatatargz ExampleNotebookwithSparkonYARNClientModeipynbzip Provide a general summary of the issue in the Title above CoNLL produce bad sentences annotations Description Provide a more detailed introduction to the issue itself and why you consider it to be a bug I train a NerDlApproach with CoNLL files using the sentences column in the inputcols After the fit when i transform a test file in a pipeline with a NerConverter Anntoator this one produces only results for the first sentence Trying to evaluate with NerHelpermeasureExact the results are bad If you make the same using the document column in NerDlApproach it works well Expected Behavior Tell us what should happen Current Behavior Tell us what happens instead of the expected behavior NERSPAN after finisher extracting from metadata Any WrappedArray entityMUTDNA sentence chunk LABELSPAN after finisher extracting from metadata Any WrappedArray entityMUTDNA sentence chunk entityMUTPRO sentence chunk Possible Solution Not obligatory but suggest a fixreason for the bug Steps to Reproduce Provide a link to a live example or an unambiguous set of steps to reproduce this bug Include code to reproduce if relevant Context How has this bug affected you What were you trying to accomplish Your Environment Include as many relevant details about the environment you experienced the bug in Spark NLP version Apache NLP version Setup and installation Pypi Conda Maven etc Operating System and version Link to your project if any Added to documentation section related OCR Provide a general summary of the issue in the Title above Description Provide a more detailed introduction to the issue itself and why you consider it to be a bug While some of the issue is specific to my environment I think it could occur in multiple contexts where the user does not have write access for temporary directories I am in an environment where I do not have write access to most folders and need to use sudo to accomplish most filesystem tasks My understanding is that spark nlp creates a temporary directory tmpsparknlpcontribspar for which in this case it doesnt have permissions Expected Behavior Tell us what should happen I should be able to load the embeddings and fit a model using them Current Behavior Tell us what happens instead of the expected behavior Py JJavaError An error occurred while calling o load javaioFileNotFoundException tmpsparknlpcontribspar Permission denied at javaioFileOutputStreamopen Native Method at javaioFileOutputStreamopenFileOutputStreamjava at javaioFileOutputStreaminitFileOutputStreamjava at javaioFileOutputStreaminitFileOutputStreamjava at comjohnsnowlabsnlpannotatorsnerdlLoadsContribcopyResourceToTmpLoadsContribscala at comjohnsnowlabsnlpannotatorsnerdlLoadsContribloadContribToClusterLoadsContribscala at comjohnsnowlabsmltensorflowReadTensorflowModelclassreadTensorflowModelTensorflowSerializeModelscala Possible Solution Not obligatory but suggest a fixreason for the bug One possible solution would be to check if the directory is writable and if not allow the user to specify their own temporary directory Steps to Reproduce Provide a link to a live example or an unambiguous set of steps to reproduce this bug Include code to reproduce if relevant bert BertEmbeddingsloadbertpath setInputCols tokensentence setOutputColbert Context How has this bug affected you What were you trying to accomplish Use pretrained BertEmbeddings for classification and NER Your Environment Include as many relevant details about the environment you experienced the bug in Version used Operating System and version desktop or mobile CentOS 