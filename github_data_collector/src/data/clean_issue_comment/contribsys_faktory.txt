Have a problem with longrunning jobs After about minutes this keeps happening which then leaves the worker running but the worker disappears from the UI The job is still active as it has a long reservefor Any suggestions on how to debug Why would the worker stop sending a heartbeat E T Z errorread tcp use of closed network connection Unexpected socket error W T Z Reaped lingering connections this is a sign your workers are having problems W T Z All worker processes should send a heartbeat every seconds This is less of a request for mperham and more of a question to the community Before moving to Faktory we made extensive use of Has anyone looked at building something similar Obviously its a bit more tricky as you would need an external data store since you cannot talk directly to the redis instance curl is not available so need another mechanism for establishing the health of faktory Hey We spoke about this via email and you asked me to create a ticket to remind you to build it Almost all customers have been running Docker Make their life easier by providing a private Faktory image repository with Faktory Pro and Enterprise docker images for ease of deployment See TODO x Prototype to determine feasibility x Machine setup x Wiki documentation x Publish images Blog post Move forward under the assumption that people are using a Statsd server that understands metric tagging Deprecate These will be removed in a future release Name Type Desc Tags jobsjobtypecount Counter Execution count for jobtype increments upon ACKFAIL queuequeue jobsjobtypefailed Counter Failed count for jobtype increments on FAIL queuequeue jobsjobtypeperform Gauge time Time between FETCH and ACK queuequeue By creating metrics for each jobtype we are exploding the cardinality which can lead to big jumps in cost of storage Many metrics SaaSes price based on unique metric name count Instead each metric will have a jobtypejobtype tag like so Name Type Desc Tags jobscount Counter Execution count increments upon ACKFAIL queuequeue jobtypejobtype jobsfailed Counter Failed count increments on FAIL queuequeue jobtypejobtype jobsperform Gauge time Time between FETCH and ACK queuequeue jobtypejobtype Add Name Type Desc Tags jobsexpired Counter Job expiration queuequeue jobtypejobtype uniqlock Counter A unique job lock was taken queuequeue jobtypejobtype uniqdenied Counter A job push was denied due to uniqueness queuequeue jobtypejobtype uniqunlock Counter A unique job lock was released queuequeue jobtypejobtype cronpush Counter A periodic job was pushed queuequeue jobtypejobtype How do I push with Ruby lib a job to a faktory queue that will be picked up by Go or other workers performasyncsomething Perhaps theres something that could be done like scaling the font size with the window width or making content wrap instead of spill A couple of times now Ive had to dump a queue because it was backlogging In this queue all jobs are meant to be unique Every few minutes I have a cron task that wakes up determines a set of work to do about k jobs and fires jobs for each unit of work to be done My goal is that each job exists at most once in the queue at any given time I had been seeing backlogs form despite setting uniquefor This doesnt make a ton of sense as I have enough workers that most of the time I should be burning through all the jobs quite quickly I suspect it has to do with notsointermittent failures see the dial issues I talk about in another ticket and the jobs being enqueued for retry Add in a short uniqueness window of hour and after a few hours I made some changes including setting uniquefor uniqueuntilstart reservefor the period of the cron job Along the way I had to dump the queue again a few more times Somehow I wound up with keys in Redis of the form uniquehash apparently set to have expiration times of seconds Clearing a queue but leaving behind uniqueness tokens is a pretty surprising behavior to say the least Perhaps you could store the uniqueness tokens for a given queue as a set and clear the set when clearing the queue That would mean uniqueness is scoped to a specific queue but honestly that was the behavior I was expecting in the first place In the meantime Im open to suggestions on how best to proceed This is basically a racetrack of sorts where I just loop through and enqueue the same things every few minutes If it takes longer than that window to burn through the whole list of jobs thats fine The units of work are hitting external systems and as such things might get bogged down from time to time A backlog of duplicate items isnt meaningfulhelpful and presents an operational headache Expiring jobs out just runs the risk of certain units of work being starved out NB is a symptom of this issue Were seeing bursts of this error in production We have workers with concurrency of plus one node that is not a Faktory worker but fires jobs into Faktory based on HTTP requests that come in Workers are seeing the error when trying to fire new jobs from existing ones and the web server is seeing these errors as well Theres a bit of a pattern involved where we get a burst once every hour spanning about minutes however I dont know if thats because were getting bursts of requests every hour or theres some sort of network tuning issue at play Weve covered the basics max files for all processes involved FINWAIT setting for all machines involved etc but are still trying to narrow this down Our current best guess is that the second dial timeout for establishing a connection to the Faktory server is relevant and Im going to be bypassing faktoryOpen to modify that value and test things At a minimum exposing this as a configurable option seems like a prudent option no Would you accept a PR that creates a second method maybe OpenWithServer allowing clients to pass in a Server object in order to allow configuring of relevant settings without having to copypaste the logic of Open