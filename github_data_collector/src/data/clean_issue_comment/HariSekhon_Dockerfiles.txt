Help me When a try to add file Hi HariSekhon i having problem to start the hadoop dockercompose with the error below dockercompose up d Starting hadoophadoop error ERROR for hadoophadoop Cannot start service hadoop driver failed programming external connectivity on endpoint hadoophadoop a d bf e f eb bd f ac feb cf c e a c e ee f Error starting userland proxy Bind for unexpected error Permission denied ERROR for hadoop Cannot start service hadoop driver failed programming external connectivity on endpoint hadoophadoop a d bf e f eb bd f ac feb cf c e a c e ee f Error starting userland proxy Bind for unexpected error Permission denied ERROR Encountered errors while bringing up the project Appreciate for any advice Jason Perhaps Im reading it wrong but it looks like the prebuilt images for spark are only for Spark x would be an improvement Im happy to try to help with this though I dont know where your prebuilts are configured nor how to run regression tests I have confirmed this on machines on the spark folder dockercompose up docker exec ti sparkspark binbash bash binsparkshell log jWARN No appenders could be found for logger orgapachehadoopmetrics libMutableMetricsFactory log jWARN Please initialize the log j system properly log jWARN See for more info Using Sparks repl log j profile orgapachesparklog jdefaultsreplproperties To adjust logging level use scsetLogLevelINFO Welcome to version Using Scala version OpenJDK Bit Server VM Java Type in expressions to have them evaluated Type help for more information Spark context available as sc WARN General Plugin Bundle orgdatanucleus is already registered Ensure you dont have multiple JAR versions of the same plugin in the classpath The URL filespark binhadoop libdatanucleuscore jar is already registered and you are trying to register an identical plugin located at URL filesparklibdatanucleuscore jar WARN General Plugin Bundle orgdatanucleusapijdo is already registered Ensure you dont have multiple JAR versions of the same plugin in the classpath The URL filespark binhadoop libdatanucleusapijdo jar is already registered and you are trying to register an identical plugin located at URL filesparklibdatanucleusapijdo jar WARN General Plugin Bundle orgdatanucleusstorerdbms is already registered Ensure you dont have multiple JAR versions of the same plugin in the classpath The URL filesparklibdatanucleusrdbms jar is already registered and you are trying to register an identical plugin located at URL filespark binhadoop libdatanucleusrdbms jar WARN Connection BoneCP specified but not present in CLASSPATH or one of dependencies WARN Connection BoneCP specified but not present in CLASSPATH or one of dependencies WARN ObjectStore Version information not found in metastore hivemetastoreschemaverification is not enabled so recording the schema version WARN ObjectStore Failed to get database default returning NoSuchObjectException WARN General Plugin Bundle orgdatanucleus is already registered Ensure you dont have multiple JAR versions of the same plugin in the classpath The URL filespark binhadoop libdatanucleuscore jar is already registered and you are trying to register an identical plugin located at URL filesparklibdatanucleuscore jar WARN General Plugin Bundle orgdatanucleusstorerdbms is already registered Ensure you dont have multiple JAR versions of the same plugin in the classpath The URL filesparklibdatanucleusrdbms jar is already registered and you are trying to register an identical plugin located at URL filespark binhadoop libdatanucleusrdbms jar WARN General Plugin Bundle orgdatanucleusapijdo is already registered Ensure you dont have multiple JAR versions of the same plugin in the classpath The URL filespark binhadoop libdatanucleusapijdo jar is already registered and you are trying to register an identical plugin located at URL filesparklibdatanucleusapijdo jar WARN Connection BoneCP specified but not present in CLASSPATH or one of dependencies WARN Connection BoneCP specified but not present in CLASSPATH or one of dependencies SQL context available as sqlContext scala scala val lines sctextFileREADMEmd javalangIllegalArgumentException javalangUnsatisfiedLinkError tmpsnappy fdb f be a libsnappyjavaso Error loading shared library ldlinuxx so No such file or directory needed by tmpsnappy fdb f be a libsnappyjavaso at orgapachesparkioSnappyCompressionCodecliftedTree CompressionCodecscala at orgapachesparkioSnappyCompressionCodecorgapachesparkioSnappyCompressionCodecversionlzycomputeCompressionCodecscala at orgapachesparkioSnappyCompressionCodecorgapachesparkioSnappyCompressionCodecversionCompressionCodecscala at orgapachesparkioSnappyCompressionCodecinitCompressionCodecscala at sunreflectNativeConstructorAccessorImplnewInstance Native Method at sunreflectNativeConstructorAccessorImplnewInstanceNativeConstructorAccessorImpljava at sunreflectDelegatingConstructorAccessorImplnewInstanceDelegatingConstructorAccessorImpljava at javalangreflectConstructornewInstanceConstructorjava at orgapachesparkioCompressionCodeccreateCodecCompressionCodecscala at orgapachesparkioCompressionCodeccreateCodecCompressionCodecscala at orgapachesparkbroadcastTorrentBroadcastorgapachesparkbroadcastTorrentBroadcastsetConfTorrentBroadcastscala at orgapachesparkbroadcastTorrentBroadcastinitTorrentBroadcastscala at orgapachesparkbroadcastTorrentBroadcastFactorynewBroadcastTorrentBroadcastFactoryscala at orgapachesparkbroadcastBroadcastManagernewBroadcastBroadcastManagerscala at orgapachesparkSparkContextbroadcastSparkContextscala at orgapachesparkSparkContextanonfunhadoopFile applySparkContextscala at orgapachesparkSparkContextanonfunhadoopFile applySparkContextscala at orgapachesparkrddRDDOperationScopewithScopeRDDOperationScopescala at orgapachesparkrddRDDOperationScopewithScopeRDDOperationScopescala at orgapachesparkSparkContextwithScopeSparkContextscala at orgapachesparkSparkContexthadoopFileSparkContextscala at orgapachesparkSparkContextanonfuntextFile applySparkContextscala at orgapachesparkSparkContextanonfuntextFile applySparkContextscala at orgapachesparkrddRDDOperationScopewithScopeRDDOperationScopescala at orgapachesparkrddRDDOperationScopewithScopeRDDOperationScopescala at orgapachesparkSparkContextwithScopeSparkContextscala at orgapachesparkSparkContexttextFileSparkContextscala at iwCiwCiwCiwCiwCiwCiwCiwCinitconsole at iwCiwCiwCiwCiwCiwCiwCinitconsole at iwCiwCiwCiwCiwCiwCinitconsole at iwCiwCiwCiwCiwCinitconsole at iwCiwCiwCiwCinitconsole at iwCiwCiwCinitconsole at iwCiwCinitconsole at iwCinitconsole at initconsole at initconsole at clinitconsole at initconsole at clinitconsole at printconsole at sunreflectNativeMethodAccessorImplinvoke Native Method at sunreflectNativeMethodAccessorImplinvokeNativeMethodAccessorImpljava at sunreflectDelegatingMethodAccessorImplinvokeDelegatingMethodAccessorImpljava at javalangreflectMethodinvokeMethodjava at orgapachesparkreplSparkIMainReadEvalPrintcallSparkIMainscala at orgapachesparkreplSparkIMainRequestloadAndRunSparkIMainscala at orgapachesparkreplSparkIMainloadAndRunReq SparkIMainscala at orgapachesparkreplSparkIMaininterpretSparkIMainscala at orgapachesparkreplSparkIMaininterpretSparkIMainscala at orgapachesparkreplSparkILoopreallyInterpret SparkILoopscala at orgapachesparkreplSparkILoopinterpretStartingWithSparkILoopscala at orgapachesparkreplSparkILoopcommandSparkILoopscala at orgapachesparkreplSparkILoopprocessLine SparkILoopscala at orgapachesparkreplSparkILoopinnerLoop SparkILoopscala at orgapachesparkreplSparkILooporgapachesparkreplSparkILooploopSparkILoopscala at orgapachesparkreplSparkILoopanonfunorgapachesparkreplSparkILoopprocess applymcZspSparkILoopscala at orgapachesparkreplSparkILoopanonfunorgapachesparkreplSparkILoopprocess applySparkILoopscala at orgapachesparkreplSparkILoopanonfunorgapachesparkreplSparkILoopprocess applySparkILoopscala at scalatoolsnscutilScalaClassLoadersavingContextLoaderScalaClassLoaderscala at orgapachesparkreplSparkILooporgapachesparkreplSparkILoopprocessSparkILoopscala at orgapachesparkreplSparkILoopprocessSparkILoopscala at orgapachesparkreplMainmainMainscala at orgapachesparkreplMainmainMainscala at sunreflectNativeMethodAccessorImplinvoke Native Method at sunreflectNativeMethodAccessorImplinvokeNativeMethodAccessorImpljava at sunreflectDelegatingMethodAccessorImplinvokeDelegatingMethodAccessorImpljava at javalangreflectMethodinvokeMethodjava at orgapachesparkdeploySparkSubmitorgapachesparkdeploySparkSubmitrunMainSparkSubmitscala at orgapachesparkdeploySparkSubmitdoRunMain SparkSubmitscala at orgapachesparkdeploySparkSubmitsubmitSparkSubmitscala at orgapachesparkdeploySparkSubmitmainSparkSubmitscala at orgapachesparkdeploySparkSubmitmainSparkSubmitscala Caused by javalangUnsatisfiedLinkError tmpsnappy fdb f be a libsnappyjavaso Error loading shared library ldlinuxx so No such file or directory needed by tmpsnappy fdb f be a libsnappyjavaso at javalangClassLoaderNativeLibraryloadNative Method at javalangClassLoaderloadLibrary ClassLoaderjava at javalangClassLoaderloadLibraryClassLoaderjava at javalangRuntimeload Runtimejava at javalangSystemloadSystemjava at orgxerialsnappySnappyLoaderloadNativeLibrarySnappyLoaderjava at orgxerialsnappySnappyLoaderloadSnappyLoaderjava at orgxerialsnappySnappyclinitSnappyjava at orgapachesparkioSnappyCompressionCodecliftedTree CompressionCodecscala more scala I tried to create volumes on solrexamplecloud but the permissions are incorrect Could you please add a chmod in the DockerFile to set the right permissions to solrexamplecloud before starting solr Hi Im using your compose for rabbitmq and ran into a little bug when running non RAM workers the joincluster fails with joining cluster via seed rabbitmanager Error operation joincluster used with invalid parameter rabbitrabbitmanager And everything is working on RAM nodes I think its about an extra space when running the joincluster without RAM set For your work 