 Need additional metrics from lighthouse Currently only metrics are exported to Google Sheet First Contentful Paint First Meaningful Paint Speed Index First CPU Idle Time to Interactive There are metrics from Audits of DevTools At least export additional one Max Potential First Input Delay Will be great if more are exported Environment pwmetrics version Chrome version Version Official Build bit OS version Win X Config CLI options pwmetrics configpwmetricsconfigjs Errors Error while trying to retrieve access token ENOENT no such file or directory mkdir Users my credentials Error while trying to retrieve access token invalidgrant at GoogleOauthgetNewToken Solution After digging out in googleoauthjs change the line as below thistokenDir pathjoinprocessenvHOME processenvHOMEPATH processenvUSERPROFILE credentials Original thistokenDir pathjoinprocessenvUSERPROFILE processenvHOME processenvHOMEPATH credentials Reason tokenDir in the original line is equal to Users username credentials tokenDir in the new line is equal to C Users username credentials which is the right for Windows Environment pwmetrics version Chrome version Version Official Build bit OS version Win X Are you considering on adding additional metrics like Time to First Byte or is there a way I can add additional metrics on my own Steps to reproduce yarn audit Stack trace high Improper Authorization Package googleapis Patched in Dependency of pwmetrics Path pwmetrics googleapis More info Environment pwmetrics version Included changes Fixes the fixme for choosing the median result by actually basing the decision off the index of the median value instead of looking up the median value Adds a new metric flag which can specify the metric to be used for the median run calculation defaulting to TTFCPUIDLE Also adds support for two special values for the metric flag although they may be better off broken out into separate flags metricall will report the medians for all metrics independently instead of the single run for the median of a single metric metricaverage will report the average for all metrics independently instead of the single run for the median of a single metric Implements Would this package be open to alternative calculations of the median run Currently it chooses the run with the median TTFCPUIDLE value but that may not necessarily be the median SI or TTI value either Depending on which value the consumer is most concerned with testing I could see misleading results by always basing the median run off TTFCPUIDLE Im wondering if it would be helpful to provide config flags to specify which nmetric to calculate the median run using Im also tossing around whether I think it would be useful to report the average for each metric independently or not Thanks Suppose Im running pwmetrics in a postmerge CI job to track my apps performance over time Id like to associate each row of data with the git commit of the code being tested so I can pinpoint where a regression or improvement was introduced If I could have an additional option in the sheets options for a comments field that would accept a string or function I could accomplish this as follows typescript moduleexports sheets type GOOGLESHEETS options comments requirechildprocessexecSyncgit revparse short HEADtoString There could be more advanced use cases that would benefit from passing in metadata to the function but for my particular use case this simple start would be fine Id be happy to contribute a PR if the feature is approved Currently we are able to publish PWMetrics in the scope of GitHub Package Registry Id like to hear from folks if everyone is ok with that cc paulirish pedro PS I can do operation work Since we have an awesome lighthousepluginfieldperformance for measuring RUM it will be good to have a flag fieldrumwhatever which will run url against it and provide real user perf metrics Blocked by 