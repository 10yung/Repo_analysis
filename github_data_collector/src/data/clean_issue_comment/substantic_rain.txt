TODO check Python package build fix DataObject destructor in Python client use better defaults for listen addrport run websockets on same port as HTTP or switch governors to Websockets change Rust client to use Websockets all hi all I have read the doc on The example given in getstarted as below python from rainclient import Client tasks blob Connect to server client Clientlocalhost Create a new session with clientnewsession as session Create task and two data objects task tasksConcatblobHello blobworld Mark that the output should be kept after submit taskoutputkeep Submit all crated tasks to server sessionsubmit Wait for completion of task and fetch results and get it as bytes result taskoutputfetchgetbytes Prints Hello world printresult This is so amusing But Python support asyncawait syntax does this project support this syntax in roadmap I find this task Update in the Python API using aiohttp for async API gavento medium Is it in order to support asyncawait syntax After supporting asyncawait syntax the example maybe shows as below python from rainclient import Client tasks blob Connect to server client Clientlocalhost Create a new session async with clientnewsession as session Create task and two data objects task tasksConcatblobHello blobworld Mark that the output should be kept after submit taskoutputkeep Submit all crated tasks to server sessionsubmit Wait for completion of task and fetch results and get it as bytes result await taskoutputfetchgetbytes Prints Hello world printresult A document to track the directions from replacing Our mid and longterm goals their priority asignee and any subtasks Any help is welcome with mentoring available for most tasks Remaining enhancements from v Will be updated after prioritization discussion Clientside protocols Replace capnp RPC and the current monitoring dashboard HTTP API with common protocol Part of more discussion there but specific to the public API Design the API calls gavento medium Implement in the server gavento medium Update in the Python API using aiohttp for async API gavento medium Update the dashboard gavento medium Improve the dashboard with more information and postmortem analysis Design and revamp the dashborad Depends on the client API development gavento mediumlow x Include stats for taskobject groups and possibly nameslabels from low Fix current bugs occurs under heavy load only medium seems to be bound to Exoscale deployment high Custom tasks subworkers in more languages Python subworker as a library low run standalone scripts as opposed to defining them in the client only Easier deployment in the cloud Deployment in the amazon cloud vojtechcima medium Packaging for easier deployment Multiple options priorities may vary spirali AppImage packages low we already have static binaries Snapcraft has a rust plugin Debother distro packages low There is cargodeb Improve Python API Pythonize the client API Draft contenttype loadersextensions gavento low x Taskobject groups and nameslabels low Improve testing infrastructure Scriptscontainers to test deployment and running in a network vojtechcima medium Test rain start and running on OpenStack Exoscale AWS Does not have to be a part of CI even for running locally Depends on part of More realworld code examples Lower priority best based on real usecases Ideas numpy subtasks CRust subworkers Enhancements to revisit in the not so distant future Integration with some popular libraries Apache Arrow contenttype Basic type and loading is implemented We could add more operations filter split merge XGBoost tasks etc Why not now Not clear what would be the demand Worker configuration files needed for common CPU and special resources GPU different subworker locatins and configurations Partially done Why not now Needs to be thoughtthrough esp wrt resources not needed now Separate session construction and running saveload session Why not now Not clear what would be the usecases not difficult when API stabilized Clients in other languages Rust C Java Why not now Not clear what would be the demand Easier after the protocolPython API stabilization Scale the scheduler benchmarks There is a benchmark in utilsbenchsimpletaskscalingpy The results as of are here Why not now While eventually crucial the scheduler is sufficient when there are tasks to be scheduled at once This PR introduces a Rust client for the cluster with a synchronous API that mirrors the Python client The client is used to implement a stop command for the cluster The terminateServer RPC call on the scheduler is now implemented with a trivial exit later it should probably do a more graceful shutdown Im going to need help with my archnemesis storing closures in structs to somehow implement Drop for Session which needs access to the client Currently a crash of a subworker may crash a worker and a crash of a worker may crash the server We need to improve this However we are not aiming for infrastructure resiliency now Subworker crash may still fail the task and so also the session and worker crash may still lose all the objects and fail all involved sessions The main goal is to keep the server running and deliver a graceful error A robust failure handling will open up the road to retrying tasks possibly on different workers and later to worker crash resiliency Improve the online monitoring to Show session list summary with more info Show more statistics and graphs eg aggregation by task type or group proposed in Show a timeline by groups or with significant all or eg named events Only display the dependency graph optionally and for reasonably small graphs As a future improvement we could display the dependency graph of groups clusters task types etc See the full details of every task and object on demand Examine history of the events in some way a timeline may be enough Do all of this for finished and closed sessions via rerunning the server with the sa This depends on a streaming API and may require a complete UI framework While Rain startup on PBS already has basic support it would be great to have scripts to setup and control at least teardown deployment on some of CloudStack API Exoscale AWS Goole cloud vojtechcima is already working on that Vojta would you please take over and fill some plans and status Add task attribute to allow only a subset of workers An empty attribute Usage Working on files local to a worker either reading or writing Useful for open and store tasks but not limited to them Selecting suitable worker subset for a taks when virtual resources are not appropriate However for GPUs and other features we should use resources Testing Constant data objects already have a similar feature for placement but this API is not planned to be published and it is not really relevant for computed data objects Alternative Every worker could be a resource required by the task However this is not ergonomic and we do not have a clear picture of how to do virtual resources in the right way Introduce optional taskdata object groups and names for better monitoring and possibly debugging Currently the data objects have a label indicating the role it plays in the producer task eg log score Introduce task and data object name arbitrary string optional to be set for any tasks the user wants to distinguish later by their name May not be unique in the session but is intended to be unique Always set by the user Goal be able to distinguish special tasks in the graph and to query them Importance low Perhaps we need to think about possible uses and name vs label on objects not to be confusing task and data object groups a textual label splitting the nodes into disjoint groups including the None group Goal to be able to monitor statistics of different taskDO groups meaningful progress reporting and stats aggregation Advantage easy to report compact pergroup aggregates even store them in the log Variant add arbitrary tags aggregate by tags combination Drawbacks are added complexity to many tag combinations and less monitoring meaning the stats no longer add to all nodes Work in progress update on Attributes Waits on internal attributes discussion and cleaup