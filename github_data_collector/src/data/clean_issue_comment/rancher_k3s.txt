I have K s release v k s installed on my Raspberry Pi cluster running on Raspbian Buster I used the standard configuration for installation and Traefik is being used as the ingress controller I would like to disable TLS verification in Traefik by setting the insecureSkipVerify setting to true Note I am running Kubernetes Dashboard with a selfsigned certificate This is on my home network and Im not too concerned about verifying the validity of the certificate K s appears to install Traefik using a Helm Chart and I can see the Traefik chart manifest is installed in varlibrancherk sservermanifeststraefikyaml I have updated the traefikyaml in this folder to include the additional setting apiVersion helmcattleiov kind HelmChart metadata name traefik namespace kubesystem spec chart set rbacenabled true sslenabled true metricsprometheusenabled true kubernetesingressEndpointuseDefaultPublishedService true New TLS skip verify certificate setting sslinsecureSkipVerify true How can I apply the updated Traefik settings from my chart manifest file Is your feature request related to a problem Please describe Automating a k s ha setup is a bit difficult because if you start k s using the clusterinit parameter the server will run forever It would be better to do a brief startup and exit the system once some self check is sucessful A subsequent start of the first server should just be without the parameter If provided it should just exit with an error like cluster already initialized The server parameter should also be similar it should be use only for the first startup It should just do a inital sync and exit Right now my Vagrant provision scripts looks like this first server K STOKENSECRET timeout s k s server clusterinit flannelifaceeth K STOKENSECRET k s server flannelifaceeth other servers K STOKENSECRET timeout s k s server server flannelifaceeth K STOKENSECRET k s server flannelifaceeth In a perfect world after cluster init all systemd service files should be the same Furthermore I should be able to reboot one server at a time without a cluster breakdown Stopping all servers and starting them one by one after a power outage should also be possible Version k s version v e a c Describe the bug The external datastore usage is too high while nothing is being deployed in the cluster except the base containers To Reproduce Run a cluster with two masters and two workers connected to a single mysql database Expected behavior Database should not grow indefinitely Actual behavior K s is adding rows per second causing the entire cluster to fail when the datastore is full Additional context Is there a way to limit the size of the external datastore I do see that the kine table has binary blobs on each row and its currently around M rows with approx G of disk used The cluster is for home use running on a PI and x VMs in esxi We should document the debug flag and its environment variable K SDEBUG We should have perhaps a Troubleshooting section in our docs that could discuss the debug flagenv var and perhaps we mention Logging here as well Logging should be separate but we could link to it from this Troubleshooting section I propose Is there any way to disable auto deployment for multiple target at once with nodeploy Opened via we use rancherk s for tracking docs currently as its easier to manage on our boards What exactly does disable network policy do From a user standpoint I don t see any docs on what the network policy is to begin with google k s network policy or search the docs page So Im not not sure what is being disabled when I use the disablenetworkpolicy flag or how it correlates to other flags such as the group for networking clustercidr value networking Network CIDR to use for pod IPs default servicecidr value networking Network CIDR to use for services IPs default clusterdns value networking Cluster IP for coredns service Should be in your servicecidr range default clusterdomain value networking Cluster Domain default clusterlocal flannelbackend value networking One of none vxlan ipsec or flannel default vxlan This is coming from ibuildthecloud we really need a someway to have stable and testing releases like v shouldnt be considered stable not saying they are low quality but theres got to be some bake in time i think conservative users should not be jumping on the release the day it comes out rancherk sv k s k s chart answer overrides can be found at bottom of this issue k s is installed as an app inside a Rancher cluster DNS resolution is not working a Helm chart fails to deploy from manifest with error helmv repo add rancher Error looks like is not a valid chart repository or cannot be reached Get dial tcp io timeout Logs troubleshooting kubectl apply f kubectl exec ti dnsutils n default nslookup rancherreleasescom connection timed out no servers could be reached kubectl exec ti dnsutils n default nslookup googlecom connection timed out no servers could be reached kubectl get pods n kubesystem NAME READY STATUS RESTARTS AGE localpathprovisioner fb bdfdf dlp Running h m metricsserver d c b l z Running h m corednsd c ddftm m Running h m helminstallrancherx nsz CrashLoopBackOff m kubectl logs corednsd c ddftm m n kubesystem INFO pluginreload Running configuration MD bf c b fcfd c cb CoreDNS linuxamd go b INFO Reloading INFO pluginreload Running configuration MD f cc e fb d a a e INFO Reloading complete INFO Reloading INFO HINFO IN udp false NXDOMAIN qrrdra s INFO pluginreload Running configuration MD d fc c df f c a a INFO Reloading complete kubectl run nginx image nginx n default kubectl run generatordeploymentappsv is DEPRECATED and will be removed in a future version Use kubectl run generatorrunpodv or kubectl create instead deploymentappsnginx created kubectl exec it n default nginx db d b xpt binbash rootnginx db d b xpt aptget update Connecting to debdebianorg Connecting to securitydebianorg C stuck here rootnginx db d b xpt cat etcresolvconf search defaultsvcclusterlocal svcclusterlocal clusterlocal k ssvcclusterlocal uswest computeinternal nameserver options ndots exit kubectl get svcep n kubesystem NAME TYPE CLUSTERIP EXTERNALIP PORTS AGE servicekubedns ClusterIP none UDP TCP TCP h m servicemetricsserver ClusterIP none TCP h m NAME ENDPOINTS AGE endpointsmetricsserver h m endpointskubedns h m endpointsrancheriolocalpath none h m kubectl get pods n kubesystem o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES localpathprovisioner fb bdfdf dlp Running h m k sagent c bb z x none none metricsserver d c b l z Running h m k sagent c bb rxgp none none corednsd c ddftm m Running h m k sagent c bb rxgp none none helminstallrancherx nsz CrashLoopBackOff m k sserver b c cbdbdwgjd none none kubectl get nodes o wide NAME STATUS ROLES AGE VERSION INTERNALIP EXTERNALIP OSIMAGE KERNELVERSION CONTAINERRUNTIME k sagent c bb rxgp Ready none h m v k s none Unknown aws containerd k s k sagent c bb z x Ready none h m v k s none Unknown aws containerd k s k sserver b c cbdbdwgjd Ready master h m v k s none Unknown aws containerd k s kubectl get pods A o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kubesystem localpathprovisioner fb bdfdf dlp Running h m k sagent c bb z x none none ingressnginx defaulthttpbackend cdffc gd cs Running h m k sagent c bb z x none none kubesystem metricsserver d c b l z Running h m k sagent c bb rxgp none none kubesystem corednsd c ddftm m Running h m k sagent c bb rxgp none none ingressnginx nginxingresscontroller klvl Running h m k sagent c bb z x none none ingressnginx nginxingresscontrollerqgqpw Running h m k sagent c bb rxgp none none ingressnginx nginxingresscontrollerhvn b Running h m k sserver b c cbdbdwgjd none none default dnsutils Running m k sserver b c cbdbdwgjd none none default nginx db d b xpt Running m s k sagent c bb z x none none kubesystem helminstallrancherx nsz CrashLoopBackOff m k sserver b c cbdbdwgjd none none Answer overrides for k s app image repository rancherk s tag v k s server enableTraefik false manifests nginxnsyaml apiVersion v kind Namespace metadata name ingressnginx nginxconfyaml kind ConfigMap apiVersion v metadata name nginxconfiguration namespace ingressnginx labels app ingressnginx nginxtcpyaml kind ConfigMap apiVersion v metadata name tcpservices namespace ingressnginx nginxudpyaml kind ConfigMap apiVersion v metadata name udpservices namespace ingressnginx nginxsayaml apiVersion v kind ServiceAccount metadata name nginxingressserviceaccount namespace ingressnginx nginxrbacyaml apiVersion rbacauthorizationk siov beta kind ClusterRole metadata name nginxingressclusterrole rules apiGroups resources configmaps endpoints nodes pods secrets verbs list watch apiGroups resources nodes verbs get apiGroups resources services verbs get list watch apiGroups extensions networkingk sio resources ingresses daemonsets verbs get list watch apiGroups resources events verbs create patch apiGroups extensions resources ingressesstatus verbs update nginxroleyaml apiVersion rbacauthorizationk siov beta kind Role metadata name nginxingressrole namespace ingressnginx rules apiGroups resources configmaps pods secrets namespaces verbs get apiGroups resources configmaps resourceNames Defaults to electionidingressclass Here ingresscontrollerleadernginx This has to be adapted if you change either parameter when launching the nginxingresscontroller ingresscontrollerleadernginx verbs get update apiGroups resources configmaps verbs create apiGroups resources endpoints verbs get nginxrbyaml apiVersion rbacauthorizationk siov beta kind RoleBinding metadata name nginxingressrolenisabinding namespace ingressnginx roleRef apiGroup rbacauthorizationk sio kind Role name nginxingressrole subjects kind ServiceAccount name nginxingressserviceaccount namespace ingressnginx nginxcrbyaml apiVersion rbacauthorizationk siov beta kind ClusterRoleBinding metadata name nginxingressclusterrolenisabinding roleRef apiGroup rbacauthorizationk sio kind ClusterRole name nginxingressclusterrole subjects kind ServiceAccount name nginxingressserviceaccount namespace ingressnginx nginxdsyaml apiVersion appsv kind DaemonSet metadata name nginxingresscontroller namespace ingressnginx spec selector matchLabels app ingressnginx template metadata labels app ingressnginx annotations prometheusioport prometheusioscrape true spec affinity nodeAffinity requiredDuringSchedulingIgnoredDuringExecution nodeSelectorTerms matchExpressions key betakubernetesioos operator NotIn values windows hostNetwork true serviceAccountName nginxingressserviceaccount tolerations effect NoExecute operator Exists effect NoSchedule operator Exists containers name nginxingresscontroller image ranchernginxingresscontrollernginx rancher args nginxingresscontroller defaultbackendservicePODNAMESPACEdefaulthttpbackend configmapPODNAMESPACEnginxconfiguration tcpservicesconfigmapPODNAMESPACEtcpservices udpservicesconfigmapPODNAMESPACEudpservices annotationsprefixnginxingresskubernetesio securityContext capabilities drop ALL add NETBINDSERVICE runAsUser env name PODNAME valueFrom fieldRef fieldPath metadataname name PODNAMESPACE valueFrom fieldRef fieldPath metadatanamespace ports name http containerPort name https containerPort livenessProbe failureThreshold httpGet path healthz port scheme HTTP initialDelaySeconds periodSeconds successThreshold timeoutSeconds readinessProbe failureThreshold httpGet path healthz port scheme HTTP periodSeconds successThreshold timeoutSeconds nginxdepyaml apiVersion appsv kind Deployment metadata name defaulthttpbackend labels app defaulthttpbackend namespace ingressnginx spec replicas selector matchLabels app defaulthttpbackend template metadata labels app defaulthttpbackend spec terminationGracePeriodSeconds tolerations effect NoExecute operator Exists effect NoSchedule operator Exists containers name defaulthttpbackend Any image is permissable as long as It serves a page at It serves on a healthz endpoint image ranchernginxingresscontrollerdefaultbackend rancher livenessProbe httpGet path healthz port scheme HTTP initialDelaySeconds timeoutSeconds ports containerPort resources limits cpu m memory Mi requests cpu m memory Mi nginxsvcyaml apiVersion v kind Service metadata name defaulthttpbackend namespace ingressnginx labels app defaulthttpbackend spec ports port targetPort selector app defaulthttpbackend rancheryaml apiVersion helmcattleiov kind HelmChart metadata name rancher namespace kubesystem spec chart rancherlatestrancher repo version targetNamespace cattlesystem set tls external ingresstlssource secret rancherImagePullPolicy Always rancherImageTag masterhead k sClusterSecret randomsecret We should document what we will be supportingmaintaining so this is clearly defined The containerd flag was accidentally added to kubelet and is deprecated but needed for cadvisor to properly connect with the k s containerd socket so adding for now For 