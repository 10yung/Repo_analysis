 I think this is wrong it should be the other way around Consider the case in which throughfrontier TF pending upper UP We should not move pending to merging because its frontier is not dominated by TF and as such we should not allow it to be merged with other batches However the expression evaluates to true since Moreover we should also enter the loop in the case of empty TF as it dominates every other frontier I think the correct check should be selfthroughfrontierisempty selfpending upperiterallt selfthroughfrontieriteranyt t lessequalt Some uses of differential dataflow aim to observe an output at a specific time and then walk away from the dataflow Such a case must currently start a streaming dataflow await the visibility of results and then proactively shut down the input source It seems that an import operator should be able to take instruction about when to drop its capability and cease propagating batches For example one could supply a vector of timestamps with the intended semantics that the output must be correct for any time less or equal an element of the vector Once the traces upper frontier is not less or equal to any element of the vector it can drop its capability Batches can also be filtered unless this breaks everything horribly by looking at the differences between its upper and lower frontiers and seeing if any timestamps are greater or lower than this bound This might be just another manifestation of but submitting anyway in case its not Using the same dataflow graph I insert a single graph edge and two nodes and then remove them at the next epoch in a loop All is fine with one worker but with w I observe that the memory footprint of the program keep growing monotonically The repro is in the dogsleak no pun intended branch of my DD fork Hi This is just a usability hint I am trying to work with custom structs in differential dataflow and I spent some hours figuring out why I cannot invoke count on collections using my struct It turned out I just forgot implement Hashable but the error message during compilation makes it very hard to arrive at that insight error E no method named count found for type differentialitembaseddifferentialdataflowCollectiondifferentialitembasedtimelydataflowscopesChild differentialitembasedtimelyworkerWorkerdifferentialitembasedtimelycommunicationAllocator T differentiallshSample in the current scope srcdifferentiallshrs count method not found in differentialitembaseddifferentialdataflowCollectiondifferentialitembasedtimelydataflowscopesChild differentialitembasedtimelyworkerWorkerdifferentialitembasedtimelycommunicationAllocator T differentiallshSample note the method count exists but the following trait bounds were not satisfied mut differentialitembaseddifferentialdataflowCollectiondifferentialitembasedtimelydataflowscopesChild differentialitembasedtimelyworkerWorkerdifferentialitembasedtimelycommunicationAllocator T differentiallshSample stditerIterator differentialitembaseddifferentialdataflowCollectiondifferentialitembasedtimelydataflowscopesChild differentialitembasedtimelyworkerWorkerdifferentialitembasedtimelycommunicationAllocator T differentiallshSample differentialitembaseddifferentialdataflowoperatorsCount warning unused import selfdifferentialdataflowoperatorsCount srcdifferentiallshrs use selfdifferentialdataflowoperatorsCount Hi I have a question on how DD decides which batches to compact Specifically when using the UnorderedInput interface to insert data into DD consider the following pseudo code and scenarios both using the Pair timestamp rust for i in m if i m let edgecapnext edgecapdelayedi i for j in n edgeinputsessionedgecapclonegiveedge i j diff edgecapdowngradei j if i m edgecap edgecapnext ii When there are only one dimensional timestamps m and n there will no call to edgecapdelayedPair in i above Does that mean DD assumes there will be no data coming in at or higher timestamps and starts compacting the batches as the timestamps move forward Specifically when computing at will DD compact all data from to into a single batch which would mean the computation at will only be accessing diffs at one earlier timestamps instead of all earlier timestamps separately Experimentally by adding the if condition at lines i and ii above which then prevents creating the capability at runtime for wcc for D timestamps m and n get better by X Now consider the case when data is inserted at timestamps that can be organized in a x D grid m and n My question is after statement ii above executes does DD reason that there will be no more inputs at i because we drop that capability and hence can start compacting earlier rows Consider the computation at timestamp does DD compact the rows from to in any way For example DD can potentially compact all data from to which would mean the computation at will only need to compute earlier diffs from earlier batches and Or does DD keep the diff data separate at all timestamps from to In that case the computation at will need to sum the earlier diffs at each of the batches with timestamps less than Or maybe DD does some compaction in between anarrayintoiter currently just works because of the autoref feature which then calls T as IntoIteratorintoiter But in the future arrays will implement IntoIterator too In order to avoid problems in the future the call is replaced by iter which is shorter and more explicit See for more information I wonder if there is a way to attach an inspect operator to a collection after the dataflow graph has been created This would be helpful in situations where I do not know ahead of time which collections I will need to inspect and I do not want to pay even the small price of attaching inspect to all collections ahead of time Thanks I might be missing something but going directly from a toplevel scope into an AltNeu subscope is a bit unergonomic at the moment because enterat assumes a Product timestamp Im havent had the time to check where this specialisation is enforced In any case its not a big deal Im working around this by simply breaking out all of the stuff that enterat is doing under the hood rust scopescopedAltNeuSTimestamp AltNeu subgraph let proposals propose ascollectione Value v eclone inner entersubgraph delaymove datum t AltNeu neu true tclone mapmove data t diff let newtime AltNeu time tclone neu true data newtime diff From quodlibetor When collections have primary keys we should be able to perform keywise actions through a cursor by traversing newtoold batches and looking for the first positive accumulation Once identified this should be the only record under the hypothesis that each prior collection accumulated to something nonnegative This seems like an eminently plausible case and the optimization of avoiding seeks into the large batches is appealing