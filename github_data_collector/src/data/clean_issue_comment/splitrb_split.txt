Describe the bug abtest triggers multiple Redis commands for all experiments for a single experiment abtestexperiment call If in a single request flow I call abtestexperiment abtestexperiment and abtestexperiment it will perform multiple commands for the same experiments overloading Redis To Reproduce Open rediscli monitor then ruby abtestexperiment A single call will perform get hget type lrange etc for all other experiments defined in the experimentsyml Expected behavior Redis calls only for the given experiment Describe the bug The Split tool doesnt proportionally split traffic across utmsource utmmedium and utmcampaign which requires us to normalize the data for traffic and calculate the statistical significance outside of split To Reproduce We see the page visits in Mixpanel and the number of page views to each variant from different utmsource utmmedium and utmcampaign is not the same Expected behavior If the traffic is split then each variant should get an equal number of page views from all the campaigns that bring users to the page Therefore we can be confident of the conversion difference and statistical significance in the Split dashboard Related to Hey folks In our application we have to know when a particular experiments winner was chosen There are already implemented integration point such as ontrial ontrialchoose ontrialcomplete onbeforeexperimentreset onexperimentreset onexperimentdelete I propose to implement a hook like onwinnerchoose If you have a solution which doesnt require the implementation of this hook please let me know If a solution doesnt exist please let me know if you are interested in this functionality or not It looks like I will add it anyway but in prospect it can be part of the library This PR removes sinatra as depency introducing a very light and very coupled routing and rendering solution The needs are small enough that the coupling introduced will probably not be a problem in the future As discussed with andrehjr this PR depends on a refactor for the force session feature and is currently broken Im keeping in WIP state for now One of my experiments have one alternative winning by but it has lower probability of winning Does it make any sense I cant figure out the math behind it There is still no sufficient confidence and I think it will normalize with more participants but I was intrigued by this and wanted to understand it better Does anyone have any idea why this is going on screen shot at With the new GDPR directive coming in on the May it would be helpful to update the README with a section on GDPR and what cookies Split stores along with a description perhaps the structureattributes of what is stored Hi weve been running a few experiments using Split and noticed nearly all of our variants were losing over the control version by pretty hefty margins So we set up a test to see how Split performed when splitting an experiment where the control and the variant were the exact same experience This test found the variant losing to the control by nearly with a confidence interval Any suggestions as to what might be causing this or how we can improve the accuracy of our results This was using the default splitting algorithm Would switching to the block randomization algorithm help us out at all img width altscreen shot at pm src Related to To reproduce Create and save SplitExperiment with some metadata Initialise and save the same experiment but with metadata nil During save experiment version will increment Initialise the same experiment as in step with metadata nil and save experiment During save experiment version will increment Expected In step the version should not be incremented because experiment configuration has not changed after step 