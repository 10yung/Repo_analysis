merge I had a general look at the BERT code If I am not wrong the BERT model in this folder is simply trained from the data itself rather than transferred from a huge corpus Could you help clarify this part a seq seqattentiontrainpy multilabelflagFalse You must feed a value for placeholder tensor decoderinput with dtype int and shape FLAGSmultilabelflagFlase decoderinput Thanks for sharing the model I was interested to run your models with the mentioned data however it was not possible I spent quite a lot time to get the data Getting the Baidu app and the data from it was a nightmare I also tried to preprocess the data as you mentioned but there are many other dependencies It seems that even in preprocessing some intermediatetemporary files are used and these files are only available in the Baidu network I signed up for the Baidu app but it does not recognizes nonchines phone number Tried a lot and gave up Is it possible to host the data somewhere else python When can I see SpanBert here i git clone the project failing how can i clone it error RPC failed curl transfer closed with outstanding read data remaining fatal The remote end hung up unexpectedly fatal early EOF fatal indexpack failed First of all thanks for your effort to make this repo interesting I ran the preprocessing notebook and was able to get some of the files however the other scripts use lot of data files which is not easily accessible I tried lot of time getting the Baidu storage account but couldnt because of oversees phone number I was just wondering if you can share the script that generates those data files you used in your scripts dataieeezhihucupdatah dataieeezhihucupvocablabelpik questiontrainset txt questiontopictrainset txt questionevalset txt