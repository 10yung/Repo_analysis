I am creating an issue on behalf of Apache Airflow community and its about Apache Airflow integration with Dask We are working on Apache Airflow still at least months to release it but we are doing various cleanup tasks One of the things we noticed is that we have a DaskExecutor It uses Dask to run the tasks The executor is very little used Its used so little that we even have all the Dask Executor tests failing for months without anyone noticing We understand that Dask is important for a number of people but also in order to keep it in Airflow we need someone who actually uses Dask and Dask Executor in Airflow to keep it in a healthy state We are querying at our devlist and checking if someone is willing to maintain it but we also think it might be a good idea to ask here We are happy as Apache Airflow committers to review and accept all the related code but we would love someone more familiar and using Dask to maintain it Note that the executor is rather simple and the tests are just a few and not very complex Its literally lines of code in total We could fix it ourselves but maybe having someone who actually uses Dask being just a bit more active in the Apache Airflow community is a good idea import daskdataframe as dd import pandas as pd pdf pdDataFrame ddf ddfrompandaspdf npartitions pdfsum Series dtype float ddfsumcompute Traceback most recent call last File stdin line in module File homeec userlocallibpython sitepackagesdaskdataframecorepy line in sum sum axisaxis skipnaskipna spliteverysplitevery outout File homeec userlocallibpython sitepackagesdaskdataframecorepy line in reductionagg resultdivisions minselfcolumns maxselfcolumns ValueError min arg is an empty sequence pdfcount Series dtype int ddfcountcompute Traceback most recent call last File stdin line in module File homeec userlocallibpython sitepackagesdaskdataframecorepy line in count resultdivisions minselfcolumns maxselfcolumns ValueError min arg is an empty sequence Same story for mean var and sem The issue stems from the following line in daskdataframecorepy resultdivisions minselfcolumns maxselfcolumns This may address and cudf and may be a reasonable alternative to The idea here is to accept a sort argument for groupby operations which can be passed along the final apply phase of applyconcatapply groupby aggregations Currently aggregations triggered by groupbyaggregate are hardcoded to use sortFalse I assume for performance reasons while others us the backends default behavior Ideally the sorting behavior should always depend on the argument added here Notes achieves something similar by exposing sort for aggregations themselves The approach used here seems easier to implementmaintain but I am open to feedback TODO Perhaps we should also set sortFalse for the first apply phase of aggregation for performace reasons TODO The changes in this PR currently address cudf but thorough testing still needs to be added cc beckernick please feel free to advise on downstream needs here Tests added passed Passes black dask flake dask x Tests added passed x Passes black dask flake dask Hello Our pipeline a has a place where we pass in a D array and a D array into daskarraymapblocks Each array has chunks along the leftmost axis as a batch size and the batch sizes are equal For example arr dchunks arr dchunks To Clarify that results in arr dshape arr dchunksize arr dshape arr dchunksize when we called damapblocksfn arr d arr d or viceversa we expected the function to be called once for each batch in this example times However the resulting behavior was it was called times The graph reflects this it has a product type of effect where each combination is called We were eventually able to get the expected behavior by swapping the axis so our arrays look like this arr dchunks arr dchunks Then we have the correct graph and results I can understand the reason for the behavior but for our pipeline it causes a major inconvenience since the left axis for us almost always the batch size So we may pass images in and get lower dimension feature sets That will always sit on the left side This is causing us to make a hacky solution to add dimensions to the lower dimension array so our arrays look like this arr dchunks arr dchunks This works but for our application is quite hacky since we have to do this dynamically to not change a bunch of code Our current solution detects the highest number of dimensions adds extra dimensions then wraps the supplied function to removed the extra dimensions before continuing It seems mapblocks is aligning it arguments axes based on a rightjustified alignment of their axes For our application it makes sense to operate on a leftjustified alignment so our arrays will always line up on the lowestindexed axis so if we pass in a NxWxH array a Nx array and a NxWxHxC array all into a mapblocks call we can do so without extensive reshaping and axis work Couple Questions Is this all correct Am I misunderstanding Is there an easy solution I am missing Is this noted in the documentation somewhere If not lets add it Thanks Fixes modifies mean to handle calculation for datetime series adds test to check result is equivalent to pandas Tests added passed Passes black dask flake dask On the images for Growth of major programming languages and Stack overflow traffic to various packages are no longer rendering correctly testupstream When running this script the following errors represented javascript import cupy as cp import daskarray as da A cpasarraynpones B cpasarraynpones A dafromarrayA chunksAshape B dafromarrayB chunksBshape C A j B Cconjcompute errors TypeError operand types all returned NotImplemented from arrayufuncufunc conjugate call array j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j j ndarray If I use Numpy the errors do not appear How can I use dasklinalgsvd with cupy array Thanks Pandas changed the repr for DataFrameinfo This is causing a couple failures pytest daskdataframeteststestdataframepytestinfo test session starts platform darwin Python pytest py pluggy hypothesis profile default databaseDirectoryBasedExampleDatabaseUserstaugspurgersandboxdaskhypothesisexamples rootdir Userstaugspurgersandboxdask inifile setupcfg plugins xdist hypothesis forked repeat cov collected item daskdataframeteststestdataframepy F FAILURES testinfo def testinfo from io import StringIO pandasformatputlines putlines testframes pdDataFrame x y indexpdInt Indexrange No RangeIndex in dask pdDataFrame for df in testframes ddf ddfrompandasdf npartitions assertinfodf ddf daskdataframeteststestdataframepy df x y ddf Dask DataFrame Structure x y npartitions int int Dask Name frompandas tasks memoryusage True def assertinfodf ddf memoryusageTrue from io import StringIO assert isinstancedf pdDataFrame assert isinstanceddf ddDataFrame bufpd bufda StringIO StringIO dfinfobufbufpd memoryusagememoryusage ddfinfobufbufda verboseTrue memoryusagememoryusage stdoutpd bufpdgetvalue stdoutda bufdagetvalue stdoutda stdoutdareplacestrtypeddf strtypedf TODO assert stdoutpd stdoutda E assert class pand bytes n class pand bytes n E Skipping identical leading characters in diff use v to show E columns E Column NonNull Count Dtype E E x nonnull int E E x nonnull int E E Full output truncated lines hidden use vv to show daskdataframeteststestdataframepy AssertionError slowest test durations s call daskdataframeteststestdataframepytestinfo durations hidden Use vv to show these durations failed in s Also daskdataframeteststestdataframepytestgroupbymultilevelinfo We can either adjust the test or more preferably update our info repr to match pandas new one