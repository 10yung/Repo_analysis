Any possibility to dump the KeywordProcessor object Or are pickle or dill modules enough for the job Thanks In order to use in languages other than English Chinese or Japanese with tokenized it is now possible to select words to be separated by word boundaries instead of nonwordboundary when separating words Hi I would like to request for a simple handy feature that will extract the nearby context of the word extracted Suppose keywordprocessoraddkeywordPython keywordprocessorextractkeywords Respected sir My Name is Akhil and I love doing python programming I am from India completed my masters degree in CSE Akhil Python or My Name is Akhil and I love doing python programming hi i am from belong we have one use case that is we need to store additional meta data along with sibling java name xyz meta count score name abc meta count score so now if sentence is like this i know xyz it should return java meta count score so if this look like generic one can we implement to the flash text library Thanks Regards Krishna for example i have a string todayIgotEmailreport how do i get email keyword from this string if i use strcontainsreportFalseregexTrue this will return this string how can we do it with flashtext Hi This is not an issue Just a question to understand if the package is feasible to solve the issue Im facing There are k words which has unique ids and k sentences If any of these words are found in any of the sentences I have to replace the word with the word and its id The catch is we need to retain the same case as in the sentence For eg Word is Information Security The sentence is information security is also called InfoSec The expected output is information security is also called InfoSec Can this scenario be handled with the help of this package By looping through all the sentences Thanks It would be good to have a count getter to obtain the number of keywords processed by Using the processor to identify the presence of the keywords but still in need to split the source and remove any delimiter doesnt sound efficient to me Looking at the class methods I couldnt get anything and the count method returns the count of occurrences found Hi I m loading million keywords their replacements into flashtext The raw data in a pandas dataframe consumes approx GB RAM profiled with pdDataFramememoryusageTrue True and guppy When I load this data to flashtext the algorithm consumes GB RAM My question is if this in intentionally and why the algorithm uses that much ram for a small dataset Did I do something wrong or is this correct I m also curious if there is a Cloud native way to use this algorithm Having the Data of the algorithm swapped out to a fast inmemory Database to have a stateless algorithm where kubernets pods can be transferedrestarted withou a long waiting period or a huge memory footprint Thanks in advance Best regards Florian