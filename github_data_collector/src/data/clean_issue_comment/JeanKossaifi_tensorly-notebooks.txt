when I run the notebook pytorchbackendcnnaccelerationtensorlyandpytorchipynb I find that the tucker decomposition convolutional layer runs slow The decomposed VGG runs slower than the original VGG why how to evaluate the runtime and calculate the speedup This is based on jcrists notebook from I have added an example using parafac to the end of the notebook I didnt know of any real life example sparse tensors that can be factored easily the nips tensor that Jim used in the notebook seems to have a very large rank so I constructed one using a random sparse factorization If you know of a better example let me know This requires 