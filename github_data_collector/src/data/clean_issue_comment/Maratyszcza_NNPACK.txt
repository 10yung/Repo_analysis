Good day everyone Recently ive noticed that nnpsoftmaxoutput result is buggyimho for some inputs Examples First one stdvectorfloat input stdvectorfloat output nnpsoftmaxoutput inputdata outputdata nullptr examples output will be And it sums to which is not equal to Ground truth values for this input Second This time i take a part of previous input for example first three numbers stdvectorfloat input stdvectorfloat output nnpsoftmaxoutput inputdata outputdata nullptr The output will be And the sum is exactly Please help me to find a reason why i have such behaviour which have strong relation to the size of the input I use libnnpack compiled for x processror with avx Thank you in advance It fails on FreeBSD with this error usrbincc fPIC O pipe fstackprotectorstrong isystem usrlocalinclude fnostrictaliasing O pipe fstackprotectorstrong isystem usrlocalinclude fnostrictaliasing fstackprotectorstrong Lusrlocallib shared Wlsonamelibnnpackso o libnnpackso srcx fma dfourier x pyo srcx fma dfourier x pyo srcx fma dwinograd x x pyo srcx fmablass gemmpyo srcx fmablasc gemmpyo srcx fmablass c gemmpyo srcx fmablasconv x pyo srcx fmablassgemmpyo srcx fmamaxpoolingpyo srcx fmarelupyo srcx fmasoftmaxpyo srcx fmablassdotxfpyo srcx fmablasshdotxfpyo CMakeFilesnnpackdirsrcinitco CMakeFilesnnpackdirsrcconvolutioninferenceco CMakeFilesnnpackdirsrcfullyconnectedinferenceco CMakeFilesnnpackdirsrcpoolingoutputco CMakeFilesnnpackdirsrcreluoutputco CMakeFilesnnpackdirsrcsoftmaxoutputco CMakeFilesnnpackdirsrcfullyconnectedoutputco CMakeFilesnnpackdirsrcreluinputgradientco CMakeFilesnnpackdirsrcconvolutioninputgradientco CMakeFilesnnpackdirsrcconvolutionkernelgradientco CMakeFilesnnpackdirsrcconvolutionoutputco CMakeFilesnnpackdirsrcx fmasoftmaxco Wlrpathwrkdirsusrportssciencennpackworkbuilddepscpuinfo depscpuinfolibcpuinfoso lpthreadpool FAILED libnnpackso usrbincc fPIC O pipe fstackprotectorstrong isystem usrlocalinclude fnostrictaliasing O pipe fstackprotectorstrong isystem usrlocalinclude fnostrictaliasing fstackprotectorstrong Lusrlocallib shared Wlsonamelibnnpackso o libnnpackso srcx fma dfourier x pyo srcx fma dfourier x pyo srcx fma dwinograd x x pyo srcx fmablass gemmpyo srcx fmablasc gemmpyo srcx fmablass c gemmpyo srcx fmablasconv x pyo srcx fmablassgemmpyo srcx fmamaxpoolingpyo srcx fmarelupyo srcx fmasoftmaxpyo srcx fmablassdotxfpyo srcx fmablasshdotxfpyo CMakeFilesnnpackdirsrcinitco CMakeFilesnnpackdirsrcconvolutioninferenceco CMakeFilesnnpackdirsrcfullyconnectedinferenceco CMakeFilesnnpackdirsrcpoolingoutputco CMakeFilesnnpackdirsrcreluoutputco CMakeFilesnnpackdirsrcsoftmaxoutputco CMakeFilesnnpackdirsrcfullyconnectedoutputco CMakeFilesnnpackdirsrcreluinputgradientco CMakeFilesnnpackdirsrcconvolutioninputgradientco CMakeFilesnnpackdirsrcconvolutionkernelgradientco CMakeFilesnnpackdirsrcconvolutionoutputco CMakeFilesnnpackdirsrcx fmasoftmaxco Wlrpathwrkdirsusrportssciencennpackworkbuilddepscpuinfo depscpuinfolibcpuinfoso lpthreadpool usrbinld error srcx fma dfourier x pyo unaligned data cc error linker command failed with exit code use v to see invocation ninja build stopped subcommand failed Looking into it it seems to be a very bad idea to compile the python code into binaries People who write in Python usually believe that Python is good enough for most everything If this isnt true then this code should be rewritten in a more performant language like C or C or Rust Writing in Python first and then discovering that this was a bad idea and trying to accelerate it using some customwritten compiler isnt a good idea IMO because it leads to errors like this Hi I am impressive in the code and trying to integrate some possible optimization I only found the function of FFTIFFT transformation for real to complex and vice verse Have you implemented the related function for complex to complex Or is it possible for a future plan Thank you very much There does not appear to be a way to get the nnpack version from nnpackh The CMakefile of NNPACK is too complex How should I build NNPACK for Linux ARM Like this shell SETTARGETSYSROOT LINUXBUILDROOThostusrarmbuildrootlinuxgnueabiHARDFLOATsysroot SETTARGETCCOMPILER LINUXTOOLCHAINbinarmlinuxgnueabiHARDFLOATgcc SETTARGETCXXCOMPILER LINUXTOOLCHAINbinarmlinuxgnueabiHARDFLOATg SETCMAKECFLAGS rdynamic pipe marm mfpuneon mcpucortexa Wall W Wnounusedparameter Wnosigncompare Wnointconversion DREENTRANT SETCMAKECXXFLAGS rdynamic pipe marm mfpuneon mcpucortexa Wall W Wnounusedparameter Wnosigncompare Wnointconversion DREENTRANT Global cache blocking values and thread scheduling dont map optimally to the new bigLITTLE systems with global task scheduling where all CPU tasks are active at the same time Even after integration with the cpuinfo library the inithwinfo function still uses fixed hardcoded values for ARM CPUs I guess the same cpuinfobased code should be used for all architectures although oddly enough I tried that locally and got no performance improvement on various phones I wrote small snippet of code to check NNPack convolution inference speed For some reason nnpconvolutioninference method returns output with zeros I am not able to figure out the issue with below snippet of code Can you please help me with the issue nnpstatus status nnpinitialize nnpconvolutiontransformstrategy transformstrategy nnpconvolutiontransformstrategyprecompute const nnpconvolutionalgorithm algorithm nnpconvolutionalgorithmauto nnpconvolutionalgorithmimplicitgemm sizet inputchannels sizet outputchannels const nnpsize inputsize const nnpsize outputsize const nnppadding inputpadding const nnpsize kernelsize const nnpsize stride stdvectorfloat inputinputsizewidth inputsizeheight inputchannels stdvectorfloat kernelinputchannels kernelsizewidth kernelsizeheight outputchannels stdvectorfloat biasoutputchannels stdvectorfloat outputoutputchannels inputsizewidth inputsizeheight kernel input bias stdvectoruint t AlignedAllocatoruint t transformedKernel workspacebuffer stdvectorfloat workspacebuffer pthreadpoolt threadpool pthreadpoolcreate sizet workspacesize status nnpconvolutioninference algorithm nnpconvolutiontransformstrategyprecompute inputchannels outputchannels inputsize inputpadding kernelsize stride NULL NULL NULL NULL NULL workspacesize nnpactivationidentity NULL NULL NULL if status nnpstatussuccess stdcout nnp failure status status stdendl return stdcout Workspace buffer size workspacesize stdendl workspacebufferresizeworkspacesize auto begin chronodurationcastchronomillisecondschronosteadyclocknowtimesinceepochcount status nnpconvolutioninference algorithm transformstrategy inputchannels outputchannels inputsize inputpadding kernelsize stride inputdata kerneldata biasdata outputdata nullptr staticcastvoidworkspacebufferdata workspacesize nnpactivationidentity NULL NULL NULL stdcout status stdendl auto end chronodurationcastchronomillisecondschronosteadyclocknowtimesinceepochcount stdcout Use time end begin times n Hi I want to know how nnpack implement the kernels used in convolution inference By reading the codes I find the computation is taken by kernels in srcx For examples the direct convolution is computed by the kernel in srcx blasconv x py which is built on the peachpy frameworkif my understanding is right Most of the codes build on peachypy is straightforward and easy to understand but some of them are hard So Id like to know if there are documents about the peachpy or the kernel implementation details Tanks Im trying to test an optimization tool that repeatedly rewrites a function in a native binary and then tests the rewrite for correctness and performance The structure of the nnpack binaries makes this process extremely complicated and error prone There is no easy way to determine which kernel will be invoked by a particular set of commandline arguments For example if I want to test nnpfft x withoffsetandstreamavx I have to step through the code and figure out which kind of inputs will be directed to that kernel There are some preparatory loops and other setup phases that are not isolated from the timed code so my tool requires special filters to ignore certain kernel invocations The validation option is not available in the xxxxbenchmark binaries So for example suppose I get a great speedup in one of my variations of the kernelnow I am facing a mountain of work to validate that my tool did not accidentally break the function My best solution has been to splice the rewritten function into one of the other binaries that has a validator but this is very time consuming because it requires patching riprelative addresses and there are more complications if my rewrite is larger than the original function At this point it has simply become too difficult to experiment with my tool on nnpack To continue what I will need is a single binary that can do the following Run any kernel in a meaningful context It doesnt matter to me what data or what kind of use case it chooses to run even if someone explained it to me I wouldnt understand I just want it to do whatever it does black box no manual Provide a singleknob commandline argument to adjust the duration of the test So if the default configuration runs for seconds then passing an argument of would make it run for seconds or for seconds Automatically evaluate every invocation for correctness It doesnt matter how and if there are errors it doesnt matter what The domain of nnpack is totally opaque to me so I will simply step through the validator and find what it did not approve in binary executable terms I would be glad to implement this test driver and submit a pull request but its a bit over my head at this point If someone can walk me through the steps in extremely concrete lowlevel terms I can probably figure out the details