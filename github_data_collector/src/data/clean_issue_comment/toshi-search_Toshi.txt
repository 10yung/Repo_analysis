After successfull build of release target Ubuntu while running RUSTBACKTRACE targetreleasetoshi I keep on getting INFO toshi Settings host port path data placeaddr loglevel info writermemory jsonparsingthreads autocommitduration bulkbuffersize mergepolicy ConfigMergePolicy kind log minmergesize Some minlayersize Some levellogsize Some consuladdr clustername kitsune enableclustering true master true nodes Such Relevance Much Index Many Search Wow ERROR toshi Error Failed registering Node InnerInnerError kind Connect cause Os code kind ConnectionRefused message Connection refused thread main panicked at internal error entered unreachable code Shutdown signal channel should not error This is a bug srcbintoshirs stack backtrace stdsysunixbacktracetracingimpunwindbacktrace at srclibstdsysunixbacktracetracinggccsrs stdpanickingdefaulthookclosure at srclibstdsyscommonbacktracers at srclibstdsyscommonbacktracers at srclibstdpanickingrs stdpanickingrustpanicwithhook at srclibstdpanickingrs at srclibstdpanickingrs stdpanickingcontinuepanicfmt at srclibstdpanickingrs stdpanickingbeginpanicfmt at srclibstdpanickingrs toshimainclosure futurestaskimplSpawnTenterclosure toshimain stdrtlangstartclosure main libcstartmain start netstat shows me that isnt in use by another process and running command with sudo doesnt change anything Message clearly state that this is a bug So is there a solution or not What happened Accidentally omitting document content returns Internal Server Error with a body of messageInternal errorurinewindex What was expected Emitting any kind of helpful message would be helpful Also in my experience when the client receives a response there is usually something informative on the serverside But in this case the server emits the same message that the client receives which isnt helpful This bug is actually just the worst offender of a whole class of bugs where if something doesnt go Toshis way it just gives back a raspberry but Id say getting a for an empty document is pretty far up the list for me How to reproduce Assuming you create an index based on the cargo test schema then send in an indexing request of the form console echo curl X PUT d newindex I totally get that refactoring to be agnostic to discovery mechanisms would be a significant time investment On that front Id be happy to contribute the kubernetes part if you decide to go that route With that said its fairly straightforward to use the kubernetes API An HTTP request is made to The response is something like this assuming serde for serialization rust deriveSerialize Deserialize Debug struct Addresses ip String serderename nodeName nodename String serderename targetRef targetref TargetRef deriveSerialize Deserialize Debug struct Items metadata Metadata subsets VecSubsets deriveSerialize Deserialize Debug struct Labels app String deriveSerialize Deserialize Debug struct Metadata serderename selfLink selflink String serderename resourceVersion resourceversion String deriveSerialize Deserialize Debug struct Metadata name String namespace String serderename selfLink selflink String uid String serderename resourceVersion resourceversion String serderename creationTimestamp creationtimestamp String labels Labels deriveSerialize Deserialize Debug struct Ports name String port i protocol String deriveSerialize Deserialize Debug struct K sEndpoint kind String serderename apiVersion apiversion String metadata Metadata items VecItems deriveSerialize Deserialize Debug struct Subsets addresses VecAddresses ports VecPorts deriveSerialize Deserialize Debug struct TargetRef kind String namespace String name String uid String serderename resourceVersion resourceversion String Retrieving the ip addresses is as simple as rust let mut listofnodes Vecnew for item in endpointsitems for subset in itemsubsets for address in subsetaddresses listofnodespushaddressip Per if leader election wanted to be done kubernetes has a unique number tied to each API object called resourceVersion Here each Address has a TargetRef field which will have resourceversion field The leader can be chosen via minmax of the resource version associated with it Kubernetes can also expose the pod name to the container via environment variable so any toshi node can know its kubernetes identifier This is just a tracking issue for additional config items that need to be added Consul client buffer size SIGTERM SIGINT should also be captured to trigger a graceful shutdown