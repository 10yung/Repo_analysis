Here is the table struct sql CREATE TABLE publictorggoods id bigserial NOT NULL goodsid int NOT NULL id orgid int NOT NULL id createdby int NOT NULL id createdtime timestamptz NOT NULL DEFAULT now CONSTRAINT torggoodspk PRIMARY KEY id Here is the model go type creatorInfo struct CreatedTime timeTime jsoncreatedTimeomitempty CreatedBy int jsoncreatedByomitempty Creator UserInfo pgfkcreatedby jsoncreatoromitempty the composited model type OrgGoods struct creatorInfo createdtime and createdby GoodsID int jsongoodsIdomitemptystring sqlnotnull id OrgID int json sqlnotnull id Code string tableName struct pgdiscardunknowncolumns now timeNow orgGoodsOrgID userOrgID orgGoodsCreatedTime now orgGoodsCreatedBy userID userID orgGoodsCode goodscode session newPgConn db sessionOpenpgDB defer sessionClose q dbModelm if lencolumns qColumncolumns err DBAddModelorgGoods code id goodsid orgid createdby createdtime err DBAddModelorgGoods if inserting without columns it works fine so weird function body func dbs dbPg AddModeldb interface m interface columns string error q dbsgetQuerydb m if q nil session newPgConn db sessionOpenpgDB defer sessionClose q dbModelm columns is alternative if lencolumns qColumncolumns err qInsert dbscheckTxErrdb err return err The model has the value assigned for createdtime and createdby definitely but it does not be scaned out and saved into db please see below the error message sql INSERT INTO torggoods code id goodsid orgid createdby createdtime VALUES DEFAULT DEFAULT DEFAULT RETURNING id createdby createdtime nil T debug layerdebugger continuing ROLLBACK nil ERRO ERROR null value in column createdby violates notnull constraint if there is no columns specified when qinsert everything is ok so whats happening when adding the columns so far I am sure it should be a bug version githubcomgopgpgv v thanks Hi there Is there a way to generate foreign key automatically in many many example Im using v with postgres after running the example After that I found that no foreign keys have been created in table ordertoitems d ordertoitems Table testordertoitems Column Type Collation Nullable Default orderid bigint itemid bigint d items Table testitems Column Type Collation Nullable Default id bigint not null nextvalitemsidseqregclass Indexes itemspkey PRIMARY KEY btree id d orders Table testorders Column Type Collation Nullable Default id bigint not null nextvalordersidseqregclass Indexes orderspkey PRIMARY KEY btree id And create table like this models interface Ordernil Itemnil OrderToItemnil for model range models err dbCreateTablemodel ormCreateTableOptions Temp false FKConstraints true if err nil panicerr Hi Im running postgres via RDS though I dont think it matters and when I try to connect with pg I get the following error pg SASL got SCRAMSHA PLUS wanted SCRAMSHA Is this possibly a known issue Thanks edit putting in my notes for anyone else who finds this later SCRAMSHA PLUS is used when the postgres server requires TLS to connect to it as I have configured my RDS instance to do Since gopg doesnt support this method yet the current options are to update postgres to allow plaintext connections stop requiring TLS or downgrade the roles password to MD instead of SCRAM I have opted for the latter solution This can itself be done in one of two ways Update your postgres authentication settings and set the inappropriately named passwordencryption parameter to md if youre using AWS RDS this is controlled via a parameter group Then update the roles password via psql postgres alter role myrole with password password or directly update the roles password To allow dumpingrestoring backups postgres lets you assign a raw md scram value to the roles password column The md format postgres uses is the string md followed by the md hash of the password concatenated with the roles name So for a role myrole and password password you can run echo n passwordmyrole md sum awk print md md a bff b feb d ac b Then set this as the password value via psql postgres alter role myrole with password md a bff b feb d ac b Hope this helps someone It allows to specify a schema name for particular model Ie you can bravely list all columns in your database using informationschema tables with such model go type Column struct tableName struct pgcolumnsschemainformationschema DBTableName string pgtablename ColumnName string pgcolumnname Should close Is there any way to shoehorn tracing with ocsql If not do you consider implementing it Currently to make a query from table located in nondefault or currently selected schema I use this hack go type queryResult struct tableName struct sqlinformationschemacolumnsaliascolumns Name string sqlname which generates a correct query I think it will be better to have posibility to set schema name using a tag in tableName like sqlcolumnsschemainformationschema and in query builder like dbModelSchemaExprschema Ive added logger zerologEvent into my context and pass it arround for logging things I wanted to include my query in context but I noticed that BeforeAfterQuery get an empty context contextBackground I tried changing my db calls to use WithContextctx but the hooks still got an empty context From gomod githubcomgopgpgv v I have defined Filter struct for WhereStruct and used the required tag as described type Filter struct Status int pgrequired filter newFilter dbModel booksWhereStructfilter But it seems that this tag doesnt work if Status is The field is missed in SQL query Link to definition Here is my attempt to make recursive with ASAP May be useful for someone A while back we tried the following change optsReadTimeout timeSecond max time for SELECT optsWriteTimeout timeSecond max time for INSERT UPDATE or DELETE We would then get occasional errors along the lines of pg cant find columnfoo in modelbar Our theory is that timed out queries can return their outputs to other queries This was an issue back when we were on v and I just managed to reproduce it on v by setting timeouts of milliseconds Got it times in an hour of testing merely by cranking up the number of workers in a sandbox environment As far as I can tell the way to reproduce this is to have a high volume of queries which are not all the same and some of which time out and some of which dont Please let me know if you need any more information I would very much like to have query timeouts so this is a big blocker