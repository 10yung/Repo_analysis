when I run python mainpy ctdet expid food batchsize lr e gpus a problem occurs RuntimeError The size of tensor a must match the size of tensor b at nonsingleton dimension which is related to CenterNetsrclibmodelsnetworksposedladcnpy line in forward out residual In focal loss paper the author mentioned that the cls bias is computed as follows for cls log PIPI and PI in that paper is set to Im very interesting that if is computed as the above formula it presents that the confidence of a point in feat map which is a positive sample is Is it calculated according to the above formula or is it some other way Does this mean that you think that for centernet each point is more likely to be a positive sample Many thanks for your answer xingyizhou have u train PE from strach but not finetune on OD I did this and get a better result i want to knowwhy u train PE finetuned on OD I am Always getting square bounding boxes I am working on a different dataset For training I used resnet as the backbone Well heatmaps are goodexact but bounding boxes are always predicted almost square for all products What may be the issue After reading and doing experiments on your CenterNet I think CenterNet is quite different from other anchorbased methods can we refer it as real anchorfree So the network produced C heatmap for object centers of each class then for wh of the object and also for offset it seems like ground truth wh is in shape of maxobj similarly theres also a densewh which seems to be in the right shape However its not quite clear which one is the right one to use wh npzerosselfmaxobjs dtypenpfloat densewh npzeros outputh outputw dtypenpfloat in the ctdet trainer it has option to use densewh but also catspecwh and wh if optwhweight if optdensewh maskweight batch densewhmask sum e whloss selfcritwhoutput wh batch densewhmask batch densewh batch densewhmask maskweight optnumstacks elif optcatspecwh whloss selfcritwh output wh batch catspecmask batch ind batch catspecwh optnumstacks else whloss selfcritreg output wh batch regmask batch ind batch wh optnumstacks In your paper it didnt say how to put bboxes wh into the this Does it also follow d gaussian or shall we just set wh for the exact cell of the box center Thanks First of all thanks again for sharing the code I got a couple more questions while reading it though Please bear with for my ignorance Why do we need for the bias of the last conv layer here is kpmodule keypoint module and what do these stand for exkp exdet ctdet ddd Found answer for the above one here Thanks Hi Xingyi Thanks for contributing such great code As we can see from the initial implementation of Hourglass they use a preactivation version of residual module and put BN and ReLU before Conv layer And in your version it seems going back to the vanilla version where Conv layer followed by ReLU and BN Besides your Hourglass residual module has one less conv layer compared with the original implementation Could you share some thoughts around this change Thanks offsetmap nHW n offset offsetmap target map Hello when I changed dla to dla the training was very normalBut when I tried to inferenceI found the result is absolutely wrong Have you ever met