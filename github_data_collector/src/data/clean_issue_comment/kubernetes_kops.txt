View the pr and in this document docsreleases NOTESmd The author of the double pr becames invalid just look at these two pr you can find the real author I noticed this while investigating and there have been other reports of issues when trying to import kops with go modules I cant find the GH issues at the moment Go modules require the v prefix and kk also tags with the v prefix We have some inconsistent tags already for we have tags of both and v which is the most recent tag with the prefix This is also why is the default version imported by go get go get v k siokops go downloading k siokops v and the latest version in go list go list m versions k siokops k siokops v alpha v v v v v v v Im proposing we switch to only tagging with the v prefix Im only updating the actual git tag and not the entire version string used throughout kops due to its larger impact Output by kops version Public URLs for kops assets Protokube tag Im hoping this is the least invasive way we can make this change If we think advanced notice is required we could tag with both formats for a number of releases before tagging only with the v prefix hold for Justins approval cc justinsb We have many references to godocorg in our documentation It is most useful for viewing the Kops API ClusterSpec InstanceGroupSpec etc Recently golang announced a new godev website that provides similar functionality Once benefit of godev is that it supports versioning so users can view the Kops API for specific Kops versions Unfortunately I dont see any more recent versions than v on the site This sounds very similar to the bug where GO MODULESon go get k siokops downloads v We should get that fixed and then update our docs to reference godev Once kopssigsk sio supports versioned docs we could have them link to the same version on godev as well Describe IN DETAIL the featurebehaviorchange you would like to see Currently when enabling audit logs sourceoriginal IP is always set to LB one According to preventing original IP address should be possible by using L LB or NLB Feel free to provide a design supporting your feature request Loadbalancer type in front of kubeapiserver should be NLB instead CLB AWS should use L instead L loadbalancer fixes upstream Tested in usgovwest zone with the following kops create cluster imageAMINAME dnsprivate zonesusgovwest a nameCLUSTERNAME kops update cluster CLUSTERNAME yes I dnsgo Private DNS skipping DNS validation I executorgo Tasks done total can run I vfscastorego Issuing new certificate apiserveraggregatorca I vfscastorego Issuing new certificate etcdpeerscamain I vfscastorego Issuing new certificate ca I vfscastorego Issuing new certificate etcdmanagercamain I vfscastorego Issuing new certificate etcdmanagercaevents I vfscastorego Issuing new certificate etcdpeerscaevents I vfscastorego Issuing new certificate etcdclientsca I executorgo Tasks done total can run I vfscastorego Issuing new certificate apiserveraggregator I vfscastorego Issuing new certificate kubecontrollermanager I vfscastorego Issuing new certificate kubescheduler I vfscastorego Issuing new certificate kubecfg I vfscastorego Issuing new certificate kops I vfscastorego Issuing new certificate apiserverproxyclient I vfscastorego Issuing new certificate master I vfscastorego Issuing new certificate kubelet I vfscastorego Issuing new certificate kubeproxy I vfscastorego Issuing new certificate kubeletapi I executorgo Tasks done total can run I launchconfigurationgo waiting for IAM instance profile nodesmissioncontrol to be ready I launchconfigurationgo waiting for IAM instance profile mastersmissioncontrol to be ready I executorgo Tasks done total can run I executorgo Tasks done total can run I dnsgo Precreating DNS records I updateclustergo Exporting kubecfg for cluster W createkubecfggo Did not find API endpoint for gossip hostname may not be able to reach cluster kops has set your kubectl context to missioncontrol Cluster is starting It should be ready in a few minutes Suggestions validate cluster kops validate cluster list nodes kubectl get nodes showlabels ssh to the master ssh i sshidrsa adminapimissioncontrol the admin user is specific to Debian If not using Debian please use the appropriate user based on your OS read about installing addons at What kops version are you running The command kops version will display this information Version alpha gitea c What Kubernetes version are you running kubectl version will print the version if a cluster is running or provide the Kubernetes version specified as a kops flag What cloud provider are you using openstack What commands did you run What is the simplest way to reproduce this issue Install cluster without floating network and using existing networks with predefined dns servers What happened after the commands executed T Z I executorgo Tasks done total can run T Z I executorgo Executing task Subnetmyexistingsubnet openstacktasksSubnet ID ba f b d d e ecb a NamemyexistingsubnetNetworkID b fce bNamemyexistingsubnetnameLifecycleSyncTagmyclusternameCIDR DNSServers TagmyclusternameLifecycleSync T Z I changesgo Field changed DNSServers actual xc a f xc a f xc a f xc a f expected T Z I changesgo Field changed Tag actual expectedmyclustername T Z W executorgo error running task Subnetmyexistingsubnet s remaining to succeed Field cannot be changed DNSServers What did you expect to happen cluster to be deployed Please provide your cluster manifest Execute kops get name myexamplecom o yaml to display your cluster manifest You may want to remove your cluster name and other sensitive information apiVersion kopsk siov alpha kind Cluster metadata name myclustername spec kubeControllerManager horizontalPodAutoscalerUseRestClients true terminatedPodGCThreshold featureGates TTLAfterFinished true api loadBalancer type Private authorization rbac channel stable cloudConfig openstack blockStorage bsversion v ignorevolumeaz false loadbalancer subnetID ba f b d d e ecb a method ROUNDROBIN provider octavia useOctavia true monitor delay m maxRetries timeout s cloudProvider openstack configBase swiftmyclusternamemyclustername docker logDriver jsonfile logOpt maxsize m maxfile etcdClusters cpuRequest m etcdMembers instanceGroup masternova name volumeType ceph instanceGroup masternova name volumeType ceph instanceGroup masternova name volumeType ceph memoryRequest Mi name main cpuRequest m etcdMembers instanceGroup masternova name volumeType ceph instanceGroup masternova name volumeType ceph instanceGroup masternova name volumeType ceph memoryRequest Mi name events fileAssets iam allowContainerRegistry true legacy false kubeScheduler featureGates TTLAfterFinished true kubeProxy featureGates TTLAfterFinished true kubeAPIServer featureGates TTLAfterFinished true admissionControl NodeRestriction AlwaysPullImages NamespaceLifecycle PodSecurityPolicy ServiceAccount DefaultTolerationSeconds LimitRanger DefaultStorageClass PersistentVolumeLabel ResourceQuota DenyEscalatingExec auditLogPath varlogkubeapiserverauditlog auditLogMaxAge auditLogMaxBackups auditLogMaxSize auditPolicyFile srvkubernetesassetsaudityaml authenticationTokenWebhookConfigFile srvkuberneteswebhookguardconfig logLevel kubelet anonymousAuth false kubernetesApiAccess kubernetesVersion masterKubelet anonymousAuth false authenticationTokenWebhook true authorizationMode Webhook readOnlyPort kubeletCgroups podruntimeslicepodruntimekubeletslice kubelet anonymousAuth false authenticationTokenWebhook true authorizationMode Webhook readOnlyPort kubeletCgroups podruntimeslicepodruntimekubeletslice kubeReserved cpu m memory Gi kubeReservedCgroup podruntimeslice enforceNodeAllocatable podskubereserved masterPublicName apimyclustername networkCIDR networkID b fce b networking canal nonMasqueradeCIDR sshAccess subnets cidr id ba f b d d e ecb a name nova type Private zone nova topology dns type Public masters public nodes public Please run the commands with most verbose logging by adding the v flag Paste the logs into this report or in a gist and provide the gist link here Anything else do we need to know Hi Im deploying a cluster into openstack All the networking exists and preconfigured by network team Setup assumes there is no floating network Cluster config is shown above Eventually my deployment job failed because of mismatch between existing DNS servers assigned to existing network and kops configuration From what Ive found looks like its impossible to define DNS servers for existing subnet Subnet configuration DNS servers cannot be assigned here because I dont use router Even if I define routerdnsServers I cannot use this option because in that case it assumes usage of floating network without any variants Please correct me if Im wrong in my assumptions or suggest a way to deploy cluster without floating network What kops version are you running The command kops version will display this information Version git b What Kubernetes version are you running kubectl version will print the version if a cluster is running or provide the Kubernetes version specified as a kops flag What cloud provider are you using AWS Govcloud What commands did you run What is the simplest way to reproduce this issue export CLUSTERNAMEmissioncontrol export AMINAME k s debianstretchamd hvmebs kops create cluster imageAMINAME dnsprivate zonesusgovwest a nameCLUSTERNAME What happened after the commands executed W executorgo error running task IAMRolePolicymastersmissioncontrol m s remaining to succeed error creatingupdating IAMRolePolicy MalformedPolicyDocument Partition aws is not valid for resource arnawsroute hostedzonexxxxxxxxxxxxxx status code request id f a c f e e c d What did you expect to happen The cluster is created Please provide your cluster manifest Execute kops get name myexamplecom o yaml to display your cluster manifest You may want to remove your cluster name and other sensitive information apiVersion kopsk siov alpha kind Cluster metadata creationTimestamp T Z name missioncontrol spec api dns authorization rbac channel stable cloudProvider aws configBase s missioncontrolstatestoremissioncontrol etcdClusters cpuRequest m etcdMembers instanceGroup masterusgovwest a name a memoryRequest Mi name main cpuRequest m etcdMembers instanceGroup masterusgovwest a name a memoryRequest Mi name events iam allowContainerRegistry true legacy false kubelet anonymousAuth false kubernetesApiAccess kubernetesVersion masterPublicName apimissioncontrol networkCIDR networking kubenet nonMasqueradeCIDR sshAccess subnets cidr name usgovwest a type Public zone usgovwest a topology dns type Private masters public nodes public apiVersion kopsk siov alpha kind InstanceGroup metadata creationTimestamp T Z labels kopsk siocluster missioncontrol name masterusgovwest a spec image k s debianstretchamd hvmebs machineType m medium maxSize minSize nodeLabels kopsk sioinstancegroup masterusgovwest a role Master subnets usgovwest a apiVersion kopsk siov alpha kind InstanceGroup metadata creationTimestamp T Z labels kopsk siocluster missioncontrol name nodes spec image k s debianstretchamd hvmebs machineType t medium maxSize minSize nodeLabels kopsk sioinstancegroup nodes role Node subnets usgovwest a Please run the commands with most verbose logging by adding the v flag Paste the logs into this report or in a gist and provide the gist link here Looks like this is the relevant JSON for the policy failure Effect Allow Action route ChangeResourceRecordSets route ListResourceRecordSets route GetHostedZone Resource arnawsroute hostedzonexxxxxxxxxxxxxx Effect Allow Action route GetChange Resource arnawsroute change Effect Allow Action route ListHostedZones Resource Effect Allow Action ecrGetAuthorizationToken ecrBatchCheckLayerAvailability ecrGetDownloadUrlForLayer ecrGetRepositoryPolicy ecrDescribeRepositories ecrListImages ecrBatchGetImage Resource Anything else do we need to know This looks similar to The file doesnt exists Brand new cluster is failing with the following logs m OK m Started LSB Start NTP daemon cloudinit Generating locales this might take a while cloudinit enUSUTF done cloudinit Generation complete cloudinit Cloudinit v running modulesconfig at Fri Jan Up seconds m OK m Started Apply the settings specified in cloudconfig bridge filtering via arpipip tables is no longer available by default Update your scripts to load brnetfilter if you need this Bridge firewalling registered nfconntrack version buckets max Initializing XFRM netlink socket Netfilter messages via NETLINK v ctnetlink v registering with nfnetlink IPv ADDRCONFNETDEVUP docker link is not ready m OK m Started Docker Application Container Engine m OK m Reached target MultiUser System m OK m Reached target Graphical Interface Starting Execute cloud userfinal scripts Starting Update UTMP about System Runlevel Changes m OK m Started Update UTMP about System Runlevel Changes cloudinit nodeup node config starting cloudinit Downloading nodeup cloudinit Total Received Xferd Average Speed Time Time Time Current cloudinit Dload Upload Total Spent Left Speed Debian GNULinux ip ttyS ip login cloudinit T cloudinit Warning Transient problem timeout Will retry in seconds retries left cloudinit T cloudinit Warning Transient problem timeout Will retry in seconds retries left cloudinit T cloudinit Warning Transient problem timeout Will retry in seconds retries left cloudinit T cloudinit Warning Transient problem timeout Will retry in seconds retries left cloudinit T cloudinit Warning Transient problem timeout Will retry in seconds retries left cloudinit T cloudinit Warning Transient problem timeout Will retry in seconds retries left cloudinit T cloudinit curl Connection timed out after milliseconds cloudinit Failed to curl Retrying cloudinit Total Received Xferd Average Speed Time Time Time Current cloudinit Dload Upload Total Spent Left Speed cloudinit T cloudinit Warning Transient problem timeout Will retry in seconds retries left cloudinit T cloudinit Warning Transient problem timeout Will retry in seconds retries left cloudinit T cloudinit Warning Transient problem timeout Will retry in seconds retries left cloudinit T cloudinit Warning Transient problem timeout Will retry in seconds retries left cloudinit T cloudinit Warning Transient problem timeout Will retry in seconds retries left includes gophercloudgophercloud 