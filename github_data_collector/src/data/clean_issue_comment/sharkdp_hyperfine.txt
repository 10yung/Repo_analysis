If I want to benchmark my program with one variable command argument I can use parameterlist On the other hand if I want to benchmark four five six variables it can get tedious It would be nice if Id be able to run hyperfine arg a a a parameterlist a parameterlist a abc parameterlist a QWE Recently there has been some changes made in hyperfine that affect how and when different commands are run To have a common understanding I think it is a good idea to summarize the current state Also as mentioned in there are cases when running preparation command only once per benchmarked command is desirable In analysis below I show it is not so hard to do and does not break the current flow of execution Current model Hyperfine gives us the possibility to run commands multiple times Lets analyze the command line and name some workflowrelated concepts hyperfine OPTIONS command command a single tested command equal to one of commands or when using parameters generated from one of command templates by applying one of parameter values measured run a single measured execution of one of commands affecting options are minruns maxruns runs warmup run nonmeasured execution of a command used eg to remove the bias related to running commands for the first time Affecting options warmup preparation an initializing code run before each measured and warmup run of a single command The preparation can be the same for all commands or specific to each of commands Affecting options prepare cleanup a cleanup code run once after all warmup and measured runs of a single command The cleanup is specified the same for all commands but is subject to parameter substitution Affecting options cleanup parameters a name and a list of values either explicit one or generated used to easily create a number of commands one for each value from a list There can be only one list of values If multiple commands are specified each of them is treated as a template for each value from the the parameters list Affecting options parameterscan parameterstepsize parameterlist since version previously preparation and cleanup was not run before warmups see To illustrate the concepts running such a code hyperfine showoutput warmup runs parameterscan n prepare echo prepA n prepare echo prepB n prepare echo prepA n prepare echo prepB n cleanup echo clean n echo cmdA n echo cmdB n we receive such an output ignoring all hyperfine output beautifying with newlines between different commandsparameters and marking what is really benchmarked with prepA cmdA prepA cmdA prepA cmdA prepA cmdA clean prepB cmdB prepB cmdB prepB cmdB prepB cmdB clean prepA cmdA prepA cmdA prepA cmdA prepA cmdA clean prepB cmdB prepB cmdB prepB cmdB prepB cmdB clean Analysis and proposed workflow Lets call each segment separated with newlines in the above output as a batch A batch corresponds to running one command generated from one of the command templates given on the command line with possibly var substituted for one parameter value The batch would be ideally run with all preparations and cleanup I think we can agree that warmup runs and measured runs should be exactly the same So we do not differentiate the prepare commands for those runs in a single batch The comprehensive workflow for a single batch could look like this batchpreparecmd warmup phase prepare cmd prepare cmd repeat warmup times end of warmup phase warmupscleanupcmd measured phase prepare cmd prepare cmd repeat required number of times end of measured phase batchcleanupcmd The proposed mapping of above commands to existing functionality batchpreparecmd missing need to be added prepare one of prepare with var substitution applied already implemented batchcleanupcmd cleanup with var substitution applied already implemented So looks like we are only missing a command to prepare a batch Let me know if I miss some important use case in the above workflow Additionally it would be nice to have multiple cleanups as we have multiple prepares WIP Hi Cool utility nathanchance is using it to benchmark some of our toolchains I was wondering if hyperfine does anything related to disabling clock frequency scaling On Android if we dont pin the cores clocks to fixed frequency the noise in the benchmark frequently makes the results unreliable sh set e getcpus echo n adb shell ls sysdevicessystemcpu grep cpu setuserspacefreq echo setting us shellcommandecho userspace sysdevicessystemcpu cpufreqscalinggovernor echo n adb shell shellcommand getmaxfreq echo looking for shellcommandcat sysdevicessystemcpu cpufreqcpuinfomaxfreq echo n adb shell shellcommand setfreq shellcommandecho sysdevicessystemcpu cpufreqscalingsetspeed echo n adb shell shellcommand verify shellcommandcat sysdevicessystemcpu cpufreqcpuinfocurfreq actualfreqadb shell shellcommand if actualfreq then echo good else echo bad fi echo finding number of cpus for cpu in getcpus do echo cpu echo setting cpu frequency scaling governor to userspace control setuserspacefreq cpu echo finding max freqency of cpu freqgetmaxfreq cpu echo setting cpu to freq setfreq cpu freq verify cpu freq done Is what I usually do Im not sure if the same sysfs nodes are exposed on x Another thing I do when benchmarking is clear the page cache rather than do warmup runs sh echo procsysvmdropcaches I couldnt quickly tell if these are done by hyperfine but maybe they might help As shown in a paper a while back the size of the environment variables can significantly affect the performance of a program due to knockon effects on the memory layout It might be helpful for accuracy to randomize the layout a bit on each run by inserting a value with a random length of say bytes into the environment