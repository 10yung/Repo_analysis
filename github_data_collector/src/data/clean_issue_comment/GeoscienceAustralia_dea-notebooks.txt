 Proposed changes Added a new notebook based on an old notebook that allows a user to select an area from an interactive map and generate a geomedian filmstrip plot that highlights change through time This uses dask to improve performance this may work on the NCI but for now Ive marked it as a Sandboxonly notebook For coastal applications users can specify a specific portion of the tidal range to generate geomedians from eg low tide high tide filmstripexample Checklist replace with x to check off x Notebook created using the DEAnotebooks template x Remove any unused Python packages from Load packages x Remove any unusedempty code cells x Remove any guidance cells eg General advice x Ensure that all code cells follow the PEP standard for code The jupyterlabcodeformatter tool can be used to format code cells to a consistent style select each code cell then click Edit and then one of the Apply X Formatter options YAPF or Black are recommended x Include relevant tags in the final notebook cell refer to the DEA Tags Index and reuse tags if possible x Clear all outputs run notebook from start to finish and save the notebook in the state where all cells have been sequentially evaluated x Test notebook on both the NCI and DEA Sandbox flag if not working as part of PR and ask for help to solve if needed x If applicable update the Notebook currently compatible with the NCIDEA Sandbox environment only line below the notebook title to reflect the environments the notebook is compatible with Currently the mapshapefile function plots each feature in a shapefile with an incrementing colour It would be far more useful if this function plotted features based on values from a field in the shapefile attribute table The Geomediancompositesipynb notebook currently fails with the following error ImportError Traceback most recent call last ipythoninput a cd c de in module import datacube from datacubestatsstatistics import GeoMedian import sys ImportError cannot import name GeoMedian This needs to be updated once either hdmedians or hdstatsodcalgo are added to the Sandbox Adding speckle filtering greatly enhances the appearance of the shipping lane notebook outputs The current example runs slowly however If we can find a way to increase performance it would be a great addition to the notebook Proposed changes This is an example notebook to demonstrate how to use the deaclassificationtools for machine learning landcover classification It attempts to be a simple example to get started open to feedback regarding complexity etc Checklist X Notebook created using the DEAnotebooks template X Remove any unused Python packages from Load packages X Remove any unusedempty code cells X Remove any guidance cells eg General advice X Ensure that all code cells follow the PEP standard for code The jupyterlabcodeformatter tool can be used to format code cells to a consistent style select each code cell then click Edit and then one of the Apply X Formatter options YAPF or Black are recommended X Include relevant tags in the final notebook cell refer to the DEA Tags Index and reuse tags if possible X Clear all outputs run notebook from start to finish and save the notebook in the state where all cells have been sequentially evaluated X Test notebook on both the NCI and DEA Sandbox flag if not working as part of PR and ask for help to solve if needed X If applicable update the Notebook currently compatible with the NCIDEA Sandbox environment only line below the notebook title to reflect the environments the notebook is compatible with The gettrainingdataforshp function from deaclassificationtoolspy currently extracts remote sensing data for a specified product for each shapefile feature that contains a class label Multiple products are helpful for effective land cover classification Current workflows run the function multiple times and concatenate the results into a single array It would be easier for users to pass a list of products eg ls nbartgeomedianannual ls nbarttmadannual Notebook for erialCP and BexDunn Currently loadard uses daskchunkstime to force data to lazy load before scenes are filtered out based on the proportion of validnon cloudy data in each timestep To override this users can set the functions daskchunks parameter However if a custom daskchunks is also passed in via a query this has no effect on the analysis Additionally even if custom daskchunks is passed in via a query unless the loadard kward lazyload is set to True data will still compute This combination of daskrelated function kwargs and query kwargs is confusing eg below and needs to be streamlined Seems like it would be a good idea to write a notebook as part of the refresh to demonstrate the functions in the deaclassificationtools module otherwise users may not be aware that the ML functionality exists Note checklist below only relevant if committing to the develop branch Proposed changes Add new script deaclassificationtoolspy to interface between xarray DataArray and Dataset and sklearn models including KMeans PCA decision trees etc This script is written to be as general as possible Functionality is exposed to the user to turn labelled DataArrays and Datasets into flat unlabelled numpy arrays of pixels and rebuild a DataArray with spatiotemporal structure from a flat numpy array This means the user can utilise a scikitlearn model of their choice and we dont need to write bespoke functions for each model This script also contains functions to rasterise polygons of training data to assist with the application of sklearns supervised classification models to spatial datasets Finally the script contains a few maybe just one handy extensions to sklearns models such as a hierarchical KMeans model of the type used for the prototype radar wetlands insight tool Checklist replace with x to check off Notebook created using the DEAnotebooks template Remove any unused Python packages from Load packages Remove any unusedempty code cells Remove any guidance cells eg General advice Ensure that all code cells follow the PEP standard for code The jupyterlabcodeformatter tool can be used to format code cells to a consistent style select each code cell then click Edit and then one of the Apply X Formatter options YAPF or Black are recommended Include relevant tags in the final notebook cell refer to the DEA Tags Index and reuse tags if possible Clear all outputs run notebook from start to finish and save the notebook in the state where all cells have been sequentially evaluated Test notebook on both the NCI and DEA Sandbox flag if not working as part of PR and ask for help to solve if needed If applicable update the Notebook currently compatible with the NCIDEA Sandbox environment only line below the notebook title to reflect the environments the notebook is compatible with 