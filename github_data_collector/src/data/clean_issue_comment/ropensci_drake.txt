 Prework x Read and abide by drakes code of conduct x Search for duplicates among the existing issues both open and closed Proposal Certainly not a new idea but I think we are now ready to try We just need to send individual target data files to the cloud eg Amazon S The rest of the storr cache can stay local The big data can live on the cloud and the swarm of tiny metadata files can still live locally That way local drake caches are highly portable and shareable and we can more easily trust the big data not to break when we move around caches What makes this idea possible now Why not earlier Because of specialized data formats Due to the mechanics of the implementation drake can bypass the storr for big files while letting storr keep taking care of the small files That means we should be able to shuffle around big data when it counts while avoiding unnecessary network transactions eg would send metadata to the cloud as well which would severely slow down data processing API I am thinking about a new argument to make and drakeconfig r makeplan storage amazons and targetspecific configurable storage r plan drakeplan smalldata getsmalldata not worth uploading to the cloud largedata target getlargedata storage amazons 