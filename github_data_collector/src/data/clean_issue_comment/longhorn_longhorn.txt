Im wondering how installing Longhorn from Rancher Apps will be affected in a Project or Namespace with set Resource Quotas since it requires deployments to have resource limits set Will Longhorn handle this correctly Longhorn ends up spiking the CPU very high when the backend becomes unavailable when it tries to ls the volume up to dozens and dozens of times Creating a cascading pile up of problems when nothing else can use the CPU anymore The best idea we can think of is how TCP handles retries when it backs off double the duration before retrying Steps to reproduce Create a pod using block volume using cat EOF kubectl apply f apiVersion v kind PersistentVolumeClaim metadata name csiblockvolumetestpvc spec accessModes ReadWriteOnce resources requests storage Gi volumeMode Block storageClassName longhorn apiVersion v kind Pod metadata name csiblockvolumetest spec containers image busybox imagePullPolicy IfNotPresent name sleep args binsh c while truedo datesleep done volumeMounts volumeDevices name longhornblk devicePath devlonghornlonghorntestblk volumes name longhornblk persistentVolumeClaim claimName csiblockvolumetestpvc EOF Check pod events using kubectl describe pod csiblockvolumetest Warning FailedMapVolume s x over s kubelet meldafrawilonghorn MapVolumeEvalHostSymlinks failed for volume pvcaeaef c ea ae fc bb lstat varlibkubeletpluginskubernetesiocsivolumeDevicespublishpvcaeaef c ea ae fc bb no such file or directory csiattacher logs I connectiongo GRPC call csiv ControllerControllerPublishVolume I connectiongo GRPC request nodeidmeldafrawilinodelonghorn volumecapabilityAccessTypeBlockaccessmodemode volumecontextfromBackupnumberOfReplicas staleReplicaTimeout storagekubernetesiocsiProvisionerIdentity driverlonghorniovolumeidpvc dca bfb c e bc I leaderelectiongo successfully renewed lease longhornsystemexternalattacherleaderdriverlonghornio I connectiongo GRPC response I connectiongo GRPC error nil After adding a new path to a node the page shows a blank page To show the UI again you have to refresh the page Ill attach the browser logs next time it happens this seems to happen when i try to delete a pv or pvc after tearing down a deployment without reboot those tgtd processes never go away rancherOS The total number of snapshotbackup schedules retain number shouldnt exceed otherwise it can cause issues like Clicking a replica name should take you to the Volume it is a part of Volume errors in UI could help troubleshoot what caused it to fault Does longhorn support simultaneous reading and writing of pods deployed on different nodes Like cephfs Hi one of my volumes is stuck in the deleting state for days The volume is released but it is now in a failed state for the last remaining node Also it is assigned to an instance manager which does not exist anymore deleting Smashing the delete button does not help I also rebooted the assigned node no changes Dropdown options are Backups Delete Create PVPVC