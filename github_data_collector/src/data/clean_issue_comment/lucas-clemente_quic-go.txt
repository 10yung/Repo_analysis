 Fixes which was deleted by Github Depends on Hey there Ive been working on a HTTP to HTTP proxy to transport allow NGINX to connect to a HTTP upstream I have been testing this by connecting to both Cloudflarecom using HTTP and the Quiche based NGINX server patches After a while few hundred requests and sometimes even immediately I start to run into STREAMSTATEERROR errors And the client wont be able to recover on its own without creating a new round tripper instance This is when connecting to the same host This is on quicgo version v When it breaks while connecting to the NGINX based server it will for example still work to Cloudflarecom so I assume something got stuck in the connection state and the client isnt cleaning up because of the error A code sample essentially boils down to this go package main import githubcomlucasclementequicgohttp log nethttp func main hclient httpClient Transport http RoundTripper httpHandleFunc funcw httpResponseWriter request httpRequest requestRequestURI requestURLHost requestHost requestURLScheme https response err hclientDorequest if err nil httpErrorw httpStatusTexthttpStatusInternalServerError errError httpStatusInternalServerError return if err responseWritew err nil logPrintfWrite error v err httpErrorw httpStatusTexthttpStatusInternalServerError errError httpStatusInternalServerError return logFatalhttpListenAndServe nil Fixes As shown by its not enough to just detect that RTT packets were sent we also need to somehow detect that they were actually processed Otherwise we cant distinguish between a successful and a reject RTT connection any plan H to support trailer Its important for file transfer streaming over H to present final status This seems to be caused by the internal send buffer of the UDP socket becoming full causing Linux kernel to return EAGAIN for writes The golang netpoller handles this by waiting until the socket is writable source This eventually causes Session to freeze on SendQueueSend Stream write buffer like TCPConn SetWriteBuffer It turns out that the Linux kernels UDP receive buffer is too small by default around k In order to support higher bandwidths we need a significantly higher value It would be good if we could set this on a perconnection basis Not sure if Go allows us to do this