Bumps jacksondatabind from to details summaryCommitssummary See full diff in compare view details br Dependabot compatibility score Dependabot will resolve any conflicts with this PR as long as you dont alter it yourself You can also trigger a rebase manually by commenting dependabot rebase dependabotautomergestart dependabotautomergeend details summaryDependabot commands and optionssummary br You can trigger Dependabot actions by commenting on this PR dependabot rebase will rebase this PR dependabot recreate will recreate this PR overwriting any edits that have been made to it dependabot merge will merge this PR after your CI passes on it dependabot squash and merge will squash and merge this PR after your CI passes on it dependabot cancel merge will cancel a previously requested merge and block automerging dependabot reopen will reopen this PR if it is closed dependabot ignore this patchminormajor version will close this PR and stop Dependabot creating any more for this minormajor version unless you reopen the PR or upgrade to it yourself dependabot ignore this dependency will close this PR and stop Dependabot creating any more for this dependency unless you reopen the PR or upgrade to it yourself dependabot use these labels will set the current labels as the default for future PRs for this repo and language dependabot use these reviewers will set the current reviewers as the default for future PRs for this repo and language dependabot use these assignees will set the current assignees as the default for future PRs for this repo and language dependabot use this milestone will set the current milestone as the default for future PRs for this repo and language You can disable automated security fix PRs for this repo from the Security Alerts page details Hi Do you still use this project If not what do you use instead Thanks myLoad gets a null clusterloadMap if we havent yet connected to the cluster which throws a NullPointerException when we try to toString it for the debug logging I went ahead and made myLoad overly robust to null things just so that it doesnt have to be concerned with which Cluster fields can be null and which cant This is a simple commit in isolation but I branched it off of the commits in since it now lives in a different file Let me know if youd rather I branch it independently I dont have hard evidence for this but I think the following is possible Receive SyncConnected for sessionid a Call onConnect zk session expires here sessionid was a Call joinCluster zkgetgetSessionId establishes new session with sessionid b and returns b namenodesnodeID created with connectionID b lifetime bound to sessionid b Complete joinCluster onConnect SyncConnected Receive SyncConnected for sessionid b Call onConnect Call previousZKSessionStillActive previousZKSessionStillActive uses session b to read namenodesnodeID finds connectionID b returns true onConnect returns early skipping cluster setup And somewhere in there an Expired event for sessionid a shuts down the cluster Im not sure if it matters when leaving the cluster in a shutdown state indefinitely because onConnect for sessionid b was tricked into skipping cluster setup After some not all expired zk sessions Im experiencing hangs when ordasity tries to set its ZooKeeperMaps back up Ive created a very basic repro here with more info in the README Code Example log In ClusterconnectionWatcher a KeeperStateDisconnected doesnt trigger forceShutdown which is responsible for calling listenershutdownWork and listeneronLeave My guess as to why this is is that its waiting for a KeeperStateExpired which does trigger forceShutdown and is otherwise identical modulo logging Based on my recent experience and the zk session statetransition docs this allows the following behavior Client partitions from zk cluster Clientside zk session timeout triggers client receives KeeperStateDisconnected Clients stays partitioned from network for arbitrarily long but ordasity continues running its previouslyclaimed work Client eventually rejoins network and regains route to zk cluster Client attempts to reconnect to the zk cluster cluster says no client receives KeeperStateExpired ordasity finally shuts down work Client establishes new session ordasity claims new work This is harmful in my application since I want work ownership to be besteffort exclusive ie nodes should minimize their overlap in work Is this a bug or was ordasity intentionally designed to maximize this kind of overlap when nodes partition I can imagine that being a useful or at least not harmful behavior in some settings If so maybe I can elaborate this proposed change to include a config option to maximize vs minimize oblivious work overlap If the user supplies their own zk client to clusterjoin or clusterconnect and their zk client has already established a session then the cluster will register a watch for a SyncConnected event but never receive one since it has already fired long ago before the cluster started listening This bug was introduced in my earlier which was a simple change that quietly violated the assumption that the clusters zk client would trigger a SyncConnected after clusterjoin This fix depends on and includes I havent actually caught any bugs with this yet but Ive been seeing some intermittent problems around expired zk sessions that Im hoping to diagnose Im coming to suspect that its due to bugs in the twitter zk watcher logic And more generally threads that die silently freak me out Note that this logging commit depends on and includes the gauged balancing policy commit from A small generalization to allow clients to stuff their own metadata into workunit znodes Jsonparse Map String String workUnitData Jsonparse Map String Any workUnitData 