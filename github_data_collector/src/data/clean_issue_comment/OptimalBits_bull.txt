 Description I experiencing problems while trying to gracefully shutdown my queues My code works or should work a bit like this I have multiple queues with named jobs I have one or more processes per queue one process for each named job obviously I have some logic that listens for interrupt signals so that I know when to try and gracefully shut down my queues I loop through my queues and run close on each of the queues I want every processes to finish their current job if they have any and then close the queue to prevent any new jobs from being processed Once all currently processing jobs have finished I want the Node process to exit The problem Im experiencing is that whenever I run close on a queue the queue will close even if theres a job currently being processed However this only happens when I have multiple processes for a queue Which is necessary since Im using named jobs Minimal Working Test code to reproduce the issue Heres a repo with some code that reproduces this behaviour and hopefully explains my problem better Whenever the second process is removed or commented out the queue will wait for the current job to end processing and then close as expected An easy to reproduce test case will dramatically decrease the resolution time javascript const testQueue new Queuetest testQueueprocessjobA async job consolelogProcess started for Job jobid return new Promiseresolve setTimeout consolelogProcess done for Job jobid resolve seconds Comment this process out to see the difference testQueueprocessjobB async Add a job to the queue testQueueaddjobA processonSIGINT async consolelogClosing Queue now await testQueueclose consolelogQueue closed processexit Run this code and press Ctrl C to exit the process within seconds Notice how the queue closes before reaching the end of the process Bull version Additional information At one point while testing this there was two jobs being processed at the same time and then the queue actually did close as expected Even though there was two processes defined which kind of contradicts my problem Unfortunately I cant figure out how to reproduce that scenario Description Im noticing that without any options out of the box that Im seeing what looks to be a solid second delay before my bull worker process acts on the job in the queue Bull startup first job second delay not sure where this is coming from queue active bull based redis connections client subscriber queues sending email queue completed Jobs thereafter add job queue waiting queue active sending email queue completed queue drained Server started and running for many minutes bull debug queue active queue bull job bull opts bull attempts bull delay bull timestamp bull backoff undefined bull name default bull data template verify message Object locals Object bull progress bull delay bull timestamp bull stacktrace bull returnvalue null bull attemptsMade bull id bull processedOn bull failedReason undefined bull debug creating bull client type client bull debug creating bull client type subscriber bull debug creating bull client type client bull debug creating bull client type subscriber bull debug creating bull client type client bull debug creating bull client type subscriber bull debug redis connection established description bull debug redis connection established description bull debug redis connection established description bull debug redis connection established description bull debug redis connection established description bull debug redis connection established description bull debug queue drained queue bull debug queue completed queue bull job bull opts bull attempts bull delay bull timestamp bull backoff undefined bull name default bull data template verify message Object locals Object bull progress bull delay bull timestamp bull stacktrace bull returnvalue bull attemptsMade bull id bull processedOn bull failedReason undefined bull finishedOn Im having a tough time debugging this lingering startup issue I also see from here that sometimes jobs just get stuck and never go into active state until the server is restarted Bull version Additional information You may report several types of issues Bug reports enhancements or questions For bug reports however you are required to provice some information so that the issue can be resolved efficiently The following template should be filled for bugs Before submitting the bug just think twice if you really need to submit the bug or you may have some issue in your own code remember that handling issues is time consuming would you better like that we spend time improving the library or on nonissues Description Hi I was trying to figure out how to perform a graceful shutdown The documentation states that when executing queuepausetrue a promise is returned that resolves when the queue is paused It will not process any new jobs and active jobs will continue until they complete It relies on QueuewhenCurrentJobsFinished which returns a promise that resolves when all jobs currently being processed by a given worker have finished Looking into QueuewhenCurrentJobsFinished it appears that it does not wait for all active jobs to finish but rather wait for a single job to complete Returns a promise that resolves when active jobs are finished returns Promise QueueprototypewhenCurrentJobsFinished function if thisbclientInitialized bclient not yet initialized so no jobs to wait for return Promiseresolve Force reconnection of blocking connection to abort blocking redis call immediately const forcedReconnection redisClientDisconnectthisbclientthen return thisbclientconnect return Promiseall thisprocessing then return forcedReconnection Why not wait for all jobs in thisprocessing to finish before resolving I am currently using a concurrency value of meaning that you would need to wait for at most promises to resolve return Promiseallthisprocessingthen return forcedReconnection This change would enable me to perform a perfect shutdown Looking forward to hearing your thoughts Hi Sorry for asking here im new in Queue Bull and Nodejs I want to add second before next queue execute but it seem immediatly executed i think when i add delay option it can make delay between next queue How i can achive that Here my code My bull repeatable job queue has stopped working twice in last two month and I checked the document found this maxStalledCount I can set But how do we manually test how this works Is there any method can stall a job is there any way to use the jobupdate inside a processor The lazy client error event handler is never unregistered when the queue is closed This becomes more problematic in a scenario in which optionscreateClient is specified You may report several types of issues Bug reports enhancements or questions For bug reports however you are required to provice some information so that the issue can be resolved efficiently The following template should be filled for bugs Before submitting the bug just think twice if you really need to submit the bug or you may have some issue in your own code remember that handling issues is time consuming would you better like that we spend time improving the library or on nonissues Description in reference page said jobprogress allows a number between and but actually it also allows passing object and other things You may report several types of issues Bug reports enhancements or questions For bug reports however you are required to provice some information so that the issue can be resolved efficiently The following template should be filled for bugs Before submitting the bug just think twice if you really need to submit the bug or you may have some issue in your own code remember that handling issues is time consuming would you better like that we spend time improving the library or on nonissues Description If options are passed as the third argument to the queue constructor but the second argument is undefined the options are ignored My use case is wanting to provide the redis URL via environment variable in production but connect to local redis in development Minimal Working Test code to reproduce the issue const queue new Queuetask processenvBULLREDISURL prefix mybullqueue Bull version I have no idea why this happens but it does When the Redis server gets rebooted and your Queue loses its connection it doesnt reconnect We have discussed this before you believe its an issue with IORedis But I am not sure now after spending several hours playing with this I can consistently repeat the issue now Some observations The only method that throws an exception is queueadd All other methods seem to work fine reconnect to Redis fine and return data fine queueclienping returns PONG fine but queueadd fails with an exception I created the following rather ugly horribly ugly code for my healthcheck as a temp workaround until I can find the root cause of this issue const job await thisqueueServicequeueaddnull removeOnComplete true delay await jobremove This code will ALWAYS throw an exception in this scenario so its a pretty good check to see Environment Kubernetes cluster I have tried the following environments and it seems to happen on each with different errors Single Redis Instance Same behavior but you get an ECCONREFUSED error from IORedis Sentinel Redis with instances masterslave if you kill all of them simultaneously you get ALL SENTINELS are down error from IORedis Both appear to behave exactly the same If you reboot your app everything works again 