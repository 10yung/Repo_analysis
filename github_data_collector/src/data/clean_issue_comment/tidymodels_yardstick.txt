 r librarytidymodels Registered S method overwritten by xts method from aszooxts zoo Attaching packages tidymodels broom recipes dials rsample dplyr tibble ggplot tune infer workflows parsnip yardstick purrr Conflicts tidymodelsconflicts x purrrdiscard masks scalesdiscard x dplyrfilter masks statsfilter x dplyrlag masks statslag x ggplot margin masks dialsmargin x recipesstep masks statsstep x recipesyjtrans masks scalesyjtrans datatwoclassexample twoclassexample dplyrfiltertruth Class rocauctruth Class Error in rocdefaultresponse predictor auc TRUE No control observation supCreated on by the reprex package v sup It would be better to return NA with a warning The plot needs predictions along the y axis and truth along the x axis img width altScreen Shot at PM src img width altScreen Shot at PM src I think that for cmheat this isnt too bad We can swap out for these lines r dplyrmutatePrediction factorPrediction levels revlevelsPrediction ggplot ggplotggplot aesTruth Prediction fill Freq For cmmosaic I think this gets us close but the labels dont look like they are placed quite right r ggplot ggplotfulldata ggplot geomrectggplot aesxmin xmin xmax xmax ymin ymin ymax ymax ggplot scalexreverse breaks xdataxmin xdataxmax labels ticklabels ggplot scaleyreverse breaks y dataymin y dataymax labels ticklabels ggplot labsx Predicted y Truth ggplot themepanelbackground ggplot elementblank ggplot coordflip This collision with readrspec could be improved if other spec showed where spec was found classmetrics yardstickmetricsetaccuracy ppv sens spec Error The combination of metric functions must be only numeric metrics a mix of class metrics and class probability metrics The following metric function types are being mixed class accuracy ppv sens other spec Call rlanglasterror to see a backtrace becomes classmetrics yardstickmetricsetaccuracy ppv sens spec Error The combination of metric functions must be only numeric metrics a mix of class metrics and class probability metrics The following metric function types are being mixed class accuracy ppv sens other spec from environment namespacereadr Call rlanglasterror to see a backtrace We can use rlangfnenv and rlangenvname to get a decent name r rlangenvnamerlangfnenvfunctionx x global r libraryyardstick For binary classification the first factor level is assumed to be the event Set the global option yardstickeventfirst to FALSE to change this libraryreadr Attaching package readr The following object is masked from packageyardstick spec libraryrlang envnamefnenvspec namespacereadr For spec and sens This would help avoid a collision with readrspec that looks like classmetrics yardstickmetricsetaccuracy ppv sens spec Error The combination of metric functions must be only numeric metrics a mix of class metrics and class probability metrics The following metric function types are being mixed class accuracy ppv sens other spec Call rlanglasterror to see a backtrace instead of geomline and use the option direction vh Ill likely add more tests as I dive in but I got the sklearn comparison set up The print method for metricset can probably be improved by unclassing it and removing all of the attributes before printing so it just prints like a function The most important thing youd do when printing it out is check the function signature so its a bit confusing that it also currently prints out all the attributes which includes the metrics attribute holding all of the individual functions in the metric set Why is the AUC result from yardstick different from that of pROC and the HandTill packages Something to do with the pattern of predicted probabilities It also seems like yardstick cannot handle the case when there is one true class with predicted classes r x structurec L L L L L L L L L L L L L L L L L L Label ca b c d class factor probs structurec Dim c L L Dimnames list NULL ca b c d auchandtill function x probs mcapconstruct suppressWarningsHandTill multcapresponse x predicted asmatrixprobs HandTill aucmcapconstruct auchandtillx probs pROCmulticlassrocx probs auc Multiclass area under the curve yardstickrocaucvecx probs Single case for level b tablex x a b c d whichx b Taking out b auchandtillx probs pROCmulticlassrocx probs auc Warning in multiclassrocmultivariateresponse predictor levels percent No observation for response levels b Warning in multiclassrocmultivariateresponse predictor levels percent The following classes were not found in response b Multiclass area under the curve yardstickrocaucvecx probs Error in rocdefaultresponse predictor auc TRUE No control observation supCreated on by the reprex package v sup I saw an interesting presentation where they optimized the lift statistic but for a fixed percentage of samples that are tested So for the example data if someone used gainpointtwoclassexample pct truth Class or whatever we call it they would get back an estimate of r librarytidymodels Registered S methods overwritten by ggplot method from quosures rlang cquosures rlang printquosures rlang Registered S method overwritten by xts method from aszooxts zoo Attaching packages tidymodels broom purrr dials recipes dplyr rsample ggplot tibble infer yardstick parsnip Conflicts tidymodelsconflicts purrrdiscard masks scalesdiscard dplyrfilter masks statsfilter dplyrlag masks statslag recipesstep masks statsstep gaincurvetwoclassexample truth Class slice A tibble x n nevents percenttested percentfound dbl dbl dbl dbl supCreated on by the reprex package v sup We can interpolate between points if we dont have the exact percentage liftpoint would return the ratio of the observed percent over the baseline rate eg prevalence Consider the following example libraryyardstick librarytidyverse df tibble truth FALSE est TRUE mutateallfactor levelscTRUE FALSE df A tibble x truth est fct fct FALSE TRUE df ppvtruth est A tibble x metric estimator estimate chr chr dbl ppv binary NA The ppv comes out as NA when it should clearly be The problem is that a formula involving specificity and sensitivity is used in the code to calculate ppv In this example the sensitivity is NA which corrupts the calculation The formula that is used is useful in settings with biased casecontrol sampling but otherwise should not be used If no prevalence is provided the calculation should be something like meantruth estTRUE TRUE 