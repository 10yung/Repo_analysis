Hi Im trying to replicate code in lme photonsleepzip in using Apache Toree notebook directly accessing PhotonML API Spark version with scala and latest Photon version jar I get error when trying to save random effects from model in the following step val dxre mixedModeltoMapgetrandget match case m RandomEffectModel val reModelRDD mmodelsRDD val retup reModelRDDmapx x x coefficientsmeanstoArrayzipWithIndex val reflat retupflatMap case store arr arrmapcell store cell val coeffNames randeffectscolumnsslice rfcols val reflatcol reflatmapstoretup storetup coeffNamesapplystoretup storetup val dxre sparkcreateDataFramereflatcolselectExpr as store as column as coeff dxre dxreshow Message console error value modelsRDD in class RandomEffectModel cannot be accessed in comlinkedinphotonmlmodelRandomEffectModel Access to protected value modelsRDD not permitted because enclosing class iw is not a subclass of class RandomEffectModel in package model where target is defined val reModelRDD mmodelsRDD Is this something related to my environment or changes in Photon ML API If related to Photon ML how can I seesave random effects without using ModelProcessingUtilssaveGameModelToHDFS Problem with ModelProcessingUtilssaveGameModelToHDFS is that it requires inputIndexMaps that is generated by AvroReader that Id like to avoid as my data is in csv thanks Hello I wonder if there are any plans to implement GLMix in tensorflow thanks Changes are Added unit tests in PriorDistributionTestscala Fixed bugs in L regularization gradient and Hessian computation The modelmetadatajson file is not saved in GCS after a full GAME run Here is the code that runs it This prevents from doing warmstart training as the prior runs file is needed Please fix Thanks Fix bug previously AvroDataReader would only repartition when IndexMapLoaders provided Removed integration test assertions that depended on strict ordering of data in DataFrame Added integration test to check that explicit repartition is called this is currently the implementation of ratio modeling for feature selection of random effect in photon I follow the algorithm described in the original publication but some twists are made according to discussion with yiming and alex x Unit tests all pass x Integration tests all pass The algorithm in realityhighly related with codebase instead of only mathematical expression is as follows pass in the featureStatisticSummary identify the binomial columns compute the lowerbound for binomial columns based on the t value select the feature based on only the following lowerbound criterionnonbinomial and intercept columns are kept automatically if t Tl Tu if Tl D select feature As a WIP commit there are things to polish in near future since we currently focus on the feasibility of this experimental method and try to minimize userside changes x unit tests not fully covering all scenarios of feature selection Currently the binomial cases are not selected we need to craft some data that covering all cases x binomial feature column identification predicate needs to be stronger Current solution is inherently flawed we need more computation at feature summary stage to ensure this one hyperparameter interface design for convenience purposes the user side interface for pass in normal distribution quartile and lowerbound threshold hyperparameter redesign x the relationship with pearson correlation feature selection We need another parameter to decide on the algorithm of feature selection or mix them in later stage x crafted test data need some change currently some unneeded feature summary entries are not carefully addressed further experiment report and benchmark report after regression tests x the way we currently keep nonbinary and intercept columns is not good for further feature ranking report planned need redesign joshvfleming ashelkovnykov When optimization tracking is enabled random effects currently track and summarize the convergence reason or lack thereof for each ID The IDs that failed to converge should be logged and output at the end of training to aid tracking bugs Its also important to question what causes models to fail to converge can we detect it before training through data validation alone While comparing sparkml and scikit we realized that Game is using a sum in the loss function computation but that is not desirable for scaling and numerical stability We should change our loss function calculations to use a mean instead Today intercept terms in GameEstimator are handled differently than they are in sparkml Predictorfit so that the same DataFrame cannot be passed as is to both to produce the same result In Game we need to add an intercept feature column if we want to get the same model out of fit as sparkml T It would be more idiomatic with Spark for us to remove all ownership of RDDs and Dataframes from the core PhotonML code transforming it into a pipeline of transforms on a RDDDataframe loaded by the client and provided to the API