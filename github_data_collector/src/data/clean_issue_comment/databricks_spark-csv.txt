I append headerOnFirstFile option If this option enabled it will generate header only according to If this option disabled sparkcsv will generate header per file Defaut is false Added the comments for csv file paths On line the code actually defaults to false for the header option This agrees with the documentation in the project README Currently running the codes below scala val schema StructType StructFieldbool BooleanType true StructFieldnullcol IntegerType true StructFieldnullcol IntegerType true Nil new CsvParser withSchemaschema withUseHeadertrue withParserLibparserLib withParseModeParseModesPERMISSIVEMODE csvFilesqlContext boolFile selectbool nullcol collect with the data below csv bool True False true throws an exception as below javalangNumberFormatException null at javalangIntegerparseIntIntegerjava at javalangIntegerparseIntIntegerjava at scalacollectionimmutableStringLikeclasstoIntStringLikescala at scalacollectionimmutableStringOpstoIntStringOpsscala at comdatabrickssparkcsvutilTypeCastcastToTypeCastscala at comdatabrickssparkcsvCsvRelationanonfunbuildScan applyCsvRelationscala at comdatabrickssparkcsvCsvRelationanonfunbuildScan applyCsvRelationscala This is related with Please refer the description there This is the change that allows an option to render errors when parsing such as number format exceptions as nulls It was in this pull request but I thought it would be cleaner to create a new one Adds Relation LineReader and BulkReader traits to avoid duplicated code Largely derived from and This is in response to the following PR created by blrnw being closed without a merge A fixedwidth parser is a very common use case that I think several users would enjoy using We plan to use this in our current production environment I dont know Scala at all so theres almost certainly cleaner ways my apologies The logging at the moment is sometimes unhelpful as its hard to see the real issue with DROPMALFORMED you see the line with another parsing mode you get the error but not the line For the context and discussion on this please refer to 