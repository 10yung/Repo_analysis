 Add coauthor instiution landing page Make titles point of slide action titles Slide Make title RF is best classifier Slide Whats rotating More dramatic figure Slide Define oblique or use different word or illustrate oblique Slide Separate into two slides to empahasize Frank Make table with columns ObliqueAxisaligned SparseDense and Robust Show prior art here Show full table at end Slide Remove words and just illustrate synthetic datasets Remove mention of benchmark data Illustrate sparse parity with XOR plot Slide SHOW splits in two dimensions for RF and FRC Sentence for each why it succeeds and why it fails Explain what we want what a good split is Slide Separate classification performance and training time into two slides Slide Too much Choose one transformation Make Frank black and thick Others grey and thin Slide One colorbar with no numbers Only three xaxis ticks Only choose Raw Affine and Corrupted Slide Next Steps Mention RerF Slide Acknowledgements Suppose the theoretically optimal split direction at an arbitrary node is defined by a vector v Assume p is large and v has a sparsity of s For RerF and RRRF let the selected split direction be hatvrerf and hatvrrrf respectively Compute the probability that the angle between hatvrerf and v and hatvrrrf and v is less than delta My intuition says that when sparsity of v is very close to one RerF has a reasonable probability of finding a split direction close to the optimal while RRRF does not When sparsity of v is close to zero RerF has no chance of getting close because of the sparsity constraint on the split directions and RRRF has a finite but very small chance of getting close because its high dimensional Even if RRRF does get close to the right split direction estimation of the location of the split will be poor if p n Therefore when p n RerF will do at least as well and sometimes substantially better than RRRF because of the bet on sparsity principle The optionextent of subsampling at large nodes could potentially substantially reduce the amount of sorting while not hurting classification performance Id like to use randomer forests on windows but for the necessary mex files such as classregtreeRCcritval are only provided in Mac and Linux mexmaci and mexa format Its understandable that you may not want to share your source code for someone to compile the mexw files themselves but is there any way you could compile the necessary routines for Windows Appreciate your code if i recall RerF is better than RF on about of the data lets compute for each dataset n p pn sum of singular values sum of squared singular values lets make a pairsplot x panels color code by much better than RF eg or so and not and see if we can see anything in the PAMI paper we really should try to answer the questions which featuressubspaces were informative why does RerF RF in terms of bias and variance what properties of data do we expect RerF RF in biau a they point out in Fig that certain greedily grown trees fail to converge here but breimans would converge can we think of an example where brieman would NOT be consistent their proof suggests such distributions exist we should think of one and show that RerF converges even though RF does not at least empirically another variant we could explore in this manuscript in particular using Random Features for LargeScale Kernel Machines to generate our random A matrix either pre or post submission but lots of people ask about it immediately after asking about performance i think we could simply count the number of times a feature is used andor the number of times a subspace is used