 Background While testing the PR I got Error deleting CloudWatch Log Metric Filter ResourceNotFoundException errors in the end of python managepy destroy operation Rerun python managepy destroy will be successful but it is not ideal Open this issue to track the bug fix progress Description Steps to Reproduce python managepy destroy Desired Change I have solution that may fix this issue It is to add a dependson block to two awscloudwatchlogmetricfilter resources to prevent the log groups deleted before the metrics Will work on this and test the idea when get time Basically add dependson varloggroupname to And add dependson awscloudwatchloggroupathenaname to Background Whilst running numerous deployments i ran into issues with the terraform elementconcat methods within the modules Description Whilst running numerous deployments i ran into issues with the terraform elementconcat methods within the modules Steps to Reproduce Currently trying to reproduce this error Desired Change All terraform modules which contain the elementconcatthingATTR become elementconcatthingATTR to ensure element can always find position Example without Example with Background Im using the release branch and I am getting a conflict between Terraform resources I do not know if the same thing happens on master Every time I run managepy build the configuration switches between two states I get the same result with terraform apply so the issue is not due to the python code changing the Terraform files Description The output from Terraform is modulecloudwatchmonitoringprodawscloudwatchmetricalarmstreamalertlambdathrottles will be updated inplace resource awscloudwatchmetricalarm streamalertlambdathrottles actionsenabled true alarmactions arnawssnseunorth srstreamalertmonitoring okactions arnawssnseunorth srstreamalertmonitoring period tags Cluster prod null Name StreamAlert If I perform the actions then rerun the command I get moduleclassifierprodlambdaawscloudwatchmetricalarmlambdathrottles will be updated inplace resource awscloudwatchmetricalarm lambdathrottles actionsenabled true alarmactions arnawssnseunorth srstreamalertmonitoring okactions arnawssnseunorth srstreamalertmonitoring period tags Cluster prod Name StreamAlert So the state keeps switching between these two settings The same thing with same changes to alarmactions okactions period and tags happens with modulecloudwatchmonitoringprodawscloudwatchmetricalarmstreamalertlambdainvocationerrors will be updated inplace resource awscloudwatchmetricalarm streamalertlambdainvocationerrors So far Ive tracked it down to the configuration in two files terraformmodulestflambdacloudwatchtf and terraformmodulestfmonitoringmaintf Both of them define resources that have the names above The default period of the one in tflambda is and the one in tfmonitoring is that is not changed in my configuration files As a workaround I tried to set cloudwatchmonitoringlambdaalarmsenabled in the cluster configuration to false then I ran managepy build then switched to true again That resulted in that I now have resources that are switching back and forth with every build moduleclassifierprodlambdaawscloudwatchmetricalarmlambdainvocationerrors will be updated inplace resource awscloudwatchmetricalarm lambdainvocationerrors moduleclassifierprodlambdaawscloudwatchmetricalarmlambdathrottles will be updated inplace resource awscloudwatchmetricalarm lambdathrottles modulecloudwatchmonitoringprodawscloudwatchmetricalarmstreamalertlambdainvocationerrors will be updated inplace resource awscloudwatchmetricalarm streamalertlambdainvocationerrors modulecloudwatchmonitoringprodawscloudwatchmetricalarmstreamalertlambdathrottles will be updated inplace resource awscloudwatchmetricalarm streamalertlambdathrottles Steps to Reproduce The cluster config if needed for reproducing this is id prod datasources kinesis srprodstreamalert sellingrange modules cloudwatchlogsdestination crossaccountids regions apsoutheast euwest uswest enabled true kinesis streams retention shards kinesisevents batchsize enabled true cloudwatchmonitoring enabled true kinesisalarmsenabled true lambdaalarmsenabled true settings lambdainvocationerrorthreshold lambdathrottleerrorthreshold kinesisiteratorageerrorthreshold kinesiswritethroughputexceededthreshold streamalert classifierconfig enablecustommetrics true loglevel debug logretentiondays memory timeout region eunorth Desired Change A stable Terraform state or if it is due to a configuration problem a warningerror should be displayed from managepy build to ryandeivert cc airbnbstreamalertmaintainers related to resolves Background Invocation of managepy init fails with an error when attempting to create the alerts table as the database does not yet exist Changes The alerts table is created after all other infrastructure Testing Unit tests all pass A basic initialization and deployment of a minimal streamalert instance was performed and the aws console was used to verify that the alerts table was created Background Currently there is a command to configure the prefix which replaces a preset value in a handful of places This can be prone to issues if one of these is missed Desired Change Store the prefix in only one place Every other place that the prefix is used like here should be programmatically interpolated instead of being hardcoded We should still support using a custom unprefix userprovided value like the kms key alias for example if the user desires to chunyonglin cc airbnbstreamalertmaintainers related to resolves Background Adding support for awsses this is a desired output and one i think others would like Changes Added awsses to docs Implemented the SESOutput Class Added tests Added additional permissions to the alertprocessor Testing ran testsscriptstestthedocssh and verified additional info was ther ran testsscriptspylintsh to verify code score and remediated any issues ran testsscriptsunittestssh to verify tests worked Tried to deploy the terraform for release this work is based off that branch and ran into some issues which i am highlighting on slack and github tldr I would like the ability to send alerts via AWS SES Simple Email Service Background Description Whilst slackteams acts as the IM version of alerting having the ability to send alerts via SES would be great for those not constantly on IM but have the ability to check emails Previously i used a lambda written in python to act as an output which sent the emails as i didnt want to write anything in python I would like to add the ability natively to SA Also we should consider the ability to send HTML structured alerts instead of text based alerts via email Desired Change Permissions AWS sesSendRawEmail Docs Code use email package to contruct the raw mime email use boto sesclientsendrawemailto send the email to chunyonglin cc airbnbstreamalertmaintainers related to resolves Background We use Microsoft Teams instead of Slack so I wanted to have the ability to use SA and Teams together Changes Implemented TeamsOutput very very similar to the SlackOutput Added Tests Updated documentation for outputsrst Testing Code I implemented the output and tested it against an Incoming Webhook on our Teams Instance wrote tests in a style similar to the SlackOutput tests Ran tests and now have Code Coverage for this new addition Ran pylint against files Documentation ran make html and visually checked addition in browser Background NOTE Before filing this issue please consider the following Have you tried pinging us on Slack Are you on the latest version of StreamAlert yes this is a feature i would like for Description Where i work we use Microsoft Teams for IM Therefor i would like the ability to integrate Microsoft Teams as an output for Streamalert I have a branch with some work commited currently but i wanted to see if the community would like this feature Current work jack outputteams Desired Change The ability to use Microsoft Teams teams as an output for alerts within StreamAlert High level overview of the desired change or outcome Alerts can be sent to teams Background I would like to configure one output on a rule but have it send to different destinations based on something inside the alert Eg account a slack channel a account b slack channel b NOTE Before filing this issue please consider the following Have you tried pinging us on Slack Are you on the latest version of StreamAlert Description I would like to write one rule and have that route the alert to an output based on information within the alert The current way i am thinking of implementing this is to use the context field and have a placeholder output such as slackplaceholder The rule will use a lookup table new feature in release to use the accountid on the record and find out which team owns that account i currently work with alot of AWS Accounts Use this team name as the descriptor i plan to add each team as an output to store the secrets relevant for that output in the s bucket and carry on the output This would be amazing as using matchers is relevant most of the time but not in a case when you want a rule to trigger regardless of the account but only notify the team that owns it I dont like the idea of notifying people who dont need to be notified 