This PR includes eightysteeles changes from but rebased and with a few little fixes for recent updates It also replaces brushfirefinatra with brushfirefinagle which does more or less the same thing but without the dependency on the EOLed Finatra which wont be published for Since this is just an example project that wasnt compiling anyway I figured a pretty major change wouldnt be a problem We could also just delete the dead code but it seems like a nice thing to have Ive added an irissrv example to the README since it works again Id like to aggregate the module in the build so that we know when it breaks but because the latest Finagle version isnt built for this causes problems for and we could use sbtdoge to fix this but in my experience its a pain and in this case probably isnt worth the effort More generally it seems like itd be a good idea to pull all of the example code into a new brushfireexample module so that stuff like localExample isnt ending up in our published artifacts If people agree I could do that in this PR or in a followup I dont intend to merge this pull request Im putting this up here to demonstrate a method for obtaining a functional iterator for trees This was a way for me to get a bit more familiar with dependent types since we need to work with the type member of FullBinaryTreeOps and generally play with Scala data structures At its cores this PR introduces a wrapper around a stack of tree nodes which allows us to iterate through the trees nodes depthfirst cc robstripe sritchiestripe thomasstripe erikstripe oscarstripe kelleystripe avistripe This PR adds some love to the Quick Start example in README Fixed an issue where the SNAPSHOT version in exampleiris didnt match what was in versionsbt Fixed an issue where orgapachehadoop was excluded from runtime by sbtassembly causing a ClassNotFoundException to be thrown by exampleiris Basically the hadoopClient in Depsscala was provided so it wasnt getting included in targetscala brushfirescalding SNAPSHOTjarwithdependenciesjar during builds To support faster training especially in local mode we could be using compressed Bonsai trees directly in Brushfire Theres really no advantage to Brushfires native tree type so we should remove it The idea is that wed batch our operations for step eg growing the tree by and make the changes to the Bonsai tree in bulk This involves both exposing some new functionality from Bonsai and replacing the use of trees and the use of the tree ops type class in Brushfire This PR gives us another way to evaluate how well predictions do against the actual known distribution The iris example has been ported to demonstrated this method in practice There is also a small refactoring of the local trainers validate method and some small refactors of other error classes This PR moves brushfirecore over to use spirealgebraPartialOrder and spirealgebraOrder where applicable At this point we dont actually use any partial orderings but the support is there Before we merge this Id like to add some tests to prove this is working and maybe flesh out support for the is present usecase a bit What do you all think we need Review by tixxit and avibryant Our tree generators for scalacheck are fairly complex so if we created a new brushfirelaws package or something that included them then they could be reused elsewhere This is super early work but we have a CsvTrainerJob which can run on arbitrary CSVs with the labels provided by the user The actual types of the values will be inferred before training This makes a few related changes to Evaluator it decouples it from Split and just has it directly operate on Iterable T which was the only part of a Split it ever cared about anyway this makes it possible to apply an Evaluator to a tree or subtree which could be useful in various ways in the future eg if we want to change Splitter to produce candidate subtrees instead of splits in this context the Iterable T represents the leaves of the subtree and so its now labeled as such rather than using Infinity to represent an unacceptable level of error it now returns Option Double so you can return None to signal that the sign has been reversed ie this is now an error value not a goodness value it doesnt return a new Split just the error the root T is now also provided None of the evaluators makes use of it but there are at least two potential uses I have in mind considering a split unacceptable based on some ratio of the leaf T to the root T eg all leaves must contain at least X of the input Ys using more global optimization criteria this comes up in particular in the explanation tree use case which I wont explain in more detail here This is WIP because I havent fully updated the training code to make use of it attn tixxit non The intent here is to capture the basic mechanics of various training steps updateTargets expand prune etc in a way that can be reused in multiple execution environments local scalding spark For now this is all added directly to and used only by the Local trainer The nearterm impact is that the local trainer will stream over its input data in the same way that the distributed trainers do rather than requiring it to all be loaded into memory For this PR to be complete we should move the training steps to their own module maybe also doing and refactor the scalding trainer to use them Its very possible that this is too much or too little abstraction right now it seems a bit overfit to the needs of the specific training steps and platforms we support and I suspect it will be brittle going forward In fact featureImportance already doesnt work with this though I can argue that we should move to a TreeTraversalbased strategy for that which would At the same time I think some approach like this will be valuable going forward and I think its better to start going imperfectly down this path 