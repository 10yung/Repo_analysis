 Users can now stream data to Solr from any Spark supported datasource In order to preserve exactly once semantics users should set id primary field field before streaming the data out to Solr CODE val collectionPMsignals val zkhostzk Replace this with your cluster zkhost val opts Mapcollection collection zkhost zkhost sampleseed val df sparkreadformatsolroptionsoptsload dfshow ERROR orgapachesolrclientsolrjimplHttpSolrClientRemoteSolrException Error from server at sort param field cant be found random at orgapachesolrclientsolrjimplHttpSolrClientexecuteMethodHttpSolrClientjava at orgapachesolrclientsolrjimplHttpSolrClientrequestHttpSolrClientjava at orgapachesolrclientsolrjimplHttpSolrClientrequestHttpSolrClientjava at orgapachesolrclientsolrjimplLBSolrClientdoRequestLBSolrClientjava at orgapachesolrclientsolrjimplLBSolrClientrequestLBSolrClientjava at orgapachesolrclientsolrjimplBaseCloudSolrClientsendRequestBaseCloudSolrClientjava at orgapachesolrclientsolrjimplBaseCloudSolrClientrequestWithRetryOnStaleStateBaseCloudSolrClientjava at orgapachesolrclientsolrjimplBaseCloudSolrClientrequestBaseCloudSolrClientjava at orgapachesolrclientsolrjSolrRequestprocessSolrRequestjava at orgapachesolrclientsolrjSolrClientquerySolrClientjava at comlucidworkssparkutilSolrQuerySupportgetNumDocsFromSolrSolrQuerySupportscala at comlucidworkssparkrddSolrRDDcalculateSplitsPerShardSolrRDDscala at comlucidworkssparkrddSelectSolrRDDanonfun applymcIspSelectSolrRDDscala Can I send a JSON string to Solr Without have to deserialize to DataFrame Thank you kiranchitturi I see the Why not like this val options Map collection solrcollectionname zkhost zkconnectstring httpBasicAuthUser httpBasicAuthUser httpBasicAuthPassword httpBasicAuthPassword val df sparkreadformatsolr optionsoptions load This seems to be more friendly to multipermission support It can be controlled by spark options This patch introduces a new configuration addnewfiles This makes nonmadatory the creation of new fields from spark and let solr dynamicaly create the fields This have been discussed Changed the httpclient class imports to httpclient x class imports This would affect anyone trying to exclude sparksolrs hadoop dependencies and include newer hadoop dependencies for other purposes This didnt seem to break anything the tests passed the project compiled and our spark jobs worked but we dont utilize any of the hadoophdfs features of the sparksolr connector However this PR should be harmless Reference issue here lucidworkssparksolr During our work to upgrade a project to a newer version of Hadoop we discovered that the sparksolr connector imports a couple of HttpClient classes NoHttpResponseException and ConnectTimeoutException from the HttpClient package included by hadoop instead of the version x equivalent classes This was discovered during a Spark job test run where we index our data into Solr one of the hosts of our SolrCloud went down and a NoHttpResponseException was supposed to be thrown However the class couldnt be found because we had excluded all the hadoop dependencies at runtime which meant HttpClient was not added as a dependency Referencing HttpClient classes would prevent the sparksolr connector from working with Hadoop version or later but changing the references now shouldnt affect its integration with hadoop csvDFwriteformatsolroptionsoptionsmodeorgapachesparksqlSaveModeOverwritesave javautilNoSuchElementException Noneget at scalaNonegetOptionscala at scalaNonegetOptionscala at comlucidworkssparkSolrRelationinsertSolrRelationscala at solrDefaultSourcecreateRelationDefaultSourcescala at orgapachesparksqlexecutiondatasourcesSaveIntoDataSourceCommandrunSaveIntoDataSourceCommandscala at orgapachesparksqlexecutioncommandExecutedCommandExecsideEffectResultlzycomputecommandsscala at orgapachesparksqlexecutioncommandExecutedCommandExecsideEffectResultcommandsscala at orgapachesparksqlexecutioncommandExecutedCommandExecdoExecutecommandsscala at orgapachesparksqlexecutionSparkPlananonfunexecute applySparkPlanscala at orgapachesparksqlexecutionSparkPlananonfunexecute applySparkPlanscala at orgapachesparksqlexecutionSparkPlananonfunexecuteQuery applySparkPlanscala at orgapachesparkrddRDDOperationScopewithScopeRDDOperationScopescala at orgapachesparksqlexecutionSparkPlanexecuteQuerySparkPlanscala at orgapachesparksqlexecutionSparkPlanexecuteSparkPlanscala at orgapachesparksqlexecutionQueryExecutiontoRddlzycomputeQueryExecutionscala at orgapachesparksqlexecutionQueryExecutiontoRddQueryExecutionscala at orgapachesparksqlDataFrameWriteranonfunrunCommand applyDataFrameWriterscala at orgapachesparksqlDataFrameWriteranonfunrunCommand applyDataFrameWriterscala at orgapachesparksqlexecutionSQLExecutionanonfunwithNewExecutionId applySQLExecutionscala at orgapachesparksqlexecutionSQLExecutionwithSQLConfPropagatedSQLExecutionscala at orgapachesparksqlexecutionSQLExecutionwithNewExecutionIdSQLExecutionscala at orgapachesparksqlDataFrameWriterrunCommandDataFrameWriterscala at orgapachesparksqlDataFrameWritersaveToV SourceDataFrameWriterscala at orgapachesparksqlDataFrameWritersaveDataFrameWriterscala elided Hi Kiran i am testing Hdp migration in that when i try to push the data from hive dataframe and job is getting hang running more time and no response even solr connected with zk successfully here is my code from pysparksql import SparkSession from pysparkllapsqlsession import HiveWarehouseSession sparkSparkSessionbuilderappNameexportProceedingDataToSolrenableHiveSupportgetOrCreate hive HiveWarehouseSessionsessionsparkbuild DF hiveexecuteQueryselect from table DFwriteformatsolroptionzkhostzkhostoptioncollectionsolrproceedingscollectionoptionbatchsize optioncommitwithin modeappendsave print Cases export to Solr successfully completed FYI i am doubting on new HDP since spark and hive catalog is separated and earlier we can use both and could able to push the data from the data frame whether hive or spark please guide me how to proceed further I appreciate your reply in this regards Thanks Raj Hi team I m just wondering if there are any missing dependencies on the classpath for this project as Im running into a class not found exception when connecting to a secure solr cloud instance basic auth ssl Everything is working as expected on a nonsecure solr cloud instance The process looks pretty straightforward according to the doco so I m wondering if I m missing anything obvious or if I need to bring any extra classes to the classpath when using this project Any advice would be greatly appreciated Thanks Dwane Environments tried and solr cloud SSL Basic Auth Plugin Rules Based Authorisation Plugin enabled Spark v SparkSolr build sparksolr shadedjar ClassNotFoundSparkSolrtxt spark binhadospark binhadoop binsparkshell master local jars sparksolr shadedjar conf sparkdriverextraJavaOptionsDbasicauthsolrSolrRocks val options Map collection MyCollection zkhost zkn zkn zkn solrSPARKTEST val df sparkreadformatsolroptionsoptionsload 