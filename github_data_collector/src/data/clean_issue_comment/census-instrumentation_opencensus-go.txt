Please answer these questions before submitting a bug report What version of OpenCensus are you using v What version of Go are you using go What did you do I was running the new goleak tool on my codebase and ran into a leak coming from OpenCensus goleak Errors on successful test run found unexpected goroutines Goroutine in state select with goopencensusiostatsviewworkerstart on top of the stack goroutine select goopencensusiostatsviewworkerstart xc Usersadamcodepkgmodgoopencensusiov statsviewworkergo x created by goopencensusiostatsviewinit Usersadamcodepkgmodgoopencensusiov statsviewworkergo x What did you expect to see I expected no goroutine leaks in libraries I depend on What did you see instead A crash from goleaks Additional context Im pulling this library in via goclouddevsecrets v but that doesnt matter here since v of OpenCensus is used in both This appears to be coming from internal global state of a default recorder being set Also there doesnt seem to be a way to retrieve this reporter to shutdown on application shutdown Is your feature request related to a problem Please describe Greetings I just started using opencensus in go The quality and framework was super easy to use the primitives are intuitive and there are great docs One thing I noticed was that runtime runmetrics metrics didnt include GC information Im wondering if runmetrics are a good place to include these metrics Describe the solution youd like If runmetrics is a good place to include these metrics I was hoping to have gc stats included in the runtime metrics exported Describe alternatives youve considered I could pretty easily instrument these in my own application even package it as a library so all my go apps could easily get these metrics Additional context This request is based on the runtime metrics that prometheus go client ships with Can see the prometheus metrics in action on their grafana dashboard goprocesses Thank you What version of OpenCensus are you using HEAD of this repo What version of Go are you using go version go darwinamd Details examplesPrintExporter seems to require being passed to viewRegisterExporter but examplesLogExporter cant be passed there because it doesnt implement the write Exporter type Instead youre expected to run the Start method on it The documentation doesnt explain this difference and Im not sure they were expected to be different Does Datadog tracing and Opencensus tracing work well together For example take the following code golang import context log datadog githubcomDataDogopencensusgoexporterdatadog opencensustrace goopencensusiotrace gopkginDataDogddtracegov ddtracetracer func main exporter err datadogNewExporterdatadogOptionsService myapp if err nil logFatalerr defer exporterStop opencensustraceRegisterExporterexporter opencensustraceApplyConfigopencensustraceConfig DefaultSampler opencensustraceAlwaysSample span ctx tracerStartSpanFromContextcontextBackground foo datadogSpanctx openCensusSpanctx spanFinish func datadogSpanctx contextContext span ctx tracerStartSpanFromContextctx datadogSpan defer spanFinish func openCensusSpanctx contextContext ctx span opencensustraceStartSpanctx opencensusSpan defer spanEnd Will the datadog span and opencensus span have the same trace id Fixes Or at least I think so I am not familiar with the code that much so would love some close eyes on this I am not sure what kind of impact this one line of code would have on performance However it does seem to fix the behavior described in the issue above and all the unit tests did pass locally I have a program that records the latency of a request using the OCHTTP server middleware In my program I am using the DataDog OpenCensus Exporter to report this metric Therefore I wanted to simulate a latency spike by having my handler do the following The first minutes the HTTP Handler takes between ms to respond The second minutes the HTTP handler takes between seconds latency spike The third minutes the HTTP handler takes between ms latency goes back to normal The latency gets reported every seconds In other words OpenCensus calls DataDogs ExportView every seconds OpenCensus collects the MinMax latencies and passes them as DistributionData to the DataDog exporter My expectation is that for every seconds OpenCensus calculates the MinMaxMeanetc and passes them to an Exporter but then it should reset those numbers so that they dont obscure the next bucket of time Is this assumption incorrect Even if this assumption is wrong lets continue with how this behavior has obscured the latency spike significantly The fact that OpenCensus does not reset the Min every seconds means that the Min stays the same across the entire lifetime of the Go process Therefore if we had a dashboard of the min value only youll see that the min never went up it stays as ms for the entirety of the minutes Even though for minutes the Min value was at least ms On the other hand if you have a max metric youll see that it went up to s but then it never went back down even though the traffic spike ended after minutes Predictably if I kill the process and restart it suddenly the dashboard gets fixed and thats because we no longer hold the minmax that was previously preventing the dashboard to be accurate See the following screen shot for demonstration img width altScreen Shot at AM src What you see here is that the Purple line is the Max metric while the Blue line is the Min metric When minutes passed the Purple line went up but the Blue line stayed down This is because even though new incoming requests were clocking between seconds OpenCensus kept comparing those numbers to the Min that was initially recorded in the first minutes However once I kill the process and restart the server resuming the traffic spike you can see the Min gets updated Similarly when the spike finished the Max never goes down until I forcefully reset the Max As far as I understand and at the very least restarting the process should not significantly affect what the chart looks like as seen in the screenshot above My understanding is that OpenCensus creates a metric for a bunch of Measures per Reporting Interval I can easily be wrong about this Therefore if I manually empty out the DistributionData after every interval you can see the Dashboard now looks very correct img width altocacc src Finally I have created a reproduction Repository here I included a Vendor folder that fmtPrintlns the incoming DistributionData and you can notice that even when the traffic starts to go up to seconds for minutes the Min value that DataDog receives stays the same AFAIK theres no need to actually set up Datadog you can just run the program and inspect the logs Also Im not familiar with the OpenCensus codebase until this issue but all I had to do to fix it was add this line Not sure how thorough the unit tests are but this didnt break any of them The default span name for the ochttpTransport plugin uses the path from the url Paths often contain unique identifier for resources for example the gcs client path contains the object identifier It might be better to go with a safer default like method which has a bounded cardinality This might be related to What version of OpenCensus are you using v What version of Go are you using go What did you do Used the default ochttpTransport What did you expect to see Low cardinality span names What did you see instead Span names that contained unique identifiers Additional context Add any other context about the problem here Implement a RateLimiting sampler as described at Adds a StartTime option when creating a new span In some cases you are able to trace only after the operation was made for example in some postoperation hookobserver func myHookctx contextContext info queryInfo span StartSpanmydatabase WithStartTimetimeNowAdd infoDuration spanEnd How do I trigger and upload trace to jaeger 