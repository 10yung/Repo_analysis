engine conf engineId rb engineFactory comactionmlenginesurUREngine sparkConf master local sparkserializer orgapachesparkserializerKryoSerializer sparkkryoregistrator orgapachemahoutsparkbindingsioMahoutKryoRegistrator sparkkryoreferenceTracking false sparkkryoserializerbuffer m sparkexecutormemory g sparkdrivermemory g sparkesindexautocreate true sparkesnodes elasticsearch sparkesnodeswanonly true algorithm indicators name buy name addtocart name view name like items Screenshot from events Screenshot from jobs Screenshot from log harnesslog Note I am using dockercompose Hi Using Harness and the included UR Im trying to use the include filter rule but I keep getting an empty result I used a set event to set a category on two items that exist in my data as such event set entityType item entityId exampleItem properties category electronics mobile expireDate T Z eventTime T Z Then I ran the train command and then this query rules name category values electronics bias but got an empty list as result The other rules bias and seem to be working as specified Hi im using the universal recommander template with predictionio to recommend products from a store My issue is that said store can delete products in back office for any reason when it does the item must of course not be recommanded anymore I then need a way to delete said item but not in a permanent way Right now the best way iv found is to give an item a specific property lets say eliminated retrain and then filter for on that property with a bias this way i just need to remove that property if i want the item back Is there a better way to do this Retrain take time does a way to avoid it exist I tried some other methods like an unset event on the item but doing so still require a retrain and it also require a full set event to bring it back if i need to do so and delete event that seems to do nothing at all but it worked when i tried with a different template similar product and without a retrain last thing i tried was the avaibleexpire date but first i dont know the expire date until someone decide to actually make it expire second i need to make the change and then retrain if i want the changes to apply so im back to square one Any answer will be very appreciated universal recommander version is predictionio version is I want to get the recommended data correctly Your thoughts and ideas I followed the steps in the official documentation Install with docker no problem document page Add the recommendation engine with harnesscli no problem the documentation page After the engine is added the test function is normal Describe your problem After the data was successfully entered by post and the training was activated the recommendation engine did not give me feedback avatar avatar avatar The training event was triggered and about pieces of data were entered in advance When the request was routed to the queries the recommended items were not returned to me The recommendation engine was running normally and the log was not reported I don t know why When I builded this projectI found errors They were called by the sbts plugins version isnt the latest Where the file is projectpluginssbt Here are the lastest plugins url orgscalariform orgscalastyle comeed si n PSThis is the first timeso made a mistakepull request in the master branchSo sorry about it It seems that UR cannot detect the GPU INFO RootSolverFactory Creating orgapachemahoutviennaclopenclGPUMMul solver INFO RootSolverFactory Unable to create class GPUMMul attempting OpenMP version INFO RootSolverFactory Creating orgapachemahoutviennaclopenmpOMPMMul solver INFO RootSolverFactory orgapachemahoutviennaclopenmpOMPMMul INFO RootSolverFactory Unable to create class OMPMMul falling back to java version There is my GPU information nvidiasmi Sat Jun NVIDIASMI Driver Version CUDA Version GPU Name PersistenceM BusId DispA Volatile Uncorr ECC Fan Temp Perf PwrUsageCap MemoryUsage GPUUtil Compute M Tesla K On Off NA C P W W MiB MiB Default Processes GPU Memory GPU PID Type Process name Usage No running processes found Can I take into account useritem locations in universal recommender If it is true how to do it Thanks Hi All I have installed prediction io on MacBook and installed the universal recommender template I was able to build the app successfully But when I do an integration test or training on other data I am getting the below error I have also copied the enginejson extract It will be very helpful if anyone share their inputs on this enginejson nothing has been changed its the default data comment This config file uses default settings for all but the required values see READMEmd for docs id default description Default settings engineFactory comactionmlRecommendationEngine datasource params name samplehandmadedatatxt appName handmade eventNames purchase view categorypref minEventsPerUser sparkConf sparkserializer orgapachesparkserializerKryoSerializer sparkkryoregistrator orgapachemahoutsparkbindingsioMahoutKryoRegistrator sparkkryoreferenceTracking false sparkkryoserializerbuffer m esindexautocreate true Exception when training INFO Engine Extracting datasource params INFO WorkflowUtils No name is found Default empty String will be used Exception in thread main javalangNoSuchMethodError orgjson sParserUtilquoteLjavalangStringLjavalangString at orgjson snativeJsonMethodsanonfun applyJsonMethodsscala at orgjson snativeJsonMethodsanonfun applyJsonMethodsscala at scalacollectionimmutableListmapListscala at orgjson snativeJsonMethodsclassrenderJsonMethodsscala at orgjson snativeJsonMethodsrenderJsonMethodsscala at orgapachepredictionioworkflowWorkflowUtilsanonfungetParamsFromJsonByFieldAndClass anonfun applyWorkflowUtilsscala at orgapachepredictionioworkflowWorkflowUtilsanonfungetParamsFromJsonByFieldAndClass anonfun applyWorkflowUtilsscala at scalaOptionmapOptionscala at orgapachepredictionioworkflowWorkflowUtilsanonfungetParamsFromJsonByFieldAndClass applyWorkflowUtilsscala at orgapachepredictionioworkflowWorkflowUtilsanonfungetParamsFromJsonByFieldAndClass applyWorkflowUtilsscala at scalaOptionmapOptionscala at orgapachepredictionioworkflowWorkflowUtilsgetParamsFromJsonByFieldAndClassWorkflowUtilsscala at orgapachepredictioniocontrollerEnginejValueToEngineParamsEnginescala at orgapachepredictionioworkflowCreateWorkflowmainCreateWorkflowscala at orgapachepredictionioworkflowCreateWorkflowmainCreateWorkflowscala at sunreflectNativeMethodAccessorImplinvoke Native Method at sunreflectNativeMethodAccessorImplinvokeNativeMethodAccessorImpljava at sunreflectDelegatingMethodAccessorImplinvokeDelegatingMethodAccessorImpljava at javalangreflectMethodinvokeMethodjava at orgapachesparkdeployJavaMainApplicationstartSparkApplicationscala at orgapachesparkdeploySparkSubmitorgapachesparkdeploySparkSubmitrunMainSparkSubmitscala at orgapachesparkdeploySparkSubmitdoRunMain SparkSubmitscala at orgapachesparkdeploySparkSubmitsubmitSparkSubmitscala at orgapachesparkdeploySparkSubmitdoSubmitSparkSubmitscala at orgapachesparkdeploySparkSubmitanon doSubmitSparkSubmitscala at orgapachesparkdeploySparkSubmitmainSparkSubmitscala at orgapachesparkdeploySparkSubmitmainSparkSubmitscala Regards Raman S V Hi dear developer I extract the code of cco training from your piour but when i ran my jar in clusters I meet the problem had a not serializable result orgapachemahoutmathDenseVector I dont understand why the mahout design its Vector no able to serialize and I also dont understand why you use RandomAccessSparseVector in distributed program how can you solve this problem Please help me thank you very much I tried to leverage the already present async interface of elasticsearch and armouring the not so crucial parts like getting the app with blocking constructs which will tell the standard scala ExecutionContext to use a separate thread for that Let me know what you think See also 