Hi as shown in the following full dependency graph of bookstore bookstore requires aiobotocore while the installed version of aioboto requires aiobotocore According to Pips first found wins installation strategy aiobotocore is the actually installed version Although the first found package version aiobotocore just satisfies the later dependency constraint aiobotocore it will lead to a build failure once developers release a newer version of aiobotocore Dependency tree bookstore aioboto install version version range aiobotocoreinstall version version range aiohttpinstall version version range asyncgeneratorinstall version version range botocoreinstall version version range docutilsinstall version version range jmespathinstall version version range wraptinstall version version range aiobotocoreinstall version version range aiohttpinstall version version range asyncgeneratorinstall version version range botocoreinstall version version range docutilsinstall version version range jmespathinstall version version range wraptinstall version version range futureinstall version version range futuresinstall version version range ipythoninstall version version range notebookinstall version version range tornadoinstall version version range Thanks for your attention Best Neolith We may be able to give people a straightforward way to revert to older versions of their notebooks by integrating directly with the Jupyter Checkpoints APIs This would allow us to surface a simple UI for users to revert to older versions of their notebooks from within the notebook itself Many thanks to romainintel for pointing out this possibility As brought up by MSeal in we should consider automating our version updates One of the easiest ways to set this up is to set up bumpversion and make that part of our releasing instructions In the meantime we will just aggregate those spots that need updating inside the RELEASINGmd instructions per e ae adcd f a b d e cc d In the classic notebook server we have the LargeFileManager to handle large files as a streamed response We could have something analogous for bookstore that would alleviate issues around archiving large files There are a few details that would need some explicit design work eg do we also stream responses back to S do we queue a save in the case where saving to S is taking a long time instead of debouncing all requests c but this seems well within scope of this project Weve added to Papermill and it makes sense to add here too In we realized that there is utility in logging more information at the point of validating the bookstore settings To keep the PR more tightly scoped we only added it to the file system cloning validation We need to make a decision on if we want to add logs for overall bookstore validation archive validation publishing validation s cloning validation Related to This issues scope is a documentation update that mentions the click process and that the extra click is needed for security and protection from malicious code execution Lets make focused on a Security audit and evaluation of Bookstore In order to help folks understand why landing on the bookstore cloning page served as texthtml we should outline the threat model security risks and mitigations Summary Some initial users have complained about having an extra click when cloning It definitely slows the intended user experience of a smooth way to share notebooks We need to mitigate the risk of users loading notebooks that they didnt wish to onto their compute Since the jupyter notebook server is one big remote code execution platform the holy grail of security vulnerabilities we have to be extra vigilant While there are many other ways to attempt to exploit the overall system we dont wish for our portion to be a wide attack vector Scenario Malicious notebook is sitting on Bucket MyBucket at path mynotebookpathipynb User is passed a link looking like With our current clone page the user has to decide if they mean to import this notebook Today when I was using bookstore to clone from S I found that responses were just hanging indefinitely It seems to be occurring when we attempt to read the s response objects body with await obj Body read Importantly it doesnt hang when using minios mock s server Does anyone have any insight as to what could be causing this kylek willingc mseal Currently we are consuming the id passed through via the sessions response in the nbclient library and not setting it to a variable My guess is that this was removed earlier because of similar issues to those raised around keeping the kernels id field I discovered this as I was writing unit tests for the client section I am going to refrain from adding it in those tests but will leave commented code in place to indicate the functionalitys future return mostly for the sake of future integration tests