how to add last model checkpoints during training Hi I cant understand how precision and recall are calculated I would like to plot a PR curve for different scorethreshold value but when I saw the code in evalpy I cant understand why FP and TP are calculated this way python falsepositives npcumsumfalsepositives truepositives npcumsumtruepositives in particular Ive doubts about npcumsum why it isnt npsum i think the algorithm does not take into the pictures without targets will it cause too few background classes or negative samples compared twostage algorithm if x y x y classname continue this code from csvgeneratorpy can you give me some advices Hi I was following along issue TypeError data type not understood and like user icoicqico was running into a new problem after using the proposed fix in AttributeError Node object has no attribute outputmasks The assumption was that it is related to tflite Well it looks like you got further along this is probably some tflite stuff I cant really help you with that Im guessing a lot of layers in kerasretinanet are not supported However I see it unrelated to tflite but when loading the Retina Net model with tfkeras in TF I would expect that to work in principle since Retinanet and TF Keras both implement the Keras API The relevant code should be from tensorflow import keras myCustomObjects backboneresnet customobjects model kerasmodelsloadmodeltmpmodelconvh myCustomObjects with this stack trace Traceback most recent call last File loadKeraspy line in module model kerasmodelsloadmodeltmpmodelconvh myCustomObjects File usrlocallibpython distpackagestensorflowcorepythonkerassavingsavepy line in loadmodel return hdf formatloadmodelfromhdf filepath customobjects compile File usrlocallibpython distpackagestensorflowcorepythonkerassavinghdf formatpy line in loadmodelfromhdf customobjectscustomobjects File usrlocallibpython distpackagestensorflowcorepythonkerassavingmodelconfigpy line in modelfromconfig return deserializeconfig customobjectscustomobjects File usrlocallibpython distpackagestensorflowcorepythonkeraslayersserializationpy line in deserialize printablemodulenamelayer File usrlocallibpython distpackagestensorflowcorepythonkerasutilsgenericutilspy line in deserializekerasobject listcustomobjectsitems File usrlocallibpython distpackagestensorflowcorepythonkerasenginenetworkpy line in fromconfig config customobjects File usrlocallibpython distpackagestensorflowcorepythonkerasenginenetworkpy line in reconstructfromconfig processnodelayer nodedata File usrlocallibpython distpackagestensorflowcorepythonkerasenginenetworkpy line in processnode outputtensors layerinputtensors kwargs File usrlocallibpython distpackageskerasbackendtensorflowbackendpy line in symbolicfnwrapper return funcargs kwargs File usrlocallibpython distpackageskerasenginebaselayerpy line in call previousmask collectpreviousmaskinputs File usrlocallibpython distpackageskerasenginebaselayerpy line in collectpreviousmask mask nodeoutputmasks tensorindex AttributeError Node object has no attribute outputmasks Seems to me like a problem with TF but I am really just starting with tensor flow What do you think Any ideas how to fix that By the way I have cloned the repo with commit da ceaf and have changed the initializer as proposed by LaurensHagendoorn Thanks Hi this might not belong here but Id still like to ask if its possible to change anchor shapes and sizes when fine tuning my custom model on new dataset Do we have to maintain the same anchor config throughout all training and fine tuning Also the same thing about imageminside and imagemaxside Does this make sense Anyway it throws IndexError if I change it without any other change I am trying to train model in Google Colab using tensorflow x bundle package for Colab and Getting Error as mention Below AttributeErrorModel object has no attribute getdistributionstrategy I am trying to predict image from train model and getting error as below W tensorflowcorecommonruntimebasecollectiveexecutorcc BaseCollectiveExecutorStartAbort Invalid argument Incompatible shapes vs node boxesmul filtereddetectionsmapwhilebody stridedslice W tensorflowcorecommonruntimebasecollectiveexecutorcc BaseCollectiveExecutorStartAbort Invalid argument Incompatible shapes vs node boxesmul It takes ms inferencing an input of shape And takes seconds inferencing an input of shape After applying some tooling to convert the KerasRetinanet model h into a Tensorflow model pb Im facing some issues When I run the KerasRetinanet model against an image boxes scores labels modelpredictonbatchnpexpanddimsimage axis the boxes scores and labels are populated with the expected results Just for information the model gives me back the detected objects I was expecting For the TensorflowRetinanet model the situation is quite different regression classification sessrun tfclassification tfregression feeddicttfinput image filterdetectionslayer kerasretinanetlayersFilterDetectionsnmsTrue boxes scores labels filterdetectionslayercall regression classification Both regression and classification have the right format Tensorclassificationconcat shape dtypefloat Tensorregressionconcat shape dtypefloat However after applying the FilterDetections the boxes scores and labels are populated with objects and the bounding boxes have a weird format For the same image with the KerasRetinanet model bounding boxes are Looks like Ive been missing something Ive been digging into the kerasretinanet code for a couple of days but so far Im stuck Any idea whats wrong in my logic or code File kerasretinanetbintrainpy line in module main File kerasretinanetbintrainpy line in main validationsteps argsstepsforvalidation AttributeError Namespace object has no attribute stepsforvalidation