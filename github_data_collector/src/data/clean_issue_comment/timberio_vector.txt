 rootubuntutest vector version vector nightly g be d x unknownlinuxmusl rootubuntutest lsbrelease a No LSB modules are available Distributor ID Ubuntu Description Ubuntu Focal Fossa development branch Release Codename focal rootubuntutest vector version vector nightly g be d x unknownlinuxmusl rootubuntutest vector test toml Running toml tests rootubuntutest cat toml datadir tmpvector sourcesss REQUIRED type journald example must be journald units currentbootonly true default sinksout inputs ss type console encoding json rootubuntutest vector config toml Jan INFO vector Log level info is enabled Jan INFO vector Loading config path toml Jan INFO vector Vector is starting version gitversionv g be d releasedSat Jan archx Jan INFO vectortopology Running healthchecks Jan INFO vectortopology Starting source ss Jan INFO vectortopology Starting sink out Jan INFO vectortopologybuilder Healthcheck Passed Jan INFO sourcenamess typejournald vectorsourcesjournald Starting journald server Failed to parse timestamp CJan INFO vector Shutting down rootubuntutest journalctl Logs begin at Tue end at Sun Jan ubuntutest kernel Linux version generic builddlcy amd gcc version Ubuntu ubuntu Ubuntu SMP Mon Dec UTC Ubuntu ge Jan ubuntutest kernel Command line BOOTIMAGEvmlinuz generic rootUUID eb bfdbc c c d bc ro splash quiet Jan ubuntutest kernel KERNEL supported cpus Jan ubuntutest kernel Intel GenuineIntel Jan ubuntutest kernel AMD AuthenticAMD Jan ubuntutest kernel Hygon HygonGenuine Jan ubuntutest kernel Centaur CentaurHauls Jan ubuntutest kernel zhaoxin Shanghai Jan ubuntutest kernel x fpu Supporting XSAVE feature x x floating point registers Jan ubuntutest kernel x fpu Supporting XSAVE feature x SSE registers Jan ubuntutest kernel x fpu Supporting XSAVE feature x AVX registers Jan ubuntutest kernel x fpu xstateoffset xstatesizes Jan ubuntutest kernel x fpu Enabled xstate features x context size is bytes using standard format Jan ubuntutest kernel BIOSprovided physical RAM map Jan ubuntutest kernel BIOSe mem x x fbff usable Jan ubuntutest kernel BIOSe mem x fc x ffff reserved Jan ubuntutest kernel BIOSe mem x f x fffff reserved Jan ubuntutest kernel BIOSe mem x x ffeffff usable Jan ubuntutest kernel BIOSe mem x fff x fffffff ACPI data Jan ubuntutest kernel BIOSe mem x fec x fec fff reserved And more important think i think you will need to add specific Ubuntu testing environment Thats required as most important distro Currently all parser transforms accept a types table for field value coercion The timestamp type is unique in that it requires an explicit strptime format to be specified toml transformscoerce type coercer transformscoercetypes timestampfield timestamps I propose that we offer a besteffort timestamp parsing option that does not require a strptime format toml transformscoerce type coercer transformscoercetypes timestampfield timestamp The formats would include IS RFC A generic time only format HHMM SS SSS SSS Z where represents an optional successive fragment The date is assumed to be the local date Im open to other formats I might be missing here Currently the coerce transform and parsing transforms offer a types table that allows users to explicitly coercer field values This is ideal for pipelines that must guarantee consistent field typing but there are use cases where dynamic typing might be preferred For these I propose that we add a typesauto option Specifically Add a boolean auto option the types table for all parser transforms Explicit types as they exist currently always take priority and should override automatic typing Integer strings should automatically be coerced to integers Float strings should automatically be coerced to floats Boolean strings as defined by this comment should automatically coerce to boolean ISO time formats should automatically coerce to timestamps Im open to other popular formats within reason Im curious what others think about this before we proceed with the work Closes Adds a pretty conservative logfmt parser It will basically scan and extract any logfmtstyle foobar pairs in the given field handling quoting and such and add them as fields on the event If there are no such pairs nothing is added and the message is passed through normally Nothing too fancy just accepts text bodies over HTTP where each line is roughly syslogformatted We do some basic parsing but fall back to forwarding the raw line if anything seems off This change introduces some improved docs on the process for proposing large PRs Weve in the past run into issues with PRs living for a while because they are large and complex This should hopefully clear some of that up Vector is fast approaching and we want to be as transparent as possible with our development and planning efforts Roadmap X complete Initial logs support milestone X complete Securely process data milestone X complete Bestinclass file source milestone X complete Initial metrics support milestone X complete Initial containers support milestone X complete Initial AWS support milestone X complete Support popular targets and CPU archs milestone X complete Config testing milestone X complete Config consistency cleanup milestone inprogress Enrich data with environment context milestone inprogress Initial GCP support milestone inprogress Data processing scripting milestone pending Initial tracing support pending Server level metrics collection milestone pending Vector observability milestone pending Formalize regression control milestone pending Endtoend event acknowledgement pending Improved high concurrency performance pending Load balancing support pending Vector to vector application level acknowledgement What does mean to us A stable API As defined by semantic versioning This includes any userfacing part of Vector the CLI configuration schema and everything exposed through Vectors components Production readiness High confidence that Vector will perform well in large demanding production environments You can read more about he we define production readiness here Achieves Vectors vision A single tool that collects all observability data and gets it to its destination What does pre mean to us Possible API changes Although we do not take this lightly and will try to bundle breaking changes into a single release to reduce the migration burden Possible roadmap changes We work closely with many Vector users and learn a lot in that process These findings are likely to alter the roadmap in nonsubstantial ways What about post We will be focusing heavily on endtoend use cases and going deeper on quality innovation within the context of these use cases For example A constant focus on performance reliability and usability Application observability Infrastructure monitoring Security compliance Improved parsing structuring Extending Vector with plugins Closes The option is added only to json encoding as text encoding was not using fields previously I decided to not implement dropping the indexed fields from events mentioned in for now as Im worried about unexpected results for existing users It can be easily changed GCP Big Query is a powerful service for analyzing large amounts of structured data If used correctly it can be a costeffective storage for log data I would like to see Vector support this service as a sink but itll require careful planning due to the many different ways Big Query can be used Context Big Query is flexible and we should consider the following features for our Big Query sink Storing JSON data in a single column Users can use Big Querys JSON functions This type of querying is slower for obvious reasons fetching more data parsing etc This type of querying is more expensive because each query must fetch and scan the entire JSON payload as opposed to individual columns Mapping structured data to individual Big Query table columns Automatic schema maintenance Streaming inserts vs batching inserts Streaming inserts have a cost per MB Note the variety of methods to batch insert This can be done through the API directly or through other GCP services cloud storage stackdriver etc This of course is not inclusive of all factors we should consider for Big Query but it helps to demonstrate the variety of options Starting simple v of this sink should solve the simplest implementation Use the streaming inserts API and stream records by no batching Assume we are writing to a table with columns timestamp and jsonevent The timestamp column should map to our own internal timestamp column The jsonevent column should contain a JSON encoded representation of our event Both of these column names should be configurable It is worth thinking about a generic column mapping configuration scheme so that users could map other custom fields to Big Query columns Include documentation on how to create a properly structured table Ideally this table would be partitioned by the timestamp day Long Term We might consider the following features for longterm development Support for using the batching API since it does not incur a cost Dynamic schema maintenance Although I think this might be better solved with a transform or something separate Blocked by and We need to cut the release This is the Great Breaking Change release Weve purposely rolled up various breaking changes to consolidate them As such this release will require a little extra work Run make release and manually curate the commits in metareleases toml Make sure everything has the proper type and scope make generate pauses for manual curation of this data Manually add upgrade guides for all breaking changes in metareleases toml You can see an example here Note we do not need individual guides for every single change we can combine them into a single guide if it makes sense and provides for a better UX Proceed with make generate to tag and finalize the reasons Note make generate is designed to be idempotent you can exit and rerun it as much as you want It is an interactive process with prompts