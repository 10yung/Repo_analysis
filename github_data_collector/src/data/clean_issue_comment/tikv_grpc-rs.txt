To keep code clean and prepare to add authentication support Signedoffby koushiro koushirocqxgmailcom When calling GetFeature it uses getfeatureasync and then immediately calls wait Is this different from just calling getfeature Eg rust match clientgetfeaturepoint When calling RecordRoute theres a loop where stuff is sent to the sink and then after that the sink is closed Whats going on with mut sink It looks like the result of sinksend is being assigned to the outer sink variable and then the resulting sink from the last iteration of the loop is closed What is the Sink being created by sendsink And why does only the last one get closed The client example makes frequent use of wait which I assume just blocks the current thread until the Future finishes What should the code look like if I want to use asyncawait I assume this would need the futures compat feature but I dont know what the details would look like Similar question for the server example How can I write a server that uses asyncawait When using the builtin cmake to build zlib it changes the source tree as madlerzlib describes This leads to the failure during generating the docs So lets switch to libzsys instead which uses its own custom script to build zlib and leave source tree as it is Switching to libzsys can also reduce the package size as we can ignore more sub modules It should improve compile time if libzsys is also a dependency of other crates The only shortcoming is that libzsys may not be compatible with grpcio but I believe the chance is quite small given its such a small library And giving its such a small library the benifits like compile time or package size described above may be too small to be observed Current API design returns Stream and Sink for streaming APIs so that users can choose whichever executors they want But it has limitations Usually only spawning the final futures into the clientcontext can achieve less context switch and maximum throughput If spawning the futures back to clientcontext there are always code path executed multiple times unnecessary For example we always know Sinkstartsend provided by grpcio can only accept one message at a time All other messages can be accepted only after next CompletionQueuenext is executed at lease once But a properly implemented future adapter usually will execute additional pollcomplete or startsend to get a NotReady result which will lock and poll unnecessary If we accept Stream and Sink as in parameter instead we can control the timing more precisely and provide best practice out of box Taking RouteGuide as an example the current API is fn routechat self ClientDuplexSenderRouteNote ClientDuplexReceiverRouteNote New API becomes fn routechat self nodes impl StreamItemRouteNote sink impl SinkItemRouteNote However the new APIs may not be friendly for users as it requires a constructed Stream and Sink Describe the bug Random crashing when sending large amounts of data through a stream We get the error RpcStatus status Unknown details SomeStream removed The following lines are logged by grpc just before the error is returned ssltransportsecuritycc Corruption detected ssltransportsecuritycc error D CPEM routinesPEMreadbiono start line secureendpointcc Decryption error TSIDATACORRUPTED To Reproduce Unfortunately the crashing is random so I dont have a magic recipe to create it The idea is im sending large amounts of data through a bidir stream which are then buffered by on the server side then answers are sent back to the client Expected behavior Stream to process normally System information CPU architecture x Distribution and kernel version docker debianstretch Any other system details we should know Using the openssl version stretchs openssl package with feature openssl I need to link against openssl because Im using other crates that depend on specifically openssl and not googles fork In the current implementation of Sinks in grpcrs every startsend will call grpccallstartbatch to send one message It makes grpc use much CPU However we can just put messages into buffer during calling Sinkstartsend but send them out when calling Sinkpollcomplete For both Sinksendall or Streamforward pollcomplete will be called after some startsends so its ok It should increase throughput especially for overload cases This PR make CallTag reusable for streaming calls Task list x PR for implementation x Benchmark result cmd runperformancetestspy l rust regexprotobufasyncstreamingqpsunconstrained patched summary qps qpsPerServerCore serverSystemTime serverUserTime clientSystemTime clientUserTime latency latency latency latency latency serverCpuUsage serverQueriesPerCpuSec clientQueriesPerCpuSec master summary qps qpsPerServerCore serverSystemTime serverUserTime clientSystemTime clientUserTime latency latency latency latency latency serverCpuUsage serverQueriesPerCpuSec clientQueriesPerCpuSec For steaming calls every subsequent messages will allocate a Batch OpSet and a new tag By design grpc core does not expect user to send next message for the same call until last tag is returned Hence these two allocations can be reused messages To reuse batch and tag we need to refactor current CallTag implement to allow a long live batch batch should also be implemented in rust as operation sets so that we have more flexible and safe way to refactor There have been a lot of alpha releases And I dont see any problem from tests and benches Maybe its time to bump a GA release But before doing so I still need to make sure whats left undone The only one PR that can still change the interfaces and need to be merged into is And brings a new version of gRPC C core I think its better that a new GA release catches up with updated C core cc overvenus hunterlxt nrc Do you have any other PRs that needs to be included