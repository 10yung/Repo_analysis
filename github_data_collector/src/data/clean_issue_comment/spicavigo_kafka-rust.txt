my producer conf is let producer ProducerfromhostssettingsBROKERtoowned split maps strimtoowned collect withacktimeoutDurationfromsecs withrequiredacksRequiredAcksOne withcompressioncompression create expectCREATEKAFKAPRODUCERERROR sometimes have tcp CLOSEWAIT tcp CLOSEWAIT tcp CLOSEWAIT tcp CLOSEWAIT tcp CLOSEWAIT tcp CLOSEWAIT tcp CLOSEWAIT tcp CLOSEWAIT tcp CLOSEWAIT tcp CLOSEWAIT tcp CLOSEWAIT tcp CLOSEWAIT tcp CLOSEWAIT tcp CLOSEWAIT tcp CLOSEWAIT tcp CLOSEWAIT tcp CLOSEWAIT tcp CLOSEWAIT tcp CLOSEWAIT tcp CLOSEWAIT tcp CLOSEWAIT tcp CLOSEWAIT tcp CLOSEWAIT tcp CLOSEWAIT tcp CLOSEWAIT All TCP Connect is CLOSEWAIT The option for the number of partitions is partitions not partition Currently the following methods of Kafka vanilla producer API are not supported the enableidempotence configuration initTransactions beginTransaction commitTransaction abortTransaction sendOffsetsToTransaction Implementing this methods will unlock Kafka transactional features like atomic publishing into several topics and exactlyonce delivery semantic Source What is the proper way of maintaining a reliable consumer And by reliable I mean that it should be available during whole program execution Particularly if theres an error on some call it should try to recover if possible eg in case of loosing connection it should reconnect to the broker Should the consumerConsumer reconstructed once again and again in such a case Or it might be some other way of handling that Hello The Error type doesnt play very nice with the failure crate Would it be possible once this lands and is published update the version of error chain used Thank you Im using the Consumer class to read a stream of messages and every so often theres a rogue big one When this happens my loops consumerpoll call returns an Err value Once this happens the consumer doesnt move past this offset and all subsequent calls to consumerpoll just return the same error The loop in greatly elided form looks like loop match consumerpoll Okmessagesets for messageset in messagesetsiter for m in messagesetmessages Do stuff with messages consumerconsumemessagesetmessagesetunwrap consumercommitconsumedunwrap Erre MessageSizeTooBig etc The questions are What does this error mean when its received by the consumer Is it that the broker rejected a message from the producer or that the consumer tried to fetch one and it meaning this program cant accommodate it Admittedly this is more of a general Kafka question Is there a way to move past the error and continue processing more messages It doesnt look like theres a messageset or message to consume and commit if you end up in the Err match arm My understanding is that what you get when you poll is a bunch of message sets potentially multiple sets containing multiple messages Is there a way to not lose an entire batch of messages when this happens examplestopicmetadata prints for example topicmetadata brokers hostport topics topicname topic pid lid lhost earliest latest size topicname xxxxxxxxxxxxport topicname xxxxxxxxxxxxport topicname xxxxxxxxxxxxport wed also like to see a partitions configured replicas and insyncreplicas these correspond to the partition metadata fields partition and isr Im still working on the LZ compression supports it not a finalized design please help to review code and give your comments Kafka use an incomplete implementation of LZ frame format I doubt whether we need a more complete implementation or just follow Kafka did Besides are there any document about how to generate testdata and write integration test kafkarust is using some systemnonrust dependencies such as minizflate libsnappy snappysys and libopenssl opensslsys etc which cause the deployment was limited by those native libraries kafka v byteorder v crc v lazystatic v flate v libc v minizsys v libc v fnv v log v openssl v bitflags v lazystatic v libc v opensslsys v libc v refslice v snappy v libc v snappysys v libc v Im working on the PR to replace snappy with snap in pure rust I think we may replace the other systemnonrust dependencies in future For example replace openssl with nativetls which may lack some features compare with openssl but I believe it will be ready sooner or later because a lot of rust crates switch to use it This is mediumdifficulty for any newcomers as it requires a somewhat intimate familiarity with the library Currently much of the code that requires doing actual networking calls to a Kafka server are not tested We can take advantage of Docker and Travis to enable integration testing the networking methods I did some research and playing around today and I came up with a setup that seems to work Basically we set up a dockercomposeyml that uses a public Kafka container from the Docker registry and give the Kafka version as a parameter Then we set up our travisyml to run the tests for each Kafka version we want to test against This will allow us not only to ensure sure the code works as intended but also verify compatibility across as many versions of Kafka as wed like I realize that this crate is still pre so retroactively adding tests for everything might be overkill since the API is still subject to change but it should help with getting at least the most important methods tested for correctness For testing locally you would just need to have Docker and Compose installed and then before running the cargo integration tests Set the KAFKAVER environment variable in your shell Bring up the desired Kafka container with dockercompose up d This commit in a branch of my fork showcases the code changes necessary It uses the latest versions of and and includes a simple integration test for loading the KafkaClient metadata This is what a Travis build would look like What do you think Update The above has been implemented and merged in Lets keep this issue around to track progress of integration test backfilling