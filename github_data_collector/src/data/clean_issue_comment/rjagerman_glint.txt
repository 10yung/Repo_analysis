 add save operation to store matrixvector into hdfs add hdfs deps on buildsbt and the hadoop version should be or higher The Problem which is used parameter server the weight vectormatrix is huge and sparse Need Save Operation to store weight into HDFS I compiled Glint using Scala and successfully got the jars Then I try to run the example in sparkmd however I got this error when initializing the client transient val client Client javalangNoSuchMethodError comtypesafeconfigConfiggetDurationLjavalangStringLjavautilconcurrentTimeUnitJ at glintClientstartClientscala at glintClientapplyClientscala at glintClientapplyClientscala at iwCiwCiwCiwCiwCiwCiwCiwCinitconsole at iwCiwCiwCiwCiwCiwCiwCinitconsole at iwCiwCiwCiwCiwCiwCinitconsole at iwCiwCiwCiwCiwCinitconsole at iwCiwCiwCiwCinitconsole at iwCiwCiwCinitconsole at iwCiwCinitconsole at iwCinitconsole at initconsole at initconsole at clinitconsole at initconsole at clinitconsole at printconsole at sunreflectNativeMethodAccessorImplinvoke Native Method at sunreflectNativeMethodAccessorImplinvokeNativeMethodAccessorImpljava at sunreflectDelegatingMethodAccessorImplinvokeDelegatingMethodAccessorImpljava at javalangreflectMethodinvokeMethodjava at orgapachesparkreplSparkIMainReadEvalPrintcallSparkIMainscala at orgapachesparkreplSparkIMainRequestloadAndRunSparkIMainscala at orgapachesparkreplSparkIMainloadAndRunReq SparkIMainscala at orgapachesparkreplSparkIMaininterpretSparkIMainscala at orgapachesparkreplSparkIMaininterpretSparkIMainscala at orgapachesparkreplSparkILoopreallyInterpret SparkILoopscala at orgapachesparkreplSparkILoopinterpretStartingWithSparkILoopscala at orgapachesparkreplSparkILoopcommandSparkILoopscala at orgapachesparkreplSparkILoopprocessLine SparkILoopscala at orgapachesparkreplSparkILoopinnerLoop SparkILoopscala at orgapachesparkreplSparkILooporgapachesparkreplSparkILooploopSparkILoopscala at orgapachesparkreplSparkILoopanonfunorgapachesparkreplSparkILoopprocess applymcZspSparkILoopscala at orgapachesparkreplSparkILoopanonfunorgapachesparkreplSparkILoopprocess applySparkILoopscala at orgapachesparkreplSparkILoopanonfunorgapachesparkreplSparkILoopprocess applySparkILoopscala at scalatoolsnscutilScalaClassLoadersavingContextLoaderScalaClassLoaderscala at orgapachesparkreplSparkILooporgapachesparkreplSparkILoopprocessSparkILoopscala at orgapachesparkreplSparkILoopprocessSparkILoopscala at orgapachesparkreplMainmainMainscala at orgapachesparkreplMainmainMainscala at sunreflectNativeMethodAccessorImplinvoke Native Method at sunreflectNativeMethodAccessorImplinvokeNativeMethodAccessorImpljava at sunreflectDelegatingMethodAccessorImplinvokeDelegatingMethodAccessorImpljava at javalangreflectMethodinvokeMethodjava at orgapachesparkdeploySparkSubmitorgapachesparkdeploySparkSubmitrunMainSparkSubmitscala at orgapachesparkdeploySparkSubmitdoRunMain SparkSubmitscala at orgapachesparkdeploySparkSubmitsubmitSparkSubmitscala at orgapachesparkdeploySparkSubmitmainSparkSubmitscala at orgapachesparkdeploySparkSubmitmainSparkSubmitscala Hey Rolf I am struggling to setup glint on a standalone spark cluster i am following your tutorial with one master and one server and i use the standard conf from your repo I can manipulate the bigVector fine on the master node but my futures time out if I try a push within and RDD foreach statement so the client seems to be running fine but i cannot be accessed from the worker nodes thanks christian Hi rjagemen Could you please help me to review the request All codes are tested on online in my cluster environment Any question is welcome and appreciate your previous work Thanks I am trying this implementation of Glint with Apache Flink I am rather new to Akka and can not figure out the following error message javalangNoSuchMethodError akkapatternAskableActorRefqmarkdefault extensionLakkaactorActorRefLjavalangObjectLakkaactorActorRef at glintClientinitClientscala at glintClientanonfunstart applyClientscala at glintClientanonfunstart applyClientscala at scalaconcurrentFutureanonfunflatMap applyFuturescala at scalaconcurrentFutureanonfunflatMap applyFuturescala at scalaconcurrentimplCallbackRunnablerunPromisescala at scalaconcurrentimplExecutionContextImplAdaptedForkJoinTaskexecExecutionContextImplscala at scalaconcurrentforkjoinForkJoinTaskdoExecForkJoinTaskjava at scalaconcurrentforkjoinForkJoinPoolWorkQueuerunTaskForkJoinPooljava at scalaconcurrentforkjoinForkJoinPoolrunWorkerForkJoinPooljava at scalaconcurrentforkjoinForkJoinWorkerThreadrunForkJoinWorkerThreadjava This message appears simply when I try to create a client ie val gc Client Is it possible that this message is caused because Apache Flink uses a different version of Akka that Glint and somehow Glint is using the Akka version that Apache Flink has imported If so how can I ensure that the right version of Akka is called for Glint and Apache Flink respectively Thanks In parameter server scenario I think network delay is always an important issue However each BigMatrix push and pull demand a tuple of size But row indices and col indices could be merge into one indices It will reduce about network consumption Also the row index must be Long is also a limit Not always Long is needed I have been playing around with glint for a few weeks and am excited about its potential However the counts I want to eventually push to the parameter server are larger than the max value of type Long I see there isnt any support for BigInts Wondering if there is any reason for this BigMatrix supports pulling a set of row indices ie matrixpullrows Array Long For push only matrixpushrows Array Long cols Array Int values Array V is supported For common use cases of pullpush a set of rows this effectively doubles the set of indices that must be sent entries per value Though Im not sure if it makes that much difference on performance it may add a bit to the comm cost of large pushes for ease of use it would be useful to be able to push a set of rows in the same manner as pull 