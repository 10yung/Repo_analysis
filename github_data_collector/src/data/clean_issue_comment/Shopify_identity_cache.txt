gmcgibbon Here is the CacheKeyLoaderbatchload implementation I mentioned It can be used to replace loadmulti which I have done temporarily for testing however we can keep the loadmulti implementation from as an optimization for the case where we are loading for a single cache fetcher Problem We want to be able to fetch cache keys across cache fetchers to reduce round trips for id embedded associations This means we need a common IdentityCachefetchmulti but then different cache keys will need to be resolved by different cache fetchers Solution Document a common interface for cache fetchers so that they can be used to batch load they cache key fetching together while also being able to resolve misses Ive moved the generic cache key loading logic into a CacheKeyLoader singleton object which can be extended with a batchload method once we have some code to use it Ive opened up the draft PR showing an implementation of CacheKeyLoaderbatchload Problem I would like a common cache fetcher interface that can be implemented for the primary index and cache attribute fetches However loading by a single key or a multiple key works differently for cached attributes since it doesnt support a fetch multi by a composite key Solution Split CachedAttribute into CachedAttributeByOne and CachedAttributeByMulti sharing a common base class Only CachedAttributeByOne of these supports fetchmulti I have made a new rails app with these models on the latest master of identitycache modelsitemrb ruby class Item ApplicationRecord include IdentityCache belongsto widget polymorphic true cachebelongsto widget cacheindex widget end modelsfoowidgetrb ruby class FooWidget ApplicationRecord include IdentityCache hasmany items as widget cachehasmany items inverse widget end modelsbarwidgetrb ruby class BarWidget ApplicationRecord include IdentityCache hasmany items as widget cachehasmany items inverse widget end schema ruby class AddTables ActiveRecordMigration def change createtable items do t tbigint widgetid null false tstring widgettype null false end createtable foowidgets createtable barwidgets addindex items i widgettype widgetid end end I would like to be able to run this ruby widgets FooWidgetfetchmulti items Itemfetchmultibywidgetwidgets But that generates an invalid SQL query ActiveRecordStatementInvalid SQLite SQLException no such column widget SELECT widget itemsid FROM items WHERE itemswidgettype AND itemswidgetid IN I cant change my cacheindex to cacheindex widgetid widgettype because fetchmultiby methods are only generated for cacheindexes with only one field I used to be able to pass in arrays into the fetchbywidgetidandsubjecttype methods in but now that doesnt work on master eg an array of widgetids generates a SQL query with NULL as the widgetid value I guess was never the way those methods were intended to be used but now I dont have a way to bulk fetch Items in a way that wont make N queries when the cache is missed I have a model where Im including IdentityCache lets call it Foo I dont necessarily want this model and all of its attributes cached but I do want its relationship cached I have a relationship defined like cachebelongsto thing The cache works just fine for doing things like calling foofetchthing but I noticed that when an attribute for the Foo model is updated I get logging that looks like IdentityCache expiringFoo expiringid and then more output saying that a delete failed for IDC blobFoo So I guess my questions are By including the module is Foo and its attributes being stored into cache whenever its called Any ideas why a delete failed would be consistently called when updating one of Foos attributes Is there a way to prevent Foo from being cached since I only want its association to be cached Given the following classes ruby class A ARBase hasmany bs inverse a cachehasmanybs embed true end class B ARBase belongsto a inverse bs hasone c cachehasonec embed true end class C ARBase hasmany b inverse c end when saving an instance of C B gets removed from the cache but not A Thus Bfetchc returns the latest version but Afetchbs return a stale collection I could reproduce this scenario as a failing test Am I missing something or is this a bug The issues here that talk about redis support are a little confusing Now that Rails includes RedisCacheStore can that be used out of the box with identitycache Alternatively were already using the redisrails gem for our caching can we use this as a backend for identitycache While debugging N queries I found an unusual behaviour in IDC when fetching records using a cached attribute IDC produces two queries instead of one The first query uses the cached attribute while the second uses the primary key After some investigation and some very helpful tips from coworkers I managed to boil the issue down to this snippet ruby migration file class CreateSampleRecords ActiveRecordMigration markaspoddedmigration def up createtable samplerecords do t tstring key ttimestamps end end def down droptable samplerecords end end ActiveRecord model class SampleRecord ApplicationRecord include IdentityCache cacheindex key end Then on rails console prymain SampleRecordcreatekey ms BEGIN SampleRecord Create ms INSERT INTO samplerecords key VALUES ms COMMIT SampleRecord x fa a id key prymain SampleRecordfetchbykey ms SELECT samplerecordsid FROM samplerecords WHERE samplerecordskey SampleRecord Load ms SELECT samplerecords FROM samplerecords WHERE samplerecordsid SampleRecord x fa f a id key I also tried with unique true it produces the same result I might be wrong but I would expect only one query to be produced by this snippet The first query already fetches the desired record I see no reason to search for the object again using the primary key Thoughts cc csfrancis Problem Currently there is a thundering herd problem when a cache key is invalidated and multiple readers get a cache miss before the first one fills the cache All the readers after the first one are redundant and this can cause a lot of load on the database especially for records with a lot of embedded associations Solution Add support for using a fill lock when using the fetch and fetchbyid class methods so the first cache miss takes a lock before filling the cache and following clients getting a cache miss do a lock wait for a short duration for the cache to be filled so they can read from the cache rather than from the database This feature is optin through specifying the filllockduration option to control how long to wait on the fill lock before reading the cache key again This should be set to just over the typical amount of time it takes to do a cache fill This way we typically will only have a single client fill the cache After a lock wait if the lock is still present eg hasnt been replaced by the value then the client doing the lock wait will try to take the lock and fill the cache value If the original lock is replaced by another clients lock then that can cause other clients to do another lock wait to avoid a slow cache fill from resulting in a thundering herd problem The lockwaitlimit option controls how many times a client should wait for different locks before raising an IdentityCacheLockWaitTimeout exception It should be set to the maximum amount of time it would take for the cache to be filled outside of a service disruption causing the database to be excessively slow so that we dont waste server capacity when there are unexpected delays There is no performance impact for cache hits or cache invalidations There is no impact on performance for cache misses when not using the fill lock When using the fill lock the client getting the first cache miss will make an extra cas memcached operation take the lock and an extra get operation that is needed to get the CAS token as part of the cache fill However during a thundering herd scenario most of the clients will only do get memcached operations to read the lock then read the value after the lock wait Cache invalidations could happen while a cache miss is being resolved which could prevent clients that are in a lock wait from reading the value after the lock wait In this case a fallback key is used so that the client resolving the cache miss can store the value someplace where waiting clients can read it A data version that is stored in the lock value is used to derive the fallback key so that it isnt filled with stale data from the perspective of the client reading it The fallback key isnt normally written to during a cache fill so a cache invalidation might also overwrite the value before the lock wait finishes but this will just cause the waiting clients to take a fill lock on the fallback key which wont be overwritten by further cache invalidations If a key happens to be both read and write heavy then the number of concurrent cache fills is still limited by the frequency of cache invalidations due to the fill lock being taken on the fallback key If memcached is down then lock waits are avoided and the database will be used without a cache fill If database exceptions interrupt the cache fill then the fill failure is marked on the lock so other clients dont wait on the lock when they would otherwise fail fast by querying the database themselves Backwards compatibility This could be rolled out in a backwards and forwards compatible way by first deploying a commit to bump the gem so that it can handle the lock values then using a following deploy to start using the fill lock Alternatives The lock wait was implemented by just using a sleep so clients could sleep longer than necessary after the cache is filled One alternative would be to use redis which supports blocking operations eg BRPOPLPUSH so the value can be returned to other clients as soon as its available However this could put a lot of additional load on redis to write to it for all cache fills and would add a new dependency that we would have to worry about failures for Another alternative would be to keep stale values around after a cache invalidation so that we could fallback to stale data on a cache miss when the fill lock is taken This is what we do in the cacheable gem However we would need to be careful with stale reads to make sure we dont fill another cache from the stale read eg page cache and to make sure we dont do stale reads where we cant tolerate stale data This would also reduce where we could use this solution There would also be a performance tradeoff on cache invalidations if we needed to preserve the stale data while also marking it as stale Writethrough caching is another alternative However this could add a lot of overhead to writes in order to fill the cache especially when there are embedded associations If there are concurrent writes then this could interrupt the cas operation needed to set the new value in the cache which would need to be resolved by retrying using freshly queried database data This could have significant impact on models that are sometimes written heavily even if in other cases they are readheavy The significantly different tradeoffs seem like they would reduce where we could use this solution Writethrough caching seems like something that could be done a lot more efficiently by using the mysql binlog which is a longer term project Not Implemented This PR doesnt implement support for fetchmulti and doesnt expose a way to provide a fill lock in all methods that do fetching However we could expand support for this feature on an as needed basis Hi Im trying to integrate kaminari with identitycache As the fetch method returns the records in Array I cant able to restrict the records on pagewise Any good practices to implement it Thank you