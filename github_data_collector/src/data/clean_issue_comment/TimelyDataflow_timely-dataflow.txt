Im not certain this is a real issue but the eventdriven example can be made to create dataflows each a sequence of nodes and doing so takes about s on my modern desktop That seems like a long time to put together what is meant to be a linear amount of information It does seem to be linear scaling up either number by changes the time by but it would still be good to get a sense for what is going on Initial investigation suggests allocations in reachability which indeed does have a bunch of small peroperator allocations Perhaps those could be consolidated somehow At the moment dataflows only shut down through clean completion of the work within them One releases all capabilities at inputs and awaits the draining of work within the dataflow One can imagine situations where illconceived dataflows start up that will create more work than the system can expect to handle and we should have a way to terminate these dataflows without awaiting their successful completion It seems like most of the dataflows are instantiated by trees of owned data with connection points back to the communication infrastructure In principle we could drop these dataflows and a fair amount of collection will happen automatically This is probably an idealized view of shutdown and there are several issues one can already expect For example if the channels associated with a dataflow go away on the receipt of messages for these channels the system cannot currently distinguish between an old dataflow that has been shut down and a new dataflow that has not yet started up The system will reconstruct these channels stash the messages and await the dataflow construction to collect them This wont happen and the effect would be a memory leak We also have existed in the pleasant world where dataflows are only shut down after the system is certain there will not be additional messages and so there may be cases where postshutdown message receipt causes more egregious errors up through panicking At the moment we just dont know This issue also relates to the use of capability multisets in that use case is for messages with empty capability sets which also have the problem that they may exist beyond the lifetime of a dataflow one could learn that the dataflow can be terminated all capabilities are gone and yet still receive messages without capabilities also currently resulting in memory leaks or worse A fix here might remove that blocker for multiset capabilities One candidate solution is to inform the communication plane of shut down channels and then to discard messages for these channels The current channel allocation mechanism uses sequentially assigned identifiers and the system should be able to distinguish between yet to be created and created and terminated channels just by looking at the identifiers and comparing them with the nexttobeassigned identifier Messages for old identifiers without an associated channel should simply be discarded Dataflow destruction should also happen on all workers but it seems like there isnt the same urgency that it happens in exactly the same order on all workers Dataflows shut down on a subset of workers should ideally just block dataflow execution on other workers rather than result in unrecoverable errors Hi Frank For profiling progress visualizing dataflow frontiers I believe I need some information in ProgressEvents on what kind of progress actually happened I think that the messages and internal fields could help me with that Currently they are only mocked up in send and receive Id be happy to create a PR and implement them if you can guide me a little on what the Vec fields are supposed to be Eg what are target and source descriptor in this context I assume they are the first two usize coordinates in the Vecs tuples Cheers Malte rust pub struct ProgressEvent List of message updates containing Target descriptor timestamp as string and delta pub messages Vecusize usize String i List of capability updates containing Source descriptor timestamp as string and delta pub internal Vecusize usize String i Hi frankmcsherry I have set up timely as a longtime service to server queries But the problem is that in timely all dataflows share the same computation resources which will lead to a long execution time for a simple dataflow when there is a big dataflow is operated at the same time So i think if you have an idea to address this problem I am learning Timely now I have run it in a distributed environment After each worker received and processed its data I want to merge the data among workers in each machine at the end of dataflow excution Does anyone know how to do it The message counter that uses a Rust MPSC can panic in shutdown if its attempt to report messages fails presumably because the other side has hung up It isnt immediately obvious why the other end is shutting down if there are still messages to read so perhaps there is more to diagnose here but weve seen this in the wild in tests Some more discipline about error cases and intended invariants would help here If a user creates an InputHandle and advances it before introducing to a dataflow the construction of the corresponding input operator will not use the new advanced capability This leads to a bit of a mess where initial invariants are not satisfied Should be an easy fix to pull this info out of the handle at construction time and initialize the input operator correctly Hello Im trying to grasp how the timely dataflow is meant to be used kind of end to end The documentation concentrates on how to do and structure the computation and its good at explaining that But I kind of still dont get how to connect it to the rest of the world To illustrate what I mean lets say I have a huge source of data lets say live stream of log messages or metrics or something like that So the whole thing will do something like this I have bunch of worker processes across machines They either read a partition of the stream each or one master reads them and inserts them into the system and itll distribute through the workers It performs the computation producing lets say events of style this service entered alert state and this service left alert state However lets say I want a static web page containing all the services in alert state at the currently newest timestamp This sounds challenging because The information about the state of different machines is scattered through the cluster not at one place They may be coming out of order OK that probably can be somehow worked around by buffering in a custom operator I guess I could somehow force my way through it but all the ideas I have in mind look like huge hacks Is there a correct way to do such things Should it be part of the book Because all the exporting of the results is done by println thatll end up in a random worker right Im currently reading timelys gitbook and so far it has been great Here are some suggestions on what to improve most examples start with extern crate timely I think this isnt necessary for Rust anymore The first code example in the iteration chapter doesnt match its surrounding description Eg LoopVariable is introduced but the example uses feedback i what is the upper bound on the number of iterations isnt reflected in the function arguments and Weve built an upper limit of doesnt exist in the example code Subsection Scopes clears this up a little but perhaps it might make sense to present the difference between feedback loopvariable earlier esp since loopvariable just uses feedback under the hood Many concepts are defined independently but never compared or connected to each other eg how capabilitydowngrade flushing relates to inputhandleadvanceto how inputhandletime relates to frontier how the source we wrote relates to providing input via an InputHandle and what the differences between InputHandle outside of a dataflow the one with time and the FrontieredInputHandle within operator closures the one with frontier and foreach are Maybe a short summary glossary comparison of concepts at the end of Chapter could make this clearer Lastly I think it might be confusing that as far as I saw its nowhere mentioned that timely cleans up when dataflows go out of scope That way especially when writing small programs youll always see some output even when you never advanced time or called workerstep Ive added use stdiostdin stdinreadline mut Stringnewunwrap at the end of the execute closure to make sure nothing is cleaned up without me interrupting the program so that I can better observe my changes Best Malte