 How to solve it Does anyone get the same results as in the paper on WMT ende dataset Who can tell me where this path can be changed I have installed this package but I dont know where to change the path below Thank you Hi I saw that you slightly changed your code and now write selfwqs nnLineardmodel nhead dk biasFalse Can you tell me the reason for using biasFalse I first thought you changed it in order for the zeros in the input to be preserved Although paddingidx is used in nnEmbedding you afterwards add the positional encoding which leads to all values in the input to be nonzero Thanks in advance for your reply could you add the bleu score result on IWSLT deen dataset for fast comparable result with other implementation thanks for your great work Let me check one point about heads order when extracting and visualizing attention weight I personally checked internal behavior in MultiheadAttention first prepare temporary params and tensor for test as below python params minibatch dmodel seqlen nhead dk dv dummy inputs for check ipt torchrandnminibatch seqlen dmodel torchSize and checked the size in before changing tensor size of query key and value and after that in MultiheadAttention python class MultiHeadAttentionnnModule MultiHead Attention module def initself nhead dmodel dk dv dropout superinit def forwardself q k v maskNone dk dv nhead selfdk selfdv selfnhead szb lenq qsize szb lenk ksize szb lenv vsize residual q q selfwqsqviewszb lenq nhead dk k selfwkskviewszb lenk nhead dk v selfwvsvviewszb lenv nhead dv printqsize ksize vsize printq q qpermute contiguousview lenq dk nb x lq x dk k kpermute contiguousview lenk dk nb x lk x dk v vpermute contiguousview lenv dv nb x lv x dv printqsize ksize vsize printq printq and output is torchSize torchSize torchSize batch seqposition head batch seqposition head tensor gradfnSelectBackward torchSize torchSize torchSize batch head seqposition batch head seqposition tensor gradfnSelectBackward tensor gradfnSelectBackward It looks like next head is following one batchsorry it is so ambiguous expression so I interpreted like that after view output tensor shape is like below head batch head batch head batchminibatchsize head batch head batch so I visualize attention weight of two head for first sequence of batch like below python attnweight modelencdecoderforward inputs returnattnsTrue printattnweight first head printattnweight batchsize second head is this collect Please how to add Attention weight plot to this code Thanks in advance Thank you for the code By default Pytorch linear layers use uniform Kaiming initialization which is meant to be used before a ReLU activation with uniform inputs However this initialization is used at the output projections of the attention and feedforward submodules With this initialization training loss does not decrease on the Ubuntu conversation corpus I found that switching to Xavier smaller init lead to convergence while even smaller values xavier lead to faster convergence This is not surprising considering each submodule uses skip connections xfx so smaller outputs lead to a similar variance between the input and output of a given Transformer layer which is what these initialization methods aim to achieve The LayerNorm corrects any changes in activation variance but this does not stop layer outputs from initializing as increasingly nonlinear early in training which can complicate gradients Also I dont believe Attention Is All You Need mentions initialization but I could be wrong Thanks What is the shape of the parameter wordprob that is input to the method advance in the beam search module I am wondering what numwords is Is this the target vocab size Also why do we get the beam a score belongs to by computing bestscoreids numwords 