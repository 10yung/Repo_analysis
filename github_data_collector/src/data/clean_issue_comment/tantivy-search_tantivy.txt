Updates the requirements on time to permit the latest version details summaryRelease notessummary pemSourced from a href releasesaemp blockquote h v h h Bugs fixedh pa href has been fixedp blockquote details details summaryCommitssummary ul liSee full diff in a href viewali ul details br Dependabot will resolve any conflicts with this PR as long as you dont alter it yourself You can also trigger a rebase manually by commenting dependabot rebase dependabotautomergestart dependabotautomergeend details summaryDependabot commands and optionssummary br You can trigger Dependabot actions by commenting on this PR dependabot rebase will rebase this PR dependabot recreate will recreate this PR overwriting any edits that have been made to it dependabot merge will merge this PR after your CI passes on it dependabot squash and merge will squash and merge this PR after your CI passes on it dependabot cancel merge will cancel a previously requested merge and block automerging dependabot reopen will reopen this PR if it is closed dependabot close will close this PR and stop Dependabot recreating it You can achieve the same result by closing it manually dependabot ignore this major version will close this PR and stop Dependabot creating any more for this major version unless you reopen the PR or upgrade to it yourself dependabot ignore this minor version will close this PR and stop Dependabot creating any more for this minor version unless you reopen the PR or upgrade to it yourself dependabot ignore this dependency will close this PR and stop Dependabot creating any more for this dependency unless you reopen the PR or upgrade to it yourself dependabot use these labels will set the current labels as the default for future PRs for this repo and language dependabot use these reviewers will set the current reviewers as the default for future PRs for this repo and language dependabot use these assignees will set the current assignees as the default for future PRs for this repo and language dependabot use this milestone will set the current milestone as the default for future PRs for this repo and language dependabot badge me will comment on this PR with code to add a Dependabot enabled badge to your readme Additionally you can set the following in your Dependabot dashboard Update frequency including time of day and day of week Pull request limits per update run andor open at any time Automerge options neverpatchminor and devruntime dependencies Outofrange updates receive only lockfile updates if desired Security updates receive only security updates if desired details This PR implements what was briefly discussed on Gitter a ReadOnlySource implementation that uses a Read Seek based API Please do note that this is for now a initial implementation a proof of concept Plenty of places do questionable things or are a bit sloppy and need to be improved but it can serve for us as a starting point for discussion That being said it does work correctly with the current Directory implementations and passes all tests The motivation for a Read Seek based implementation is this Directory implementation that transparently encryptsdecrypts files that Tantivy readswrites This could be quite important for client applications that provide full text search over sensitive data I am sure that there are more usecases for such an Read Seek based API and making the Directory more flexible is a welcome improvement A downside of this API is that allocating memory buffers and copying data is necessary instead of the direct access that we get with the slice based approach This seems like a reasonable start to this Would close but Id imagine we want to do some more work here This ticket is about fixing the heuristic of the LogMergePolicy There is actually two different problems in this ticket Problem The relative benefit of merging segments diminishes as the segment gets bigger and bigger We should add a configurable maxsegmentsize expressed in number of documents to limit the size of two segments that can be considered for merge Problem The current merge policy only takes in account the size in number of documents to merge two segments We do not take in account the number of deleted documents This can lead to the following problem A very large segment is created and the index consists in this large segment and a lot of small segments Until the small segments get merged up to the point when minmergesize large segment are created the large segment never gets merged This can take a very long time eg months In the meanwhile deletes also happen and this mammoth segment may actually contain a very large ratio of deletes The heuristic should be changed to take in account the ratio of deleted file to avoid this problem Tantivy needs a thread pool to handle documents which added by users adddocument thread for each index writer not users adddocument thread direactly like lucene I think it will needs a lot of threads when indexing multiple indices which will cause high load averge I have been asked about the scoring algorithm that tantivy uses and realised that neither I nor the documentation have a canonical description for it apart from The larger the number the more relevant the document to the search I think it will be great to add more information and run through an example query on an index to show why queries return results in that order and how a user might debug specific queries Who do we expect to read this People building a fulltext search engine are interested in efficiently storing and ranking documents against queries The score of each document is arguably THE most important data type that we return to users in every query I expect most users of tantivy will want to read about the Score type at one point or another types of users knowledgeable about building search engines and wants to confirm the validity of tantivys scoring algorithm expect to see tfidf BM and other known someone for whom tantivy might be the first experience building a search application with little background on document scoring want answers to specific questions and some further reading material Questions these users want to answer Why are search results in this order What is this score field Why is it a float How does each subquery in the full query eg q titlepresident AND bodyObama OR bodybarack AND year contribute to the final score of a document I want to boostexpected a specific document higher up in the set of results for a given query how do I do that Suggested style of documentation Prose A detailed highlevel explanation for document scoring how is each query scored how are scores of different subqueries combined Code doctest doesnt need to asserttest anything that walks through an example of debugging a unexpectedly lowranking document using Queryexplain and showing how the example query can be rewritten Provide further reading material Give links to tfidf BM wikipedia pages and the Queryexplain method If you do this ticket you will learn The full lifecycle of a tantivy query from query to score per document tantivy helper methods for debugging such queries writing concise yet informative documentation for powerusers and amateurs at the same time If an application has more than one server having one thread pool per index is not a good idea We should probably externalize the executor so that it can used with several IndexReader Tantivy is great Were developing a ESlikemuch simpler and specific to our needs system based on tantivy Currently were doing some optimization and we need some info about the format of various tantivy files like idx and term Is there any doc available or should we read some source code Thanks Two users requested to easily access the Top K int values of an index A typical usage could be to display the last N documents of a mailbox for instance This ticket requires to tackle first I want to get the largest number from a indexed u filed but from the API seems I need first get the filed count then use RangeQuery to search Or is there other more efficient way which more like equivalent to order by filed desc limit in sql let count let query RangeQuerynewu field count count searchersearch query TopDocswithlimit searchersearch AllQuery TopDocswithlimit this will scan all the docs let query RangeQuerynewu field is there any implementation like this 