title says it all Hello I really like Figaro thanks for it I use Figaro for several internal projects I even got practicalprobabilisticprogramming from Manning but there are a few questions Is this project dead Are there any plans for a future release Dear Figaro Team No luck when installing Figaro on Mac OS X both with Scala and No installation is successful Error message The application figaro osxinstallerapp can not be opened It may be damaged or incomplete Tried a few times on different macs and no success Please check if anyone can install these on their computers Thanks Best I was poking around but could not find anything does Figaro include any test for comparing whether two elements are generating from the same distribution could be KL divergence but pretty much anything will do for me that reasonably reliably detects if we are dealing with the same distribution Hi team I just wrote this wiki entry but Im not sure whether that results in a notification to any committers so I wanted to bring it to your attention more explicitly My apologies if this isnt the appropriate process Downloading the binary figaro linuxx installerrun gives a corrupted file Im a beginer in figaro and my IDE is Intellij when I run a simple code without printlnVariableEliminationprobability I dont have any error but when I use it this is my program for testing import comcrafigarolanguage import comcrafigaroalgorithmfactoredVariableElimination object testFigaro def mainargs Array String val sunnyToday Flip printlnVariableEliminationprobabilitysunnyToday true And this is error Exception in thread main javalangNoClassDefFoundError scalareflectruntimepackage at comcrafigaroalgorithmfactoredProbQueryVariableEliminationmakeResultFactorVariableEliminationscala at comcrafigaroalgorithmfactoredProbQueryVariableEliminationfinishVariableEliminationscala at comcrafigaroalgorithmfactoredVariableEliminationanonfundoElimination VariableEliminationscala at scalaruntimejava JFunction mcVspapplyJFunction mcVspjava at comcrafigaroalgorithmfactoredVariableEliminationoptionallyShowTimingVariableEliminationscala at comcrafigaroalgorithmfactoredVariableEliminationdoEliminationVariableEliminationscala at comcrafigaroalgorithmfactoredVariableEliminationdoEliminationVariableEliminationscala at comcrafigaroalgorithmfactoredProbQueryVariableEliminationdoEliminationVariableEliminationscala at comcrafigaroalgorithmfactoredVariableEliminationveVariableEliminationscala at comcrafigaroalgorithmfactoredVariableEliminationveVariableEliminationscala at comcrafigaroalgorithmfactoredProbQueryVariableEliminationveVariableEliminationscala at comcrafigaroalgorithmfactoredVariableEliminationrunVariableEliminationscala at comcrafigaroalgorithmfactoredVariableEliminationrunVariableEliminationscala at comcrafigaroalgorithmfactoredProbQueryVariableEliminationrunVariableEliminationscala at comcrafigaroalgorithmOneTimedoStartOneTimescala at comcrafigaroalgorithmOneTimedoStartOneTimescala at comcrafigaroalgorithmfactoredProbQueryVariableEliminationdoStartVariableEliminationscala at comcrafigaroalgorithmAlgorithmstartAlgorithmscala at comcrafigaroalgorithmAlgorithmstartAlgorithmscala at comcrafigaroalgorithmfactoredProbQueryVariableEliminationstartVariableEliminationscala at comcrafigaroalgorithmfactoredVariableEliminationprobabilityVariableEliminationscala at comcrafigaroalgorithmfactoredVariableEliminationprobabilityVariableEliminationscala at testFigaromaintestFigaroscala at testFigaromaintestFigaroscala Caused by javalangClassNotFoundException scalareflectruntimepackage at javanetURLClassLoaderfindClassURLClassLoaderjava at javalangClassLoaderloadClassClassLoaderjava at sunmiscLauncherAppClassLoaderloadClassLauncherjava at javalangClassLoaderloadClassClassLoaderjava more Please help me What should I do to deal with This issue is partly trying to raise a red flag and partly trying to grasp why Figaros learning logic works the way it does The way it works appears to hinder our efforts at machine learning by introducing statistical insignificance for observations regarding less than oftenly relevant or active cases in our model Well try to provide a specific boiled down example Consider the following Figaro model class Modelparameters ParameterCollection modelUniverse Universe val flip Flip myFlip modelUniverse val weightTrue FlipparametersgetweightTrueelemweightTrue modelUniverse val weightFalse FlipparametersgetweightFalseelemweightFalse modelUniverse val cpd RichCPDflip OneOftrue weightTrue weightFalseelemcpd modelUniverse Note that the model contains only two regular elements cpd and flip and the CPD is constructed using two different weights one for the case when flip is true and one for when it is false The parameters are both initialized as Beta elements postulating the fact that we have no prior beliefs regarding the parameters and we place these Beta elements inside the ModelParameters construct Next consider the following observations taking place for i to val model new ModelmodelParameterspriorParameters universe modelflipobservetrue modelcpdobservetrue At this point weve learned that the cpd element very oftenly turns out true given that flip is also true But what about cpds probability of being true when flip is false Indeed we have had no observations that signify this case for us Accordingly we get the following outcomes for the parameters MAP values weightTrue weightFalse First question What is up with the counting of the alpha and beta values of weightFalse here As mentioned weve not seen a single sample of what probability cpd should have when flip is false Even so it would appear that Figaro records cases on either side This gives us a probability of which is actually fine for now but what happens when we also start observing things about the case where flip is observed to be false Lets take a look We add a single additional Modelinstance with observations val model new ModelmodelParameterspriorParameters universe modelflipobservefalse modelcpdobservetrue After this additional evidence is supplied we expect the following We will maintain the same probability of cpd being true when flip is known to be true namely This is what we expect since the new observation does not fit into the bucket of cases where flip is true We will expect that the probability of cpd being true when flip is known to be false to change from unknown Beta no prior beliefs to having seen exactly one case to support the claim and none for the opposite case cpd being false when flip is false However here are the actual results weightTrue weightFalse Second question Why is it that our belief of weightFalse is not here Weve seen exactly one instance where flip was false thus activating or making relevant this weight Why is all this noise of the other irrelevant instances present Third question Similarly for weightTrue Why does this latest case affect weightTrue Even in such an odd manner to add almost to its learnedAlpha and nearly nothing to its learnedBeta Notes We used EMWithVE regular training for this test setup Can provide the specific sourcecode if desired Using Online EM makes no difference Building the model without RichCPD eg with Ifconstructs makes no difference Regardless of whether elements are declared before the If or inside of it when testing whether the laziness of If mattered Taking a broader perspective In our actual model we are dealing with a larger model which contains more CPDs as in this fashion Essentially the effect of this odd behaviour has various effects on our training and thus our evaluations All of our learned parameters are flooded by the statistical insignificance introduced in the same way as in the example above All of our parameters are heavily guided by whatever prior beliefs we supply to the Betaelements when initializing them Basically Beta isnt a special case where a problem exists Say we initialize a parameter with Beta namely positive case and negative then with no relevant cases observed we get as expected but not with learnedAlpha and learnedBeta values of and as expected but instead with modelinstances with observations learnedAlpha and learnedBeta This is concerning since this reproduces the problem examplified above no matter what prior beliefs are passed on since the significance of one actually relevant data instance wth observations is naturaly extremely low when compared to all data instances with observations in total In general we fear that this voids our usage of the builtin machine learning setup via the EMalgorithm in Figaro In case our concerns are actually what is void we would very much like an explanation that either a defends the results observed with backing in some theoretical foundation that we appear to be missing or b suggests an error in our usage of Figaro which makes it act strangely along with a concrete modification to the given example which will make Figaro behave as we expect We highly appreciate any input anyone may have toward a resolution for this problem Thank you very much Best regards Hopeful students of the IT University of Copenhagen Christian and Kenneth chnritudk and kulritudk Since you helped us before I am going to go ahead and add a tag for you apfeffer Hi We are doing our masters thesis at the IT University of Copenhagen and we have a series of questions that we hope there exists some useful answers for We are working with a setup very similar to the spamfilter application case from chapter in the Practical Probabilistic Programming book and our questions regard the efficiency of learning for such a model In essence we have several model instances which all reference some shared Betaelements for learning which in effect results in quite a large net of connected elements We are looking to be able to perform the learning of our Betaelements but without having to evaluate the entire network of connected elements at once but instead train and learn from each individual model instance one at a time instead Here are some more specific questions Why does EMWithVE use a completely different setup ExpectationMaximizationWithFactors compared to the other inference algorithms when used with EM What are the optimizations differences that apply here and is there some litterature that you could point us to that would help us understand some of the differences If we attempt use GeneralizedEM with VE it seems that that all active elements in the universe thereby all our connected model instances are passed as inputs to the inference algorithm As the amount of model instances increases this quickly becomes infeasible for an algorithm such as VE If we consider the spam filter case from Chapter would it not be possible to use the inference algorithm on each sample separately and then combine their results during the expectation step rather than attempting to calculate the sufficient statistics for all model instances elements all at once We figured that this splittingapproach might be feasible with VE if each individual model instance is not very complex and also have the added benefit of being parallelizable since each sample can be reasoned about separately if we can use StructuredVE for the task Is there a reason why this approach is not used Is it not feasible If it is possible could you provide some pointers for how we can achieve this goal To bring about our perspective we are trying to optimize our trainingsetup for our thesis work such that an alteration to the probabilistic model will take a little time as possible to see the effect of both in regards to training and of course evaluation The setup with our model instances getting tangled into each other due to the shared Betaelements seems to meddle with the efficiency of most inference algorithms in Figaro that are usable with EM Is there some other approach that we could go with as an alternate setup As another note we believe that we are able to build our model in such a fashion that we should have little to no hidden variables namely in the learningcase and only a single one in the evaluationphase which should help the efficiency of whatever inference algorithm we end up with Also according to litterature if one has no hidden variables then you are in fact in the complete data case meaning that Maximum Likelihood Estimation should be feasible for the problem namely the simple learning of the frequencies of ones dataset rather than requiring the use of EM Is there some way to access the MLE logic that is used as part of the EMalgorithm from somewhere in the source code Thanks a lot Hoping the best Best regards Christian and Kenneth Students at the IT University of Copenhagen Denmark Hi I am new to Figaro and I would like to create a model that will be used in more than one actor In fact I have created a router with workers They execute in different threads as you know Each worker actor creates new instance of a class that creates the model all variables then make evidences and Inferences with VariableElimination When I create only one worker that is there is no multithreading it works perfectly Bu with more than one I got this error key not found Select visit novisit I have implemented the well known ChestClinic model like this val universe UniversecreateNew val worldTravel Select visit novisit sworldTravelmsgIndex universe val smoking Select smoker nosmoker ssmokingmsgIndex universe val tuberculosis CPDworldTravel visit Select present absent novisit Select present absent stuberculosismsgIndex universe val lungCancer CPDsmoking smoker Select present absent nosmoker Select present absent slungCancermsgIndex universe val bronChitis CPDsmoking smoker Select present absent nosmoker Select present absent sbronChitismsgIndex universe def tbOrCFcntuberculosis Symbol lungCancer Symbol Boolean tuberculosis lungCancer match case present present true case present absent true case absent present true case absent absent false val tuberculosisOrCancer Applytuberculosis lungCancer tbOrCFcn stuberculosisOrCancermsgIndex universe val xRayPrior ChaintuberculosisOrCancer tb Boolean iftb Constant else Constant sxRayPriormsgIndex universe val xRay CPDtuberculosisOrCancer true Select abnormal normal false Select abnormal normal sxRaymsgIndex universe val dyspnea CPDtuberculosisOrCancer bronChitis true present Select present absent true absent Select present absent false present Select present absent false absent Select present absent sdyspneamsgIndex universe As you can see I have create a new Universe and created the variables in it The msgIndex is an Int that indexes each worker actor created Before that using the default universe it never worked but after that sometimes I can create the model but the error appears in some executions The VariableElimination comes just after the model creation val tbPriorAlg VariableEliminationtuberculosisOrCancer universe tbPriorAlgstart val tbPrior tbPriorAlgprobabilitytuberculosisOrCancer true ifisDebug printlnPrior probability the Tuberculosis Or Cancer on tbPriortoString But even if I remove the VariableElimination the error occurs Can you help me Thanks in advance