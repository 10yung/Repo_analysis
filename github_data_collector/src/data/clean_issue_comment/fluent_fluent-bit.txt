 Bug Report Describe the bug fluentbit didnt create message field but concatenate to previous field Your Environment Include as many relevant details about the environment you experienced the bug in Version used tdagentbit Configuration PARSER Name headlog Format regex Regex time d d d d d d level AZ TimeFormat Ymd HMS TimeKey time PARSER Name restlog Format regex Regex mix time d d d d d d level AZ message TimeFormat Ymd HMS TimeKey time INPUT Name tail Tag sampletag Path etctdagentbitlogspythonApplog Multiline On ParserFirstline headlog Parser restlog Operating System and version debian To Reproduce Log sample ERROR Exception on main handler Traceback most recent call last File pythonloggerpy line in makelog return word IndexError string index out of range Output tdagentbit Fluent Bit v tdagentbit Copyright C Treasure Data tdagentbit info storage initializing tdagentbit info storage inmemory tdagentbit info storage normal synchronization mode checksum disabled maxchunksup tdagentbit info engine started pid tdagentbit info sp stream processor started rabcons tdagentbit sampletag levelERROR tdagentbit Exception on main handler tdagentbit Traceback most recent call last tdagentbit File pythonloggerpy line in makelog tdagentbit return word tdagentbit IndexError string index out of range Expected behavior message field created and multiline log is there Addition info if im writing into restlog something like exceptionExceptionmessage it can create correct exception filed and multiline message field But exactly i dont know what will be placed in second line if im writing into restlog something like message or even mixmessage it create multiple message fileds and only last one available in kibanaelastic Currently fluentbit logs can be redirected to a file using LogFile But there is no configuration option to limit the size of the file Additionally there is no log rotation that can be enabled for fluentbit logs These might be useful when we want to collect fluentbit process logs itself and also to debug Added new configuration parameters LogFileSize Size limit in MB for the file specified by LogFile LogFileHistory Max number of backups to be kept These backups are kept as LogFile LogFile etc Sample configuration SERVICE Flush Daemon Off LogLevel debug LogFile fbloglog LogFileSize LogFileHistory This PR has the corresponding changes online report offline report Bug Report Describe the bug Using nested wild cards in the Path gives the following error intail Cannot read info from C ProgramData Docker containers log Running dir in powershell works as expected and it lists out the said log files In Linux all varlogcontainerslog works as it searches the sub directories for log files too PS Ive tried log log and it did not work for me To Reproduce Rubular link if applicable Example log message if applicable intail Cannot read info from C ProgramData Docker containers log Steps to reproduce the problem Expected behavior I expect Path should be able to recursively search for log files in the location that Im pointing to similar to how dir does it Screenshots If applicable add screenshots to help explain your problem Your Environment Include as many relevant details about the environment you experienced the bug in Version used The current fluentbit for windows exe on the official site gives an error related to missing DLL files Im using the bit version shared by fujimotos Configuration Environment name and version eg Kubernetes What version Server type and version Operating System and version Filters and plugins Additional context How has this issue affected you What are you trying to accomplish Providing context helps us come up with a solution that is most useful in the real world Im trying to tail all of the docker container logs using fluentbit for windows on a machine Bug Report Fluentbit not able to send logs to aws hosted elastic search which port and url should be configured as the elastic serach host kubectl logs podfluentbit w n n logging Fluent Bit v Copyright C Treasure Data info storage initializing info storage inmemory info storage normal synchronization mode checksum disabled maxchunksup info engine started pid info filterkube https hostkubernetesdefaultsvc port info filterkube local POD info OK info filterkube testing connectivity with API server info filterkube API server connectivity OK info httpserver listen iface tcpport info sp stream processor started Signedoffby Khem Raj rajkhemgmailcom Signedoffby Atibhi Agrawal AtibhiAgrawaliiitborg Issue Bug Report Describe the bug One pod from a daemonset is marked as not ready beacuse it fails its readiness probe To Reproduce Use the readiness probe in a DaemonSet as described in Expected behavior The readiness probe should not fail Your Environment Openshift Fluentbit Additional context In my case I have pods running in the daemonset and only one is failing If I perform a curl to the failing pod it returns a error fluentbitp trq Running m The api is up curl v About to connect to port Trying Connected to port GET apiv HTTP UserAgent curl Host Accept HTTP OK Server Monkey Date Thu Jan GMT TransferEncoding chunked Connection to host left intact fluentbitversion editionCommunityflags FLBHAVEPARSERFLBHAVERECORDACCESSORFLBHAVESTREAMPROCESSORFLBHAVETLSFLBHAVESQLDBFLBHAVEMETRICSFLBHAVEHTTPSERVERFLBHAVESYSTEMDFLBHAVEFORKFLBHAVETIMESPECGETFLBHAVEGMTOFFFLBHAVEUNIXSOCKETFLBHAVEPROXYGOFLBHAVESYSTEMSTRPTIMEFLBHAVEJEMALLOCFLBHAVELIBBACKTRACEFLBHAVEREGEXFLBHAVELUAJITFLBHAVECTLSFLBHAVEACCEPT FLBHAVEINOTIFY But the prometheus endpoint does not curl v About to connect to port Trying Connected to port GET apiv metricsprometheus HTTP UserAgent curl Host Accept HTTP Not Found Server Monkey Date Thu Jan GMT TransferEncoding chunked Connection to host left intact set the logging to default but there are no errors Fluent Bit v Copyright C Treasure Data debug storage cio stream new stream registered tail debug storage cio stream new stream registered tail debug storage cio stream new stream registered tail debug storage cio stream new stream registered tail info storage initializing info storage inmemory info storage normal synchronization mode checksum disabled maxchunksup info engine started pid debug engine coroutine stack size bytes K debug intail inotify watch fd debug intail scanning path varlogcontainersns log debug intail Cannot read info from varlogcontainersns log debug intail inotify watch fd debug intail scanning path varlogcontainersns log debug intail Cannot read info from varlogcontainersns log debug intail inotify watch fd debug intail scanning path varlogcontainersns log debug intail add to scan queue varlogcontainersrpp r kwns rapc b c c bf a ecd a ec de a f f cddd log offset debug intail inotify watch fd debug intail scanning path varlogcontainersorchestratorlog debug intail add to scan queue varlogcontainersapigw lfk korchestratorapigw fc d bc aae c ad ced c cd fb dbdbe blog offset debug upstreamha opening file fluentbitetc upstreamconf info filterkube https hostkubernetesdefaultsvc port info filterkube local POD info OK info filterkube testing connectivity with API server debug filterkube API Server nslogging podfluentbitsrhn httpdo HTTP Status info filterkube API server connectivity OK debug router match rule tail forward debug router match rule tail forward debug router match rule tail forward debug router match rule tail forward debug router match rule tail forward debug router match rule tail forward debug router match rule tail forward debug router match rule tail forward info httpserver listen iface tcpport info sp stream processor started Signedoffby Atibhi Agrawal AtibhiAgrawaliiitborg Related to issue This is my log message before parsing containerid aae c cbdae e e a b aba f f e ef containername ecsLogisticsLDWtestLogistics LogisticsLDWtestLogisticscef afc c dbe db ec instanceid i a e dc e e ecscluster LDWtestLogistics ecstaskarn arnawsecsuswest task b a d da ba fe d ecstaskdefinition LogisticsLDWtestLogistics log threadhttpnginx exec levelINFOloggerNamecomenterprisecloudLogisticsapiv KafkaConsumerV reqMsg requestId da d bbb c e aead f message Request to list consumer details endOfBatchfalseloggerFqcnorgapachelog jCategoryinstantepochSecond nanoOfSecond contextMapthreadId threadPriority r source stdout Then with JSON parsing it looks like this if I only use the json parser containerid aae c cbdae e e a b aba f f e ef containername ecsLogisticsLDWtestLogistics LogisticsLDWtestLogisticscef afc c dbe db contextMap ec instanceid i a e dc e e ecscluster LDWtestLogistics ecstaskarn arnawsecsuswest task b a d da ba fe d ecstaskdefinition LogisticsLDWtestLogistics endOfBatch false instant epochSecond nanoOfSecond level INFO loggerFqcn orgapachelog jCategory loggerName comenterprisecloudLogisticsapiv KafkaConsumerV reqMsg requestId da d bbb c e aead f message Request to list consumer details source stdout thread httpnginx exec threadId threadPriority The reqMsg field is JSON in itself and I am not sure why this is not getting formatted as JSON Is there a different parser I need to use or a different regex to parse this Any help is appreciated I tried with the following customparsersconf PARSER Name json Format json TimeKey time TimeFormat dbYHMS z DecodeFieldAs json message PARSER Name docker Format json TimeKey time TimeFormat YmdTHMSL TimeKeep On DecodeFieldAs escaped log DecodeFieldAs escaped reqMsg I tried both json and escaped decoder types here and the fluentbitconf is as below SERVICE ParsersFile fluentbitparserscustomparsersconf Flush Grace FILTER Name parser Match KeyName log Parser json ReserveData True FILTER Name parser Match KeyName log Parser docker ReserveData True