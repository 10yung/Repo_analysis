Continue the work started at feature requests Create a new ipfs add command that adds a file to the repository in burnonread mode Perhaps the command would be ipfs add burnonread When the content is read for the first time it is provided then deleted maybe with another option for secure deletion This command would facilitate using ipfs for relatively easy moderately private communications especially for people who run ipfs on their computers I could email you a link with a hash of a message that I added to my own ipfs repo then after you read it the message would be deleted Users who want to be more secure could add a random string at the end of the message so that a short message is not easily inferred by an intermediary who has a rainbow table or the message could be encrypted GPGPGP or otherwise The value of these features compared to using Enigmail or manual GPGPGP in an email is that novice recipients typically do not understand how to keep their private key secure and they dont understand how to keep their message history secure When using regular email with GPG years from now the recipients computer could be seized and the key could be used to read all the old email traffic With the ipfs burnonread option I could email links to a novice the novice could open them the message would appear and then disappear so that the history of communications would be exceedingly difficult to recreate All you need is an appropriate ipfs web browser addin Burnonread combined with direct network connections so that the CONTENT of the messages bypasses intermediaries such as email providers might prove to become extremely important as governments consider mandating backdoors to encryption or encrypted devices and encrypted email To make this even more secure and faster two features could be added if they can not already be acheived in some way ADDED FEATURE Im not sure about the official policy for announcing to the world that a file has been added to a repo but the burnonread messages should not be announced to the network For one thing a private message is not intended for the whole universe so Spamming all peers with an announcement is just a waste of bandwidth The other thing is that quiet operation improves privacy and reduces the chance that a powerful adversary will detect that the hash exists and try to read the message before the intended recipient reads it Burnonread would be useless without this feature ADDED FEATURE VERSION Easiest for the recipient of a message to read the message Establish a different format of an ipfs link that looks specifically at the specified address for the specified hash maybe something like ipfs tcp QmNsVsMWYbukZjiBiccMjJABCDEFGHIJKLMNOPQRST Web browser addins would have to capture this and fetch it ADDED FEATURE VERSION Easier to program to provide faster functionality for a variety of applications but clumsier for novice users Add a command like this ipfs dht find provs QmXgqKTbzdh pQtKFb SpMCpDDcKR ujqk pKph aCNF In other words see if this hash can be found at any of the following addresses eg a server farm that hosts content that the provider wants to be found quickly as soon as the content is created but the hashes are not announced to the universe thx for your consideration here ps the system would also need an expiration date so that if I send you a burnonread link and you dont read it in a few days the server would delete it Nov Motivation When a file is updated and resync again decrement its block duplication on nodes all over the world and decrease communication cost only downloaded the updated blocks and save storage only updated section of the file as blocks will be stored Problem Example there is a targz file which contains a datatxt file filetargz GB stored in my IPFSrepo which is pulled from another Nodea I open the datatxt file and added a single character in a random locations in the file beginning of the file middle of the file and end of the file and compress it again as filetargz and store it in my IPFSrepo Here update is only few kilobytes When I deleted a single character at the beginning of a file since the hash of all kbblocks will be altered which will lead to download complete file to be downloaded As a result when nodea wants to reget the updated targz file a resync will take place and whole file will be downloaded all over again As a result there will be duplication of blocks GB in this example even the change is made only for few kilobytes And iteratively this duplication will be distributed to all over the peers which is very inefficient and consumes high amount of storage and additional communication cost over time Solution Other clouds are try to solve this problem using BlockLevel File Copying On their case like IPFS since blocklist is considered for BlockLevel File Copying when a file is updated a character is added at the beginning of the file Dropbox OneDrive will reupload the whole file since the first blocks hash will be change and it will also affectchange the hash of all the consequent blocks This doesnt solve the problem I believe better soluiton is to consider between each commits of the files approach that gitdiff uses could be considered This will only uploads the changed diff parts of the file that will be few kilobytes on the example I give and its diffed blocks will be merged when other nodes pull that file So as communication cost only few kilobytes of that will be transferes and that amount of data will be added to storage will be only few kilobytes as well I know that it will be difficult to redesign IPFSs design but this could be done as a wrapper solution that combines IPFS and git and users can use it for very large files based on their needs This problem is not considered as priority by IPFS team but at least it should be on the priority IPFS team is considering adding that eventually but it s not a priority Please see discussion I have already opened Please feel free to add your ideas in to them Does IPFS provide blocklevel file copying feature Efficiency of IPFS for sharing updated file Motivation Users should be able to publish and resolve IPNS records when disconnected from the global network This is important in two cases The user is trying to reresolve content that has been resolved at some point This is useful for offline document viewing Two users are offline and are trying to use IPNS Proposal Libp p Parts Users should be allowed to publish records when offline This should not produce an error However it might produce a warning Goipfs currently allows this with an allowoffline flag but thats not very userfriendly Users should be allowed to resolve expired records as long as some warning is displayed Problem At the moment I have IPFS nodes running on my machine embedded by following applications IPFS Desktop Textile Desktop Radicle Service And when Ill start test driving anytype Ill likely end up with nodes It is not ideal because IPFS is resource intensive On my Macbook Pro IPFS Node drains battery about times faster than without That multiplied by x or possibly more as more apps become popular is terrifying As user I would like a canonical place for the data stored by apps to be aggregated at instead it gets scattered around bunch of IPFS nodes That also means there is no simple way for me to mirror data on other nodes sever or other device More broadly current situation does not align with WebOS vision We are replicating data silos that we hate from the conventional web sure data isnt gated behind corporate server farm but if user cant easily interact with data trapped in other app embedded node end result is not much different With the caveat that here you can copy dataset across nodes but propagating changes across nodes is non trivial apps generally dont have a good story even for doing copying Proposal ipfs daemon and ipfs tool and library should by default should attempt to discover if IPFS service is already running and if so just act as a frontend to it otherwise spawn a service and still act as a frontend to it That would make all embedders by default share same IPFS node Dealing with version incompatibilities is going to be an interesting challenge but even if this would address problem for same versioned nodes that would be a good step forward IMO Essential Use Cases IPFS user creates new IPNS folders files and related metadata encrypted for privacy at rest by default providing local record of functional equivalence between encrypted key decrypted blocks tracked and linked within a locallyimmutable IPLD mDAG by their respective multihash content IDs Local Access Control configuration API or command set allows gradual structured key sharing with node peers or predefined node groups in shared lockboxes in keeping with CapabilityBased Security standards Access is granted to the public via structured IPNS manifest distribution only when global publishing is explicitly intended using metadata and keysets signed by the intended public authors publicprivate key pair A network consensus timestamp may be applied at time of public signing in order to help establish public data source attribution and provenance if desired by the author Use Cases IPFS user creates new IPNS folders files and related metadata encrypted for privacy and only uses IPFS and limited IPNS data distribution for remote backup purposes Encryption keys are thus only ever held locally and never distributed Local access control configuration API allows gradual structured key sharing with known peers or among preexisting membership node groups in shared lockboxes in keeping with CapabilityBased Security standards Decrypted data or necessary decryption keys are only revealed to the public when global publishing is explicitly intended using metadata and keysets signed by the public authors publicprivate key pair with a network consensus timestamp if public data source attribution and provenance are desired by the author Upon publication the node may utilize a group shared publicprivate key pair to sign all published metadata and keys in order to gain pseudonymous privacy protection via group membership Each group lockbox membership or private network is able to collectively determine their own publishing revocation membership local incentives write access and blacklist policies via consensus protocol selection and ownermoderator assignments Each decision will create a different automated control on group node shared key distribution and map Access Control configuration API calls to Capability Based Security key allocation Nested arrangements and crossover membership of multiple group lockboxes are possible as a result of key distribution being innately publicprivate key pair and signature source agnostic Higher level or external identity confirmation systems are necessary to prevent such complex multimembership arrangements as a group defined policy that influences group node Access Control configurations Foundational Features Functionality Finish KeyStore and KeyChain systems and add group lockbox distribution so access is not node dependent Key exchange Access Control configuration and network API establishes private network and group membership status as a result of encryption at rest over the wire and maps into CapabilityBased Security models Group lockbox membership ad hoc arrangements allow private networks to form federate or subdivide at will Group members may publish data to the public as a unified group identity via publicprivate key sharing which should be automated or managed according to each groups predefined publication policies Every request for a key access needs to occur inside an encrypted LibP P channel or tunnel and request messages must be signed by a valid group member or known permitted peer publicprivate key pair IPNS manifest distribution must not permit accidental leaking of decrypted block or key data prior to Access Control configuration API permitted release This potential leak issue creates a good argument for keeping IPNS manifest data and distribution separate from but related to any encryption key records and management Even releasing the multihash ID of any decrypted blocks prior to permitted publication represents a potential security threat so the IPNS manifest records should only reference encrypted block multihashes prior to permitted release Existing Projects Organizations Working in this Area Please note node and group key management does not create nor enforce any identity or reputation systems directly but it can enable linking such systems to an Access Control layer as desired by individual group or node owners Private IPFS networks Peergos uPort Sovrin spherebox Google Circles Groups and apps for Work Dropbox Business Backblaze Groups GitHub private repositories Facebook groups Discourse Slack Additional reference materials It is entirely possible especially as IPFS is not yet widely adopted that a peer may have a file but does not know a corresponding IPFS url because it just stores the full hash of the file in some file catalograther than a pointer to a magic DHT shard as seems to be the case in bitswap To this end I think a peer should also be able to list want hashes to a connected peer Enquiry Setup A peer enquires to connected hosts about what forms of catalog hash they have md sha sha optional The peer produces a bare hash wantlist of the form hashtypemulticodechash Peering Algorithm On another peer Read wantlist of peer p Ask a barewantlistprovider if that file is on file ipfs add nocopy file if available Notify peer p of The corresponding ipfs url or File Unavailable Bare wantlist provider A bare wantlist provider is a program registered with the ipfs daemon Some configuration or environment variable will be set to its path and accepted hash types The wantlist provider will be invocable in the form cmdName hashNamehash And shall print the path of the corresponding file then exit or If the file is not found or any other condition for which the peer does not want to serve the file exit Idea rustup but for IPFS This tool would Manage multiple IPFS installs multiple implementations and multiple versions Provide a virtual ipfs command that launches the correct IPFS version downloading it if necessary Requirements V Fetch IPFS over IPFS when possible Fallback on fetching IPFS over HTTPs Support rolling back to a previous release V Support both jsipfs and goipfs V Support auto update User Story The user must be able to Run ipfs ARBITRARYCOMMAND and have that command run on the correct version of IPFS Run an ipfs update latest command to update to the latest stable version of their chosen IPFS implementation go or js Run ipfs update vXYZ to update to a specific version of their chosen IPFS dist Run ipfs update langvXYZ to switch to update to a specific version of lang Run ipfs update langlatest to switch to the latest IPFS version in lang Pass transportipfshttpsauto to ipfs update to force a specific transport V Design Like rustup this project should provide a virtual ipfs command that downloads the correct IPFS versiondistribution and runs it Unlike rustup it should support downloading IPFS over IPFS if a working version of IPFS is already installed When the ipfs command is run If ipfs is installed Run it Otherwise Download IPFS over https from distipfsio When the ipfs update command is run If ipfs is installed Use it to check for an update by checking ipfs cat ipnsdistipfsiogoipfslatest If theres an update available fetch the correct build with IPFS Otherwise Check for a new release If an update is available fetch the correct build over HTTPS Save the IPFS binary to a IPFSPATHreleasesVERSIONARCHITECTURE Write the current IPFS version to the config file as UpdateVersion When the ipfs update VERSION command is run do the same thing but use the specified version Or as Ive affectionally named it bitswaxx which is like bitswap concatenated and alphad I had this idea which is a cross between delegated routing and the friends of friends policy that scuttlebutt has This is likely to highlight my inexperience in networkprotocol design and its more than likely to cover ground and research already discountedbut if this triggers ideas in someone else or can be adapted somehow then it will make writing this down worthwhile The idea is get others to fetch content for you on the bitswap level In short your peers wantlist becomes your wantlist Imagine this if I have peers and each of those peers has peers then Ive increased the scope of peers who might have the content Im looking for from to This comes in flavours These peers are my friends their wantlist is my wantlist We cant just concat ALL wantlists together because the world will converge on a single dataset and well ALL end up storing each others stuff Hard disks explode obvious problem We could do this for peers we deem to be friends When we receive their wantlist it gets added to our wantlist until such time as our friend stops wanting it or we receive it If we start wanting it ourselves then that trumps the friend want It could allow actual IRL friends to host each others content or allow a public nodes to befriend all nodes in a particular webmobiledesktop application Hop based wants eg wants hops away are added to my wantlist The hops are configurable Bitswap is hop based Bitswaxx would be hop based Each want would carry a hop field a number that describes how far away the want came from and is incremented by on receipt A want desired by your node is sent with a hop of A want that came from another peer would be sent with whatever the received hop was Wed have to decide what to do with duplicate wants with differing hop counts take the lowest Provided the hop count for a want you receive is for example the want will be added to your wantlist This is of course until such time as the peer who wanted it stops wanting it or we receive it If we start wanting it ourselves then that trumps the want from the peer Known issuesopen problems Not intelligent Arguably peers of peers are no more likely to have the content you want than the peers you have and it would be better to intelligently converge on a peer who actually has that content This is what the DHT does and it is known to be slow Bitswaxx might be an effective inbetween method thats slower than bitswap but faster than DHT The more peers we can ask for content the more likely well find it Right We cant actually connect to peers simultaneously but we can if we leverage our peer network Disk space Storing content albeit temporarily will have a considerable disk impact Bandwidth Bitswap will become considerably more chatty Multiple peers will receive content they dont want and may never provide it to the peer that does Abuse As with all relays this is open to abuse Were trusting the hop field Complicated If we want to be able to remove the want from our list because we dont want it ourselves we need to track which peers want what memory issues It would be fantastic to be able to track dependencies across all the dozens of repos and thousands of issues It would be fantastic to see whats blocked as easily as the below See for a proposal on how to implement this pretty reasonably on top of regular github issues RichardLitt this may be a super valuable dev workflow tool to contribute 