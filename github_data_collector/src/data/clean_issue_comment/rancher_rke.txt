RKE version Typeprovider of hosts VirtualBoxBaremetalAWSGCEDO Baremetal We use metallb to assign public ips to LoadBalancer services and it works really well we also want those public IPs to get DNS entries via the coredns k sexternal feature currently we have to replace the coredns config with this apiVersion v kind ConfigMap metadata name coredns namespace kubesystem data Corefile errors health ready kubernetes k srdstibodk inaddrarpa ip arpa pods insecure fallthrough inaddrarpa ip arpa prometheus forward cache loop reload loadbalance k sexternal svcexamplecom apiVersion rbacauthorizationk siov kind ClusterRole metadata labels kubernetesiobootstrapping rbacdefaults k saddon corednsaddonsk sio name systemcoredns rules apiGroups resources endpoints services pods namespaces verbs list watch apiGroups resources nodes verbs get apiVersion rbacauthorizationk siov kind ClusterRoleBinding metadata annotations rbacauthorizationkubernetesioautoupdate true labels kubernetesiobootstrapping rbacdefaults k saddon corednsaddonsk sio name systemcoredns roleRef apiGroup rbacauthorizationk sio kind ClusterRole name systemcoredns subjects kind ServiceAccount name coredns namespace kubesystem apiVersion v kind Service metadata name corednsexternal namespace kubesystem labels k sapp kubednsexternal kubernetesioclusterservice true kubernetesioname corednsexternal addonmanagerkubernetesiomode Reconcile annotations prometheusioport prometheusioscrape true spec selector k sapp kubedns type LoadBalancer loadBalancerIP ports name dns port protocol UDP The Service exposes coredns via a fixed IP so the external DNS server can delegate the svcexamplecom zone to it The ClusterRole and Binding is needed to allow coredns to discover the load balancer services The only change to the ConfigMap is k sexternal svcexamplecom This bit creates DNS entries on the form servicenamespacesvcexamplecom that resolve to the loadBalancerIP of the service We have this all working but when upgrading the cluster the ConfigMap gets overwritten and things fall apart until our own one is loaded again even when our own ConfigMap is listed in the addonsinclude section of clusteryml I would love to be able to get the above objects simply by addning externalzone and externalip to the clusteryml like this dns provider coredns externalzone svcexamplecom externalip upstreamnameservers Perhaps a partial solution would be to be able to flag objects so they arent overwritten in cases like this Ihave a runing rke cluster v in a one note debian buster installation but i cant reach networks outside of my node RKE version bash version of rke usernode rke v rke version v Docker version docker versiondocker info preferred bash usernode docker info Client Debug Mode false Server Containers Running Paused Stopped Images Server Version Storage Driver overlay Backing Filesystem extfs Supports dtype true Native Overlay Diff true Logging Driver jsonfile Cgroup Driver cgroupfs Plugins Volume local Network bridge host ipvlan macvlan null overlay Log awslogs fluentd gcplogs gelf journald jsonfile local logentries splunk syslog Swarm inactive Runtimes runc Default Runtime runc Init Binary dockerinit containerd version b a c af e c db c f fa runc version e f a c f e d a c b d aa init version fec Security Options apparmor seccomp Profile default Kernel Version amd Operating System Debian GNULinux buster OSType linux Architecture x CPUs Total Memory GiB Name node ID VJ ZSO MDH PMS YMTHLXXCZSX YH U NISNJF K ILWW Docker Root Dir varlibdocker Debug Mode false Registry Labels Experimental false Insecure Registries Live Restore Enabled false WARNING No swap limit support Operating system and kernel cat etcosrelease uname r preferred bash Kernel usernode uname r amd Typeprovider of hosts Baremetal clusteryml file yml If you intened to deploy Kubernetes in an airgapped environment please consult the documentation on how to configure custom RKE images nodes address node fritzbox port internaladdress node fritzbox role controlplane worker etcd hostnameoverride user rke dockersocket varrundockersock sshkey sshkeypath sshidrsa sshcert sshcertpath labels taints services etcd image extraargs extrabinds extraenv externalurls cacert cert key path uid gid snapshot null retention creation backupconfig null kubeapi image extraargs extrabinds extraenv serviceclusteriprange servicenodeportrange podsecuritypolicy false alwayspullimages false secretsencryptionconfig null auditlog null admissionconfiguration null eventratelimit null kubecontroller image extraargs extrabinds extraenv clustercidr serviceclusteriprange scheduler image extraargs extrabinds extraenv kubelet image extraargs extrabinds extraenv clusterdomain clusterlocal infracontainerimage clusterdnsserver failswapon false generateservingcertificate false kubeproxy image extraargs extrabinds extraenv network plugin canal options nodeselector authentication strategy x sans webhook null addons addonsinclude systemimages etcd ranchercoreosetcdv rancher alpine rancherrketoolsv nginxproxy rancherrketoolsv certdownloader rancherrketoolsv kubernetesservicessidecar rancherrketoolsv kubedns rancherk sdnskubedns dnsmasq rancherk sdnsdnsmasqnanny kubednssidecar rancherk sdnssidecar kubednsautoscaler rancherclusterproportionalautoscaler coredns ranchercorednscoredns corednsautoscaler rancherclusterproportionalautoscaler kubernetes rancherhyperkubev rancher flannel ranchercoreosflannelv rancher flannelcni rancherflannelcniv rancher caliconode ranchercaliconodev calicocni ranchercalicocniv calicocontrollers ranchercalicokubecontrollersv calicoctl calicoflexvol ranchercalicopod daemonflexvolv canalnode ranchercaliconodev canalcni ranchercalicocniv canalflannel ranchercoreosflannelv canalflexvol ranchercalicopod daemonflexvolv weavenode weaveworksweavekube weavecni weaveworksweavenpc podinfracontainer rancherpause ingress ranchernginxingresscontrollernginx rancher ingressbackend ranchernginxingresscontrollerdefaultbackend rancher metricsserver ranchermetricsserverv windowspodinfracontainer rancherkubeletpausev sshkeypath sshidrsa sshcertpath sshagentauth false authorization mode none options ignoredockerversion false kubernetesversion privateregistries ingress provider options nodeselector extraargs dnspolicy extraenvs extravolumes extravolumemounts clustername cloudprovider name prefixpath addonjobtimeout bastionhost address port user sshkey sshkeypath sshcert sshcertpath monitoring provider options nodeselector restore restore false snapshotname dns null shelldemoyml file yml apiVersion v kind Pod metadata name shelldemo spec volumes name shareddata emptyDir containers name multitool image praqmanetworkmultitool volumeMounts name shareddata mountPath usrsharenginxhtml dnsPolicy Default Steps to Reproduce bash rancher up kubectl apply f shelldemoyml Results not rachabel networkes outside the cluster bash usernode kubectl exec it shelldemo binping c PING bytes of data ping statistics packets transmitted received packet loss time ms command terminated with exit code So without route Internet and DNS are also not reachable bash usernode kubectl exec it shelldemo binping c wwwheisede ping wwwheisede Try again command terminated with exit code my notes of searching the error Nodes could access other networks bash version of rke usernode rke v rke version v version of docker usernode Projectswudo rke docker v Docker version build a ea ip settings usernode ip a lo LOOPBACKUPLOWERUP mtu qdisc noqueue state UNKNOWN group default qlen linkloopback brd inet scope host lo validlft forever preferredlft forever inet scope host validlft forever preferredlft forever enp s f BROADCASTMULTICASTUPLOWERUP mtu qdisc mq state UP group default qlen linkether fda c brd ffffffffffff inet brd scope global dynamic noprefixroute enp s f validlft sec preferredlft sec inet a f f d cf f e scope global temporary dynamic validlft sec preferredlft sec inet a f fffe fda c scope global dynamic mngtmpaddr noprefixroute validlft sec preferredlft sec inet fe fffe fda c scope link noprefixroute validlft forever preferredlft forever version of debian usernode cat etcdebianversion cd usernode cat etcosrelease PRETTYNAMEDebian GNULinux buster NAMEDebian GNULinux VERSIONID VERSION buster VERSIONCODENAMEbuster IDdebian HOMEURL SUPPORTURL BUGREPORTURL Kernel usernode uname r amd router is reachble usernode ping c PING bytes of data bytes from icmpseq ttl time ms ping statistics packets transmitted received packet loss time ms rtt minavgmaxmdev ms real world is reachble usernode ping c wwwgooglede PING wwwgooglede bytes of data bytes from zrh s inf e net icmpseq ttl time ms wwwgooglede ping statistics packets transmitted received packet loss time ms rtt minavgmaxmdev ms routes are set like this usernode ip route default via dev enp s f proto dhcp metric dev cni proto kernel scope link src dev docker proto kernel scope link src linkdown dev enp s f proto kernel scope link src metric firewall settings usernode sudo iptables L Chain INPUT policy ACCEPT target prot opt source destination Chain FORWARD policy DROP target prot opt source destination DOCKERUSER all anywhere anywhere DOCKERISOLATIONSTAGE all anywhere anywhere ACCEPT all anywhere anywhere ctstate RELATEDESTABLISHED DOCKER all anywhere anywhere ACCEPT all anywhere anywhere ACCEPT all anywhere anywhere Chain OUTPUT policy ACCEPT target prot opt source destination Chain DOCKER references target prot opt source destination Chain DOCKERISOLATIONSTAGE references target prot opt source destination DOCKERISOLATIONSTAGE all anywhere anywhere RETURN all anywhere anywhere Chain DOCKERISOLATIONSTAGE references target prot opt source destination DROP all anywhere anywhere RETURN all anywhere anywhere Chain DOCKERUSER references target prot opt source destination RETURN all anywhere anywhere Warning iptableslegacy tables present use iptableslegacy to see them and the iptableslegacy roules usernode sudo iptableslegacy L Chain INPUT policy ACCEPT target prot opt source destination KUBESERVICES all anywhere anywhere ctstate NEW kubernetes service portals KUBEEXTERNALSERVICES all anywhere anywhere ctstate NEW kubernetes externallyvisible service portals KUBEFIREWALL all anywhere anywhere Chain FORWARD policy ACCEPT target prot opt source destination KUBEFORWARD all anywhere anywhere kubernetes forwarding rules KUBESERVICES all anywhere anywhere ctstate NEW kubernetes service portals ACCEPT all anywhere ACCEPT all anywhere Chain OUTPUT policy ACCEPT target prot opt source destination KUBESERVICES all anywhere anywhere ctstate NEW kubernetes service portals KUBEFIREWALL all anywhere anywhere Chain KUBEEXTERNALSERVICES references target prot opt source destination Chain KUBEFIREWALL references target prot opt source destination DROP all anywhere anywhere kubernetes firewall for dropping marked packets mark match x x Chain KUBEFORWARD references target prot opt source destination DROP all anywhere anywhere ctstate INVALID ACCEPT all anywhere anywhere kubernetes forwarding rules mark match x x ACCEPT all anywhere kubernetes forwarding conntrack pod source rule ctstate RELATEDESTABLISHED ACCEPT all anywhere kubernetes forwarding conntrack pod destination rule ctstate RELATEDESTABLISHED Chain KUBESERVICES references target prot opt source destination from a started docker container not in cluster internt is also reachble usernode docker run a stdout i t praqmanetworkmultitool binping c wwwheisede The directory usrsharenginxhtml is not mounted Overwriting the default indexhtml file with some useful information PING wwwheisede bytes of data bytes from wwwheisede icmpseq ttl time ms wwwheisede ping statistics packets transmitted received packet loss time ms rtt minavgmaxmdev ms But inside k s all pods i cant reach the surrounding network nodes PCs or router bash kubectl exec it shelldemo binbash bash then inside the Pod bash ip a lo LOOPBACKUPLOWERUP mtu qdisc noqueue state UNKNOWN group default qlen linkloopback brd inet scope host lo validlft forever preferredlft forever eth if BROADCASTMULTICASTUPLOWERUP mtu qdisc noqueue state UP group default linkether a a a f brd ffffffffffff linknetnsid inet scope global eth validlft forever preferredlft forever routing inside my pod bash ip route default via dev eth dev eth proto kernel scope link src via dev eth ping cluster intern bash ping c PING bytes of data bytes from icmpseq ttl time ms bytes from icmpseq ttl time ms ping statistics packets transmitted received packet loss time ms rtt minavgmaxmdev ms inside the pod could reach the hosting node bash ping c PING bytes of data bytes from icmpseq ttl time ms bytes from icmpseq ttl time ms ping statistics packets transmitted received packet loss time ms rtt minavgmaxmdev ms but I cant reach the PCs router or dns ping c PING bytes of data C ping statistics packets transmitted received packet loss time ms RKE version Docker version W rafthttp health check for peer f e f could not connect dial tcp connect no route to host prober ROUNDTRIPPERRAFTMESSAGE W rafthttp health check for peer f e f could not connect dial tcp connect no route to host prober ROUNDTRIPPERRAFTMESSAGE W rafthttp health check for peer f e f could not connect dial tcp connect no route to host prober ROUNDTRIPPERSNAPSHOT W rafthttp health check for peer f e f could not connect dial tcp connect no route to host prober ROUNDTRIPPERSNAPSHOT W rafthttp health check for peer f e f could not connect dial tcp io timeout prober ROUNDTRIPPERRAFTMESSAGE W rafthttp health check for peer f e f could not connect dial tcp io timeout prober ROUNDTRIPPERRAFTMESSAGE RKE version INFO Running RKE version v Docker version docker versiondocker info preferred Client Docker Engine Community Version API version Go version go Git commit Built Sun Feb OSArch linuxamd Experimental false Server Docker Engine Community Engine Version API version minimum version Go version go Git commit Built Sun Feb OSArch linuxamd Experimental false Operating system and kernel cat etcosrelease uname r preferred NAMERancherOS VERSIONv IDrancheros IDLIKE VERSIONIDv PRETTYNAMERancherOS v HOMEURL SUPPORTURL BUGREPORTURL BUILDID rancher Typeprovider of hosts VirtualBoxBaremetalAWSGCEDO AHV clusteryml file clustername rancher sshagentauth true sshkeypath pathtokey kubernetesversion v rancher nodes address xxxxxxxxxxxx user rancher role controlplaneworkeretcd address xxxxxxxxxxxx user rancher role controlplaneworkeretcd address xxxxxxxxxxxx user rancher role controlplaneworkeretcd services etcd snapshot true retention h creation h kubeapi serviceclusteriprange podsecuritypolicy true kubecontroller clustercidr serviceclusteriprange kubelet failswapon true clusterdnsserver authorization mode rbac Steps to Reproduce rke up cluster with kubernetesversion v rancher Change to kubernetesversion v rancher and run rke up Results INFO dns removing DNS provider kubedns WARN Failed to deploy DNS addon execute job for provider coredns Failed to get job complete status for job rkekubednsaddondeletejob in namespace kubesystem Failed job pods had error message about not finding Deployments matching extentionsv beta or appsv beta I inspected the job and kubedns manifests With the upgrade to the kubedns and kubednsautoscaler deployments had apiVersion appsv I edited the rkekubednsaddon configmap in the kubesystem namespace changing apiVersion to appsv for both deployments and ran rke up again This workaround seemed to solve the problem There were no errors and after kubedns was deleted CoreDNS deployed without issue I suspect this is related to the deprecation of these apiVersions in k s v RKE version rke v rke version v Docker version docker versiondocker info preferred docker version Client Docker Engine Community Version API version Go version go Git commit dd dd f Built Wed Jul OSArch linuxamd Experimental false Server Docker Engine Community Engine Version API version minimum version Go version go Git commit dd dd f Built Wed Jul OSArch linuxamd Experimental false docker info Containers Running Paused Stopped Images Server Version Storage Driver overlay Backing Filesystem extfs Supports dtype true Native Overlay Diff true Logging Driver jsonfile Cgroup Driver cgroupfs Plugins Volume local Network bridge host macvlan null overlay Log awslogs fluentd gcplogs gelf journald jsonfile local logentries splunk syslog Swarm inactive Runtimes runc Default Runtime runc Init Binary dockerinit containerd version b a b e eb a d ce b c fb runc version e d a fabd a ad d a eeede f init version fec Security Options seccomp Profile default Kernel Version rancher Operating System RancherOS v OSType linux Architecture x CPUs Total Memory GiB Name rke ID XHDNHTIGB NAYMYU Q C XQSLVU AQGZ LAYGRF XFM A Docker Root Dir mntdatadocker Debug Mode client false Debug Mode server false Registry Labels Experimental false Insecure Registries Live Restore Enabled false Product License Community Engine Operating system and kernel cat etcosrelease uname r preferred cat etcosrelease NAMERancherOS VERSIONv IDrancheros IDLIKE VERSIONIDv PRETTYNAMERancherOS v HOMEURL SUPPORTURL BUGREPORTURL BUILDID Typeprovider of hosts VirtualBoxBaremetalAWSGCEDO Azure VM clusteryml file nodes address rke role controlplane etcd worker user rancher sshkeypath sshidrsarke address rke role controlplane etcd worker user rancher sshkeypath sshidrsarke address rke role worker etcd controlplane user rancher sshkeypath sshidrsarke ingress provider nginx extraargs httpport httpsport Steps to Reproduce I wish to configure the provided Rancher nginxingresscontroller to listen on ports HTTP and HTTPS When installing RKE I specify the above ports in the clusteryml file The RKE cluster deploys fine however nginxingresscontroller continues to be bound to HostPort and Results To fix this after RKE is deployed I modified nginxingresscontroller daemonset resource like so kubectl edit daemonset nginxingresscontroller n ingressnginx I did a a search and replace saved the resource config then did kubectl delete pod l appingressnginx n ingressnginx to get fresh pods deployed with the modified config Rancher now is bound to HostPort and Below is my current daemonset resource config which binds HostPort HTTP and HTTPS kubectl get daemonset nginxingresscontroller n ingressnginx o yaml apiVersion appsv kind DaemonSet metadata annotations deprecateddaemonsettemplategeneration fieldcattleiopublicEndpoints nodeNamelocalmachinebvcdxaddresses rke port protocolTCPpodNameingressnginxnginxingresscontrollerd m dallNodesfalsenodeNamelocalmachinebvcdxaddresses rke port protocolTCPpodNameingressnginxnginxingresscontrollerd m dallNodesfalsenodeNamelocalmachine f dbaddresses rke port protocolTCPpodNameingressnginxnginxingresscontroller xt ballNodesfalsenodeNamelocalmachine f dbaddresses rke port protocolTCPpodNameingressnginxnginxingresscontroller xt ballNodesfalsenodeNamelocalmachinegqw xaddresses rke port protocolTCPpodNameingressnginxnginxingresscontrollerwk sallNodesfalsenodeNamelocalmachinegqw xaddresses rke port protocolTCPpodNameingressnginxnginxingresscontrollerwk sallNodesfalse kubectlkubernetesiolastappliedconfiguration apiVersionappsv kindDaemonSetmetadataannotationsnamenginxingresscontrollernamespaceingressnginxspecselectormatchLabelsappingressnginxtemplatemetadataannotationsprometheusioport prometheusioscrapetruelabelsappingressnginxspecaffinitynodeAffinityrequiredDuringSchedulingIgnoredDuringExecutionnodeSelectorTerms matchExpressions keybetakubernetesioosoperatorNotInvalues windows keynoderolekubernetesioworkeroperatorExists containers args nginxingresscontrollerdefaultbackendservicePODNAMESPACEdefaulthttpbackendconfigmapPODNAMESPACEnginxconfigurationtcpservicesconfigmapPODNAMESPACEtcpservicesudpservicesconfigmapPODNAMESPACEudpservicesannotationsprefixnginxingresskubernetesiohttpport httpsport env namePODNAMEvalueFromfieldReffieldPathmetadatanamenamePODNAMESPACEvalueFromfieldReffieldPathmetadatanamespace imageranchernginxingresscontrollernginx rancher livenessProbefailureThreshold httpGetpathhealthzport schemeHTTPinitialDelaySeconds periodSeconds successThreshold timeoutSeconds namenginxingresscontrollerports containerPort namehttpcontainerPort namehttps readinessProbefailureThreshold httpGetpathhealthzport schemeHTTPperiodSeconds successThreshold timeoutSeconds securityContextcapabilitiesadd NETBINDSERVICE drop ALL runAsUser hostNetworktrueserviceAccountNamenginxingressserviceaccounttolerations effectNoExecuteoperatorExistseffectNoScheduleoperatorExists creationTimestamp T Z generation name nginxingresscontroller namespace ingressnginx resourceVersion selfLink apisappsv namespacesingressnginxdaemonsetsnginxingresscontroller uid baa b e a e a spec revisionHistoryLimit selector matchLabels app ingressnginx template metadata annotations prometheusioport prometheusioscrape true creationTimestamp null labels app ingressnginx spec affinity nodeAffinity requiredDuringSchedulingIgnoredDuringExecution nodeSelectorTerms matchExpressions key betakubernetesioos operator NotIn values windows key noderolekubernetesioworker operator Exists containers args nginxingresscontroller defaultbackendservicePODNAMESPACEdefaulthttpbackend configmapPODNAMESPACEnginxconfiguration tcpservicesconfigmapPODNAMESPACEtcpservices udpservicesconfigmapPODNAMESPACEudpservices annotationsprefixnginxingresskubernetesio httpport httpsport env name PODNAME valueFrom fieldRef apiVersion v fieldPath metadataname name PODNAMESPACE valueFrom fieldRef apiVersion v fieldPath metadatanamespace image ranchernginxingresscontrollernginx rancher imagePullPolicy IfNotPresent livenessProbe failureThreshold httpGet path healthz port scheme HTTP initialDelaySeconds periodSeconds successThreshold timeoutSeconds name nginxingresscontroller ports containerPort hostPort name http protocol TCP containerPort hostPort name https protocol TCP readinessProbe failureThreshold httpGet path healthz port scheme HTTP periodSeconds successThreshold timeoutSeconds resources securityContext capabilities add NETBINDSERVICE drop ALL runAsUser terminationMessagePath devterminationlog terminationMessagePolicy File dnsPolicy ClusterFirst hostNetwork true restartPolicy Always schedulerName defaultscheduler securityContext serviceAccount nginxingressserviceaccount serviceAccountName nginxingressserviceaccount terminationGracePeriodSeconds tolerations effect NoExecute operator Exists effect NoSchedule operator Exists updateStrategy rollingUpdate maxUnavailable type RollingUpdate status currentNumberScheduled desiredNumberScheduled numberAvailable numberMisscheduled numberReady observedGeneration updatedNumberScheduled Hello RancherRKE team Im using RKE to manage the clusters Rke up command is updating cluster fastly but there is something wrong on upgrade and update progress I think RKE do not drain the k s nodes according to that this can cause down time Regarding to that I have added pod eviction option for rke I want to test and improve these PR maybe we can improve all together I will expect your feedbacks D Thanks Hi all The approach that RKE offers is really nice and what would be really interesting would be to be able to store the rke cluster state file remotely to facilitate collaboration as terraform allows it on the same basis of state management Also because it stores secrets we would like to be able to store it on a secure storage space Does it make sense as a feature request This PR corrects the log output during rke up The output still contains the legacy configmap name clusterstate The new cluster state is saved in fullclusterstate Can Kubernetes initialized by rke decide which node a pod runs on based on the nodes memory footprint rke version v docker Version OS ubuntu cat clusteryml nodes address hostnameoverride prek smaster user app role controlplane etcd address hostnameoverride prek snode user app role worker address hostnameoverride prek snode user app role worker clustername precluster sshkeypath rootrkepreidrsarke kubernetesversion v rancher ingress provider nginx services etcd extraargs electiontimeout heartbeatinterval quotabackendbytes snapshot true creation m s retention h kubeapi serviceclusteriprange servicenodeportrange podsecuritypolicy false kubecontroller clustercidr serviceclusteriprange kubelet clusterdomain clusterprelocal clusterdnsserver network plugin canal dns provider coredns rke tried add secondary scheduler but the pods with second scheduler is always in pendding status follow the guide in offical kubernetes page rke website give none document how to apply multiple scheudler I guess this is no supported labk sdemoslave kubernetes kubectl get pods NAME READY STATUS RESTARTS AGE annotationsecondscheduler Pending h 