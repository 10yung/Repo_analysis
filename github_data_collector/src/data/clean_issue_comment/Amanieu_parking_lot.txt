On macOS and iOS as of last year or so many kernel synchronization mechanisms support priority inheritance out of the box For a list of things that support it see this header and scroll down a bit When using one of those mechanisms a waiting thread knows which thread its waiting for and if the waiter has higher priority than the waitee the kernel temporarily boosts the waitees priority to match Priority inheritance is clearly useful across processes One might think that its not very useful within a single process since its rare for threads in the same process to be set at different priorities But even if theyre set at the same priority they can have different effective priorities if one of them has been boosted by priority inheritance from another process If that thread then tries to lock a mutex held by another thread it should be able to pass on its boost to the latter thread For this reason the list of mechanisms supporting priority inheritance includes pthread mutexes as well as ulocks another kind of userland mutex As far as I can tell parkinglot currently doesnt support priority inheritance on any platform park has no way of notifying the kernel which thread its waiting for It would be good to fix this though it might require substantial API changes Currently the Fairness section of the parkinglotMutex documentation has this text A typical unfair lock can often end up in a situation where a single thread quickly acquires and releases the same mutex in succession which can starve other threads waiting to acquire the mutex While this improves performance because it doesnt force a context switch when a thread tries to reacquire a mutex it has just released this can starve other threads This mutex uses eventual fairness to ensure that the lock will be fair on average without sacrificing performance This is done by forcing a fair unlock on average every ms which will force the lock to go to the next thread waiting for the mutex Additionally any critical section longer than ms will always use a fair unlock which has a negligible performance impact compared to the length of the critical section You can also force a fair unlock by calling MutexGuardunlockfair when unlocking a mutex instead of simply dropping the MutexGuard I feel like this could be improved by replacing mentions of performance with throughput and perhaps establishing that thread starvation has implications on latency This comes up as depending on the audience performance can mean either throughput or latency and explicitly calling out these two kinds of performances makes for less thinking time This blog post Mutexes Are Faster Than Spinlocks generated a reddit discussion about lock implementatons This made some users benchmark parkinglot on windows and some results shown are very problematic others could just be better since parkinglot is always faster than spin on linux and mac but not on windows for some reason I did not run this benchmark since I dont currently have windows installed but since the user hadnt filed an issue I decided to post it here their comment was The results Benchmark code Windows Pro Intel Core i k GHz stablex pcwindowsmsvc default rustc e extreme contention cargo run release Finished release optimized targets in s Running target release lockbenchexe Options nthreads nlocks nops nrounds stdsyncMutex avg ms min ms max ms parkinglotMutex avg ms min ms max ms spinMutex avg ms min ms max ms AmdSpinlock avg ms min ms max ms stdsyncMutex avg ms min ms max ms parkinglotMutex avg ms min ms max ms spinMutex avg ms min ms max ms AmdSpinlock avg ms min ms max ms heavy contention cargo run release Finished release optimized targets in s Running target release lockbenchexe Options nthreads nlocks nops nrounds stdsyncMutex avg ms min ms max ms parkinglotMutex avg ms min ms max ms spinMutex avg ms min ms max ms AmdSpinlock avg ms min ms max ms stdsyncMutex avg ms min ms max ms parkinglotMutex avg ms min ms max ms spinMutex avg ms min ms max ms AmdSpinlock avg ms min ms max ms light contention cargo run release Finished release optimized targets in s Running target release lockbenchexe Options nthreads nlocks nops nrounds stdsyncMutex avg ms min ms max ms parkinglotMutex avg ms min ms max ms spinMutex avg ms min ms max ms AmdSpinlock avg ms min ms max ms stdsyncMutex avg ms min ms max ms parkinglotMutex avg ms min ms max ms spinMutex avg ms min ms max ms AmdSpinlock avg ms min ms max ms no contention cargo run release Finished release optimized targets in s Running target release lockbenchexe Options nthreads nlocks nops nrounds stdsyncMutex avg ms min ms max ms parkinglotMutex avg ms min ms max ms spinMutex avg ms min ms max ms AmdSpinlock avg ms min ms max ms stdsyncMutex avg ms min ms max ms parkinglotMutex avg ms min ms max ms spinMutex avg ms min ms max ms AmdSpinlock avg ms min ms max ms I was trying to put my finger on what exactly it was that made RefMutdowngrade unsound in and wrote it down as part of a blog post To quote from there sorry to quote myself RwLockWriteGuard in parkinglot has a downgrade method which turns it into a RwLockReadGuard It is instructive to explore why RefCells RefMut cant provide a similar method to turn it into a Ref The signature of the closure used in RefMutmap is FnOnce mut T mut U This gives it an interesting property it allows you to bypass conventions of wrapped interior mutability types because you have exclusive access with asmut Because RefMutmap made the promise to wrapped types that its reference is unique the returned RefMut has to remain unique It cant be turned into a Ref of which multiple can exist Example of how it can go wrong rust let refcell RefCellnewCellnew u let refmut refcellborrowmut RefMutmaprefmut x xgetmut let ref RefMutdowngraderefmut let cellref ref let ref refcellborrow We can now mutate the Cell through ref while there also exists a reference to its interior For RwLockWriteGuard however downgrade is sound because its map method returns another type But that returned type MappedRwLockWriteGuard now has to remain unique so MappedRwLockWriteGuarddowngrade is unsound The deadlock detection code which can be enabled through a feature will break if a mutex can be unlocked in a different thread than the one that locked it It was decided that disabling MutexGuard Send if that feature is enabled would be unacceptable since it might break code that relies on MutexGuard Send Originally posted by Amanieu in While I understand the reasoning in the quote above its not ideal This a bit weird for me to say considering I proposed and implemented the PR that introduced the deadlock detection rofl Parkinglot is the only crate that cancould do Sendable guard AFAIK So maybe should try to preserve it somehow Sendable guards may come in handy as it just did for me and youd have to use an unsafe impl to work around it and hope that nobody enables deadlock detection Some ideas each with its own updownsides Reintroduce Sendable guards which is implicitly a deadlock feature Create a SendableMutexRwLock or something like that that dont participate in the deadlock detection Same as above but in another crate This is very rough but it was borne out of necessity Essentially parking is not something you can do in wasm so I was encountering flurries of parkinglot panics and not just from So this reimplements RawMutex and RawRwLock in the same way std has vastly simplified implementations for wasm Theres a bit of cruft and I cant claim to understand the meaning of the translated cmpxchg in RwLock but it seems to work and doesnt panic all the time Clearly it needs more eyes on it though Questions Does it actually need the deadlock calls I think the deadlock system might be using Instantnow What is the best practice to profile the RwLock or Mutex in parkinglot including Acquire locks TPS Histogram for lock wait time including min avg p p and max I know some tools like perf lock on Linux but it does not work well enough for me I really prefer to the Deadlock Detection feature that integrated within the parkinglot so any tool to achieve my goal I have a very weird situation where compiling a library on the machine my program is running will cause the mutex to get stuck locking But only when called from specific places There are a lot of moving parts to my project so I honestly doubt much can be fixed from it All I know is that using the mutex from parkinglot causes it to get stuck locking while using the stdsync mutex works fine Below Ive included a detailed description of my situation But as I said I honestly dont expect anything helpful to come from this due to the complexity of the whole thing If theres any other information I can give to help with the issue let me know My project is a discord bot using the Serenity crate Im implementing a fancy system to dynamically reload commands from a rust dylib helped by the libloading crate To keep things threadsafe I have a mutex around the inner framework of the bot which is modified whenever something is reloaded I noticed I got segfaults and other weird problems all of a sudden and it was quite confusing After some debugging I found out that some calls to lock the inner framework never completed leaving things in a weird broken state Specifically if Im recompiling the program and I issue a command that locks the inner framework the locking process will get stuck Notably this seems to only happen when the lock request is coming from some of the dynamically loaded code since the main core of the bot can still lock the framework no problem Whether its coincidental that its the dynamically loaded code that is able to cause it problem is unknown Ive not been able to make a minimally reproducible version of the error outside of my specific situation Though it is consistently reproducible within what I have Ive tried using the fair unlocking thinking there was locking contention but that had no effect Using trylockuntil failed after the timeout proving to me that its not in fact crashed the thread just stuck locking forever As I said switching to the stdsync mutex is the only thing that seems to fix the problem Heres a link to the repo of my project The code theres not entirely up to date but the part that causes is the issue is still present The underlying syscall is not implemented for Instantnow for the wasm unknownunknown target unless recompiling rust with xargo and enabling wasmsyscall panicked at Time system call is not implemented by WebAssembly host srclibstdsyswasmmodrs Stack Error at Modulewbgnew cb e ede webpackpkgspawnchainjs at wbgnew cb e ede at consoleerrorpanichookhookh df f ab fc wasmfunction at coreopsfunctionFncallh e a c ddf b wasmfunction at stdpanickingrustpanicwithhookh f f aaaaa wasmfunction at stdpanickingbeginpanich beec bc d wasmfunction at stdsyswasmTimeSysCallperformh f f fac d wasmfunction at stdsyswasmtimeInstantnowh de a c aa a wasmfunction at stdtimeInstantnowh c feeca e wasmfunction at parkinglotcoreparkinglotHashTablenewh e e aa d wasmfunction Finding a way to substitute the use of Instant in configuration or using a hooks system would be helpful to make this work for wasm unknownunknown to allow plugging in a time source that works for Web vs WASI etc Hello I really like your job I use this crate really a lot and I found the lack of a Future integration quite sad because I often write futurebased software and using locks inside them isnt a good choice So I spent a bit of my free time and I came up with this really simple and quite incomplete implementation I dont know if you had other plans about this topic maybe youll find my work horrible but I wanted to share my two cents Best regards