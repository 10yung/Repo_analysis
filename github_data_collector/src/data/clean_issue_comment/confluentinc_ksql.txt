 Describe the bug When inserting a record into a stream thats serialized with Avro a poor error message is raised if ksqlDB cant connect to Schema Registry To Reproduce With ksqlDB where Schema Registry is not configured on the server sql CREATE STREAM s c varchar WITH kafkatopict partitions keyc valueformat avro insert into s c values a Expected behavior An error message indicating that ksqlDB couldnt connect to Schema Registry Actual behaviour Failed to insert values into S Could not serialize row a This error message is tough because in a more complex setting it leads the user to believe that its a problem with their data Kafka Topic should be KSQL object or similar its definitely not Kafka topic though In this example here the topic is BAR but SHOW QUERIES says its FOO which is the name of the stream ksql CREATE STREAM FOO WITH KAFKATOPICBAR AS SELECT FROM KSQLPROCESSINGLOG Message Stream FOO created and running Created by query with query ID CSASFOO ksql SHOW QUERIES Query ID Kafka Topic Query String CSASFOO FOO CREATE STREAM FOO WITH KAFKATOPICBAR PARTITIONS REPLICAS AS SELECT FROM KSQLPROCESSINGLOG KSQLPROCESSINGLOG EMIT CHANGES For detailed information on a Query run EXPLAIN Query ID ksql This may seem like a nit but if youre using SHOW QUERIES and want to match a query with a stream this mislabeling makes things more difficult This is to revert the temporary disabling of deprecated API usage in KsLocator once we land fix for BREAKING CHANGE standalone literals that used to be doubles will now be interpreted as BigDecimal In most scenarios this wont affect any queries as the DECIMAL can autocast to DOUBLE in the case were the literal standsalone the output schema will be a DECIMAL instead of a DOUBLE To specify a DOUBLE literal use scientific notation eg E fixes Description The DIGIT DIGIT DIGIT pattern will now resolve to a Decimal while anything in scientific notation DIGIT DIGIT EXPONENT DIGIT EXPONENT will resolve to a floating point double value This is in line with SQL Server IBM DB and is mostly consistent with Postgres which claims that Constants that contain decimal points andor exponents are always initially presumed to be type numeric numeric is the equivalent of our decimal type Testing done Added new tests were applicable and updated all old tests that used double constants Reviewer checklist Ensure docs are updated if necessary eg if a user visible feature is being added or changed Ensure relevant issues are linked description should include text like Fixes issue number Is your feature request related to a problem Please describe Today we cant INSERT VALUES with an empty array even though we can infer the type from the schema because the code path doesnt have this information Describe the solution youd like Pass along the target schema when producing values to insert into See for more info Parent issue instead of JsonObject Please see Parent issue eg in ApiTest Although IMHO the benefits are not clear and there is a significant learning curve and more complexity in constructing asserts Description This patch removes the qualifier field from LogicalSchema and ColumnRef We still need to support column reference expressions with qualifiers These are now implemented using a new expression type called QualifiedColumnReferenceExp QualifiedColumnReferenceExp contains a SourceName qualifier and a ColumnRef The parser creates a QualifiedColumnReferenceExp for subexpressions that have a source dereference Most of the query execution utilities codegen expression analyzer should only worry about schemas and expressions Since schemas dont specify a source these classes cannot handle qualified column references Therefore these classes now throw when they see qualified column references This also means that LogicalPlanner has to resolve all qualified column refs to unqualified column refs in the final plan Finally without source qualifiers joins are problematic because the two sources may have clashing names To solve this LogicalPlanner first projects each source to a new schema that includes the source name as a prefix It also rewrites all expressions in the query to use the prefixed column names BREAKING CHANGE the schema in repartitionchangelog topics for joins now includes the source name as a prefix How to review First look at the changes to LogicalSchema and ColumnRef to drop qualifiers Then look at the new QualifiedColumnReference expression type and see how its handled by the different expression visitors SqlToJavaVisitor ExpressionTypeManager ExpressionFormatter Finally review the changes to engine particularly those to LogicalPlanner to build the logical plan with the right column references Parent issue Consider basing the new API server config on AbstractConfig