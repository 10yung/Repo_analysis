Reopend from NoamLR lr optimizerlr modelsize minstep step warmupstep WarmupLR lr optimizerlr warmupstep minstep step warmupstep WarmupLR is almost same as NoamLR except for the scale factor for the base lr In this new scheduler the maximum learning rate is scheduled to equal to the optimizers learning rate as it I think this is more intuitive With this new scheduler I changed NoamLR to Deprecated I modified the model a little to make it fit my task and ran it with the WSJ recipe In the training stage it worked well but when doing recognition it gave error as follows splitjson INFO gshs tgatslabbowenzespnettoolsvenvbinpython gshs tgatslabbowenzespnetegswsjasr utilssplitjsonpy parts dumptestdev deltafalsedatajson splitjson INFO gshs tgatslabbowenzespnettoolsvenvbinpython gshs tgatslabbowenzespnetegswsjasr utilssplitjsonpy parts dumptesteval deltafalsedatajson splitjson INFO number of utterances splitjson INFO number of utterances bash line Killed asrrecogpy config confdecodeyaml ngpu backend pytorch recogjson dumptestdev deltafalsesplit uttdata json resultlabel exptrainsi pytorchtrainnopreprocessdecodetestdev decodelmword data json model exptrainsi pytorchtrainnopreprocessresultsmodellast avgbest wordrnnlm exptrainrnnlmpytorchlmword rnnlmmodelbest exptrainsi pytorchtrainnopreprocessdecodetestdev decodelmword logdecode log exptrainsi pytorchtrainnopreprocessdecodetestdev decodelmword logdecode log bash line Killed asrrecogpy config confdecodeyaml ngpu backend pytorch recogjson dumptestdev deltafalsesplit uttdata json resultlabel exptrainsi pytorchtrainnopreprocessdecodetestdev decodelmword data json model exptrainsi pytorchtrainnopreprocessresultsmodellast avgbest wordrnnlm exptrainrnnlmpytorchlmword rnnlmmodelbest exptrainsi pytorchtrainnopreprocessdecodetestdev decodelmword logdecode log exptrainsi pytorchtrainnopreprocessdecodetestdev decodelmword logdecode log bash line Killed asrrecogpy config confdecodeyaml ngpu backend pytorch recogjson dumptesteval deltafalsesplit uttdata json resultlabel exptrainsi pytorchtrainnopreprocessdecodetesteval decodelmword data json model exptrainsi pytorchtrainnopreprocessresultsmodellast avgbest wordrnnlm exptrainrnnlmpytorchlmword rnnlmmodelbest exptrainsi pytorchtrainnopreprocessdecodetesteval decodelmword logdecode log exptrainsi pytorchtrainnopreprocessdecodetesteval decodelmword logdecode log The error led me to the log files decode log but when I opened them no error or traceback was shown in the log Also most of the decode logs have already shown the predictions and recognition results the results seem favorable enough but still I cannot avoid such line Killed error Ive run the recognition part several times it always gave three line killed errors but the decode log files that were pointed to differed each time It would be very helpful if you can indicate what is probably going wrong as I dont really have any clue about it thanks a lot In the directory egsljspeechtts runsh line teachermodelpath I fist run the this scirpt for with trainconfigconftuningtrainpytorchtransformerv yaml Now I want to train a fastspeech model but I am not sure which path is the teachermodelpath does anybody can help me teachermodelpath teacherdecodeconfigconfdecodeforknowledgedistyaml dofilteringfalse whether to do filtering using focus rate focusratethres for phn taco around phn transformer around if you want to do filtering please carefully check this threshold thanks for any help I just started to modify JSUT recipe to make it ASRTTS Ref creatorscanasrtts v After Stage I found the above ASRTTS recipe uses XVector Pretained Model English Check pretrained model existence if e nnetdir then echo Xvector model does not exist Download pretrained model wget tar xvf sitwv atargz mv sitwv aexpxvectornnet a exp rm rf sitwv atargz sitwv a fi Extract xvector for name in trainset devset evalset do sidnnet xvectorextractxvectorssh cmd traincmd mem G nj nj nnetdir datanamemfcc nnetdirxvectorsname done Update json for name in trainset devset evalset do localupdatejsonsh dumpdirnamedatajson nnetdirxvectorsnamexvectorscp done I wonder if there is any pretained XVector Model if not I guess I have to create by myself to apply ASRTTS to JSUT assuming XVector Pretained model is language dependent Thank you I am getting this error Any solution please sudo runsh dockergpu dockeregs chime asr dockerfolders exportcorpora CHiME CHiME dlayers ngpu sudo password for shafkat Building docker image Now running docker build buildarg FROMTAGgpucuda cudnn u buildarg THISUSERshafkat buildarg THISUID f prebuiltDockerfile t espnetespnetgpucuda cudnn u usershafkat Sending build context to Docker daemon kB Step ARG FROMTAG Step FROM espnetespnetFROMTAG b f b Step LABEL maintainer Nelson Yalta nyalta gmailcom Using cache ed c bb d Step ARG THISUSER Using cache c f b e Step ARG THISUID Using cache ce ad c bb e Step RUN if z THISUID then useradd m r u THISUID g root THISUSER fi Running in c c f useradd UID is not unique The command binsh c if z THISUID then useradd m r u THISUID g root THISUSER fi returned a nonzero code Hi there Im currently decoding my CTC model mtlalpha with pure CTC mode ctcweight and find that in current implementation ctcprefixsearch will also compute blank tokens nextlabelscore which to my understanding shall be excluded Details below With ctcweight ctcbeam becomes the size of whole vocabulary including blank token Then blank tokenid will be included in localbestids and prefixscore computed To my understanding blank should be excluded here This is what I get after I run make checkinstall command Building CXX object CMakeFilestesttimedirtestsrandomcppo Linking CXX executable testtime make Leaving directory dataahnafespnetasrespnettoolswarptransducerbuild Built target testtime make Leaving directory dataahnafespnetasrespnettoolswarptransducerbuild make Leaving directory dataahnafespnetasrespnettoolswarptransducerbuild venvbinactivate cd warptransducerpytorchbinding python setuppy install running install running bdistegg running egginfo creating warprnntpytorchegginfo writing warprnntpytorchegginfoPKGINFO writing dependencylinks to warprnntpytorchegginfodependencylinkstxt writing toplevel names to warprnntpytorchegginfotopleveltxt writing manifest file warprnntpytorchegginfoSOURCEStxt reading manifest file warprnntpytorchegginfoSOURCEStxt writing manifest file warprnntpytorchegginfoSOURCEStxt installing library code to buildbdistlinuxx egg running installlib running buildpy creating build creating buildliblinuxx creating buildliblinuxx warprnntpytorch copying warprnntpytorchinitpy buildliblinuxx warprnntpytorch running buildext building warprnntpytorchwarprnnt extension creating buildtemplinuxx creating buildtemplinuxx src gcc pthread B dataahnafespnetasrespnettoolsvenvcompilercompat Wlsysroot Wsigncompare DNDEBUG g fwrapv O Wall Wstrictprototypes fPIC Idataahnafespnetasrespnettoolswarptransducerinclude Idataahnafespnetasrespnettoolsvenvlibpython sitepackagestorchlibinclude Idataahnafespnetasrespnettoolsvenvlibpython sitepackagestorchlibincludetorchcsrcapiinclude Idataahnafespnetasrespnettoolsvenvlibpython sitepackagestorchlibincludeTH Idataahnafespnetasrespnettoolsvenvlibpython sitepackagestorchlibincludeTHC Idataahnafespnetasrespnettoolsvenvincludepython m c srcbindingcpp o buildtemplinuxx srcbindingo stdc fPIC DWARPRNNTENABLEGPU DTORCHAPIINCLUDEEXTENSIONH DTORCHEXTENSIONNAMEwarprnnt DGLIBCXXUSECXX ABI cc plus warning command line option Wstrictprototypes is valid for CObjC but not for C In file included from dataahnafespnetasrespnettoolsvenvlibpython sitepackagestorchlibincludeTHCTHCh from srcbindingcpp dataahnafespnetasrespnettoolsvenvlibpython sitepackagestorchlibincludeTHCTHCGeneralh fatal error cublasv h No such file or directory include cublasv h compilation terminated error command gcc failed with exit status Makefile recipe for target warptransducerdone failed make warptransducerdone Error espnet shafkatservermasterdataahnafespnetasrespnettools Things I have done to install Installed the kaldi package using conda conda install c condaforge kaldi Cloned the espnet repo from github Used this command make j PYTHONVERSION CUDAVERSION Got kaldidone error though Used this command after then make checkinstall And got the abovementioned error Am I missing something to do Anyone test this This PR is too complex to explain so Id like to show the examples only eg nodes and processes per node ie worldsize without any launcher multiprocessingdistributed is true by default host python m espnet binasrtrain ngpu distrank distworldsize distmasteraddr host distmasterport host python m espnet binasrtrain ngpu distrank distworldsize distmasteraddr host distmasterport eg nodes and processes per node ie worldsize with launcher with slurm srun c N ntaskspernode gres gpu python m espnet binasrtrain ngpu distinitmethod filepwdexpasrtraindistinitopenssl rand base distlauncher slurm with mpi This doesnt mean backendmpi mpirun np python m espnet binasrtrain ngpu distinitmethod filepwdexpasrtraindistinitopenssl rand base distlauncher mpi If your file system doesnt support fcntl then use distmasteraddr and distmasterport eg Single node and multiGPUs multiprocessingdistributed mode default This mode is more efficient than Conventional DataParallel python m espnet binasrtrain ngpu Conventional DataParallel multiGPUs python m espnet binasrtrain multiprocessingdistributed false ngpu I added droplast arguments for Batch Sampler For training droplast is true In Distributed training minibatch is divided by worldsize and each worker must have or more batchsize To avoid batchsize droplasttrue for training For training droplast is false For validation droplast is false Validation mode perform only at RANK worker For inference droplast is false By default droplast is false the PR is related to it also takes some findings from for running ami babel commonvoice The asr is still in training waiting for the result