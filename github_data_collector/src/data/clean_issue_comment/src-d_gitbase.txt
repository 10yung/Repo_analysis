Hey when I try to run the following example in the db SELECT repositoryid filepath JSONUNQUOTEJSONEXTRACTbl linenum JSONUNQUOTEJSONEXTRACTbl author JSONUNQUOTEJSONEXTRACTbl text FROM SELECT repositoryid filepath EXPLODEBLAMErepositoryid commithash filepath AS bl FROM refcommits NATURAL JOIN blobs NATURAL JOIN commitfiles WHERE refname HEAD AND NOT ISBINARYblobcontent as p WHERE JSONEXTRACTbl text LIKE TODO I get the following error ERROR HY unknown error A function blame not found Im new to sourced and using the community edition Could you guys point me in the right direction For some reason SHOW FUNCTION STATUSis working either so Im having problems debugging this MySQL gitbase select blobhash repositoryid from blobs natural join repositories where blobhash in ec b ddb adf ebddbc c aad fe fd d eff afc cea c ed eb daf e f ea d fe af blobhash repositoryid aad fe fd d eff afc cea c githubcombblfshjavascriptdriver ed eb daf e f ea d fe af githubcomsrcdenry aad fe fd d eff afc cea c githubcombblfshpythondriver ec b ddb adf ebddbc c githubcomsrcdgomysqlserver aad fe fd d eff afc cea c githubcombblfshrubydriver ed eb daf e f ea d fe af githubcomsrcdgitbase rows in set sec MySQL gitbase select blobhash repositoryid from blobs where blobhash in ec b ddb adf ebddbc c aad fe fd d eff afc cea c ed eb daf e f ea d fe af blobhash repositoryid aad fe fd d eff afc cea c githubcombblfshpythondriver aad fe fd d eff afc cea c githubcombblfshjavascriptdriver ed eb daf e f ea d fe af githubcomsrcdenry aad fe fd d eff afc cea c githubcombblfshrubydriver ec b ddb adf ebddbc c githubcomsrcdgitbase ed eb daf e f ea d fe af githubcomsrcdgitbase ec b ddb adf ebddbc c githubcomsrcdgomysqlserver ed eb daf e f ea d fe af githubcomsrcdgomysqlserver rows in set sec also note that removing the natural join makes things go much faster it was my understanding that normally we want to join with repositories to benefit from some specific optimizations although Im guessing that filtering with blobhash makes those optimizations moot In gitbase schema introspection is fast and full MySQL ConnectorJ JDBC metadata call that gets all columns for all tables at once metaDatagetColumnsgitbase is converted to calls like the following for each table SHOW FULL COLUMNS FROM committrees FROM gitbase LIKE In and rc the above queries are very slow several minutes and even fail for some tables completely in seems to fix that The above prevents from using gitbase in DB tools like JetBrains DataGrip The previous version was unable to bold the secondlevel title I keep getting unknown error object not found but there are no logs about this error for debugging Example docker exec it ca mysql Welcome to the MariaDB monitor Commands end with or g Your MySQL connection id is Server version Vitess Copyright c Oracle MariaDB Corporation Ab and others Type help or h for help Type c to clear the current input statement MySQL none SELECT commitauthorname FROM commits ERROR HY unknown error object not found But all I see in the logs time T Z leveldebug msgexecuting query querySELECT commitauthorname FROM commits time T Z levelinfo msgaudit trail actionauthorization address connectionid permissionread pid querySELECT commitauthorname FROM commits successtrue systemaudit userroot time T Z levelinfo msgaudit trail actionquery address connectionid duration s errobject not found pid querySELECT commitauthorname FROM commits successfalse systemaudit userroot I tried adding Gitbase as a MySQL database to metabase but unfortunately it got stuck trying to connect over and over again asking for some variables that werent supported This doesnt have high priority but would be a nice addition to make work Error on gitbase side gscgitbase time T Z levelinfo msgNewConnection client gscgitbase time T Z levelerror msgCannot parse client handshake response from client Code INTERNAL nparseClientHandshakePacket cant read connection attribute value n Error from the client side gscgitbasesparkconnectorjupyter Exception in thread main javasqlSQLNonTransientConnectionException Could not connect to gitbase unexpected end of stream read bytes from socket was closed by server gscgitbasesparkconnectorjupyter at orgmariadbjdbcinternalutilexceptionsExceptionMappergetExceptionMapperjava gscgitbasesparkconnectorjupyter at orgmariadbjdbcinternalutilexceptionsExceptionMappergetExceptionExceptionMapperjava gscgitbasesparkconnectorjupyter at orgmariadbjdbcinternalprotocolAbstractConnectProtocolconnectWithoutProxyAbstractConnectProtocoljava gscgitbasesparkconnectorjupyter at orgmariadbjdbcinternalutilUtilsretrieveProxyUtilsjava gscgitbasesparkconnectorjupyter at orgmariadbjdbcMariaDbConnectionnewConnectionMariaDbConnectionjava Only happening with the latest v beta release sql SELECT uastextract uastblobcontent csharp csharpBinaryExpressionAddExpressionLeftuastString csharpInterpolatedStringExpressioncsharpInterpolatedStringTextToken startswithnormalizespaceValue SELECT or startswithnormalizespaceValue select or startswithnormalizespaceValue UPDATE or startswithnormalizespaceValue update or startswithnormalizespaceValue DELETE or startswithnormalizespaceValue delete or startswithnormalizespaceValue INSERT or startswithnormalizespaceValue insert or startswithnormalizespaceValue CREATE or startswithnormalizespaceValue create or startswithnormalizespaceValue ALTER or startswithnormalizespaceValue alter or startswithnormalizespaceValue DROP or startswithnormalizespaceValue drop pos AS positions repositoryid filepath FROM SELECT frepositoryid ffilepath bblobcontent FROM SELECT FROM refs r NATURAL JOIN commitblobs cb NATURAL JOIN blobs WHERE rrefname HEAD AND NOT ISBINARYblobcontent b INNER JOIN SELECT repositoryid filepath blobhash FROM refs r NATURAL JOIN commitfiles cf WHERE rrefname HEAD f ON bblobhash fblobhash AND brepositoryid frepositoryid WHERE languageffilepath bblobcontent C t WHERE positions IS NOT NULL This could be parallelized adding an exchange over the topmost projection Instead we do this serially causing extremely low performance on queries using uast functions on the topmost projects under certain conditions discovered at considering the old Im not sure if you already discarded the interoperability with MySQL Workbench so feel free to direct close if it makes no sense at all dagger I tried to connect to gitbase using MySQL Workbench even its not explicitly supported by gitbase and I found some issues that made Workbench to crash currentuser is not supported show status is not supported show engines is not supported I could mock data for all of them in local gitbase and doing so Workbench was able to start with a warning Incompatiblenonstandard server version or connection protocol detected A connection to this database can be established but some MySQL Workbench features may not work properly since the database is not fully compatible with the supported versions of MySQL MySQL Workbench is developed and tested for MySQL Server versions and Continue anyway Abort If you continue wink Workbench opens the pannel with the connection but there is no tables shown in the left panel plus a log sql Error loading schema content Error Code MySQLResultSetgetString invalid value of columnIndex And fetching commits also fails sql select from commits select from commits LIMIT Fetching select from commits LIMIT Error Code Please reportn charsetnr I wonder if you could give some hint to let someone work on this during some OSD thinking or just discourage it if you see it an impossible thing Right now since partition means repository we know joins by repository can only happen in the same partition Instead we iterate and try to join with all of the partitions together Imagine we have partitions These are the rows returned by each partition in the left side of a join P P P These are the rows returned by each partition in the right side of a join P P P We are joining rows with rows which produces rows that are then filtered by the join conditions but we still make those k iterations Instead if we did this per partition these would be the produced rows then filtered by conditions P P P The total amount of rows produced is rows which is a of the number of rows generated before This number grows enormously as the number of partitions and rows grow What could we do A rule that runs at the end of the analysis and transforms joins the ones left after the squash into something like Concat InnerJoin PartitionTableTableA PartitionTableTableB PartitionTable is a table that will only return the rows for one partition Concat is a node that will iterate over all partitions and transform all its Table children into PartitionTable Then all the rows of each partition will be put together and returned to the user This will also happen in parallel Essentially Concat is like an Exchange The only thing it differs is the fact that it can handle binary nodes and not only unary nodes This is something that cannot be done in gomysqlserver but can be done here since we know for certain that partitions are the same for each table Called it Concat but the name is pretty lame so we should think of a better name like PartitionExchange BinaryExchange or something like that This should make not squashed joins and in a real life applications you will have many of them because leaves will be subqueries much much faster