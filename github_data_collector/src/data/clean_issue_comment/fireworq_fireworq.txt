Great project I am very interested in using this in my own project but would like to deploy in Kubernetes and am finding a bit difficult to build these images locally not using dockercompose Any chance you can create a docker image and push an official image to docker hub Pull request to List at In order to improve performance maybe there will be useful to add some benchmarks to readme some Sidekiq vs Fireworq performance architecture diagram etc Hello I was wondering if it is possible to save the job when they finish and the result is successful now I think they are eliminated but for example if I need to keep track of the job that results successful and the result of each job means something useful to verify maybe I could have an option that saves the successful jobs for a period of time and then it is eliminated that would help not to save a lot of data in the database pd Merry Christmas santa Hi there First of all nice promising project Im looking into using it but there are some things Id like to point out you seem to rely on MySQLs locking functions GETLOCK and ISUSEDLOCK for coordination While thats ok for small singleinstance MySQL instances that wont work on XtraDB clusters I have a node XtraDB cluster that I would like to use with fireworq for highavailability and redundancy but because of the nature of distributed transactions and locking these functions are not permitted ERROR HY PerconaXtraDBCluster prohibits use of GETLOCK with pxcstrictmode ENFORCING same with LOCK TABLE and friends I suggest using an external etcd instance for leader election if you dont want to depend on etcd you may implement your own leader election logic using raft so all fireworq nodes would talk directly to each other to determine whos the leader in the fireworq cluster if the leader goes down the other remaining machines would decide between themselves who the new leader is we can get this for free with etcd relying on AUTOINCREMENT IDs While that works great for singleinstance MySQL servers it makes things more complicated when running under a cluster because nodes need to coordinate between them so IDs dont collide Since youre already using uint s for IDs I would suggest using something more deterministic like the current time in nanoseconds resolution or something else more guaranteed to be unique so that IDs can be generated by fireworq nodes themselves and INSERTed directly into the database without relying on AUTOINCREMENT while guaranteeing that theyre unique and timesorted and thus having good indexing properties The end result would be that job queueing could be done by any fireworq instance in the cluster not just the master with minimal to no coordination between them eliminating the primarybackup roles It would be possible to scale up fireworq to have many nodes capable of working at the same time for highavailability redundancy and increased performance For example imagine that you have a node fireworq cluster and youre able to queue jobs to any of them at any given time and have a load balancer in front of them so that downed fireworq nodes are automatically worked around This could be done by adding another field to a job queue like ringkey INT UNSIGNED NOT NULL and whenever pushing a new job to a queue you just add a random uint value there lets say from to this will be the maximum number of nodes a cluster can have Whenever nodes join or leave the cluster you rebalance their ring assignments so each currentlyalive node gets assigned a portion of the ring of all possible values from till For example if your cluster has nodes the ring would look like node from to keys node from to keys node from to keys node from to keys So for example node would poll the database for available work to be done by including in the query something like AND ringkey BETWEEN AND Say node goes offline because the machine was turned off the leader node determined by the leader election algorithm either by using an external etcd instance or builtin Raft would detect that and rebalance the ring between the remaining nodes that are alive and announce to all machines this new distribution so our distribution now would look like node from to keys node from to keys node from to keys This coordination between the leader node and followers could also be backed by etcd or managed manually through raft if you want to avoid adding an external dependency If the leader itself goes down Raftetcd takes care of a new election for a new leader and once that new leader is elected it triggers the same rebalance process this way all nodes keep on working as if nothing happened and can scale in capacity up and down MySQLPXC scaling would be the bottleneck What do you think Is that something you guys would be interested in implementing If not I could look into adding support for that myself using etcd since Ill need it but it might be a while until Ill be able to start working on that Nice project but adding a new database backend just for the fireworq is not very nice What about having different kind of storage backend like MySQL MongoDB or others 