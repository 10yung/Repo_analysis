Hello I had previously been able to link applications against jemalloc on Aarch However starting glibc I was unable to do so Here is a minimal error Minimal error bash eochoaosprey temp cat ac include stdlibh int main int argc char argv malloc eochoaosprey temp gcc O g ac ljemalloc lpthread static eochoaosprey temp aout Bus error core dumped Please make sure that either glibc is installed and you are running on an aarch machine or that you specifically link against glibc like so eochoaosprey temp gcc O g ac ljemalloc lpthread static pathtolibca Stack trace Using gdb I found the stack trace when SIGBUS was sent Two interesting things to note is that the error is triggered when clockgettime is called and very early during the program during libcinitfirst x efaf adad c c in x cb in clockgettime clockid tp xffffffffc at sysdepsunixsysvlinuxclockgettimec x b c in nstimeget time xffff b at srcnstimec x bb in nstimeupdateimpl time xffff b at srcnstimec x c in arenadecayreinit decay xffff ab decayms at srcarenac x b in arenadecayinit decay xffff ab decayms stats xffff df at srcarenac x in jearenanew tsdn x ind extenthooks x jeextenthooksdefault at srcarenac x fec in arenainitlocked tsdn x ind extenthooks x jeextenthooksdefault at srcjemallocc x a b in jearenainit tsdn x ind extenthooks x jeextenthooksdefault at srcjemallocc x f a in mallocinitharda locked at srcjemallocc x fb in mallocinithard at srcjemallocc x b in mallocinit at srcjemallocc x a in imallocinitcheck sopts xffffffffe dopts xffffffffe f at srcjemallocc x bf in imalloc sopts xffffffffe dopts xffffffffe f at srcjemallocc x d in jemallocdefault size at srcjemallocc x ff in malloc size at srcjemallocc x f c in dlgetorigin at sysdepsunixsysvlinuxgenericdloriginc x ce a in dlnondynamicinit at dlsupportc x cf in libcinitfirst argcargcentry argvargventry xfffffffff envp xfffffffff at csuinitfirstc x b ac in libcstartmain main x c main argc argv xfffffffff init x b libccsuinit fini x b libccsufini rtldfini x stackendoptimized out at csulibcstartc x c in start at sysdepsaarch startS Bisecting glibc I did a git bisect from glibc to glibc and found the offending commit which enables VDSO for static linking Since clockgettime is part of VDSO this means that clockgettime should be able to link statically as well The problem However dlnondynamicinit which has a run time path to malloc is called before VDSO has been setup Because malloc is now jemalloc it will attempt to use clockgettime which has not been setup properly and thus access invalid memory The possible solution Im also filing a bug in glibc tracker to apply this patch This makes the sample program run however there is another setupvdso function which without the patch applied is called before VDSOSETUP and I am unsure about the interactions between them diff git acsuinitfirstc bcsuinitfirstc index f d cc d c acsuinitfirstc bcsuinitfirstc init int argc char argv char envp libcargv argv environ envp ifdef VDSOSETUP VDSOSETUP endif ifndef SHARED First the initialization which normally would be done by the dynamic linker dlnondynamicinit endif ifdef VDSOSETUP VDSOSETUP endif initmisc argc argv envp Initialize ctype data It will happen jemalloc error when we do reboot test In the androidexternaljemallocnewincludejemallocinternalbitmaph bitmapsfu bitmapset gp bitmap binfolevels i groupoffset goff g gp error in this DEBUG pid tid name Binder systembinvold DEBUG uid DEBUG signal SIGSEGV code SEGVMAPERR fault addr xa adc DEBUG r ea ac r ea c r a ab r fffffe DEBUG r r ffffffdf r ea f r ea DEBUG r r ffbbd b r ea r ea DEBUG ip ea sp ffbbd a lr f pc ea ddc DEBUG DEBUG backtrace DEBUG pc ddc apexcomandroidruntimelibbioniclibcso arenaslabregalloc BuildId e da c d d d edbd a dd DEBUG pc apexcomandroidruntimelibbioniclibcso jearenamallochard BuildId e da c d d d edbd a dd DEBUG pc cb apexcomandroidruntimelibbioniclibcso jemalloc BuildId e da c d d d edbd a dd DEBUG pc apexcomandroidruntimelibbioniclibcso malloc BuildId e da c d d d edbd a dd DEBUG pc c systemliblibcso operator newunsigned int BuildId c ffca f fae c DEBUG pc systembinvold std basicstringbufchar std chartraits std allocatorstr const BuildId d e ca c d f b cf f ac DEBUG pc systemliblibbaseso androidbaseLogMessageLogMessage BuildId a cfcb efe fdbfb b DEBUG pc e b systembinvold preparedirstd basicstring std allocator const unsigned short unsigned int unsigned int BuildId d e ca c d f b cf f ac DEBUG pc f systembinvold fscryptprepareuserstoragestd basicstring std allocator const unsigned int int int BuildId d e ca c d f b cf f ac DEBUG pc c systembinvold fscryptinituser BuildId d e ca c d f b cf f ac DEBUG pc cd systembinvold androidvoldVoldNativeServiceinitUser BuildId d e ca c d f b cf f ac DEBUG pc d f systembinvold androidosBnVoldonTransactunsigned int androidParcel const androidParcel unsigned int BuildId d e ca c d f b cf f ac DEBUG pc ee systemliblibbinderso androidBBindertransactunsigned int androidParcel const androidParcel unsigned int BuildId d df a b ab fae a cba DEBUG pc b a systemliblibbinderso androidIPCThreadStateexecuteCommandint BuildId d df a b ab fae a cba DEBUG pc addf systemliblibbinderso androidIPCThreadStategetAndExecuteCommand BuildId d df a b ab fae a cba DEBUG pc b b systemliblibbinderso androidIPCThreadStatejoinThreadPoolbool BuildId d df a b ab fae a cba DEBUG pc b e systembinvold main BuildId d e ca c d f b cf f ac DEBUG pc apexcomandroidruntimelibbioniclibcso libcinit BuildId e da c d d d edbd a dd DEBUG pc b systembinvold startmain BuildId d e ca c d f b cf f ac n DEBUG pc anonymouseb e Reentrancy is already set for other nonnominal tsd states reincarnated and minimalinitialized Add purgatory to be safe and consistent The decrementrecentcount call is made to be async and theres no concurrent lock holdings at any time The issue was detected in a stress test I was writing Im also appending the stress test to this PR Resolves Also added stronger assertions in the buffered writer initializer This can help triage issues eg inefficiency caused by frequent thread creation termination Dumping process mapping in a native manner superseeds A few remarks Im chopping lastN dumping into batches each under the profrecentallocmtx so that sampled malloc and free can proceed between the batches rather than being blocked until the entire dumping process finishes Im using the existing profdumpmtx to cover the entire dumping process during which I first change the limit to unlimited so that existing records can stay then perform the dumping batches and finally revert the limit back and shorten the record list profdumpmtx serves to only permit one thread at a time to dump either the lastN records or the original stacktracebased profiling information An alternative approach is to use a separate mutex for the lastN records so that the two types of dumping can take place concurrently Thoughts Im additionally changing the mallctl logic for reading and writing the limit they now need the profdumpmtx For reading this ensures that whats being read is always the real limit For writing this ensures that the application cannot change the limit during dumping The downside is that the mallctl calls are blocked until the entire dumping process finishes but I think its fine because the mallctl calls are very rare and only initiated by the application Im increasing the buffer size to be the same as the size used by stats printing and the original profiling dumping I think I could even consolidate the lastN buffer with the original profiling buffer especially since Im already using profdumpmtx Thoughts I could have a separate commit for that since thatd also need some refactoring of the original profiling dumping logic The batch size is chosen to be I figured making such a choice is quite tricky and heres how I get it The goal Im pursuing is to find a batch size so that each batch can trigger at most one IO procedural call the worst case blocking time is always at least one IO so a smaller batch size cannot reduce the worst case blocking time while a larger batch size can multiply the worst case blocking time The amount of output per record depends primarily on a the length of the stack trace and b whether the record has been released two stack traces if released one if not I examined lastN dumps from production per service and found that one of the services happened to have both the longest stack traces and the highest proportion of released records and the average length per record for that service is in the order of characters in compact JSON format So if I set the batch size to be records each batch will at most output less than but close to K characters which is the size of the buffer