Reproducible example rust extern crate coio extern crate envlogger use coioScheduler fn main envloggerinit Schedulernew run struct A impl Drop for A fn drop mut self Schedulersched let a A panicPANICKED in coroutine unwrap While the coroutine is unwinding Drop of A will be called Coroutine is yield in the drop function Coroutine is now suspended which means that the currently thread is still in panicking status but execution process is now be switched to another coroutine If the other coroutine panic too then it will definitely cause panic while panicking If I understand it correctly the coroutines here can move from thread to thread Now imagine I have something that is not Send for whatever reason And I dont mean something like Rc which could cause havoc if I put it into TLS but thats another issue but something like ZeroMQ socket which is promised to crash the whole application if ever touched from a different thread But it is on the stack so it passes all compile time checks Moving the coroutine to another thread would now cause an UB while the user used only safe Rust This bug was originally found in The reason was stdlib uses a PANICCOUNT in TLS to ensure no panic while panicking in runtime But obviously Coroutines in coio can be migrated between threads which means that it turns out to cause data race because compiler still think that we are running in the same thread so we may access to the other threads TLS without any synchronization method We wanted to solve this PANICCOUNT partially in but because stdlib relies heavily on TLS such as println it will also causes SIGSEGV randomly rust printlnBefore Schedulersched Switch out Well now this Coroutine may already been stolen by the other thread And then resumed by the other thread printlnAfter Rusts compiler dont know that we have switched to another thread so it may inline those TLS calls We are looking for a solution for this bug discussing in here if you have any idea please help coroutineunwind was doing undefined things Ive replaced it with an AtomicBool flag triggering the same unwind code moved to yieldwith This isnt optimal yet Ill try to get it passing through the data field as zonyitoo suggests but it does pass all tests even with inlinealways Still needs x data field decide on proper inline attribute x outline the cold branch probably x delete commentedout code Now we can only add inlinenever on the Coroutineyieldwith method You can reproduce it anytime by replacing it with inlinealways and then call cargo test release you will see the coroutineunwindsondrop test will fail cargo test release test coroutinetestcoroutineunwindsondrop FAILED failures coroutinetestcoroutineunwindsondrop stdout thread coroutinetestcoroutineunwindsondrop panicked at assertion failed left right left right srccoroutiners note Run with RUSTBACKTRACE for a backtrace failures coroutinetestcoroutineunwindsondrop test result FAILED passed failed ignored measured error test failed Now in Coio Coroutine is the minimum execution routine It would be nice if we can have a coroutinelocal macro implementation just like threadlocal in std Implementation detail will be discussed later Minimal test case rust extern crate coio use coioScheduler use coiosyncmpsc fn main Schedulernewrun let tx rx mpscsyncchannel let h Schedulerspawnmove txsend unwrap asserteqrxrecvunwrap hjoinunwrap unwrap The program would panic with message thread Processor panicked at called Resultunwrap on an Err value RecvError srclibcoreresultrs note Run with RUSTBACKTRACE for a backtrace thread main panicked at called Resultunwrap on an Err value Any srclibcoreresultrs All IO operations needs to support a timeout parameters including readtimeout and writetimeout This issue was related to and I have implemented an experimental version in branch experimentaliotimeout But because of we may got a new implementation of runtime So this issue will be solved after the main part of new designed runtime is settle down Right now Processors will wait forever in its own channel until got notified by ProcMessage So it wont try to steal jobs from the other Processors So we have to add something to achieve Processor wont spin itself when it has nothing to do Processor can process the ProcMessages in time Processor can steal jobs from the other Processors Please add comments below Hi Zonytoo Id like to use coio in a publicfacing service accepting anonymous TCP connections If only to avoid file descriptors exhaustion there has to be a limit on the maximum amount of open connections While just closing a socket after having accepted it if we get close to the limit is an option a better practice is to close the oldest connection instead However I didnt see any obvious ways to do this when using coio How would you do TCP reuse 