Ive got a configuration similar to but where the storeremote operations are sometimes blocking Im also using an IndexedDBBucket for all sources which is persisting the transform log and related to this issue the requestsync queues In this scenario Ive discovered that if I reload the browser while some request is pending remote response then the storerequests and remoterequests items in the bucket contains those requests and upon reload both sources immediately attempt to reprocess those requests prior to activation If a request in the queue references some other identifier eg a findRelatedRecords request then because the store as a MemorySource starts out empty then it throws an exception Error Record not found somemodelaed a c ec b c bb f at new Exception vendorjs at new RecordException vendorjs at new RecordNotFoundException vendorjs at findRelatedRecords vendorjs at MemoryCachequery vendorjs at MemoryCachequery vendorjs at MemorySourcequery vendorjs at MemorySourceprotoquery vendorjs unhandled rejection Since this occurs prior to source activation it is not possible to pull the data from the backup and sync it into the store as described in From what I can tell in the only way to defer the processing of the requestQueue until a later point is to configure the requestQueueSettings with autoProcess false However this setting appears to require that all future operations on the source be followed with a call to requestQueueprocess which is not ideal Truthfully I think several other problems with the request queue initiating autoprocessing on instantiation No strategies are yet connected so replication of data from one source to another is missed and failures are not handled as expected It would be preferable to discard unnecessary queries for which the application is clearly not waiting on a response If there is a sequence of pending updates in the remote request queue it would be preferable to distill them down to a smaller set before sending them to the server Im filing this issue because I dont see an immediate solution for this problem and I am interested in receiving feedback on possible workarounds If it were possible to defer the request queue processing until source activation that would at least give the opportunity inspect and manipulate the queue which would solve some of the problem However the coordinator activates sources before strategies so the problem of missing strategies bullet above would still remain Ive found that invalid strategies can be added to a coordinator and even activated but they fail silently even with blocking true during task processing Some examples of invalid strategies Using SyncStrategy with JSONAPISource which is not Syncable Misspelling the action to invoke for the target this is generalization of the first case Misspelling the on to listen for the source In the first two cases the generated listener throws an exception when trying to invoke the missing action but unless the listener was attached to a beforeXxx event the exception is swallowed by settleInSeries In the third case the generated listener is attached to an event that will never fire so it has no effect To avoid confusion and help identify invalid strategies it would be ideal if these cases were detected during coordinatorstrategy activation I have a typical optimistic query fetching strategy btw memory remote sources Also I have a transform listener that queries the cache whenever there is a transform event on the memory source Also I use the liveQuery listener on the record cache in order to rerender the react components But while paginating when I skip the pages the cache query returns empty I can see this is happening because of the cache pagination logic js query page articlesoffset limit storememorycachequeryt tfindRecordsarticlepage offset limit returns the record set as expected skipped page query page articlesoffset limit page storememorycachequeryt tfindRecordsarticlepage offset limit returns empty Any thoughts on how to deal with this Orbit allows to pass to a source a batch of operations Transform and since beta of query expressions Query The problem is the way Query and Transform are handled is all or nothing Where it is aligned with JSONAPI spec proposition for operations it is a problem for different style of APIs We might want find a way to return partial successerror from sources This a new proposition to introduce query observers This one is not relying on any known interface such as RX or async iterator It uses orbit events It does expose an LiveQuery interface but it is simply an object with a subscribe method with returns a subscription with an unsubscribe method The current one has a downside of looking like an RXZen Observable but not being one I am worried it might confuse folks One option I considered is to pass the callback directly to liveQuery method but it makes it for a complex signature because of optional options and id arguments What do you think dgeb Hi I have used orbit with ember with JSONAPI very nice implementation Now I need to work on app which backend is in RESTApi Is there any guide on how to use that or if even this is implemented if not any plans for that thanks While I was trying to figure out the handling of dasherized fields something that made it harder for me to understand what was going on was that JSONAPISource will pass an invalid attribute through to the server and return it in the returned record For completelywrong attributes some servers would error out which would make the problem obvious Im not sure if erroring out is in the JSONAPI spec Even if it is in the spec some noncompliant servers may silently ignore invalid attributes and so it would be useful for Orbit to proactively error out in those cases In the case of dasherized fields it was more confusing because I was passing the dasherizedversion which is a valid server attribute but it doesnt correspond to the default formatting on the client side the client side should be camelized So when the server returned the dasherizedversion upon first load it didnt map to the local dasherizedversion but when I uploaded a dasherizedversion it did map to the dasherizedversion in the local field Lets look at a reproduction then proposals I have for two changes that could help Reproduction Example code Client API The schema is configured with createdat and deliveredat fields It seems that they should be createdAt and deliveredAt as Orbit automatically maps dasherized remote fields to camelized local ones But the scenario Im reproducing is accidentally using the dasherized fields locally const schema new Schema models widget attributes name type string createdat type datetime deliveredat type datetime When the app is loaded it loads all widgets which returns data id type widgets links self attributes name Widget createdat T Z deliveredat T Z So we have createdat and deliveredat fields present Then the app attempts to log out the results I get img width altScreen Shot at AM src So createdat and deliveredat fields are not shown This seems to be because Orbit camelizes the attributes by default it looks for createdAt and deliveredAt attributes in the schema and since they arent found the attributes are ignored Theres no problem with this behavior Then the app attempts to write deliveredat I call recordToUpdate recordToUpdate attributes recordToUpdateattributes deliveredat new Date memory updatet tupdateRecordrecordToUpdate thenresult consolelogresult It sends data type widgets id attributes name Widget deliveredat T Z And logs out the result as well So its able to write a deliveredat field and it includes it in the returned data afterward I get the same behavior if I remove deliveredat from the schema entirely The attribute returned from the server is not exposed in the schema But I can update the attribute and the updated attribute is included in the record resolved from the update call Proposal I think it would be more predictable for Orbit to not persist unrecognized attributes to the server and not return unrecognized attributes from update calls It could either error out warn or silently omit those attributes As this is a behavior change at least a temporary deprecation warning is probably a good idea This would lead to more consistency overall Orbit does not passthrough data returned from the server unchanged but only if attributes are included in the schema But currently it sends invalid attributes to the server and returns them in the resolved updated records which is inconsistent If updates would also ignore or errorwarn on invalid attributes that would be more consistent Once update attribute validation was in place another question arises in this specific code sample the dasherized attribute is a valid local attribute and presumably it would make sense to serialize it to dasherized in transport But due to Orbits default camelization behavior the data coming back from the server would be converted from dasherized to camelized and so would no longer match the field The problem in that case seems to be that our attribute name in the schema uses a format dasherized that will never match data coming from the server because Orbit camelizes it The best way to expose that could be to raise a warning that that dasherized attribute name is invalid With JSONAPISources Orbit by default seems to handle taking dasherized fields such as createdat and exposing them as camelized fields such as createdAt Is this documented anywhere in the guides I wasnt able to find it Like I imagine a lot of JSONAPI users I use Rails an JSONAPIResources which has a default behavior of mapping underscored field names such as createdat to dasherized such as createdat Having all three formats in my default stack is pretty confusing Its true that each works with the default of the platform its on But its a lot of automatic mapping thats not totally obvious in the guides of Orbit or JSONAPIResources Shortterm I think it would be helpful to document this mapping in the guides clearly If youre interested I could try to identify a place to put it and create a PR Longterm I think it would be worth considering migrating Orbit to not adjust fields by default I think a little more cumbersome squarebracket lookup is worth the predictability and I think it would help new users adopt the library Im sure this has been discussed before what are your thoughts on the tradeoffs This behavior may occur in the following situation We have object type planet We store some information for planet in toplevel meta field We have two ways to get planet data both return same data with same meta field Endpoint planetid In included part of planetfilter Memory cache is enabled Then you take the following steps Ensure that memory cache is empty Fetch planetfilter classification terrestrial If you look at memory cache of earth you see meta field is present Fetch planet data with planetearth In network tools you see that both requests have returned same data for earth both have same meta field If you look at memory cache of earth you see the same data but without meta field Expected behavior Either meta field is not supposed to be cached so it should not be present in cache after first request Either meta field is supposed to be cached so it should still be present after second request Why problem occurs I think related logic is contained in mergeRecords function When cache entry is empty this function copies entire record to cache without changes thats why meta field is present after the first fetch During the second fetch merging occurs and meta fields gets removed Perhaps itis a sort of XY problem connected with a misuse of meta field Actually weve solved it by getting rid of meta fields 