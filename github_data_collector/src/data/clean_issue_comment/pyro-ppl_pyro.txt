This ports the neutra example from numpyro to test the NeutraReparam reparametrizer I am not seeing good results with the IAFNormal autoguide and specially with the default setting where it uses a single flow TODOs Try on another dataset maybe high dimensional Neals funnel Have a more general flow based autoguide to wrap over BNAF which is not currently available as an autoguide The samples from the warped posterior look far from gaussian and this probably needs to be investigated neutrapdf This PR fixes a logic gap for assign selftransforms in the MCMC api It can be the case that both MCMCtransforms is None and MCMCkerneltransforms is None in which case MCMCtransforms should be set to an empty dict This bug appears when using HMCNUTS with a potentialfn Note sure on the style guide for inline comments so will leave them as is Tasks Reuse traces across waketheta and wakephi loss computations Attempt to remove zeroexpectation terms similar to pyroinferDicecomputeexpectation Add stronger tests and tests for new opspacked utilities x Add example pyro svdklpy running error when enable trace Was exactly running with only jit set to True on line default setting can successfully be running Running in conda environment with py pyrotutorialyml set as name py pyrotutorial dependencies Core python numpy pandas seaborn jupyter torchvision pillow Language tools pip pip pyroppl requires pytorch and torch to be higher version pyroppl extras Guidelines NOTE Issues are for bugs and feature requests only If you have a question about using Pyro or general modeling questions please post it on the forum If you would like to address any minor bugs in the documentation or source please feel free to contribute a Pull Request without creating an issue first Please tag the issue appropriately in the title eg bug feature request discussion etc Please provide the following details Issue Description Provide a brief description of the issue Environment For any bugs please provide the following OS and python version PyTorch version or if relevant output of pip freeze Pyro version output of python c import pyro print pyroversion python pyro conda Code Snippet Provide any relevant code snippets and commands run to replicate the issue homeuserminiconda envspy pyrotutoriallibpython sitepackagespyrocontribgpkernelsisotropicpy TracerWarning Converting a tensor to a Python boolean might cause the trace to be incorrect We cant record the data flow of Python values so this value will be treated as a constant in the future This means that the trace might not generalize to other inputs if Xsize Zsize homeuserminiconda envspy pyrotutoriallibpython sitepackagespyrocontribgpmodelsvsgppy TracerWarning Converting a tensor to a Python index might cause the trace to be incorrect We cant record the data flow of Python values so this value will be treated as a constant in the future This means that the trace might not generalize to other inputs Kuuview M selfjitter add jitter to the diagonal homeuserminiconda envspy pyrotutoriallibpython sitepackagespyrodistributionsutilpy TracerWarning Converting a tensor to a Python boolean might cause the trace to be incorrect We cant record the data flow of Python values so this value will be treated as a constant in the future This means that the trace might not generalize to other inputs eyeview minm n nn homeuserminiconda envspy pyrotutoriallibpython sitepackagespyrodistributionsutilpy TracerWarning Converting a tensor to a Python index might cause the trace to be incorrect We cant record the data flow of Python values so this value will be treated as a constant in the future This means that the trace might not generalize to other inputs eyeview minm n nn homeuserminiconda envspy pyrotutoriallibpython sitepackagespyrocontribgplikelihoodsmulticlasspy TracerWarning Converting a tensor to a Python boolean might cause the trace to be incorrect We cant record the data flow of Python values so this value will be treated as a constant in the future This means that the trace might not generalize to other inputs if fswapsize selfnumclasses Traceback most recent call last File svdklpy line in module mainargs File svdklpy line in main trainargs trainloader gpmodule optimizer lossfn epoch File svdklpy line in train loss lossfngpmodulemodel gpmoduleguide File homeuserminiconda envspy pyrotutoriallibpython sitepackagespyroinfertracemeanfieldelbopy line in differentiableloss return selfdifferentiablelossargs kwargs File homeuserminiconda envspy pyrotutoriallibpython sitepackagespyroopsjitpy line in call selfcompiled key torchjittracecompiled paramsandargs selfjitoptions File homeuserminiconda envspy pyrotutoriallibpython sitepackagestorchjitinitpy line in trace forceoutplace File homeuserminiconda envspy pyrotutoriallibpython sitepackagespyroopsjitpy line in compiled return poutinereplayselffn paramsconstrainedparamsargs kwargs File homeuserminiconda envspy pyrotutoriallibpython sitepackagespyropoutinemessengerpy line in contextwrap return fnargs kwargs File homeuserminiconda envspy pyrotutoriallibpython sitepackagespyroinfertracemeanfieldelbopy line in differentiableloss lossparticle selfdifferentiablelossparticlemodeltrace guidetrace File homeuserminiconda envspy pyrotutoriallibpython sitepackagespyroinfertracemeanfieldelbopy line in differentiablelossparticle klqp kldivergenceguidesite fn modelsite fn File homeuserminiconda envspy pyrotutoriallibpython sitepackagestorchdistributionsklpy line in kldivergence return funp q File homeuserminiconda envspy pyrotutoriallibpython sitepackagespyrodistributionsklpy line in klindependentindependent kl kldivergencep q File homeuserminiconda envspy pyrotutoriallibpython sitepackagestorchdistributionsklpy line in kldivergence return funp q File homeuserminiconda envspy pyrotutoriallibpython sitepackagestorchdistributionsklpy line in klmultivariatenormalmultivariatenormal punbroadcastedscaletrilshape This issue proposes a new statement in Pyro a pyrobarrier statement to allow interaction between an inference algorithm and a models local state The interface is py thing pyrobarrierthing where thing is a recursive data structure whose elements may be Python atoms which will be unaffected Python collections which will be recursed roughly py barrier xyz barrierx barriery barrierz PyTorch tensors with named dimensions or Funsors with named free variables Note this feature crucially relies on PyTorch named dimensions to pass eventdim metadata through arithmetic computations in user code Using pyrobarrier we can reimplement SMC in a more Pyronic way avoding restrictions on an initstep interface For example we could implement SMC via effects acting on a pyrobarrier statement in the following annotated model py def modeldata z pyrosamplezinit Normal latent state k z control cost z cumulative cost of controller for t x in enumeratedata zkcost pyrobarrierzkcost triggers resampling k torchwherez k k k torchwherez k k k torchwhere z z k k z pyrosamplefzt Normalzk pyrosamplefxt Normalz obsx return cost While in Pyro this statement has limited power supporting only a few features like SMC resampling and enumerationdelayed sampling it becomes more powerful in a PPL with better support for lazy computation such as funsorminipyro Id like to add this to the pyroapi so we can experiment with more inference algorithms in a generic way For details see draft design doc and draft Funsor implementation Ive continued the feature request of bmazoure in changing a number of the implementation details adding tests docs etc This should be good to go for implementing Glow normalizing flow and Neural Spline FLow This is a new transform for reshaping the event shape of a random variable akin to TensorFlow I had to make some changes to TransformedDistribution to make it reshapeaware and created a modified version in pyrodistributions The implementation will assume that df throughout Right now there is nothing implemented yet Ill add more details below The math is mostly corresponding to GammaGaussian op Reference Robust Bayesian Filtering and Smoothing Using Student s t Distribution Tasks x Implement StudentT op except the sum method x Consider to implement the approximated sum method Im not sure if this is necessary Edit this is necessary and is the most tricky part x Implement the approximated tensordot x Add test to verify that this match the sequential approximation mechanism in the above reference x Add tests similar to GaussianGammaGaussian ops coauthored with iffsid Statistically batching allows us to trade off the speed of taking gradient steps for lower variance gradient estimators This can be done in a for loop However batching is most useful for parallelizing the computation of the log probs of the model and the guide by using low level numerical optimizations Plating is a useful construct for defining conditionally independent but identical random variables While this is useful for defining hierarchical models it is unsuited for batching since it changes the definition of the model This leads to issues like IWAEbased algorithms failing silently additional multiplicative factors in ELBObased gradients and being able to define nonsensical guides We propose defining a special keyword for plates that are supposed to indicate batching which circumvents the above problems More details Consider a model pzpxz where z and x can have multiple sites and each site can have different shapes depending on the distribution batch shapes but also the plates internal to the model Given a dataset of training xs we often want to take the gradient of log px averaged over a batch subset of xs as an estimator of Exs grad log px In practice it is suggested that we change the model by adding a batching plate so that the model becomes pz B x B prodb pzbpxb zb Problem IWAEbased objectives fail silently In IWAEbased objectives like RenyiELBO and RWS we want to compute the model objective as logsumexp over the particle dimension and then average over the batch dimension However since there is no distinction between a batch dimension and a dimension of an actual plate IWAEbased models are forced to sum over the batch dimension before taking the logsumexp over the particle dimension here and here This is wrong since the order of these operation matters Problem ELBObased gradients have wrong multiplicative factors We re interested in estimating Exs grad log px using a Monte Carlo estimator which boils down to taking an average of log px over xs However if we change the model from pz x to pz B x B using a plate in order to accomplish this the resulting estimator will instead be a sum of log px over xs which is off by a multiplicative factor of B While this is fine if we use adaptive optimizers like Adam it is wrong for nonadaptive ones like SGD Problem Ability to define nonsensical guides Consider the case of VAEs where latent vectors have dimension Dz and data has dimension Dx If we change the model to pz B x B we have the ability to write the guide to take in B Dx observations and output a distribution over B Dz which is not necessarily independent in the batch dimension eg a multivariate normal distribution over B Dz dimensions This is clearly nonsensical for the VAE model where we just want the guide to map from data of shape Dx to distributions on vectors of shape Dz While this is not as big as a problem as the previous ones it is an indication that changing the model definition in order to do batching is unnatural Proposed solution Use a plate with a special name for batching Users will still deal with batching like beforeby adding a plate around the body of model manuallybut they are forced to name this plate using a string with a special keyword value This keyword is then used internally by IWAEbased and ELBObased objectives to compute the correct gradient estimators This doesn t solve the third problem Mentioned by fritzo in Also related Here is one reason to change it which might not have been explicitly mentioned in that PR I wanted to change our tutorials examples to start using pyrodeterministic but the following behavior under Predictive will be unintuitive for users It is obvious what is happening once you realize deterministic is an alias for a particular sample statement and how broadcasting works in the presence of event dims etc which is probably too much to ask python def forwardself x yNone sigma pyrosamplesigma distUniform mean pyrodeterministicmean selflinearxsqueeze with pyroplatedata xshape obs pyrosampleobs distNormalmean sigma obsy Here the size of the mean variable will be lenx under Predictive and not lenx which one might expect The reason is that mean is just a sample site under the hood and will prepend to the left based on maxplatenesting in this case but since the remaining dims of mean are considered event dims we prepend 