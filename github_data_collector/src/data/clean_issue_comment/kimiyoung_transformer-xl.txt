mlen mems size if mems is not None else klen mlen qlen if selfsamelength allones wordembnewonesqlen klen masklen klen selfmemlen if masklen maskshiftlen qlen masklen else maskshiftlen qlen decattnmask torchtriuallones mlen torchtrilallones maskshiftlenbyte None else decattnmask torchtriu wordembnewonesqlen klen diagonal mlenbyte None what if mems is None that is mlen is zero Actually decattnmask will be allone matrix My test demo shows that it seems to cause bad results in eval phase samelength is True Hi kimiyoung Your transformerxl idea is interesting I have some confusion about the git resp In pytorch and tensorflow branch billion experiment parameters seems have some difference First memlen and targetlen is in pytorch branch but in tensorflow branch Second warmupsteps is in pytorch branch but in tensorflow branch I ran two branch default experiments When I ran tensorflow resp it occurs OOM in Tesla v GB If I modify memlen and targetlen from to the same to pytorch branch it will run successfully When I ran pytorch resp ppl will not be converged well after globalsteps warmupsteps Experiment result log is following Eval at step time s valid loss valid ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl Eval at step time s valid loss valid ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl epoch step batches lr msbatch loss ppl Eval at step time s valid loss valid ppl I think I only need to set batchsize smaller or memlen smaller from to because of GPU memory limitation How memlen and targetlen affects the experiment result How billion experiment can be converged Would you please give me some advices Hi thanks for the highquality code and paper A hopefully quick question in Section you said faster evaluation I dont quite understand why the vanilla model is very slow could you clarify a bit or give some concrete example Thanks How to we extend the model for sentiment analysis task Hi thank you for the open source After I finished running runwt basesh with GPUs my final result looked like this Eval at step time s valid loss valid ppl End of training End of training test loss test ppl Evaluating with bsz tgtlen extlen memlen clamplen Time s mssegment test loss test ppl Would you mind tell me which test ppl is the correct one The one reported in your paper github If is the correct test ppl I wonder why it cannot achieve as you said in your github since every random seed has been fixed By the way will the result be the same if I use only GPUs to train wt base Thanks a lot Can anyone point out how I can get only the log probability for actual decoding from the Adaptive Softmax without getting the loss The forward method in the class in this repo only gives the loss but not the log probs Thanks Function signature is def updatememsself hids mems qlen mlen And the call is newmems selfupdatememshids mems mlen qlen mlen and qlen probably misordered in the function call no decoder found in modelpy in tf implementation Thanks for the repo It seems you dont have validation step in training loop and I can only assume that you have tuned the trainsteps so the models wont overfit when training on those corpora memtioned in your paper right But what if I want to train on my own corpus Do this mean I need to write validation code myself Burce