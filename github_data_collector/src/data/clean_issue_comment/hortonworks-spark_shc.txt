 alphabetic splits can be easily supported What changes were proposed in this pull request Adds two properties that allow the first and last split to be set by the user This allows the use of more complex split preferences as well as enables scenarios such as numericonly keys which were underserved in earlier versions How was this patch tested Two unit tests were added One tests that for alphabetic keys the rows are dispersed between regions correctly The other uses numeric keys and sets the first and last splits accordingly This also checks that the rows have been dispersed appropriately across regions If you have keys that start exclusively with numeric values all rows will be put into a single region That region will be split as necessary based on the settings in HBase but the other regions will remain empty For example if I request regions it will create regions between aaaaaa and zzzzzz If I then add million rows with numeric keys all of them will go into the first region If I add more rows then the HBase cluster may decide to split the first region but the other regions will remain empty Ive included some source code below that Ive written which demonstrates this issue easily Increasing the number of samples allows you to see the new regions were created but even at million you wouldnt expect the minimum and maximum keys to be in the same region if you asked for regions val numValues val startValue val df sparkrangestartValue startValue numValuesselectidcaststring as patid litRULIDVALUE as rulid litROWKEYVALUE as rowkey Cast DataTypes val cirfitereddataDF dfselect patid as fullrowkey rulid rowkey as ROWKEY Schema to load into HBase perf table def mbrcirloadhbasecatalog s tablenamespace u sandbox namePerfTest rowkeykey columns fullrowkeycfrowkey colkey typestring rulidcfP colrulid typestring ROWKEYcfP colrowkey typestring stripMargin cirfitereddataDFshowfalse cirfitereddataDFwriteoptionsMapHBaseTableCatalogtableCatalog mbrcirloadhbasecatalog HBaseTableCatalognewTable formatorgapachesparksqlexecutiondatasourceshbasesave I find jar in I dont know which one is for the spark hbase can be used can you make a list dependency groupIdcomhortonworksgroupId artifactIdshccoreartifactId version s version dependency Is this the latest version in thread main javalangNoSuchMethodError orgapachesparksqlinternalSQLConfLEGACYPASSPARTITIONBYASOPTIONSLorgapachesparkinternalconfigConfigEntry at orgapachesparksqlDataFrameWritersaveToV SourceDataFrameWriterscala at orgapachesparksqlDataFrameWritersaveDataFrameWriterscala Guys Heres what I did Forked the package made adjustments in HBaseRelationscala so that while using this for Google BigTable the class must not call methods of Namespaces viz getNamespaceDescriptor createNamespaces etc Check this Compiled my version of package without big fat jar file its become hardly KB Bundled this package in my code where Im using SHC for DF write operations as HBase format If I run the job I see issues like javalangNoClassDefFoundError orgapachehadoophbaseclientTableDescriptor at orgapachesparksqlexecutiondatasourceshbaseDefaultSourcecreateRelationHBaseRelationscala Caused by javalangClassNotFoundException orgapachehadoophbaseclientTableDescriptor If I have compiled SHC on my local and if I want to use it in my spark job without obstacles like above what should I do Where am I making mistake I want to push data into an already existing table single column family no records I am using shccore s on a windows machine I have hbase installed and use scala When I try to push data I got first the following error orgapachesparksqlexecutiondatasourceshbaseInvalidRegionNumberException Number of regions specified for new table must be greater than After following the advice of this link I added HBaseTableCatalognewTable to my options It still failed but with javalangIllegalArgumentException Can not create a Path from a null string Following this link I added to my catalog tableCoderPrimitiveType Still facing the same error I saw people are expecting some clarification about that issue It is known issue and apparently it seems fixed I do not know what to do next Is there a solution about this Not able to store data in hbase It seems to to some jar file isse I am using Spark using sparksubmit packages comhortonworksshccore s repositories Is it possible to provide sparkhbasehost and sparkhbaseport in code I tried following val sparkConf new SparkConfsetAppNameSparkHBasesetMasterlocal sparkConfsetsparkhbasehost hbasedevmyorgnet val spark SparkSession SparkSessionbuilder configsparkConf getOrCreate but getting error as shc tries to connect to localhost INFO ClientCnxn Opening socket connection to server localhost Will not attempt to authenticate using SASL unknown error WARN ReadOnlyZKClient x a fd to localhost failed for get of hbasehbaseid code CONNECTIONLOSS retries WARN ClientCnxn Session x for server null unexpected error closing socket connection and attempting reconnect javanetConnectException Connection refused at sunniochSocketChannelImplcheckConnectNative Method at sunniochSocketChannelImplfinishConnectSocketChannelImpljava at orgapachezookeeperClientCnxnSocketNIOdoTransportClientCnxnSocketNIOjava at orgapachezookeeperClientCnxnSendThreadrunClientCnxnjava 