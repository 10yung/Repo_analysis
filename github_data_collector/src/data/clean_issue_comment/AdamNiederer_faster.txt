 Compilation with the latest version on cratesio does not work however master works fine Would be nice if you could update cratesio rust vec u itermap x x as u x as u sumu is essentially what Id like to use this crate to SIMDaccelerate The issue Im bumping into is reading the slice as u x such that I can then use u x as Fromu x from to cast it What is the best way to do this currently Related to and Opening another ticket since this is a separate discussion from and might be more controversial The more I look into the upcoming stdsimd the more I wonder if faster should not become a thinner SIMDfriendly iteration library that neatly plugs into stdsimd and is really good at handling variable slices zipping instead of providing a blanket implementation over stdarch Right now it seems that many common intrinsics and operations faster provides on packed types are or might be implemented in stdsimd compare coresimdppsv At the same time for things that wont be in stdsimd and will be more platform specific faster will have a hard time providing a consistent performance story anyway By that reasoning I see a certain appeal primarily focusing on a more consistent crossplatform experience with a much lighter code base eg imagine faster without arch and intrin and using mostly stdsimd instead of vektor Faster could also integrate stdarch specific functions and types but rather as extensions and helpers eg for striding for special use cases instead of using them as internal fundamentals While working on I noticed what looks like performance regressions in the cargo bench in particular functions like mapsimd and mapscalar but quite a few others test testsmapscalar bench nsiter test testsmapsimd bench nsiter However comparing to the commit before the refactoring the numbers are mostly unchanged I then assumed its related to unfortunate default feature flags on my machine but playing with avx and sse didnt have any effect either I also have a first implementation of and it actually looks like no fallbacks are emitted for mapsimd Tried to cross check that with radare but have some problems locating the right symbol disassembly for the benchmarks Lastly the functions mapscalar and mapsimd differ a bit but even when I make them equal eg sqrt vs rsqrt the difference remains Is that a known issue Did rustc became so good in autovectorization Any suggestions how to extract the disassembly from testsmapsimd and testsmapscalar Running on rustc nightly fd d MBP i U Update I linked the latest faster version from my SVM library and I dont see these problems in production csvmpredictsv attr problems bench nsiter faster AVX csvmpredictsv attr problems bench nsiter scalar Update Seems to be related to some intrinsics When I dissect the benchmark I get test testsmapscalar bench nsiter without abs test testsmapscalar bench nsiter with abs test testsmapsimd bench nsiter without abs test testsmapsimd bench nsiter with abs I now think that each intrinsic should have its own benchmark eg intrinsicabsscalar intrinsicabssimd Update oh boy I think that by arcane magic Rust imports and prefers stdsimdf x and friends over the faster types and methods So when you do myf sabs it calls stdsimdf x abs not fasterarchcurrentintrinabs The reason I think thats the problem is you can now easily do myf ssqrte which isnt implemented in faster but in stdsimd Whats more annoying is that it doesnt warn about any collision and that stdsimd is actually slower than vanilla Rust TODO Investigate import tree why that happens Clean up imports if import problem Have singleintrinsic benchmarks to detect bad intrinsics Have Rust warn somehow if similar name conflict happens again Remove all usages of featurestdsimd except in librs Update Now one more thing makes sense I sometimes got use of unstable library feature stdsimd in test cases and I didnt understand why Probably because thats where the stdsimd builtins were used After my last PR I noticed you had to clean up a bit That made me wonder if it makes sense to configure and use clippy and rustfmt Since clippy is probably less controversial I went ahead and addressed all current issues either by changing code where I thought clippy made sense or disabling lints eg where faster bends rules for speed Some of the more pedantic lints could be discussed eg usage and formatting of number literals unreadableliteral unseparatedliteralsuffix I think rustfmt makes sense as well but needs more configuration to resonate with the code I found a few settings that worked for me eg maxwidth be set rather high not to break up most macros which makes them harder to read However you should probably take the lead on that one Let me know what you think about clippy in particular unreadableliteral unseparatedliteralsuffix and typecomplexity I prefer the former no opinion on rd I can then create another PR Hi I am trying to port a project to aarch and wasm using the rust migration branch As of today I receive lots of use cratevektorx Could not find x in vektor Ideally faster would have fallbacks for notyet supported architectures That way I could just write my SIMD code once using the provided API instead of having two separate implementations Do you have any shortterm plans of making such a fallback available for the version Also while I am not a Rust expert I have days to look into this myself If you think its feasible to outline a solution you prefer Id be happy to try to help you out Im trying to get into SIMD by implementing a trivial operation XOR unmasking of a byte stream as required by the WebSocket specification The implementation in x intrinsics is actually very straightforward but I have a hard time wrapping my head around expressing it in terms of Faster iterators API The part Im having trouble with is getting an input u to cycle within a SIMD vector of u I have looked at load which does accept u as input but its behavior in case of length mismatch is completely undocumented Its also not obvious what offset parameter does Casting the input u to u calling vecsu s and then downcasting repeatedly to get a SIMD vector of u but Downcast seems to do not at all what I want Getting a SIMD vector of length and arbitrary type inside it load u into it lengths now match so it should work then downcast repeatedly until I get a vector of u with arbitrary length Except there seems to be no way to request a SIMD vector of length and arbitrary type After over an hour of headscratching Ive noticed that Fromu x is implemented for u x so I could replace Downcast with it in approach and probably get the correct result except I have no idea how such conversions interact with host endianness I actually expected this to be a trivial task I guess for someone familiar with SIMD it is but for the likes of me a snippet in examples folder that loads u into a vector would go a long way Or perhaps even a convenience function in the API that deals with endianness properly to make it harder to mess up If I am reading the code correctly it looks like in the case of SSE Faster currently falls back to calling roundfloor etc on each individual lane via the fallback macro You may be able to use these methods instead Or Agner Fog has a different method in his vector library edit Agners functions are slower but can handle floating point values that dont fit in an i the first functions only handle values that do fit in an i I have a function which looks vaguely like this rust struct Rect real f imag f struct KetRefa real a f imag a f impla KetRefa pub fn dotself other KetRef Rect asserteqselfreallen otherreallen asserteqselfreallen otherimaglen asserteqselfreallen selfimaglen zipselfreal selfimag otherreal otherimag mapar ai br bi let real ar br ai bi let imag ar bi ai br Rect real imag foldRectzero ab a b Converting it to use faster requires two passes over the arrays I am unable to produce both real and imag in one pass because simdmap requires the function output to be a single vector rust pub fn dotK AsKetRefself other K Rect use fasterprelude let other otherasketref asserteqselfreallen otherreallen asserteqselfreallen otherimaglen asserteqselfreallen selfimaglen let real selfrealsimditerf s selfimagsimditerf s otherrealsimditerf s otherimagsimditerf s zipsimdmapar ai br bi ar br ai bi simdreducef s acc v acc vsum let imag selfrealsimditerf s selfimagsimditerf s otherrealsimditerf s otherimagsimditerf s zipsimdmapar ai br bi ar bi ai br simdreducef s acc v acc vsum Rect real imag So is it faster Well actually yes It is plenty faster up to a point Change in runtime for different ket lengths dot change dot change dot change dot change dot change dot change Yikes Once we hit elements there is almost no speedup I suspect it is because at this point memory has become the bottleneck and most of what was gained by using SIMD was lost by making two passes over the arrays It would be nice to have an API that allowed this do be done in one pass by allowing a mapping function to return a tuple producing a new PackedZippedIterator or similar