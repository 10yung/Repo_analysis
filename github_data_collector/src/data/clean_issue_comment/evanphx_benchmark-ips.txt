 That way warmup is more similar to actual measurements and not wildly different due to the overhead of going through calltimes in and out on every cycle This also communicates to the JIT that cycles is not always which means the method compiled during warmup is more likely to be reused Before the compiled method could deoptimize because runwarmup always calls calltimes with cycles and the JIT might speculate on that to remove the loop but then runbenchmark suddenly calls calltimes with cycles which needs the loop and therefore causes deoptimization and later recompilation during the actual measurements This PR addresses the second issue of the benchmark shown in I believe it also addresses cc ioquatix ruby frozenstringliteral true require benchmarkips module ActiveSupport class StringInquirer String def methodmissingmethodname arguments if methodname self methodname else super end end end end def env env ActiveSupportStringInquirernewdevelopment end Benchmarkips do x xiterations xreportenvdevelopment do envdevelopment end xreportenv development do env development end end When running this benchmark on MRI with current master I see ruby Ihomeeregoncodebenchmarkipslib benchdevrb Warming up envdevelopment k i ms env development k i ms envdevelopment k i ms env development k i ms Calculating envdevelopment M is M in s env development M is M in s envdevelopment M is M in s env development M is M in s I noticed the warming times which are per ms are not really close to a th of the measurement times which are in ms second This sounds surprising as on MRI for this benchmark I would expect no difference between warmup and actual measurement Yet we see env development is k i ms M is during warmup and then its M is during measurements Did MRI get magically more than twice faster when it realized we are actually benchmarking and not just warming up I would not think so The reason is the warmup phase uses calltimescycles while the measurement phase uses calltimescycles with cycles typically far great than in this case around Using calltimes has a significant overhead as shown here because every time we need to go in calltimes Read a few instance variables go in the loop for just one call exit the loop and return from calltimes So instead if we adapt the warmup to call calltimes with enough cycles to take at least ms we will reduce the overhead significantly as this PR does ruby Ihomeeregoncodebenchmarkipslib benchdevrb Warming up envdevelopment k i ms env development M i ms envdevelopment k i ms env development M i ms Calculating envdevelopment M is M in s env development M is M in s envdevelopment M is M in s env development M is M in s Now warmup and measurement timings are consistent and make sense Of course nobody should use only warmup times for interpreting results but nevertheless I believe its good for warmup and measurements times to match as it can be an indication of whether enough warmup happened and how stable is the code being benchmarked The same issue also happens on TruffleRuby because the warmup loop cannot be compiled efficiently with the previous code only with OSR after many many iterations Without this PR Warming up envdevelopment k i ms env development M i ms envdevelopment k i ms env development M i ms Calculating envdevelopment M is M in s env development B is B in s envdevelopment M is M in s env development B is B in s With this PR Warming up envdevelopment M i ms env development M i ms envdevelopment M i ms env development M i ms Calculating envdevelopment M is M in s env development B is B in s envdevelopment M is M in s env development B is B in s And warmup is then consistent with measurements instead of being apparently much slower cc chrisseaton That way there is no polymorphism for actioncall inside calltimes and the inline cache for actioncall remains monomorphic and well optimized independent of the job order While running this fairly simple benchmark on MRI and TruffleRuby I noticed issues ruby frozenstringliteral true require benchmarkips module ActiveSupport class StringInquirer String def methodmissingmethodname arguments if methodname self methodname else super end end end end def env env ActiveSupportStringInquirernewdevelopment end Benchmarkips do x xiterations xreportenvdevelopment do envdevelopment end xreportenv development do env development end end This PR addresses the first issue The second is The first issue which applies more to Ruby implementations able to inline Ruby methods like TruffleRuby and JRuby is that the call to actioncall inside calltimes the innermost loop of benchmarkips becomes polymorphic It is important for performance to compile that loop and let the JIT know exactly which Proc will be called inside the loop However with the current code all Procs given to BenchmarkIPSJobreport will be observed for that actcall call site during warmup And then for runbenchmark the inline cache will have as many entries as blocks given to report which results in something like this ruby i while i times actcall is compiled to something like if actequal FIRSTPROC FIRSTPROCcall elsif actequal SECONDPROC SECONDPROCcall end i end This is not good for performance because those checks for the Proc make the loop body much larger and the JIT will be less happy to inline any of these Proccall since its not just one but a lot of them and its unclear which one is to prioritize and might cause unfair inlining for the different blocks What we want instead is a copy of the loop for each block to measure which we can easily achieve by defining a method on the singleton class of each JobEntry which was already done for reportString but not other cases That way we end up with ruby i while i times if actequal PROCOFTHATENTRY PROCOFTHATENTRYcall very likely to be inlined which is critical for performance to avoid a noninlined call in the innermost loop and optimize the loop together with the Proc else deoptimize end i end So in practice it means running this benchmark on TruffleRuby without this change gives ruby v truffleruby like ruby GraalVM CE Native x linux ruby Ihomeeregoncodebenchmarkipslib benchdevrb Warming up envdevelopment k i ms env development M i ms envdevelopment k i ms env development M i ms Calculating envdevelopment M is M in s env development M is M in s envdevelopment M is M in s env development M is M in s And with this change Warming up envdevelopment k i ms env development M i ms envdevelopment k i ms env development M i ms Calculating envdevelopment M is M in s env development B is B in s envdevelopment M is M in s env development B is B in s We see the reported performance of env development radically changes with this PR In fact env development is well optimized as it should and is in real code and the PR lets it do that while before benchmarkips itself would prevent running it a full speed due to internal polymorphism cc chrisseaton Do you think its possible to add a offset andor factor to control the times parameter It would be nice if we could tell it to run at least iterations minimum for example FWIW I believe Ruby support can be dropped as is EOL and will be EOL soon Showerthought The reason why benchmarkips is so great is it helps us to set and decide on what would otherwise be pretty arbitrary parameters for benchmarks particularly of course number of iterations I think we could go further A benchmark is warmed up when iterations of the benchmark dont get significantly faster Rather than just running for seconds a benchmark iteration can run until significance is achieved or a timeout is reached Once a result is achieved it can be immediately replicated by iterating the benchmark again If the result is replicated stop If it isnt replicated try again and if it doesnt replicate after X attempts error out Just some ideas I think I just want to get rid of the three remaining config points in benchmarkips warmup time iteration runtime and number of iterations Im interested in comparing two or more implementations of the same thing There is some one off setup cost establishing the connection and then the repeated times cost ie what Im interested in For one benchmark the setup cost might be N and for the other N The iteration cost is largely the same However what ends up happening is that for the benchmark with N setup overhead the times repeats is much larger than the benchmark with N overhead This causes the effect of the setup to be even more pronounced because benchmarkips will set times to say for the case of N setup cost and for the case of N setup cost so you end up with N number of repeats until seconds is elapsed vs N number of repeats until seconds is elapsed Ultimately it makes the nd case look much better even though the different is mostly in the setup overhead Is there some way to take this bias into account My initial thoughts were to use the upper bound for times or perhaps the average so that each benchmark would be largely running with the same number proportionally of setup overhead to number of times This unifies the double stdout and suite calls It also allows a user to choose a different output Addresses for warmup and benchmark but not compare Outstanding Changing compare is not as easy the output needs to be passed in Pass in the stream from one of the outs Pass in a stream to config and use that throughout Enhance StdoutReport and change compare to use the outs Let me know which you approach you prefer I can fixup this PR or just make another Sample Code ruby require benchmarkips mystream StringIOnew Benchmarkips do x xconfigquiet true suite BenchmarkIPSJobStdoutReportnewmystream xreportmul xreportpow xcompare end puts results mystreamstring It would be nice to have a way to capture the output to a String in order to print it or send it to an external service like Bugsnag Also in a PASS like heroku its hard to get the reports because the logs are combined from logger stdout and stderr By output I mean the full report Calculating addition k i ms addition k i ms addition k i ms additiontestlonglabel k i ms addition M is M addition M is M addition M is M additiontestlonglabel M is M Comparison addition is addition is x slower additiontestlonglabel is x slower addition is x slower 