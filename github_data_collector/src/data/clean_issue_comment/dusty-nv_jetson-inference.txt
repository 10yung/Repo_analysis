Hello I am currently trying to run custom models on the Nvidia Xavier using this jetsoninference repo I have gone through the tutorials using the prebuilt networks just fine but when I try to use custom models I run into problems Pretrained MobileNetV model from Torchvision I converted it to ONNX format using this Exported model has been tested with ONNXRuntime and the result looks good but fails the TensorRT onnx test usrsrctensorrtbintrtexec onnxonnxmodelsMobileNetV onnx RUNNING TensorRTtrtexec usrsrctensorrtbintrtexec onnxonnxmodelsMobileNetV onnx I Model Options I Format ONNX I Model onnxmodelsMobileNetV onnx I Output I Build Options I Max batch I Workspace MB I minTiming I avgTiming I Precision FP I Calibration I Safe mode Disabled I Save engine I Load engine I Inputs format fp CHW I Outputs format fp CHW I Input build shapes model I System Options I Device I DLACore I Plugins I Inference Options I Batch I Iterations I Duration s ms warm up I Sleep time ms I Streams I ExposeDMA Disabled I Spinwait Disabled I Multithreading Disabled I CUDA Graph Disabled I Skip inference Disabled I Input inference shapes model I Inputs I Reporting Options I Verbose Disabled I Averages inferences I Percentile I Dump output Disabled I Profile Disabled I Export timing to JSON file I Export output to JSON file I Export profile to JSON file I Input filename onnxmodelsMobileNetV onnx ONNX IR version Opset version Producer name pytorch Producer version Domain Model version Doc string W TRT Calling isShapeTensor before the entire network is constructed may result in an inaccurate result E TRT Network has dynamic or shape inputs but no optimization profile has been defined E TRT Network validation failed E Engine creation failed E Engine set up failed FAILED TensorRTtrtexec usrsrctensorrtbintrtexec onnxonnxmodelsMobileNetV onnx I have tried PyTorch which fails at converting model to ONNX due to dynamicaxes label PyTorch and Pretrained MobileNetV model found here Passes the trtexec PASSED TensorRTtrtexec usrsrctensorrtbintrtexec onnxonnxmodelsmobilenetv onnx but runs into an error when loading model with jersoninference imagenetconsolepy airplane jpg output jpg modelmobilenetv onnx jetsoninferenceinitpy jetsoninference initializing Python bindings jetsoninference registering module types jetsoninference done registering module types jetsoninference done Python binding initialization jetsonutilsinitpy jetsonutils initializing Python bindings jetsonutils registering module functions jetsonutils done registering module functions jetsonutils registering module types jetsonutils done registering module types jetsonutils done Python binding initialization loaded airplane jpg x channels jetsoninference PyTensorNetNew jetsoninference PyImageNetInit jetsoninference imageNet loading network using argv command line params jetsoninference imageNetinit argv imagenetconsolepy jetsoninference imageNetinit argv airplane jpg jetsoninference imageNetinit argv output jpg jetsoninference imageNetinit argv modelmobilenetv onnx TRT imageNet failed to initialize jetsoninference imageNet failed to load builtin network googlenet PyTensorNetDealloc Traceback most recent call last File imagenetconsolepy line in module net jetsoninferenceimageNetoptnetwork sysargv Exception jetsoninference imageNet failed to load network jetsonutils freeing CUDA mapped memory Pretrained MobileNetV Caffe Model model found here imagenetconsolepy airplane jpg output jpg prototxt MobileNetdeployprototxt modelMobileNetSSDdeploycaffemodel labelslabelsmaptxt inputblobdata outputblobsoftmax jetsoninferenceinitpy jetsoninference initializing Python bindings jetsoninference registering module types jetsoninference done registering module types jetsoninference done Python binding initialization jetsonutilsinitpy jetsonutils initializing Python bindings jetsonutils registering module functions jetsonutils done registering module functions jetsonutils registering module types jetsonutils done registering module types jetsonutils done Python binding initialization loaded airplane jpg x channels jetsoninference PyTensorNetNew jetsoninference PyImageNetInit jetsoninference imageNet loading network using argv command line params jetsoninference imageNetinit argv imagenetconsolepy jetsoninference imageNetinit argv airplane jpg jetsoninference imageNetinit argv output jpg jetsoninference imageNetinit argv prototxt jetsoninference imageNetinit argv MobileNetdeployprototxt jetsoninference imageNetinit argv modelMobileNetSSDdeploycaffemodel jetsoninference imageNetinit argv labelslabelsmaptxt jetsoninference imageNetinit argv inputblobdata jetsoninference imageNetinit argv outputblobsoftmax imageNet loading classification network model from prototxt model MobileNetSSDdeploycaffemodel classlabels labelsmaptxt inputblob data outputblob softmax batchsize TRT TensorRT version TRT loading NVIDIA plugins TRT Plugin Creator registration succeeded GridAnchorTRT TRT Plugin Creator registration succeeded GridAnchorRectTRT TRT Plugin Creator registration succeeded NMSTRT TRT Plugin Creator registration succeeded ReorgTRT TRT Plugin Creator registration succeeded RegionTRT TRT Plugin Creator registration succeeded ClipTRT TRT Plugin Creator registration succeeded LReLUTRT TRT Plugin Creator registration succeeded PriorBoxTRT TRT Plugin Creator registration succeeded NormalizeTRT TRT Plugin Creator registration succeeded RPROITRT TRT Plugin Creator registration succeeded BatchedNMSTRT TRT Could not register plugin creator FlattenConcatTRT in namespace TRT completed loading NVIDIA plugins TRT detected model format caffe extension caffemodel TRT desired precision specified for GPU FASTEST TRT requested fasted precision for device GPU without providing valid calibrator disabling INT TRT native precisions detected for GPU FP FP INT TRT selecting fastest native precision for GPU FP TRT attempting to open engine cache file MobileNetSSDdeploycaffemodel GPUFP engine TRT cache file not found profiling network model on device GPU TRT device GPU loading usrbin MobileNetSSDdeploycaffemodel TRT failed to retrieve tensor for Output softmax Segmentation fault core dumped Another Caffe Model found here imagenetconsolepy airplane jpg output jpg prototxtMobileNetCaffemobilenetv deployprototxt modelMobileNetCaffemobilenetv caffemodel labelslabelsmaptxt inputblobdata outputblobsoftmax jetsoninferenceinitpy jetsoninference initializing Python bindings jetsoninference registering module types jetsoninference done registering module types jetsoninference done Python binding initialization jetsonutilsinitpy jetsonutils initializing Python bindings jetsonutils registering module functions jetsonutils done registering module functions jetsonutils registering module types jetsonutils done registering module types jetsonutils done Python binding initialization loaded airplane jpg x channels jetsoninference PyTensorNetNew jetsoninference PyImageNetInit jetsoninference imageNet loading network using argv command line params jetsoninference imageNetinit argv imagenetconsolepy jetsoninference imageNetinit argv airplane jpg jetsoninference imageNetinit argv output jpg jetsoninference imageNetinit argv prototxtMobileNetCaffemobilenetv deployprototxt jetsoninference imageNetinit argv modelMobileNetCaffemobilenetv caffemodel jetsoninference imageNetinit argv labelslabelsmaptxt jetsoninference imageNetinit argv inputblobdata jetsoninference imageNetinit argv outputblobsoftmax imageNet loading classification network model from prototxt MobileNetCaffemobilenetv deployprototxt model MobileNetCaffemobilenetv caffemodel classlabels labelsmaptxt inputblob data outputblob softmax batchsize TRT TensorRT version TRT loading NVIDIA plugins TRT Plugin Creator registration succeeded GridAnchorTRT TRT Plugin Creator registration succeeded GridAnchorRectTRT TRT Plugin Creator registration succeeded NMSTRT TRT Plugin Creator registration succeeded ReorgTRT TRT Plugin Creator registration succeeded RegionTRT TRT Plugin Creator registration succeeded ClipTRT TRT Plugin Creator registration succeeded LReLUTRT TRT Plugin Creator registration succeeded PriorBoxTRT TRT Plugin Creator registration succeeded NormalizeTRT TRT Plugin Creator registration succeeded RPROITRT TRT Plugin Creator registration succeeded BatchedNMSTRT TRT Could not register plugin creator FlattenConcatTRT in namespace TRT completed loading NVIDIA plugins TRT detected model format caffe extension caffemodel TRT desired precision specified for GPU FASTEST TRT requested fasted precision for device GPU without providing valid calibrator disabling INT TRT native precisions detected for GPU FP FP INT TRT selecting fastest native precision for GPU FP TRT attempting to open engine cache file MobileNetCaffemobilenetv caffemodel GPUFP engine TRT cache file not found profiling network model on device GPU TRT device GPU loading MobileNetCaffemobilenetv deployprototxt MobileNetCaffemobilenetv caffemodel Error Weight is outside of Error Weight is outside of Error Weight is outside of Error Weight is outside of Error Weight is outside of Error Weight is outside of Error Weight is outside of Error Weight is outside of Error Weight is outside of Error Weight is outside of Error Weight is outside of Error Weight is outside of Error Weight is outside of Error Weight is outside of Error Weight is outside of Error Weight is outside of Error Weight is outside of TRT device GPU failed to parse caffe network TRT device GPU failed to load MobileNetCaffemobilenetv caffemodel TRT failed to load MobileNetCaffemobilenetv caffemodel TRT imageNet failed to initialize jetsoninference imageNet failed to load builtin network googlenet PyTensorNetDealloc Traceback most recent call last File imagenetconsolepy line in module net jetsoninferenceimageNetoptnetwork sysargv Exception jetsoninference imageNet failed to load network jetsonutils freeing CUDA mapped memory hr I know jetsoninference supplies a prebuilt MobileNetV SSD but I am trying to get a custom mobilenetV model personally trained to work but decided to see if I could get the original to work first above models Ideally I would like to be able to get my custom PyTorch models converted to ONNX or native PyTorch support and run them on the Xavier Thank you for any and all help dustynv I bought my jetson nano board Could you please explain what is the difference between Jetson inference SDK object detection code vs Deepstream SDk object detection code when should we go for what highly appreciated your explanation I followd this repo and run the demo of imageNet successfully But Im wondering how speedup does TensorRT do Is there any way to DISABLE tensorRT on this example if not how to run without tensorRT and compare the speedup of tensorRT on image classification thanks Addresses issue Add binding to another overload of segNetMask Use integer division in segnetcamerapy Usage of the new python binding python classes is an uint array classes jetsonutilscudaAllocMappedwidth height netMaskClassclasses width height cudaToNumpy assumes float Need to dividie output length by mask jetsonutilscudaToNumpyclasses width height convert resulting float view to uint view masknp maskviewuint reshapewidth height Look like the demo cannot be run with tensorRT Tried detectnetcamera not working Hey there during inference stdout is flooded with thouds of messages like this jetsonutils cudaFromNumpy ndarray dim jetsonutils cudaFromNumpy ndarray dim jetsonutils cudaFromNumpy ndarray dim jetsonutils freeing CUDA mapped memory jetsoninference PyDetectionDealloc jetsoninference PyDetectionDealloc jetsonutils cudaFromNumpy ndarray dim jetsonutils cudaFromNumpy ndarray dim jetsonutils cudaFromNumpy ndarray dim jetsonutils freeing CUDA mapped memory jetsoninference PyDetectionDealloc jetsoninference PyDetectionDealloc jetsoninference PyDetectionDealloc jetsonutils cudaFromNumpy ndarray dim jetsonutils cudaFromNumpy ndarray dim I tried python import logging loggingbasicConfiglevelloggingERROR But this did not help There are methods in segNeth show below I try to used the first method in python but failed I want to know how to input the first parameter to distinguish the two methods in python When extracting the model tar zxvf ResNet targz There comes an error ResNet ResNet train prototxt ResNet ResNet mean binaryproto ResNet solver prototxt ResNet deploy prototxt ResNet ResNet caffemodel tarSkipping to next header gzipstdinunexpected end of file tarchild returned status tarError is not recoverableexiting now I was successfully able to build and run to install tensorflow after following this guide Target tensorflowtoolspippackagebuildpippackage uptodate bazelbintensorflowtoolspippackagebuildpippackage INFO Elapsed time s Critical Path s INFO processes local INFO Build completed successfully total actions I ran these commands to verify the installation python c import tensorflow as tf printtfversion homepathlocallibpython sitepackagestensorflowpythonframeworkdtypespy FutureWarning Passing type or type as a synonym of type is deprecated in a future version of numpy it will be understood as type type npqint npdtype qint npint homepathlocallibpython sitepackagestensorflowpythonframeworkdtypespy FutureWarning Passing type or type as a synonym of type is deprecated in a future version of numpy it will be understood as type type npquint npdtype quint npuint homepathlocallibpython sitepackagestensorflowpythonframeworkdtypespy FutureWarning Passing type or type as a synonym of type is deprecated in a future version of numpy it will be understood as type type npqint npdtype qint npint homepathlocallibpython sitepackagestensorflowpythonframeworkdtypespy FutureWarning Passing type or type as a synonym of type is deprecated in a future version of numpy it will be understood as type type npquint npdtype quint npuint homepathlocallibpython sitepackagestensorflowpythonframeworkdtypespy FutureWarning Passing type or type as a synonym of type is deprecated in a future version of numpy it will be understood as type type npqint npdtype qint npint homepathlocallibpython sitepackagestensorflowpythonframeworkdtypespy FutureWarning Passing type or type as a synonym of type is deprecated in a future version of numpy it will be understood as type type npresource npdtype resource npubyte python c import tensorflow as tf printtfcontribeagernumgpus homepathlocallibpython sitepackagestensorflowpythonframeworkdtypespy FutureWarning Passing type or type as a synonym of type is deprecated in a future version of numpy it will be understood as type type npqint npdtype qint npint homepathlocallibpython sitepackagestensorflowpythonframeworkdtypespy FutureWarning Passing type or type as a synonym of type is deprecated in a future version of numpy it will be understood as type type npquint npdtype quint npuint homepathlocallibpython sitepackagestensorflowpythonframeworkdtypespy FutureWarning Passing type or type as a synonym of type is deprecated in a future version of numpy it will be understood as type type npqint npdtype qint npint homepathlocallibpython sitepackagestensorflowpythonframeworkdtypespy FutureWarning Passing type or type as a synonym of type is deprecated in a future version of numpy it will be understood as type type npquint npdtype quint npuint homepathlocallibpython sitepackagestensorflowpythonframeworkdtypespy FutureWarning Passing type or type as a synonym of type is deprecated in a future version of numpy it will be understood as type type npqint npdtype qint npint homepathlocallibpython sitepackagestensorflowpythonframeworkdtypespy FutureWarning Passing type or type as a synonym of type is deprecated in a future version of numpy it will be understood as type type npresource npdtype resource npubyte I tensorflowcoreplatformcpufeatureguardcc Your CPU supports instructions that this TensorFlow binary was not compiled to use AVX FMA I tensorflowstreamexecutorcudacudagpuexecutorcc successful NUMA node read from SysFS had negative value but there must be at least one NUMA node so returning NUMA node zero I tensorflowcorecommonruntimegpugpudevicecc Found device with properties name GeForce GTX major minor memoryClockRateGHz pciBusID totalMemory GiB freeMemory GiB I tensorflowcorecommonruntimegpugpudevicecc Adding visible gpu devices I tensorflowcorecommonruntimegpugpudevicecc Device interconnect StreamExecutor with strength edge matrix I tensorflowcorecommonruntimegpugpudevicecc I tensorflowcorecommonruntimegpugpudevicecc N I tensorflowcorecommonruntimegpugpudevicecc Created TensorFlow device joblocalhostreplica task deviceGPU with MB memory physical GPU device name GeForce GTX pci bus id compute capability But still my digits says Tensorflow support disabled Can anyone please guide me through this Thanks Getting this error since latest Jetson Nano SD image came out It matters not which network you attempt to use Maybe something changed the directory structure