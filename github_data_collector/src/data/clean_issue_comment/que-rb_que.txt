 Hey team Im opening this issue to get a sense of whether Que maintainers are supportive of an attempt to instrument the upstream Que project with native Prometheus metrics We have an old fork of Que that weve modified to include this instrumentation but are keen to move back to upstream before we try porting our metrics it would be good to understand if the project would be receptive to this attempt Ive attached a dashboard from a staging environments as an example of some toplevel metrics we have Weve also tracked internal processes like job lock acquisition which has helped us alert on degraded performance as an example whenever long snapshots are held open in the past Is this something that would be welcomed by the maintainers Cheers Lawrence screencapturegrafanagocardlessiodGyEPq Zmzque Im trying to integrate Que with Rails ActiveJob and it seems to be working fine with the exception of the queue name I found on this comment that named queue support was removed from ActiveJob My infrastructure has workers listening on different queues and I use the queue name to determine which workers will process the job The lack of support for named queues in ActiveJob is critically important to me so I cannot use the ActiveJob integration Are there any plans to support named queues in ActiveJob Ive been using for some basic exclusive locking around job execution in my admittedly pretty small production app using que for a couple months now Saw que fly by on HN the other day and figured it might be good to open source the locking code and share it so I published that over there From the quelocks readme quelocks adds an optin feature to QueJobs allowing jobs to specify that exactly one instance of a job should be executing at once This is useful for jobs that are doing something important that should only ever happen one at a time like processing a payment for a given user or super expensive jobs that could cause thundering herd problems if enqueued all at the same time This adds a link to the readme so folks can at least find quelocks but its worth noting that Ive been the only user and not at crazy scale so it might not meet the standard of what should be linked to in the readme I dont mind waiting for some more users or something like that before adding this but I figured Id open the PR to see what the community thinks We are deploying que workers in our Kubernetes environments and would like to add Liveness and Readiness checks Our immediate problem use case is that during startup worker init CPU levels spike so the autoscaler brings in more cascading pods Adding an initialDelaySeconds can help with that but the bigger issue of worker health is still to be addressed Weve had a few ideas including something as simple as the worker writing a file to disk when it is ready which can be looked for or an embedded sinatra app that can be curled from inside the container to avoid opening up ports thus readinessProbe exec command curl silent showerror fail initialDelaySeconds periodSeconds Beyond that it gets more complicated How to tell between a hung worker and one that is processing a very long task etc How are others handling this issue Its great that the que worker process gives itself a bit of time upon receiving SIGTERM to finish the outstanding jobs If those jobs are short theres not much sense in throwing away all the work theyve done so far if theyre just about to complete With long running jobs however like CSV exports S uploads data api extracts etc the worker process ends up refusing to shut down if its running any of them This is more of an ergonomics thing but when developing locally its annoying to have to kill the que process if its working on anything long running and in production the orchestrator cant tell the difference between que being stuck or hung for some actually important issue or if it was just working on a long running job that would be fine to abort I think itd be great if que killed its own workers after getting SIGTERM and waiting x seconds so that it could exit cleanly and let developers writing quoteunquote bad long running jobs restart the process easily See for sidekiqs config param that accomplishes the same thing Certain jobs are not written to the quejobs table although the quejobsidseq PG sequence is incremented which suggests the insert is rolled back This might be related to Here is a list of some examples that work or fail ruby PackMaterialAppERPPurchaseInvoiceJobenqueueabc deliveryid FAILS Change ERP to Erp in the classname PackMaterialAppErpPurchaseInvoiceJobenqueueabc deliveryid WORKS Using Queenqueue to allow enqueue of a class that does not exist Queenqueue abc deliveryid jobclass PackMaterialAppERPPurchaseInvoiceJob queue packmat FAILS Change ERP to Erp in the classname Queenqueue abc deliveryid jobclass PackMaterialAppErpPurchaseInvoiceJob queue packmat WORKS Add an s to the module name Queenqueue abc deliveryid jobclass PackMaterialsAppERPPurchaseInvoiceJob queue packmat WORKS Change the args to a single Hash usr abc deliveryid Queenqueue usr abc deliveryid jobclass PackMaterialAppERPPurchaseInvoiceJob queue packmat WORKS Change the args to a simple array abc Queenqueue abc jobclass PackMaterialAppERPPurchaseInvoiceJob queue packmat WORKS Note all of these calls to enqueue return successfully eg ruby QueJob x a dd e queattrs priority runat id jobclassPackMaterialAppERPPurchaseInvoiceJob errorcount lasterrormessagenil queuepackmat lasterrorbacktracenil finishedatnil expiredatnil args abc deliveryid data Que beta Sequel sequelpg PostgreSQL So far I dont have much details We switched to que last week and noticed that every day some worker will stop executing new jobs Restarting the process fixes the issue Im now in the process of figuring this out Im using Que x with RailsActiveJob we have workers running on separate heroku dynos each worker is running threads each worker has a pool of connections for locker So far I noticed the worker will stop processing jobs after a few hours after booting up Connections will be closed but threads will be still running Hi Were seeing an uncontrolled growth of the quejobsdataginidx index and wonder if anyone else has seen this or has remedies Were using que within Rails wrapped with ActiveJob on Postgres This weekend we performed VACUUM FULL of our quejobs table as quejobsargsginidx grew up to GB and we noticed autovacuum was struggling The autovacuum would run for minutes take a minute nap and then run again By the time it would run again it would already have thousands of dead rows to clean up The VACUUM FULL has helped a lot the autovacuum seems to run fast enough I cant see how fast its running suddenly it doesnt appear in logs but I can see that the timestamp of last autovacuum updates frequently There are only hundreds of dead rows now to clean up The table is now at the stable size around MB but the quejobsargsginidx index seems to grow daily h after VACUUM FULL it grew to MB morning after it was already MB and days later its at MB The quejobsdataginidx is only kB Im starting to believe this could be a PG bug and doesnt have anything to do with que I guess Im still curious if anyone else has seen this happen in their setup and if theres a remedy Were considering to rebuild the index nightly or even dropping it completely as index is times the size of the table itself in less then h so nightly rebuilds might not have any effect See spec failures at among others Its possible that things are working fine and the specs just need to be updated though I havent paid attention to developments in Railsland in a while