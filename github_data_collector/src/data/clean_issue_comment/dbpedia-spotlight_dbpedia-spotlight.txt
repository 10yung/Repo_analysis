This is a version of that tries to remove some duplicated commits The original PR adds Log Linear Model based weighting to the disambiguation procedure It also adds some new features such as a bias term and a lexical similarity measure of the surface form and candidate entity name This is the PR for the second part of my GSoC project adding LLMbased weighting to the disambiguation model Ive also added some new features such as a bias term and a lexical similarity measure of the surface form and candidate entity name This is an attempt on cleaning the rest module Tried to get rid of repeated codemethods follwing DRY as much as I could confidence param refers to disambiguationConfidence added a new parameter spotterConfidence Got rid of spotlightInterface Server has a SpotlightModel Endpoints call methods of spotlight model spot calls spot annotate calls firstBest candidates calls nBest I couldnt see any difference between disambiguate and annotate so I merged both endpoints in the same jersey resource Resources only gather parameters and send them to spotlightModel and then to the output manager to get the correct output this resulted in lots of logic being removed from serverjava and the resources themselves ToDo Properly test this branch x Test Json Xml request Test form requests Other nice things to do probably in another PR x add swagger bump to jersey x replace queryparams for beanparam It would be nice to update to jersey x in such way we could create a bean with all the params fields and avoid naming them over and over in every endpoint The problem is when I tried to bump jersey version I found errors in other modules uima and this was already a big PR the entity topic model implementation is very simple and preliminary but fast and working at the moment It relies on the statistical backend for training spotterstores It needs a lot of memory at the moment though Probabilistic count stores might be a nice idea to reduce those requirements I also migrated the whole project to further testing would be nice eg creating spotlight model I dont have the raw counts Currently disambiguation on CSAW has a precision of around after iterations of training which is compared to our common sense baseline with a little worse which means that the context model actually hurts performance compared to only using surfaceform to resource probabilities Edit Includes now new LinearChainCRFSpotter and a new spotter for pretrained factorie nermodels see PretrainedFactorieNerSpotter eg SimpleNerSpotter BilouConllNerSpotter This PR adds a feature to dbpediaspotlight namely weights associated with the annotations extractedTo do so all you need to do is add a line containing relevancescoringdefault to your modelproperties file in the model folder If that line is not present spotlight behaves as usual We use the context vector overlap number a times a topic is spotted in the text the overlap among context words for the topics For example Microsoft being a context words for the topics We normalize the output score using minmax normalization this step could be improved Given some toy data that we had manually annotated we realized that this method gives results close to human judgements than those given by other topic extractors such as Zemanta or Alchemy If you think this is a good idea we could manually annotate some establish dataset like the milne witten and write a paper as an attempt to reproduce the results in a more formal way This is meant to be a reviewfeedback pr verbose explanation and details 