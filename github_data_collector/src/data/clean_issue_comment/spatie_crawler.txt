Hello I receive this answer when Im using the crawler like this How to continue to a page that gets status code and redirected Im creating this issue to keep track of the tasks for v New features proper support for persistent databasebacked queues allowing to perform a crawl in discrete steps at different points in time and resume an interrupted crawl note that this would benefit a lot from putting a failed URL back onto the persistent crawl queue if youre willing to revisit this automatically set the Referer header to be discussed ship a PDO implementation of a databasebacked crawl queue which would also serve as a reference implementation Fixes fix the performance issues and as a result positive side effect removing one dependency Code improvements X Get rid of id in CrawlUrl Guzzles Pool can yield string keys as well we do not need an int and can use the URL as the key to greatly simplify the code implemented in Improve performance of CollectionCrawlQueue by using an indexed array rather than looping through the URLs Add return types wherever possible example CrawlQueuegetFirstPendingUrl CrawlUrl Change signature of CrawlQueuehas to CrawlUrl crawlUrl to be consistent with the other methods Decouple CrawlRequestFailedFulfilledClass from Guzzle by replacing it with an interface this will allow better static analysis than a invokable class name and make the contract more explicit such as public function crawlFailedRequestException exception CrawlUrl url Make CrawlProfile an interface instead of an abstract class Cleanup remove deprecated methods CrawlerendsWith If possible get rid of CrawlQueueget introduced in as a temporary workaround for Guzzles inability to use objects as keys Ive been experiencing very slow crawling after a few hundred pages To ensure that the problem did not come from the clientserver connection I used a cache middleware with a greedy cache strategy and performed a dry run to store all the pages into the cache before doing any performance testing Heres a scatter plot of the time it takes to crawl a single page crawltimeplot It takes an average of s to process a single page after only pages To understand what made it this slow I ran the xdebug profiler here is the result xdebugprofile It looks like recursive calls to CrawleraddToDepthTree are eating of the CPU and make the crawling shockingly slow Before I dig further into the code Im not yet familiar with it is there any easy fix that comes to your mind to speed this process up Note I cannot attach the cachegrind file as its GB