I missed an injection point added in People on the internet kept echoing the idea that Ludwig is drag and drop but I have not seen any documentation that make this claim is it true For reference here are some drag and drop repos rd party drag and drop Describe the bug Some datasets cause hangup with no progress being made To Reproduce Unfortunately I dont know if I can provide a reproducer since the data is custom data However I am happy to provide log info or any other data that would be helpful The log shows deadlock on st epoch stdout stdout TRAINING stdout stdout stdout stdoutEpoch Training its stdout Evaluation train its stdout Train set K rows rows features Valid set K rows stderr W horovodcommonstallinspectorcc One or more tensors were submitted to be reduced gath ered or broadcasted by subset of ranks and are waiting for remainder of ranks for more than seconds This may indicate that different r anks are trying to submit different tensors or that only subset of ranks is submitting tensors which will cause deadlock stderrStalled ranks stderr optimizerDistributedAdamOptimizerAllreduceHorovodAllgatheroptimizergradientsconcat optimizerDistributedAdamOpt imizerAllreduceHorovodAllgatheroptimizergradientsconcat optimizerDistributedAdamOptimizerAllreduceHorovodAllgatheroptimizer gradientsconcat optimizerDistributedAdamOptimizerAllreduceHorovodAllgatheroptimizergradientsconcat optimizerDistribute dAdamOptimizerAllreduceHorovodAllgatheroptimizergradientsconcat optimizerDistributedAdamOptimizerAllreduceHorovodAllgatherop timizergradientsconcat stderr optimizerDistributedAdamOptimizerAllreduceHorovodAllgatheroptimizergradientsconcat optimizerDistributedAdamOpt imizerAllreduceHorovodAllgatheroptimizergradientsconcat optimizerDistributedAdamOptimizerAllreduceHorovodAllgatheroptimizer gradientsconcat optimizerDistributedAdamOptimizerAllreduceHorovodAllgatheroptimizergradientsconcat optimizerDistribute dAdamOptimizerAllreduceHorovodAllgatheroptimizergradientsconcat optimizerDistributedAdamOptimizerAllreduceHorovodAllgatherop timizergradientsconcat Expected behavior Expect to make training progress Environment please complete the following information OS Ubuntu Version Python version Ludwig version horovodrun v Any pointers to overcome this If split is not full ie I only want to use training or validation or test data to run a test for example and I dont have a hdf file data is loaded anyway but without honoring the split request Reference Code Pull Requests Resolves Summary of changes Added function kfoldcrossvalidate function in trainpy that calls fulltrain k times Added function generatekfoldsplits function to utilsdatautilspy Create kfoldtrainingstatisticsjson statistics file to hold results from the kfold cv run A new parameter kfold is now available for the ludwig train command This is the help output for the new parameter kf KFOLD kfold KFOLD number of folds for a kfold cross validation run Work to be completed Add aggregate node to the foldtrainingstatisticsjson file Parallel processing for kfold cv Note I have a working version of parallel processing in a separate branch I figure it is easier to debug issues with the serial version Once we are comfortable with how kfold cv processing occurs Ill add the parallel processing functionality Parallel processing is implemented using the joblib package add unit test for kfold cv function update documentation I will start with saying sorry because this is not a feature request but a thank you for what has been developed into Ludwig and a concern about its slow updatescommits over the last month There are scantly any day which I dont refresh Ludwig Github to see if there is any new updates Hence me noticing the commits had become less and less frequent When I found out about Ludwig I was ecstatic This is exactly what is needed and already being done in other respects with stable baselines for RL and Optuna bayesian optimization libraries Please continue developing this treasure I am not being able to contribute as such a thing I feel is beyond me at this point technically Having said that I have no doubt that Ludwig will capture more attention as it progresses Thank you again and best of luck with continued development Feel free to delete this as it is really not about a feature request Describe the bug Although pip install ludwig viz was sucessful any call of ludwig visualize results in the message matplotlib or seaborn are not installed In order to install all visualization dependencies run pip install ludwig viz To Reproduce Steps to reproduce the behavior pyenv virtualenv ludwig pyenv global ludwig pip install ludwig pip install ludwig pip install ludwig viz ludwig train datacsv trainingdatacsv modeldefinitionfile modeldefinitionyaml ludwig visualize visualization learningcurves trainingstatistics resultsexperimentrun trainingstatisticsjson Expected behavior Should render graphs and not return an error Environment please complete the following information OS MacOS Python version Ludwig version Additional context If I start Python from CLI I can import matplotlib and seaborn with no problem Apart see shell output below portabelludwig user pip install ludwig viz Requirement already satisfied ludwig viz in Usersuserpyenvversions envsludwiglibpython sitepackages Requirement already satisfied abslpy in Usersuserpyenvversions envsludwiglibpython sitepackages from ludwig viz Requirement already satisfied numpy in Usersuserpyenvversions envsludwiglibpython sitepackages from ludwig viz Requirement already satisfied tqdm in Usersuserpyenvversions envsludwiglibpython sitepackages from ludwig viz Requirement already satisfied Cython in Usersuserpyenvversions envsludwiglibpython sitepackages from ludwig viz Requirement already satisfied tensorflow in Usersuserpyenvversions envsludwiglibpython sitepackages from ludwig viz Requirement already satisfied PyYAML in Usersuserpyenvversions envsludwiglibpython sitepackages from ludwig viz Requirement already satisfied pandas in Usersuserpyenvversions envsludwiglibpython sitepackages from ludwig viz Requirement already satisfied scikitlearn in Usersuserpyenvversions envsludwiglibpython sitepackages from ludwig viz Requirement already satisfied tabulate in Usersuserpyenvversions envsludwiglibpython sitepackages from ludwig viz Requirement already satisfied scipy in Usersuserpyenvversions envsludwiglibpython sitepackages from ludwig viz Requirement already satisfied h py in Usersuserpyenvversions envsludwiglibpython sitepackages from ludwig viz Requirement already satisfied matplotlib extra viz in Usersuserpyenvversions envsludwiglibpython sitepackages from ludwig viz Requirement already satisfied seaborn extra viz in Usersuserpyenvversions envsludwiglibpython sitepackages from ludwig viz Requirement already satisfied six in Usersuserpyenvversions envsludwiglibpython sitepackages from abslpyludwig viz Requirement already satisfied gast in Usersuserpyenvversions envsludwiglibpython sitepackages from tensorflow ludwig viz Requirement already satisfied googlepasta in Usersuserpyenvversions envsludwiglibpython sitepackages from tensorflow ludwig viz Requirement already satisfied keraspreprocessing in Usersuserpyenvversions envsludwiglibpython sitepackages from tensorflow ludwig viz Requirement already satisfied wheel in Usersuserpyenvversions envsludwiglibpython sitepackages from tensorflow ludwig viz Requirement already satisfied protobuf in Usersuserpyenvversions envsludwiglibpython sitepackages from tensorflow ludwig viz Requirement already satisfied tensorboard in Usersuserpyenvversions envsludwiglibpython sitepackages from tensorflow ludwig viz Requirement already satisfied wrapt in Usersuserpyenvversions envsludwiglibpython sitepackages from tensorflow ludwig viz Requirement already satisfied astor in Usersuserpyenvversions envsludwiglibpython sitepackages from tensorflow ludwig viz Requirement already satisfied grpcio in Usersuserpyenvversions envsludwiglibpython sitepackages from tensorflow ludwig viz Requirement already satisfied kerasapplications in Usersuserpyenvversions envsludwiglibpython sitepackages from tensorflow ludwig viz Requirement already satisfied termcolor in Usersuserpyenvversions envsludwiglibpython sitepackages from tensorflow ludwig viz Requirement already satisfied tensorflowestimator rc rc in Usersuserpyenvversions envsludwiglibpython sitepackages from tensorflow ludwig viz Requirement already satisfied pythondateutil in Usersuserpyenvversions envsludwiglibpython sitepackages from pandas ludwig viz Requirement already satisfied pytz in Usersuserpyenvversions envsludwiglibpython sitepackages from pandas ludwig viz Requirement already satisfied joblib in Usersuserpyenvversions envsludwiglibpython sitepackages from scikitlearnludwig viz Requirement already satisfied cycler in Usersuserpyenvversions envsludwiglibpython sitepackages from matplotlib extra vizludwig viz Requirement already satisfied kiwisolver in Usersuserpyenvversions envsludwiglibpython sitepackages from matplotlib extra vizludwig viz Requirement already satisfied pyparsing in Usersuserpyenvversions envsludwiglibpython sitepackages from matplotlib extra vizludwig viz Requirement already satisfied setuptools in Usersuserpyenvversions envsludwiglibpython sitepackages from protobuf tensorflow ludwig viz Requirement already satisfied werkzeug in Usersuserpyenvversions envsludwiglibpython sitepackages from tensorboard tensorflow ludwig viz Requirement already satisfied markdown in Usersuserpyenvversions envsludwiglibpython sitepackages from tensorboard tensorflow ludwig viz portabelludwig user ludwig visualize visualization learningcurves trainingstatistics resultsexperimentrun trainingstatisticsjson matplotlib or seaborn are not installed In order to install all visualization dependencies run pip install ludwig viz Hi w nderlust is there any plan to implement Smiths One Cycle policy in the learning rate scheduler I see that currently the default policy is to anneal the LR or the batch size The One Cycle scheduler has shown very good results since it was introduced and it seems much faster to converge than other schedulers Hello In Numerical Input Features and Encoders the raw float values coming from the input placeholders are passed through a single neuron for scaling purposes Does this model is capable of dealing with trainingset that comes from a set of nonlinear equations single nonlinear equation Would love to see more information regarding the sigmoid function used in that singe neuron or other relevant information if exists Thx 