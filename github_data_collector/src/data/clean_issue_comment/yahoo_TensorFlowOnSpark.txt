Environment Python version eg Spark version eg TensorFlow version eg TensorFlowOnSpark version eg master Cluster version eg Standalone Hadoop CDH Hadoop I am running the hadoopspark installation on AWS EMR at the moment Describe the bug I am trying to run mnist example and I having an issue when performing the data prep using the tensorflowdatasets package In my code mnistdatasetuppy loads the data to HDFS as opposed to local file system as seen below import tensorflowdatasets as tfds mnist info tfdsloadmnist withinfoTrue datadirhdfsdefaultuserhadooptensorflowdatas Perhaps the exception shown below is not pertaining to TensorflowOnSpark directly but I wanted to see leewyang can provide some adviseassistance here Appreciate your time Logs I am receiving the following when running the spark application loadFileSystems error unable to get stack trace for javalangNoClassDefFoundError exception ExceptionUtilsgetStackTrace error hdfsBuilderConnectforceNewInstance nndefault port kerbTicketCachePathNULL userNameNULL error Spark Submit Command Line I have tried various variations including providing LDLIBRARYPATH to the executor env SPARKHOMEbinsparksubmit deploymode cluster queue default numexecutors conf sparkexecutorEnvCLASSPATHhadoop classpath glob executormemory G archives mnistmnistzipmnist jars hdfsuserUSERtensorflowhadoop jarhdfsuserUSERsparktensorflowconnector jar TensorFlowOnSparkexamplesmnistmnistdatasetuppy output cluster format tfr I have performed the hadoop classpath glob and verified that the full list of jars are present on both master and slave nodes Weird part is that when running the same python snippet on pyspark shell after setting up CLASSPATH it seems run perfectly fine import tensorflowdatasets as tfds mnist info tfdsloadmnist withinfoTrue datadirhdfsdefaultuserhadooptensorflowdatas Is there a known limitation around the length that can be passed via Spark Submit Additionally see a related issue here Initial code to leverage Spark GPU resource allocation I confirm that this contribution is made under the terms of the license found in the root directory of this repositorys source tree and that I have the authority necessary to make this contribution on behalf of its copyright owner when execute export PYTHONROOTPython export LDLIBRARYPATHPATH export PYSPARKPYTHONPYTHONROOTbinpython export SPARKYARNUSERENVPYSPARKPYTHONPythonbinpython export PATHPYTHONROOTbinPATH export QUEUEqueuesearch export SPARKHOMEdatasoftspark SPARKHOMEbinsparksubmit master yarn deploymode cluster queue QUEUE numexecutors executormemory G pyfiles TensorFlowOnSparktfsparkzipTensorFlowOnSparkexamplesmnistsparkmnistdistpy conf sparkdynamicAllocationenabledfalse conf sparkyarnmaxAppAttempts archives hdfsuserUSERPython Centos zipPython TensorFlowOnSparkexamplesmnistsparkmnistsparkpy images mnistcsvtrainimages labels mnistcsvtrainlabels mode train model mnistmodel orgapachesparkapipythonPythonException Traceback most recent call last File data emryarnlocalusercachezhaoslappcacheapplication containere pysparkzippysparkworkerpy line in main process File data emryarnlocalusercachezhaoslappcacheapplication containere pysparkzippysparkworkerpy line in process serializerdumpstreamfuncsplitindex iterator outfile File data emryarnlocalusercachezhaoslappcacheapplication containere pysparkzippysparkrddpy line in pipelinefunc File data emryarnlocalusercachezhaoslappcacheapplication containere pysparkzippysparkrddpy line in pipelinefunc File data emryarnlocalusercachezhaoslappcacheapplication containere pysparkzippysparkrddpy line in pipelinefunc File data emryarnlocalusercachezhaoslappcacheapplication containere pysparkzippysparkrddpy line in func File data emryarnlocalusercachezhaoslappcacheapplication containere pysparkzippysparkrddpy line in func File data emryarnlocalusercachezhaoslappcacheapplication containere tfsparkziptensorflowonsparkTFSparkNodepy line in train AttributeError AutoProxy getqueue object has no attribute put at orgapachesparkapipythonBasePythonRunnerReaderIteratorhandlePythonExceptionPythonRunnerscala at orgapachesparkapipythonPythonRunneranon readPythonRunnerscala at orgapachesparkapipythonPythonRunneranon readPythonRunnerscala at orgapachesparkapipythonBasePythonRunnerReaderIteratorhasNextPythonRunnerscala at orgapachesparkInterruptibleIteratorhasNextInterruptibleIteratorscala at scalacollectionIteratorclassforeachIteratorscala at orgapachesparkInterruptibleIteratorforeachInterruptibleIteratorscala at scalacollectiongenericGrowableclasspluspluseqGrowablescala at scalacollectionmutableArrayBufferpluspluseqArrayBufferscala at scalacollectionmutableArrayBufferpluspluseqArrayBufferscala at scalacollectionTraversableOnceclasstoTraversableOncescala at orgapachesparkInterruptibleIteratortoInterruptibleIteratorscala at scalacollectionTraversableOnceclasstoBufferTraversableOncescala at orgapachesparkInterruptibleIteratortoBufferInterruptibleIteratorscala at scalacollectionTraversableOnceclasstoArrayTraversableOncescala at orgapachesparkInterruptibleIteratortoArrayInterruptibleIteratorscala at orgapachesparkrddRDDanonfuncollect anonfun applyRDDscala at orgapachesparkrddRDDanonfuncollect anonfun applyRDDscala at orgapachesparkSparkContextanonfunrunJob applySparkContextscala at orgapachesparkSparkContextanonfunrunJob applySparkContextscala at orgapachesparkschedulerResultTaskrunTaskResultTaskscala at orgapachesparkschedulerTaskrunTaskscala at orgapachesparkexecutorExecutorTaskRunneranonfun applyExecutorscala at orgapachesparkutilUtilstryWithSafeFinallyUtilsscala at orgapachesparkexecutorExecutorTaskRunnerrunExecutorscala at javautilconcurrentThreadPoolExecutorrunWorkerThreadPoolExecutorjava at javautilconcurrentThreadPoolExecutorWorkerrunThreadPoolExecutorjava at javalangThreadrunThreadjava sparksubmit verbose pwdTensorFlowOnSparkexamplesmnistmnistdatasetuppy output userhdfsjupytermnistkerberoscsv Errors with python cant open file TensorFlowOnSparkexamplesmnistsparkmnistsparkpy Errno No such file or directory Thanks Normally TFoS launches the TF process as the user who started the Spark application However weve seen in some cases that it appears that the TF process is running as the yarn user which can cause issues with writing the model file to the users HDFS directory The workaround is to create an HDFS directory that grants write permissions to the yarn user