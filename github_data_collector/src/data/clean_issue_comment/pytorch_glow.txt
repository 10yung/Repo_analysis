Summary NFC Fix a bunch of compilation warnings GCC Documentation NA Test Plan NA Summary Currently due to assert in LoadergetModelOptPath one cannot use mmodelonnx but have to do mmodelonnx instead The patch removes the assert and makes the function return as the directory Documentation NA Test Plan manually tested Summary Fix for Enable the constant folding by default Documentation None Test Plan None Summary Previously if a Tensor was not partial we left unpaddedSize representing that the actual unpaddedSize was that based on the Type This PR changes this so that unpaddedSize is equal to the actual number of allocated bytes pointed to by data This includes fixing Tensorassign so that it respects unpadded size ie if you clone a partial tensor the new tensor is also partial Additionally includes fixing Tensorreset so that if you reset a partial tensor with a type with the same number of bytes but without specifying it should be partial that the old buffer is not reused this could have caused buffer overflow Test Plan Updated unpaddedSize unit test with a few more checks Alternative fix to with a couple other fixes CC jsubag Whenever I use the Glow frontend tools modelcompiler on Windows I get the notification libunwind pc not in table pc xFFFFFFFFFFFFFFFF like below The notification seems like a warning the tool seems to work properly but I was wondering if I should worry about it Thanks Right now Imageclassifier primary works with well image classification networks There are other networks such as Segementation Object Detection The majority of the code to execute them is the same Main differences are how some preprocessing stuff and outputs are handled For example for object detection not only they have multiple outputs but also usually MAAP score is calculated by python scripts Would people be open to idea of refactoring common code in to a seperate file with some hooks for preprocessing and post processing So that imageclassifier and maybe future apps like objectdetection be a wrapper around it I am still thinking full implementation through but wanted to open this to conversation to judge if there is an interest or none starter Before I invest too much time in to it Using marchnative for Release build doesnt seem to be a good practice If built on a topend CPU the compiler wont be able to run on older machines I see two possible solutions for this Introduce a CMake flag to control march value can keep default to be native Use less aggressive march value like x coreavxcoreavx I believe marchnative is passed only if built on x This should only affect Intrerpreter backend performance as LLVMbased CPU backend would still use current machine target while JITing Any preferences or other suggestions clang fails in this line You need to replace NULL with or nothing at all Summary By default lets test all ICE cores if the test supports it by setting parallelCloneCount Unit tests run pretty quickly so this doesnt cost much in runtime overhead and gets us better coverage on the NNPI HW and stack cl opt parallelclonecount will still workoverride whatever the backend sets as its own default This included a fix to avoid stdmoving a Tensor which later led to a nullptr deref Differential Revision D 