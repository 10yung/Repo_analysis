 update the code to fastai Because of update the code cell cannot run in newest version of fastai Hi Sylvain thanks for the awesome exposition in your DeepPainterlyHarmonization notebook I am however running into an issue during the second training phase Not changing anything was able to reproduce the code up until In With the following warning IndexError too many indices for tensor of dimension Heres the detailed warning IndexError Traceback most recent call last ipythoninput eb f f df in module niter while niter maxiter optimizersteppartialstepfinalloss anaconda envspytorchp libpython sitepackagestorchoptimlbfgspy in stepself closure evaluate initial fx and dfdx origloss closure loss floatorigloss currentevals ipythoninput e eba in steplossfn global niter optimizerzerograd loss lossfnoptimgv lossbackward niter ipythoninput d a cbf c in finallossoptimgv closs contentlossoutftrs sloss stylelossoutftrs hloss histloss outftrs outftrs tloss tvlossoptimgv return closs ws sloss wh hloss wtv tloss ipythoninput d c ea a in histlossoutftrs mask VtorchTensormfcontiguousview requiresgradFalse ofmasked of mask ofmasked torchcat ofmasked i mask unsqueeze for i in rangeofmaskedsize loss Fmselossofmasked Vremaphistofmasked sh requiresgradFalse return loss ipythoninput d c ea a in listcomp mask VtorchTensormfcontiguousview requiresgradFalse ofmasked of mask ofmasked torchcat ofmasked i mask unsqueeze for i in rangeofmaskedsize loss Fmselossofmasked Vremaphistofmasked sh requiresgradFalse return loss IndexError too many indices for tensor of dimension Hey I was implementing cycle policy as an exercise And I have a few observations from my experiments I have a Model Resnet Batch size for training Batch size for testing Optimser optimSGDnetparameters lr momentum weightdecay e Total number of epochs cycle policy Learning rate goes from to and back till epochs Then model is trained for epochs at learning rate No cyclic momentum used or adamw I achieved a test set accuracy of in epochs This seems like a big difference from the epochs at batch size that is quoted in your blog post Am I doing something wrong Is the number of epochs a good metric to base your results on as those are dependant on the batch size The whole point of using super convergence is using high learning rates to converge quicker but it seems like using low learning rates is faster to train 