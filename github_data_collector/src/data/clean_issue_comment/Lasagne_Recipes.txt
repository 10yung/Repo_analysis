Hi I try to run the resnet model on cifar dataset with only the difference of the training data in DeepResidualLearningCIFAR py but it causes the error like this Starting training Traceback most recent call last File homechangchenanaconda libpython sitepackagestheanocompilefunctionmodulepy line in call selffn if outputsubset is None else RuntimeError error getting worksize CUDNNSTATUSBADPARAM During handling of the above exception another exception occurred Traceback most recent call last File resnetpy line in module mainkwargs File resnetpy line in main trainerr trainfninputs targets File homechangchenanaconda libpython sitepackagestheanocompilefunctionmodulepy line in call storagemapgetattrselffn storagemap None File homechangchenanaconda libpython sitepackagestheanogoflinkpy line in raisewithop reraiseexctype excvalue exctrace File homechangchenanaconda libpython sitepackagessixpy line in reraise raise valuewithtracebacktb File homechangchenanaconda libpython sitepackagestheanocompilefunctionmodulepy line in call selffn if outputsubset is None else RuntimeError error getting worksize CUDNNSTATUSBADPARAM Apply node that caused the error GpuDnnConvalgosmall inplaceTrue numgroups GpuContiguous GpuContiguous GpuAllocEmptydtypefloat contextnameNone GpuDnnConvDescbordermodehalf subsample dilation convmodecross precisionfloat numgroups Constant Constant Toposort index Inputs types GpuArrayTypeNonefloat D GpuArrayTypeNonefloat D GpuArrayTypeNonefloat D theanogoftypeCDataType object at x fa a Scalarfloat Scalarfloat Inputs shapes No shapes Inputs strides No strides Inputs values not shown not shown not shown capsule object NULL at x fa c e Outputs clients GpuElemwisesubnoinplaceGpuDnnConvalgosmall inplaceTrue numgroups InplaceGpuDimShufflex xx GpuContiguousGpuDnnConvalgosmall inplaceTrue numgroups GpuElemwisesubnoinplaceGpuDnnConvalgosmall inplaceTrue numgroups GpuElemwiseCompositei i i i gpuarray Backtrace when the node is createduse Theano flag tracebacklimitN to make it longer File resnetpy line in module mainkwargs File resnetpy line in main prediction lasagnelayersgetoutputnetwork File homechangchenanaconda libpython sitepackageslasagnelayershelperpy line in getoutput alloutputs layer layergetoutputforlayerinputs kwargs File homechangchenanaconda libpython sitepackageslasagnelayersconvpy line in getoutputfor conved selfconvolveinput kwargs File homechangchenanaconda libpython sitepackageslasagnelayersconvpy line in convolve extrakwargs HINT Use the Theano flag exceptionverbosityhigh for a debugprint and storage map footprint of this apply node Seems something wrong happend in the convolution operation Could you please give any advice Thanks a lot I was trying to implement the following piece of code I am getting an error pertaining to the training function in Theano I am using the CK dataset When I hadnt reshaped my inputvar I got an error TypeError Bad input argument to theano function with name CUsersTathagat Dasguptaspyderfredtestpy at index based Wrong number of dimensions expected got with shape L L L After reshaping the array by using npexpanddims the error changes to Bad input argument to theano function with name CUsersTathagat Dasguptaspyderfredtestpy at index based Wrong number of dimensions expected got with shape L L L L Though the input placeholder is a d tensor the error says expected How is this possible Why is the error changing Pls suggest solutions Stuck in a dead end Presently LasagneRecipes does not seem to provide an example of an object detector Our contribution is a highperformance lowcomplexity singleshot detector SSD This detector is based on the SqueezeDet paper originally implemented in TensorFlow Our port to Lasagne includes some additional features for userfriendliness Hi following the Guided Backpropagation recipeexample I had error for if theanosandboxcudacudaenabled because I guess I use newer theano lasagne versions with CuDNN etc I have no cuda under theanosandbox two solutions skip the if and use the else option maybetogpu lambda x x use gpu with gpuarraydnn maybetogpu theanogpuarraydnnasgpuarrayvariable x maybetogpuxNone and then a few lines below as above add None as second input argument outp maybetogpuselfnonlinearityinpNone Thanks for the examples Unfortunately this comes a bit late as theano has recently merged a PR adding some bindings to warpctc But I wanted to finish this anyway so here it is This implementation is written in pure theano uses an overriden gradient computation which is more resilient to precision issues fairly compact suggestions for improvements and readability are welcome works in log space for the most parts to prevent precision issues so does warpctc Note that I havent use the rescaling trick though dont know if warpctc uses it I think it can still be useful to anyone who wants to modify the original cost function And it can run without extra dependency on any plateform where theano runs already Notes I havent battle tested the code just run tests so far It seems to give results very close to warpctc as it should difference on the gradients The code uses OpFromGraph which is relatively recent in the theano codebase I have no demo so far contributions are welcome for that I think the test script is a poor replacement for a real demo We can have an example of a Siamese network similar to Sander Dielemans suggestion and convolutional spatial transformer as discussed here From my understanding the convolutional spatial transformer applies the ST on the feature patchwise with distinct affine transformation parameters I have implemented this very recently and will open a PR if you are fine with having these examples in the Recipes f k PS I have a few PRs and issues open in Lasagne which Im yet to resolve The past two weeks were a bit hectic for me and I will try to resolve them by this weekend Apologies After training the script prints some segmentation results These are mostly empty and suggest that the network did not train properly which is not the case This is due to a bug in the segoutput definition which I fixed in this pull request Cheers Fabian I modified FabianIsensee implementation of D UNet to D UNet using the papers description The implementation is here I tried it on a D medical image dataset but I cannot say I am getting satisfying results So I am not sure if my implementation is buggy or in general D UNet does not perform well FabianIsensee you mentioned you have implemented D UNet what is your experience Do you get good results FabianIsensee I am trying to modify the categoricalcrossentropy loss function to dicecoefficient loss function in the Lasagne Unet example I found this implementation in Keras and I modified it for Theano like below def dicecoefypredytrue smooth ytruef Tflattenytrue ypredf TflattenTargmaxypred axis intersection Tsumytruef ypredf return intersection smooth Tsumytruef Tsumypredf smooth def dicecoeflossypred ytrue return dicecoefypred ytrue I am not sure if there is problem with my implementation or Dice coefficient is not robust See output during training validation In comparison when I use categorical crossentropy I get like AUC I was wondering if anyone has played with Dice on UNet Started Experiment Epoch train accuracy train loss val accuracy val loss val AUC score Epoch train accuracy train loss val accuracy val loss val AUC score Epoch train accuracy train loss val accuracy val loss val AUC score Epoch train accuracy train loss val accuracy val loss val AUC score gyglim I was wondering if you could share your code on training the C D eg training dataset optimizer I am trying to train C D on my own data but either get NaN or memory problems How did you manage to train such a massive network Thanks