Hi I can read redshift data into spark dataframe using databricks data source below from pysparksql import SQLContext from pyspark import SparkConf SparkContext sc SparkContextconfSparkConfsetAppNameMyAppsetMasterlocal hadoopconfscjschadoopConfiguration hadoopconfsetfss nimpl orgapachehadoopfss nativeNativeS FileSystem hadoopconfsetfss nawsAccessKeyId XXXXXXXXXXXXXXXXXX hadoopconfsetfss nawsSecretAccessKey oZTJs aNFXXXXXXXXXXXXXXXXXBL DTY sparkdf sqlcontextread formatcomdatabrickssparkredshift optionurl jdbcredshiftrxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx optiondbtable publicevents optiontempdir s nbigquerytorsrstempdata load But when I am trying to insert first entry of dataframe into same table using code below it give the error as shown in the stack trace printdf count dfcount first df sparkdflimit printType is typefirst df printFirst hunderead first df printFirst hunderead count first dfcount for row in first dfrddcollect printrow Write back to a table first dfwrite formatcomdatabrickssparkredshift optionurl jdbcredshiftxxxxxxxxxxxxxxxxxxxxxxxxx optiondbtable publicevents optiontempdir s nbigquerytorsrstempdata modeoverwrite save This is the error I am getting while writing loaded databack to redshift I tried all four modeappend ignoreoverwriteetc with no luck I am stuck here since long Any leads highly appreciated Thanks Traceback most recent call last File homeubuntutrelldsframeworkdataengineeringdatamigrationt py line in module modeoverwrite File homeubuntuspark binhadoop pythonlibpysparkzippysparksqlreadwriterpy line in save File homeubuntuspark binhadoop pythonlibpy j srczippy jjavagatewaypy line in call File homeubuntuspark binhadoop pythonlibpysparkzippysparksqlutilspy line in deco File homeubuntuspark binhadoop pythonlibpy j srczippy jprotocolpy line in getreturnvalue py jprotocolPy JJavaError An error occurred while calling o save orgapachesparksqlcatalysterrorspackageTreeNodeException execute tree Exchange SinglePartition LocalLimit Scan RedshiftRelationpublicevents eventdate eventtimestamp Leventname eventparams userid eventprevioustimestamp Leventbundlesequenceid Luserpseudoid userproperties userfirsttouchtimestamp Leventservertimestampoffset Ldevice geo appinfo trafficsource streamid platform PushedFilters ReadSchema structeventdatestringeventtimestampbiginteventnamestringeventparamsstringuseridstr at orgapachesparksqlcatalysterrorspackageattachTreepackagescala at orgapachesparksqlexecutionexchangeShuffleExchangeExecdoExecuteShuffleExchangeExecscala at orgapachesparksqlexecutionSparkPlananonfunexecute applySparkPlanscala at orgapachesparksqlexecutionSparkPlananonfunexecute applySparkPlanscala at orgapachesparksqlexecutionSparkPlananonfunexecuteQuery applySparkPlanscala at orgapachesparkrddRDDOperationScopewithScopeRDDOperationScopescala at orgapachesparksqlexecutionSparkPlanexecuteQuerySparkPlanscala at orgapachesparksqlexecutionSparkPlanexecuteSparkPlanscala at orgapachesparksqlexecutionInputAdapterinputRDDsWholeStageCodegenExecscala at orgapachesparksqlexecutionBaseLimitExecclassinputRDDslimitscala at orgapachesparksqlexecutionGlobalLimitExecinputRDDslimitscala at orgapachesparksqlexecutionWholeStageCodegenExecdoExecuteWholeStageCodegenExecscala at orgapachesparksqlexecutionSparkPlananonfunexecute applySparkPlanscala at orgapachesparksqlexecutionSparkPlananonfunexecute applySparkPlanscala at orgapachesparksqlexecutionSparkPlananonfunexecuteQuery applySparkPlanscala at orgapachesparkrddRDDOperationScopewithScopeRDDOperationScopescala at orgapachesparksqlexecutionSparkPlanexecuteQuerySparkPlanscala at orgapachesparksqlexecutionSparkPlanexecuteSparkPlanscala at orgapachesparksqlexecutionDeserializeToObjectExecdoExecuteobjectsscala at orgapachesparksqlexecutionSparkPlananonfunexecute applySparkPlanscala at orgapachesparksqlexecutionSparkPlananonfunexecute applySparkPlanscala at orgapachesparksqlexecutionSparkPlananonfunexecuteQuery applySparkPlanscala at orgapachesparkrddRDDOperationScopewithScopeRDDOperationScopescala at orgapachesparksqlexecutionSparkPlanexecuteQuerySparkPlanscala at orgapachesparksqlexecutionSparkPlanexecuteSparkPlanscala at orgapachesparksqlexecutionQueryExecutiontoRddlzycomputeQueryExecutionscala at orgapachesparksqlexecutionQueryExecutiontoRddQueryExecutionscala at orgapachesparksqlDatasetrddlzycomputeDatasetscala at orgapachesparksqlDatasetrddDatasetscala at comdatabrickssparkredshiftRedshiftWriterunloadDataRedshiftWriterscala at comdatabrickssparkredshiftRedshiftWritersaveToRedshiftRedshiftWriterscala at comdatabrickssparkredshiftDefaultSourcecreateRelationDefaultSourcescala at orgapachesparksqlexecutiondatasourcesSaveIntoDataSourceCommandrunSaveIntoDataSourceCommandscala at orgapachesparksqlexecutioncommandExecutedCommandExecsideEffectResultlzycomputecommandsscala at orgapachesparksqlexecutioncommandExecutedCommandExecsideEffectResultcommandsscala at orgapachesparksqlexecutioncommandExecutedCommandExecdoExecutecommandsscala at orgapachesparksqlexecutionSparkPlananonfunexecute applySparkPlanscala at orgapachesparksqlexecutionSparkPlananonfunexecute applySparkPlanscala at orgapachesparksqlexecutionSparkPlananonfunexecuteQuery applySparkPlanscala at orgapachesparkrddRDDOperationScopewithScopeRDDOperationScopescala at orgapachesparksqlexecutionSparkPlanexecuteQuerySparkPlanscala at orgapachesparksqlexecutionSparkPlanexecuteSparkPlanscala at orgapachesparksqlexecutionQueryExecutiontoRddlzycomputeQueryExecutionscala at orgapachesparksqlexecutionQueryExecutiontoRddQueryExecutionscala at orgapachesparksqlDataFrameWriteranonfunrunCommand applyDataFrameWriterscala at orgapachesparksqlDataFrameWriteranonfunrunCommand applyDataFrameWriterscala at orgapachesparksqlexecutionSQLExecutionwithNewExecutionIdSQLExecutionscala at orgapachesparksqlDataFrameWriterrunCommandDataFrameWriterscala at orgapachesparksqlDataFrameWritersaveToV SourceDataFrameWriterscala at orgapachesparksqlDataFrameWritersaveDataFrameWriterscala at sunreflectNativeMethodAccessorImplinvoke Native Method at sunreflectNativeMethodAccessorImplinvokeNativeMethodAccessorImpljava at sunreflectDelegatingMethodAccessorImplinvokeDelegatingMethodAccessorImpljava at javalangreflectMethodinvokeMethodjava at py jreflectionMethodInvokerinvokeMethodInvokerjava at py jreflectionReflectionEngineinvokeReflectionEnginejava at py jGatewayinvokeGatewayjava at py jcommandsAbstractCommandinvokeMethodAbstractCommandjava at py jcommandsCallCommandexecuteCallCommandjava at py jGatewayConnectionrunGatewayConnectionjava at javalangThreadrunThreadjava Caused by javaioIOException s nbigquerytors Bad Request at orgapachehadoopfss nativeJets tNativeFileSystemStoreprocessExceptionJets tNativeFileSystemStorejava at orgapachehadoopfss nativeJets tNativeFileSystemStoreprocessExceptionJets tNativeFileSystemStorejava at orgapachehadoopfss nativeJets tNativeFileSystemStorehandleExceptionJets tNativeFileSystemStorejava at orgapachehadoopfss nativeJets tNativeFileSystemStoreretrieveMetadataJets tNativeFileSystemStorejava at sunreflectNativeMethodAccessorImplinvoke Native Method at sunreflectNativeMethodAccessorImplinvokeNativeMethodAccessorImpljava at sunreflectDelegatingMethodAccessorImplinvokeDelegatingMethodAccessorImpljava at javalangreflectMethodinvokeMethodjava at orgapachehadoopioretryRetryInvocationHandlerinvokeMethodRetryInvocationHandlerjava at orgapachehadoopioretryRetryInvocationHandlerinvokeRetryInvocationHandlerjava at orgapachehadoopfss nativeProxy retrieveMetadataUnknown Source at orgapachehadoopfss nativeNativeS FileSystemgetFileStatusNativeS FileSystemjava at orgapachehadoopfsGlobbergetFileStatusGlobberjava at orgapachehadoopfsGlobberglobGlobberjava at orgapachehadoopfsFileSystemglobStatusFileSystemjava at orgapachehadoopmapreducelibinputFileInputFormatsingleThreadedListStatusFileInputFormatjava at orgapachehadoopmapreducelibinputFileInputFormatlistStatusFileInputFormatjava at orgapachehadoopmapreducelibinputFileInputFormatgetSplitsFileInputFormatjava at orgapachesparkrddNewHadoopRDDgetPartitionsNewHadoopRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at scalaOptiongetOrElseOptionscala at orgapachesparkrddRDDpartitionsRDDscala at orgapachesparkrddUnionRDDanonfun applyUnionRDDscala at orgapachesparkrddUnionRDDanonfun applyUnionRDDscala at scalacollectionTraversableLikeanonfunmap applyTraversableLikescala at scalacollectionTraversableLikeanonfunmap applyTraversableLikescala at scalacollectionIndexedSeqOptimizedclassforeachIndexedSeqOptimizedscala at scalacollectionmutableWrappedArrayforeachWrappedArrayscala at scalacollectionTraversableLikeclassmapTraversableLikescala at scalacollectionAbstractTraversablemapTraversablescala at orgapachesparkrddUnionRDDgetPartitionsUnionRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at scalaOptiongetOrElseOptionscala at orgapachesparkrddRDDpartitionsRDDscala at orgapachesparkrddMapPartitionsRDDgetPartitionsMapPartitionsRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at scalaOptiongetOrElseOptionscala at orgapachesparkrddRDDpartitionsRDDscala at orgapachesparkrddMapPartitionsRDDgetPartitionsMapPartitionsRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at scalaOptiongetOrElseOptionscala at orgapachesparkrddRDDpartitionsRDDscala at orgapachesparkrddMapPartitionsRDDgetPartitionsMapPartitionsRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at scalaOptiongetOrElseOptionscala at orgapachesparkrddRDDpartitionsRDDscala at orgapachesparkrddMapPartitionsRDDgetPartitionsMapPartitionsRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at scalaOptiongetOrElseOptionscala at orgapachesparkrddRDDpartitionsRDDscala at orgapachesparkrddMapPartitionsRDDgetPartitionsMapPartitionsRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at scalaOptiongetOrElseOptionscala at orgapachesparkrddRDDpartitionsRDDscala at orgapachesparkShuffleDependencyinitDependencyscala at orgapachesparksqlexecutionexchangeShuffleExchangeExecprepareShuffleDependencyShuffleExchangeExecscala at orgapachesparksqlexecutionexchangeShuffleExchangeExecprepareShuffleDependencyShuffleExchangeExecscala at orgapachesparksqlexecutionexchangeShuffleExchangeExecanonfundoExecute applyShuffleExchangeExecscala at orgapachesparksqlexecutionexchangeShuffleExchangeExecanonfundoExecute applyShuffleExchangeExecscala at orgapachesparksqlcatalysterrorspackageattachTreepackagescala more Caused by orgjets tserviceimplrestHttpException Bad Request at orgjets tserviceimplresthttpclientRestStorageServiceperformRequestRestStorageServicejava at orgjets tserviceimplresthttpclientRestStorageServiceperformRequestRestStorageServicejava at orgjets tserviceimplresthttpclientRestStorageServiceperformRestHeadRestStorageServicejava at orgjets tserviceimplresthttpclientRestStorageServicegetObjectImplRestStorageServicejava at orgjets tserviceimplresthttpclientRestStorageServicegetObjectDetailsImplRestStorageServicejava at orgjets tserviceStorageServicegetObjectDetailsStorageServicejava at orgjets tserviceStorageServicegetObjectDetailsStorageServicejava at orgapachehadoopfss nativeJets tNativeFileSystemStoreretrieveMetadataJets tNativeFileSystemStorejava more INFO SparkContext Invoking stop from shutdown hook INFO AbstractConnector Stopped Spark e ef HTTP http Hi We are using Spark SparkRedshift version Suddenly we found out using SparkRedshift to generate parquet files from an existing Redshift table cannot be read by the Spark anymore This maybe is caused by Redshifts side changes but we cannot find out it We like the performance and throughput of SparkRedshift and use the following sample code to bring one table data from Redshift to HDFS val df sparksqlContextreadformatcomdatabrickssparkredshiftoptionurl surl suseruser passwordjdbcPasswordoptiondbtable stablenameoptiontempdir ss Destoptionforwardsparks credentialstrueload dftoDFdfcolumnsmaptoLowerCasewriteformatparquetmodeSaveModeOverwriteoptioncompressionsnappysavestagePathtoString After that if we tried to read the parquet files generated val df sparkreadparquetstagePathtoString dfcount works but following spark code wont work sparkshell SPARKMAJORVERSION is set to using Spark Welcome to version scala val df sparkreadparquetredshiftparquetfiles df orgapachesparksqlDataFrame domain string scanlastupdatetime timestamp more fields scala dfcount WARN Utils Truncated the string representation of a plan since it was too large This behavior can be adjusted by setting sparkdebugmaxToStringFields in SparkEnvconf res Long scala dffilterdomain is not nullcount Stage WARN TaskSetManager Lost task in stage TID p hdp adprodccnet executor javalangArrayIndexOutOfBoundsException at orgapachesparksqlexecutionvectorizedOnHeapColumnVectorputBytesOnHeapColumnVectorjava at orgapachesparksqlexecutionvectorizedColumnVectorappendBytesColumnVectorjava at orgapachesparksqlexecutionvectorizedOnHeapColumnVectorputByteArrayOnHeapColumnVectorjava at orgapachesparksqlexecutiondatasourcesparquetVectorizedPlainValuesReaderreadBinaryVectorizedPlainValuesReaderjava at orgapachesparksqlexecutiondatasourcesparquetVectorizedRleValuesReaderreadBinarysVectorizedRleValuesReaderjava at orgapachesparksqlexecutiondatasourcesparquetVectorizedColumnReaderreadBinaryBatchVectorizedColumnReaderjava at orgapachesparksqlexecutiondatasourcesparquetVectorizedColumnReaderreadBatchVectorizedColumnReaderjava at orgapachesparksqlexecutiondatasourcesparquetVectorizedParquetRecordReadernextBatchVectorizedParquetRecordReaderjava at orgapachesparksqlexecutiondatasourcesparquetVectorizedParquetRecordReadernextKeyValueVectorizedParquetRecordReaderjava at orgapachesparksqlexecutiondatasourcesRecordReaderIteratorhasNextRecordReaderIteratorscala at orgapachesparksqlexecutiondatasourcesFileScanRDDanon hasNextFileScanRDDscala at orgapachesparksqlcatalystexpressionsGeneratedClassGeneratedIteratorscannextBatchUnknown Source at orgapachesparksqlcatalystexpressionsGeneratedClassGeneratedIteratoraggdoAggregateWithoutKeyUnknown Source at orgapachesparksqlcatalystexpressionsGeneratedClassGeneratedIteratorprocessNextUnknown Source at orgapachesparksqlexecutionBufferedRowIteratorhasNextBufferedRowIteratorjava at orgapachesparksqlexecutionWholeStageCodegenExecanonfun anon hasNextWholeStageCodegenExecscala at scalacollectionIteratoranon hasNextIteratorscala at orgapachesparkshufflesortBypassMergeSortShuffleWriterwriteBypassMergeSortShuffleWriterjava at orgapachesparkschedulerShuffleMapTaskrunTaskShuffleMapTaskscala at orgapachesparkschedulerShuffleMapTaskrunTaskShuffleMapTaskscala at orgapachesparkschedulerTaskrunTaskscala at orgapachesparkexecutorExecutorTaskRunnerrunExecutorscala at javautilconcurrentThreadPoolExecutorrunWorkerThreadPoolExecutorjava at javautilconcurrentThreadPoolExecutorWorkerrunThreadPoolExecutorjava at javalangThreadrunThreadjava Basically any column using isNULL or isNOTNULL filter will cause the javalangArrayIndexOutOfBoundsException Currently s amazonawscom is being appended to s urls This doesnt work for other regions like gov cloud and results in your s requests going to the wrong place When unloading a redshift query to s redshift behavior has shifted to no longer unload any files where the old behavior was to unload empty files This causes the sparkredshift library to fail with an s file not found error This commit fixes that and returns an empty dataframe Fix for issue When i try to query the list of tables in a schema from a Redshift DB I get following error I have tried to use query and dbtable options with the same result When i query the DB with lets say dbeaver I can extract the list of tables with no problem If I use below script with a real table it works fine DataBricks includes Apache Spark Scala javasqlSQLException Exception thrown in awaitResult databrickssparkpythonpysparksqldataframepy in showself n truncate vertical if isinstancetruncate bool and truncate printselfjdfshowStringn vertical else printselfjdfshowStringn inttruncate vertical databrickssparkpythonlibpy j srczippy jjavagatewaypy in callself args answer selfgatewayclientsendcommandcommand returnvalue getreturnvalue answer selfgatewayclient selftargetid selfname for temparg in tempargs This is the script i use JDBCURL jdbcredshiftxyzredshiftamazonawscom xyzuseruser passwordpwd SQLQUERY SELECT FROM informationschematables t WHERE ttableschema schemaname AND ttabletype BASE TABLE REDSHIFTS TEMPFOLDER s axyz df sparkread formatcomdatabrickssparkredshift optionurl JDBCURL optionquery SQLQUERY optiontempdir REDSHIFTS TEMPFOLDER optionforwardsparks credentials true load dfshow I cant seem to get any query to work using temporary STS tokens Anytime token is included in the query it fails with a When I try to interact using the awscli with the same credentials everything works Perhaps the sparkredshift code does more with the bucket than the role I am using allows for Is there list of the minimum set of permissions needed to unload data img width altScreen Shot at AM src DBR MT AWS See Issue I think it is a very trivial fix but will save lot of time of developers facing the issue Please review and merge Thanks 