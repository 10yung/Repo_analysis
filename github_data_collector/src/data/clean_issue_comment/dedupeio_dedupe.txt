 fastcluster x pyhacrf x hcluster x pylbfgs BTrees x affinegap levenshtein search doublemetaphone zopeindex persistent We use tuples extensively throughout the code base mainly for memory reasons It would make for easier to understand code if we used namedtuples which apparently dont add memory overhead Whoops mdashoriginally posted this as dedupeiodedupeexamples whereas it is probably better for this repo Moving it over here instead Thanks for this awesome project Were really excited to start using it That said weve had a lot of trouble understanding the documentation In particular for blocks it appears to be a combination of missing information out of date and internally inconsistent The clearest example of this is in the documentation for Gazetteer link Were using Gazetteer on a large quantity of data so were trying to use matchBlocks instead of match but even with reading the source code we havent been able to figure out how to properly set up the blocks As an initial example the Gazetteer object in the docs actually defines matchBlocks twice with different documentation that includes different examples First definition with the final example block structure as follows the example itself is also syntactically invalid the parens dont match up The second redefinition doesnt have a direct page link so go here and then scroll up slightly to see it Final example block structure it gives StaticGazetteer has the same issue presumably due to inheritance Can you clarify what the right way to do blocking is Thanks so much would make life a bit easier Running into the error OverflowError Python int too large to convert to C ssizet Referenced and and made changes to corepy to int which then causes the error ValueError dimensions are too large arrays and shapes with a total size greater than intp are not supported Calls Traceback most recent call last File fXXXXXXXXXXXXXXXXthirdpartydedupetestpy line in module dedupersampletempd tempz File C Python Python lib sitepackages dedupe apipy line in sample indexincludeexamples File C Python Python lib sitepackages dedupe labelerpy line in init samplesize File C Python Python lib sitepackages dedupe labelerpy line in sample randomsamplesize File C Python Python lib sitepackages dedupe corepy line in randomPairsMatch i j numpyunravelindexrandompairs nrecordsA nrecordsB File arrayfunction internals line in unravelindex ValueError dimensions are too large arrays and shapes with a total size greater than intp are not supported I am using MongoDB to get clusterids and passing the data records in the code to dedupermatchBlocks as below but I am not getting any clusterids back as clustereddupes is always empty am I passing the data to match blocks in a wrong format if this is wrong format for match blocks please let me know the correct format Records with smallerids ETY EtyNbr ETY blockid smallerids Alpha A City EAST HARTFORD ClusterInd N Cntry US Fname A DALE JUDY FullAddr LANGFORD LN Lname HUSTON State CT Zip clusterconf clusterid ETY EtyNbr ETY blockid smallerids Alpha A City EAST HARTFORD ClusterInd N Cntry US Fname A DALE JUDY FullAddr LANGFORD LN Lname HUSTON State CT Zip clusterconf clusterid When I pass the data to match blocks as below where with set is empty then I am getting clusterids from clustereddupes Records without smallerids ETY EtyNbr ETY blockid smallerids Alpha A City EAST HARTFORD ClusterInd N Cntry US Fname A DALE JUDY FullAddr LANGFORD LN Lname HUSTON State CT Zip clusterconf clusterid set ETY EtyNbr ETY blockid smallerids Alpha A City EAST HARTFORD ClusterInd N Cntry US Fname A DALE JUDY FullAddr LANGFORD LN Lname HUSTON State CT Zip clusterconf clusterid set Hi I upgraded the library and ran gazette but its getting much slower than the previous versions I found it started to get slower since from version And the function its slower is dedupecorescoreGazette I compared the GitHub from and and cannot tell the reason why its happening After some investigation I think it might be the multiprocessing in the later version is not working properly I tried it with both multiprocessing and no multiprocessing and running no multiprocessing is even faster than the current one with multiprocessing Could you help investigate this Thank you Hello Im trying to use mysql example with my data I have a db with fields famnamsnamsexdtbeth with full name sex and date of birth My task is to find doubles To train the model I select records from db and create manually doubles doubles and one marked as unsure After training is complete I try to use it on the whole database I get the error below I found closed issues with this problem but I could not find a solution Traceback most recent call last File D Distr WPy mysqlexamplepy line in module for cluster scores in clustereddupes File C Program Files x Python lib sitepackages dedupe apipy line in matchBlocks threshold File C Program Files x Python lib sitepackages dedupe corepy line in scoreDuplicates raise BlockingErrorNo records have been blocked together dedupecoreBlockingError No records have been blocked together Is the data you are trying to match like the data you trained on When i was labeling I matched two records but then in the output doesnt match I remove the settings file and with dedupermarkPairslabeledexamples I specify this match but it still doesnt appear