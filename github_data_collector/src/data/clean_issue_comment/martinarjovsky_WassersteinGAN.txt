There are two questions firstly dose the discriminator have batchnormalization layer secondly if so should the trainable parameters on batchnorm layer be clipped Thanks in advance In vanilla GANs a sigmoid activation is applied on the output layer for G and D See For WGAN there is none for D and we get a score instead of a probability However in the code there is no activation eg sigmoid for WGANMLP also for G whereas there is tanh for WGANDCGAN Is there a specific reason Thanks in advance Hello Im trying to train WGAN in order to increase the amount of images img that I have from my own dataset The size of my images are x but only for testing purposes of this net I am resizing them to x To add my own dataset I just added this code lines to the mainpy file elif optdataset own normalize transformsNormalizemean std dataset dsetImageFolderrootoptdataroot transformtransformsCompose transformacionesTransformaciones transformsRandomHorizontalFlip transformsResizeoptimageSizeoptimageSize transformsToTensor normalize Transformaciones is the data augmentation module that i have build I launch the training with extra layers and epoch The thing is that generator net only produces noise I tried to use the generator net when the results were like lossG approx and I only had noise in the images I am doing something wrong Is because I have only a few images to train Someone has tried to use an own dataset and managed to made it work Thanks Hi I dont use torch a lot and I have a question regarding the implementation of the discriminator loss at line errD errDreal errDfake where errDreal is the gradient of real samples line errDrealbackwardone and errDfake is the gradient of fake samples errDfakebackwardmone However in the paper it seems that errD needs to be maximized while it is minimized here Thanks Traceback most recent call last File mainpy line in module netG dcganDCGANGoptimageSize nz nc ngf ngpu nextralayers File homemaliWassersteinGANmastermodelsdcganpy line in init nnConvTranspose dnz cngf biasFalse File homemalianaconda libpython sitepackagestorchnnmodulesmodulepy line in addmodule raise KeyErrormodule name cant contain KeyError module name can t contain the version on my computer are pytorch torchvision I guess it should be run in old version could anyone tell me which version can run it The new version of torchvision has modified this parameter parseraddargumentimageSize typeint default helpthe height width of the input image to network the default imagesize is how to define a new image size like mn For WGAN it should maximize the loss of Discriminator and minimize the loss of negative Generator However it did just in the opposite way in the codes am I wrong I think it should like this errDrealbackwardmone in errDfakebackwardone in errGbackwardmone in In the paper you report a negative result that WGAN training becomes unstable at times when one uses a momentum based optimizer such as Adam with B on the critic or when one uses high learning rates You advocate using RMSProp for the discriminator instead Yet in the implementation although RMSProp is the default there is an option to use Adam line Is this included for consistency with your evaluation or have you found settings for which Adam is effective with the WGAN