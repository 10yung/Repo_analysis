DESCRIPTION avoid hanging on connections dropped during ROLLBACK SAVEPOINT While trying to create a test case for I came across a hang The cause is that ROLLBACK SAVEPOINT leaves the connection alone until adaptiveexecutorc gets the connection again from placementconnectionc proceeds to poll it for readablewriteable This is to track improvements and features that can be implemented upon I will cleanup and edit this issue but for now just pasting marcocitus s ideas from so we dont lose track of them Some ideas for future improvements Replace readintermediateresult function scan with a custom scan that returns tuples as they are parsed That saves a lot of data copying especially in large INSERTSELECTs Clean up intermediate results at the end of execution to avoid keeping and potentially flushing a lot of data on disk Support GROUP BY distributioncolumnoftargettable queries in the final INSERTSELECT step Some ideas for exciting new use cases for appendmostly data that look more feasible now Do all your preprocessing in Citus Simply shove your JSON objects into a distributed table with a bigserial as its distribution column and then transform it in parallel using INSERTSELECT Create a new distributed table type which has no distribution column and exactly one shard on each table could use append and supports metadata syncing Writes are always done to the local shard Allows you to write locally on any node providing infinite scale and extremely high writeavailability and INSERTSELECT is used to put data into more structured form Create appendpartitioned tables where each shard is a foreign table Use INSERTSELECT to put the data into a more structured form Use intermediate result infrastructure for repartition joins allows combining subqueries and CTEs with repartition joins Change the distribution column type of your table Keep N copies of your data with different distribution columns to do different types of joins Updates multiextension to show values rather than result of equality comparison in order to see what those different values are DESCRIPTION avoid marking reference table shards unhealthy in the presence of savepoints Alternative to TODO add test Fixes When we sync metadata we always do so as superuser because we need to modify tables like pgdistpartition directly However we also create tables as superuser which means that we need to open a new session that cannot see uncommitted CREATE TYPE That means we cannot support a transaction that first creates a type and then a distributed table that depends on that type We currently work around this by creating the types in a nontransactional way which creates other issues To stop this issue from proliferating we need to create distributed tables as a the current user when inside a transaction block while changing the metadata as superuser This is nontrivial since the OID of the uncommitted table and distribution column type need to be written to the metadata table I can reproduce this on v and master as well SQL psql c CREATE TABLE userstable userid int time timestamp value int value int value float value bigint p postgres psql c SELECT createdistributedtableuserstable userid p postgres WITH a AS SELECT FROM usersTable SELECT count userid FROM a GROUP BY userid HAVING maxvalue SELECT maxvalue FROM a ERROR result does not exist CONTEXT while executing command on localhost In INSERTSELECT the planner allows subqueries in RETURNING to pass through and fail on the worker postgres INSERT INTO test y x SELECT FROM test ON CONFLICT DO NOTHING RETURNING SELECT maxx FROM test LIMIT ERROR relation publictest does not exist CONTEXT while executing command on localhost When I use mastercopyshardplacement to recover a broken shard the new shard is created without the default privileges defined in the schema In my particular case I am also working with a partitioned table As updates to the parent table are coming in during the mastercopyshardplacement operation the new shard immediately fails with a permission error I can work around the issue by locking the parent table doing the copy operation and manually setting the correct permissions I am running Citusdata and postgresql We currently use workerhashpartitiontable workerfetchpartitionfile and workermergefilesintotable to perform repartition jobs However these functions are not aware of distributed transactions may leave files behind and have various inefficiencies We would like to use workerpartitionqueryresult fetchintermediateresults and readintermediateresult as used in This is dependent on removing tasktracker since intermediate result functions require being in a distributed transactions Once we made this conversion we can use CTEs and recursively planned subqueries within repartition joins Marcos test case runs into with this PR But thats addressed in TODO write a test case The connection pointer is NULL warning is the result of FinishRemoteTransactionCommit calling HandleResultTransactionResultError Maybe connectionSetChanged true isnt enough to prevent the connection from being reused spoiler we need to check pgConn nullness elsewhere in code Fixes 