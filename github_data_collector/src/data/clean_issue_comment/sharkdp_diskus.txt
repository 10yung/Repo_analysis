Maybe I were doing it wrong The computed directory is my clippy build console usrbindu sch G G total diskus GB bytes hyperfine diskus usrbindu sch Benchmark diskus Time mean ms ms User s System s Range min max ms ms runs Benchmark usrbindu sch Time mean ms ms User ms System ms Range min max ms ms runs Summary usrbindu sch ran times faster than diskus Meta diskus b e cf but with cargo update As I suggested on a commit This is a rudimentary implementation and just outputs the total number of files found as well It includes directories in the count as well and is very naive in its reporting in that sense We leverage diskus for reporting and this metric is helpful for us Havent submitted many PRs before so let me know if you need other things added This should fix I basically removed rayon and handled the threads manually using crossbeam I also took the liberty to make the diskus binary use the diskus library instead of just moding walkrs With a sufficiently large directory the threads can overflow their stacks because walk is recursive I encountered a similar issue when rewriting the du implementation in uutilscoreutils so I decided to test diskus and found the same problem One of the most common usecases for du hs by far for me and also what dust caters to is to do du hs to find the largest directories in a given directory usually the current one It doesnt seem like diskus can currently operate in that mode Itd be awesome if that kind of tell me whats large quickndirty mode could be supported somehow