My pytorch version is cpu and tensorflow version is dev torchserializationpyline incheckseekable NoneType object has no attribute seek You can only torchload from a file that is seekablePlease preload the data into a buffer like ioBytesIO and try to load from it instead But how should I do to sovle the quetions Another question OSErrorUnable to load weights from pytorch checkpoint fileIf you tried to load a PyTorch model from a TF checkpointplease set fromtftrue Questions Help A clear and concise description of the question Hello Im wondering what is the structure of the model saved after finetuning For example after the sequence classification finetuning how to show the layers information of newlyformed model Is new models sentence vector different from the original one which is extracted from pretrained model Questions Help A clear and concise description of the question How to start a server and client to get feature vectors or Which part of the code should I study in Im trying to run TFBertForTokenClassification with tensorflowdatasetsloadgluesst import tensorflow as tf import tensorflowdatasets from transformers import tokenizer BertTokenizerfrompretrainedbertbaseuncased model TFBertForTokenClassificationfrompretrainedbertbaseuncased data tensorflowdatasetsloadgluesst traindataset glueconvertexamplestofeaturesdata train tokenizer maxlength tasksst validdataset glueconvertexamplestofeaturesdata validation tokenizer maxlength tasksst traindataset traindatasetshuffle batch repeat validdataset validdatasetbatch optimizer tfkerasoptimizersAdamlearningrate e epsilon e clipnorm loss tfkeraslossesSparseCategoricalCrossentropyfromlogitsTrue metric tfkerasmetricsSparseCategoricalAccuracyaccuracy modelcompileoptimizeroptimizer lossloss metrics metric history modelfittraindataset epochs stepsperepoch validationdatavaliddataset validationsteps while modelfit I get this error Train for steps validate for steps Epoch WARNINGtensorflowGradients do not exist for variables tfbertfortokenclassification bertpoolerdensekernel tfbertfortokenclassification bertpoolerdensebias when minimizing the loss WARNINGtensorflowGradients do not exist for variables tfbertfortokenclassification bertpoolerdensekernel tfbertfortokenclassification bertpoolerdensebias when minimizing the loss WARNINGtensorflowGradients do not exist for variables tfbertfortokenclassification bertpoolerdensekernel tfbertfortokenclassification bertpoolerdensebias when minimizing the loss WARNINGtensorflowGradients do not exist for variables tfbertfortokenclassification bertpoolerdensekernel tfbertfortokenclassification bertpoolerdensebias when minimizing the loss ETA InvalidArgumentError Traceback most recent call last ipythoninput f b b in module history modelfittraindataset epochs stepsperepoch validationdatavaliddataset validationsteps frames usrlocallibpython distpackagessixpy in raisefromvalue fromvalue InvalidArgumentError root errors found Invalid argument assertion failed Condition x y did not hold elementwise x lossoutput lossSparseSoftmaxCrossEntropyWithLogitsShape y lossoutput lossSparseSoftmaxCrossEntropyWithLogitsstridedslice node lossoutput lossSparseSoftmaxCrossEntropyWithLogitsassertequal AssertAssert defined at ipythoninput f b b Reshape Invalid argument assertion failed Condition x y did not hold elementwise x lossoutput lossSparseSoftmaxCrossEntropyWithLogitsShape y lossoutput lossSparseSoftmaxCrossEntropyWithLogitsstridedslice node lossoutput lossSparseSoftmaxCrossEntropyWithLogitsassertequal AssertAssert defined at ipythoninput f b b successful operations derived errors ignored Opinferencedistributedfunction Function call stack distributedfunction distributedfunction Bug Important information Model I am using Bert Language I am using the model on English The problem arise when using x the official example scripts give details I have a venv running with TF and transformers and I am running mrpc dataset with BERT Heres the code traindataset glueconvertexamplestofeaturesdata train tokenizer maxlength taskmrpc validdataset glueconvertexamplestofeaturesdata validation tokenizer maxlength taskmrpc traindataset traindatasetshuffle batch repeat validdataset validdatasetbatch The tasks I am working on is an official GLUESQUaD task give the name To Reproduce Steps to reproduce the behavior I am using the official code on hugging face for BERT and sequence calssification and it is not working tokenizer BertTokenizerfrompretrainedbertbasecased model TFBertForSequenceClassificationfrompretrainedbertbasecased forcedownloadTrue data tensorflowdatasetsloadgluemrpc If you have a code sample error messages stack traces please provide it here as well Error message File hugfbertpy line in module traindataset glueconvertexamplestofeaturesdata train tokenizer maxlength taskmrpc File Usersns knDocumentsinsighttransformerssrctransformersdataprocessorsgluepy line in glueconvertexamplestofeatures loggerinfoWriting example dd exindex lenexamples TypeError object of type OptionsDataset has no len Expected behavior A clear and concise description of what you expected to happen Environment OS mac Python version Tensorflow version PyTorch Transformers version or branch Using GPU Distributed or parallel setup Any other relevant information Additional context Add any other context about the problem here Just an option to save the model to other than the working directory Default functionality hasnt changed Bug Important information Model I am using Bert XLNet GPT Language I am using the model on English Chinese English The problem arise when using tokenizer GPT TokenizerFastfrompretrainedgpt The tasks I am working on is my own task or dataset To Reproduce Steps to reproduce the behavior tokenizer GPT TokenizerFastfrompretrainedgpt If you have a code sample error messages stack traces please provide it here as well ipythoninput b a cde fe in module tokenizer GPT TokenizerFastfrompretrainedgpt mediabahramNew VolumeProjectsPythonLMtransformerstokenizationutilspy in frompretrainedcls inputs kwargs return clsfrompretrainedinputs kwargs classmethod mediabahramNew VolumeProjectsPythonLMtransformerstokenizationutilspy in frompretrainedcls pretrainedmodelnameorpath initinputs kwargs Instantiate tokenizer try tokenizer clsinitinputs initkwargs except OSError raise OSError mediabahramNew VolumeProjectsPythonLMtransformerstokenizationgpt py in initself vocabfile mergesfile unktoken bostoken eostoken padtomaxlength addprefixspace maxlength stride truncationstrategy kwargs AttributeError Tokenizer object has no attribute withpretokenizer Expected behavior A clear and concise description of what you expected to happen AttributeError Tokenizer object has no attribute withpretokenizer Environment OS Ubuntu Python version PyTorch version PyTorch Transformers version or branch Using GPU yes Distributed or parallel setup No Any other relevant information Additional context Add any other context about the problem here Questions Help A clear and concise description of the question I put the wikitrainraw and the wikitestraw in dataset then run the command python runlmfinetuningpy outputdiroutput modeltyperoberta modelnameorpathrobertabase dotrain traindatafiledatasetwikitrainraw doeval evaldatafiledatasetwikitestraw mlm errors Traceback most recent call last File runlmfinetuningpy line in module main File runlmfinetuningpy line in main traindataset loadandcacheexamplesargs tokenizer evaluateFalse File runlmfinetuningpy line in loadandcacheexamples blocksizeargsblocksize File runlmfinetuningpy line in init assert ospathisfilefilepath AssertionError then I find the code in runlmfinetuningpy I dont know change filepath in def init def initself tokenizer args filepathtrain blocksize I hope you can help me out This is an explanation and a proposed fix for The code set padtokenlabelid and increase the total number of labels numlabels lenlabels but made no change to the label list Thus the first label in label list has the same index as padtokenlabelid Following instructions in README take GermEval as an example for one sentence in test dataset the token Aachen is labeled as BLOC BLOC is the first label in label list yet because of the collision with padtokenlabelid both pad tokens and Aachen are encoded as And the testpredictionstxt is also off by one IPERpart bis IPERpart IPERpart wurde IPERpart The fix adds a placeholder label PAD at position when loading the datasets and all labels positions are shifted by The resulting encoding for the same sample sentence And the testpredictionstxt thus has correct index O bis O O wurde O 