 Hello Is it possible to enable streaming mode that will process changes to the output automatically whenever there are changes available in kafka without running spark submit in interval mode Thank you spark in spark standalone cluster running on EC s no k s java metorikku v amazonlinux job logs attached from separate spark submit runs both same command as below you can see first run wrote a None partition while second run wrote proper date partition Run number onwards also worked well like run homeec usersparkhomebinsparksubmit master sparkdomain conf sparkdriverextraJavaOptionsDinputwildcard Ddatestr Dhourstr Dmonthstr conf sparksqlparquetwriteLegacyFormattrue conf sparkexecutorextraJavaOptionsDinputwildcard Ddatestr Dhourstr Dmonthstr executorcores executormemory g drivermemory g name teslacatalystrawcsvteslaukteslaukrawINGEST class comyotpometorikkuMetorikku verbose deploymode cluster s abuckmetorikkujar config homeec userjsonconfigssijobteslacatalystmetorikkuinputrawcsvteslaukteslaukrawinputyaml I implemented a ElasticsearchInstrumentation which will send metrics to Elasticsearch However ElasticsearchClient is always closed before sending metrics The cause is from following code line at Jobscala val instrumentationClient instrumentationFactorycreate sparkContextaddSparkListenernew SparkListener override def onJobEndtaskEnd SparkListenerJobEnd Unit instrumentationClientclose A spark application usually creates several jobs each operation action will create a job And instrumentationClient will be closed multiple times when these jobs finished I think the instrumentationClient can be closed only when the application ends Hi Spark dataframe does only support append or overwrite data I implement a simple JdbcUpsertWriter to upsert data in relation database It will check whether the dataframe row exists in database If the row existsgenerates an UPDATE statement and if row does not exists creates an INSERT statement output dir xy partition by colTicker new source data for colTicker can appear everyday sometimes new values sometimes existing values example day had files under below folder structure colTickerAMZN colTickerMSFT lets say rows in the file day had file colTickerGOOG day had files colTickerMSFT lets say rows in the file colTickerIBM colTickerLYFT i want to replace data so not storing days versionsduplicates for same colTicker but keep data for all distinct colTicker partitions so my expected output data after each day day output data under outputdir colTickerAMZN colTickerMSFT day output data under outputdir colTickerAMZN existing data should be untouched was loaded in day only never removed colTickerMSFT colTickerGOOG day output data under outputdir colTickerAMZN existing data should be untouched was loaded in day only never removed colTickerMSFT previous should be overwritten colTickerGOOG existing data should be untouched was loaded in day only never removed colTickerIBM colTickerLYFT issue is if i try savemodeappend then after day MSFT shows rows if i try savemodeoverwrite then after day AMZNGOOG data is gone 