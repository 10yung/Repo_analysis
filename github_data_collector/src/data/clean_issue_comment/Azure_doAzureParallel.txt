Fix timeout error message in waitForTasksToComplete For a job with tasks from a time point all the remaining tasks will fail with the following errors Error in savelist namesGlobalEnv file outfile version version error writing to connection Calls quit syssaveimage saveimage save Execution halted This error happens randomly to me so I cannot give a minimal reproducible example Sometimes If I submit the exactly same job again all the tasks can be successfully completed So I believe it is possibly due to problems of Azure I am not familiar with Azure therefore I am not sure what info I should provide here Please let me know if any additional info is helpful Thanks Proposed fix for When adding a task the creation of the upload token for the standard job files like the file txt containing the log of task ignores if the user maybe has set a longer expiry time for their output files In my case I have long running tasks and changed the following line from the documentation example to R make token expire days after creation writeToken storageClientgenerateSasTokenw c outputFolder end Systime So the SAS token for my output files now has the se parameter in the URI like this se T A A Z while in the URI for txt it looks like this se T A A Z so my own upload gets ignored because the task crashes with a FileUploadAccessDenied Error for the txt before my upload is run I am not sure if this is definied here or here If you could point me in the right direction I could try a fix Is there a reason this line references a release from September On the same topic the run command seems outdated as well For comparison see Hello I am able to run the sample getting started code but when I attempt to load a package from CRAN my nodes fail to start Can anyone show me what I might be doing wrong Here is my cluster file This is the same file from getting started but Ive included the hypervolume package name hv vmSize StandardD v maxTasksPerNode poolSize dedicatedNodes min max lowPriorityNodes min max autoscaleFormula QUEUE containerImage rockertidyverselatest rPackages cran hypervolume github bioconductor commandLine subnetId I run the following librarydoAzureParallel setCredentialscredentialsjson cluster makeClusterclusterjson But all my nodes fail to boot Here is an example In showNodesFailurenodesWithFailures The following nodes failed while running the start task tvm t zp This package has some other dependencies including rgeos which can be finicky to install Ive tried specifying just rgeos by itself but that also fails Is there a better way to install packages Thank you I am experiencing the known issue with autoscale and github package installation where the error message is Error HTTP error API rate limit exceeded for But heres the good news Authenticated requests get a higher rate limit Check out the documentation for more details Rate limit remaining Rate limit reset at UTC To increase your GitHub API rate limit Use usethisbrowsegithubpat to create a Personal Access Token Use usethiseditrenviron and add the token as GITHUBPAT Execution halted However I have set the githubAuthenticationToken in the credentialsjson file Is the environmental variable not yet set when the github install occurs with the packages are specified in the clusterjson file Possibly relevant I am using a custom docker image but I want to install the packages from git as I am iterating on package implementation I am not sure how to make a reproducible example but it occurs when scaling up from to nodes Here is my clusterjson in case it helps to reproduce name psmirnov vmSize StandardD v maxTasksPerNode poolSize dedicatedNodes min max lowPriorityNodes min max autoscaleFormula QUEUE containerImage bhklabpharmacogxv rPackages cran MASS tictoc mvtnorm abind polynom memoise purrr matrixStats github bhklabmCI bhklabfastCI bioconductor commandLine subnetId The sharedKeys object is not necessary to select the storageAccount object Also the storageAccountName is already an object in this script but was referred as a string Before submitting a bug please check the following Start a new R session Check your credentials file Install the latest doAzureParallel package Submit a minimal reproducible example run sessionInfo Description Hello This is my first time writing an issue at github as a first time user of Azure batch cluster As a beginner I started with trying the given example in this tutorial I have just copy pasted exactly as it is It seems everything running fine All the nodes are up and running The Progress section shows But When it comes to Merging then I see following error Progress Running Queued Completed Failed Tasks have completed Merging resultsAn error has occurred in the merge task of the job job Error handling is set to stop and has proceeded to terminate the job The user will have to handle deleting the job If this is not the correct behavior change the errorhandling property to pass or remove in the foreach object Use the getJobFile function to obtain the logs For more information about getting job logs follow this link in efunobj substituteex parentframe edata object results not found After going through some other issues in this page I stopped merging by using enableCloudCombine FALSE option Then I get following error Error in efunobj substituteex parentframe edata object results not found Any help is highly appreciated Thanks Moeen Description If you do not set up an Azure storage container for every foreach loop you get the cryptic error message Error in base txt FALSE mode decoding from base failed It happened to me but after google it I only found this previous closed issue It is a stupid mistake but it should have a clear error message An improvement could be that the storage account information should be checked when the clusters it is spin up