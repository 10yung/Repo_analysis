Using the python integration of fastBPE could fasten up the code because bootstraping in unix implies a fork everytime you are trying to use the cpp only implementation This slows down the execution depending on the used ram When I run embedsh with any language code other than en the following warning comes up I am able to get the embeddings but I doubt whether the correct tokenization happened Is there any fix for this perl warning Setting locale failed perl warning Please check that your locale settings LANGUAGE unset LCALL unset LCCTYPE UTF LANG enUSUTF are supported and installed on your system Token method silently failed when shell command failed This PR will raise a python error to propagate the shell error I am trying to embed some strings but facing an issue When passing a list of string containing different language like French the imput and output counts are not matching Like if I pass French strings I get output of shape In some cases the output increases It works fine for English but emoticons or different language characters are resulting in this issue Any help from embed import SentenceEncoder EncodeLoad EncodeFile from textprocessing import Token BPEfastApply SplitLines JoinEmbed from indexing import IndexCreate def linestoindexlang str lines List modelpath str bpecodepath str usecpu bool False batchsize int Suitable for small amounts of data with tempfileTemporaryDirectory as tmpdirname target strPathtmpdirname source with opentarget w as fout foutwrite njoinlines return textfilepipeline lang target modelpath bpecodepath usecpu returnsindex batchsizebatchsize def textfilepipelinelang str inputpath str modelpath str bpecodepath str usecpu bool batchsize int returnsindex Suitable for small amounts of data encoder SentenceEncoder modelpath maxsentencesbatchsize maxtokens cpuusecpu with tempfileTemporaryDirectory as tmpdirname tmpdir Pathtmpdirname Token inputpath strtmpdir token langlang romanizeFalse lowercaseTrue gzipFalse verboseTrue BPEfastApply strtmpdir token strtmpdir bpe bpecodepath verboseTrue overwriteTrue EncodeFile encoder strtmpdir bpe strtmpdir enc verboseTrue overwriteTrue if returns embeddings return npfromfilestrtmpdir enc dtypenpfloat count data index IndexCreate strtmpdir enc FlatL verboseTrue saveindexFalse return data index When not using the unify option there still will be an index of unique items built in TextLoadUnify that is based on unique lines This messes up at least the score option of scoring sentence pairs So this for line in fin newind lensent ind indsappendsent indsetdefaultline newind if argsunify if inds newind sentsappendline nu else sentsappendline nu should be changed to for line in fin if argsunify newind lensent ind indsappendsent indsetdefaultline newind if inds newind sentsappendline nu else sentsappendline indsappend nu nu hi we know that Laser has used different tokenizers for languages like English Chinese Japanese if one sentence has multiple languages eg a mix of English and Chinese in one sentence Nike or multilingual batch I like Nike how to use a mix of tokenizer in these cases Reduce the latency of scoring through the api by not instanciating the encoder every time a call is made to the endpoint I found that in CPU mode the speed of the generation of embedding is about sentences per hour Is that slow or normal Hello I have the training data with labels in English Now I want to use this data to predict for other languages I saw XLM and LASER both support the crosslingual classification However they dont have the benchmark on the same dataset therefore its difficult to know which model is better Does someone help me in determining whichXLM or LASER is better for crosslingual classification Since Token builds the shell command as a string it may be vulnerable to some shell injection attacks The Python documentation explicitly warns against this 