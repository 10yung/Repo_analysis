Thanks a bunch for this package I sometimes have a hyperparameter value that is a Nonetype but TBX errors when it is passed in I was wondering what the right way to handle this is Maybe in summarypy this case should be handled as a typeDataTypeDATATYPEUNSET In from tensorboardX import summary In summaryhparams Out function tensorboardXsummaryhparamshparamdictNone metricdictNone In summaryhparamsa None score NotImplementedError Traceback most recent call last ipythoninput b in module summaryhparamsa None score miniconda libpython sitepackagestensorboardXsummarypy in hparamshparamdict metricdict if not isinstancev int or not isinstancev float v makenpv ssihparams k numbervalue v miniconda libpython sitepackagestensorboardXx numpy in makenpx return checknanpreparemxnetx raise NotImplementedError Got but expected numpy array or torch tensorformattypex NotImplementedError Got class NoneType but expected numpy array or torch tensor Trying to get the PR AUC from tensorboardx seems like we only have PR so far implement the methods equivalent to tfmetricsauc Id like to annotate on logs especially what epoch a log belongs to Iterationlevel or each nthiteration logging makes it hard to know what epoch a specific data point is If I do epochlevel logging it leads to a low number of observations and I have to wait quite long for information An obvious solution would be to provide an additional parameter or add tuple support for globalstep Is there a reason why this is not possible yet This issue is related to but I dont think the questions was properly answered I am experiencing crashesfreezing in my keras code The code freezes without an error it just stops Code i am using selfwriter SummaryWriterTensy selfwriteraddscalarsSoftmax Action predictedaction Action predictedaction Action predictedaction selfgstep Ive calculated that it is around the flush time that it crashes wheter its the first flush round or later in the run nonetheless it crashes every run making my run proccess unreliable to do a long run If it succeeds to save then there is nothing wrong with the file itself Are there any known issues of tensorboardx freezing code I didnt want to report this as a bug because it might just me making some mistake Hope someone can help Describe the bug For function addembedding when the input label is using utf Chinese characters it can export tensorboard log file However when you check the projector using tensorboard logdir runs it crashed to load data raising exception txt UnicodeDecodeError utf codec cant decode byte xb in position invalid start byte Minimal runnable code to reproduce the behavior coding utf from tensorboardX import SummaryWriter writer SummaryWriter writeraddembeddingnprandomrandom writerclose Note that are Chinese characters Expected behavior The exported tensorboard log file uses utf encode so that tensorboard can show Chinese or other nonEnglish words Environment protobuf tensorboardX torch Python environment Anaconda py Bug A clear and concise description of what the bug is To Reproduce Steps to reproduce the behavior Run my script below python import torch import torchnn as nn import torchnnfunctional as F from torchutilstensorboard import SummaryWriter from tensorboardX import SummaryWriter bug bool type inputs class Net nnModule def initself dropout superNet selfinit selffc nnLinear selffc nnLinear selfdropout nnDropoutdropout def forwardself x usedropoutFalse x Freluselffc x if usedropout x selfdropoutx or other operations x Freluselffc x return x with SummaryWriterbugs as w net Net inputx torchrandn waddgraphnet inputx True bug None type inputs might be arguments default value class Net nnModule def initself superNet selfinit selffc nnLinear selffc nnLinear selffc nnLinear selffc nnLinear def forwardself x yNone zNone x Freluselffc x if y is not None y Freluselffc y x x y if z is not None z Freluselffc z x x z x Freluselffc x return x with SummaryWriterbugs as w net Net inputx torchrandn inputy None inputz torchrandn waddgraphnet inputx inputy inputz bug List type inputs dict or other python buildin types like intstr may also meet this question class Net nnModule def initself superNet selfinit selffclist nnLinear for in range selffcn nnLinear def forwardself x indexlistNone if index is not None for i in index x Freluselffclist i x x Freluselffcnx return x with SummaryWriterbugs as w net Net inputx torchrandn index waddgraphnet inputx index and you can see the tracetake bug as an example Error occurs No graph saved Traceback most recent call last File input line in module File ApplicationsPyCharmappContentshelperspydevpydevbundlepydevumdpy line in runfile pydevimportsexecfilefilename globalvars localvars execute the script File ApplicationsPyCharmappContentshelperspydevpydevimpspydevexecfilepy line in execfile execcompilecontents n file exec glob loc File UserswangyuanzhengDownloadsxxxxxxxprojectalbertpytorchdevaddgraphbugpy line in module waddgraphnet inputx True File Userswangyuanzhenganaconda envsCCFBigDatatorchlibpython sitepackagestorchutilstensorboardwriterpy line in addgraph selfgetfilewriteraddgraphgraphmodel inputtomodel verbose File Userswangyuanzhenganaconda envsCCFBigDatatorchlibpython sitepackagestorchutilstensorboardpytorchgraphpy line in graph raise e File Userswangyuanzhenganaconda envsCCFBigDatatorchlibpython sitepackagestorchutilstensorboardpytorchgraphpy line in graph trace torchjittracemodel args File Userswangyuanzhenganaconda envsCCFBigDatatorchlibpython sitepackagestorchjitinitpy line in trace checktolerance forceoutplace moduleclass File Userswangyuanzhenganaconda envsCCFBigDatatorchlibpython sitepackagestorchjitinitpy line in tracemodule moduleccreatemethodfromtracemethodname func exampleinputs varlookupfn forceoutplace RuntimeError Type Tuple Tensor bool cannot be traced Only Tensors and possibly nested Lists Dicts and Tuples of Tensors can be traced toTraceableIValue at torchcsrcjitpybindutilsh frame c ErrorErrorc SourceLocation std basicstringchar std chartraitschar std allocatorchar const x c e in libc dylib frame torchjittoTraceableIValuepybind handle x in libtorchpythondylib frame torchjittoTypedStackpybind tuple const x e edf in libtorchpythondylib frame void pybind cppfunctioninitializetorchjitscriptinitJitScriptBindingsobject void torchjitscriptModule std basicstringchar std chartraitschar std allocatorchar const pybind function pybind tuple pybind function bool pybind name pybind ismethod pybind siblingtorchjitscriptinitJitScriptBindingsobject void torchjitscriptModule std basicstringchar std chartraitschar std allocatorchar const pybind function pybind tuple pybind function bool pybind name const pybind ismethod const pybind sibling const lambdapybind detailfunctioncall invokepybind detailfunctioncall x e e in libtorchpythondylib frame pybind cppfunctiondispatcherobject object object x fe d c in libtorchpythondylib omitting python frames If you have a code sample error messages stack traces please provide it here as well Expected behavior A clear and concise description of what you expected to happen writeraddgraph should run normally Environment Please copy and paste the output from our environment collection script or fill out the checklist below manually You can get the script and run it with wget For security purposes please check the contents of collectenvpy before running it python collectenvpy Collecting environment information PyTorch version Is debug build No CUDA used to build PyTorch None OS Mac OSX GCC version Could not collect CMake version Could not collect Python version Is CUDA available No CUDA runtime version No CUDA GPU models and configuration No CUDA Nvidia driver version No CUDA cuDNN version No CUDA Versions of relevant libraries pip numpy pip torch pip torchvision conda torch pypi pypi conda torchvision pypi pypi Additional context Add any other context about the problem here TensorboardXSummaryWriteraddgraph has the same bug as torchutilstensorboardSummaryWriteraddgraph Besides this bug I hope addgraph could accept not only a tuple as positional arguments but also a dict as keyword arguments for the modelforwards input addhparam creates a subdirectory with name timetime In tensorboard this shows up as an individual run Is this intended behavior I think it would be much cleaner to put it in the same events file or if that is not possible at least in the same directory of the actual run Having addscalars as different runs makes sense I guess Different behavior could be implemented using Custom Scalars But for HParams I do not see why it should be a separate run When I projecting an embedding with different labels for example python writeraddembeddingsameembedding labelsstrtwo tagflabelsstrtwo writeraddembeddingsameembedding labelsstrone taglabelsstrone I got two different pictures just like these two pictures So why relatively distances between points are different when projecting one embedding with different labels Hi there I am not sure if you mentioned it somewhere but I was thinking about adding information about hyperparameters to my summaries Problem is that I already have training runs and I would just like to reopen those files and add the information to them I can write a script for that but is it possible to open and add information to an existing summary Thank you Christian