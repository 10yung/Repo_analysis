 pytorch google Hi I am trying to reproduce the red graph provided in the paper which shows the Average PSNR vs epochs for the Set psnrvsepoch I am using the Keras implementation provided here it includes Residual Learning and Batch Normalization and Adam optimization as well and I am training epochs However instead of a stable graph like the one in the paper I get a graph with pronounced drops similar to the blue and black graphs in the paper Furthermore even in the last epochs where the network should be converging the Average PSNR often shows significant drops which are related to the presence of odd spots in some of the denoised images dncnn test dncnn I have experimented with different settings in particular with different BN momentum and the situation becomes better when it is set to a high value as Even so the graph I get is never something similar to the red one in the paper I am afraid these spots in the denoised images appear quite randomly with different shapes and locations for different trainings Has anyone experienced the same or something similar I would appreciate any help thank you Hi Thanks for release the code I have read your code Its brilliant but I found a small problem in your pytorch code when get patches you data augment have some problems about cv imresize you can check you codes in DnCNNmasterTrainingCodesdncnnpytorchdatageneratorpy line The parameters used for batch normalization are not specified in the original paper I dont know how batch norm works in Matlab I have tried to read the codes but its very difficult to me so I tried looking in keras The parameters used seemed very odd in addition the batch normalisation is involved in a code mess so I looked in pytorch and saw that they are different In keras the momentum is or depending on where you look and in pytorch the momentum is Note that this differs from the original unofficial keras implementation loss is inf When training in pytorch with SGD loss is inf how to do with it Hi in your paper u stated that MSE is used to optimized the network However in your code it is using Sum Square Error It that a mistake or there is specific reason for that Mind to share some experience On line we have loss criterionmodelbatchy batchx batchy clean noise modelbatchy x output of the network If so shouldnt the loss function be loss criterionmodelbatchynoise In the paper I see you proposed a DnCNN model which can either denoise a picture or perform super resolution on a picture However if I want to denoise a picutre and perform super resolution on it at the same time is this model capable of this task I trained a model by myself but the result is very poor if the input is both noisy and bicubic magnified Could you offer me some advice on it I wanted to know at which epoch did your loss started to converge