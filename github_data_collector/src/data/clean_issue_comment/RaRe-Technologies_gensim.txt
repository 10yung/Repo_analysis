importing direct from keras raises error AttributeError module tensorflow has no attribute getdefaultgraph as recommended by the tf community we should import keras as tensorflowkeras Problem description Update an existing LDA model with an incremental approach We create a LDA model for a collection of documents on demand basis We save the resulting model file on the cloud When a new LDA request arrives with fresh data I need a way to incrementally update the model live training wit the these data Typically I would use ldaupdate But what happens when the ldaupdate takes as input a corpus that includes the same documents of the previous model Assumed to have model trained on corpus and a new corpus which is the best approach to do the incremental training of the new model against corpus I have seen a ldadiff function So one could train model and then run mdiff annotation model diffmodel then check diff and annotation and decide to promote model Does it make sense Which is the best criterion to promote the new model then Thank you in advance Stepscodecorpus to reproduce python from smartopen import open load the existing LDA model currentmodel LdaModelloadopens modelpath load the corpus data TextCorpus opens corpuspath corpussentences datagettexts dictionary Dictionarycorpussentences corpus dictionarydoc bowtext for text in corpussentences update current model on the corpus currentmodelupdatecorpus Versions Linux genericx withUbuntu bionic Python default Nov GCC NumPy SciPy gensim FASTVERSION Hi I use the gensim wrapper LdaMallet link to run MALLET Gensim library provide a parameter workers to assign the numthreads argument in MALLET Ref Gensim Code line But I found the workers seems not working here is the different setting and running time workers run time sec workers run time min s workers run time min s workers run time min s No matter I run this on my computer openjdk version OpenJDK Runtime Environment build u b ubuntu b OpenJDK Bit Server VM build b mixed mode or on the Colab openjdk version OpenJDK Runtime Environment build postUbuntu ubuntu OpenJDK Bit Server VM build postUbuntu ubuntu mixed mode sharing the results are similar more workers spent more time and I have also tried mallet mallet Dose it means I am not using a proper way to run MALLET LDA in parallel Thanks reference code code in gensim python i tried with different workers workers gensimmodelswrappersLdaMalletmalletpath corpuscorpus numtopicsnumtopics id wordid word optimizeinterval iterations workersworkers the equivalent commands in mallet key in shell ignore the IO setting binmallet traintopics numthreads Im trying to run naivebayespy from the following file but i get the error mentioned in the title i have tried uninstalling gensim and reinstalling it again using both pip and conda but nothing seems to worki also have mingw installed and have added bin to path as the warning suggests but it doesnt work eitherIm using windows bit Screenshot Our implementation of FastText training errorbackpropagation does some fishy things that deviate from the FB reference implementation For example at we simply shortcircuit skip to the next loop when an exponent is out of the desired range The same approach appears in Word Vec and Doc Vec cython code as well However the seeminglyanalogous code in Facebooks FastText instead clips the values to in these cases allowing backprop to proceed See Our deviation from Facebooks codes practice is suspicious on both correctness consistency grounds This simple continue does however match the behavior we copied longago from word vecc Other perhapsmore superficial changes are that FBs code makes its lookuptables slots long instead of but allows exponents to instead of Again our FT implementation seems to have copied our copyofword vecc choices instead of the reference FB implementation choices If anything it could make more sense to update the word vecderived code with these newer choices as they at least plausibly represent practices improved by experience Problem description Hey everyone I encountered an issue when loading a pretrained facebook FastText models Loading a GB pretrained model blows up to more than GB in RAM on my machine when loading with Gensim So my computer keeps swapping memory like crazy and never loads the model It would be awesome if we could lower the memory footprint in Gensims FastText loading mechanism Is this a known problem and is anyone aware how to fix it Stepscodecorpus to reproduce Download a pretrained FastText model eg ccen bin from Try to load the model using loadfacebookmodelccen bin Versions Please provide the output of python Darwin x i bit Python packaged by condaforge default Dec Clang tagsRELEASE final NumPy SciPy gensim My pc is cpu with cuda GPUbut when i use ldacorpus modelsldamulticoreLdaMulticorecorpustraincorpus numtopicsnumtopics id worddictionary alpha eta minimumprobability iterations workers using the code output nothing and mechine seems get dead cycle but there are nothing output when i use the below code ldacorpus modelsLdaModeltraincorpus numtopicsnumtopics id worddictionary alpha eta minimumprobability updateevery iterationsiterations got work why not use multicore when in linuxubuntu any help is thanks very much Problem description Im trying to use lemmatize function to my text but getting StopIteration exception Stepscodecorpus to reproduce from gensimutils import lemmatize s lemmatizeeight prints Result python lempy Traceback most recent call last File usrlocallibpython sitepackagespatterntextinitpy line in read raise StopIteration StopIteration The above exception was the direct cause of the following exception Traceback most recent call last File lempy line in module s lemmatizeeight File usrlocallibpython sitepackagesgensimutilspy line in lemmatize parsed parsecontent lemmataTrue collapseFalse File usrlocallibpython sitepackagespatterntexteninitpy line in parse return parserparses args kwargs File usrlocallibpython sitepackagespatterntextinitpy line in parse s i selffindtagss i kwargs File usrlocallibpython sitepackagespatterntexteninitpy line in findtags return Parserfindtagsself tokens kwargs File usrlocallibpython sitepackagespatterntextinitpy line in findtags lexicon kwargsgetlexicon selflexicon or File usrlocallibpython sitepackagespatterntextinitpy line in len return selflazylen File usrlocallibpython sitepackagespatterntextinitpy line in lazy selfload File usrlocallibpython sitepackagespatterntextinitpy line in load dictupdateself xsplit for x in readselfpath if lenxsplit File usrlocallibpython sitepackagespatterntextinitpy line in genexpr dictupdateself xsplit for x in readselfpath if lenxsplit RuntimeError generator raised StopIteration Versions Im using MacOS Python import platform printplatformplatform Darwin x i bit import sys printPython sysversion Python default Sep Clang clang import numpy printNumPy numpyversion NumPy import scipy printSciPy scipyversion SciPy import gensim printgensim gensimversion from gensimmodels import word vecprintFASTVERSION word vecFASTVERSION gensim from gensimmodels import word vecprintFASTVERSION word vecFASTVERSION FASTVERSION pip freeze grep pattern pattern pip freeze grep gensim gensim PR to test changes to build recipes adjusting Python otherlibrary versions See discussion Replaces I recommend the next major gensim release x also drop official Python testingsupport Rationale its close to full endoflife under months to currently receiving only security fixes major Pythonusing cloud services including Heroku AWS Lambda Google CloudColab etc already only support Python some useful features like the variable annotations needed for dataclasses only arrive in for most users moving forward to x isnt hard those absolutely stuck on can use an earlier gensim version or try their luck in an unsupported configuration some modules might still work in Python Meanwhile Python has been out a few months with support at Heroku AWS Lambda and deserves added buildtest support So we could replace the buildstest in our CI setup with a new target 