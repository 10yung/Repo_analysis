just as above In file mmdetcorebboxsamplerohemsamplepy at line topklossinds losstopknumexpected when I do not change just add ohemsample in config it error out like list RuntimeError invalid argument k not in range for dimension so i debug in it and search for the func topk i modify it like this topklossinds losstopknumexpected dim it runs well so i just confuseand make the issue thanks best wish Hi all After a discussion with hellock and yhcao we decide to encapsulate box and mask heads into RoI head to reduce the codebases complexity This refactoring has several advantages as follows The refactoring reduces most of the duplicate code in the detectors For example when adding new methods like Double head RCNN and Cascade RCNN the previous implementation always need to create a new file in the detectors directory and copy the same code for RPN Since most of these modifications are about the RCNN process it seems to be better to encapsulate the second stage including box head mask head mask scoring head and potentially key point head etc into the RoI heads to reduce the duplicate codes and logics The refactoring only creates box assigners and samplers once in RoI heads The previous implementation creates assigner and sampler during each iteration in the training process this is unnecessary since they are the same during the training process Now the assigner and sampler are created once as a submodule of RoI heads to reduce the unnecessary overhead The refactoring will ease the processes when code refactoring and adding new methods as it reduces duplicate code and logic and seems to be more conceptually simple than before Notice The structure of the detectors are changed as all configs of twostage detectors are changed Thus it might have compatibility issues with the previous model zoo We will rebenchmark all the models in the configs when all the refactorings are finished which will be released in early February Problems description Hello When I wanted to do the training with the following code sudo docker run rm it v pwdpwd w pwd name cascademask cascademask python toolstrainpy configsmaskrcnnr fpn xpy a RuntimeError happened as below RuntimeError DataLoader worker pid is killed by signal Bus error It is possible that dataloaders workers are out of shared memory Please try to raise your shared memory limit However as I checked my server it still had much plenty memory as the project was under training I am confused why this error happened with free memory I am very grateful for any help The details of my nvidiasmi is Traceback INFO Distributed training False INFO MMDetection Version rc e ebb INFO Config model settings model dict typeMaskRCNN pretrainedtorchvisionresnet backbonedict typeResNet depth numstages outindices frozenstages stylepytorch neckdict typeFPN inchannels outchannels numouts rpnheaddict typeRPNHead inchannels featchannels anchorscales anchorratios anchorstrides targetmeans targetstds lossclsdict typeCrossEntropyLoss usesigmoidTrue lossweight lossbboxdicttypeSmoothL Loss beta lossweight bboxroiextractordict typeSingleRoIExtractor roilayerdicttypeRoIAlign outsize samplenum outchannels featmapstrides bboxheaddict typeSharedFCBBoxHead numfcs inchannels fcoutchannels roifeatsize numclasses targetmeans targetstds regclassagnosticFalse lossclsdict typeCrossEntropyLoss usesigmoidFalse lossweight lossbboxdicttypeSmoothL Loss beta lossweight maskroiextractordict typeSingleRoIExtractor roilayerdicttypeRoIAlign outsize samplenum outchannels featmapstrides maskheaddict typeFCNMaskHead numconvs inchannels convoutchannels numclasses lossmaskdict typeCrossEntropyLoss usemaskTrue lossweight model training and testing settings traincfg dict rpndict assignerdict typeMaxIoUAssigner posiouthr negiouthr minposiou ignoreiofthr samplerdict typeRandomSampler num posfraction negposub addgtasproposalsFalse allowedborder posweight debugFalse rpnproposaldict nmsacrosslevelsFalse nmspre nmspost maxnum nmsthr minbboxsize rcnndict assignerdict typeMaxIoUAssigner posiouthr negiouthr minposiou ignoreiofthr samplerdict typeRandomSampler num posfraction negposub addgtasproposalsTrue masksize posweight debugFalse testcfg dict rpndict nmsacrosslevelsFalse nmspre nmspost maxnum nmsthr minbboxsize rcnndict scorethr nmsdicttypenms iouthr maxperimg maskthrbinary dataset settings datasettype CocoDataset dataroot datacoco imgnormcfg dict mean std torgbTrue trainpipeline dicttypeLoadImageFromFile dicttypeLoadAnnotations withbboxTrue withmaskTrue dicttypeResize imgscale keepratioTrue dicttypeRandomFlip flipratio dicttypeNormalize imgnormcfg dicttypePad sizedivisor dicttypeDefaultFormatBundle dicttypeCollect keys img gtbboxes gtlabels gtmasks testpipeline dicttypeLoadImageFromFile dict typeMultiScaleFlipAug imgscale flipFalse transforms dicttypeResize keepratioTrue dicttypeRandomFlip dicttypeNormalize imgnormcfg dicttypePad sizedivisor dicttypeImageToTensor keys img dicttypeCollect keys img data dict imgspergpu workerspergpu traindict typedatasettype annfiledataroot annotationsinstancestrain json imgprefixdataroot train pipelinetrainpipeline valdict typedatasettype annfiledataroot annotationsinstancesval json imgprefixdataroot val pipelinetestpipeline testdict typedatasettype annfiledataroot annotationsinstancesval json imgprefixdataroot val pipelinetestpipeline optimizer optimizer dicttypeSGD lr momentum weightdecay optimizerconfig dictgradclipdictmaxnorm normtype learning policy lrconfig dict policystep warmuplinear warmupiters warmupratio step checkpointconfig dictinterval yapfdisable logconfig dict interval hooks dicttypeTextLoggerHook dicttypeTensorboardLoggerHook yapfenable evaluation dictinterval runtime settings totalepochs distparams dictbackendnccl loglevel INFO workdir workdirsmaskrcnnr fpn x loadfrom None resumefrom None workflow train INFO load model from torchvisionresnet Downloading to rootcachetorchcheckpointsresnet c e pth M M MBs WARNING The model and loaded state dict do not match exactly unexpected key in source statedict fcweight fcbias loading annotations into memory Done t s creating index index created INFO Start running host root a fad d workdir homeChutianXu instancecascademaskworkdirsmaskrcnnr fpn x INFO workflow train max epochs ERROR Unexpected bus error encountered in worker This might be caused by insufficient shared memory shm Traceback most recent call last File toolstrainpy line in module main File toolstrainpy line in main timestamptimestamp File mmdetectionmmdetapistrainpy line in traindetector timestamptimestamp File mmdetectionmmdetapistrainpy line in nondisttrain runnerrundataloaders cfgworkflow cfgtotalepochs File optcondalibpython sitepackagesmmcvrunnerrunnerpy line in run epochrunnerdataloaders i kwargs File optcondalibpython sitepackagesmmcvrunnerrunnerpy line in train selfmodel databatch trainmodeTrue kwargs File mmdetectionmmdetapistrainpy line in batchprocessor losses modeldata File optcondalibpython sitepackagestorchnnmodulesmodulepy line in call result selfforwardinput kwargs File optcondalibpython sitepackagestorchnnparalleldataparallelpy line in forward return selfmoduleinputs kwargs File optcondalibpython sitepackagestorchnnmodulesmodulepy line in call result selfforwardinput kwargs File mmdetectionmmdetcorefp decoratorspy line in newfunc return oldfuncargs kwargs File mmdetectionmmdetmodelsdetectorsbasepy line in forward return selfforwardtrainimg imgmeta kwargs File mmdetectionmmdetmodelsdetectorstwostagepy line in forwardtrain x selfextractfeatimg File mmdetectionmmdetmodelsdetectorstwostagepy line in extractfeat x selfbackboneimg File optcondalibpython sitepackagestorchnnmodulesmodulepy line in call result selfforwardinput kwargs File mmdetectionmmdetmodelsbackbonesresnetpy line in forward x selfconv x File optcondalibpython sitepackagestorchnnmodulesmodulepy line in call result selfforwardinput kwargs File optcondalibpython sitepackagestorchnnmodulesconvpy line in forward return selfconv dforwardinput selfweight File optcondalibpython sitepackagestorchnnmodulesconvpy line in conv dforward selfpadding selfdilation selfgroups File optcondalibpython sitepackagestorchutilsdatautilssignalhandlingpy line in handler errorifanyworkerfails RuntimeError DataLoader worker pid is killed by signal Bus error It is possible that dataloaders workers are out of shared memory Please try to raise your shared memory limit here is my inference script import mmcv import os import numpy as np from tqdm import tqdm import json import cv def showresultresult bboxresult result bboxes npvstackbboxresult labels npfullbboxshape i dtypenpint for i bbox in enumeratebboxresult labels npconcatenatelabels return bboxeslabels configfile datalzyinternmmdetectionconfigsmyconfigimageanncascadercnnx x dfpn xpy checkpointfile datalzyinternmmdetectionmodelscascadercnnx x dfpn xpth model initdetectorconfigfile checkpointfile device cuda image mmcvimreaddatalzyinterndatasetcocotrain jpg result inferencedetectormodelimage bboxlabel showresultresult printlenbbox printlenlabel the checkpoint model is download from and the config file is the deafult version of mmdetection but why is output of lenbbox and lenlabel is Is the inference is not work Added random classmethods to SamplingResult and AssignResult These allow for the simple creation of demo data that can be use for testing debugging and potentially more although I dont think these are as useful as random boxes might be Fix issue in SamplingResultinit Ive been hitting issues where sometimes gtbboxes looks like its had its dimensions squashed This causes an error when doing the index selection Adding two checks fixes it Add rng as attribute of RandomSampler To ensure that the new random methods could be seeded to consistently reproduce an issue if needed I had to ensure that the RandomSampler class could be seeded I used ensurerng which I added in a previous PR to accomplish this simplify Misc I also added a to function to SamplingResult where it moves the data onto a different device I wrote this to test something that turned out not to be an issue but I figure its a nice API to have so I left it in I also added info properties to SamplingResult and RandomSampler as well as implementing the nice debugging string for SamplingResult I did end up using my ubelt library to accomplish some of these tasks The core fix and the random clasmethods dont rely on ubelt so if there is any resistance to adding a dependency I can remove the features that depend on ubelt and still have a functionally useful PR However Ive been maintaining and keeping ubelt stable for years it has test coverage it itself is fairly small and its own dependencies are minimal I set the model savedir google drive dir when using google colaboratoryBut I met os errorWhyHow to solve that bugI always lost model weight due to the very short time keeping the connection Hi guys I want to know how to get the batch size of the model in MMDetection Because I did not see it in a config file Any answer would be appreciated As the GETTINGSTARTEDmdtrainwithmultiplegpus says we can launch single machine multigpu training using toolsdisttrainsh and the training goes well I have two questions In single machine multigpu training does the argument gpus in toolstrainpy become useless I try command CUDAVISIBLEDEVICES PORT bash toolsdisttrainsh configsmsrcnnmsrcnnr caffefpn xpy validate gpus nvidiasmi shows gpus are running I try command CUDAVISIBLEDEVICES PORT bash toolsdisttrainsh configsmsrcnnmsrcnnr caffefpn xpy validate gpus nvidiasmi shows gpus are running Is toolsdisttrainsh the only way for single machine multigpu training I try command python toolstrainpy configsmsrcnnmsrcnnr caffefpn xpy gpus for multigpu training but encounter error like this Traceback most recent call last File toolstrainpy line in module main File toolstrainpy line in main timestamptimestamp File localmmdetectionmmdetapistrainpy line in traindetector timestamptimestamp File localmmdetectionmmdetapistrainpy line in nondisttrain runnerrundataloaders cfgworkflow cfgtotalepochs File usrlocallibpython distpackagesmmcvrunnerrunnerpy line in run epochrunnerdataloaders i kwargs File usrlocallibpython distpackagesmmcvrunnerrunnerpy line in train selfmodel databatch trainmodeTrue kwargs File localmmdetectionmmdetapistrainpy line in batchprocessor losses modeldata File usrlocallibpython distpackagestorchnnmodulesmodulepy line in call result selfforwardinput kwargs File usrlocallibpython distpackagestorchnnparalleldataparallelpy line in forward return selfgatheroutputs selfoutputdevice File usrlocallibpython distpackagestorchnnparalleldataparallelpy line in gather return gatheroutputs outputdevice dimselfdim File usrlocallibpython distpackagestorchnnparallelscattergatherpy line in gather res gathermapoutputs File usrlocallibpython distpackagestorchnnparallelscattergatherpy line in gathermap for k in out File usrlocallibpython distpackagestorchnnparallelscattergatherpy line in genexpr for k in out File usrlocallibpython distpackagestorchnnparallelscattergatherpy line in gathermap return Gatherapplytargetdevice dim outputs File usrlocallibpython distpackagestorchnnparallelfunctionspy line in forward ctxinputsizes tuplemaplambda i isizectxdim inputs File usrlocallibpython distpackagestorchnnparallelfunctionspy line in lambda ctxinputsizes tuplemaplambda i isizectxdim inputs IndexError dimension specified as but tensor has no dimensions I noticed that there is a function getpointssingle in fcosheadpy that maps the positions on multilevel feature maps to the points on the image plane I was wondering if there is an existing inverse function that can map the point on the image plane to the position on the feature map Thanks