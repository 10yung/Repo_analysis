This adds the libaio option priopercent which allows a certain percentage of commands to be sent at a higher priority Since this only is relevant to reads on the drive side only reads will be broken down into multiple priorities like this The latencies of the two priority types will be reported separately in addition to the combined latencies This will only affect normal and json reporting Appveyor and Travis tests having been failing with t b aae bafio Expected result fio runs and completes the job Buggy result fio segfaults test ioenginenull size g iosubmitmodeoffload iodepth Previously this only happened on Windows but there have been recent failures on other platforms Failures occurring on multiple platforms suggests that there is a genuine problem Windows failures both x and x macOS failures Linux failure It need use env to run the program in different linux distributions In Debian like distributions which bash binbash In RHEL like distributions which bash usrbinbash revert commit cd a Signedoffby Changcheng Liu changchengliualiyuncom As the title suggests when there is a difference between the indicated and observedrequested The storage subsystems configs are A x TB NvME Gen rulers in a PCIe bus and an Intel CPU PCIe engineering A x TB NvME Gen rulers in a PCIe bus A x TB SSDs From our xperf traces on our filesystem we have observed that in we are not the bottleneck so we are suspecting that FIO might not be submitting the appropriate requests When using DiskSpd and CFStest we have no problem sustaining and equivalent load For example fio thread direct ioenginewindowsaio timebased ramptime runtime directoryv FIODirectory size GB rwwrite iodepth bs K nrfiles name xXR numjobs fallocatetruncate unlink IO depths submit complete fio thread direct ioenginewindowsaio timebased ramptime runtime directoryv FIODirectory size GB rwwrite iodepth bs K nrfiles name xXR numjobs fallocatetruncate unlink IO depths submit complete fio thread direct ioenginewindowsaio timebased ramptime runtime directoryv FIODirectory size GB rwwrite iodepth bs K nrfiles name xXR numjobs fallocatetruncate unlink IO depths submit complete None of these specs yield a volumefs queue depth of anything close to queued IOs a minute average of sec samples yields a queued IOs a minute average of sec samples yields a queued IOs a minute average of sec samples yields a queued IOs The same is true symptom is valid for iodepth in all storage subsystems we only test up to iodepth for SSDs responding to Sitsofes questions How much of a CPU is being taken up by fio alone Is there any spare CPU is negligible around Does xperf also confirm a max outstanding depth of only IOs Xperf and controller firmware confirm the observations with regards to queued depths recieved and latencies not being an issue If memory serves DiskSpd also sets the watermark if needs be it will write the whole file once if it cant just mark all unwritten data as valid Do things change if you use a perexisting file thats been entirely written at least once fio thread direct ioenginewindowsaio directoryv size GB rwwrite iodepth bs K nrfiles name xXR numjobs fallocatetruncate groupreporting overwrite or running the same test twice IO depths submit complete As for overwrite this is one of the reasons why I suggested the set valid data not only to fix the very slow first allocations but because data in place writes behave differently than first allocations The overwrite command is painfully slow especially when testing with large TB and multiple files s it would take an unreasonable time to set up the tests As the script is compatible both with python and python then it makes sense do not hardcode to python and use the system pytohn version There are plenty of containers for fio but there are old and cant really trust them fio is really good to test volume accesses from DockerKubernetes so having an official and trustable Docker image would be really great Would you mind or anyone else to build that image This one doesnt do all that much clever eg I assume gettimeofday will be fast and accurate enough to time multiple clockgettime calls and it introduces a conditional into the clock path Even so it seems to give numbers similar to using the cpu clock and I think the cost in accuracy is made up for by the lower overhead What do you think Jens I am trying to write files in a job and then read them in another job So I have set filenameformat to match and this works with fewer than files per job But when nrfiles the read job unlinks one file To see this more clearly I have set allowfilecreate In the below output notice the output of ls at the end its missing one file I cant think of any reason for this so I guess its a bug This is fio version rootfreenas fio namewritefile rwwrite bs M size G endfsync filenameformatoctafile jobnum filenum nrfiles writefile g rwwrite bsR KiB KiB W KiB KiB T KiB KiB ioenginepsync iodepth fio Starting process writefile Laying out IO files files total MiB Jobs f F r KiBsw KiBs r w IOPS eta m s writefile groupid jobs err pid Thu Oct write IOPS BW MiBs MBs MiB msec clat usec min max avg stdev lat usec min max avg stdev clat percentiles usec th th th th th th th th th th th th th th th th th lat usec lat msec cpu usr sys ctx majf minf IO depths submit complete issued rwts total short dropped latency target window percentile depth Run status group all jobs WRITE bw MiBs MBs MiBs MiBs MBs MBs io MiB MB run msec rootfreenas ls octa file file file file file file file file file file rootfreenas fio namereadfile rwread bs M size G filenameformatoctafile jobnum filenum allowfilecreate nrfiles debugfile fio set debug option file file add file octafile file resize file array to files file file x a d octafile added at file add file octafile file file x a octafile added at file add file octafile file file x a octafile added at file add file octafile file file x a octafile added at file add file octafile file file x a d octafile added at file add file octafile file file x a octafile added at file add file octafile file file x a b octafile added at file add file octafile file file x a d octafile added at file add file octafile file file x a ed octafile added at file add file octafile file file x a octafile added at readfile g rwread bsR KiB KiB W KiB KiB T KiB KiB ioenginepsync iodepth fio Starting process file setup files file get file size for x a d octafile file get file size for x a octafile file get file size for x a octafile file get file size for x a octafile file get file size for x a d octafile file get file size for x a octafile file get file size for x a b octafile file get file size for x a d octafile file get file size for x a ed octafile file get file size for x a octafile readfile Laying out IO files files total MiB file layout unlink octafile file open file octafile flags fio file creation disallowed by allowfilecreate Run status group all jobs rootfreenas ls octa file file file file file file file file file When I use a fixediops workload with rateiops I see IOPS in the IOPS log but do not see it from the histogram log This is reproducible In output below column is the number of IO operations that are recorded in the histogram log for each time interval this column is obtained by summing the counters in the histogram bucket for that record Spot checks show that it is correct But it does not come out to IOPS only about half that See files in attached zip file inconsistentiopshistzip benglandbenelaptop inconsistentiopshist fiohistologpctilespy xclathist log fio version bucket groups bucket bits time quantum sec percentiles buckets per group buckets per interval output unit usec time millisec percentiles in increasing order with values in usec msecsincestart samples min median max 