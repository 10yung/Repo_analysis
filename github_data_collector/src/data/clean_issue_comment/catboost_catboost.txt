Problem Training slows down proportionally to the number of GPUs I use Eg on device the first iterations take sec with devices about seconds an with devices about seconds When I switch to CPU its roughly x slower So the fastest way was using a single GPU Is it a bug Dataset dense features BERT embeddings vectors k samples catboost version Operating System Debian GNULinux CPU vCPU GPU x V Problem Today we can load the model by passing its path to the loadmodel method Catboost version Operating System Ubuntu CPU IntelR CoreTM i U CPU GHz Language Python Hi Nowadays its not very practical to load models from paths only Especially when you work with cloudbased infrastructure where you can have your model stored in S GSWhatelse buckets Maybe something like loadblobmodel could improve the situation or maybe make loadmodel capable of working with file instances instead of paths What do think about such features Many thanks The problem Im looking to emit prediction intervals for each predicted value the mean in regression I need that these intervals cover say of true values and be as narrow as possible In other words I want to learn and emit variance or noise which cant be explained by features of the model in each region of data each sample would have different intervals determined by input vector For example Predicting income by number of education years Given there is no additional data we have I would expect lower variance of income for lower education and higher variance for higher education Another example predicting how much years left to leave by age and health data of a person Young would have larger variance while old lower and old and unhealthy even more lower There are two main methods Im aware of to do it nonparametric methods using Quantile Loss where trying to get of coverage we can train three models Quantilealpha for the median and alpha and for the lower and higher bounds quantiles respectfully parametric method suppose some distribution of the noise and learn its parameters For example to get coverage suppose Normal distribution and train models for mean and stdev Then calculate the interval using stdev bayesian methods essentially parametric as well but the modeling is done using probabilistic methods The question As mentioned I want the interval be as narrow as possible but still satisfy the needed coverage which means learning separate models for quantiles or parameters for parametric methods wouldnt provide optimal solution in terms of coverage and width Im looking for the loss function which can optimize two things simultaneously Is there something builtin already in some library and if not what would be the simplest way to implement it Both parametric and non parametric methods are accepted Thanks in advance Alexander Example The data in example is simulated independent variables stage categorical and age axis X axis Y is the predicted value The bounds and mean created by separate quantile models But real data is much more complex so separate models approach not creating nice results Problem getfeatureimportance with fstrtypeShapValues has poor performances catboost version Operating System Ubuntu CPU IntelR CoreTM i U CPU GHz model type Regression model format cbm model size mb Greetings First of all thanks a lot for such a great library Im facing some issues with explanation of predicted values While the prediction works blazingly fast the explanation of predictions through shapvalues shows poor performances In order to understand my issue here are few figures Prediction time sec Shap values time sec Here is a portion of my code python features MODELgetfeatureimportancedatapool fstrtypeShapValues shapvalues basevalue npsplitfeatures My primary goal is to explain the prediction for a given item at the same time when prediction is made Ideally I would like to launch everything inside of a GCP cloudfunction Many thanks Problem Hi it is straightforward to verify that the C api gives different prediction results to the python api when the CatboostRegressor model is trained in python on any m float features and n categorical features saved in cpp format and executed This is not the case with n categorical features Can you please investigate params boostingtypeOrdered numboostround lossfunctionRMSE evalmetricRMSE model CatBoostRegressorparams modelfitPoolXlabely catfeatures AB modelsavemodelfilename formatcpp ModelCalcerWrapper calcermodelcbm stdvectorfloat floatFeatures stdvectorstdstring catFeatures a b stdcout calcerCalcfloatFeatures catFeatures stdendl catboost version Operating System Unix Problem When fitting a catboostclassifier model i get with evaluation set xtestytest I get max accuracy i have also set accuracy as evalmetric However when I use modelpredict at xtest afterwards I find that of samples are misclassified Am I missing something ps no categorical features in this test catboost versioncatboost Operating Systemmac os img width altcatboostmismatch src Thanks in advance CPU GPU Hello Does anybody knows how really works Dum Models and the different strategies At documentation there is no especific information and no sources of how it works Thanks Problem Testing binary classification using python and c tester programs The results are different Is it normal catboost version v Operating System Ubuntu CPU IntelR CoreTM Duo CPU T GHz GPU Hello Ive just set up a simple example in which CatBoost behaves in a strange way The training set consists in data points and the model should learn this simple function A B C The model gives the correct output for A according to training examples and D handling an unseen level I get approximately the average label on the training set However for B and C the model fails to learn the simple function from catboost import CatBoostRegressor Initialize data traindata A B C trainlabels Initialize CatBoostRegressor model CatBoostRegressorcatfeatures Fit model modelfittraindata trainlabels verboseFalse print fformatmodelpredict A print fformatmodelpredict B print fformatmodelpredict C print fformatmodelpredict D Output Expected output catboost In the code below when using getfeatureimportance to get ShapValues and the expected value I am getting a strangely different behavior from what is described in this tutorial notebook The mentioned above code import catboost from catboost import CatBoostClassifier Pool from catboostdatasets import titanic from sklearnmodelselection import traintestsplit import numpy as np traindf testdf catboostdatasetstitanic traindffillna inplaceTrue testdffillna inplaceTrue X traindfdropSurvived axis y traindfSurvived catix npwhereXdtypes npfloat Xtrain Xval ytrain yval traintestsplitX y randomstate Xtest testdf params iterations learningrate evalmetric Accuracy randomseed logginglevel Silent trainpool PoolXtrain ytrain catfeaturescatix validatepool PoolXval yval catfeaturescatix model CatBoostClassifierparams modelfittrainpool evalsetvalidatepool printmodelpredicttrainpool predictiontypeRawFormulaValmeanround prints shapvalues modelgetfeatureimportancetrainpool typeShapValues expectedvalue shapvalues printexpectedvalueround prints In the code above we obtain different values given by modelpredict and by shapvalues whereas if you follow the shapvaluestutorialipynb you would obtain same value shapvalues modelgetfeatureimportancePoolX y typeShapValues expectedvalue shapvalues printexpectedvalueround modelpredictXmeanround prints After some exploration I have found the workaround which gives the expected value for the first code with titanic dataset shapvaluessumaxis meanround If possible could you please explain this inconsistency and what causes it Thank you for a great tool that you provided the community with catboost version Operating System Ubuntu CPU Intel Core i CPU 