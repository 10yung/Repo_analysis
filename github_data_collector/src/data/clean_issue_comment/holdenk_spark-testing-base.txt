holdenk taking up again The full outer join does not care about ordering or partitioning and works on distributed larger datasets I also included an optional skip of the schema equality One common use case was to load input and expected data from a csv and compare the transformed input to expected output when loading a csv Spark automatically makes all fields nullable regardless of any schema the output of the tested functionality could have nonnullable fields eg groupbycount results in a nonnullable count column For unit tests its often just interesting whether or not the data matches not if spark inferred the schema correctly Ive noticed that some extra folders like sparkuuid in SystemgetPropertyjavaiotmpdir are created But some of them are not deleted Problem is somewhere here it returns true file is not deleted I read a bit about javaioFiledelete seems that file cannot be deleted by two general reasons permissions and busying by some process So since some of them are deleted looks like permissions is not the reason Its really annoying to collect megabytes in temp directory Spark version scala version Any thoughts here Build is failing due to warn UNRESOLVED DEPENDENCIES warn warn orgapachesparksparkstreamingkafka not found warn warn warn Note Unresolved dependencies path warn orgapachesparksparkstreamingkafka UsersamoralessparktestingbasebuildsbtL warn comholdenkarauroot error sbtlibrarymanagementResolveException unresolved dependency orgapachesparksparkstreamingkafka not found error at sbtinternallibrarymanagementIvyActionsresolveAndRetrieveIvyActionsscala error at sbtinternallibrarymanagementIvyActionsanonfunupdateEither IvyActionsscala error at sbtinternallibrarymanagementIvySbtModuleanonfunwithModule Ivyscala error at sbtinternallibrarymanagementIvySbtanonfunwithIvy Ivyscala error at sbtinternallibrarymanagementIvySbtsbtinternallibrarymanagementIvySbtaction Ivyscala error at sbtinternallibrarymanagementIvySbtanon callIvyscala error at xsbtbootLocksGlobalLockwithChannel Locksscala error at xsbtbootLocksGlobalLockxsbtbootLocksGlobalLockwithChannelRetries Locksscala error at xsbtbootLocksGlobalLockanonfunwithFileLock applyLocksscala error at xsbtbootUsingwithResourceUsingscala error at xsbtbootUsingapplyUsingscala error at xsbtbootLocksGlobalLockignoringDeadlockAvoidedLocksscala error at xsbtbootLocksGlobalLockwithLockLocksscala error at xsbtbootLocksapply Locksscala error at xsbtbootLocksapplyLocksscala error at sbtinternallibrarymanagementIvySbtwithDefaultLoggerIvyscala error at sbtinternallibrarymanagementIvySbtwithIvyIvyscala error at sbtinternallibrarymanagementIvySbtwithIvyIvyscala error at sbtinternallibrarymanagementIvySbtModulewithModuleIvyscala error at sbtinternallibrarymanagementIvyActionsupdateEitherIvyActionsscala error at sbtlibrarymanagementivyIvyDependencyResolutionupdateIvyDependencyResolutionscala error at sbtlibrarymanagementDependencyResolutionupdateDependencyResolutionscala error at sbtinternalLibraryManagementresolve LibraryManagementscala error at sbtinternalLibraryManagementanonfuncachedUpdate LibraryManagementscala error at sbtutilTrackedanonfunlastOutput Trackedscala error at sbtinternalLibraryManagementanonfuncachedUpdate LibraryManagementscala error at scalautilcontrolExceptionCatchapplyExceptionscala error at sbtinternalLibraryManagementanonfuncachedUpdate LibraryManagementscala error at sbtinternalLibraryManagementanonfuncachedUpdate adaptedLibraryManagementscala error at sbtutilTrackedanonfuninputChanged Trackedscala error at sbtinternalLibraryManagementcachedUpdateLibraryManagementscala error at sbtClasspathsanonfunupdateTask Defaultsscala error at scalaFunction anonfuncompose Function scala error at sbtinternalutiltildegreateranonfunu TypeFunctionsscala error at sbtstdTransformanon workSystemscala error at sbtExecuteanonfunsubmit Executescala error at sbtinternalutilErrorHandlingwideConvertErrorHandlingscala error at sbtExecuteworkExecutescala error at sbtExecuteanonfunsubmit Executescala error at sbtConcurrentRestrictionsanon anonfunsubmitValid ConcurrentRestrictionsscala error at sbtCompletionServiceanon callCompletionServicescala error at javautilconcurrentFutureTaskrunFutureTaskjava error at javautilconcurrentExecutorsRunnableAdaptercallExecutorsjava error at javautilconcurrentFutureTaskrunFutureTaskjava error at javautilconcurrentThreadPoolExecutorrunWorkerThreadPoolExecutorjava error at javautilconcurrentThreadPoolExecutorWorkerrunThreadPoolExecutorjava error at javalangThreadrunThreadjava error update sbtlibrarymanagementResolveException unresolved dependency orgapachesparksparkstreamingkafka not found I noticed that DataFrameGenerator always returns a nonnull value even for columns which are nullable Is this the desired behavior If not I would write a PR to change this upon clicking it I see this WARN ServletHandler Error for apiv applications javalangNoClassDefFoundError orgglassfishjerseyinternalinjectAbstractBinder and nothing shows up in the tab view Spark Hello everyone Im working on a project in Azure Cloud and HDInsight cluster In this project Im working with very specific storages Because of this I would like to while my driver and executor are on my local computer I want them to write on this storages by default I have already set up my own SparkSession and SparkContext trait with this configurations and it work Now I would like to integrate your library for being able to easily test my dataframes I have seen this issue for work arounds about the HiveSupport It worked Now when Im integrating Azure configurations Im getting NPE on Hive Database Creation I would like to know if you have any idea about a solution Now enough talking here is my code Trait for setup my Azure configuration Codes scala trait AzureTU Azure storage account for storing test blob storage protected val storageAccountName String storageAccountName Blob storage name protected val storageBlobName String blobStorageName Complete URI for test blob storage protected val blobStorageURI String swasbsstorageBlobNamestorageAccountNameblobcorewindowsnet Storage account key for creating blob storage protected val storageAccountKey String myAccountKey Azure Data Lake Store name protected val adlStoragename String myDataLakeName Azure Data Lake Store full adress protected val adlStorageURI String sadladlStoragenameazuredatalakestorenet Returning map for hadoop azure settings protected def azureHadoopSettings Map String String printlnReturning Hadoop Azure MapsfsazureaccountkeystorageAccountNameblobcorewindowsnet storageAccountKey fsazure orgapachehadoopfsazureNativeAzureFileSystem fswasbsimpl orgapachehadoopfsazureNativeAzureFileSystem fsdefaultFS blobStorageURI fsadloauth accesstokenprovidertype ClientCredential fsadloauth refreshurl s fsadloauth clientid myClientId fsadloauth credential myCredential dfsadlsoauth accesstokenprovider orgapachehadoopfsadlsoauth ConfCredentialBasedAccessTokenProvider fsadlimpl orgapachehadoopfsadlAdlFileSystem fsAbstractFileSystemadlimpl orgapachehadoopfsadlAdl Sources Blob storage Databricks Datalake Databricks My personnal trait for my SparkSession Codes scala import comsncfssgcdcbibigdatatrvslogsCdCLogs import comsncfssgcdcbibigdatatrvstestspropsAzureTU import orgapachesparksqlSparkSession import orgapachesparkSparkConf SparkContext This trait is here for instantiate all UnitTests classes trait SparkLocalConf extends AzureTU with CdCLogs Configuration list for local configuration see sparksqlsourcespartitionOverwriteMode val confs Map String String Map sparksqlsourcespartitionOverwriteMode dynamic this needs to be used to reproduce the hive dynamic partitions system for more details sparksqlwarehousedir sblobStorageURItmpsparkwarehouse azureHadoopSettings def conf SparkConf new SparkConfsetAllconfs SparkSession object to reuse in every classes Some configurations are set up here implicit lazy val spark SparkSession SparkSession builder appNameappName mastermaster configconf enableHiveSupport getOrCreate SparkContext object to reuse in every classes implicit lazy val sc SparkContext thissparksparkContext Name of the application for the SparkSession Manager lazy protected val appName String getClassgetName Master for the SparkSession Default to yarn This can be local or local for testing purposes only protected val master String local Tests for execution if needed scala import orgapachelogginglog jscalaLogging import orgscalatestBeforeAndAfterEach FlatSpec Matchers class SparkLocalConfTest extends FlatSpec with BeforeAndAfterEach with Logging with Matchers with SparkLocalConf behavior of SparkLocalConf import sparkimplicits private this val testDb String getClassgetNamesplit last override def beforeEach Unit sparksqlsdrop database if exists testDb cascade sparksqlscreate database if not exists testDb it should be able to use HiveSupport in SparkSession in noException should be thrownBy sparksqlscreate table testDbtest as select as a sparksqlContexttableNamestestDbcontainstest shouldBe true it should create database on blob storage in val expected stheUriOfMyBlobStoragetmpsparkwarehousetestDbtoLowerCasedb val actual sparksqlsdescribe database testDb get info from database wheredatabasedescriptionitem Location keep only line with location information selectdatabasedescriptionvalue select value from location collectmkStringreplaceAll actual shouldBe expected The trait resulting of the issue linked below Codes scala import javaioFile import comholdenkarausparktestingDataFrameSuiteBase import comsncfssgcdcbibigdatatrvstestspropsAzureTU import orgapachesparkSparkConf import orgscalatestBeforeAndAfterAll Suite import orgapachesparksqlinternalStaticSQLConfCATALOGIMPLEMENTATION private spark trait DataFrameSuiteBaseWorkAround extends DataFrameSuiteBase with AzureTU with BeforeAndAfterAll self Suite override def afterAll superafterAll new Filemetastoredbdblckdelete new Filemetastoredbdbexlckdelete Configuration list for local configuration see sparksqlsourcespartitionOverwriteMode val confs Map String String Map sparksqlsourcespartitionOverwriteMode dynamic this needs to be used to reproduce the hive dynamic partitions system for more details sparksqlwarehousedir sblobStorageURItmpsparkwarehouse CATALOGIMPLEMENTATIONkey hive azureHadoopSettings If I dont put Azure Settings this seems to work override def conf SparkConf superconfsetAllconfs Tests for execution if needed scala import comsncfssgcdcbibigdatatrvstestspropsAzureTU import orgscalatest Ignore this tests just here to tests no production code class DataFrameSuiteBaseWorkAroundTest extends FlatSpec with DataFrameSuiteBaseWorkAround with BeforeAndAfterAll with BeforeAndAfterEach with Matchers with AzureTU behavior of SparkLocalConf import sparkimplicits private this val testDb String getClassgetNamesplit last override def beforeEach Unit sparksqlsdrop database if exists testDb cascade sparksqlscreate database if not exists testDb The NPE seems to be here it should be able to use HiveSupport in SparkSession in should work because of workarounds noException should be thrownBy sparksqlscreate table testDbtest as select as a sparksqlContexttableNamestestDbcontainstest shouldBe true it should create database on blob storage in val expected sblobStorageURItmpsparkwarehousetestDbtoLowerCasedb val actual sparksqlsdescribe database testDb get info from database wheredatabasedescriptionitem Location keep only line with location information selectdatabasedescriptionvalue select value from location collectmkStringreplaceAll actual shouldBe expected I really hope you could help me on this point since I wasnt able to find any clue on the solution Of course if you ever need some precisions please let me know Is there an option to turn Spark UI back on To do a big more heavy debugging Added hadoop dependency And look to see what things we were doing to support previous versions and clean them up