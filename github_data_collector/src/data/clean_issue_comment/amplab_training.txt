Hello On page Introduction to Scala command SourcefromFilexxxgetLinestoArray threw an error at me javaniocharsetMalformedInputException Input length It is an easy fix by setting the encoding to UTF Please consider updating the command on the tutorial page Thanks SourcefromFilexxxUTF getLinestoArray Thanks Zilu I followed all the instructions up to step and i already have my key pair on my AWS console as instructed First i ran this sparkec i ampcamppem k ampcamp copy launch amplabtraining I got the following error InvalidKeyPairNotFound Then i changed my region like this sparkec i ampcamppem k ampcamp r uswest copy launch amplabtraining Now i have the following error InvalidAMIIDNotFound Any suggestions Following the steps mentioned on For Java Edited the Tutorialjava file along with twittertxt setting the corresponding credentials However while running the command sbtsbt package run Following error Retrying connect to server ip ec internal Already tried times error runmain javanetConnectException Call to ip ec internal failed on connection exception javanetConnectException Connection refused javanetConnectException Call to ip ec internal failed on connection exception javanetConnectException Connection refused at orgapachehadoopipcClientwrapExceptionClientjava at orgapachehadoopipcClientcallClientjava at orgapachehadoopipcRPCInvokerinvokeRPCjava at comsunproxyProxy getProtocolVersionUnknown Source at orgapachehadoopipcRPCgetProxyRPCjava at orgapachehadoopipcRPCgetProxyRPCjava at orgapachehadoophdfsDFSClientcreateRPCNamenodeDFSClientjava at orgapachehadoophdfsDFSClientinitDFSClientjava at orgapachehadoophdfsDFSClientinitDFSClientjava at orgapachehadoophdfsDistributedFileSysteminitializeDistributedFileSystemjava at orgapachehadoopfsFileSystemcreateFileSystemFileSystemjava at orgapachehadoopfsFileSystemaccess FileSystemjava at orgapachehadoopfsFileSystemCachegetFileSystemjava at orgapachehadoopfsFileSystemgetFileSystemjava at orgapachehadoopfsPathgetFileSystemPathjava at sparkSparkContextsetCheckpointDirSparkContextscala at sparkstreamingStreamingContextcheckpointStreamingContextscala at sparkstreamingapijavaJavaStreamingContextcheckpointJavaStreamingContextscala at TutorialmainTutorialjava at sunreflectNativeMethodAccessorImplinvoke Native Method at sunreflectNativeMethodAccessorImplinvokeNativeMethodAccessorImpljava at sunreflectDelegatingMethodAccessorImplinvokeDelegatingMethodAccessorImpljava at javalangreflectMethodinvokeMethodjava Caused by javanetConnectException Connection refused at sunniochSocketChannelImplcheckConnectNative Method at sunniochSocketChannelImplfinishConnectSocketChannelImpljava at orgapachehadoopnetSocketIOWithTimeoutconnectSocketIOWithTimeoutjava at orgapachehadoopnetNetUtilsconnectNetUtilsjava at orgapachehadoopipcClientConnectionsetupConnectionClientjava at orgapachehadoopipcClientConnectionsetupIOstreamsClientjava at orgapachehadoopipcClientConnectionaccess Clientjava at orgapachehadoopipcClientgetConnectionClientjava at orgapachehadoopipcClientcallClientjava at orgapachehadoopipcRPCInvokerinvokeRPCjava at comsunproxyProxy getProtocolVersionUnknown Source at orgapachehadoopipcRPCgetProxyRPCjava at orgapachehadoopipcRPCgetProxyRPCjava at orgapachehadoophdfsDFSClientcreateRPCNamenodeDFSClientjava at orgapachehadoophdfsDFSClientinitDFSClientjava at orgapachehadoophdfsDFSClientinitDFSClientjava at orgapachehadoophdfsDistributedFileSysteminitializeDistributedFileSystemjava at orgapachehadoopfsFileSystemcreateFileSystemFileSystemjava at orgapachehadoopfsFileSystemaccess FileSystemjava at orgapachehadoopfsFileSystemCachegetFileSystemjava at orgapachehadoopfsFileSystemgetFileSystemjava at orgapachehadoopfsPathgetFileSystemPathjava at sparkSparkContextsetCheckpointDirSparkContextscala at sparkstreamingStreamingContextcheckpointStreamingContextscala at sparkstreamingapijavaJavaStreamingContextcheckpointJavaStreamingContextscala at TutorialmainTutorialjava at sunreflectNativeMethodAccessorImplinvoke Native Method at sunreflectNativeMethodAccessorImplinvokeNativeMethodAccessorImpljava at sunreflectDelegatingMethodAccessorImplinvokeDelegatingMethodAccessorImpljava at javalangreflectMethodinvokeMethodjava javalangRuntimeException Nonzero exit code at scalasyspackageerrorpackagescala Steps followed Access key ID and Secret Access Key generated for AWS Environment variable set Git clone done trainingscripts sparkec i homechaiDownloadstdkeypairvirginapem k tdkeypairvirginio copy launch amplabtraining Errno Not a directory It is unable to find the pem file despite giving it the right directory path for the same Hi I implemented the exercise provided at spark mllib training But in final part it is recommended to implement a matrix factorization to improve the algorithm I could not find any example to do that Is there anyone who can show me a way to handle this Hi I would like to run the training exercises on a Google Compute Engine cluster as I dont have an account on Amazon AWS I was able to copy the wikipedia pagecounts data successfully to Google Compute Engines equivalent of S but I noticed that the data was enhanced to insert the date stamp as the st field in the input files Can you provide me with a pointer to the code that you used to do this or show me where I can copy the modified pagecounts data from I copied the raw data from here Any help you can provide would be much appreciated Running through the exercise code here are some issues I found Data Exploration using Spark SQL page parquetFile has been deprecated and the resulting code should be changed to wikiData sqlCtxreadparquetdatawikiparquet Explore InMemory Data Store Tachyon page the tachyon folder is now a subfolder of spark TACHYONWORKERMEMORYSIZE is already set at GB When I try to format the storage using the command tachyon format class tachyonFormat cannot be found to fix export TACHYONJARSTACHYONHOMElibtachyonassembliesVERSIONjarwithdependenciesjar the command tachyon runTests fails all the tests In the section Run Spark on Tachyon the command binsparkshell is specific to only Scala Should be generalized for users using other languages eg Python Querying compressed RDDs with Succinct Spark page Correct articleIdscount to say articleIdsRDDcount val succinctWikiKV wikiKVmapt t t getBytessuccinctKV is missing an ending parentheses ie Should combine val wikiKV sctextFiledatasuccinctwikilargetxt mapsplit mapt t toLong t into one line val wikiKV sctextFiledatasuccinctwikilargetxtmapsplitmapt t toLong t Change val wikiSuccinctKV scsuccinctKV Long datasuccinctsuccinctwikilarge wikiSuccinctKV count to val succinctWikiKV scsuccinctKV Long datasuccinctsuccinctwikilarge succinctWikiKV count Change val articleIdsRDD succinctWikiKV regexSearchstanfordberkeley edu to val articleIdsRDD succinctWikiKV regexSearchstanfordberkeley edu Hello On the pyspark link seems to be broken on section the javascala one doesnt load or is very slow as well Thanks N The exercise at the end How many articles contain the word california requires you to use the text field of wikiData Unless there was some explanation of the schema I missed earlier its a confusing exercise to do since the text field hasnt been mentioned anywhere It would also suffice to mention some way to explore the schema eg wikiDataschemafields 