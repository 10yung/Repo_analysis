Hi I had the issue with withbias which was resolved using Now it seems there is an issue with transposed pytorchexamplescppdcgandcgancpp error struct torchnnConvOptions has no member named transposed transposedtrue win x libtorch no gpudebug Training on CPU Warning torchnnFeatureDropout module is deprecatedUse Dropout d instead FeatureDropoutImpl at torch csrc api src nn modules dropoutcpp include torchtorchh include cstddef include cstdio include iostream include string include vector Where to find the MNIST dataset const char kDataRoot data The batch size for training const int t kTrainBatchSize The batch size for testing const int t kTestBatchSize The number of epochs to train const int t kNumberOfEpochs After how many batches to log a new update with the loss value const int t kLogInterval struct Net torchnnModule Net conv torchnnConv dOptions kernelsize conv torchnnConv dOptions kernelsize fc fc registermoduleconv conv registermoduleconv conv registermoduleconv drop conv drop registermodulefc fc registermodulefc fc torchTensor forwardtorchTensor x x torchrelutorchmaxpool dconv forwardx x torchrelu torchmaxpool dconv dropforwardconv forwardx x xview x torchrelufc forwardx x torchdropoutx p trainingistraining x fc forwardx return torchlogsoftmaxx dim torchnnConv d conv torchnnConv d conv torchnnFeatureDropout conv drop torchnnLinear fc torchnnLinear fc template typename DataLoader void train sizet epoch Net model torchDevice device DataLoader dataloader torchoptimOptimizer optimizer sizet datasetsize modeltrain sizet batchidx for auto batch dataloader auto data batchdatatodevice targets batchtargettodevice optimizerzerograd auto output modelforwarddata auto loss torchnlllossoutput targets ATASSERTstdisnanlosstemplate itemfloat lossbackward optimizerstep if batchidx kLogInterval stdprintf rTrain Epoch ld ld ld Loss f epoch batchidx batchdatasize datasetsize losstemplate itemfloat template typename DataLoader void test Net model torchDevice device DataLoader dataloader sizet datasetsize torchNoGradGuard nograd modeleval double testloss int t correct for const auto batch dataloader auto data batchdatatodevice targets batchtargettodevice auto output modelforwarddata testloss torchnllloss output targets weight torchReductionSum template itemfloat auto pred outputargmax correct predeqtargetssumtemplate itemint t testloss datasetsize stdprintf nTest set Average loss f Accuracy f n testloss staticcastdoublecorrect datasetsize auto main int torchmanualseed torchDeviceType devicetype if torchcudaisavailable stdcout CUDA available Training on GPU stdendl devicetype torchkCUDA else stdcout Training on CPU stdendl devicetype torchkCPU torchDevice devicedevicetype Net model modeltodevice auto traindataset torchdatadatasetsMNISTkDataRoot maptorchdatatransformsNormalize maptorchdatatransformsStack const sizet traindatasetsize traindatasetsizevalue auto trainloader torchdatamakedataloadertorchdatasamplersSequentialSampler stdmovetraindataset kTrainBatchSize auto testdataset torchdatadatasetsMNIST kDataRoot torchdatadatasetsMNISTModekTest maptorchdatatransformsNormalize maptorchdatatransformsStack const sizet testdatasetsize testdatasetsizevalue auto testloader torchdatamakedataloaderstdmovetestdataset kTestBatchSize torchoptimSGD optimizer modelparameters torchoptimSGDOptions momentum for sizet epoch epoch kNumberOfEpochs epoch trainepoch model device trainloader optimizer traindatasetsize testmodel device testloader testdatasetsize This is pull request was created automatically because we noticed your project was missing a Code of Conduct file Code of Conduct files facilitate respectful and constructive communities by establishing expected behaviors for project contributors This PR was crafted with love by Facebooks Open Source Team This is pull request was created automatically because we noticed your project was missing a Contributing file CONTRIBUTING files explain how a developer can contribute to the project which you should actively encourage This PR was crafted with love by Facebooks Open Source Team In this example the RL model is distributed across one agent and multiple observers Each observer has a replicated submodel of the RL model which all connect to the submodel on the agent During training this example uses distributed autograd to set gradients for all submodels Then it uses RPC calls to collect gradients from all observers to the agent sums those gradients applies the gradient to the local dummy model on agent and then broadcast the model parameters back to the observers to update their models After epochs I got the loss in test is The Acc is also in test My question is the result is right I used the mobilenetv what I want to do is pretrain a new mobilenetv with new architecture Should I run next epochs pytroch python systemubuntu cuda when i run imagenet mainpy in multinodes there is a error likessingle node can run Use GPU for training Use GPU for training creating model resnet creating model resnet idd miscibvwrapcu NCCL WARN Failed to open libibverbsso NCCL version cuda idd miscibvwrapcu NCCL WARN Failed to open libibverbsso idd includesocketh NCCL WARN Connect to failed Connection refused Traceback most recent call last File distrainpy line in module main File distrainpy line in main mpspawnmainworker nprocsngpuspernode argsngpuspernode args File usrlocalanaconda libpython sitepackagestorchmultiprocessingspawnpy line in spawn while not spawncontextjoin File usrlocalanaconda libpython sitepackagestorchmultiprocessingspawnpy line in join raise Exceptionmsg Exception Process terminated with the following error Traceback most recent call last File usrlocalanaconda libpython sitepackagestorchmultiprocessingspawnpy line in wrap fni args File mntsdczhangwgcvimagereviewsrcdistrainpy line in mainworker model torchnnparallelDistributedDataParallelmodel deviceids argsgpu File usrlocalanaconda libpython sitepackagestorchnnparalleldistributedpy line in init selfbroadcastbucketsize File usrlocalanaconda libpython sitepackagestorchnnparalleldistributedpy line in distbroadcastcoalesced distdistbroadcastcoalescedselfprocessgroup tensors buffersize False RuntimeError NCCL error in pytorchtorchlibc dProcessGroupNCCLcpp unhandled system error does somebody konw how to fix it thanks a lot Fixes This script uses some bashspecific features such as function keyword operator FUNCNAME