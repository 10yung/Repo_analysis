 Upstream dependencies which removes redundant MXnet Python code after major release Goal goal goal ale ale ale partially resolve make our current python codebase fully pep compatible provide GitHub hook to check pep compatibility for future commits build the team culture for the pep coding style for our python codebase In order to fully resolve we need to run a linter which can be only done incrementally I will cover this later Methodology No magic but blackI magic I m pretty confident about the tool black and its reformatting result because of Also as a temporary safety measure Black will check that the reformatted code still produces a valid AST that is equivalent to the original This slows it down If youre feeling confident use fast Step by step make source code pep compliant black the python dir first to make sure the test directory is left intact If all tests are passed Im confident to move to the next step All done files reformatted files left unchanged make test code pep I used LibriSpeech to classify the speakers I broke up my data and trained it with CreateML and with classes the outcome is excellent or at least it claims on testing However I would like to make the model updatable and this is something I cant do with CreateML since it uses a GLM at the end So I tried using the TC sound classifier since it has a neural network classifier at the end with updatable dense layers I went through the sample project in the documentation and everything worked fine with TCs sound classifier Then I tried to use my own data the same I used with CreateML and it never learns at all Is there something I might be missing Is CreateMLs approach super different than TCs Ive looked at both models with CoreML tools and see that theyre both pipeline classifiers and that they start with a preprocessing model a feature abstractor and then for TC the neural network and for CreateML the GLM Any recommendations on getting TCs sound classifier to work with my data Theyre sample rate waves with silence removed and then divided into second files Thanks Until gets resolved marked the failing nondeterministic test as xfail The output is not reproducible ie The topics give different words with every run There is no random seed parameter in tctopicmodelcreate Could there be a good reason for this I want the results to be reproducible Thanks Remove documentation building from makewheelsh Create standalone script to make documentation Creaet standalone GitLab job Fixes Fixes the second bullet point of TLDR The perf regression is caused by that the TF graph is initialized adapted for each image input during stylization predict The change will make the TF graph only initialized once during a prediction The way to do it is to define the image H and W as the dynamic dimension value instead of static resolves on the CPU path review suggestion When you review the code please ignore srcpythonturicreatetestteststyletransferpy because it mainly has formatting changes report load the same weights and predict on the same image In printcontentsf contentfeaturename Height Width conclusion compared with from ms ms compared with on single image predict ms ms expected slow down since initializing the graph will take ms batch stylizing images ms numberofimages numberofstylesms the initialization cost is amortized by subsequent calls to the stylize is the estimated data preparation ms and graph initialization ms overhead which is pretty accurate IMO for my machine We can do better to see how much we can reduce the data preparation overhead at the CPP implementation level result since everything runs in the python layer the model state is reserved and no extra initialization is needed In timeit img modelstylizecontentsf contentfeaturename style ms ms per loop mean std dev of runs loop each This profile is pretty accurate for stylize since there is no overhead of neural net architecture initialization result if I run predict on singe image with a single style the overhead of graph construction ms is not amortized Images Processed Elapsed Time Percent Complete ms s ms per loop mean std dev of runs loop each But if we run in a batch manner with data replicated times In printcontentsf contentfeaturename Height Width Height Width Height Width Height Width Height Width Height Width Height Width Height Width Height Width Height Width rows x columns Note Only the head of the SFrame is printed You can use printrowsnumrowsm numcolumnsn to print more rows and columns and then feed it into predict stylization compuation flow In timeit img modelstylizecontentsf contentfeaturename style Images Processed Elapsed Time Percent Complete ms s s s s s s s s s s s s s s s m s min s s per loop mean std dev of runs loop each The performance is averaged to ms for each image input which is approximate There is a unit test failure testcreateemptydataset in OSOD that fails nondeterministically with two different errors On macOS with Python OneObjectDetectorSmokeTesttestcreatewithemptydataset self turicreatetesttestoneshotobjectdetectorOneObjectDetectorSmokeTest testMethodtestcreatewithemptydataset def testcreatewithemptydatasetself with selfassertRaisesToolkitError tconeshotobjectdetectorcreateselftrain targetselftarget testoneshotobjectdetectorpy toolkitsoneshotobjectdetectoroneshotobjectdetectorpy in create data target backgrounds toolkitsoneshotobjectdetectorutilaugmentationpy in previewsynthetictrainingdata backgrounds backgroundsapplylambda im tcimageanalysisresize datastructuressarraypy in apply dryrun fni for i in selfhead if i is not None im backgrounds backgroundsapplylambda im tcimageanalysisresize im intimwidth intimheight imchannels E AttributeError str object has no attribute width toolkitsoneshotobjectdetectorutilaugmentationpy AttributeError On macOS with Python OneObjectDetectorSmokeTesttestcreatewithemptydataset self turicreatetesttestoneshotobjectdetectorOneObjectDetectorSmokeTest testMethodtestcreatewithemptydataset def testcreatewithemptydatasetself with selfassertRaisesToolkitError tconeshotobjectdetectorcreateselftrain targetselftarget testoneshotobjectdetectorpy toolkitsoneshotobjectdetectoroneshotobjectdetectorpy in create data target backgrounds toolkitsoneshotobjectdetectorutilaugmentationpy in previewsynthetictrainingdata backgrounds backgroundsapplylambda im tcimageanalysisresize datastructuressarraypy in apply dryrun fni for i in selfhead if i is not None datastructuressarraypy in listcomp dryrun fni for i in selfhead if i is not None datastructuressarraypy in generator ret selfproxyiteratorgetnextelemsatatime cysarraypyx in turicreatecythoncysarrayUnitySArrayProxyiteratorgetnext cysarraypyx in turicreatecythoncysarrayUnitySArrayProxyiteratorgetnext cyflexibletypepyx in turicreatecythoncyflexibletypepylistfromflexlist cyflexibletypepyx in turicreatecythoncyflexibletypepyobjectfromflexibletype cycpputilspxd in turicreatecythoncycpputilscpptostr E UnicodeDecodeError utf codec cant decode byte xca in position invalid continuation byte cycpputilspyx UnicodeDecodeError Repro steps Run the Python unit test This unit test should be rewritten or removed Failure example JSONTesttestarbitraryjson self turicreatetesttestjsonJSONTest testMethodtestarbitraryjson hypothesissettingsderandomizeTrue suppresshealthcheck hypothesisHealthChecktooslow hypothesisgivenhypothesisjson def testarbitraryjsonself jsonobj testjsonpy hypothesiscorepy in execute testname textrepr self hypothesiscoreStateForActualGivenExecution object at x e message Hypothesis testarbitraryjsonselfturicreatetesttestjsonJSONTest testMethodtestarbitraryjson jsonobj t produces unreliable results Falsified on the first call but did not on a subsequent one def flakyself message if lenselffalsifyingexamples raise Flakymessage E hypothesiserrorsFlaky Hypothesis testarbitraryjsonselfturicreatetesttestjsonJSONTest testMethodtestarbitraryjson jsonobj t produces unreliable results Falsified on the first call but did not on a subsequent one hypothesiscorepy Flaky Example Falsifying example testarbitraryjsonselfturicreatetesttestjsonJSONTest testMethodtestarbitraryjson jsonobj Y AB h A CkEm EUUY e JBbEZ LgZcU Y Zvgdo ddDng o Qj jrL Unreliable test timings On an initial run this test took ms which exceeded the deadline of ms but on a subsequent run it took ms which did not If you expect this sort of variability in your test timings consider turning deadlines off for this test by setting deadlineNone Comparing run time across runs doesnt make sense In many cases the unit test returns early if the random input isnt in the correct format Fixes issues with CoreML integration on OSX 