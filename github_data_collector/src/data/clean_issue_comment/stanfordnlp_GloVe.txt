item token ohly has dimensions in glovetwitter B dtxt It appears that in the vocabcountc the authors malloced some memory to store the word read from the corpus when they build the vocabulary but they didnt free these memories in the end I think this will lead to a memory leak issue right Can anyone tell me that if I am right and how to fix this issue I encounter cost nan after a few iterations from to on my own dataset of Gb The vectors are also all nans I have read this issue but decreasing learning rate doesnt solve my problem I tried and the same results The cost is decreasing for a few steps but then goes to NaN Like this glove I have a really small vocabulary size of less than thats intentional I pretokenized my corpus this way and a large vector size tried and Cooccurrence matrix seems to be constructed fine and weights about Mb vocab file also looks good And even resulted vectors after a few iterations before nan appears seem good and do not completely fail on lexical similarity task However I would like to continue training since I am not sure that the model has converged Please give some advice about how to avoid nan in cost and vectors I installed glove with git clone cd glove make if it matters buildcooccur memory vocabfile vocabtxt verbose windowsize traindatadatatxt cooccurrencebin COUNTING COOCCURRENCES window size context symmetric max product overflow length Reading vocab from file vocabtxtloaded words Building lookup tabletable contains elements Processed tokens Writing cooccurrences to disk files in total Merging cooccurrence files processed linesUnable to open file overflow bin when I train glove with big dataset G I met the problem anyone knows why Hi I just want some help in getting the training vocabulary word frequency from the binary file bin of embedding trained by word vec or GloVe I know that fastText offers such API like getwords but I just dont know how to do the same thing on pretrained GloVe models Thanks in advance I am not able to run the vocabcountc file on codeblocks IDE which IDE I need to use to generate vectors for my own corpus please help This PR fixes two issues with NaNs during training First the checks for NaNs in glovec did not work with the compiler options from Makefile Ofast disables checks for NaNs and Infs Changed to O by default so that users at least see the error messages during training Second default learning rate causes on some data exploding gradients and as a result Infs and NaNs People on the internet recommend lowering learning rate in such a case but this solution is not perfect users have to get NaNs in their embeddings then to google for a solution then train again Now there is a gradient components clipping parameter with a reasonable default which allows training with arbitrary initial learning rates including default on arbitrary data out of the box and no fiddling is required base twmbpmjalalglove mjalal cd GloVe make mkdir p build gcc srcglovec o buildglove lm pthread Ofast marchnative funrollloops Wnounusedresult gcc srcshufflec o buildshuffle lm pthread Ofast marchnative funrollloops Wnounusedresult gcc srccooccurc o buildcooccur lm pthread Ofast marchnative funrollloops Wnounusedresult gcc srcvocabcountc o buildvocabcount lm pthread Ofast marchnative funrollloops Wnounusedresult base twmbpmjalalGloVe mjalal demosh mkdir p build gcc srcglovec o buildglove lm pthread Ofast marchnative funrollloops Wnounusedresult gcc srcshufflec o buildshuffle lm pthread Ofast marchnative funrollloops Wnounusedresult gcc srccooccurc o buildcooccur lm pthread Ofast marchnative funrollloops Wnounusedresult gcc srcvocabcountc o buildvocabcount lm pthread Ofast marchnative funrollloops Wnounusedresult Resolving mattmahoneynet mattmahoneynet Connecting to mattmahoneynet mattmahoneynet connected HTTP request sent awaiting response OK Length M applicationzip Saving to text zip text zip M MBs in s MBs text zip saved Archive text zip inflating text BUILDING VOCABULARY Processed tokens Counted unique words Truncating vocabulary at min count Using vocabulary of size COUNTING COOCCURRENCES window size context symmetric max product overflow length Reading vocab from file vocabtxtloaded words Building lookup tabletable contains elements Processed tokens Writing cooccurrences to disk files in total Merging cooccurrence files processed lines SHUFFLING COOCCURRENCES array size Shuffling by chunks processed lines Wrote temporary files Merging temp files processed lines TRAINING MODEL Read lines Initializing parametersdone vector size vocab size xmax alpha iter cost iter cost iter cost iter cost iter cost iter cost iter cost iter cost iter cost iter cost iter cost iter cost iter cost iter cost iter cost Traceback most recent call last File evalpythonevaluatepy line in module main File evalpythonevaluatepy line in main vectordim lenvectors ivocab TypeError object of type map has no len base twmbpmjalalGloVe mjalal In the original paper best window size is why the default window size here is 