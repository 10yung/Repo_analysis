hi We are managing offsets externally in HBase We havent enabled spark checkpointing So everytime we want to start job we are providing offsets which are correct when cross checked with Kafka Only sometimes we get this exception numRecords cannot be negative Is it the problem cause of property sparkstreamingkafkaconsumercacheenabled set to true inside compute method of DirectKafkaInputDStream class or checkpointing which is done internally by spark or some other issue lib sparkstreamingkafka Thanks in advance Hi in case I am getting OffsetOutOfRangeException how can I handle it should I catch it somewhere in the code and ignore it in case I just want to continue for the next offset range Or perhapses any option to restart in such case to a specific offset for each partition also just to verify I see in the code that you set autooffsetresetnone is that only for the executers Thanks Hi koeninger In spark x Im looking at an efficient way to convert the Kafka DStream to DF for performing a query over temp table In my case the stream data is structured like csv data I want to parse and store it in same format in tempTable The stream data may not have the header so Ive to place default colnames such as col colN Any help would be appreciate Thanks Karthik Very good article on idempotent and transactional exactly once But I am hitting the following scenario where checkpoint itself is failing and cause partial writes Can you take a look and suggest I am using spark streaming directStream I am hitting the following Scenario and I would like your suggestion on how to design atomicity Here are pseudo codes to describe the flow and key points S createDirectStreamkafka I have OffsetRange associated with each RDD S print good S S flatMapsome transformation It does not require checkpoint S print good S S updateStateByKeyrequire checkpoint checkpoint failed due to hdfs issue for example S print nothing print out S foreachRDD SaveToElasticSearch write to Elastic Search fine S foreachRDD SaveToElasticSearch nothing written to Elastic Search I was hoping the batch is atomic ie as long as there are errors offsets will not change and writes will not happen But issues I have observed Kafka offsets kept moving on to next batch even there are dependent stream failed eg S Partial writes went to Elastic Search We would like to see the offset stops if anything in this job failed and spark streaming will recover by itself from the right offsets Write all streams in one unit Any suggestions Tian 