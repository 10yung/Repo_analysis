Describe the bug In yarn mode only the hive table stored as textfile can be accessed Both parquet orc files cannot In local mode everything is working ok error stack javaioInvalidClassException orgapachesparksqlexecutiondatasourcesFilePartition local class incompatible stream classdesc serialVersionUID local class serialVersionUID at javaioObjectStreamClassinitNonProxyObjectStreamClassjava at javaioObjectInputStreamreadNonProxyDescObjectInputStreamjava at javaioObjectInputStreamreadClassDescObjectInputStreamjava at javaioObjectInputStreamreadOrdinaryObjectObjectInputStreamjava at javaioObjectInputStreamreadObject ObjectInputStreamjava at javaioObjectInputStreamdefaultReadFieldsObjectInputStreamjava at javaioObjectInputStreamreadSerialDataObjectInputStreamjava at javaioObjectInputStreamreadOrdinaryObjectObjectInputStreamjava at javaioObjectInputStreamreadObject ObjectInputStreamjava at javaioObjectInputStreamreadObjectObjectInputStreamjava at orgapachesparkserializerJavaDeserializationStreamreadObjectJavaSerializerscala at orgapachesparkserializerJavaSerializerInstancedeserializeJavaSerializerscala at orgapachesparkexecutorExecutorTaskRunnerrunExecutorscala at javautilconcurrentThreadPoolExecutorrunWorkerThreadPoolExecutorjava at javautilconcurrentThreadPoolExecutorWorkerrunThreadPoolExecutorjava at javalangThreadrunThreadjava Driver stacktrace at orgapachesparkschedulerDAGSchedulerorgapachesparkschedulerDAGSchedulerfailJobAndIndependentStagesDAGSchedulerscala at orgapachesparkschedulerDAGScheduleranonfunabortStage applyDAGSchedulerscala at orgapachesparkschedulerDAGScheduleranonfunabortStage applyDAGSchedulerscala at scalacollectionmutableResizableArrayclassforeachResizableArrayscala at scalacollectionmutableArrayBufferforeachArrayBufferscala at orgapachesparkschedulerDAGSchedulerabortStageDAGSchedulerscala at orgapachesparkschedulerDAGScheduleranonfunhandleTaskSetFailed applyDAGSchedulerscala at orgapachesparkschedulerDAGScheduleranonfunhandleTaskSetFailed applyDAGSchedulerscala at scalaOptionforeachOptionscala at orgapachesparkschedulerDAGSchedulerhandleTaskSetFailedDAGSchedulerscala at orgapachesparkschedulerDAGSchedulerEventProcessLoopdoOnReceiveDAGSchedulerscala at orgapachesparkschedulerDAGSchedulerEventProcessLooponReceiveDAGSchedulerscala at orgapachesparkschedulerDAGSchedulerEventProcessLooponReceiveDAGSchedulerscala at orgapachesparkutilEventLoopanon runEventLoopscala at orgapachesparkschedulerDAGSchedulerrunJobDAGSchedulerscala at orgapachesparkSparkContextrunJobSparkContextscala at orgapachesparkSparkContextrunJobSparkContextscala at orgapachesparkSparkContextrunJobSparkContextscala at orgapachesparkSparkContextrunJobSparkContextscala at orgapachesparkrddRDDanonfuncollect applyRDDscala at orgapachesparkrddRDDOperationScopewithScopeRDDOperationScopescala at orgapachesparkrddRDDOperationScopewithScopeRDDOperationScopescala at orgapachesparkrddRDDwithScopeRDDscala at orgapachesparkrddRDDcollectRDDscala at orgapachesparksqlexecutionSparkPlanexecuteCollectSparkPlanscala at orgapachesparksqlDatasetorgapachesparksqlDatasetcollectFromPlanDatasetscala at orgapachesparksqlDatasetanonfuncollect applyDatasetscala at orgapachesparksqlDatasetanonfuncollect applyDatasetscala at orgapachesparksqlDatasetanonfun applyDatasetscala at orgapachesparksqlexecutionSQLExecutionanonfunwithNewExecutionId applySQLExecutionscala at orgapachesparksqlexecutionSQLExecutionwithSQLConfPropagatedSQLExecutionscala at orgapachesparksqlexecutionSQLExecutionwithNewExecutionIdSQLExecutionscala at orgapachesparksqlDatasetwithActionDatasetscala at orgapachesparksqlDatasetcollectDatasetscala at yaooqinnkyuubioperationstatementExecuteStatementInClientModeexecuteExecuteStatementInClientModescala at yaooqinnkyuubioperationstatementExecuteStatementOperationanon anon runExecuteStatementOperationscala at yaooqinnkyuubioperationstatementExecuteStatementOperationanon anon runExecuteStatementOperationscala at javasecurityAccessControllerdoPrivilegedNative Method at javaxsecurityauthSubjectdoAsSubjectjava at orgapachehadoopsecurityUserGroupInformationdoAsUserGroupInformationjava at yaooqinnkyuubioperationstatementExecuteStatementOperationanon runExecuteStatementOperationscala at javautilconcurrentExecutorsRunnableAdaptercallExecutorsjava at javautilconcurrentFutureTaskrunFutureTaskjava at javautilconcurrentThreadPoolExecutorrunWorkerThreadPoolExecutorjava at javautilconcurrentThreadPoolExecutorWorkerrunThreadPoolExecutorjava at javalangThreadrunThreadjava Caused by javaioInvalidClassException orgapachesparksqlexecutiondatasourcesFilePartition local class incompatible stream classdesc serialVersionUID local class serialVersionUID at javaioObjectStreamClassinitNonProxyObjectStreamClassjava at javaioObjectInputStreamreadNonProxyDescObjectInputStreamjava at javaioObjectInputStreamreadClassDescObjectInputStreamjava at javaioObjectInputStreamreadOrdinaryObjectObjectInputStreamjava at javaioObjectInputStreamreadObject ObjectInputStreamjava at javaioObjectInputStreamdefaultReadFieldsObjectInputStreamjava at javaioObjectInputStreamreadSerialDataObjectInputStreamjava at javaioObjectInputStreamreadOrdinaryObjectObjectInputStreamjava at javaioObjectInputStreamreadObject ObjectInputStreamjava at javaioObjectInputStreamreadObjectObjectInputStreamjava at orgapachesparkserializerJavaDeserializationStreamreadObjectJavaSerializerscala at orgapachesparkserializerJavaSerializerInstancedeserializeJavaSerializerscala at orgapachesparkexecutorExecutorTaskRunnerrunExecutorscala Additional context spark kyuubi hadoop hdp kerberos on hi kyuubi can run spark sql sript now i try to run spark sql script and Passing parameters to script but not runing is issues double click table or select without where conditions will query all the data can only show first rows Describe the bug when i set the keytab and principal in the sparkdefaultsconfxml and start the kyuubi the error happenedSo i discard the kerberos auth and delete the keytab and pricipal in the sparkdefaultsconfxml and start the kyuubi server the message in log display when i make the zkha true kyuubi will use kerberos to auth and the error is there is no sparkyarnkeytabAs a resulthow do i connect to zeekeeper without kerberos My zookeeper is no auth To Reproduce Steps to reproduce the behavior Configurations frist i export environment export HADOOPCONFDIRetchadoopconf export SPARKHOMEoptsparkspark binhadoop second i editor sparkdeaultsconf file to add these sparkyarnprincipal HADOOPCOM sparkyarnkeytab keytab Environments sparkspark binhadoop linuxcentos java kyuubimaster hadoop CDH zookeeper cdh Operations optsparkkyuubi binspark binstartkyuubish master yarn conf sparkkyuubihaenabledtrue conf sparkkyuubihazkquorum conf sparkkyuubihazkclientport conf sparkkyuubiauthenticationKERBEROS conf sparkdrivermemory g conf sparkhadoopfshdfsimpldisablecachetrue conf sparkexecutorheartbeatInterval s See error INFO stateConnectionStateManager State change CONNECTED ERROR clientZooKeeperSaslClient An error javasecurityPrivilegedActionException javaxsecuritysaslSaslException GSS initiate failed Caused by GSSException No valid credentials provided Mechanism level Server not found in Kerberos database UNKNOWNSERVER occurred when evaluating Zookeeper Quorum Members received SASL token This may be caused by Javas being unable to resolve the Zookeeper Quorum Members hostname correctly You may want to try to adding Dsunnetspinameserviceprovider dnssun to your clients JVMFLAGS environment Zookeeper Client will go to AUTHFAILED state ERROR zookeeperClientCnxn SASL authentication with Zookeeper Quorum member failed javaxsecuritysaslSaslException An error javasecurityPrivilegedActionException javaxsecuritysaslSaslException GSS initiate failed Caused by GSSException No valid credentials provided Mechanism level Server not found in Kerberos database UNKNOWNSERVER occurred when evaluating Zookeeper Quorum Members received SASL token This may be caused by Javas being unable to resolve the Zookeeper Quorum Members hostname correctly You may want to try to adding Dsunnetspinameserviceprovider dnssun to your clients JVMFLAGS environment Zookeeper Client will go to AUTHFAILED state ERROR curatorConnectionState Authentication failed Error Starting Kyuubi by client yaooqinnkyuubiserviceServiceException Unable to create Kyuubi namespace kyuubiserver on ZooKeeper Run with help for usage help or verbose for debug output INFO serverKyuubiServer Shutting down KyuubiServer INFO utilShutdownHookManager Shutdown hook called Expected behavior A clear and concise description of what you expected to happen Screenshots If applicable add screenshots to help explain your problem Additional context Add any other context about the problem here Has the project completed the support for kudu now I havent found the code to support kudu I only saw the document introduction of integrated kudu Can you introduce the implementation logic of integrated kudu This error occurs when you try to create multiple spark contexts situation like bellow ERROR KyuubiOperation Error executing query as bipdz select from bitmptmpproductlistallnew limit Current operation state RUNNING javaioIOException orgapachesparkSparkException Failed to get broadcast piece of broadcast at orgapachesparkutilUtilstryOrIOExceptionUtilsscala at orgapachesparkbroadcastTorrentBroadcastreadBroadcastBlockTorrentBroadcastscala at orgapachesparkbroadcastTorrentBroadcastvaluelzycomputeTorrentBroadcastscala at orgapachesparkbroadcastTorrentBroadcastvalueTorrentBroadcastscala at orgapachesparkbroadcastTorrentBroadcastgetValueTorrentBroadcastscala at orgapachesparkbroadcastBroadcastvalueBroadcastscala at orgapachesparkrddHadoopRDDgetJobConfHadoopRDDscala at orgapachesparkrddHadoopRDDgetPartitionsHadoopRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at scalaOptiongetOrElseOptionscala at orgapachesparkrddRDDpartitionsRDDscala at orgapachesparkrddMapPartitionsRDDgetPartitionsMapPartitionsRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at scalaOptiongetOrElseOptionscala at orgapachesparkrddRDDpartitionsRDDscala at orgapachesparkrddMapPartitionsRDDgetPartitionsMapPartitionsRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at scalaOptiongetOrElseOptionscala at orgapachesparkrddRDDpartitionsRDDscala at orgapachesparkrddUnionRDDanonfun applyUnionRDDscala at orgapachesparkrddUnionRDDanonfun applyUnionRDDscala at scalacollectionparallelAugmentedIterableIteratorclassmap combinerRemainsIteratorscala at scalacollectionparallelimmutableParVectorParVectorIteratormap combinerParVectorscala at scalacollectionparallelParIterableLikeMapleafParIterableLikescala ERROR Utils Exception encountered orgapachesparkSparkException Failed to get broadcast piece of broadcast at orgapachesparkbroadcastTorrentBroadcastanonfunorgapachesparkbroadcastTorrentBroadcastreadBlocks applymcVIspTorrentBroadcastscala at orgapachesparkbroadcastTorrentBroadcastanonfunorgapachesparkbroadcastTorrentBroadcastreadBlocks applyTorrentBroadcastscala at orgapachesparkbroadcastTorrentBroadcastanonfunorgapachesparkbroadcastTorrentBroadcastreadBlocks applyTorrentBroadcastscala at scalacollectionimmutableListforeachListscala at orgapachesparkbroadcastTorrentBroadcastorgapachesparkbroadcastTorrentBroadcastreadBlocksTorrentBroadcastscala at orgapachesparkbroadcastTorrentBroadcastanonfunreadBroadcastBlock applyTorrentBroadcastscala at orgapachesparkutilUtilstryOrIOExceptionUtilsscala at orgapachesparkbroadcastTorrentBroadcastreadBroadcastBlockTorrentBroadcastscala at orgapachesparkbroadcastTorrentBroadcastvaluelzycomputeTorrentBroadcastscala at orgapachesparkbroadcastTorrentBroadcastvalueTorrentBroadcastscala at orgapachesparkbroadcastTorrentBroadcastgetValueTorrentBroadcastscala at orgapachesparkbroadcastBroadcastvalueBroadcastscala at orgapachesparkrddHadoopRDDgetJobConfHadoopRDDscala at orgapachesparkrddHadoopRDDgetPartitionsHadoopRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at scalaOptiongetOrElseOptionscala at orgapachesparkrddRDDpartitionsRDDscala at orgapachesparkrddMapPartitionsRDDgetPartitionsMapPartitionsRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at scalaOptiongetOrElseOptionscala at orgapachesparkrddRDDpartitionsRDDscala at orgapachesparkrddMapPartitionsRDDgetPartitionsMapPartitionsRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at orgapachesparkrddRDDanonfunpartitions applyRDDscala at scalaOptiongetOrElseOptionscala at orgapachesparkrddRDDpartitionsRDDscala at orgapachesparkrddUnionRDDanonfun applyUnionRDDscala at orgapachesparkrddUnionRDDanonfun applyUnionRDDscala at scalacollectionparallelAugmentedIterableIteratorclassmap combinerRemainsIteratorscala at scalacollectionparallelimmutableParVectorParVectorIteratormap combinerParVectorscala at scalacollectionparallelParIterableLikeMapleafParIterableLikescala at scalacollectionparallelTaskanonfuntryLeaf applymcVspTasksscala at scalacollectionparallelTaskanonfuntryLeaf applyTasksscala at scalacollectionparallelTaskanonfuntryLeaf applyTasksscala at scalacollectionparallelTaskclasstryLeafTasksscala at scalacollectionparallelParIterableLikeMaptryLeafParIterableLikescala at scalacollectionparallelAdaptiveWorkStealingTasksWrappedTaskclasscomputeTasksscala at scalacollectionparallelAdaptiveWorkStealingForkJoinTasksWrappedTaskcomputeTasksscala at scalaconcurrentforkjoinRecursiveActionexecRecursiveActionjava at scalaconcurrentforkjoinForkJoinTaskdoExecForkJoinTaskjava at scalaconcurrentforkjoinForkJoinPoolWorkQueuerunSubtaskForkJoinPooljava at scalaconcurrentforkjoinForkJoinPooltryHelpStealerForkJoinPooljava at scalaconcurrentforkjoinForkJoinPoolawaitJoinForkJoinPooljava at scalaconcurrentforkjoinForkJoinTaskdoJoinForkJoinTaskjava at scalaconcurrentforkjoinForkJoinTaskjoinForkJoinTaskjava at scalacollectionparallelForkJoinTasksWrappedTaskclasssyncTasksscala at scalacollectionparallelAdaptiveWorkStealingForkJoinTasksWrappedTasksyncTasksscala at scalacollectionparallelAdaptiveWorkStealingTasksWrappedTaskclassinternalTasksscala at scalacollectionparallelAdaptiveWorkStealingForkJoinTasksWrappedTaskinternalTasksscala at scalacollectionparallelAdaptiveWorkStealingTasksWrappedTaskclasscomputeTasksscala Is your feature request related to a problem Please describe When submit a kyuubiAppMaster it would only upload the jars under SPARKJARSDIR to kyuubiStagingDir However there are some jarsDir under SPARKJARSDIR would be used by some spark plugins such as ranger atlas Describe the solution youd like For the libDir under SPARKJARSDIR when copied to jarsStream we create zipEntry with libDirName prefix and keep its origin directory structure Is your feature request related to a problem Please describe For the kyuubiAppMaster it should have two modes specific mode for a user and normal mode for all user For the normal mode it receive all requests from all users and it would be alive until be killed Therefor its session management has no difference with local kyuubiServer For the specific mode it only receive the requests from a specific user and it should unregister itself when it has idled for a define timeout Describe the solution youd like There should be a parameter present the mode of kyuubiAppMaster sparkkyuubiyarnappmasterspecificmode For the specific mode we would implement SpecifiCSparkSessionCacheMgr extends sparkSessionCacheMgr which only has one active sparkContext and the sparkSession cleaner thread should stop the kyuubiAppMaster when sparkContext idle for more than defined duration Is your feature request related to a problem Please describe Now kyuubi only has kyuubiSession with local mode which is weak scaleable Therefor we implement kyuubiClusterSession which will submit a kyuubiAppMaster and communicate with this appMaster to deliver statements and get computation results Describe the solution youd like The parameter below present the kyuubi session mode sparkkyuubisessionmode clientcluster When the kyuubiSession mode is cluster the implement of kyuubiSession is KyuubiClusterSession The kyuubiClusterSession will check whether there is already exist an kyuubiAppMaster belong to current user if not a new kyuubiAppMaster would be create Then the kyuubiClusterSession would communicate with this appMaster and deliver its statement and get computation result 