Similar to the AutoExtendVisibility middleware is unaware of the update of visibility timeout Here is an simple example to illustrate ruby class NoopWorker ApplicationWorker shoryukenoptions queue Settingsbgqueueslow autovisibilitytimeout true def performmsghandler params The queue lelve visibility timeout is set to seconds by default msghandlervisibilitytimeout minutes sleep minutes msghandlerdelete end end T Z TIDovh lqc INFO Starting T Z TIDovh qp x NoopWorkerdevmarkhuanglow f b d bebe a d ffdea f INFO started at T Z TIDovh qp bk WARN Extending message devmarkhuanglow f b d bebe a d ffdea f visibility timeout by s T Z TIDovh rqmww WARN Extending message devmarkhuanglow f b d bebe a d ffdea f visibility timeout by s T Z TIDovh qp bk WARN Extending message devmarkhuanglow f b d bebe a d ffdea f visibility timeout by s T Z TIDovh qp bk WARN Extending message devmarkhuanglow f b d bebe a d ffdea f visibility timeout by s T Z TIDovh qp bk WARN Extending message devmarkhuanglow f b d bebe a d ffdea f visibility timeout by s T Z TIDovh rqmww WARN Extending message devmarkhuanglow f b d bebe a d ffdea f visibility timeout by s T Z TIDovh rqmww WARN Extending message devmarkhuanglow f b d bebe a d ffdea f visibility timeout by s T Z TIDovh qp x NoopWorkerdevmarkhuanglow f b d bebe a d ffdea f WARN exceeded the queue visibility timeout by ms T Z TIDovh qp x NoopWorkerdevmarkhuanglow f b d bebe a d ffdea f INFO completed in ms Ideally the auto extension of message visibility should be triggered seconds after manually bumping the visibility timeout However as shown in the log its unnecessarily triggered multiple times according to the default queue visibility timeout regardless the fact that its already been changed for this message Wonder if you think this is identified as unexpected behavior Also unfortunately I dont have an idea on top of my head that could fix this but still wanna bring it up here and maybe discuss about it This PR tries to address it Adds the ability to query visibilitytimeout from message level Changes the logic inside Timing middleware to print exceeds visibility timeout warning according to message level visibilitytimeout Happy new year According to AWS documenation doc its possible to set the VisibilityTimeout for a specific message regardless of the VisibilityTimeout value in the queue level This is useful since in our application messages in one queue can be processed by different shoyuken workers which takes up very different amount of time We want to set the visibility timout to a bigger value for those messages that takes longer time so that it wont get processed multiple times Fortunately we were able to use the Messagevisibilitytimeout setter to change the messages visibility timeout from seconds to hours in this case However shoryuken would log warnings like this started at exceeded the queue visibility timeout by ms completed in ms which is misleading I think this is caused by shoryuken always uses the queue level visibility timeout and not respecting the visibility timeout in the message level Im happy to open a PR if this is a legit issue or otherwise I might be missing something here Thanks doc We just moved a large workload over to a pool of Shoryuken workers from Resque and have been noticing a memory leak the same workload did not exhibit the same memory growth behavior in Resque This is our first use of Shoryuken so we dont have another version to compare it to From Gemfile gem awssdks gem awssdksqs gem shoryuken From Gemfilelock awssdkcore awseventstream awspartitions awssigv jmespath awssdks awssdkcore awssdkkms awssigv awssdksqs awssdkcore awssigv shoryuken awssdkcore concurrentruby thor config aws receivemessage attributenames ApproximateReceiveCount SentTimestamp concurrency Screen Shot at AM The above chart shows a per container memory usage over time overlaid is an instance with the min max and the mean memory usage All of the containers are cycled and then you can see that they steadily grow over time There are processes in one container Each process grows to use about GB of memory before the container OOMs I spent some time looking at heap dumps and it shows that there is a significant growth of the following items over time usrlocalbundlegemsawssdkcore libseahorseutilrb STRING usrlocalbundlegemsawssdkcore libseahorseclienthandlerlistentryrb HASH Updating the awssdksqs version to did not fix any issues those two items still show up as culprits for leakage Any suggestions or further questions would be greatly appreciated thanks Would it be possible to only log a single event for exceptions Currently there is one log event being created for the exception message and one log event for the exception backtrace This can sometimes make it challenging to debug in a high volumenoisy application as additional tagging is needed to identify the two events as related or they need to be manually identified ex instead of something like loggererror Processor failed exmessage loggererror exbacktracejoin n unless exbacktracenil simplify to loggererror Processor Failed exmessage Backtrace exbacktracejoin n When running Workers using InlineExecutor performasync does not run the server middleware Given ruby require spechelper require shoryuken RSpecdescribe middleware for InlineExecutor do before do Enable inline execution Shoryukenworkerexecutor ShoryukenWorkerInlineExecutor Add middleware Shoryukenconfigureserver do config configservermiddleware do chain chainadd middlewareclass end end end letmiddlewareclass do stubconstTestMiddleware Classnew do def callworkerinstance queue sqsmsg body yield end end end context when a ShoryukenWorker class do subjectworkerclass do stubconstTestWorker Classnew do include ShoryukenWorker shoryukenoptions queue default def performsqsmsg body end end end describe performasync do subjectperformasync workerclassperformasyncbody letbody test before do expectanyinstanceofworkerclassto receiveperform andcalloriginal expectanyinstanceofmiddlewareclassto receivecall andcalloriginal end it do expect performasync tonot raiseerror end end end end I expect the test to pass but it fails with Failures middleware for InlineExecutor when a ShoryukenWorker class performasync should not raise Exception FailureError DEFAULTFAILURENOTIFIER lambda failure opts raise failure Exactly one instance should have received the following messages but didnt call Removing the expectanyinstanceofmiddlewareclassto receivecall expectation allows the test to pass Sidekiq supports this by running the middleware stack Much appreciated sqsmanagementconsole 