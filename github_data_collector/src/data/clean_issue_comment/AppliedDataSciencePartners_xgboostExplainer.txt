Hi I want to keep somme variable like an ID raw After Xgboost esxplainer I lost my ID have you an idea to keep my variable Thanks a lot Paul Hello Thanks for creating this useful package The waterfall plots are quite informative and intuitive I found that when I varied the basescore in the buildExplainer function the predicted values output in the showWaterfall function varied significantly Concerned about the accuracy of predicted values in the actual xgboost model the predicted values varied but not nearly as much Is this an error Or am I doing something incorrectly Should the basescore entered in the buildExplainer always match whatever was entered in the actual xgboost model This is what I observed for a single predicted outcome basescore pred in both xgb predict function and explainer waterfall function basescore pred in xgb predict function but explainer waterfall function was basescore pred in xgb predict function but explainer waterfall function was Note in all three examples the xgb model used in the explainer function had a basescore of Therefore it varied from the basescore entered in the explainer in the nd and rd examples Thanks for any suggestions I would like to package this for to make it simpler to install in our environments This requires though that releases of xgboostExplainer are done git tags are sufficient When running the code exemple from the basic buildExplainer function i get the following error Creating the trees of the xgboost modelError in xgbmodeldttreecolnames model xgbmodel trees treesidx unused argument trees treesidx Each call to showWaterfall runs explainPredictions For datasets with hundreds or thousands of columns this is tedious Runtime can be hours long To expedite the function call showWaterfall should also accept an explanations variable that references prediction breakdowns that have already been made maybe like this showWaterfall functionxgbmodel explainer DMatrix datamatrix idx type binary threshold limits cNA NA explanations NULL ifisnullexplanations breakdown explanations idx else breakdown explainPredictionsxgbmodel explainer sliceDMatrixasintegeridx etc Building the explainer relies on creating several new columns with names such as tree and leaf If those column names are already used in the training dataset one can run into issues specifically In my case I ran into this issue when using the explainer on a Natural Language Processing xgb model My model was counting the instances of the word tree in the document within a column also named tree The error is triggered when trying to row bind the treelistbreakdown and treebreakdown lists within the call to buildExplainerFromTreeList A warning when the dataset already uses column names that include key terms such as tree leaf and intercept would be helpful Perhaps written into the first few lines of the function Similarly the explainPredictions function uses variable names that can cause issues In my case I had a column named x in my dataset that caused problems with the x used as the iterator in the for loop for going through all the trees While I can get the package to work with these kinds of models the stated predictions dont match when the model is not reglinear using regression type Would it be possible to support these types of regression I have been trying to figure out what xgboostExplainer does under the hood and came across something strange to my eyes anyway when looking at the definition of findLeavesR I cant work out why findLeaves is used in the definition of findLeaves see line how you can have three parameters of findLeaves see line when it only has two by the definition being created What does the with FALSE part do Any help with either of these queries would be really appreciated Hi Is there a way to enforce constraints in the features contribution calculation for example to enforce some features to have positive contribution Please note Im already using monotonicity constraints in the xgboost training Thanks Paolo I just observed that in the example code given on the mediumcom blog you calculated individual weights by simply calling rowSumspredbreakdown on the data set containing the feature contributions I think this is not correct because adding negative and positive feature contributions may cancel out effects Shouldnt it be sth like weights applypredbreakdown functionx sumabsx I was referring to this blog 