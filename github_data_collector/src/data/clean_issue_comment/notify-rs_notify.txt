 If reporting a bug fill out the below Otherwise if asking a question or suggesting a feature or something else remove everything before continuing System details Please include ALL of the following OSPlatform name and version macOS Rust version if building from source rustc version rustc e Notify version or commit hash if building from git And as much of the following as you can think is relevant Filesystem type and options APFS If youre running as a privileged user root System no If youre running in a container details on the runtime and overlay no If youre running in a VM details on the hypervisor no remove the ones that are not relevant What you did as detailed as you can Writes are generated to a file every second using the following C program However the events are only noticed by notifys FSEventsWatcher every seconds when the file is closed and reopened include stdlibh include stdioh include unistdh int main while FILE f fopenUsersbrennantesttxt w for int i i i fputsHello n f fflushf sleep printfClosing file n fclosef What you expected Notify should trigger once per second when watching Usersbrennantesttxt and running the above C program What happened It only triggers every seconds Thank you Hi Im new in rust and new in notify I want to monitor content change in a file using code below When I change my sql the event will be always CreateAny Shouldnt it be ModifyDataChange Im confused about this How can I get the changed content correctly use crossbeamchannelunbounded use notifyRecommendedWatcher RecursiveMode Result Watcher EventKind Config use stdtimeDuration use notifyeventModifyKind CreateKind DataChange fn main Result Create a channel to receive the events let tx rx unbounded Automatically select the best implementation for your platform let mut watcher RecommendedWatcher Watchernewtx Durationfromsecs Add a path to be watched All files and directories at that path and below will be monitored for changes watcherwatchUserssamuel sql RecursiveModeRecursive loop match rxrecv Okevent if eventisok let realevent eventunwrap match realeventkind EventKindAny printlnany kind realeventkind EventKindAccessaccesskind printlnaccess kind accesskind EventKindCreatecreatekind printlncreate kind createkind EventKindModifymodifykind printlnmodify kind modifykind EventKindRemoveremovekind printlnremove kind removekind EventKindOther printlnother kind printlndefault kind else Errerr printlnwatch error err Ok Update fseventsys to Related If reporting a bug fill out the below Otherwise if asking a question or suggesting a feature or something else remove everything before continuing System details Please include ALL of the following OSPlatform name and version macOS on a MacBook Pro Zoll Rust version if building from source rustc version rustc nightly e c Notify version or commit hash if building from git Hi Users report that rustanalyzer sometimes hangs during shutdown The stack trace points to this code Downstream issue with captured stack trace cc killercup Windows provides an error code to the completion routine that is called when a new file event is received In the code ie here the only error that is checked for is ERROROPERATIONABORTED however its possible for other error codes to be generated Notably when a watched folder is deleted the error code appears to be access denied however the file is not properly deleted until the watch handle is closed This results in various unwanted behavior depending on how the folder is deleted Deleting the file directly generally results in being marked as deleted but not actually deleted and any attempt to use the directory returning an access denied status code Deleting the file through explorer sometimes results in the watcher being flooded with a continuous stream of notifications This happens fairly frequently in tools like rustanalyzer that like to dynamically watch subfolders to avoid listening to all notification events in busy root directories One solution might be to stop the watcher like in the ERROROPERATIONABORTED case possibly notifying any clients about the error allowing the handle to be closed eg maybe like this However Im not certain whether there are other errors that could occur where it is reasonable to continue listening for events Another solution might be just to propagate back to the user and force them to deal with stopping the watcher If reporting a bug fill out the below Otherwise if asking a question or suggesting a feature or something else remove everything before continuing System details Please include ALL of the following Linux CentOS kernel el ugx custom rebuild with some support for GPFS rustc c dc notify main brach commit aac And as much of the following as you can think is relevant I use notify in The issue I opened for this is at FS devmappervg scratch on tmp type ext rwrelatimedataordered Hypervisor is kvm in an OpenNebula env Underlying storage is Ceph but I do not think that is relevant here What you did as detailed as you can I created K files in a directory that was watched The watcher threads always stops but not always after the same number of files The notify queue size was raised to K These are the events seen T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None T sarchivelib DEBUG Event received Event kind CreateAny paths tmptorque SC attrtracker None attrflag None attrinfo None attrsource None What you expected It should just be able to keep track of all events especially since the queue is big enough What happened The watcher thread and the debouncer Threads and below event loop are waiting and seem to be deadlocked There is no further processing of events Thread Thread x f a LWP x f df in pthreadcondwaitGLIBC from lib libpthreadso x a ccd d in crossbeamutilssyncparkerInnerparkh f e f e c x a cc cb in crossbeamutilssyncparkerParkerparkh dacaa c ead x a f b in sarchivelibsignalhandleratomich d e d d c d f x a f in crossbeamutilsthreadScopedThreadBuilderspawnu bu bclosureu du dh eeb f e x a eb in stdsyscommonbacktracerustbeginshortbacktracehdc f b d a x a ea a in rustmaybecatchpanic at srclibpanicunwindlibrs x a fe d in LTFu asu allocboxedFnBoxLTAGTGTcallboxh a ade f d x a e e in callonce at rustc c dc c c f e d b cdc srcliballocboxedrs startthread at srclibstdsyscommonthreadrs threadstart at srclibstdsysunixthreadrs x f df dd in startthread from lib libpthreadso x f ead in clone from lib libcso Thread Thread x f LWP x f df in pthreadcondwaitGLIBC from lib libpthreadso x a f in wait at srclibstdsysunixcondvarrs wait at srclibstdsyscommoncondvarrs wait at srclibstdsynccondvarrs park at srclibstdthreadmodrs x a cc f in crossbeamchannelcontextContextwaituntilh bf e a da x a cc cc in crossbeamchannelcontextContextwithu bu bclosureu du dhb c e cedc d x a cbcaf in LTstdthreadlocalLocalKeyLTTGTGTtrywithh x a caf in crossbeamchannelselectrunselecthe afd d e fc x a cb in crossbeamchannelselectSelectselecthced ab c c x a in sarchivelibmonitorh efb a f x a in crossbeamutilsthreadScopedThreadBuilderspawnu bu bclosureu du dh c cfc ba db e x a eb in stdsyscommonbacktracerustbeginshortbacktracehdc f b d a x a ea a in rustmaybecatchpanic at srclibpanicunwindlibrs x a fb d in LTFu asu allocboxedFnBoxLTAGTGTcallboxh f dc fb a b x a e e in callonce at rustc c dc c c f e d b cdc srcliballocboxedrs startthread at srclibstdsyscommonthreadrs threadstart at srclibstdsysunixthreadrs x f df dd in startthread from lib libpthreadso x f ead in clone from lib libcso Thread Thread x f LWP x f df in pthreadcondwaitGLIBC from lib libpthreadso x a f in wait at srclibstdsysunixcondvarrs wait at srclibstdsyscommoncondvarrs wait at srclibstdsynccondvarrs park at srclibstdthreadmodrs x a cc f in crossbeamchannelcontextContextwaituntilh bf e a da x a cc cc in crossbeamchannelcontextContextwithu bu bclosureu du dhb c e cedc d x a cbcaf in LTstdthreadlocalLocalKeyLTTGTGTtrywithh x a caf in crossbeamchannelselectrunselecthe afd d e fc x a cb in crossbeamchannelselectSelectselecthced ab c c x a c in sarchivelibprocessh bbd ae b b x a d in crossbeamutilsthreadScopedThreadBuilderspawnu bu bclosureu du dh c ff e f e x a eb in stdsyscommonbacktracerustbeginshortbacktracehdc f b d a x a ea a in rustmaybecatchpanic at srclibpanicunwindlibrs x a d in LTFu asu allocboxedFnBoxLTAGTGTcallboxhae da cd a d x a e e in callonce at rustc c dc c c f e d b cdc srcliballocboxedrs startthread at srclibstdsyscommonthreadrs threadstart at srclibstdsysunixthreadrs x f df dd in startthread from lib libpthreadso x f ead in clone from lib libcso Thread Thread x f LWP x f dfc ed in llllockwait from lib libpthreadso x f df dcb in Llock from lib libpthreadso x f df c in pthreadmutexlock from lib libpthreadso x a f in notifydebouncetimerScheduleWorkerrunh a d f x a ff in stdsyscommonbacktracerustbeginshortbacktracehe ad cfadd x a c e in stdpanickingtrydocallh d a bc f x a ea a in rustmaybecatchpanic at srclibpanicunwindlibrs x a fc f in LTFu asu allocboxedFnBoxLTAGTGTcallboxh b c a d x a e e in callonce at rustc c dc c c f e d b cdc srcliballocboxedrs startthread at srclibstdsyscommonthreadrs threadstart at srclibstdsysunixthreadrs Type return to continue or q return to quit x f df dd in startthread from lib libpthreadso x f ead in clone from lib libcso Thread Thread x f LWP x f dfc ed in llllockwait from lib libpthreadso x f df dcb in Llock from lib libpthreadso x f df c in pthreadmutexlock from lib libpthreadso x a df in notifydebouncetimerWatchTimerignoreh e c x a b d in notifydebounceDebounceeventh eb b afa ac x a f in notifydebounceEventTxsendh e f ce ad c x a in notifyinotifyEventLoopeventloopthreadh a f e f x a c in stdsyscommonbacktracerustbeginshortbacktraceh d f b c e x a c in stdpanickingtrydocallh da af e d c e c x a ea a in rustmaybecatchpanic at srclibpanicunwindlibrs x a ffa in LTFu asu allocboxedFnBoxLTAGTGTcallboxheaa d ac ec x a e e in callonce at rustc c dc c c f e d b cdc srcliballocboxedrs startthread at srclibstdsyscommonthreadrs threadstart at srclibstdsysunixthreadrs x f df dd in startthread from lib libpthreadso x f ead in clone from lib libcso Thread Thread x f LWP x f df in pthreadcondwaitGLIBC from lib libpthreadso x a cd in crossbeamutilssyncwaitgroupWaitGroupwaith d b ae f x a b in crossbeamutilsthreadscopeh e e b eff ee x a d in sarchivemainhcf a ed d x a dbe in stdrtlangstartu bu bclosureu du dh aee ed e fb x a a in closure at srclibstdrtrs docallclosurei at srclibstdpanickingrs x a ea a in rustmaybecatchpanic at srclibpanicunwindlibrs x a ae d in tryi closure at srclibstdpanickingrs catchunwindclosurei at srclibstdpanicrs langstartinternal at srclibstdrtrs x a dbb in main Relevant code pub fn monitorpath Path s SenderTorqueJobEntry sigchannel Receiverbool notifyResult let tx rx unbounded create a platformspecific watcher let mut watcher RecommendedWatcher Watchernewtx Durationfromsecs infoWatching path path if let Erre watcherwatch path RecursiveModeNonRecursive return Erre loop select recvsigchannel b if let Oktrue b return Ok recvrx event match event Oke checkandqueues eunwrap Erre errorError on received event e break Ok Thank you Currently if the Recursive bit is specified every subdirectory of the specified directory will be watched unconditionally In some situations its possible to exclude entire directories potentially with many files eg build artifacts from the watcher For some backends I imagine that this wouldnt provide much benefit as they natively support recursive watching but for others it may be much cheaper if the directory is ignored completely It would be awesome if the RecursiveMode enum could be extended with a filter callback which would have to run quickly and be callable from any thread for example rust type Filter dyn Fn Path bool Send Sync enum RecursiveMode NonRecursive Recursive RecursiveFilteredBoxFilter It may be worthwhile to also filter events with this callback but its also probably fine if occasionally events are delivered for filtered paths womanshrugging When reading through the next ideas draft I noticed that one of the benefits of the new design is the ability to dynamically switch between backends asneeded depending on system resources etc As someone who wants to use notify as a nice plug play backend for filesystem watching that sounds like a great feature which reduces ergonomics burden produces a better experience tada That being said I often have other watcher tools already set up on my large projects such as watchman Given the dynamic watcher model I wonder if it would be possibleworthwhile to detect a running watchman instance and use it as a crossplatform backend if it happens to already be watching the repository in question Im not sure whether or not the new notify model supports backends being provided by external crates If it does this feature may make less sense to include in core and could perhaps be implemented externally This is a major breaking Comment here with things that can be changedupgraded with this move supports the Edition Looking forward I also want to establish a strategy for pushing the minimum rustc up more regularly one thought is to support at least N months of rustc but be free to upgrade from older stuff However I intend to maintain the pushing the minimum rustc is a breaking change guarantee Another way is to allow majors to optionally push the minimum rustc as far as they want up to the latest stable as of release but only allow one such push every six months or every year unless exceptional circumstances and be conservative about those pushes Were allocating a lot of PathBufs all over the place often by cloning Reducing this would improve performance for Notifys runtime Some notes We probably want to own them References are a mess but RcsArcs would work okay Beyond just removing a lot of copies of the same path it should be possible to optimise the number of copies of path segments there are Ideally if both abc asd and abe are in Notify in various places only one each of a b c d e and s would be allocated Whatever solution needs to convert cleanly to Path or PathBuf for the public API Whatever solution needs to work equally well under all platforms with all styles of paths think windows network paths Probably ought to think about URLs as well if nothing else than to support RedoxOS eventually down the line If possible this optimisation should hold for the entire process instead of just one instance of Notify Need to investigate existing solutions before building a custom one if possible Possibly also handle inodes where available