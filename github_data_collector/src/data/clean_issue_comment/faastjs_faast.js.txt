This PR contains the following updates Package Type Update Change typesawslambda devDependencies patch Renovate configuration date Schedule At any time no schedule defined verticaltrafficlight Automerge Enabled recycle Rebasing Whenever PR becomes conflicted or if you modify the PR title to begin with rebase nobell Ignore Close this PR and you wont be reminded about this update again rebasecheck If you want to rebaseretry this PR check this box This PR has been generated by WhiteSource Renovate View repository job log here Ive just started researching options for a large scale batch computing job and although pricing isnt a huge worry since its a oneoff job it is something that interests me in case I end up building a deployment scriptframework that can be used on future projects Google Cloud Function GB GHz hr core Google Preemptible n highcpu GB per core GHz hr core So the cloud function approach appears to be x the price of the preemptible machine approach And if you take into account clock speed then its somewhere near x the price I also havent taken into account the invocation pricing of the cloud functions but Im assuming that would be small The preemptible machine approach also has the advantage of being able to run code for longer than or mins which is very handy for me it saves me splitting up files that need to be processed into smaller digestiblein minutes chunks and coding all the logic to handle merging and stuff BrandiATMuhkuh who you were talking to on the hacker news thread seems to have switched to manycore machines after trying the cloud function approach and I wonder if pricing was a consideration there If Ive not made any mistakes or incorrect assumptions then is it within the scope of faastjs to consider preemptible machines Since youve implemented a local provider Im assuming that youve done most of the legwork required to be able to deploy it to an actual machine rather than a cloud function Currently CommonOptionsaddZipFile and CommonOptionsaddDirectory add files into the code package that is uploaded directly There is a MB limit on AWS for the code package Lambda Layers allows us to increase this limit to MB We should use Lambda Layers to implement these options A separate layer can be created for each value there is a limit of layers for each function then all can be added to the function Layers are already used for implementing packageJson so this should be relatively straightforward as the garbage collector should work asis if we follow the faast naming convention Currently with AWS using the packageJson option with faastjs causes a new Lambda Layer to be created that can be reused if the same packagejson is used by future functions only if useDependencyCaching is true which is the default The layers created for caching are deleted by garbage collection using the same retentionInDays value as other resources In practice this means that every h the packages will be reinstalled from scratch once which makes function creation slower than needed in most cases We could do one of several things to improve this Add a separate retention period to the options which is longer Say days Use a smarter caching algorithm to keep a small number of layers around For example deleting the layer with the oldest creation date after reaching a limit of say layers The limit could be configured Note that the longer the packages are cached the more likely there will be minor version updates that are missed On the other hand more frequent updates means longer lambda creation time and the possibility that code breaks because dependencies are quietly updated that doesnt fully respect semver Suggest going with Not sure what to do about the quiet update problem maybe faastjs should issue a note to the console 