Thanks for your sharing I have read your paper and I am confused about how to make sure the number of the kernels in conv operation In operationspy it seems like cincout but I wonder why I always think that NAS is to determine not only the TYPE of the operations but also the NUMBERs like the number of the kernels which makes it difficult I dont know if there is anything that I misunderstand Hi Genotype concatenation calculation and printing has some bug Printing During printing of genotype in search process the main function calls genotype function which is member function of class Network There is a line in genotype function concat range selfstepsselfmultiplier selfsteps This line produces and not which should have been the case So the line should be replaced with something like concat for i in range selfstepsselfmultiplier selfsteps concatappendi Calculation According to the paper only the feature maps of those nodes whose outputs are not used as inputs to any other nodes in the cell are concatenated to form the output of the cell But according to the code in genotype function in Network class it seems the code is blindly concatenating all internal nodes of the cell So according to me there should be a logic to check if top most probable operations doesnt have a node in any of the nodes of the cell then include the node in concatenation Does the above claims make sense Hi I want to ask if I want to apply it to the time series what should I modify I have modified the final output and loss functions but a few epochs later Nan will appear quark Hi I have some question about the base framework in Darts How did you choose the framework just follow NasNet I found that in this framework Relu is placed at the front end of each module before the conv rather than after it This arrangement might result in more relu operations and is not friendly to some hardware Im confused about this could you give me some explanation thank you The alpha variables value of avgpool and skipconnect is the mostly biggest over the others and none alpha is very big to when search the normalcell And it is the same with reducecell Hi I have tried to repeat the architecture search on CIFAR for times using exactly the same code with the following command python trainsearchpy unrolled But unfortunately none of the experiments generates the exact same cells published in the paper and every experiment produced different cell structure Any advice on why is that the case Thanks Thanks for your great work I find some inconsistency about the architecture parameters initialization between your paper and your code In your paper you said using zero initialization for architecture variables which implies equal amount attention over all possible ops However in your code you initialize the architecture variables randomly Variable e torchrandnk numopscuda requiresgradTrue in your code Could you please explain why you use this random initialization quark hi firstly thanks for your work Im really interested in Darts But I have some confusion about my reproduce result I found that the network I searched was too big compared to the network released in Darts The paramsize of my result model are generally M while released mode s is nearly M Could you tell me the reason why the results are different Or I just should continue to search for more networks Thanks In rnnutilspy The below code is meaning add a mask to embeding But it run error at torch Does anyone know to realize it at torch Thank you X embedbackendEmbeddingapplywords maskedembedweight paddingidx embedmaxnorm embednormtype embedscalegradbyfreq embedsparse 