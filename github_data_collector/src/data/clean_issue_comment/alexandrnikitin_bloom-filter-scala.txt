 Morton Filters Faster SpaceEfficient Cuckoo Filters via Biasing Compression and Decoupled Logical Sparsity by Alex D Breslow and Nuwan S Jayasena use the fact that after i i the right most of set bit of i will be unset ref Should calculate bitCount in combine Instantiating a simple bloom filter with the following command val expectedElements val falsePositiveRate Double val bf BloomFilter String expectedElements falsePositiveRate at the last line val bf I am getting the following error error runmain javalangNoClassDefFoundError scalaProductclass error javalangNoClassDefFoundError scalaProductclass error at bloomfilterCanGenerateHashFromCanGenerateHashFromStringinitCanGenerateHashFromscala error at bloomfilterCanGenerateHashFromCanGenerateHashFromStringclinitCanGenerateHashFromscala Im using the library with ApacheSpark specifically Im optimizing a sparse join by creating a bloom filter per partition this required wrapping the bloom filter instances with a class that implements finalaize If I hadnt done so Id get a massive memory leak cry the wrapper solved part of my problems but not all as Spark is unaware of the memory usage of the bloom filters which may lead to situations where it misses a memory spill sparks way of freeing up memory my thought is that implementing UnsafeBitArray in terms of a plain Java long array Long should not harm capabilities or performance of this class while still allowing for proper GC of these objects I think an even further possibility is using multiple arrays to avoid huge allocations JVM heap like many other memory allocators suffers both from tiny and huge allocations each having its own merits alexandrnikitin what do you think does it worth a benchmark perhaps in a separate branch to avoid complexities of side by side support for both code paths Guava already has this as part of its interface Hi currently the BloomFilter and its buffer are not serializable can this be added same goes for kryo support can be added with an optional dependency Ive actually already implemented this as part of my spark application another related issue the provided tofrom stream methods work one long at a time havent really benchmarked this but my intuition is that this is slowwwww I suggest modify the current methods or add new ones that takesallocates a buffer and leverage unsafes ability to copy memory ranges spark tungstens style will you be willing to accept pull requests thanks Eyal 