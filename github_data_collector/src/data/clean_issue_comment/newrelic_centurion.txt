Currently I just see how to set the network mode Is it possible to add the container to some networks with it as well So something that would result in parameters like netbatman netbanana Currently when a rolling deploy errors during the process of stopping an existing container and spinning up the new one the deploy stops and exits by raising that error The longer the list of servers youre deploying to the more of a pain it is to determine which servers did and did not successfully deploy and youre left in an inconsistent state This PR adds an option rollingdeployondockerfailure that defaults to exit which preserves the existing behavior When set to continue Centurion will try to deploy to every host on its list and keep a running collection of the errors it encounters along the way When all the servers are done it will raise a single error with a concatenation of all the error messages it encountered This should Ensure hosts that are healthy at the time of deploy get deployed to regardless of the health of other hosts in the list Make it easier to see at the end of the deploy failure which hosts are unhealthy and still running the old container The deployrepair task is not only undocumented but uses a naive invocation of httpstatusok unless methodhttpstatusokcallserver port fetchstatusendpoint Users with custom health checks cant avail themselves of this useful task This resolves merge conflicts in in preparation for merge in This is a Good Thing when were talking about rollingdeploy but a Bad Thing when were talking about deploy because it causes the service to be down longer than necessary The deploy task call stopcontainers from a DockerHostGroupeachinparallel call instead Relates to though that approach should not be used cc johannespetzold ean Hi Im trying to deploy using an private dockerregistry dockerdistribution Have I misconfigured my registry or is there simply no support for v api yet centurion p myApp e production Invoke environmentproduction firsttime Invoke environmentcommon firsttime Execute environmentcommon Execute environmentproduction Invoke centurionsetup firsttime Invoke centurioncleanenvironment firsttime Execute centurioncleanenvironment Execute centurionsetup Invoke list firsttime Execute list Invoke listtags firsttime Execute listtags GET E T ERROR Couldnt communicate with Registry ExconResponse x d databodyhtml r nheadtitle Not Foundtitlehead r nbody bgcolor white r ncenterh Not Foundh center r nhrcenternginx center r nbody r nhtml r n headersServernginx DateMon Sep GMT ContentTypetexthtml ContentLength Connectionkeepalive status statuslineHTTP Not Found r n reasonphraseNot Found bodyhtml r nheadtitle Not Foundtitlehead r nbody bgcolor white r ncenterh Not Foundh center r nhrcenternginx center r nbody r nhtml r n headersServernginx DateMon Sep GMT ContentTypetexthtml ContentLength Connectionkeepalive status remoteipxxxx localport localaddressxxxx Note If i do try to replace the v to a v and visit the link manually I do indeed get to the place i should be instead of a Some saas analytics platforms like New Relic have an API that can be called during deployment of new versions I think it would be cool if centurion had support for something like this Maybe just a method that could be overridden to make the API call at start or end of deployment We need to override the deployment stages to allow centurion to enable controlled container shutdown during a deploy For example we have a service container that is running a set of background tasks On a deploy we need centurion to notify the service that a shutdown is requested Then centurion should monitor a status endpoint until the service indicates that all background processes have terminated At that time centurion will be clear to bring down the container and deploy the new image Centurions rollingdeploy action iterates over the hosts for each it kills the old container starts a new one then waits for the new one to successfully healthcheck before moving on With this paradigm you clearly need to have at least two hosts running something to maintain uptime but if the second host is already down for some reason having only two instances means deploying or running a migration command will blindly take your only other instance down Rolling deploy and also anywhere we pick a host to do something on ought to look for any down hosts and deploy to them first In my rake file I would like to not specify the hostport value even if my container exposes a port Is this possible When I leave out that line I get the following error I T INFO Connecting to Docker on blddocker Userspairingrvmgemsruby p gemscenturion libcenturiondeploydslrb in publicportfor undefined method values for nilNilClass NoMethodError from Userspairingrvmgemsruby p gemscenturion libcenturiondeployrb in stopcontainers from Userspairingrvmgemsruby p gemscenturion libtasksdeployrake in block levels in top required from Userspairingrvmgemsruby p gemscenturion libcenturiondeploydslrb in call from Userspairingrvmgemsruby p gemscenturion libcenturiondeploydslrb in block levels in oneachdockerhost from Userspairingrvmgemsruby p gemscenturion libcenturiondockerservergrouprb in call from Userspairingrvmgemsruby p gemscenturion libcenturiondockerservergrouprb in block in each from Userspairingrvmgemsruby p gemscenturion libcenturiondockerservergrouprb in each from Userspairingrvmgemsruby p gemscenturion libcenturiondockerservergrouprb in each from Userspairingrvmgemsruby p gemscenturion libcenturiondeploydslrb in block in oneachdockerhost from Userspairingrvmgemsruby p gemscenturion libcenturiondeploydslrb in tap from Userspairingrvmgemsruby p gemscenturion libcenturiondeploydslrb in oneachdockerhost from Userspairingrvmgemsruby p gemscenturion libtasksdeployrake in block levels in top required from Userspairingrvmrubiesruby p libruby raketaskrb in call from Userspairingrvmrubiesruby p libruby raketaskrb in block in execute from Userspairingrvmrubiesruby p libruby raketaskrb in each from Userspairingrvmrubiesruby p libruby raketaskrb in execute from Userspairingrvmrubiesruby p libruby raketaskrb in block in invokewithcallchain from Userspairingrvmrubiesruby p libruby monitorrb in monsynchronize from Userspairingrvmrubiesruby p libruby raketaskrb in invokewithcallchain from Userspairingrvmrubiesruby p libruby raketaskrb in invoke from Userspairingrvmgemsruby p gemscenturion libcapistranodslrb in invoke from Userspairingrvmgemsruby p gemscenturion libtasksdeployrake in block in top required from Userspairingrvmrubiesruby p libruby raketaskrb in call from Userspairingrvmrubiesruby p libruby raketaskrb in block in execute from Userspairingrvmrubiesruby p libruby raketaskrb in each from Userspairingrvmrubiesruby p libruby raketaskrb in execute from Userspairingrvmrubiesruby p libruby raketaskrb in block in invokewithcallchain from Userspairingrvmrubiesruby p libruby monitorrb in monsynchronize from Userspairingrvmrubiesruby p libruby raketaskrb in invokewithcallchain from Userspairingrvmrubiesruby p libruby raketaskrb in invoke from Userspairingrvmgemsruby p gemscenturion libcapistranodslrb in invoke from Userspairingrvmgemsruby p gemscenturion bincenturion in top required from Userspairingrvmgemsruby p bincenturion in load from Userspairingrvmgemsruby p bincenturion in main from Userspairingrvmgemsruby p binrubyexecutablehooks in eval from Userspairingrvmgemsruby p binrubyexecutablehooks in main 