LTO cannot be checked unless the language is specified with the project option Move the check to reenable LTO Fixes f d ec Fixing issue I noticed this by accident while reviewing users of the CMAKEINTERPROCEDURALOPTIMIZATION option No need to merge for now I am here due to a general mild confusion due to the lack of architecturaldesign documentation Hopefully the outcome of discussion here will be better documentation and perhaps even better implementation Exhibit A simdjson streams Let the code speak Questions are in the comments cpp include simdjsonh include simdjsoncpp int parsefileconst char filename paddedstring copies the json string taken from a file into the memory true after this line p contains potentially very large amount of heap alocated storage right simdjsonpaddedstring p simdjsongetcorpusfilename ParsedJson seems to be what the Tape is it is a final product of json parsing also staying in the memory thus it will contain a very large amount of heap allocated storage true thus size of the heap taken in this function will be minimum twice the size of the file read in True simdjsonParsedJson pj simdjsonJsonStream jsp int parseres simdjsonSUCCESSANDHASMORE while parseres simdjsonSUCCESSANDHASMORE I fail to see how is JsonStream helping here it seems stream vs tape interaction can be encapsulted into the ParsedJson parseres jsjsonparsepj Do something with pj Talking about API in general I fail to see what might be wrong with the following refactoring of the above beside the fact huge amounts might be allocated on the heap leading to excessive paging cpp include simdjsonh include simdjsoncpp int parsefileconst char filename refactored into standard container simdjsonParsedJson parsedjson filename thus one can do standard C range for example for auto domiterator parsedjson domiterator type is simdjsonParsedJsonIterator refactored into standard C bidirectional iterator thus one can use std alorigthms for example auto firstimage stdfind parsedjsonbegin parsedjsonend string const key auto value if key Image return true else return false made me think how does simdjson handle white spaces in the input with C raw string literals one can easily introduce white spaces in the json input We divide documents into small eg kB chunks The document is copied in units of say kB where it may be padded as needed We run stage over it then stage Then we continue parsing with the next block Better yet stage can do the copy since it is reading the data in any case This would solve and might improve performance over large documents Reference How fast can you allocate a large block of memory in C We are currently somewhat stuck with respect to how we might support efficiently large files issues and others I feel that it is currently blocking psychologically a move to a wider tape because I am scared of the unforeseen and costly consequences There are nontrivial issues with respect to memory usage when the files get large Ideally youd want to have small stage building cachefitting indexes and work from there But it is hard when our stage is a monolith hard to change without significant performance penalties Instead of working harder maybe we should be working smarter and simply move to more flexible code Furthermore if you have very large files you probably do not want to materialize our kind of DOM a tape It is more likely that you will be want to eat up the data into your own favorite data structure The issue has to do with how tightly integrated everything in our code is So it is not easy or even possible to handle files of different size differently The tape production should be just one possible output We should allow and encourage users to provide their own consumer A consumer would have to implement methods such as these ones C return true if you want to continue false to stop bool founddoubledouble d bool foundint int t d bool founduint uint t d bool foundstartobject bool foundendobject bool foundstartarray bool foundendarray bool foundtrue bool foundfalse bool foundnull void foundendofdocument void founderrorint errorcode gives me a pointer to an available buffer where I could write up to budget bytes return null if unavailable char stringbuffersizet budget I wrote a string of length len on the string buffer bool foundstringsizet len So the structuralparser that John jkeiser wrote and that currently handles what we call stage would become a template where every instance of writetapeand similar functions would become a call to the generic consumer So we would separate the production of the tape from stage If done right with a template and proper inlining reallyinline there should be no performance penalty over our current code So the byte tape we have right now would become our first consumer Regarding the wider tape it makes it a much safer choice Indeed we could keep the byte tape and construct a new consumer that would be a byte tape Regarding UTF validation I would give the user a choice as a compiletime templated parameter to do it as part of stage If the user does not want to do UTF validation in stage then they become responsible for doing it in their consumer In practice the consumer could write to a string buffer and then validate this string buffer once We could make it easy with a clean API for the users to do this work We also make it a bit easier to add support for fancy features like large integers The consumer can have options we could have a bool supportsbigint const functions that activates only if bigint are needed This makes it possible to add features without necessarily having to include it into our tape and default consumers By itself this does not solve the large document problems However it modularize the problem and gives us a fighting chance Eg currently the only way we could support GB documents is by changing the tape structure This makes it a daunting task Now we could say that we have consumers that work only regular JSON documents and have special consumers that scale better But how can we ever crack the large document problem I think that the bulk of the problem resides in this one function that John jkeiser wrote C char advancechar idx pjstructuralindexes i c buf idx return c For the most part structuralparser only needs c and idx as well as access to buf But exactly how advancechar does it work the rest of the code does not need to care This means that structuralparser could take another templated parameter that would provide this function char advancechar and little else This means in particular that if I want to do stage into chunks I could C char advancechar ifi pjnstructuralindexes if I am running out of indexes refresh call stage over a new block of data somehow i idx pjstructuralindexes i c buf idx return c Furthermore currently pjstructuralindexes is a bit array which limits us to GB documents but obvious if advancechar can be replaced then we can have a large file provider of some kind Comments and pull requests invited Here is the first sketch of a proposal to have a wider and flatter byte tape up from our current hybrid byte tape This is issue In this new tape all nodes would be represented with a single byte value Except for scopes arrays and objects that would contain two byte values The tape could be iterated efficiently in either order forward and backward The tape would have an accompanying string tape as currently but the string tape would contain only UTF string content the strings would be appended one after the other Currently the string tape contains bit lengthprefixed strings In the new model the string length would be part of the tape Having exactly byte values all the way ensures that we could navigate the document backward and forward without any problem Issue It can also improve performance when navigating in forward order since there is no possible misprediction when loading the next tape element We would support arbitrarily large JSON documents containing arbitrarily large strings The new string tape would be made of just valid UTF string content so we could do UTF validation there instead of doing it in stage as is done currently This could potentially improve the performance drastically This is issue General formal of the tape elements We would reserve a byte out of each tape element for an ASCII character identifying the nature of the node t f n l u d r where r stands for root Simple JSON values Simple JSON nodes are represented with one tape element containing the type n t f and nothing else This is massively wasteful of memory since we use one byte out of But it is not clear what else to do without wasting performance Both bit ARM and x can turn a byte write into a single instruction if the write to the tape is a constant value Integer and Double values Numbers are already represented as byte values on the tape so that they would not change Integer values are represented as two bit tape elements The bit value l followed by the bit integer value litterally Integer values are assumed to be signed bit values using twos complement notation The bit value u followed by the bit integer value litterally Integer values are assumed to be unsigned bit values Float values are represented as two bit tape elements The bit value d followed by the bit double value litterally in standard IEEE notation We have bytes of free space but it is not clear what to do with it There are demands to support big integers and arbitraryprecision values We could engineer a special secondary tape where such values are coded and we could refer to this offtape value Performance consideration We store numbers of the main tape because we believe that locality of reference is helpful for performance Root node Each JSON document will have two special tape elements representing a root node one at the beginning and one at the end The first byte tape element contains the marker value r and the location on the tape of the last root element as a bit value The last byte tape element contains the value r All of the parsed document is located between these two tape elements It might be possible to use the extra space leftover to store other useful information Hint We can read the first tape element to determine the length of the tape Strings We store string values using UTF encoding with null termination on a separate tape A string value is represented on the main tape as the byte tape element with the marker and a bit pointer to the string tape We use the remaining bytes to store the length of the string Thus we would limit strings within JSON documents to bytes but thats truly enormous Short strings proposal We would introduce a short string type It would be made of short strings containing fewer than bytes and not nullterminated We would use null padding The benefit of these short strings is that they would fit in the main tape so that they could drastically improve the performance when querying the documents Many JSON documents are filled with short strings Arrays JSON arrays are represented using two byte tape elements The first byte tape element contains the value marker We store the number of elements in the array We store a pointer to second byte tape element The second byte tape element contains the value We store the number of elements in the array e a pointer to first byte tape element All the content of the array is located between these two tape elements including arrays and objects Knowing up front how many elements are in the array would solve issue Explanation we need a first and last tape element if we are to be able to navigate the document in backward order Performance consideration We can skip the content of an array entirely by accessing the first tape element reading the payload and moving to the corresponding index on the tape Objects JSON objects are represented using two byte tape elements The first byte tape element contains the marker value We store the number of keys in the object We store a pointer to second byte tape element The second byte t tape element contains the marker value We store the number of keys in the object We store a pointer to first byte tape element Inbetween these two tape elements we alternate between key which must be strings and values A value could be an object or an array All the content of the object is located between these two tape elements including arrays and objects Performance consideration We can skip the content of an object entirely by accessing the first tape element reading the payload and moving to the corresponding index on the tape Tradeoffs A byte tape would be able to support very large files whereas our byte tape is limited to GB files However somewhat ironically a byte tape might use more memory and thus make page allocation more expensive a bottleneck when processing large files Doubling the size of the tape would make page allocation more expensive and could make the processing of large files much more expensive On some systems memory allocation runs far slower than we can parse eg GBs especially when using small pages cc jkeiser This code moves the structural indexing from stage to stage on the theory that stage already has a loop that runs once per structural so this is less taxing there At the moment the combined stages run about slower thus engineering research Stage Stores only structural bits in stage removing the highly branchy index gathering Stage Gathers one index per ADVANCE in stage This is now done with functions instead of definestrying to remove those where it makes sense Empty Blocks We do not store empty masks for byte blocks devoid of structural characters so that stage can be assured of having a bit to process on every iteration To compensate for this stage detects when these bit blank spots happen in the three places they can happen spaces if the document contains one or more byte blocks full of spaces or the tail of a numberstringliteral plus spaces filling a block the next offset will always land on a space This should be exceedingly rare Each time we read a structural we check for spaces increment the offset by and retry ED now that I say this out loud Im pretty sure its not true If a block consists of string characters an end quote and a space the block will have no structurals but the next block may have a structural that lands on a string character which could be anything Posting anyway because a some of the todos may fix it and b even negative results have useful learnings strings if a string contains one or more byte blocks we adjust the offset after parsing the string numbers if a number contains one or more byte blocks filled with digitssignifiers we adjust the offset after parsing the number NOTE we dont actually do this yet its a TODO Strings we return the end offset of the string so the caller can adjust the offset Further experiments Write out the base index of each block in stage and dont do any compensation in stage Write out structural masks in stage and amortize the cost somehow Adjust structural offsets based on the end index of each value before doing any space adjustments Write out an structural end character mask along with structural masks and maybe special characters like for string and and e for numbers in an end structural mask Move UTF validation to the string tape to see if it improves stage enough to compensate theory being that a branchless stage will have disproportionate benefits Do string and number index adjustment inside the string parsing algorithm possibly changing the algorithm to run on block boundaries to match up better Dont have two stages pass the structurals list directly on to stage for each block series of blocks It would helpful when an iterator is at an int or double value to have an option to parse it as a string This allows the consumer to decide how to do the parsing it is helpful for me for dealing with the BigInt class I can hack up something good enough for myself but might be useful for others to have an officially supported way Currently stage is mostly a bulky function Moreover it can only process a whole JSON document or nothing In PR it was rewritten using a functions and an objectoriented approach but with a significant performance penalty This new PR tries to simplify stage and add the ability to process only part of a JSON document with theoretically the ability to resume parsing the rest of the document There are many applications of such an approach One is parallelization as in PR another one is to do away with the need for whole string padding The idea here is that you can stop parsing before the beginning of an array or an object So there are only two states to keep track off either you are at the start of the array or at the start of an object This PR has a small performance penalty However it may be a penalty that we are willing to pay for the improved flexibility This PR is deliberately minimalist instead of doing massive changes I try to change as little as possible Comments are invited cc jkeiser piotte pauldreik This is a workinprogress version of simdjson which can parse large documents in a multithreaded fashion There is an interleave of stage and stage The expectation is that we should be able to boost the parsing speed of large files by while using cores This would solve I brought it here from a fork piotte did It is not clean and not approved by J r mie but I wanted to start discussion around it early The stage has been reengineered to be more flexible Basically you can run stage over pieces of a JSON document not just entire JSON documents This implies that we must be able to persist the current state and resume where we were eg in the middle of an object Doing so has other engineering benefits For example it should allow us to avoid having to pad the inputs solving We could also allow the user to just parse the documents in small pieces Besides the fact that this PR is not clean there are still unresolved performance issues The expectation is that the more flexible stage should run at the same speed We worked hard to keep the fancy computed gotos Sadly the singlethreaded can be far worse For large files this is fine because the second thread more than compensate but for small files we cant use the new approach without a substantial penalty The new stage should be identical in computation cost except for one additional comparison and conditional jump per structural element We can expect that this comparison and condition jump should be highly predictable and effectively free So something else is hurting us Unfortunately the new code is quite different from the old code in structure so a straightforward comparison is not trivial I expect that piotte might come around clean and report When he does this PR will become obsolete Cheers Daniel