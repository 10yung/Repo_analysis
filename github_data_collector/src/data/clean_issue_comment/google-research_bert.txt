 Hi I have a few questions about my development Whats the primary difference between pretraining from pretrained checkpoints and finetuning the models How much data do I need to pretrain the model What will be the format of the custom corpus will it be a text file containing sequences per line Could you suggest any other way how can I finetune the BERT with the unsupervised dataset no label dataset an only text file containing sequences What trainbatchsize do I need to set while pretraining when I am setting maxseqlength I have been using bert embeddings from the pretrained bertbaseuncased for various downstream tasks When i try to understand the theory of how it is contextual how the other tokens in a sentence play a role in determining the meaning or context of every individual token the learning is quite clear But when I think of it as a single token A sentence of a single token Questions like what has the model really learnt when it looks at it standalone arise I have been testing it and the results are quite misleading I dont want to use different techniques for same task but at different granularity level ie single term ngrams sentences Coding is more similar to killing than it is to programming Can one help me understand this discrepancy and is it expected jacobdevlingoogle hsm img width altScreenshot at AM src trainInputExamples trainapplylambda x bertrunclassifierInputExampleguidNone Globally unique ID for bookkeeping unused in this example texta x DATACOLUMN textb None label x LABELCOLUMN axis I get the below error trainInputExamples trainapplylambda x bertrunclassifierInputExampleguidNone Globally unique ID for bookkeeping unused in this example AttributeError module bert has no attribute runclassifier occurred at index why not train all tokens can someone give a straight answer or point me another pdf where it came from at first place I want to pretrain BERT on Urdu There are a few things which I would like to know BERT multilingual model does not contain all the tokenstokenized words in vocabtxt So if I want to pretrain do I need to enhance the vocabulary If so Then how do I enhance the vocabulary Replace other language words with my language words or some other technique Please explain One more thing if I run pretraining on a large corpus of data without changing the vocab Will my model perform any better I am using flask to call BERT model to extract features for my sentences and prediction on top of that embeddings for simple classification I am using threadedTrue in flask but still for multiuserrequest say its taking almost minutes for sentences on GB windows CPU It can be flask server issue as well as it may or may not support multi threading Is there any thing possible from BERT end to allow parallelism to make response faster on Windows CPU I am using BERT embeddings and using them to classify my basic Keras model But every time I pass in my sentences its loading the entire BERT graph which is making it too slow for for my prediction service Is there any circumvention to this problem Time taken to extract sentences embedding minutes Approx Arch GB RAM WindowsCPU I read the code found that in the comments tokentypeids tfconstant But I think tokentypeids can only be or am I wrong CONTRIBUTINGmd 