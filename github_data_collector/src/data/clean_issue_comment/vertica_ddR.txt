I think it make sense to extend S method as instead of using asdlist asdarray and so on Correct me if I missed something Looking to forkdriverR we can see that dmapply is essentially mcmapply So we rely on the fact that every object can be easily lazily copied from master to worker process But we are missing the fact that it is possible that elements of dlist can be objects which keep some data out of Rs heap in external pointers I read the article dmapply A functional primitive to express distributed machine learning algorithms in R and found following diagram img width altscreen shot at src That was surprise to see peertopeer communication I started to investigated to ddR code and found following ddRRL L So essentially all communications go through master Do I miss something or this is just misleading diagram Im not that experienced with snow clusters Can peertopeer communication be potentially done on parallel package framework I am running into a memory issue with drandomForest I receive this message which we have all seen before Error in drandomForestdefaultm y ntree ntree nExecutor nExecutor cannot allocate vector of size Gb function call treeresult drandomForest Target dataasdataframesignalDF mtrypredictors nExecutor Executing on a bit machine with Gb memory predictors and the the dataset contains million rows The size of the data is slightly over GB We are using the function in classification mode y is set as a factor Predictors are a combination of numeric vars and factors Please advise Thanks This is a question more than an issue I would like to create a new backend using Service Fabric Stateful services can you lead me to some examples or at least where in source I should be looking for the interfaces I need to implement to create my own backend Thanks Probably the most common operation in R is subsetting ie and While trying to use ddR and looking at opsR I notice that the subsetting operators all collect How would one return a distributed object from subsetting Ie remove of the rows of a dframe that contain NA The resulting dframe will still be large so its best to keep it distributed One idea is to have DObjects closed under subsetting and leave it to the user call collect explicitly a asdframeiris colnamesa SepalLength SepalWidth PetalLength PetalWidth Species aSpecies Species NA Levels setosa versicolor virginica irisSpecies setosa setosa setosa setosa setosa setosa setosa setosa setosa setosa setosa setosa setosa setosa setosa setosa setosa setosa Probably the best way to fix this is to include dframes in OO model as described in the design section of the wiki This is too slow systemtime colnamesds user system elapsed Here ds is a a dframe with partitions Each chunk is about million rows and columns Running on a Macbook pro with a FORK cluster Following up on here are some ideas for improvements to ddR x Makefile to automate testing Doc build and examples See x Overhaul init and useBackend methods See x Internal documentation on ddRs programming model and how to write backends See ddR wiki Rewrite examples for clarity reproducibility and best practices Examples of reading processing writing actual data Benchmarks for various backends from dselivanov Simplifying dodmapply Set up continuous integration on Travis or similar service Probably requires admin access to repo Making distributed objects act more like their local counterparts through more OO code The changes below might require more conversation since I dont know the reasons behind the design decisions Allow partitioning dataframes only on rows Make ddR column major order like R Change name from arrays to matrices The changes in brought this bug out Global variables are not exported to a PSOCK cluster This causes the kmeans example to fail A minimal example libraryddR globvar f functionx globvar useBackendparallel typeFORK Works fine dl dlist letters collectdlapplydl f useBackendparallel typePSOCK Fails to find globvar dl dlist letters collectdlapplydl f So I think we should make a call for how ddR should work for portability Heres what I see as the options Only allow pure functions The simplest approach Add a parameter to pass an environment where the function is to be evaluated Supported by Spark and parallel backends Programmatically gather function dependencies from the code SparkR does this Right now is the most appealing because its clear whats happening would be not enough for example I often compose a large function out of several small functions is appealing but is significantly more complex 