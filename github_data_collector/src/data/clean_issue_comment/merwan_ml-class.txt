Your code for gradient descent for multivariate regression is not vectorized Heres a vectorized implementation function theta Jhistory gradientDescentMultiX y theta alpha numiters GRADIENTDESCENTMULTI Performs gradient descent to learn theta theta GRADIENTDESCENTMULTIx y theta alpha numiters updates theta by taking numiters gradient steps with learning rate alpha Initialize some useful values m lengthy number of training examples Jhistory zerosnumiters sum zerossizeX for iter numiters YOUR CODE HERE Instructions Perform a single gradient step on the parameter vector theta Hint While debugging it can be useful to print out the values of the cost function computeCostMulti and gradient here notsumXthetay sum alphamsumnotsumX thetathetasum Save the cost J in every iteration Jhistoryiter computeCostMultiX y theta end end The size and number of bedrooms should be scaled with the previously computed mean mu and standard deviation sigma From Andrew Ngs exercise sheet Implementation Note When normalizing the features it is important to store the values used for normalization the mean value and the stan dard deviation used for the computations After learning the parameters from the model we often want to predict the prices of houses we have not seen before Given a new x value living room area and number of bed rooms we must rst normalize x using the mean and standard deviation that we had previously computed from the training set In week exercise validationCurvem could you explain why your code works and this doesnt for i lengthlambdavec lambda lambdaveci theta trainLinearRegX y lambda errortraini linearRegCostFunctionX y theta lambda errorvali linearRegCostFunctionXval yval theta lambda end The above snippet makes perfect sense to me but it isnt correct sir id like to know the paper you have been following which refers to the equations in the comments in svmtraimm file could you pls send the link for it email mail rohillagmailcom 