Fixes #4257 (again) since the previous fixes were not complete.

Fixes #4307.
fix a typo

Fixes #

## Proposed Changes

  -
  -
  -

## Any background context you want to provide?

As documented in #4257 
https://github.com/STEllAR-GROUP/hpx/pull/4257#issuecomment-575370006
and 
https://github.com/STEllAR-GROUP/hpx/pull/4257#issuecomment-575547782
I would like to use apex with user MPI driven jobs. I have modified the init of APEX to use MPI with an experimental (not cleaned up) version looking like 
```
#ifdef HPX_HAVE_APEX
#if !defined(HPX_HAVE_NETWORKING) && defined(HPX_HAVE_LIB_MPI)
        int rank_=0, size_=1, is_initialized_=0;
        // Check if MPI_Init has been called previously
        MPI_Initialized(&is_initialized_);
        if(is_initialized_) {
            MPI_Comm_rank(MPI_COMM_WORLD, &rank_);
            MPI_Comm_size(MPI_COMM_WORLD, &size_);
        }
        else {
            int required, provided;
            required = MPI_THREAD_MULTIPLE;
            MPI_Init_thread(0, nullptr, required, &provided);
            if (provided < MPI_THREAD_FUNNELED) {
                std::cout << "Your MPI installation doesn't allow multiple threads. Exiting.\n";
                std::terminate();
            }
            MPI_Comm_rank(MPI_COMM_WORLD, &rank_);
            MPI_Comm_size(MPI_COMM_WORLD, &size_);
            std::cout << "Initializing mpi on rank " << rank_ << " of " << size_ << std::endl;
        }
        std::cout << "Initializing instrumentation with rank " << rank_ << " of " << size_ << std::endl;
        util::external_timer::init(nullptr, rank_, size_);
#else
        util::external_timer::init(nullptr, hpx::get_locality_id(),
                hpx::get_initial_num_localities());
#endif
```
I have enabled debugging in OTF2 and I see output from my test
```
Initializing mpi on rank 0 of 2                                                                                                                                                               
Initializing instrumentation with rank 0 of 2                                                                                                                                                 
Initializing mpi on rank 1 of 2                                                                                                                                                               
Initializing instrumentation with rank 1 of 2                                                                                                                                                 
Rank 0 of 2.                                                                                                                                                                                  
Rank 1 of 2.                                                                                                                                                                                  
[OTF2] src/OTF2_Archive.c:93: Entering function 'OTF2_Archive_Open': OTF2_archive/APEX                                                                                                        
[OTF2] Active debug module(s): ARCHIVE READER ANCHOR_FILE COLLECTIVES POSIX SION SION_RANK_MAP SION_COLLECTIVES LOCKS                                                                         
[OTF2] Active debug module(s): ARCHIVE READER ANCHOR_FILE COLLECTIVES POSIX SION SION_RANK_MAP SION_COLLECTIVES LOCKS                                                                         
[OTF2] src/OTF2_Archive.c:93: Entering function 'OTF2_Archive_Open': OTF2_archive/APEX                                                                                                        
[OTF2] src/otf2_collectives.c:102: Entering function 'otf2_collectives_get_rank'                                                                                                              
[OTF2] src/otf2_collectives.c:102: Entering function 'otf2_collectives_get_rank'                                                                                                              
[OTF2] src/otf2_collectives.c:181: Entering function 'otf2_collectives_bcast': (nil), 0x7ffecb0440c0, 1, INT64, 0                                                                             
Rank 1 Fudged my_OTF2Bcast                                                                                                                                                                    
[OTF2] src/otf2_collectives.c:181: Entering function 'otf2_collectives_bcast': (nil), 0x7ffcbbf858f0, 1, INT64, 0                                                                             
Rank 0 Fudged my_OTF2Bcast                                                                                                                                                                    

Elapsed time: 0.0599711 seconds                                                                                                                                                               
Cores detected: 8                                                                                                                                                                             
Worker Threads observed: 4                                                                                                                                                                    
Available CPU time: 0.239884 seconds                                                                                                                                                          

Counter                                   : #samples | minimum |    mean  |  maximum |  stddev                                                                                                
------------------------------------------------------------------------------------------------                                                                                              
                    1 Minute Load average :        1      1.100      1.100      1.100      0.000                                                                                              
------------------------------------------------------------------------------------------------                                                                                              

Timer                                                : #calls  |    mean  |   total  |  % total  
------------------------------------------------------------------------------------------------
                                           <unknown> :        1      0.006      0.006      2.666
                                       Dataflow Task :        1      0.001      0.001      0.444
hpx::components::server::runtime_support::shutdow... :        1      0.006      0.006      2.650
hpx::components::server::runtime_support::load_co... :        1      0.006      0.006      2.484
hpx::components::server::runtime_support::call_sh... :        2      0.001      0.003      1.176
hpx::agas::server::primary_namespace::colocate_ac... :        2      0.000      0.000      0.083
hpx::actions::action<void (*)(hpx::components::se... :        2      0.000      0.000      0.129
                        async_launch_policy_dispatch :        3      0.000      0.000      0.073
                                           backtrace :        1      0.001      0.001      0.351
   hpx::parallel::execution::parallel_executor::post :        3      0.000      0.001      0.476
                                          run_helper :        1      0.010      0.010      3.988
                                           APEX Idle :                          0.205     85.480
------------------------------------------------------------------------------------------------
                                        Total timers : 18
Closing OTF2 event files...
Writing OTF2 definition files...
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/4294967296.def'
Writing OTF2 Global definition file...
[OTF2] src/otf2_collectives.c:102: Entering function 'otf2_collectives_get_rank'
Writing OTF2 Node information...
Writing OTF2 Communicators...
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/0.def'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/4294967297.def'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/1.def'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/4294967298.def'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/2.def'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/4294967299.def'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/3.def'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/4294967300.def'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/4.def'
[OTF2] src/otf2_collectives.c:102: Entering function 'otf2_collectives_get_rank'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/4294967300.evt'
Closing the archive...
[OTF2] src/otf2_collectives.c:102: Entering function 'otf2_collectives_get_rank'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX.otf2'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/4.evt'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/4294967299.evt'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/3.evt'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/4294967298.evt'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/2.evt'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/4294967297.evt'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/1.evt'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/4294967296.evt'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/0.evt'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX.def'
done.
```
but when loaded in vampir, I do not see "Dataflow Task" for example - there may be other tasks missing, but I didn't look for them. The trace shows two ranks, each with 4 threads, but not all tasks are present, or labelled incorrectly?
When I run the same job with mpi ranks =1, then I see the correct task names.

What other functions must I implement to get this to work?

This is a screenshot of the output from a mpi rank node run
```
rm -rf OTF2_archive && ~/apps/mpich/3.3.1/bin/mpiexec -n 1 bin/scheduler_priority_check_test                                                                        
Initializing mpi on rank 0 of 1                                                                                                                                                               
Initializing instrumentation with rank 0 of 1                                                                                                                                                 
Rank 0 of 1.                                                                                                                                                                                  
[OTF2] src/OTF2_Archive.c:93: Entering function 'OTF2_Archive_Open': OTF2_archive/APEX                                                                                                        
[OTF2] Active debug module(s): ARCHIVE READER ANCHOR_FILE COLLECTIVES POSIX SION SION_RANK_MAP SION_COLLECTIVES LOCKS                                                                         
[OTF2] src/otf2_collectives.c:102: Entering function 'otf2_collectives_get_rank'                                                                                                              
[OTF2] src/otf2_collectives.c:181: Entering function 'otf2_collectives_bcast': (nil), 0x7ffc9365bb50, 1, INT64, 0                                                                             
Rank 0 Fudged my_OTF2Bcast                                                                                                                                                                    

Elapsed time: 0.029733 seconds
Cores detected: 8
Worker Threads observed: 4
Available CPU time: 0.118932 seconds

Timer                                                : #calls  |    mean  |   total  |  % total  
------------------------------------------------------------------------------------------------
                                           <unknown> :        1      0.001      0.001      0.922
                                       Dataflow Task :        1      0.001      0.001      0.882
hpx::components::server::runtime_support::shutdow... :        1      0.001      0.001      0.840
hpx::components::server::runtime_support::load_co... :        1      0.006      0.006      5.153
hpx::components::server::runtime_support::call_sh... :        2      0.001      0.003      2.433
hpx::agas::server::primary_namespace::colocate_ac... :        2      0.000      0.000      0.152
hpx::actions::action<void (*)(hpx::components::se... :        2      0.000      0.000      0.248
                        async_launch_policy_dispatch :        3      0.000      0.000      0.144
                                           backtrace :        1      0.001      0.001      0.556
   hpx::parallel::execution::parallel_executor::post :        3      0.000      0.001      0.951
                                          run_helper :        1      0.010      0.010      8.519
                                           APEX Idle :                          0.094     79.198
------------------------------------------------------------------------------------------------
                                        Total timers : 18
Closing OTF2 event files...
Writing OTF2 definition files...
Writing OTF2 Global definition file...
[OTF2] src/otf2_collectives.c:102: Entering function 'otf2_collectives_get_rank'
Writing OTF2 Node information...
Writing OTF2 Communicators...
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/0.def'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/1.def'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/2.def'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/3.def'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/4.def'
Closing the archive...
[OTF2] src/otf2_collectives.c:102: Entering function 'otf2_collectives_get_rank'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX.otf2'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/4.evt'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/3.evt'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/2.evt'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/1.evt'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX/0.evt'
[OTF2] src/otf2_file_posix.c:141: File to open: 'OTF2_archive/APEX.def'
done.
```
![image](https://user-images.githubusercontent.com/941548/72601964-b8c1e500-3916-11ea-9cc4-240ae1ac676b.png)

and this is the vampir view of the two rank run, there is not DataFlow task (among others missing)
![image](https://user-images.githubusercontent.com/941548/72601846-7bf5ee00-3916-11ea-9401-050dec2e1805.png)


https://github.com/STEllAR-GROUP/hpx/blob/master/libs/errors/src/exception.cpp#L40

https://github.com/STEllAR-GROUP/hpx/blob/master/src/custom_exception_info.cpp#L51

Version 1.4.0
A user on IRC posted a request to bind tasks to threads, but when using the default executor, the schedulehint was not being taken from the executor, instead the default was being used.
This PR introduces `hpx::functional::tag_invoke` which is an implementation
of https://wg21.link/p1895. This is needed to implement https://wg21.link/p0443.
## Expected Behavior

I expected the `1d_stencil_7` example to scale well over threads in a single node. When adding workers to a constant size problem, I expect the run time to decrease. Ideally proportionally.

## Actual Behavior

It does not seem to scale well (strong scaling). Output on a cluster node:

```
# Localities,OS_Threads,Execution_Time_sec,Points_per_Partition,Partitions,Time_Steps
# 1,     1,     4.274528498, 10000,                1000,                 100
# 1,     2,     2.865365903, 10000,                1000,                 100
# 1,     4,     2.256500222, 10000,                1000,                 100
# 1,     8,     1.992941052, 10000,                1000,                 100
# 1,     16,    1.916695974, 10000,                1000,                 100
# 1,     32,    3.198471846, 10000,                1000,                 100
```

After a drop in the execution time, it even increases. In an experiment I ran, the parallel efficiency drops to well below 20%.

## Steps to Reproduce the Problem

  1. Build release build of HPX, with examples
  1. Download the attached [shell script](https://github.com/STEllAR-GROUP/hpx/files/4045605/run_stencil.txt). Edit it for the location of the HPX examples and possibly the number of workers
  1. Run the shell script. It will print execution times for different numbers of threads.

## Specifications

  - HPX Version: upcoming 1.4.0, commit d1c61de
  - Platform (compiler, OS): gcc-7.2.0, CentOS 7, node with two AMD EPYC 7451 24-Core Processors and 256 GB memory
  - [output of --hpx:dump-config](https://github.com/STEllAR-GROUP/hpx/files/4045554/dump-config.txt)
  - [output of --hpx:info](https://github.com/STEllAR-GROUP/hpx/files/4045556/info.txt)

## Notes and questions

- I spent quite some time trying to get my own programs to scale well and, so far, I have not been successful. Therefore I turned to the HPX examples to see whether they scale well.
- Is stencil_7 supposed to scale well?
- If so, what could be possible reasons when it doesn't yet?

Fixes #2997? Cleans up thread executors. Some are rewritten, some deprecated, and some are simply typedefed to other executors. The changes are not 100% backwards compatible (i.e. some APIs that existed using the old interface have been completely removed). I think this should be okay, but let's gather some feedback until the next release if some of them should be reintroduced.

Summary:

- Moved all executors to `hpx/parallel/executors` and left only forwarding headers and typedefs for the `hpx::threads::executors` namespace in `hpx/runtime/threads/executors`.
- `service_executor` rewritten using the the newer executor interface. The old `service_executor` claimed to block until all outstanding work was done, but this wasn't the case. I've made it non-blocking now since if someone relied on it they were very lucky, or else noone actually relied on that functionality. I'd add it back only if someone requests the functionality.
- `current_executor` is typedefed to `thread_pool_executor` (after some constructors were added to it). The nullary constructor just grabs the current thread pool and submits tasks to that.
- `pool_executor` inherits `thread_pool_executor`.
- `default_executor` is typedefed to `parallel_executor`.
- `embedded_thread_pool_executor` are deprecated behind a CMake option.
- `this_thread_executor` deprecated.
- `thread_pool_attached_executor` deprecated, with a replacement based on `thread_pool_executor` (`compute::host::block_executor` is now based on this replacement).
- `thread_pool_os_executors` deprecated.
## Expected Behavior

Given a system with a single clean OpenMPI installation an mpi hello world program finds a given Comm_size and Comm_rank. (for example 0/2 and 1/2 for mpirun -np 2)

We now compile HPX with MPI_Parcelport=ON with this same OpenMPI installation.
Then an HPX application run with mpirun -np 2 should find those same ranks and sizes as hpx::get_locality_id() and hpx::find_all_localities().size().

## Actual Behavior

All hpx::find_all_localities() method calls return vectors with size 1, no communication between localities is possile.


## Steps to Reproduce the Problem
A docker container containing a minimal HPX hello world setup is available at
docker://kilianwerner/hpxminimal

The used docker folder including all source and configuration is available at
https://github.com/KilianWerner/HPXMinimalDocker

Running on any host with singularity and openmpi version 4.0.1 the following commands 

sudo singularity build min.sif docker://kilianwerner/hpxminimal
mpirun -np 2 singularity exec min.sif /opt/mpitest

result in output:
Hello, I am rank 1/2Hello, I am rank 0/2

However
mpirun -np 2 singularity exec min.sif /home/myUser/MinimalExample/build/hpxmin

results in output:

Localities: 1Threads: 4Localities: 1Threads: 4

## Specifications
  - HPX Version: 1.3.0
  - Platform (compiler, OS): ubuntu:disco
  - OpenMPI Version: 4.0.1


## Comments:
Our work group often stumbles into the problem that hpx::find_all_localities().size() = 1 regardless of the mpi backend situation. Most of the times this is "solved" by changing to older versions of hpx, openmpi, use other mpi implementations (like intel mpi) and just generally swap things around until hpx localities find each other. 

As we are now trying to get a robust common solution working in a docker container, we face the problem again and need hpx to behave consistent with raw mpi applications in a robust manner.