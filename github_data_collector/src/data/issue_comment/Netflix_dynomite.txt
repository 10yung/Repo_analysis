This superseeds https://github.com/Netflix/dynomite/pull/576 as I cannot push to the original feature branch.
@axelfauvel if you are not ok with this, can you please merge the current state of dynomite in your branch, so I could pick from there?
@smukil I think I have all your remarks covered in the origanl PR. Hopefully we can take it from here.

This commit is based on
https://github.com/orange-cloudfoundry/dynomite/commit/7aa41a49773270d069b79e8a38e45c25ee1e0e09
from @axelfauvel in https://github.com/Netflix/dynomite/pull/576 and tries to
close https://github.com/Netflix/dynomite/issues/46.

Unfortunatelly the initial commit was already so old and the dynomite code base already evolved,
that it was easier to not jump directly on this. Especically as there were some refactorings
requested.

# Redis Datastore Authentification
If Dynomite is configured to require a password via config option `requirepass` the following
behaviour will be applied:

1. On Dynomite startup, the server authenticates with the backend itself
   by calling the datastore agnostic function g_datastore_auth.
2. The corresponding Redis response will be handeled in g_is_authenticated.
   Dynomite will exit if authentification to the datatstore was not successful.
3. Each newly created client connection will require authentification.
4. Clients can authentificate itself by issue the AUTH command against dynomite.
5. Dynomite will check the password and simulate an AUTH response.
6. If AUTH was successful, the auth_required flag on the connection is reset and
   the client can process further commands through this connection.
Dose  Dynomite  support all the redis versionï¼Ÿ Or Dynomite have no relate with the version of redis?
Hi All, I am new to DynomiteDB, As of now, I have set up two racks in the same DC. 

rack1 ( 3 nodes )
rack2 ( 3 nodes )

Dynomite DB is working good, I see replication is happening, Now my Question here is, How to setup Haproxy or Dyno.

How clients need to hit a single IP where that should take care of which rack is active and send traffic to that rack. 

can you explain how to setup Dyno step by step including config file and path? 
Can I use system environment variables instead of config files when starting dynomite?

I use it in k8s and docker.

There are many environment, and only one config file in docker image can be used when starting.

Then I want to use system environment variables instead of config file

for example there are previously 3 servers in a single rack, now I'm doing linear scalabiliy, scale UP, make it 6 server in this rack, I guess there must be some data migration happen from the previous 3 servers to the newly added server right? even could be very little keys, since consistent hashing is not perfect. So how does dynomite handle this case by design?
Dynomite log gets flooded with the following messages:

`[2019-10-17 17:29:41.494] client_unref_internal_try_put:95 <CONN_CLIENT 0x562116106bc0 -1 from 'localhost:'> unref owner <POOL 0x5621160f9510 'dyn_o_mite'>
[2019-10-17 17:29:41.498] client_unref_internal_try_put:95 <CONN_CLIENT 0x562116106bc0 -1 from 'localhost:'> unref owner <POOL 0x5621160f9510 'dyn_o_mite'>
[2019-10-17 17:29:45.002] client_unref_internal_try_put:95 <CONN_CLIENT 0x562116130730 -1 from 'localhost:'> unref owner <POOL 0x5621160f9510 'dyn_o_mite'>
[2019-10-17 17:29:45.002] client_unref_internal_try_put:95 <CONN_CLIENT 0x562116106bc0 -1 from 'localhost:'> unref owner <POOL 0x5621160f9510 'dyn_o_mite'>
[2019-10-17 17:29:45.662] client_unref_internal_try_put:95 <CONN_CLIENT 0x562116106bc0 -1 from 'localhost:'> unref owner <POOL 0x5621160f9510 'dyn_o_mite'>
[2019-10-17 17:29:45.666] client_unref_internal_try_put:95 <CONN_CLIENT 0x562116106bc0 -1 from 'localhost:'> unref owner <POOL 0x5621160f9510 'dyn_o_mite'>
[2019-10-17 17:29:45.739] client_unref_internal_try_put:95 <CONN_CLIENT 0x562116106bc0 -1 from 'localhost:'> unref owner <POOL 0x5621160f9510 'dyn_o_mite'>
[2019-10-17 17:29:45.741] client_unref_internal_try_put:95 <CONN_CLIENT 0x562116106bc0 -1 from 'localhost:'> unref owner <POOL 0x5621160f9510 'dyn_o_mite'>
[2019-10-17 17:29:46.497] client_unref_internal_try_put:95 <CONN_CLIENT 0x562116106bc0 -1 from 'localhost:'> unref owner <POOL 0x5621160f9510 'dyn_o_mite'>
[2019-10-17 17:29:46.501] client_unref_internal_try_put:95 <CONN_CLIENT 0x562116106bc0 -1 from 'localhost:'> unref owner <POOL 0x5621160f9510 'dyn_o_mite'>
[2019-10-17 17:29:50.003] client_unref_internal_try_put:95 <CONN_CLIENT 0x562116130730 -1 from 'localhost:'> unref owner <POOL 0x5621160f9510 'dyn_o_mite'>
[2019-10-17 17:29:50.003] client_unref_internal_try_put:95 <CONN_CLIENT 0x562116106bc0 -1 from 'localhost:'> unref owner <POOL 0x5621160f9510 'dyn_o_mite'>
[2019-10-17 17:29:50.034] client_unref_internal_try_put:95 <CONN_CLIENT 0x562116130730 -1 from 'localhost:'> unref owner <POOL 0x5621160f9510 'dyn_o_mite'>
[2019-10-17 17:29:50.034] client_unref_internal_try_put:95 <CONN_CLIENT 0x562116106bc0 -1 from 'localhost:'> unref owner <POOL 0x5621160f9510 'dyn_o_mite'>
[2019-10-17 17:29:50.665] client_unref_internal_try_put:95 <CONN_CLIENT 0x562116106bc0 -1 from 'localhost:'> unref owner <POOL 0x5621160f9510 'dyn_o_mite'>
[2019-10-17 17:29:50.668] client_unref_internal_try_put:95 <CONN_CLIENT 0x562116106bc0 -1 from 'localhost:'> unref owner <POOL 0x5621160f9510 'dyn_o_mite'>
[2019-10-17 17:29:50.739] client_unref_internal_try_put:95 <CONN_CLIENT 0x562116106bc0 -1 from 'localhost:'> unref owner <POOL 0x5621160f9510 'dyn_o_mite'>`

Almost everything works correctly, but we cannot set loglevel higher than 3



Dynomite version: v0.6.16 (but it happens also on 0.6.15)

Config:
dyn_o_mite:
  datacenter: dc
  rack: rack1
  dyn_listen: 10.5.3.211:8101
  dyn_seeds:
  - 10.5.4.212:8101:rack2:dc:1383429731
  - 10.5.6.213:8101:rack3:dc:1383429731
  listen: /var/run/dynomite/dynomite.sock
  servers:
   - /var/run/redis/redis.sock:1
  timeout: 30000
  tokens: '1383429731'
  secure_server_option: none
  data_store: 0
  write_consistency : DC_SAFE_QUORUM
  stats_listen: 0.0.0.0:22222
  mbuf_size: 16384
  max_msgs: 100000
  server_connections: 1
Hello there!
This is more a report than an issue, but here we go.
Until yesterday I was suffering with the memory leak mentioned in this pull request: https://github.com/Netflix/dynomite/pull/710. So I've upgraded 2 of my servers with **v0.6.16**, but after this my dynomite returns an empty string in the REST API (http://hostname:22222/info) and when I run `dynomite --version` also.

Here is an example:

![image](https://user-images.githubusercontent.com/5176702/66590136-76924600-eb66-11e9-87d6-266a9c93f037.png)

![image](https://user-images.githubusercontent.com/5176702/66590375-033d0400-eb67-11e9-951c-eb7db02a86a8.png)

In my second DC, where I did not upgrade dynomite yet, it appears like this:

![image](https://user-images.githubusercontent.com/5176702/66590554-662e9b00-eb67-11e9-83f7-8fef81d4b22c.png)


![image](https://user-images.githubusercontent.com/5176702/66590632-95450c80-eb67-11e9-9d0c-e0e374ac2f5e.png)

I did the upgrade just by running the `build.sh` script in release's root directory.
Dont know for sure if it is a real problem or just a build/compile mistake.

Gustavo

This took a while to figure out.

DC 1 is a VM on Azure.
DC 2 is an on-prem VM.

After a little while they got out of sync and connection timeouts occured.

This is due to the way Azure handles TCP connections. All VMs are NATted 1:1 and TCP connections are silently dropped (no TCP RST) after 4 minutes of inactivity.

Dynomite respects Linux tcp keepalive values. But these keepalive probes start after 4 hrs of inactivity. So the connection drops after 4 minutes and Dynomite gets a read timeout after 30 seconds and drops the packets.

To fix it I changed the tcp keepalive settings on the linux vm to the following:

```
net.ipv4.tcp_keepalive_time = 120
net.ipv4.tcp_keepalive_intvl = 30
net.ipv4.tcp_keepalive_probes = 8
```

This seems to have remediated the issue.
On remote host:
```
root@test-y:~/memtier/memtier_benchmark-1.2.17# redis-benchmark -h test-b -p 8102 -t ping
Error: Connection reset by peer
```
On dynomite host:
```
root@test-b:~/src/dynomite/dynomite# src/dynomite -c ../redis-node.yml
[2019-10-02 11:50:27.159] dn_print_run:206 dynomite-v0.6.15-6-g3b153a0 built for Linux 4.15.18-20-pve x86_64 started on pid 1083
[2019-10-02 11:50:27.159] dn_print_run:212 run, rabbit run / dig that hole, forget the sun / and when at last the work is done / don't sit down / it's time to dig another one
...

[2019-10-02 11:51:38.694] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2c6240 19 from '10.100.223.206:57668'>
[2019-10-02 11:51:38.695] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2d9d90 20 from '10.100.223.206:57670'>
[2019-10-02 11:51:38.697] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2da0a0 21 from '10.100.223.206:57672'>
[2019-10-02 11:51:38.698] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2da3d0 22 from '10.100.223.206:57674'>
[2019-10-02 11:51:38.699] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2da700 23 from '10.100.223.206:57676'>
[2019-10-02 11:51:38.700] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2daa30 24 from '10.100.223.206:57678'>
[2019-10-02 11:51:38.701] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2dad60 25 from '10.100.223.206:57680'>
[2019-10-02 11:51:38.702] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2db090 26 from '10.100.223.206:57682'>
[2019-10-02 11:51:38.703] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2db3c0 27 from '10.100.223.206:57684'>
[2019-10-02 11:51:38.704] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2db6f0 28 from '10.100.223.206:57686'>
[2019-10-02 11:51:38.706] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2dba20 29 from '10.100.223.206:57688'>
[2019-10-02 11:51:38.708] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2dbd50 30 from '10.100.223.206:57690'>
[2019-10-02 11:51:38.710] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2dc080 31 from '10.100.223.206:57692'>
[2019-10-02 11:51:38.711] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2dc3b0 32 from '10.100.223.206:57694'>
[2019-10-02 11:51:38.712] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2dc6e0 33 from '10.100.223.206:57696'>
[2019-10-02 11:51:38.712] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2dca10 34 from '10.100.223.206:57698'>
[2019-10-02 11:51:38.713] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2dcd40 35 from '10.100.223.206:57700'>
[2019-10-02 11:51:38.714] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2dd070 36 from '10.100.223.206:57702'>
[2019-10-02 11:51:38.714] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2dd3a0 37 from '10.100.223.206:57704'>
[2019-10-02 11:51:38.716] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2dd6d0 38 from '10.100.223.206:57706'>
[2019-10-02 11:51:38.717] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2dda00 39 from '10.100.223.206:57708'>
[2019-10-02 11:51:38.717] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2ddd30 40 from '10.100.223.206:57710'>
[2019-10-02 11:51:38.718] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2de060 41 from '10.100.223.206:57712'>
[2019-10-02 11:51:38.719] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2de390 42 from '10.100.223.206:57714'>
[2019-10-02 11:51:38.720] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2de6c0 43 from '10.100.223.206:57716'>
[2019-10-02 11:51:38.722] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2de9f0 44 from '10.100.223.206:57718'>
[2019-10-02 11:51:38.723] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2ded20 45 from '10.100.223.206:57720'>
[2019-10-02 11:51:38.724] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2df050 46 from '10.100.223.206:57722'>
[2019-10-02 11:51:38.725] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2df380 47 from '10.100.223.206:57724'>
[2019-10-02 11:51:38.727] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2df6b0 48 from '10.100.223.206:57726'>
[2019-10-02 11:51:38.728] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2df9e0 49 from '10.100.223.206:57728'>
[2019-10-02 11:51:38.728] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2dfd10 50 from '10.100.223.206:57730'>
[2019-10-02 11:51:38.729] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2e0040 51 from '10.100.223.206:57732'>
[2019-10-02 11:51:38.730] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2e0370 52 from '10.100.223.206:57734'>
[2019-10-02 11:51:38.731] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2e06a0 53 from '10.100.223.206:57736'>
[2019-10-02 11:51:38.732] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2e09d0 54 from '10.100.223.206:57738'>
[2019-10-02 11:51:38.733] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2e0d00 55 from '10.100.223.206:57740'>
[2019-10-02 11:51:38.733] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2e1030 56 from '10.100.223.206:57742'>
[2019-10-02 11:51:38.734] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2e1360 57 from '10.100.223.206:57744'>
[2019-10-02 11:51:38.735] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2e1690 58 from '10.100.223.206:57746'>
[2019-10-02 11:51:38.735] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2e19c0 59 from '10.100.223.206:57748'>
[2019-10-02 11:51:38.736] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2e1cf0 60 from '10.100.223.206:57750'>
[2019-10-02 11:51:38.736] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2e2020 61 from '10.100.223.206:57752'>
[2019-10-02 11:51:38.737] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2e2350 62 from '10.100.223.206:57754'>
[2019-10-02 11:51:38.737] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2e2680 63 from '10.100.223.206:57756'>
[2019-10-02 11:51:38.738] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2e29b0 64 from '10.100.223.206:57758'>
[2019-10-02 11:51:38.739] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2e2ce0 65 from '10.100.223.206:57760'>
[2019-10-02 11:51:38.740] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2e3010 66 from '10.100.223.206:57762'>
[2019-10-02 11:51:38.741] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2e3340 67 from '10.100.223.206:57764'>
[2019-10-02 11:51:38.742] proxy_accept:203 <CONN_PROXY 0x555a6f2c2190 13 listening on '0.0.0.0:8102'> accepted <CONN_CLIENT 0x555a6f2e3670 68 from '10.100.223.206:57766'>
[2019-10-02 11:51:38.742] req_server_enqueue_imsgq:913 Bad tailq NEXT(0x555a6f2b5dd8->tqh_last) != NULL
Aborted (core dumped)
```
Config:
```
root@test-b:~# cat src/dynomite/redis-node.yml 
dyn_o_mite:
  datacenter: dc
  rack: rack1
  dyn_listen: 0.0.0.0:8101
  dyn_seeds:
  - test-c:8101:rack2:dc:1383429731
  - test-d:8101:rack3:dc:1383429731
  listen: 0.0.0.0:8102
  servers:
  - 127.0.0.1:6379:1
  tokens: '1383429731'
  # secure_server_option: datacenter
  # pem_key_file: conf/dynomite.pem
  data_store: 0
  read_consistency : DC_SAFE_QUORUM
  write_consistency : DC_SAFE_QUORUM
  stats_listen: 0.0.0.0:22221
```

Hi,

Current setup is 2 DC, 1 rack with 1 node each. We are using 0.6.15 and we started noticing the following issue when there is a spike in activity in our service connected to dynomite/redis.

```
[2019-09-10 00:48:24.194] core_print_peer_status:62 0)0x20afd40 dc-us rack1 x NORMAL
[2019-09-10 00:48:24.196] core_print_peer_status:62 1)0x20aff80 dc-eu rack2 x
1:rack2:dc-eu:0 NORMAL

[2019-09-11 01:18:45.462] core_timeout:478 <REQ 0x21ed6a0 15466492:0::0 REQ_REDIS_HMSET, len:19
4> on <CONN_SERVER 0x20a5390 6 to '127.0.0.1:6379:1'> timedout, timeout was **2000**
[2019-09-11 01:18:45.596] core_close:419 close <CONN_SERVER 0x20a5390 6 to '127.0.0.1:6379:1'>
on event 00FF eof 0 done 0 rb 3518003933 sb 1009952142: Connection timed out
[2019-09-11 01:18:45.597] server_close:255 close <CONN_SERVER 0x20a5390 6 to '127.0.0.1:6379:1'
> Dropped 0 outqueue & 13 inqueue requests
[2019-09-11 01:18:45.597] event_del_conn:211 epoll ctl on e 0 sd 6 failed: No such file or dire
ctory
[2019-09-11 01:18:45.597] req_forward_to_peer:629 Failed to forward request to local datastore.
[2019-09-11 01:18:45.597] req_forward_to_peer:629 Failed to forward request to local datastore.
[2019-09-11 01:18:45.597] req_forward_to_peer:629 Failed to forward request to local datastore.
[2019-09-11 01:18:45.597] req_forward_to_peer:629 Failed to forward request to local datastore.
[2019-09-11 01:18:45.597] req_forward_to_peer:629 Failed to forward request to local datastore.
[2019-09-11 01:18:45.597] req_forward_to_peer:629 Failed to forward request to local datastore.
[2019-09-11 01:18:45.597] req_forward_to_peer:629 Failed to forward request to local datastore.
[2019-09-11 01:18:45.597] req_forward_to_peer:629 Failed to forward request to local datastore.
```

The timeout could be understandable, but not sure why the 'No such file or directory' error or the failed to forward request to local datastore which never recovers. This causes our service to crash.

Any help or clarification on this issue is appreciated.

