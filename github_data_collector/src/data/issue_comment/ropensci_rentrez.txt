https://github.com/ropensci/rentrez/blob/master/README.md#web_history-and-big-queries
While it is stated that 'When I wrote this that was a little over 200 000 SNPs', I have got the following messages:
```
> snp_search <- entrez_search(db="snp", 
+                             term="(Y[CHR] AND Homo[ORGN]) NOT 10001:2781479[CPOS]")
> snp_search
Entrez search result with 0 hits (object contains 0 IDs and no web_history object)
 Search term (as translated):  (Y[CHR] AND Homo[All Fields]) NOT 10001[CHRPOS] :  ... 
```

Attached are my R code and log file.
[Rcode.txt](https://github.com/ropensci/rentrez/files/4008284/Rcode.txt)
[log.txt](https://github.com/ropensci/rentrez/files/4008285/log.txt)
Hi rentrez developers,

I have a situation where i want to search for a taxa and specific gene, and only download a random subsample of these search results.
```
search_results <- entrez_search(db = "nuccore", term = "Drosophila[ORGN] AND COI", use_history = TRUE)
length(search_results$ids)
[1] 8832

subsample= 1000
ids_subsample <- sample(search_results$ids, subsample)
```
As a subsample of 1000 ids is too large to feed directly into entrez_fetch, the only way i see to handle this is to then use entrez_post to upload the subsampled ids in chunks, and entrez_fetch to then download the chunks.

```
ids <- sample(search_results$ids, subsample )
chunk_size <- 100
ids_chunked <- split(ids, ceiling(seq_along(ids)/chunk_size))

for (l in 1:length(ids_chunked)) {
  upload <- entrez_post(db="nuccore", id=ids_chunked[[l]])
  dl <- entrez_fetch(db = "nuccore", web_history = upload, rettype = "fasta", retmax = 10000)
  cat(dl, file="out.fa", append=TRUE)
}
```

However this is quite slow. Instead, the documentation for entrez_post seems to suggest that i should be able to append the ids to an existing web_history object, and then the entire web_history object could be downloaded a single entrez fetch call. I tried this with the below code, however in this case entrez_fetch only downloads the last chunk of 100 ids i uploaded:

```
#create new webhistory object
upload <- entrez_post(db="nuccore", id=ids_chunked[[1]])
#Add to web history object
for (l in 2:length(ids_chunked)) {
  upload <- entrez_post(db="nuccore", id=ids_chunked[[l]], web_history=upload)
}

dl <- entrez_fetch(db = "nuccore", web_history = upload, rettype = "fasta", retmax = 10000)
cat(dl, file="out.fa", append=FALSE) 
```
Do you have any input on what i am doing wrong here, or suggestions on better ways to do this (i.e. can i somehow subset the webhistory object directly on the NCBI server without having to post the ids again?)

Cheers,
Alex


Hello, 
I am new to R and also to NCBI database. I am trying out some simple searches to understand the package. I started with a simple search based on a name.

```
taxon_search = entrez_search(db="taxonomy", term="Spirometra erinaceieuropaei")
> taxon_search$ids
[1] "99802"
```
So far so good. Now I want to use this taxon id to see all the links in the database. I am not sure if this is the right query. Why do I get only one link? 
```
> all_the_links <- entrez_link(dbfrom='gene', id=99802, db='all')
> all_the_links$links
elink result with information from 1 databases:
[1] gene_gene_h3k4me3
```
If I start with a gene id, I get more links: 

```
> all_the_links <- entrez_link(dbfrom='gene', id=6446572, db='all')
> all_the_links$links
elink result with information from 17 databases:
 [1] gene_genome                gene_bioproject           
 [3] gene_cdd                   gene_gene_h3k4me3         
 [5] gene_gene_neighbors        gene_genome2              
 [7] gene_nuccore               gene_nuccore_pos          
 [9] gene_nucleotide            gene_nucleotide_pos       
[11] gene_pmc_nucleotide        gene_protein              
[13] gene_protein_refseq        gene_proteinclusters      
[15] gene_pubmed_pmc_nucleotide gene_sparcle              
[17] gene_taxonomy  
```
And from there I can find the taxon id. 
```
> all_the_links$links$gene_taxonomy
[1] "99802"
```

How do I find a link to [6446572](https://www.ncbi.nlm.nih.gov/gene/6446572) from the taxon search. 

thanks. 
--sharif 
I am trying to analyze around ~100,000 PubMed case reports and tried to get the data using entrez_fetch as shown below:
`for(i in seq(1,10000,25)){
  info <- entrez_fetch(db="pubmed", web_history=pubmed_search$web_history,
                       rettype="xml", retmax=25, retstart=i, parsed = TRUE)
  cat(saveXML(info), "\n", file="sample.xml", append=TRUE)
  cat(i+24, "sequences downloaded\r")
}`

This code snippet is very similar to the one found in the documentation. However, after a few thousand files, an error occurs and I discovered it's because the database won't allow me to download it - I think it's because it might be more than 3 requests per second. Does anyone know a workaround for this?
I'm trying to access a really large number of records (10,000 to be exact) and used the tutorial to try and attain this. So, I run the following code, first to save web history:

> pubmed_search <- entrez_search(db = "pubmed", term = "Case Reports[Filter] AND cardiovascular disease AND English[lang] AND 2009:2019[PDat])", retmax = 792711, use_history = TRUE)

Then, I try to download first 10,000 files:

> for( seq_start in seq(1,10000,100)){
>   recs <- entrez_summary(db="pubmed", web_history=pubmed_search$web_history,
>                        retmax=100, retstart=seq_start)
>   cat(seq_start+99, "sequences downloaded\r")
> }
> length(recs)

But, I only get 100 files, not 10,000. Can someone help with this, I'm quite confused here as to how to use the web history feature
This is required to be auto-detected by GitHub
When `api_key` is not provided, Entrez allows 3 requests per second, thus `sleep_time` should be at least (1/3) second. Rounding error caused it to wait for less than (1/3), resulting in _API rate limit exceeded_ error. Fix it into 0.34.
Hi,

I was wondering if anyone might have a suggestion to return the DOI for a given pubmed id in a robust manner?

I tried to use the id converter, but for a number of my ids the converter says the pubmed id is invalid.
For example, this id 26479441 is a valid pubmed id but the converter thinks it is invalid  https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/?tool=my_tool&email=my_email@example.com&ids=26479441

The closest that I have gotten is to use entrez_fetch and parse out the ELocationID, which if it exists will contain the DOI, but it might also contain other identifiers which I am not clear on how to exclude from the XML parsing, see example code below. 

I have (I believe) come across pubmed ids with DOIs that don't have the ELocationID xml tag.

Any pointers/tips would be greatly appreciated.

Iain

#example parsing from https://github.com/ropensci/rentrez/issues/100

library(rentrez)
library(XML)

return_doi<-function(pubmed_id){
entrez_xml <- entrez_fetch(db="pubmed", id=pubmed_id, rettype="xml")
parsed_xml<-XML::xmlParse(entrez_xml)
elocation_id<-XML::xpathSApply(parsed_xml, "//ELocationID", XML::xmlValue)
return(elocation_id)
}

return_doi(26479441) # should be 10.1038/nchembio.1936
#> [1] "10.1038/nchembio.1936"
return_doi(28917822) # should be "10.1016/j.drudis.2017.09.004"
#> [1] "S1359-6446(17)30102-2"        "10.1016/j.drudis.2017.09.004"


curious how to show PubMed IDs per year for a specific search?

ex:
2017 PubmID #
2017 PubmID #
2017 PubmID #
2017 PubmID #
2016 PubmID #
2016 PubmID #
2016 PubmID #
2016 PubmID #

I'm able to plot papers by year, but what i really need is a list of pubmed IDs per year. thoughts?

These queries throw an error:
Q1: id_list <- rentrez::entrez_search(db="clinvar", term="human[Organism]", retmax = 100000, retstart = 0)
Q2: id_list <- rentrez::entrez_search(db="clinvar", term="human[Organism]", retmax = 200, retstart = 100000)

Error in ans[[1]] : subscript out of bounds

But these queries don't:
id_list <- rentrez::entrez_search(db="clinvar", term="human[Organism]", retmax = 99999, retstart = 0)
id_list <- rentrez::entrez_search(db="clinvar", term="human[Organism]", retmax = 200, retstart = 99999)

The first query attempts to retrieve the first 100,000 records and the second one 200 records starting from the position 100,000

I tried with retstart = 200,000 and thowrs the same error... but everything works fine with 199,000