Could you please add functions to reshape the data from wide to long format and vice versa, please?
It's called reshape, gather, melt, spread, dcast...

If done with dplyr or data.table it crashes with large datasets.
I'm trying to save a DataFrame to CSV.
The generated file has only the header and presented the error described in the title of this issue.

Steps:
```
install.packages("sparklyr")
install.packages('dplyr')
install.packages('chunked', repos=c('https://cran.rstudio.com', 'http://edwindj.github.io/drat'))

library(sparklyr)
library(dplyr)
library(config)
library(DBI)
library(chunked)
library(crassy) 

conf <- spark_config()

conf$spark.executor.memoryOverhead ="2g"

conf$spark.executor.memory <- "4g"
conf$spark.executor.cores <- 2
conf$spark.executor.instances <- 4
#conf$spark.shuffle.service.enabled <- TRUE
#conf$spark.dynamicAllocation.enabled <- TRUE
conf$spark.dynamicAllocation.enabled <- FALSE
conf$sparklyr.defaultPackages = c("com.datastax.spark:spark-cassandra-connector_2.11:2.4.1", "org.mongodb.spark:mongo-spark-connector_2.11:2.4.0","com.databricks:spark-csv_2.11:1.3.0")
conf$spark.serializer    =    "org.apache.spark.serializer.KryoSerializer"

# Connect to spark
sc <- spark_connect(master = "spark://myspark:7077", 
                    spark_home = "/spark/home",
                    version = "2.4.0",
                    config = conf)

csv_file_path <- "/home/data/events.csv"
mongo_dbname <- "mydb"
mongo_collection <- "events"

sql_txt <- "SELECT id_api, cast(geometry.coordinates as string) as geo, isoDateTime FROM mongo_waze_tbl"


mongo_uri <- paste("mongodb://foo:bar*@10.8.0.5/",mongo_dbname,".",mongo_collection, "?readPreference=primaryPreferred",sep = "")

load <- invoke(spark_get_session(sc), "read") %>%
  invoke("format", "com.mongodb.spark.sql.DefaultSource") %>%
  invoke("option", "spark.mongodb.input.uri", mongo_uri) %>%
  invoke("option", "keyspace", mongo_dbname) %>%
  invoke("option", "table", mongo_collection) %>%
  invoke("option", "header", TRUE) %>%
  invoke("load")

mongo_df <- sparklyr:::spark_partition_register_df(sc, load, "mongo_waze_tbl", 0, FALSE)

mongo_flat_df <- tbl(sc, sql(sql_txt))
mongo_flat_chunked_df <- read_chunkwise(mongo_flat_df, chunk_size = 5000)
write_chunkwise(mongo_flat_chunked_df,csv_file_path)

```


Output:
```
Warning messages:
1: In FUN(X[[i]], ...) :
  Unsupported type 'logical'; using default type 'string'
2: In FUN(X[[i]], ...) :
  Unsupported type 'logical'; using default type 'string'
3: In FUN(X[[i]], ...) :
  Unsupported type 'logical'; using default type 'string'
4: In FUN(X[[i]], ...) :
  Unsupported type 'logical'; using default type 'string'
5: In FUN(X[[i]], ...) :
  Unsupported type 'logical'; using default type 'string'
6: In FUN(X[[i]], ...) :
  Unsupported type 'logical'; using default type 'string'
7: In FUN(X[[i]], ...) :
  Unsupported type 'logical'; using default type 'string'
```
"The readr::read_fwf() is a nice implementation of fwf input, and might be a model for work on something comparable for this package."

I am quoting from https://github.com/xiaodaigh/disk.frame/issues/57
Great package! As a potential enhancement, it would be useful if one could process compressed files (e.g., gzipped txt files) using chunked. 
I have an R workflow that now needs to merge some large csv files into a single file. I find I am running out of memory so the merge has started crashing - these files are getting larger each month so it needs a fix. I could write some other script like Perl or whatever, but I would prefer to keep the whole workflow in R. It's on windows so some of the cmd script solutions would be horrible (and hard to check the formats as I go).

I've been looking around to see how to read files in chunks using R and your library seems perfect. So, I have a question more than an issue....
Do you think it would be possible to use it to merge files (in this case it is a known set of file names each time). I can't yet see a way to do it.
thanks very much for a useful library, we are now using it to filter extracts out of some very large source files and it's simplified quite a bit of our data cleaning/processing already.
When using `read_csv_chunkwise` if there is data missing in the first line of file (or second line, as the case may be `header = TRUE`) there is no option to use `fill=TRUE` and the following error is shown:

`Error in scan(file = file, what = what, sep = sep, quote = quote, dec = dec,  :    line 1 did not have 13 elements`

can this be solved somehow ? 
We could implement a chunk wise sample_n / sample_frac with:

``` r
library(tidyverse)
big <- rerun(1000, iris) %>% bind_rows()
path <- tempfile()
write_csv(big, path)

library(chunked)
sample_n.chunkwise <- function(.data, size){
  cmd <- lazyeval::lazy(sample_n(.data, size))
  chunked:::record(.data, cmd)
}

read_csv_chunkwise(path) %>% 
  sample_n(1) %>% 
  collect() 
```
The sample would be done in each chunk that way. 

What do you think about that? 
If it sounds like a good idea, let me know and I'll send you a PR. 
Hello,

I was wondering if you might be able to support reading in .txt files with your package, as I have data which is delimited in chunks, and would be a good test case for your package:
Data format:

> beer/name: Sausa Weizen
> beer/beerId: 47986
> beer/brewerId: 10325
> beer/ABV: 5.00
> beer/style: Hefeweizen
> review/appearance: 2.5
> review/aroma: 2
> review/palate: 1.5
> review/taste: 1.5
> review/overall: 1.5
> review/time: 1234817823
> review/profileName: stcules
> review/text: A lot of foam. But a lot.	In the smell some banana, and then lactic and tart. Not a good start.	Quite dark orange in color, with a lively carbonation (now visible, under the foam).	Again tending to lactic sourness.	Same for the taste. With some yeast and banana.		
> 
> beer/name: Red Moon
> beer/beerId: 48213
> beer/brewerId: 10325
> beer/ABV: 6.20
> beer/style: English Strong Ale
> review/appearance: 3
> review/aroma: 2.5
> review/palate: 3
> review/taste: 3
> review/overall: 3
> review/time: 1235915097
> review/profileName: stcules
> review/text: Dark red color, light beige foam, average.	In the smell malt and caramel, not really light.	Again malt and caramel in the taste, not bad in the end.	Maybe a note of honey in teh back, and a light fruitiness.	Average body.	In the aftertaste a light bitterness, with the malt and red fruit.	Nothing exceptional, but not bad, drinkable beer.		


Is this what chunked was designed for, or is this a bad use case? Thanks.
