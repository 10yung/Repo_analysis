
使用ansj_seg5.1.6
使用DicLibrary.insertOrCreate("dish", "酸汤鱼", "userDefine", 10000)添加自定义字典。
当在命令行中直接使用DicAnalysis.parse("酸汤鱼大份")时能够分词得到 “酸汤鱼|大|份”。
但是将分词过程放在rdd中处理时sc.textFile("").map(e=>DicAnalysis.parse(e))时得到的分词结果变成了“酸|汤|鱼|大|份”。
        DicLibrary.insert(DicLibrary.DEFAULT, "南通市崇川区南川园路");
        Result result = ToAnalysis.parse("南通市崇川区南川园路南川园新村xx幢yy单元mm室");
        List<Term> terms = result.getTerms();
        for (Term term : terms) {
            System.out.print(term.getName() + "\t");
        }
结果显示为：南通市	崇	川	区	南川	园	路南	川	园	新村	xx	幢	yy	单元	mm	室	
而如果设置为
        DicLibrary.insert(DicLibrary.DEFAULT, "南通市崇川区南川园");
        Result result = ToAnalysis.parse("南通市崇川区南川园路南川园新村xx幢yy单元mm室");
        List<Term> terms = result.getTerms();
        for (Term term : terms) {
            System.out.print(term.getName() + "\t");
        }
结果显示：南通市崇川区南川园	路南	川	园	新村	xx	幢	yy	单元	mm	室	
少了一个“路”字就能识别出来，否则识别不出来，请问是哪里没有设置对么？或者是Bug？

自带的maven依赖与使用者的会冲突，建议去掉
 <dependency>
            <groupId>org.nlpcn</groupId>
            <artifactId>nlp-lang</artifactId>
            <version>1.7.7</version>
        </dependency>
        <dependency>
            <groupId>org.ansj</groupId>
            <artifactId>ansj_seg</artifactId>
            <version>5.1.6</version>
        </dependency>
这样引用的jar包，提示java.lang.NoClassDefFoundError: org/nlpcn/commons/lang/util/logging/LogFactory这个错误，我看ansj_seg源码中引用的是nlp-lang的1.7.8  这是什么意思呢
例如，“计划 n 100 v 100”，就是名词的词频100个，动词的词频100个。可以这么做吗？
可否根据我使用的领域，自定义词典？
为了达到更好的效果，模型训练的代码是否是开源的？
有没有操作文档？
<dependency>
	<groupId>org.nlpcn</groupId>
	<artifactId>nlp-lang</artifactId>
	<version>1.7.8</version>
	<scope>compile</scope>
</dependency>

没有对应版本。
添加自定义词汇，是中英文混合的。就无法分词了。
请问这个需要设置什么吗？
