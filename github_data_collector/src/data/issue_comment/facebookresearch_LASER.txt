Using the python integration of fastBPE could fasten up the code, because bootstraping in unix implies a fork everytime you are trying to use the cpp only implementation. This slows down the execution depending on the used ram.
https://github.com/glample/fastBPE

When I run `embed.sh` with any language code other than "en", the following warning comes up, I am able to get the embeddings but I doubt whether the correct tokenization happened? Is there any fix for this?
```
perl: warning: Setting locale failed.
perl: warning: Please check that your locale settings:
	LANGUAGE = (unset),
	LC_ALL = (unset),
	LC_CTYPE = "UTF-8",
	LANG = "en_US.UTF-8"
    are supported and installed on your system.
```
`Token` method silently failed when shell command failed.
This PR will raise a python error to propagate the shell error.
I am trying to embed some strings, but facing an issue. When passing a list of string containing different language (like French), the imput and output counts are not matching. Like if I pass 4 French strings, I get output of shape (2,1024). In some cases the output increases. It works fine for English, but emoticons or different language characters are resulting in this issue. Any help?

```
from embed import SentenceEncoder, EncodeLoad, EncodeFile
from text_processing import Token, BPEfastApply, SplitLines, JoinEmbed
from indexing import IndexCreate

def lines_to_index(lang: str, lines: List, model_path: str, bpe_code_path: str, use_cpu: bool = False, batch_size: int = 32):
    """Suitable for small amounts of data."""
    with tempfile.TemporaryDirectory() as tmpdirname:
        target = str(Path(tmpdirname) / "source")
        with open(target, "w") as fout:
            fout.write("\n".join(lines))
        return text_file_pipeline(
            lang, target, model_path, bpe_code_path, use_cpu, returns="index", batch_size=batch_size
        )

def text_file_pipeline(lang: str, input_path: str, model_path: str, bpe_code_path: str, use_cpu: bool, batch_size: int,  returns="index"):
    """Suitable for small amounts of data."""
    encoder = SentenceEncoder(
        model_path,
        max_sentences=batch_size,
        max_tokens=10000,
        cpu=use_cpu)
        
    with tempfile.TemporaryDirectory() as tmpdirname:
        tmpdir = Path(tmpdirname)
        Token(
            input_path,
            str(tmpdir / "token"),
            lang=lang,
            romanize=False,
            lower_case=True, gzip=False,
            verbose=True)
        BPEfastApply(
            str(tmpdir / "token"),
            str(tmpdir / "bpe"),
            bpe_code_path,
            verbose=True, over_write=True)
        EncodeFile(
            encoder,
            str(tmpdir / "bpe"),
            str(tmpdir / "enc"),
            verbose=True, over_write=True)
        if returns == "embeddings":
            return np.fromfile(str(tmpdir / "enc"), dtype=np.float32, count=-1)
        data, index = IndexCreate(
            str(tmpdir / "enc"), 'FlatL2',
            verbose=True, save_index=False)
        return data, index
```
When not using the "--unify" option, there still will be an index of unique items built in TextLoadUnify that is based on unique lines. This messes up (at least) the "--score" option of scoring sentence pairs. 

So, this:

    for line in fin:
        new_ind = len(sent2ind)
        inds.append(sent2ind.setdefault(line, new_ind))
        if args.unify:
            if inds[-1] == new_ind:
                sents.append(line[:-1])
                nu += 1
        else:
            sents.append(line[:-1])
            nu += 1

should be changed to:

    for line in fin:
        if args.unify:
            new_ind = len(sent2ind)
            inds.append(sent2ind.setdefault(line, new_ind))
            if inds[-1] == new_ind:
                sents.append(line[:-1])
                nu += 1
        else:
            sents.append(line[:-1])
            inds.append( nu )
            nu += 1
 
hi, 
we know that Laser has used different tokenizers for languages like English, Chinese, Japanese.
if one sentence has multiple languages
e.g. 
a mix of English and Chinese in one sentence ["我喜欢Nike"]
or multilingual batch ["I like Nike", "我喜欢耐克"]

how to use a mix of tokenizer in these cases?
Reduce the latency of scoring through the api by not instanciating the encoder every time a call is made to the endpoint.
I found that in CPU mode, the speed of the generation of embedding is about 26 sentences per hour. Is that slow or normal?
Hello,
I have the training data with labels in English. Now, I want to use this data to predict for other languages. I saw XLM and LASER both support the cross-lingual classification. However, they don't have the benchmark on the same dataset, therefore, it's difficult to know which model is better. Does someone help me in determining which(XLM or LASER) is better for cross-lingual classification?


Since `Token()` [builds the shell command as a string](https://github.com/facebookresearch/LASER/blob/311b25e19dcc30e40caa00b09e4983f22bc34d41/source/lib/text_processing.py#L91-L101), it may be vulnerable to some shell injection attacks.

The Python documentation explicitly warns against this. https://docs.python.org/3.8/library/subprocess.html#security-considerations