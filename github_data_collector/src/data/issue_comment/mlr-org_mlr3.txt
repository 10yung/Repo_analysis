Hi, I would like to tune the number of "top" features to include in a model as a hyperparameter, where the features are ranked by some importance measure as in the filtering method of feature selection. In the old mlr, I can do this by using `makeFilterWrapper()` around a learner, passing an argument like `fw.method` and then tune a corresponding hyperparameter with a name like `fw.abs`. Sorry but I haven't figured out how to do this in mlr3. I failed to find the relevant info in the documentation, and in the manual (mlr3book) it explains ways to obtain the importance measure of the features, but doesn't seem to explain how to use it during tuning. Much appreciated if any guidance could be provided on this.
```r
task = tsk("mtcars")
learner = lrn("regr.featureless")
learner$train(task)
task_data = as.data.table(task)
task_data$mpg = NULL
preds = learner$predict_newdata(task_data)
preds
```
```
<PredictionRegr> for 32 observations:
                   row_id truth response
     rbind_20394c37812f_1    NA 20.09062
    rbind_20394c37812f_10    NA 20.09062
    rbind_20394c37812f_11    NA 20.09062
---                                     
     rbind_20394c37812f_7    NA 20.09062
     rbind_20394c37812f_8    NA 20.09062
     rbind_20394c37812f_9    NA 20.09062
```

The row ids are weird and in the wrong order. It does not happen with all tasks and seems to be unrelated of the learner (also tried rpart)
why does this run?

```
task = tsk("pima")
learner = lrn("classif.svm")
learner$train(task)
```


This issue is to track the various discussions about `predict_type` and `predict_types`. I'll summarise everything discussed on Mattermost below. If I've copied anything wrong please edit.

___

Raphael: Why can't all return types be returned by default?
Bernd: Some return types require extra hyper-parameters and could cause unnecessary overhead 

Raphael: Can we change `predict_type` default in `LearnerClassif` to `prob`?
Bernd: This crashes if a learner cannot return `prob`, whereas all classif learners return `response`
Michel: I also find it annoying that you are often forced to change the predict_type to prob in classif

Raphael: Can we allow `predict_type` to be a vector to increase user flexibility over returns?
Michel: This is possible

Raphael: Does every learner need the same default `predict_type` or can we allow this to be changed in a learner definition?

Franz: A problem with always returning prob and response is less flexibility in determining how to perform the reduction from prob to response (e.g. default threshold value). Proposed solution: `predict_types` is a class property (trait) and `predict_type` is a mutable object property; `predict_type` can be a vector; if a default for one pt to another isn't obvious (e.g. threshold value) then this should be done via pipelines (see https://github.com/mlr-org/mlr3proba/blob/master/R/PipeOpCrankCompositor.R for example)

mlrhyperopt is my favorite way for using mlr, because most of the time it brings me straight to what i am interested in with very little code. mlrhyperopt also gives easy access to all innovative tuning strategies just by changing function arguments and a database of hyperparameters. having this would convince me to make the switch from mlr to mlr3. do you plan an mlrhyperopt pendant in mlr3?
@mboecker will do this for all Learner classes now (in the beginning)
Fixes #404.

@berndbischl volunteered for a review.
Similar to tidymodels/butcher.
`auto_test` fails if the named vector returned by `learner$importance()` contains a name which is not present in `task$feature_names`.  However, some learners return the variable importance for each level of a factor. As a result, the returned named vector contains names like `factor.level`.  For example `h2o::h2o.deeplearning` in [mlr](https://github.com/mlr-org/mlr/blob/master/R/RLearner_classif_h2odeeplearning.R).

Do we want to support variable importance for factor levels?

As we discussed @jakob-r I tried to convert our mlr3 docs to the R6 Roxygen docs. I'm not sure if "@usage NULL" should still be needed? I included it because otherwise the usage section would just say "LearnerClassif".