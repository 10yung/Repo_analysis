I'm not certain this is a real issue, but the `event_driven` example can be made to create 1000 dataflows each a sequence of 1000 nodes, and doing so takes about 10s on my modern desktop. That seems like a long time to put together what is meant to be a linear amount of information. It does seem to be linear (scaling up either number by 10 changes the time by 10), but it would still be good to get a sense for what is going on. Initial investigation suggests allocations in reachability, which indeed does have a bunch of small per-operator allocations. Perhaps those could be consolidated somehow?
At the moment dataflows only shut down through clean completion of the work within them. One releases all capabilities at inputs, and awaits the draining of work within the dataflow. One can imagine situations where ill-conceived dataflows start up that will create more work than the system can expect to handle, and we should have a way to terminate these dataflows *without* awaiting their successful completion.

It seems like most of the dataflows are instantiated by trees of owned data, with connection points back to the communication infrastructure. In principle we could drop these dataflows, and a fair amount of collection will happen automatically.

This is probably an idealized view of shutdown, and there are several issues one can already expect. 

For example, if the channels associated with a dataflow go away, on the receipt of messages for these channels the system cannot currently distinguish between an old dataflow that has been shut down, and a new dataflow that has not yet started up. The system will reconstruct these channels, stash the messages, and await the dataflow construction to collect them. This won't happen, and the effect would be a memory leak.

We also have existed in the pleasant world where dataflows are only shut down after the system is certain there will not be additional messages, and so there may be cases where post-shutdown message receipt causes more egregious errors, up through panicking. At the moment we just don't know.

This issue also relates to the use of capability multisets, in that use case is for messages with empty capability sets, which also have the problem that they may exist beyond the lifetime of a dataflow: one could learn that the dataflow can be terminated (all capabilities are gone) and yet still receive messages without capabilities, also currently resulting in memory leaks or worse. A fix here might remove that blocker for multiset capabilities.

---

One candidate solution is to inform the communication plane of shut down channels, and then to discard messages for these channels. The current channel allocation mechanism uses sequentially assigned identifiers, and the system should be able to distinguish between "yet to be created" and "created and terminated" channels just by looking at the identifiers and comparing them with the next-to-be-assigned identifier. Messages for old identifiers without an associated channel should simply be discarded.

Dataflow destruction should also happen on all workers, but it seems like there isn't the same urgency that it happens in exactly the same order on all workers. Dataflows shut down on a subset of workers should (ideally) just block dataflow execution on other workers rather than result in unrecoverable errors. 

![image](https://user-images.githubusercontent.com/7560282/68039097-c49bf480-fca1-11e9-838b-d28bde01913e.png)

Hi Frank!

For profiling progress & visualizing dataflow frontiers, I believe I need some information in `ProgressEvent`s on what kind of progress actually happened. I think that the `messages` and `internal` fields could help me with that [1].

Currently they are only mocked up in `send(...)` and `receive(...)`. I'd be happy to create a PR and implement them, if you can guide me a little on what the `Vec` fields are supposed to be. E.g., what are target and source descriptor in this context (I assume they are the first two `usize` coordinates in the `Vec`'s tuples)?

Cheers,
Malte

[1]
```rust
pub struct ProgressEvent {
    ...
    /// List of message updates, containing Target descriptor, timestamp as string, and delta.
    pub messages: Vec<(usize, usize, String, i64)>,
    /// List of capability updates, containing Source descriptor, timestamp as string, and delta.
    pub internal: Vec<(usize, usize, String, i64)>,
}
```
Hi, @frankmcsherry,
I have set up timely as a long-time service to server queries. But the problem is that in timely all dataflows share the same computation resources, which will lead to a long execution time for a simple dataflow when there is a big dataflow is operated at the same time. So i think if you have    an idea to address this problem?
I am learning Timely now. I have run it in a distributed environment. After each worker received and processed its data, I want to merge the data among workers in each machine at the end of dataflow excution. Does anyone know how to do it?
The message counter that uses a Rust MPSC can panic in shutdown if its attempt to report messages fails (presumably because the other side has hung up). It isn't immediately obvious why the other end is shutting down if there are still messages to read, so perhaps there is more to diagnose here, but we've seen this in the wild in tests. Some more discipline about error cases and intended invariants would help here.
If a user creates an `InputHandle` and advances it before introducing to a dataflow, the construction of the corresponding input operator will not use the new advanced capability. This leads to a bit of a mess, where initial invariants are not satisfied.

Should be an easy fix to pull this info out of the handle at construction time and initialize the input operator correctly.
Hello

I'm trying to grasp how the timely dataflow is meant to be used kind of end to end. The documentation concentrates on how to do and structure the computation and its good at explaining that. But I kind of still don't get how to connect it to the rest of the world.

To illustrate what I mean, let's say I have a huge source of data ‒ let's say live stream of log messages or metrics or something like that. So, the whole thing will do something like this:

* I have bunch of worker processes across machines.
* They either read a partition of the stream each, or one „master“ reads them and inserts them into the system and it'll distribute through the workers.
* It performs the computation, producing let's say events of style „this service entered alert state“ and „this service left alert state“.

However, let's say I want a static web page containing all the services in alert state at the currently newest timestamp. This sounds challenging, because:
* The information about the state of different machines is scattered through the cluster, not at one place.
* They may be coming out of order (OK, that probably can be somehow worked around by buffering in a custom operator).

I guess I could *somehow* force my way through it, but all the ideas I have in mind look like huge hacks. Is there a correct way to do such things? Should it be part of the book? Because all the „exporting“ of the results is done by `println` ‒ that'll end up in a random worker, right?
I'm currently reading timely's gitbook, and so far it has been great!

Here are some suggestions on what to improve:

- most examples start with `extern crate timely;`, I think this isn't necessary for Rust 2018 anymore
- The first code example in the iteration chapter doesn't match its surrounding description: E.g. `LoopVariable` is introduced, but the example uses `feedback()`, "(i) what is the upper bound on the number of iterations" isn't reflected in the function arguments, and "We've built an upper limit of 100" doesn't exist in the example code
- Subsection `Scopes` clears this up a little, but perhaps it might make sense to present the difference between `feedback` & `loop_variable` earlier (esp. since `loop_variable` just uses `feedback` under the hood)
- Many concepts are defined independently, but never compared or connected to each other: e.g. how `capability.downgrade()` & `flushing` relates to `input_handle.advance_to()`, how `input_handle.time()` relates to `frontier`, how the `source` we wrote relates to providing input via an `InputHandle`, and what the differences between `InputHandle` outside of a dataflow (the one with `.time()`) and the `(Frontiered)InputHandle` within operator closures (the one with `.frontier()` and `for_each()`) are. Maybe a short summary / glossary / comparison of concepts at the end of Chapter 3 could make this clearer?
- Lastly, I think it might be confusing that (as far as I saw) it's nowhere mentioned that timely cleans up when dataflows go out of scope. That way, especially when writing small programs, you'll always see some output, even when you never advanced time or called `worker.step()`. I've added `use std::io::stdin; stdin().read_line(&mut String::new()).unwrap();` at the end of the `execute` closure to make sure nothing is cleaned up without me interrupting the program so that I can better observe my changes

Best, Malte