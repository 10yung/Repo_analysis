engine conf:
```
{
    "engineId": "rb",
    "engineFactory": "com.actionml.engines.ur.UREngine",
    "sparkConf": {
        "master": "local",
        "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
        "spark.kryo.registrator": "org.apache.mahout.sparkbindings.io.MahoutKryoRegistrator",
        "spark.kryo.referenceTracking": "false",
        "spark.kryoserializer.buffer": "300m",
        "spark.executor.memory": "3g",
        "spark.driver.memory": "3g",
        "spark.es.index.auto.create": "true",
        "spark.es.nodes": "elasticsearch",
        "spark.es.nodes.wan.only": "true"
    },
    "algorithm":{
        "indicators": [
            {
                "name": "buy"
            },
            {
                "name": "add-to-cart"
            },
            {
                "name": "view"
            },
            {
                "name": "like"
            }
        ]
    }
}
```

items:

![Screenshot from 2019-12-29 12-14-12](https://user-images.githubusercontent.com/8361283/71554345-6ab47280-2a37-11ea-955f-e18e8d1e6efd.png)

events:

![Screenshot from 2019-12-29 12-16-19](https://user-images.githubusercontent.com/8361283/71554360-828bf680-2a37-11ea-837c-66558776e33b.png)

jobs:

![Screenshot from 2019-12-29 12-17-25](https://user-images.githubusercontent.com/8361283/71554373-9899b700-2a37-11ea-8a08-319f4cae635c.png)

**log**: [harness.log](https://github.com/actionml/universal-recommender/files/4007905/harness.log)

**Note**
I am using docker-compose
Hi!

Using Harness 0.5.1 and the included UR.

I'm trying to use the include filter rule, but I keep getting an empty result. I used a $set event to set a category on two items that exist in my data as such:

```
{
   "event" : "$set",
   "entityType" : "item",
   "entityId" : "exampleItem",
   "properties" : {
      "category": ["electronics", "mobile"],
      "expireDate": "2020-10-05T21:02:49.228Z"
   },
   "eventTime" : "2019-12-17T21:02:49.228Z"
}
```
Then I ran the train command and then this query:

```
{
   "rules": [
    {
      "name": "category",
      "values": ["electronics"],
      "bias": -1
    }]
}
```
but got an empty list as result.
The other rules (bias = 0 and > 0) seem to be working as specified.
Hi, i'm using the universal recommander template with predictionio to recommend products from a store. My issue is that said store can delete products in back office, for any reason, when it does the item must of course not be recommanded anymore. I then need a way to delete said item but not in a permanent way. Right now the best way i'v found is to give an item a specific property (let's say "eliminated"), re-train and then filter for on that property with a bias = 0, this way i just need to remove that property if i want the item back .
Is there a better way to do this? Re-train take time , does a way to avoid it exist? 
I tried some other methods like an $unset event on the item , but doing so still require a re-train and it also require a full $set event to bring it back if i need to do so , and $delete event, that seems to do nothing at all, but it worked when i tried with a different template (similar product) and without a re-train. last thing i tried was the avaible/expire date but first i don't know the expire date until someone decide to actually make it expire ,second i need to make the change and then re-train if i want the changes to apply, so i'm back to square one.
Any answer will be very appreciated.
universal recommander version is 0.7.3 
predictionio version is 0.13.0

# I want to get the recommended data correctly.

# Your thoughts and ideas
I followed the steps in the official documentation https://actionml.com/docs/h_ur.
1. Install with docker, no problem, document page: https://actionml.com/docs/harness_container_guide
2. Add the recommendation engine with harness_cli, no problem, the documentation page: https://actionml.com/docs/h_ur_quickstart
3. After the engine is added, the test function is normal.

# Describe your problem
After the data was successfully entered by post and the training was activated, the recommendation engine did not give me feedback.

1
![avatar](https://s2.ax1x.com/2019/11/21/Mh4mvQ.png)
2
![avatar](https://s2.ax1x.com/2019/11/21/MhfXlQ.png)
3
![avatar](https://s2.ax1x.com/2019/11/21/Mh4dbR.png)
The training event was triggered and about 10,000 pieces of data were entered in advance. When the request was routed to the queries, the recommended items were not returned to me. The recommendation engine was running normally, and the log was not reported. I don’t know why.
When I builded this project,I found errors.

They were called by the sbt's plugins version isn't the latest.
Where  the file is
```
project/plugins.sbt
```.
Here are the lastest plugins' url:
1. [org.scalariform](https://github.com/sbt/sbt-scalariform)
2. [org.scalastyle](http://www.scalastyle.org/sbt.html)
3. [com.eed3si9n](https://github.com/sbt/sbt-assembly#using-published-plugin)
---
PS:This is the first time,so made a mistake,pull request in the master branch.So sorry about it.
It seems that UR cannot detect the GPU.

```
[INFO] [RootSolverFactory$] Creating org.apache.mahout.viennacl.opencl.GPUMMul solver
[INFO] [RootSolverFactory$] Unable to create class GPUMMul: attempting OpenMP version
[INFO] [RootSolverFactory$] Creating org.apache.mahout.viennacl.openmp.OMPMMul solver
[INFO] [RootSolverFactory$] org.apache.mahout.viennacl.openmp.OMPMMul$
[INFO] [RootSolverFactory$] Unable to create class OMPMMul: falling back to java version
```

There is my GPU information:
```
nvidia-smi
Sat Jun  1 15:37:51 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           On   | 00000000:00:04.0 Off |                    0 |
| N/A   31C    P8    24W / 149W |      0MiB / 11441MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
```
Can I take into account user/item locations in universal recommender? If it is true how to do it? Thanks.
Hi All,

I have installed prediction io on MacBook and installed the universal recommender template. I was able to build the app successfully. But, when I do a=n integration test or training on other data, I am getting the below error. I have also copied the engine.json extract. It will be very helpful if anyone share their inputs on this.

engine.json (nothing has been changed. its the default data)

{
  "comment":" This config file uses default settings for all but the required values see README.md for docs",
  "id": "default",
  "description": "Default settings",
  "engineFactory": "com.actionml.RecommendationEngine",
  "datasource": {
    "params" : {
      "name": "sample-handmade-data.txt",
      "appName": "handmade",
      "eventNames": ["purchase", "view", "category-pref"],
      "minEventsPerUser": 3
    }
  },
  "sparkConf": {
    "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
    "spark.kryo.registrator": "org.apache.mahout.sparkbindings.io.MahoutKryoRegistrator",
    "spark.kryo.referenceTracking": "false",
    "spark.kryoserializer.buffer": "300m",
    "es.index.auto.create": "true"
  },


Exception when training -

[INFO] [Engine] Extracting datasource params...
[INFO] [WorkflowUtils$] No 'name' is found. Default empty String will be used.

Exception in thread "main" java.lang.NoSuchMethodError: org.json4s.ParserUtil$.quote(Ljava/lang/String;)Ljava/lang/String;

	at org.json4s.native.JsonMethods$$anonfun$2.apply(JsonMethods.scala:42)

	at org.json4s.native.JsonMethods$$anonfun$2.apply(JsonMethods.scala:42)

	at scala.collection.immutable.List.map(List.scala:284)

	at org.json4s.native.JsonMethods$class.render(JsonMethods.scala:42)

	at org.json4s.native.JsonMethods$.render(JsonMethods.scala:62)

	at org.apache.predictionio.workflow.WorkflowUtils$$anonfun$getParamsFromJsonByFieldAndClass$2$$anonfun$2.apply(WorkflowUtils.scala:177)

	at org.apache.predictionio.workflow.WorkflowUtils$$anonfun$getParamsFromJsonByFieldAndClass$2$$anonfun$2.apply(WorkflowUtils.scala:168)

	at scala.Option.map(Option.scala:146)

	at org.apache.predictionio.workflow.WorkflowUtils$$anonfun$getParamsFromJsonByFieldAndClass$2.apply(WorkflowUtils.scala:168)

	at org.apache.predictionio.workflow.WorkflowUtils$$anonfun$getParamsFromJsonByFieldAndClass$2.apply(WorkflowUtils.scala:159)

	at scala.Option.map(Option.scala:146)

	at org.apache.predictionio.workflow.WorkflowUtils$.getParamsFromJsonByFieldAndClass(WorkflowUtils.scala:159)

	at org.apache.predictionio.controller.Engine.jValueToEngineParams(Engine.scala:363)

	at org.apache.predictionio.workflow.CreateWorkflow$.main(CreateWorkflow.scala:222)

	at org.apache.predictionio.workflow.CreateWorkflow.main(CreateWorkflow.scala)

	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.lang.reflect.Method.invoke(Method.java:498)

	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)

	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:849)

	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)

	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)

	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)

	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)

	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)

	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)



Regards,
Raman S V
Hi, dear developer
    I extract the code of cco training from your pio-ur，but when i ran my jar in clusters, I meet the problem -- "had a not serializable result: org.apache.mahout.math.DenseVector".
    I dont understand why the mahout design it`s Vector no able to serialize and I also dont understand why you use RandomAccessSparseVector in distributed program.
    how can you solve this problem? Please help me, thank you very much.


I tried to leverage the already present async interface of elasticsearch and armouring the not so crucial parts (like getting the app) with blocking constructs which will tell the standard scala ExecutionContext to use a separate thread for that.

Let me know what you think,
See also https://github.com/apache/predictionio/pull/495