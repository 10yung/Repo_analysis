In the training phase the number of predicted RoIs + scores is determined by hyperparameters like  `POST_NMS_ROIS_TRAINING`. Caffe inits layers to (1,21) or (1,84) and then they are resized to match the number of predictions, e.g. `(1000, 21)` and `(1000,84)`. This is done in custom layers, and I don't have a problem with that. At the same time, in order to get the loss, layers like `cls_score, bbox_pred, fc6/7`, are also resized to match the size (while keeping the number of weights the same). I don't see how Caffe does it out of the box, but I can't find the wrapper for that. Could anyone help? 
hello, when I compile caffe I get this error:
```
src/caffe/layers/roi_align_layer.cpp:30:19: error: expected initializer before ‘<’ token
 void ROIAlignLayer<Dtype>::LayerSetUp(const vector<Blob<Dtype>*>& bottom,
                   ^
src/caffe/layers/roi_align_layer.cpp:45:19: error: expected initializer before ‘<’ token
 void ROIAlignLayer<Dtype>::Reshape(const vector<Blob<Dtype>*>& bottom,
                   ^
src/caffe/layers/roi_align_layer.cpp:62:19: error: expected initializer before ‘<’ token
 void ROIAlignLayer<Dtype>::Forward_cpu(const vector<Blob<Dtype>*>& bottom,
                   ^
src/caffe/layers/roi_align_layer.cpp:203:19: error: expected initializer before ‘<’ token
 void ROIAlignLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top,
                   ^
In file included from ./include/caffe/blob.hpp:8:0,
                 from src/caffe/layers/roi_align_layer.cpp:15:
src/caffe/layers/roi_align_layer.cpp:261:19: error: ‘ROIAlignLayer’ is not a class template
 INSTANTIATE_CLASS(ROIAlignLayer);
```
how to fix it?
can you give some advises? thank you very much~
Hi, I was trying to read the source code of ROIAlignLayer.cpp and had some difficulties understanding the bilinear interpolation part in your implementation as following.

              for (int h_idx = floor(hstart); h_idx <= ceil(hend) && h_idx < height_; ++h_idx) {
                for (int w_idx = floor(wstart); w_idx <= ceil(wend) && w_idx < width_; ++w_idx) {
                  if (counter < 4) {
                    b_index[counter] = ((((n*channels_ + c) * height_) + h_idx ) * width_ )+ w_idx; 
                    b_index_curr[counter] = (h_idx*width_) + w_idx; 
                    //Normalize h_idx and w_idx
                    h_idx_n =  static_cast<Dtype>( (2*(static_cast<Dtype>(h_idx) - roi_start_h) / (roi_end_h - roi_start_h)) - 1);
                    w_idx_n =  static_cast<Dtype>( (2*(static_cast<Dtype>(w_idx) - roi_start_w) / (roi_end_w - roi_start_w))  - 1);
                    h_idx_n = min(max(h_idx_n, static_cast<Dtype>(-1.0)),one);
                    w_idx_n = min(max(w_idx_n, static_cast<Dtype>(-1.0)),one);
                    multiplier[counter] = max(zero,static_cast<Dtype>(1 - fabs(x_smp_n - w_idx_n))) 
                                             * max(zero,static_cast<Dtype>(1 - fabs(y_smp_n - h_idx_n)));

                    bisampled[smp/2] += batch_data[b_index_curr[counter]]*multiplier[counter];  
                    ++counter; 
		 } else { 
		    goto stop;  
		 }
                } // w_idx
              } //h_idx

Suppose if a ROI bin's width ranges from 0.5 to 2.1, and height ranges from 0.9 to 1.5. Then the hstart=0.9, hend=1.5, wstart=0.5, wend =2.1. And suppose smp = 0.

Then the loop starts with h_idx = floor(0.9) = 0, w_idx =floor(0.5) = 0, bisampled[0] is added with the feature map value at (0,0) multiplying a weight. count = 1.

In the second loop, h_idx = 0, ++w_idx =1, bisampled[0] is added with the feature map value at (0,1) multiplying a bilinear interpolation weight, count = 2

In the third loop, h_idx = 0, ++w_idx =2, bisampled[0] is added with the feature map value at (0,2) multiplying a bilinear interpolation weight. count = 3

In the fourth loop, h_idx = 0, ++w_idx =3, bisampled[0] is added with the feature map value at (0,3) multiplying a bilinear interpolation weight. count =4, then it goes to stop function. 

My question is, to get the bisampled[0], isn't it to sum up the bilinear interpolation weighted feature map value at (0,0), (0,1),(1,0) and (1,1) since the upper-left corner of the ROI bin is at (0.9,0.5)?  Or is there anything I misunderstood?  Hope you can answer my question. Thanks!

can not find ROIAlignLayer head file
Can anyone give a clarification on the intuition of using no_loss_mask = -1?
https://github.com/jasjeetIM/Mask-RCNN/blob/master/functions/mask_rcnn/mask_get_minibatch.m#L131

As far as I can tell, cross-entropy loss expects target = [0, 1]. 
@jasjeetIM 
Thanks for your code.
Do you have any trained models now? 
I would really appreciate it if you can share one trained caffe model.
Thank you!  
Hi, Thanks for your code.
I readed your roi_align_layer.cpp. I am not sure if this code will work when the ROI bin size is larger than 2x2. In your code it looked like that you only calculate one value(as maxvalue) in each ROI bin.

I think we should use VALID normalizer for the mask loss so that the objective can be correctly scaled. What's your experience about this? @jasjeetIM 
Hi @jasjeetIM ,
          After adding the roi align layer，I replace the roi pooling layer in the trainval.prototxt like the following:

...................
  top: "conv5_3"
}
layer {
  name: "roi_align5"
  type: "ROIAlign"
  bottom: "conv5_3"
  bottom: "rois"
  top: "pool5"
  roi_align_param {
    pooled_w: 7
    pooled_h: 7
    spatial_scale: 0.0625 # 1/16
  }
}
layer {
  name: "fc6"
...................

Did it right? If so, did  you compare the  roi align layer and roi pooling layer in object detection? Could it increase the mAP?

Thanks in advance.  



Hi @jasjeetIM ,

I took your ROIAlign layers and integrated them with my py-Faster-RCNN to replace ROIPool layers in my code base. But I am getting bounding box loss as NAN for all the iterations so far while training Fast-RCNN (stage-2) in alternate training optimization. Did you also face any such problem while training? Please let me know.
I shall dig deeper and get back if I find anything worth sharing. Following are my sample logs of the loss for your reference.

```
I0707 10:38:07.833065 10773 solver.cpp:228] Iteration 1940, loss = nan
I0707 10:38:07.833112 10773 solver.cpp:244]     Train net output #0: loss_bbox = nan (* 1 = nan loss)
I0707 10:38:07.833133 10773 solver.cpp:244]     Train net output #1: loss_cls = 87.3365 (* 1 = 87.3365 loss)
```

Thanks