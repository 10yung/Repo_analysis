LightGBM on GPU, LightGBM version 2.2.4

Running same system and same data/model takes 1 second, but happen to be originally hung for 17  hours.  By hang here I mean abnormal state, but the process is still using 100% of 1 core most of the time, sometimes long periods of 0% though.  Process sits on GPU but doesn't have any utilization.

Happens on multiple GPU types, but this one is Tesla T4.


Backtrace for python from faulthandler:

```
an 16 18:11:06 ip-10-10-4-5 dai-env.sh[7084]: Current thread 0x00007fb07b1be740 (most recent call first): 
Jan 16 18:11:06 ip-10-10-4-5 dai-env.sh[7084]: File "/opt/h2oai/dai/cuda-10.0/lib/python3.6/site-packages/lightgbm_gpu/basic.py", line 1896 in update 
Jan 16 18:11:06 ip-10-10-4-5 dai-env.sh[7084]: File "/opt/h2oai/dai/cuda-10.0/lib/python3.6/site-packages/lightgbm_gpu/engine.py", line 248 in train 
Jan 16 18:11:06 ip-10-10-4-5 dai-env.sh[7084]: File "/opt/h2oai/dai/cuda-10.0/lib/python3.6/site-packages/lightgbm_gpu/sklearn.py", line 593 in fit 
Jan 16 18:11:06 ip-10-10-4-5 dai-env.sh[7084]: File "/opt/h2oai/dai/cuda-10.0/lib/python3.6/site-packages/lightgbm_gpu/sklearn.py", line 798 in fit 
Jan 16 18:11:06 ip-10-10-4-5 dai-env.sh[7084]: File "/opt/h2oai/dai/python/lib/python3.6/concurrent/futures/process.py", line 175 in _process_worker 
Jan 16 18:11:06 ip-10-10-4-5 dai-env.sh[7084]: File "/opt/h2oai/dai/python/lib/python3.6/multiprocessing/process.py", line 93 in run 
Jan 16 18:11:06 ip-10-10-4-5 dai-env.sh[7084]: File "/opt/h2oai/dai/python/lib/python3.6/multiprocessing/process.py", line 258 in _bootstrap 
Jan 16 18:11:06 ip-10-10-4-5 dai-env.sh[7084]: File "/opt/h2oai/dai/python/lib/python3.6/multiprocessing/popen_fork.py", line 80 in _launch 
Jan 16 18:11:06 ip-10-10-4-5 dai-env.sh[7084]: File "/opt/h2oai/dai/python/lib/python3.6/multiprocessing/popen_fork.py", line 26 in __init__ 
Jan 16 18:11:06 ip-10-10-4-5 dai-env.sh[7084]: File "/opt/h2oai/dai/python/lib/python3.6/multiprocessing/context.py", line 277 in _Popen 
Jan 16 18:11:06 ip-10-10-4-5 dai-env.sh[7084]: File "/opt/h2oai/dai/python/lib/python3.6/multiprocessing/context.py", line 223 in _Popen 
Jan 16 18:11:06 ip-10-10-4-5 dai-env.sh[7084]: File "/opt/h2oai/dai/python/lib/python3.6/multiprocessing/process.py", line 105 in start 
Jan 16 18:11:06 ip-10-10-4-5 dai-env.sh[7084]: File "/opt/h2oai/dai/python/lib/python3.6/concurrent/futures/process.py", line 446 in _adjust_process_count 
Jan 16 18:11:06 ip-10-10-4-5 dai-env.sh[7084]: File "/opt/h2oai/dai/python/lib/python3.6/concurrent/futures/process.py", line 427 in _start_queue_management_thread 
Jan 16 18:11:06 ip-10-10-4-5 dai-env.sh[7084]: File "/opt/h2oai/dai/python/lib/python3.6/concurrent/futures/process.py", line 466 in submit 
Jan 16 18:11:06 ip-10-10-4-5 dai-env.sh[7084]: File "/opt/h2oai/dai/python/lib/python3.6/concurrent/futures/process.py", line 175 in _process_worker
```


gdb backtrace:
```
#0  0x00007ffe829436c2 in clock_gettime ()
#1  0x00007fb079e417ed in clock_gettime () from /lib64/libc.so.6
#2  0x00007faf2a46d72e in ?? () from /lib64/libnvidia-opencl.so.1
#3  0x00007faf2a53dc57 in ?? () from /lib64/libnvidia-opencl.so.1
#4  0x00007faf2a45585c in ?? () from /lib64/libnvidia-opencl.so.1
#5  0x00007faf2a442dd5 in ?? () from /lib64/libnvidia-opencl.so.1
#6  0x00007faf2a442fc0 in ?? () from /lib64/libnvidia-opencl.so.1
#7  0x00007faf2a3177aa in ?? () from /lib64/libnvidia-opencl.so.1
#8  0x00007faf2a31a422 in ?? () from /lib64/libnvidia-opencl.so.1
#9  0x00007faf2a3322c3 in ?? () from /lib64/libnvidia-opencl.so.1
#10 0x00007faf2a3356b3 in ?? () from /lib64/libnvidia-opencl.so.1
#11 0x00007faf2a335c28 in ?? () from /lib64/libnvidia-opencl.so.1
#12 0x00007faf2a325298 in ?? () from /lib64/libnvidia-opencl.so.1
#13 0x00007faffc706a6e in boost::compute::command_queue::enqueue_write_buffer(boost::compute::buffer const&, unsigned long, unsigned long, void const*, boost::compute::wait_list const&) ()
   from /opt/h2oai/dai/cuda-10.0/lib/python3.6/site-packages/lightgbm_gpu/lib_lightgbm.so
#14 0x00007faffc6fba42 in LightGBM::GPUTreeLearner::ConstructGPUHistogramsAsync(std::vector<signed char, std::allocator<signed char> > const&, int const*, int, float const*, float const*, float*, float*) ()
   from /opt/h2oai/dai/cuda-10.0/lib/python3.6/site-packages/lightgbm_gpu/lib_lightgbm.so
#15 0x00007faffc6fbd52 in LightGBM::GPUTreeLearner::ConstructHistograms(std::vector<signed char, std::allocator<signed char> > const&, bool) () from /opt/h2oai/dai/cuda-10.0/lib/python3.6/site-packages/lightgbm_gpu/lib_lightgbm.so
#16 0x00007faffc7160ee in LightGBM::SerialTreeLearner::FindBestSplits() () from /opt/h2oai/dai/cuda-10.0/lib/python3.6/site-packages/lightgbm_gpu/lib_lightgbm.so
#17 0x00007faffc717e37 in LightGBM::SerialTreeLearner::Train(float const*, float const*, bool, json11::Json&) () from /opt/h2oai/dai/cuda-10.0/lib/python3.6/site-packages/lightgbm_gpu/lib_lightgbm.so
#18 0x00007faffc57b7c5 in LightGBM::GBDT::TrainOneIter(float const*, float const*) () from /opt/h2oai/dai/cuda-10.0/lib/python3.6/site-packages/lightgbm_gpu/lib_lightgbm.so
#19 0x00007faffc51e41e in LGBM_BoosterUpdateOneIter () from /opt/h2oai/dai/cuda-10.0/lib/python3.6/site-packages/lightgbm_gpu/lib_lightgbm.so
#20 0x00007fb06d0fedcc in ffi_call_unix64 () from /opt/h2oai/dai/lib/libffi.so.6
#21 0x00007fb06d0fe6f5 in ffi_call () from /opt/h2oai/dai/lib/libffi.so.6
#22 0x00007fb06d31302b in _call_function_pointer (argcount=2, resmem=0x7ffe828d01b0, restype=<optimized out>, atypes=<optimized out>, avalues=0x7ffe828d0190, pProc=0x7faffc51e3e0 <LGBM_BoosterUpdateOneIter>, flags=4353)
    at /opt/h2oai/dai/src/Python-3.6.4/Modules/_ctypes/callproc.c:809
#23 _ctypes_callproc (pProc=pProc@entry=0x7faffc51e3e0 <LGBM_BoosterUpdateOneIter>, argtuple=argtuple@entry=0x7fb024ea4048, flags=4353, argtypes=argtypes@entry=0x0, restype=restype@entry=0x2466998, checker=0x0)
    at /opt/h2oai/dai/src/Python-3.6.4/Modules/_ctypes/callproc.c:1147
#24 0x00007fb06d309efc in PyCFuncPtr_call (self=self@entry=0x7fb025096368, inargs=inargs@entry=0x7fb024ea4048, kwds=kwds@entry=0x0) at /opt/h2oai/dai/src/Python-3.6.4/Modules/_ctypes/_ctypes.c:3962
#25 0x00007fb07aa93b5d in _PyObject_FastCallDict (func=func@entry=0x7fb025096368, args=<optimized out>, nargs=2, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2331

```



terminal output for lightgbm running when hung:
```
an 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [LightGBM] [Warning] Starting from the 2.1.2 version, default value for the "boost_from_average" parameter in "binary" objective is true.
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: This may cause significantly different results comparing to the previous versions of LightGBM.
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: Try to set boost_from_average=false, if your old models produce bad results
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [LightGBM] [Info] Number of positive: 82603, number of negative: 261573
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [LightGBM] [Info] This is the GPU trainer!!
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [LightGBM] [Info] Total Bins 1718
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [LightGBM] [Info] Number of data: 344176, number of used features: 196
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [LightGBM] [Info] Using requested OpenCL platform 0 device 0
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [LightGBM] [Info] GPU programs have been built
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [LightGBM] [Info] Size of histogram bin entry: 12
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [LightGBM] [Info] 196 dense feature groups (32.82 MB) transferred to GPU in 0.033426 secs. 0 sparse feature groups
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [LightGBM] [Warning] Cannot change max_bin after constructed Dataset handle.
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [LightGBM] [Warning] Cannot change bin_construct_sample_cnt after constructed Dataset handle.
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [LightGBM] [Warning] Cannot change min_data_in_bin after constructed Dataset handle.
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240002 -> initscore=-1.152667
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [LightGBM] [Info] Start training from score -1.152667
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [1]        2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 1.36442        2_Lag:panel_date .beg_mod__dur .1's auc: 1
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: Training until validation scores don't improve for 30 rounds.
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [2]        2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 1.30675        2_Lag:panel_date .beg_mod__dur .1's auc: 1
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [3]        2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 1.25341        2_Lag:panel_date .beg_mod__dur .1's auc: 1
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [4]        2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 1.20387        2_Lag:panel_date .beg_mod__dur .1's auc: 1
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [5]        2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 1.15769        2_Lag:panel_date .beg_mod__dur .1's auc: 1
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [6]        2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 1.11448        2_Lag:panel_date .beg_mod__dur .1's auc: 1
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
Jan 16 00:58:46 ip-10-10-4-5 dai-env.sh[7084]: [7]        2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 1.07393        2_Lag:panel_date .beg_mod__dur .1's auc: 1
```
then hung.


Repeat of same exact model/pickled state written before the hang:

```
[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the "boost_from_average" parameter in "binary" objective is true. 
This may cause significantly different results comparing to the previous versions of LightGBM. 
Try to set boost_from_average=false, if your old models produce bad results 
[LightGBM] [Info] Number of positive: 82603, number of negative: 261573 
[LightGBM] [Info] This is the GPU trainer!! 
[LightGBM] [Info] Total Bins 1718 
[LightGBM] [Info] Number of data: 344176, number of used features: 196 
[LightGBM] [Info] Using requested OpenCL platform 0 device 0 
[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation 
[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins... 
[LightGBM] [Info] GPU programs have been built 
[LightGBM] [Info] Size of histogram bin entry: 12 
[LightGBM] [Info] 196 dense feature groups (32.82 MB) transferred to GPU in 0.034075 secs. 0 sparse feature groups 
[LightGBM] [Warning] Cannot change max_bin after constructed Dataset handle. 
[LightGBM] [Warning] Cannot change bin_construct_sample_cnt after constructed Dataset handle. 
[LightGBM] [Warning] Cannot change min_data_in_bin after constructed Dataset handle. 
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240002 -> initscore=-1.152667 
[LightGBM] [Info] Start training from score -1.152667 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[1]    2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 1.36442    2_Lag:panel_date .beg_mod__dur .1's auc: 1 
Training until validation scores don't improve for 30 rounds. 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[2]    2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 1.30675    2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[3]    2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 1.25341    2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[4]    2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 1.20387    2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[5]    2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 1.15769    2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[6]    2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 1.11448    2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[7]    2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 1.07393    2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[8]    2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 1.03579    2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[9]    2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 0.999806   2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[10]   2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 0.965798   2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[11]   2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 0.933588   2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[12]   2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 0.903027   2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[13]   2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 0.873981   2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[14]   2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 0.846334   2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[15]   2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 0.819981   2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[16]   2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 0.794827   2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[17]   2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 0.770791   2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[18]   2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 0.747795   2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[19]   2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 0.725772   2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[20]   2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 0.704659   2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[21]   2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 0.684402   2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[22]   2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 0.664947   2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[23]   2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 0.646249   2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[24]   2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 0.628264   2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[25]   2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 0.610953   2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[26]   2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 0.594278   2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[27]   2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 0.578207   2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[28]   2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 0.562707   2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[29]   2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 0.54775    2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[30]   2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 0.533309   2_Lag:panel_date .beg_mod__dur .1's auc: 1 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf 
[31]   2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 0.519359   2_Lag:panel_date .beg_mod__dur .1's auc: 1 
Early stopping, best iteration is: 
[1]    2_Lag:panel_date .beg_mod__dur .1's binary_logloss: 1.36442    2_Lag:panel_date .beg_mod__dur .1's auc: 1 
                  0 
count 344177.000000 
mean       0.240004 
std        0.427086 
min        0.000000 
25%        0.000000 
50%        0.000000 
75%        0.000000 
max        1.000000 
```


Pickle and script:

[hang_pickle.zip](https://github.com/microsoft/LightGBM/files/4073126/hang_pickle.zip)

Just remove the line from the py file that has:
```
from h2oaicore.lightgbm_dynamic import got_cpu_lgb, got_gpu_lgb
```
That's just our way of dynamically importing which lightgbm we want.
`lgb.plot.importance()` in the R package is currently untested. In this PR, I propose a basic test that will cover all of the code in that function and at least help us catch some types of issues in CI.

This PR increases the R package's test coverage from `71.29%` to  `72.29%`.

I also clarified the documentation on the `cex` parameter so that it is easier for users  to identify valid values to pass.
We use `roxygen2` to document the R package. You can read more about this project [here](http://r-pkgs.had.co.nz/man.html), but basically you use specially-formatted comments in source code files and they get translated into R's special flavor of LaTeX (`.Rd`).

Right now, `lightgbm`'s roxygen2 stuff is relying on the  _convention_ that the first of these comments maps to the `title` element of a documentation page and the the first of these comments after that (broken by a blank line) becomes the `description` element. This can be error-prone, so in the interest of being explicit in this PR I propose that we _explicitly_ use the `@title` and `@description` tags.

Other changes in this PR:

* added previously-undeclared import of `utils`
* removed redundant uses of `@rdname` and replaced them with `@name`
* added docs on the expected form of `group` when using  `Dataset$setinfo("group")`
* clarified documentation on the expected form of `categorical_feature`

This PR also fixes LaTeX formatting issues that were smashing parameter names into their descriptions like this:

**Before**

<img width="541" alt="lgb train-params-before" src="https://user-images.githubusercontent.com/7608904/72234749-1e6b4580-3594-11ea-9809-722ef7b6c811.png">

**After**

<img width="851" alt="lgb train-params-after" src="https://user-images.githubusercontent.com/7608904/72234765-3216ac00-3594-11ea-864a-9a5aef24d6b9.png">


Current, `Dataset$setinfo()` has the behavior "if you pass in something for  'group', convert it to integer before storing it". That behavior is not currently tested. This PR adds a test for it.
In this PR, I propose some more user-friendly documentation and error messages for `lgb.get.eval_result()`. I think it would be helpful to be more explicit about how someone can find valid values of `data_name` and `eval_name`.

I've also added tests on the validations in this function, since currently they are not covered by tests.
<!--
Please search your question on previous issues, Stack Overflow (https://stackoverflow.com/questions/tagged/lightgbm) or other search engines before you open a new one.
-->

<!--
For bugs and unexpected issues, please provide the following information, so that we could reproduce them on our system.
-->

## Environment info

Operating System: Ubuntu

CPU/GPU model: CPU

C++/Python/R version: python

LightGBM version or commit hash: 2.2.3

I'm not sure if this is a bug or a misusage from my part, but I was dealing with a classification problem with unbalanced classes (where the positive class proportion was bigger than the negative one) setting in my company. If I set the parameter `is_unbalanced=True` or scale_pos_weight for lightgbm the model always predicted positive class, resulting in True Positive and False Positive only.

As this wasn't expected, I tried to use `class_weight=balanced` even though this is specified in the documentation that for this kind of problem it should not be used. However, the results made much more sense since there were fell False Negatives and True Negatives in this case. 

I got a little confused by getting these results and any appreciation would be grateful.
Option to use extremely randomized trees as base learner, as requested in #2583.
While working through the R package, I noticed several places where we have some validation of inputs to function calls below other running code. For example, right now if you pass something like a `data.frame` to `data` in `lgb.cv()` without including `labels`, you won't find out that you have a malformed training dataset  until after a lot of other steps have run, including initializing a `Predictor`.

In this PR,  I propose a few changes throughout  the package which move input validation up toward the beginning of functions, so that users are alerted sooner when something is wrong. I've also added unit tests on these checks  to be sure they're behaving correctly.
Part of #2604.

Not sure about the implementation, maybe it's better to have something like `feature_infos_json_`?
Operating System: Ubuntu 16.04 LTS

CPU/GPU model: Xeon / 1080ti

C++/Python/R version: 3.6.6

LightGBM version or commit hash: 2.2.4

[lgbm_waitforfailure.zip](https://github.com/microsoft/LightGBM/files/3999018/lgbm_waitforfailure.zip)

```
2.2.4
python lgbm_waitforfailure.py: /root/repo/LightGBM/compute/include/boost/compute/utility/wait_list.hpp:166: void boost::compute::wait_list::wait() const: Assertion `clWaitForEvents(size(), get_event_ptr()) == 0' failed.
Aborted (core dumped)
```

```
#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51
#1  0x00007f70f267d801 in __GI_abort () at abort.c:79
#2  0x00007f70f266d39a in __assert_fail_base (fmt=0x7f70f27f47d8 "%s%s%s:%u: %s%sAssertion `%s' failed.\n%n", assertion=assertion@entry=0x7f7079221aa0 "clWaitForEvents(size(), get_event_ptr()) == 0", file=file@entry=0x7f7079221a58 "/root/repo/LightGBM/compute/include/boost/compute/utility/wait_list.hpp", line=line@entry=166, function=function@entry=0x7f707923d800 <boost::compute::wait_list::wait() const::__PRETTY_FUNCTION__> "void boost::compute::wait_list::wait() const") at assert.c:92
#3  0x00007f70f266d412 in __GI___assert_fail (assertion=0x7f7079221aa0 "clWaitForEvents(size(), get_event_ptr()) == 0", file=0x7f7079221a58 "/root/repo/LightGBM/compute/include/boost/compute/utility/wait_list.hpp", line=166, function=0x7f707923d800 <boost::compute::wait_list::wait() const::__PRETTY_FUNCTION__> "void boost::compute::wait_list::wait() const") at assert.c:101
#4  0x00007f7078fe4446 in boost::compute::wait_list::wait() const [clone .part.134] () from /home/jon/.pyenv/versions/3.6.6/lib/python3.6/site-packages/lightgbm_gpu/lib_lightgbm.so
#5  0x00007f70791d52ac in void LightGBM::GPUTreeLearner::WaitAndGetHistograms<LightGBM::HistogramBinEntry>(LightGBM::HistogramBinEntry*) () from /home/jon/.pyenv/versions/3.6.6/lib/python3.6/site-packages/lightgbm_gpu/lib_lightgbm.so
#6  0x00007f70791cadd5 in LightGBM::GPUTreeLearner::ConstructHistograms(std::vector<signed char, std::allocator<signed char> > const&, bool) () from /home/jon/.pyenv/versions/3.6.6/lib/python3.6/site-packages/lightgbm_gpu/lib_lightgbm.so
#7  0x00007f70791e50ee in LightGBM::SerialTreeLearner::FindBestSplits() () from /home/jon/.pyenv/versions/3.6.6/lib/python3.6/site-packages/lightgbm_gpu/lib_lightgbm.so
#8  0x00007f70791e6e37 in LightGBM::SerialTreeLearner::Train(float const*, float const*, bool, json11::Json&) () from /home/jon/.pyenv/versions/3.6.6/lib/python3.6/site-packages/lightgbm_gpu/lib_lightgbm.so
#9  0x00007f707904a7c5 in LightGBM::GBDT::TrainOneIter(float const*, float const*) () from /home/jon/.pyenv/versions/3.6.6/lib/python3.6/site-packages/lightgbm_gpu/lib_lightgbm.so
#10 0x00007f7078fed41e in LGBM_BoosterUpdateOneIter () from /home/jon/.pyenv/versions/3.6.6/lib/python3.6/site-packages/lightgbm_gpu/lib_lightgbm.so
#11 0x00007f70c8554dae in ffi_call_unix64 () from /usr/lib/x86_64-linux-gnu/libffi.so.6
#12 0x00007f70c855471f in ffi_call () from /usr/lib/x86_64-linux-gnu/libffi.so.6
#13 0x00007f70c8768d5d in _call_function_pointer (argcount=2, resmem=0x7fff29cd4f70, restype=<optimized out>, atypes=<optimized out>, avalues=0x7fff29cd4f50, pProc=0x7f7078fed3e0 <LGBM_BoosterUpdateOneIter>, flags=4353) at /tmp/python-build.20190530091007.28149/Python-3.6.6/Modules/_ctypes/callproc.c:809
#14 _ctypes_callproc (pProc=pProc@entry=0x7f7078fed3e0 <LGBM_BoosterUpdateOneIter>, argtuple=argtuple@entry=0x7f70796c7748, flags=4353, argtypes=argtypes@entry=0x0, restype=0x561d5fdafe98, checker=0x0) at /tmp/python-build.20190530091007.28149/Python-3.6.6/Modules/_ctypes/callproc.c:1166
#15 0x00007f70c875fae7 in PyCFuncPtr_call (self=self@entry=0x7f70796d1c00, inargs=inargs@entry=0x7f70796c7748, kwds=kwds@entry=0x0) at /tmp/python-build.20190530091007.28149/Python-3.6.6/Modules/_ctypes/_ctypes.c:3962
#16 0x00007f70f2aa3189 in _PyObject_FastCallDict (func=0x7f70796d1c00, args=<optimized out>, nargs=<optimized out>, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2331
#17 0x00007f70f2aa3621 in _PyObject_FastCallKeywords (func=func@entry=0x7f70796d1c00, stack=stack@entry=0x561d62f90da0, nargs=nargs@entry=2, kwnames=kwnames@entry=0x0) at Objects/abstract.c:2496
#18 0x00007f70f2b9bb69 in call_function (pp_stack=pp_stack@entry=0x7fff29cd5298, oparg=<optimized out>, kwnames=kwnames@entry=0x0) at Python/ceval.c:4854
#19 0x00007f70f2ba0ec1 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3328
#20 0x00007f70f2b9ba0a in _PyEval_EvalCodeWithName (_co=0x7f70d447ca50, globals=<optimized out>, locals=locals@entry=0x0, args=args@entry=0x561d616851a8, argcount=argcount@entry=1, kwnames=0x7f70d448a4f8, kwargs=0x561d616851b0, kwcount=1, kwstep=1, defs=0x7f70d44859e0, defcount=2, kwdefs=0x0, closure=0x0, name=0x7f70f31395a8, qualname=0x7f70d447d3f0) at Python/ceval.c:4159
#21 0x00007f70f2b9bca2 in fast_function (kwnames=0x7f70d448a4e0, nargs=1, stack=0x561d616851a8, func=0x7f70796b2950) at Python/ceval.c:4971
#22 call_function (pp_stack=pp_stack@entry=0x7fff29cd5540, oparg=<optimized out>, kwnames=kwnames@entry=0x7f70d448a4e0) at Python/ceval.c:4851
#23 0x00007f70f2ba0f41 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3344
#24 0x00007f70f2b9ba0a in _PyEval_EvalCodeWithName (_co=0x7f70796a95d0, globals=<optimized out>, locals=locals@entry=0x0, args=args@entry=0x561d60799ba0, argcount=argcount@entry=3, kwnames=0x7f70d4483160, kwargs=0x561d60799bb8, kwcount=10, kwstep=1, defs=0x7f70796ba060, defcount=14, kwdefs=0x0, closure=0x0, name=0x7f70ae073b20, qualname=0x7f70ae073b20) at Python/ceval.c:4159
#25 0x00007f70f2b9bca2 in fast_function (kwnames=0x7f70d4483148, nargs=3, stack=0x561d60799ba0, func=0x7f70796b7158) at Python/ceval.c:4971
#26 call_function (pp_stack=pp_stack@entry=0x7fff29cd57e0, oparg=<optimized out>, kwnames=kwnames@entry=0x7f70d4483148) at Python/ceval.c:4851
#27 0x00007f70f2ba0f41 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3344
#28 0x00007f70f2b9ba0a in _PyEval_EvalCodeWithName (_co=0x7f70796ce8a0, globals=<optimized out>, locals=locals@entry=0x0, args=args@entry=0x561d61676388, argcount=argcount@entry=3, kwnames=0x7f7079fdc5b8, kwargs=0x561d616763a0, kwcount=13, kwstep=1, defs=0x7f70d44673a8, defcount=15, kwdefs=0x0, closure=0x0, name=0x7f70f0faeb20, qualname=0x7f70796be270) at Python/ceval.c:4159
#29 0x00007f70f2b9bca2 in fast_function (kwnames=0x7f7079fdc5a0, nargs=3, stack=0x561d61676388, func=0x7f70796b7bf8) at Python/ceval.c:4971
#30 call_function (pp_stack=pp_stack@entry=0x7fff29cd5a80, oparg=<optimized out>, kwnames=kwnames@entry=0x7f7079fdc5a0) at Python/ceval.c:4851
#31 0x00007f70f2ba0f41 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3344
#32 0x00007f70f2b9ba0a in _PyEval_EvalCodeWithName (_co=0x7f70796c3030, globals=<optimized out>, locals=locals@entry=0x0, args=args@entry=0x561d5fcb93c0, argcount=argcount@entry=3, kwnames=0x7f70f12796e0, kwargs=0x561d5fcb93d8, kwcount=10, kwstep=1, defs=0x7f7079fdc650, defcount=13, kwdefs=0x0, closure=0x7f70796bb9e8, name=0x7f70f0faeb20, qualname=0x7f70796c10c0) at Python/ceval.c:4159
#33 0x00007f70f2b9bca2 in fast_function (kwnames=0x7f70f12796c8, nargs=3, stack=0x561d5fcb93c0, func=0x7f70796c41e0) at Python/ceval.c:4971
#34 call_function (pp_stack=pp_stack@entry=0x7fff29cd5d20, oparg=<optimized out>, kwnames=kwnames@entry=0x7f70f12796c8) at Python/ceval.c:4851
#35 0x00007f70f2ba0f41 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3344
#36 0x00007f70f2b9ba0a in _PyEval_EvalCodeWithName (_co=_co@entry=0x7f70f302eae0, globals=globals@entry=0x7f70f30e01b0, locals=locals@entry=0x7f70f30e01b0, args=args@entry=0x0, argcount=argcount@entry=0, kwnames=kwnames@entry=0x0, kwargs=0x0, kwcount=0, kwstep=2, defs=0x0, defcount=0, kwdefs=0x0, closure=0x0, name=0x0, qualname=0x0) at Python/ceval.c:4159
#37 0x00007f70f2b9c06e in PyEval_EvalCodeEx (_co=_co@entry=0x7f70f302eae0, globals=globals@entry=0x7f70f30e01b0, locals=locals@entry=0x7f70f30e01b0, args=args@entry=0x0, argcount=argcount@entry=0, kws=kws@entry=0x0, kwcount=0, defs=0x0, defcount=0, kwdefs=0x0, closure=0x0) at Python/ceval.c:4180
#38 0x00007f70f2b9c09b in PyEval_EvalCode (co=co@entry=0x7f70f302eae0, globals=globals@entry=0x7f70f30e01b0, locals=locals@entry=0x7f70f30e01b0) at Python/ceval.c:731
#39 0x00007f70f2bd23da in run_mod (arena=0x7f70f30fb078, flags=0x7fff29cd603c, locals=0x7f70f30e01b0, globals=0x7f70f30e01b0, filename=0x7f70f0fc8920, mod=0x561d5fd2c248) at Python/pythonrun.c:1025
#40 PyRun_FileExFlags (fp=fp@entry=0x561d5fca55a0, filename_str=filename_str@entry=0x7f70f1292470 "lgb_prefit_1a802b24-2bff-488d-8a72-142dff84ba18.py", start=start@entry=257, globals=globals@entry=0x7f70f30e01b0, locals=locals@entry=0x7f70f30e01b0, closeit=closeit@entry=1, flags=0x7fff29cd603c) at Python/pythonrun.c:978
#41 0x00007f70f2bd254d in PyRun_SimpleFileExFlags (fp=fp@entry=0x561d5fca55a0, filename=<optimized out>, closeit=closeit@entry=1, flags=flags@entry=0x7fff29cd603c) at Python/pythonrun.c:420
#42 0x00007f70f2bd2943 in PyRun_AnyFileExFlags (fp=fp@entry=0x561d5fca55a0, filename=<optimized out>, closeit=closeit@entry=1, flags=flags@entry=0x7fff29cd603c) at Python/pythonrun.c:81
#43 0x00007f70f2bf0b2e in run_file (p_cf=0x7fff29cd603c, filename=0x561d5fc5d6c0 L"lgb_prefit_1a802b24-2bff-488d-8a72-142dff84ba18.py", fp=0x561d5fca55a0) at Modules/main.c:340
#44 Py_Main (argc=<optimized out>, argv=<optimized out>) at Modules/main.c:810
#45 0x0000561d5ea54b40 in main (argc=2, argv=<optimized out>) at ./Programs/python.c:69

```