Hi.
I believe the library needs some clarifications, maybe proofs of concept also.
For example, is it suitable for high dimensionalities? What about sparse representations? What about large datasets?
Hello,

I believe there is a bug in LSH.scala.
At line 28 `signatures` is an RDD of `((vectorIndex, bandID), minHashValue)`

So the first groupByKey will create an RDD of `( (vectorIndex, bandID), Iterable of minHashValues)` but for a specific bandID nothing guarantees us that all the Iterables of minHashValues will be ordered in the same way.

What I mean is that for example for two SparseVectors `i` and `j`, for a specific band `b` and lets say two hash functions per band, groupByKey may result in something like this:

```
((i,b), Iterable[ minHashValue from H1, minHashValue from H2 ] )
((j,b), Iterable[ minHashValue from H2, minHashValue from H1 ] )

```
So even if `i` and `j` have identical signatures in that band because of this "swapping" that may happen during groupByKey they won't go into the same bucket (with high probability).


Also [here](http://apache-spark-user-list.1001560.n3.nabble.com/Is-shuffle-quot-stable-quot-td7628.html#a7633
) is a link with Matei responding to a question on whether the order assumed here is actually preserved.
This commit adds support for calculating Jaccard similarity using
bag/multiset semantics, as described on pgs. 76-77 in chapter 3 of
Mining of Massive Datasets (MMDS).

MMDS uses the example of movie ratings:

> If ratings are 1-to-5-stars, put a movie in a customer's set n times
> if they rated the movie n-stars. Then, use Jaccard similarity for bags
> when measuring the similarity of customers. The Jaccard similarity for
> bags B and C is defined by counting an element n times in the
> intersection if n is the minimum of the number of times the element
> appears in B and C. In the union, we count the element the sum of the
> number of times it appears in B and C.

To fit a model using bag semantics, the user:
- Instantiates an LSH model with the variable repeatedItems set to
  true. This is an optional variable that is false by default.
- Passes their data into the model as a List or RDD of SparseVectors,
  where the indices of each SparseVector correspond to the distinct
items in the set, and the values of each correspond to the number of
times each corresponding item is repeated in the set.

The only difference in running the model and getting output is that the
Jaccard similarity between two sets with bag semantics has a maximum of
0.5 rather than 1.0.
The current shell script does not include the correct .jar file so when you try to import com.invincea.spark.hash.{LSH, LSHModel} it says it does not exist. This should be fixed by updating ./shell_local.sh to contain `$SPARK_HOME/bin/spark-shell \
  --master local[*] \
  --executor-memory 8G \
  --driver-memory 8G \
  --jars target/spark-hash-0.1.3.jar $@`

It seems useful to use a SparseVector for the input data sets when you only mean to use the indices. Why not use a Set or a List instead?
