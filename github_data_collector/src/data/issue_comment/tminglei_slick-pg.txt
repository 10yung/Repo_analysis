if you import `com.github.tminglei.slickpg.ExPostgresProfile.api._`，
and import `slick.jdbc.PostgresProfile.api._` meanwhile，you will compile error.

if you import `com.github.tminglei.slickpg.ExPostgresProfile.api.Over`，
and import `slick.jdbc.PostgresProfile.api._` meanwhile , you will get exception `slick.SlickException: Unexpected node WindowFuncExpr`.

So, you should  import `com.github.tminglei.slickpg.ExPostgresProfile.api._` instead of `slick.jdbc.PostgresProfile.api._` if you want to use slick-pg
I have the following model:

	final case class Wallet(
							   ...
							   pointExpiry: Option[OffsetDateTime],
							   spinExpiry: OffsetDateTime,
							   ...
							   id: Int = 0,
						   )

	final class WalletTable(tag: Tag) extends Table[Wallet](tag, "wallet") {
		def id = column[Int]("id", O.PrimaryKey, O.AutoInc)
		...
		def pointExpiry = column[Option[OffsetDateTime]]("point_expiry")
		def spinExpiry = column[OffsetDateTime]("spin_expiry")
		...
		def * = (..., pointExpiry, spinExpiry, ..., id).mapTo[Wallet]
	}

I tried to `INSERT`, and here's the log:

	15 00:30:19 [DB-1] DEBUG slick.compiler.QueryCompiler - Source:
	| Bind
	|   from s2: TableExpansion
	|     table s3: Table wallet
	|     columns: TypeMapping
	|       0: ProductNode
	...
	|         4: Path s3.point_expiry : Option[java.time.OffsetDateTime']
	|         5: Path s3.spin_expiry : java.time.OffsetDateTime'
	...
	|         16: Path s3.id : Int'
	|   select: Pure t4
	|     value: Path s2.id : Int'
	...
	15 00:30:19 [DB-1] DEBUG slick.jdbc.JdbcBackend.statement - Preparing insert statement (returning: id): insert into "wallet" (...,"point_expiry","spin_expiry",...)  values (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)
	15 00:30:19 [DB-1] DEBUG slick.jdbc.JdbcBackend.statement - Executing prepared update: HikariProxyPreparedStatement@2041279209 wrapping insert into "wallet" (...,"point_expiry","spin_expiry",...)  values (...,NULL,'2100-12-31T00:00+07:00',...)
	RETURNING "id"
	15 00:30:19 [DB-1] DEBUG slick.jdbc.JdbcBackend.parameter - /-----+-----+-----+---------+------------------------+-----+-----+---------+---------+-----+--------+---------+---------+---------------------------+---------------------------\
	15 00:30:19 [DB-1] DEBUG slick.jdbc.JdbcBackend.parameter - | 1   | 2   | 3   | 4       | 5                      | 6   | 7   | 8       | 9       | 10  | 11     | 12      | 13      | 14                        | 15                        |
	15 00:30:19 [DB-1] DEBUG slick.jdbc.JdbcBackend.parameter - | Int | Int | Int | VARCHAR | String                 | Int | Int | DECIMAL | DECIMAL | Int | String | Boolean | VARCHAR | String                    | String                    |
	15 00:30:19 [DB-1] DEBUG slick.jdbc.JdbcBackend.parameter - |-----+-----+-----+---------+------------------------+-----+-----+---------+---------+-----+--------+---------+---------+---------------------------+---------------------------|
	15 00:30:19 [DB-1] DEBUG slick.jdbc.JdbcBackend.parameter - | 1   | 1   | 1   | NULL    | 2100-12-31T00:00+07:00 | 100 | 100 | NULL    | NULL    | 0   | 111    | true    | NULL    | 2019-12-15T00:30:05.37... | 2019-12-15T00:30:05.37... |
	15 00:30:19 [DB-1] DEBUG slick.jdbc.JdbcBackend.parameter - \-----+-----+-----+---------+------------------------+-----+-----+---------+---------+-----+--------+---------+---------+---------------------------+---------------------------/


What I don't understand: **If I copy-paste the generated INSERT statement from the log and run it manually, the data is successfully inserted.**

But from Tomcat (where I'm calling it from) it throws error:

	15-Dec-2019 00:30:19.560 SEVERE [http-nio-8080-exec-1] org.apache.catalina.core.StandardWrapperValve.invoke Servlet.service() for servlet [jersey-servlet] in context with path [] threw exception [org.glassfish.jersey.server.ContainerException: org.postgresql.util.PSQLException: ERROR: column "point_expiry" is of type date but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 225] with root cause
		org.postgresql.util.PSQLException: ERROR: column "point_expiry" is of type date but expression is of type character varying
	  Hint: You will need to rewrite or cast the expression.
	  Position: 225
			at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2505)
			at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2241)
			at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:310)
			at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:447)
			at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:368)
			at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:158)
			at org.postgresql.jdbc.PgPreparedStatement.executeUpdate(PgPreparedStatement.java:124)
			at com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyPreparedStatement.java:61)
			at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeUpdate(HikariProxyPreparedStatement.java)
			at slick.jdbc.LoggingPreparedStatement.$anonfun$executeUpdate$5(LoggingStatement.scala:170)

What do I miss?
I have a table 

```
  case class JsonBean2(id: Long, json: JsValue)

  class JsonTestTable(tag: Tag) extends Table[JsonBean2](tag, "JsonTest2") {
    def id = column[Long]("id", O.AutoInc, O.PrimaryKey)
    def json = column[JsValue]("json", O.Default(Json.parse(""" {"a":"v1","b":2} """)))
    def * = (id,  json) <> (JsonBean2.tupled, JsonBean2.unapply)
  }
  val JsonTests = TableQuery[JsonTestTable]
```
and I insert data
```

  val testRec5 = JsonBean2(33L, Json.parse(""" {"interestedIn":[11],"countries":["IT", "UK"]} """))
  val testRec6 = JsonBean2(34L, Json.parse(""" {"interestedIn":[12],"countries":["US", "UK"]} """))
  val testRec7 = JsonBean2(35L, Json.parse(""" {"interestedIn":[13],"countries":["IT2", "UK"]} """))
  val testRec8 = JsonBean2(36L, Json.parse(""" {"interestedIn":[14],"countries":["IT3", "UK"]} """))
  val testRec9 = JsonBean2(37L, Json.parse(""" {"interestedIn":[16],"countries":["IT1", "UK"]} """))

```
I know how to filter in array of string
```
db.run(
        JsonTests
          .filter(row => row.json.+>("countries") ?| List("US").bind)
          .result
    )
```

but how I can filter in array of Int?

```
db.run(
        JsonTests
          .filter(row => row.json.+>("interestedIn") ?| List(11).bind)
          .result
    )
```

my variant does not work

It would be nice if we could use `.filter` on components of a `groupBy` aggregation, translating to postgres `FILTER` syntax. Right now, we need to use `Case.If(...` manually.

https://medium.com/little-programming-joys/the-filter-clause-in-postgres-9-4-3dd327d3c852
I need to take the min of the max in each group, e.g.:

```scala
import com.github.tminglei.slickpg.ExPostgresProfile.api._
import com.github.tminglei.slickpg.agg.PgAggFuncSupport.GeneralAggFunctions._

...

Table.map{ q =>
        (
          max(q.col).over.partitionBy(q.id).sortBy(q.col),
          q.id
        )
      }
.groupBy{_ => true}
      .map {
        case (_, query) =>
          query.map(_._1).min
      }
```

bit it attempts to compose the window function inside the aggregate, which is not allowed and yields:

> The future returned an exception of type: org.postgresql.util.PSQLException, with message: ERROR: aggregate function calls cannot contain window function calls

Can it be supported such that it does a subquery instead?
Hi, I'm trying to use slick-pg to use the Large Object Postgresl functionality, but keep getting the following exception, which I do not understand.

```
[info] - must Control Emulations *** FAILED ***
[info]   org.h2.jdbc.JdbcSQLDataException: Invalid value "interface org.postgresql.PGConnection" for parameter "iface" [90008-199]
[info]   at org.h2.message.DbException.getJdbcSQLException(DbException.java:587)
[info]   at org.h2.message.DbException.getJdbcSQLException(DbException.java:427)
[info]   at org.h2.message.DbException.get(DbException.java:205)
[info]   at org.h2.message.DbException.getInvalidValueException(DbException.java:280)
[info]   at org.h2.jdbc.JdbcConnection.unwrap(JdbcConnection.java:1942)
[info]   at com.zaxxer.hikari.pool.ProxyConnection.unwrap(ProxyConnection.java:455)
[info]   at com.github.tminglei.slickpg.lobj.LargeObjectSupport.$anonfun$buildLargeObjectUploadAction$1(LargeObjectSupport.scala:25)
[info]   at com.github.tminglei.slickpg.lobj.LargeObjectSupport.$anonfun$buildLargeObjectUploadAction$1$adapted(LargeObjectSupport.scala:23)
[info]   at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70)
[info]   at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69)
[info]   at slick.dbio.DBIOAction$$anon$1.$anonfun$run$1(DBIOAction.scala:186)
[info]   at scala.collection.Iterator.foreach(Iterator.scala:941)
[info]   at scala.collection.Iterator.foreach$(Iterator.scala:941)
[info]   at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
[info]   at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[info]   at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[info]   at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[info]   at slick.dbio.DBIOAction$$anon$1.run(DBIOAction.scala:186)
[info]   at slick.dbio.DBIOAction$$anon$1.run(DBIOAction.scala:183)
[info]   at slick.basic.BasicBackend$DatabaseDef$$anon$3.liftedTree1$1(BasicBackend.scala:276)
[info]   at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:276)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[info]   at java.lang.Thread.run(Thread.java:748)
```


The exception is thrown from the following action, taken from . the README [here](https://github.com/tminglei/slick-pg/tree/868f3666197ff3a0a1f233c2a9905feb483551f4/core/src/main/scala/com/github/tminglei/slickpg/lobj). 

```
    val driver = new LargeObjectSupport with ExPostgresProfile {}
    driver.buildLargeObjectUploadAction(inputStream)
```

I have also tried using the `MyPostgresProfile` example profile from the main README, which gives the same exception.

Thanks for any assistance.
Which version of this project is safe to use in production?
Hi,


Are you accepting forks for this repo? Would you be interested in supporting the "FILTER" aggregate function?
https://modern-sql.com/feature/filter


It is extremely useful for implemeting the "pivot method":
https://modern-sql.com/use-case/pivot


I've implemented a custom aggregation function - "array agg", but it involved quite some work (see my below comment).

Do you have any guiding principles for how implementing something like FILTER could be done cleanly with your api?

Using `gEquals` for geometry equality comparisons: 
```scala
  def foo(toFind: Option[Geometry]): DBIO[Seq[Int]] = {
    toFind match {
      case Some(f) => someTable.filter(_.geometry.gEquals(f.bind)).map(_.id).result
      case None    => DBIO.successful(Seq.empty)
    }
  }
```
Getting an error with the types of `ST_Equals`:

```
org.postgresql.util.PSQLException: ERROR: function st_equals("someSchema".geometry, bytea) does not exist
  Hint: No function matches the given name and argument types. You might need to add explicit type casts.
```
What is that `bytea` doing? Or is this likely something to be related to the schema that PostGIS is installed onto?

Thanks
We would like to be able to use the sum window function documented [here](https://www.postgresql.org/docs/9.1/tutorial-window.html).

e.g.
SELECT salary, sum(salary) OVER (ORDER BY salary) FROM empsalary;