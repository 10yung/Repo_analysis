The kubelet fails to start if a machine's hostname is not set. If
networkd doesn't set it in time, the kubelet service fails to start.
Addionally, this adds retries to container pulls to ensure that any
temporary network failures don't cause fatal errors if we can't pull
images.
This implements `osctl cluster destroy` for Firecracker, adds
new utility command `osctl cluser show`.

Firecracker mode now has control process for firecracker VMs, allowing
clean reboots and background operations.

Lots of small fixes to Firecracker mode, clean CNI shutdown, cleaning up
netns, etc.
If we don't block, there is the potential for multiple shutdown,
reboot, and upgrade requests to be processed.
Documentation on talos.dev and documentation in osctl state that you must supply a "load balancer IP or DNS name" for when you run `osctl config generate` but does not specify that a port is required. If a port is not specified bootkube fails. This should probably be corrected by both having a default of 6443 and updating documentation to inform the user of what we're actually specifying and why.
osctl has references like `<id>` that are unclear. For example:
```
Usage:
  osctl logs <id> [flags]
```
Flags are well described but id is never clarified either in the docs or in the cli help.
Add pre-flight checks for CNI modules required, /dev/kvm accessibility, etc.

This should make it easier for first-time users to successfully complete `osctl cluster create --provisioner=firecracker`
The `client.Creds` struct was not used very often, and made using the
`client.NewClient` function impossible to use in combination with the
`RemoteRenewingFileCertificateProvider`. This modifies
`client.NewClient` to accept a `tls.Config` instead of `client.Creds`,
allowing for the use of `RemoteRenewingFileCertificateProvider` with
`client.NewClient`.
What occurs:

I installed a cluster with 3 master nodes:

```
root@matchbox:~# kubectl get nodes
NAME   STATUS   ROLES    AGE     VERSION
mal    Ready    master   15m     v1.17.0
wash   Ready    master   9m53s   v1.17.0
zoe    Ready    master   10m     v1.17.0
root@matchbox:~# kubectl get pods -n kube-system
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-648f4868b8-4sp56   1/1     Running   0          16m
calico-node-5bfdx                          1/1     Running   0          16m
calico-node-nxwwx                          1/1     Running   0          11m
calico-node-s6lkd                          1/1     Running   0          10m
coredns-5bcc66f5bd-zsjkw                   1/1     Running   0          16m
kube-apiserver-cljr7                       1/1     Running   0          10m
kube-apiserver-x7hsd                       1/1     Running   3          16m
kube-apiserver-zqsdc                       1/1     Running   0          10m
kube-controller-manager-7977b8b84f-6m745   1/1     Running   0          16m
kube-controller-manager-7977b8b84f-skbcd   1/1     Running   0          16m
kube-proxy-7vgdm                           1/1     Running   0          11m
kube-proxy-9k2lx                           1/1     Running   0          16m
kube-proxy-cmjqz                           1/1     Running   0          10m
kube-scheduler-6db5bfb7bc-dpmw6            1/1     Running   1          16m
kube-scheduler-6db5bfb7bc-qtmzk            1/1     Running   0          16m
pod-checkpointer-6h7s8                     1/1     Running   0          10m
pod-checkpointer-6h7s8-zoe                 1/1     Running   0          10m
pod-checkpointer-lflph                     1/1     Running   0          10m
pod-checkpointer-lflph-wash                1/1     Running   0          9m48s
pod-checkpointer-pgxml                     1/1     Running   0          16m
pod-checkpointer-pgxml-mal                 1/1     Running   0          15m
root@matchbox:~# osctl service etcd -e 172.24.3.31
NODE     172.24.3.31
ID       etcd
STATE    Running
HEALTH   OK
EVENTS   [Running]: Health check successful (7m21s ago)
         [Running]: Started task etcd (PID 2819) for container etcd (7m22s ago)
         [Preparing]: Creating service runner (7m24s ago)
         [Preparing]: Running pre state (7m56s ago)
         [Waiting]: Waiting for service "containerd" to be "up" (7m57s ago)
root@matchbox:~# osctl service etcd -e 172.24.3.32
NODE     172.24.3.32
ID       etcd
STATE    Running
HEALTH   OK
EVENTS   [Running]: Health check successful (1m51s ago)
         [Running]: Started task etcd (PID 2832) for container etcd (1m51s ago)
         [Preparing]: Creating service runner (1m57s ago)
         [Preparing]: Running pre state (2m22s ago)
         [Waiting]: Waiting for service "containerd" to be "up" (2m23s ago)
root@matchbox:~# osctl service etcd -e 172.24.3.33
NODE     172.24.3.33
ID       etcd
STATE    Running
HEALTH   OK
EVENTS   [Running]: Health check successful (1m3s ago)
         [Running]: Started task etcd (PID 2853) for container etcd (1m4s ago)
         [Preparing]: Creating service runner (1m18s ago)
         [Preparing]: Running pre state (1m50s ago)
         [Waiting]: Waiting for service "containerd" to be "up" (1m51s ago)
```

I issued the command:

`osctl upgrade -i docker.io/andrewrynhard/installer:acpi -n 172.24.3.31`

To get the latest ACPI fixes by Andrew.  The output of dmesg:

```
172.24.3.31: user: warning: [2020-01-13T02:11:21.070055262Z]: [talos] [phase]: cordon and drain node
172.24.3.31: user: warning: [2020-01-13T02:11:21.154438262Z]: [talos] skipping DaemonSet pod calico-node-5bfdx
172.24.3.31: user: warning: [2020-01-13T02:11:21.156598262Z]: [talos] skipping DaemonSet pod kube-proxy-9k2lx
172.24.3.31: user: warning: [2020-01-13T02:11:21.158418262Z]: [talos] skipping DaemonSet pod pod-checkpointer-pgxml
172.24.3.31: user: warning: [2020-01-13T02:11:21.160359262Z]: [talos] skipping DaemonSet pod kube-apiserver-x7hsd
172.24.3.31: user: warning: [2020-01-13T02:11:27.103059262Z]: [talos] leaving etcd cluster
172.24.3.31: user: warning: [2020-01-13T02:11:27.107235262Z]: [talos] service[etcd](Stopping): Sending SIGTERM to task etcd (PID 2835, container etcd)
172.24.3.31: user: warning: [2020-01-13T02:11:28.696288262Z]: [talos] service[etcd](Finished): Service finished successfully
172.24.3.31: user: warning: [2020-01-13T02:11:35.766529262Z]: [talos] WARNING: failed to evict pod: failed to evict pod kube-system/kube-controller-manager-7977b8b84f-skbcd: etcdserver: request timed out
172.24.3.31: user: warning: [2020-01-13T02:11:35.772629262Z]: [talos] WARNING: failed to evict pod: failed waiting on pod kube-system/kube-controller-manager-7977b8b84f-6m745 to be deleted: 2 error(s) occurred:\x0a\x09pod is still running on the node\x0a\x09failed to get pod kube-system/kube-controller-manager-7977b8b84f-6m745: etcdserver: request timed out
172.24.3.31: user: warning: [2020-01-13T02:11:35.784143262Z]: [talos] WARNING: failed to evict pod: failed waiting on pod kube-system/kube-scheduler-6db5bfb7bc-dpmw6 to be deleted: 2 error(s) occurred:\x0a\x09pod is still running on the node\x0a\x09failed to get pod kube-system/kube-scheduler-6db5bfb7bc-dpmw6: etcdserver: request timed out
172.24.3.31: user: warning: [2020-01-13T02:11:35.789531262Z]: [talos] WARNING: failed to evict pod: failed waiting on pod kube-system/coredns-5bcc66f5bd-zsjkw to be deleted: 2 error(s) occurred:\x0a\x09pod is still running on the node\x0a\x09failed to get pod kube-system/coredns-5bcc66f5bd-zsjkw: etcdserver: request timed out
172.24.3.31: user: warning: [2020-01-13T02:11:35.794822262Z]: [talos] WARNING: failed to evict pod: failed waiting on pod kube-system/calico-kube-controllers-648f4868b8-4sp56 to be deleted: 2 error(s) occurred:\x0a\x09pod is still running on the node\x0a\x09failed to get pod kube-system/calico-kube-controllers-648f4868b8-4sp56: etcdserver: request timed out
172.24.3.31: user: warning: [2020-01-13T02:11:35.799568262Z]: [talos] WARNING: failed to evict pod: failed to evict pod kube-system/pod-checkpointer-pgxml-mal: etcdserver: request timed out
```

It doesn't finish upgrading, nor does it reboot.   In one instance, I saw a "could not drain node" failure and "upgrade failed", although I couldn't reproduce that on the latest attempt.

When I got the "upgraded failed" and reboot, I get a partitioned cluster.  The 1st node comes up, and I believe re-initializes as a 1-node etcd cluster.  Whatever the case, I know if I issue `kubectl get pods -n kube-system` three times, and given that the haproxy config in my environment is round robin, 1/3rd of the time I get "no pods in the namespace" and 2/3rds of the time I get the listing of pods I expect.

What I expected:

The upgrade to succeed.

Details about my environment:

v0.3.0-beta.3 PXE booted to a laptop
haproxy load balancer in front of the API server
calico manifests for a custom CNI
What occurs:

I installed a node with an "init" configuration.

I issued the command:

`osctl upgrade -i docker.io/andrewrynhard/installer:acpi -n 172.24.3.31`

To get the latest ACPI fixed by Andrew.  The output of dmesg:

`172.24.3.31: user: warning: [2020-01-13T00:01:55.481220792Z]: [talos] [phase]: cordon and drain node`
`172.24.3.31: user: warning: [2020-01-13T00:01:55.538204792Z]: [talos] skipping DaemonSet pod kube-proxy-gcc76`
`172.24.3.31: user: warning: [2020-01-13T00:01:55.540434792Z]: [talos] skipping DaemonSet pod calico-node-h2kvx`
`172.24.3.31: user: warning: [2020-01-13T00:01:55.543688792Z]: [talos] skipping DaemonSet pod kube-apiserver-fnq84`
`172.24.3.31: user: warning: [2020-01-13T00:01:55.546667792Z]: [talos] skipping DaemonSet pod pod-checkpointer-vpgnl`
`172.24.3.31: user: warning: [2020-01-13T00:01:58.677078792Z]: [talos] leaving etcd cluster`
`172.24.3.31: user: warning: [2020-01-13T00:01:58.681622792Z]: [talos] [phase]: cordon and drain node error running task: etcdserver: re-configuration failed due to not enough started members`

It doesn't reboot.  When an `osctl reboot` is issued, dmesg logs a message but fails:

`172.24.3.31: user: warning: [2020-01-13T00:12:35.111990792Z]: [talos] reboot via API received`

What I expected:

The upgrade to succeed.

Details about my environment:

v0.3.0-beta.3 PXE booted to a laptop
haproxy load balancer in front of the API server
calico manifests for a custom CNI

The automated certificate renewal code seems to fail if the init node's trustd service is not running. The client should try all known trustd endpoints.