Hi, thank you for your contribution to Mantl! Before we can accept any code into
master, we need it to meet the following criteria. If there are any you can't
satisfy yourself, go ahead and open the pull request anyway and we'll help you
test. Feel free to delete this message once you're done. Thanks again!

- [ ] Installs cleanly on a fresh build of most recent master branch
- [ ] Upgrades cleanly from the most recent release
- [ ] Updates documentation relevant to the changes

I basically have the same question based on #1439. Would appreciate if anyone can provide some guidance on adding the wildcard entries in the consul DNS, which is new thing for me.
This url [https://dl.bintray.com/shippedrepos/mantl-1.3/](url) can no longer be used for package downloads. It's forbidden. Does anyone know what's happening, how can i fix that
Been days now trying to traefik to do ssl/https. Seems its an IMPOSSIBILITY for the thing.

Hi! We'd love to help you with whatever problem you're having with Mantl. To
help us, please provide the following along with a description of your issue.
You can delete this message once you're done. Thanks!

- Logs from the failing component, if applicable. Might be any of the following:
    - `journalctl --no-pager -l -u <component>`
    - `docker logs <container>`
    - `kubectl logs <pod>`
    - Sandbox logs from the Mesos UI

- Changes made to `sample.yml`, if applicable.
- Ansible version (`ansible --version`):
- Python version (`python --version`):
- Git commit hash or branch:
- Cloud Environment:
- Terraform version (`terraform version`):



most of the containers are showing x509 errors due to aws internal dns name as nodename not being in certificate

kubectl cluster-info dump

==== START logs for kube-system/wensleydale-1979955250-g4x1c ====
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"Get https://ip-10-1-1-161.us-west-2.compute.internal:10250/containerLogs/kube-system/wensleydale-1979955250-g4x1c/cheese: x509: certificate is valid for localhost, mantl-kubeworker-001, mantl-kubeworker-001.node.consul, mantl-kubeworker-001.ec2.internal, ip-10-1-1-161, ip-10-1-1-161.node.consul, ip-10-1-1-161.ec2.internal, *.service.consul, kubernetes.default.svc.cluster.local, not ip-10-1-1-161.us-west-2.compute.internal","code":500}

20:35 $ ansible --version
ansible 2.3.0.0
  config file = /Users/kbroughton/projects/mantl/ansible.cfg
  configured module search path = Default w/o overrides
  python version = 2.7.13 |Anaconda custom (x86_64)| (default, Dec 20 2016, 23:05:08) [GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]
python --version 2.7.13
08:05 $ git log -n 3
commit 5503a4296b6c4c61847f6f0cf8f6e46219c3f531
Merge: b8b032e1 04cdffda
Author: Thomas Vincent <thomasvincent@gmail.com>
Date:   Mon Jun 5 01:26:03 2017 -0700- Cloud Environment:
✔ ~/projects/mantl [master ↓·4|✚ 1…3]
08:06 $ terraform --version
Terraform v0.9.6


It looks like either the nodename should be set to a mantl friendly name / hostname (mantl-control-01 ...) or the cert should be build with the aws internal dns name.
move etcD to 3.1.8
Was discussing with @thomasvincent on Mattermost that changes in the recent minor release of Kubernetes v1.6.0+ have changed the way that access to a cluster is granted and propagated.

The old method that I was using in 1.5.2 was to log into an API/controller node in the cluster directly and run kubectl against 127.0.0.1:8080 -- this mode of access has gone away with 1.6, now I think you are meant to connect via SSL with the generated certificate admin credential for the first time, and perhaps also grant permissions to users somehow using RBAC controls that are in beta now.

I have no idea how this works but I am sure that how it works is new and complicated, and should be mentioned in the FAQ.
Updating Traefik to latest version and fix #2011

fixes #1744
Removes deprecated call in api-server yaml file
tracking issue
Error validating: 32 error(s) occurred:

* Variable 'control_count': duplicate found. Variable names must be unique.
* Variable 'worker_count': duplicate found. Variable names must be unique.
* Variable 'edge_count': duplicate found. Variable names must be unique.
* Variable 'control_count': duplicate found. Variable names must be unique.
* Variable 'control_type': duplicate found. Variable names must be unique.
* Variable 'datacenter': duplicate found. Variable names must be unique.
* Variable 'edge_count': duplicate found. Variable names must be unique.
* Variable 'edge_type': duplicate found. Variable names must be unique.
* Variable 'kubeworker_count': duplicate found. Variable names must be unique.
* Variable 'long_name': duplicate found. Variable names must be unique.
* Variable 'short_name': duplicate found. Variable names must be unique.
* Variable 'ssh_key': duplicate found. Variable names must be unique.
* Variable 'worker_count': duplicate found. Variable names must be unique.
* Variable 'worker_type': duplicate found. Variable names must be unique.
* Variable 'ssh_user': duplicate found. Variable names must be unique.
* Variable 'control_count': duplicate found. Variable names must be unique.
* Variable 'worker_count': duplicate found. Variable names must be unique.
* Variable 'kubeworker_count': duplicate found. Variable names must be unique.
* Variable 'edge_count': duplicate found. Variable names must be unique.
* Variable 'control_count': duplicate found. Variable names must be unique.
* Variable 'edge_count': duplicate found. Variable names must be unique.
* Variable 'image': duplicate found. Variable names must be unique.
* Variable 'kubeworker_count': duplicate found. Variable names must be unique.
* Variable 'short_name': duplicate found. Variable names must be unique.
* Variable 'worker_count': duplicate found. Variable names must be unique.
* terraform-py: module repeated multiple times
* control-nodes: module repeated multiple times
* edge-nodes: module repeated multiple times
* worker-nodes: module repeated multiple times
* kubeworker-nodes: module repeated multiple times
* ssh-key: module repeated multiple times
* network: module repeated multiple times