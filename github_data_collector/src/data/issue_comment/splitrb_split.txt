**Describe the bug**

`ab_test` triggers multiple Redis commands for all experiments for a single experiment `ab_test(:experiment_1)` call. 

If in a single request flow I call `ab_test(:experiment_1)`, `ab_test(:experiment_2)` and `ab_test(:experiment_3)` it will perform multiple commands for the same experiments overloading Redis. 

**To Reproduce**

Open `redis-cli monitor`  then:

```ruby
ab_test(:experiment_1)
```

A single call will perform `get`, `hget`, `type`, `lrange` etc for all other experiments defined in the `experiments.yml`. 

**Expected behavior**

Redis calls only for the given experiment.
**Describe the bug**
The Split tool doesn't proportionally split traffic across utm_source, utm_medium and utm_campaign which requires us to normalize the data for traffic and calculate the statistical significance outside of split.

**To Reproduce**
We see the page visits in Mixpanel and the number of page views to each variant from different utm_source, utm_medium and utm_campaign is not the same.

**Expected behavior**
If the traffic is split 50:50, then each variant should get an equal number of page views from all the campaigns that bring users to the page. Therefore, we can be confident of the conversion difference and statistical significance in the Split dashboard.


Related to https://github.com/splitrb/split/issues/571
Hey folks.

In our application, we have to know when a particular experiment's winner was chosen.

There are already implemented integration point such as:
- `on_trial`
- `on_trial_choose`
- `on_trial_complete`
- `on_before_experiment_reset`
- `on_experiment_reset`
- `on_experiment_delete`

I propose to implement a hook like:
- `on_winner_choose`

If you have a solution which doesn't require the implementation of this hook please let me know.

If a solution doesn't exist, please let me know if you are interested in this functionality or not. It looks like I will add it anyway, but in prospect, it can be part of the library.
This PR removes sinatra as depency, introducing a very light (and very coupled) routing and rendering solution. The needs are small enough that the coupling introduced will probably not be a problem in the future.
As discussed with @andrehjr , this PR depends on a refactor for the force session feature and is currently broken.
I'm keeping in WIP state for now.

One of my experiments have one alternative winning by 7% but it has lower probability of winning (41%). Does it make any sense? I can't figure out the math behind it.

There is still no sufficient confidence, and I think it will normalize with more participants, but I was intrigued by this and wanted to understand it better. Does anyone have any idea why this is going on?

![screen shot 2018-08-08 at 17 36 23](https://user-images.githubusercontent.com/2954723/43863826-461e017c-9b34-11e8-857f-7a391deb145a.png)

With the new GDPR directive coming in on the 25 May 2018, it would be helpful to update the README[1] with a section on GDPR and what cookies Split stores along with a description (perhaps the structure/attributes) of what is stored[2].

[1] https://github.com/splitrb/split/blob/master/README.md
[2] https://www.itgovernance.eu/blog/en/how-the-gdpr-affects-cookie-policies
Hi, we've been running a few experiments using Split and noticed nearly all of our variants were losing over the control version by pretty hefty margins. So we set up a test to see how Split performed when splitting an 'experiment' where the control and the variant were the exact same experience. This test found the variant losing to the control by nearly 20% with a 95% confidence interval. Any suggestions as to what might be causing this or how we can improve the accuracy of our results? This was using the default splitting algorithm. Would switching to the block randomization algorithm help us out at all?

<img width="865" alt="screen shot 2018-04-13 at 1 52 37 pm" src="https://user-images.githubusercontent.com/1613599/38750241-56174e44-3f22-11e8-9b21-75be9aee962f.png">

Related to: https://github.com/splitrb/split/blob/33b76480791a777f7c0e88ca9850ad896404cc05/lib/split/experiment.rb#L448

To reproduce. 
1. Create and save Split::Experiment with some metadata
2. Initialise and save the same experiment but with `metadata: nil`
   -> During save experiment version will increment
3. Initialise the same experiment as in step 2 (with `metadata: nil`) and save experiment
   -> During save experiment version will increment

Expected:

In step 3 the version should not be incremented because experiment configuration has not changed after step 2.


