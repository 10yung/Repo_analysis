So for example, a user left a tab open on the Hangfire Recurring Jobs page when the sql server was rebooted overnight (this usually takes 20 minutes or so), the server obviously spews out errors during the time that SQL is offline however once SQL comes back online it's like hangfire has saturated all the available connections in the process? 

I end up with the error:

	System.InvalidOperationException: Timeout expired.  The timeout period elapsed prior to obtaining a connection from the pool.  This may have occurred because all pooled connections were in use and max pool size was reached.
	   at System.Data.ProviderBase.DbConnectionFactory.TryGetConnection(DbConnection owningConnection, TaskCompletionSource`1 retry, DbConnectionOptions userOptions, DbConnectionInternal oldConnection, DbConnectionInternal& connection)
	   at System.Data.ProviderBase.DbConnectionInternal.TryOpenConnectionInternal(DbConnection outerConnection, DbConnectionFactory connectionFactory, TaskCompletionSource`1 retry, DbConnectionOptions userOptions)
	   at System.Data.ProviderBase.DbConnectionClosed.TryOpenConnection(DbConnection outerConnection, DbConnectionFactory connectionFactory, TaskCompletionSource`1 retry, DbConnectionOptions userOptions)
	   at System.Data.SqlClient.SqlConnection.TryOpen(TaskCompletionSource`1 retry)
	   at System.Data.SqlClient.SqlConnection.Open()
	   at Hangfire.SqlServer.SqlServerStorage.CreateAndOpenConnection()
	   at Hangfire.SqlServer.SqlServerStorage.UseConnection[T](DbConnection dedicatedConnection, Func`2 func)
	   at Hangfire.SqlServer.SqlServerMonitoringApi.GetStatistics()
	   at System.Lazy`1.ViaFactory(LazyThreadSafetyMode mode)
	   at System.Lazy`1.ExecutionAndPublication(LazyHelper executionAndPublication, Boolean useDefaultConstructor)
	   at System.Lazy`1.CreateValue()
	   at Hangfire.Dashboard.DashboardMetrics.<>c.<.cctor>b__1_0(RazorPage page)
	   at Hangfire.Dashboard.JsonStats.Dispatch(DashboardContext context)
	   at Hangfire.Dashboard.AspNetCoreDashboardMiddleware.Invoke(HttpContext httpContext)
	   at Microsoft.AspNetCore.Builder.Extensions.MapMiddleware.Invoke(HttpContext context)
	   at Microsoft.AspNetCore.Builder.RouterMiddleware.Invoke(HttpContext httpContext)
	   at Microsoft.AspNetCore.Authentication.AuthenticationMiddleware.Invoke(HttpContext context)
	   at Microsoft.AspNetCore.StaticFiles.StaticFileMiddleware.Invoke(HttpContext context)
	   at Microsoft.AspNetCore.StaticFiles.StaticFileMiddleware.Invoke(HttpContext context)
	   at Microsoft.AspNetCore.Server.Kestrel.Core.Internal.Http.HttpProtocol.ProcessRequests[TContext](IHttpApplication`1 application)

And it is only fixed by stopping and restarting the dotnet process

I am using hangfire v 1.7.1, with the following SqlServerStorageOptions

	new SqlServerStorageOptions
	{
		CommandBatchMaxTimeout = TimeSpan.FromMinutes(5),
		SlidingInvisibilityTimeout = TimeSpan.FromMinutes(5),
		QueuePollInterval = TimeSpan.Zero,
		UseRecommendedIsolationLevel = true,
		UsePageLocksOnDequeue = true,
		DisableGlobalLocks = true,
	}

i was add job by 
RecurringJob.AddOrUpdate( () => TaskManger.run(), "0/40 * * * * *");

it should be 00->40->20->00.....  seconds

but it ran by 00->40->00->40.....

what can i do?
We have multiple instances of a windows service running that starts Hangfire with TopShelf. We’re using version 1.6.22 in production with Sql storage on Azure (though I’ve been able to reproduce this issue on the latest release 1.7.8 as well). We typically have 3-4 recurring jobs (a couple that run every 30 minutes, others that run once per day) and those recurring jobs spawn several hundred or thousand individual jobs when they run. 

In general, this works just fine. Periodically though (varies between weekly and every couple weeks) we’ll see a situation where jobs will be enqueued and some processing (up to the count of workers setup in the instance) but jobs actually stop running and new jobs submitted stack up in the enqueued state. A restart of the windows service will typically correct this and get things moving again, but the jobs are time-sensitive (dealing with closing financial transactions so money moves from one account to another) and if we do not catch it right away it can create delays in funding. Our production environments will have more than one server (each server having its own windows service, pointing at the same SQL Server) sharing the processing, and when this happens we will see it happen on each of the servers at the same time.

We’ve enabled trace logging trying to find any issues that might set this off. We haven’t logged anything absolutely conclusive, but we did have several occurrences where the log showed that a connection could not be obtained from the pool: 

```
Error occurred during execution of 'Worker #f973b5b4' process. Execution will be retried (attempt #1) in 00:00:01 seconds.
 System.InvalidOperationException: Timeout expired.  The timeout period elapsed prior to obtaining a connection from the pool.  This may have occurred because all pooled connections were in use and max pool size was reached.
   at System.Data.ProviderBase.DbConnectionFactory.TryGetConnection(DbConnection owningConnection, TaskCompletionSource`1 retry, DbConnectionOptions userOptions, DbConnectionInternal oldConnection, DbConnectionInternal& connection)
   at System.Data.ProviderBase.DbConnectionInternal.TryOpenConnectionInternal(DbConnection outerConnection, DbConnectionFactory connectionFactory, TaskCompletionSource`1 retry, DbConnectionOptions userOptions)
   at System.Data.ProviderBase.DbConnectionClosed.TryOpenConnection(DbConnection outerConnection, DbConnectionFactory connectionFactory, TaskCompletionSource`1 retry, DbConnectionOptions userOptions)
   at System.Data.SqlClient.SqlConnection.TryOpenInner(TaskCompletionSource`1 retry)
   at System.Data.SqlClient.SqlConnection.TryOpen(TaskCompletionSource`1 retry)
   at System.Data.SqlClient.SqlConnection.Open()
   at Hangfire.SqlServer.SqlServerStorage.CreateAndOpenConnection()
   at Hangfire.SqlServer.SqlServerJobQueue.DequeueUsingTransaction(String[] queues, CancellationToken cancellationToken)
   at Hangfire.SqlServer.SqlServerJobQueue.Dequeue(String[] queues, CancellationToken cancellationToken)
   at Hangfire.SqlServer.SqlServerConnection.FetchNextJob(String[] queues, CancellationToken cancellationToken)
   at Hangfire.Server.Worker.Execute(BackgroundProcessContext context)
   at Hangfire.Server.ServerProcessExtensions.Execute(IServerProcess process, BackgroundProcessContext context)
   at Hangfire.Server.AutomaticRetryProcess.Execute(BackgroundProcessContext context)
```

This was logged for the Hangfire.Server.Worker, Hangfire.Server.RecurringJobScheduler and Hangfire.Server.DelayedJobScheduler. It didn’t seem to be immediately after this that the issue above occurred, but close enough to be suspicious. 

To attempt to reproduce this locally I setup a vanilla console application with the latest 1.7.8 release pointing to a local sql database containing only the Hangfire tables. I created a simple class that would create a sql connection, open it, wait 2 seconds and close it and when the application starts up I queued up 1000 of those jobs. Once the application starts I give it just a moment then go into SSMS and take the local database offline, having it kill any active sessions. This starts generating errors in the Hangfire logs of course. I leave that offline for several minutes, then bring the database back online. The dashboard recovers at this point, and occasionally jobs resume processing at this point and everything completes OK. However, other times this will recreate what I described above. When I’ve recreated this locally I also received an error in the log:

Unable to signal the stopped event for BackgroundDispatcher: it was already disposed

Though this may be unrelated.

When I got it in this state locally I noticed if I try to do a select against the JobQueue table in sql the query doesn’t return any rows and just continues to run. Digging into that a little more I saw 20 transactions left open. Once I killed those sessions the JobQueue table was queryable again. 

I used stdump to get stack traces when the issue occurred and after I killed the sql processes that had the open transactions (traces attached)

[HangfireTest.log](https://github.com/HangfireIO/Hangfire/files/4047162/HangfireTest.log)
[stdump - after kill.txt](https://github.com/HangfireIO/Hangfire/files/4047163/stdump.-.after.kill.txt)
[stdump - pre kill 2.txt](https://github.com/HangfireIO/Hangfire/files/4047164/stdump.-.pre.kill.2.txt)
[stdump - pre kill 1.txt](https://github.com/HangfireIO/Hangfire/files/4047165/stdump.-.pre.kill.1.txt)
[HangfireTest.1.log](https://github.com/HangfireIO/Hangfire/files/4047166/HangfireTest.1.log)

Hi,

This PR adds support for "job descriptions", which are long-form blocks of HTML prose which offer more information on a particular job than the name field can allow. If present, the description field is displayed on the Job Details page of the dashboard.

The attribute itself is symmetrical to the JobDisplayNameAttribute. Likewise, I have added DashboardOptions.DescriptionFunc to override the description provider if desired.

Thanks
I have implemented  IDashboardAuthorizationFilter, it works fine locally.  

I am running the site in a virtual directory. 

When I push to the server (via Azure Devops), I get a 404 error when accessing the dashboard.

I added this to web.config per a Stack Overflow post:
'<add name="hangfireDashboard" path="hangfire" type="System.Web.DefaultHttpHandler" verb="*" />'

now I am getting:
HTTP Error 500.21 - Internal Server Error 
Handler "hangfireDashboard" has a bad module "ManagedPipelineHandler" in its module list

Server is Windows Server 2012 R2 IIS 8.5

Is it possible to use BackgroundJob.ContinueJobWith with the extension HttpJobs?

I have developed an api website where I have 3 methods, and these must be executed synchronously

Thank you very much for your help
regards
Not sure if this is simply a documentation concern but the [`LocalRequestsOnlyAuthorizationFilter`](https://github.com/HangfireIO/Hangfire/blob/master/src/Hangfire.Core/Dashboard/LocalRequestsOnlyAuthorizationFilter.cs) being applied automatically for the following use case may cause confusion for some folks.

```csharp
app.UseEndpoints(endpoints =>
{
    endpoints 
        .MapHangfireDashboard()
        .RequireAuthorization("myPolicy");
});
```

When this is what the user was probably wanting to do:

```csharp
app.UseEndpoints(endpoints =>
{
    var dashboardOptions = new DashboardOptions()
    {
        Authorization = new IDashboardAuthorizationFilter[] { }
    };

    endpoints 
        .MapHangfireDashboard(dashboardOptions)
        .RequireAuthorization("myPolicy");
});
```

## Potential resolution / question

Should calling `RequireAuthorization("myPolicy")` in this manner also unhook the default local only authorization?
I'm working on a Binance bot project and I decided to use Hangfire for each running bot thread. How do I cancel a specific BackgroundJob? RecurringJobs allow me to specify a job name while BackgroundJob doesn't.

```
private UpdateSubscription _subscription;
private readonly CancellationTokenSource _cts = new CancellationTokenSource();

public void Run(Bot bot)
{
    BackgroundJob.Enqueue(() => Start(bot, _cts.Token));
}

public void Start(Bot bot, CancellationToken token)
{
    // heavy logic
    _subscription = _socketClient.SubscribeToKlineUpdates(bot.CryptoPair.Symbol, bot.TimeInterval.Interval /*KlineInterval.OneHour*/, async data =>
    {
        ... logic ...

        if (token.IsCancellationRequested)
        {
            await _socketClient.Unsubscribe(_subscription);
        }
    }
}

// Stop specific bot?
public void Stop(string botName)
{
    //_cts.Cancel();
}
```
Hi,

we have a recurring job that runs every 10 minutes:
```
[AutomaticRetry(Attempts = 0, OnAttemptsExceeded = AttemptsExceededAction.Delete)]
[JobDisplayName("Update status from {1}")]
public async Task UpdateStatusAsync(int providerId, string providerName, PerformContext context)
{
}
```

If the server is disabled for 24 hours, when it starts up, Hangfire it trying to run this job almost 300 times right away.  I can't imagine this is the expected way its supposed to work, but if it is, how can we configure Hangfire to NOT do that, and just run the next scheduled recurring task since re-starting back up.

I know there's some hacky ways to fix this, but it really seems there should be a built in way to deal with it.

Is there some documentation I am missing?  If so, I apologize up front.
