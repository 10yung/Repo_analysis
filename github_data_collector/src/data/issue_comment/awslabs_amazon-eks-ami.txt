Is it possible to retrieve the cluster name on the EKS node/pod without having to pass this as an environment variable to the configuration?
*Issue

#99 

*Description of changes:*

See CIS Benchmark 4.2.6

By submitting this pull request, I confirm that you can use, modify, copy, and redistribute this contribution, under the terms of your choice.

<!-- If this is a security issue, please do not discuss on GitHub. Please report any suspected or confirmed security issues to AWS Security https://aws.amazon.com/security/vulnerability-reporting/ -->

*Issue

#99 

*Description of changes:*

See CIS Benchmark 4.2.9

By submitting this pull request, I confirm that you can use, modify, copy, and redistribute this contribution, under the terms of your choice.

<!-- If this is a security issue, please do not discuss on GitHub. Please report any suspected or confirmed security issues to AWS Security https://aws.amazon.com/security/vulnerability-reporting/ -->

*Issue

#99 

*Description of changes:*

See CIS Benchmark 4.2.4

By submitting this pull request, I confirm that you can use, modify, copy, and redistribute this contribution, under the terms of your choice.

<!-- If this is a security issue, please do not discuss on GitHub. Please report any suspected or confirmed security issues to AWS Security https://aws.amazon.com/security/vulnerability-reporting/ -->

*Description of changes:* Update Makefile to reflect the correct kubernetes_version and kubernetes_build_date values used to generate the official AWS AMIs.


By submitting this pull request, I confirm that you can use, modify, copy, and redistribute this contribution, under the terms of your choice.

<!-- If this is a security issue, please do not discuss on GitHub. Please report any suspected or confirmed security issues to AWS Security https://aws.amazon.com/security/vulnerability-reporting/ -->

We're in eu-central-1 using EKS 1.14 with t3.xlarge worker nodes. After updating to AMI v20191119 the allocatable memory went down from 16132820Ki to 13461148Ki. Also the allocatable CPU went down to 3920m which is less important. Wondering what's going on here.

<!-- Please use this template while reporting a bug and provide as much info as possible. Please also search for existing open and closed issues that may answer your question. Thanks!-->

**What happened**: upgraded EKS cluster from 1.13 to 1.14. Cloudformation stack consits of EKS cluster and associated ASG with UpdatePolicy set to AutoScalingRollingUpdate. I updated ami version in launch config and rolling update was initiated by the time EKS was updating kubernetes software version. 
https://github.com/awslabs/amazon-eks-ami/blob/master/files/bootstrap.sh#L236 fast fails due to set -o options after 20 min. I took more than 20min for my cluster to update.

**What you expected to happen**:
bootstrap waits until EKS is ready.

**How to reproduce it (as minimally and precisely as possible)**:
Upgrade EKS from 1.13 to 1.14. Fleet already running bootstrap.sh and waiting for the cluster to be available

**Anything else we need to know?**:

**Environment**:
- AWS Region: all
- Instance Type(s): all
- EKS Platform version (use `aws eks describe-cluster --name <name> --query cluster.platformVersion`): eks.6
- Kubernetes version (use `aws eks describe-cluster --name <name> --query cluster.version`): 1.14
- AMI Version: amazon-eks-node-1.14-v20191213 (ami-087a82f6b78a07557)
- Kernel (e.g. `uname -a`): all
- Release information (run `cat /etc/eks/release` on a node):
<!-- Put release info in the triple backticks below-->
```
```

<!-- If this is a security issue, please do not discuss on GitHub. Please report any suspected or confirmed security issues to AWS Security https://aws.amazon.com/security/vulnerability-reporting/ -->

*Issue #, if available:*

https://github.com/aws/containers-roadmap/issues/142

*Description of changes:*

IPVS is a relatively common feature request for EKS users. Their primary use case is generally to gain access to more advanced load balancing algorithms (like least-connections) than what is provided by IPTables, and for the performance benefit when running large numbers of Services and Pods (IPVS is better at handling large numbers of rule-lookups than IPTables).

We can see this item is on the GitHub Containers Roadmap with 30 thumbsups, but it hasn't yet been implemented: https://github.com/aws/containers-roadmap/issues/142

Currently we don't provide IPVS as an option in the EKS AMIs since upstream Kubernetes has not set this as the default (yet). However, Kubernetes has flagged IPVS has "generally available" as of 1.11, hence users are interested in testing it out.

There are some issues with IPVS which are being ironed out which explains why it hasn't yet become the default for upstream Kubernetes. However, since we have interest from users already, I think it's worthwhile to provide this as an option for users to at least test out for themselves. We can emphasize that the feature is "beta", but still make it easy enough for eager users to begin testing in anticipation for it becoming a future Kubernetes default.

This also benefits upstream Kubernetes since EKS users can funnel their feedback upstream if they encounter any issues with IPVS during testing.

I'm opening a Pull Request which implements this feature with the following implementation:

- Define an Environment Variable in the eks-worker-al2.json file where users can define the "netfilter_module". The options are "iptables" (which I've set as the default), and "ipvs".
- Modify install-worker.sh to install the ipvsadm package and relevant kernel modules only when the netfilter_module env var is set to "ipvs".
- We do not remove the IPTables setup section of install-worker.sh because IPTables is still necessary for the AWS CNI to function. The AWS CNI uses IPTables for creating an SNAT rule for Pod traffic leaving the VPC. Therefore, IPTables needs to be setup to ensure Pods can connect to endpoints outside the VPC.
- Modify the EKS Logs Collector script to gather IPVS rules by default. This is inside a "try" block, so even if IPVS isn't installed because the user selected IPTables, it won't affect the script. Additionally, I have not added the `ipvsadm` command to the REQUIRED_UTILS section, since we don't need to fail when this command isn't installed.

Testing:

- I've tested the default IPTables setup by just running `make` in the repo without selecting IPVS. I've launched a Node Group from the resulting AMI and confirmed that k8s Services are operating normally using IPTables, and `ipvsadm` is not installed, as expected. So the existing functionality is still working as expected.

- Then I've modified the netfilter_module env var to "ipvs" and run `make` again. I've modified the kube-proxy-config ConfigMap to switch the "mode" from "iptables" to "ipvs". I've launched a new Node Group from the resulting AMIs, and cordoned all existing Nodes to force my Pods to schedule to the new Node. After creating a Deployment and exposing it as a ClusterIP Service, running `ipvsadm -l -n` on the Node confirmed the expected rules were created successfully. Connecting to the Service endpoint worked correctly. I've recreated the above test with a NodePort Service successfully as well. Connecting from the Pod to the internet also works correctly, indicating the SNAT rules created by the CNI for traffic leaving the VPC is still working as expected.

- Running the new logs collector script correctly bundles the IPVS rules into the networking/ipvs_rules.txt file.

If you have any questions about the above, please let me know.

###

By submitting this pull request, I confirm that you can use, modify, copy, and redistribute this contribution, under the terms of your choice.

<!-- If this is a security issue, please do not discuss on GitHub. Please report any suspected or confirmed security issues to AWS Security https://aws.amazon.com/security/vulnerability-reporting/ -->

<!-- Please use this template while reporting a bug and provide as much info as possible. Please also search for existing open and closed issues that may answer your question. Thanks!-->

**What happened**:

We were using the CloudFormation template to create the stack, which contains several ASGs as work node in the EKS cluster. However when we tried to add a second stack using the same template, PODs in the second stack cannot resolve any domain, but can ping 8.8.8.8. 

Here's a minimal reproducible example. https://github.com/left4taco/AWS_EKS_MRE

It's very easy to follow. We've also add unique name to all the resources created by the template so that the second cluster has no conflicts with the first one. Could someone from AWS give it a look? I believe this might lead to discovery of a bug. 

**Environment**:
- AWS Region: us-west-2
- Instance Type(s): c5.18xlarge
- EKS Platform version (use `aws eks describe-cluster --name <name> --query cluster.platformVersion`): eks2 ~ eks5
- Kubernetes version (use `aws eks describe-cluster --name <name> --query cluster.version`): 1.14
- AMI Version: Most recent(20191126?)

<!-- Put release info in the triple backticks below-->
```
```

<!-- If this is a security issue, please do not discuss on GitHub. Please report any suspected or confirmed security issues to AWS Security https://aws.amazon.com/security/vulnerability-reporting/ -->

In order to support `--random-fully` we need at least iptables-1.6.2. The latest Amazon Linux 2 supports 1.8.2. We need to update to that version and configure legacy mode in order for things to work well with Kubernetes. 

**Environment**:
- AWS Region:
- Instance Type(s):
- EKS Platform version (use `aws eks describe-cluster --name <name> --query cluster.platformVersion`):
- Kubernetes version (use `aws eks describe-cluster --name <name> --query cluster.version`):
- AMI Version:
- Kernel (e.g. `uname -a`):
- Release information (run `cat /etc/eks/release` on a node):
<!-- Put release info in the triple backticks below-->
```
```

<!-- If this is a security issue, please do not discuss on GitHub. Please report any suspected or confirmed security issues to AWS Security https://aws.amazon.com/security/vulnerability-reporting/ -->
