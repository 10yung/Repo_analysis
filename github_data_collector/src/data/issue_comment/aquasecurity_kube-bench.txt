Currently, we kube-bench as part of a Jenkins Job. Since the kube-bench pod does not return an exit code, the job passes regardless of the results. For CICD, it would be helpful to have an option for an exit code similar to Trivy.
Bitwise AND between two value in order to compare file permissions
solved issue https://github.com/aquasecurity/kube-bench/issues/541.
According to the new format, changed all the yaml configuration files to comply 
Some checks in the kube-bench might currently not be feasible for a user and the user wants to ignore them. 
Currently the user can only specify all tests except for those via `--check`.
It would be good if kube-bench could provide a CLI argument, that allows to soft-fail on specified tests, so they still would run but would not alter the return code:

1.1.1.2 and 1.1.1.3 fail, a user could run:
```kube-bench --expect-fail "1.1.1.2,1.1.1.3"``` and still the return code would be 0 (if all other checks pass)
Fixes #541 

Additionally the format of all file permissions checks was streamlined. Some of the config files had the order of the keys different, now the order is the same for all configs. 
Adds Retry Logic Until Timeout.
Fixes #551
Integration tests:  Fixes #559
Adds `generateDiff` function to make it easier to see the difference between expected output and result.

Here is a sample output:
```console
--- FAIL: TestRunWithKind (160.86s)
    --- FAIL: TestRunWithKind/kube-bench (9.12s)
        integration_test.go:77: expected results
            
            Expected    (<)
            Result      (>)
            
            line: 25
            < [PASSa] 1.1.23 Ensure that the --service-account-lookup argument is set to true (Scored)
            > [PASS] 1.1.23 Ensure that the --service-account-lookup argument is set to true (Scored)
            line: 31
            < [bFAIL] 1.1.29 Ensure that the --client-ca-file argument is set as appropriate (Scored)
            > [FAIL] 1.1.29 Ensure that the --client-ca-file argument is set as appropriate (Scored)
            line: 516
            < 1 checks INFO2
            < 1 checks INFO3
            > [[NO MORE DATA]]
            
    --- PASS: TestRunWithKind/kube-bench-node (3.03s)
    --- PASS: TestRunWithKind/kube-bench-master (3.04s)
FAIL
FAIL    github.com/aquasecurity/kube-bench/integration  160.896s
FAIL
make: *** [integration-tests] Error 1
```
In the event of failure, the output of `TestRunWithKind()` is a giant string of the expected and actual output, both of which are hundreds of lines long. To figure out the difference, I have ended up copying these strings into files, replacing the `\n` with newlines, and doing a file diff. Would be nice if the test output showed this diff automatically. 
Hi Team,

Is it possible to have a biweekly release (bump the PATCH number of vMAJOR.MINOR.PATCH) for downstream projects to pick up the latest fixes quickly?


For running my own experiments I have added config paths for [Canonical's MicroK8s](https://github.com/ubuntu/microk8s). What do you think? Would it make sense to merge these to upstream?
Running the integrations tests I see output like this: 

```
using ticker and an timer...
thePod (kube-bench-t55bx) - status:"Running" 
using ticker and an timer...
thePod (kube-bench-t55bx) - status:"Running" 
using ticker and an timer...
thePod (kube-bench-t55bx) - status:"Failed" 
thePod (kube-bench-t55bx) - Failed - retrying...
Found (2) pods
pod (kube-bench-wfr6z) - "Pending"
using ticker and an timer...
thePod (kube-bench-wfr6z) - status:"Succeeded" 
=== RUN   TestRunWithKind/job-node
```
I think these pods are running the kube-bench job, but why does the first of them fail? And why doesn't this matter for the integration test to succeed? 