[aurioc] AURemoteIO.cpp:1086:Initialize: failed: -10851 (enable 2, outf< 2 ch,  44100 Hz, Float32, non-inter> inf< 2 ch,      0 Hz, Float32, non-inter>)
ca_require_noerr: [result = destInfo->Initialize (this), -10851] (goto gohome;)
Error: Failed to initialize graph (-10851)

```
- (void) configureAudioSession
{

    AVAudioSession* session = [AVAudioSession
                               sharedInstance];

    // Don't activate the audio session yet
    [session setActive:NO error:NULL];

    // Play music even in background and dont stop playing music
    // even another app starts playing sound
    [session setCategory:AVAudioSessionCategoryRecord error:NULL];
    
    double sampleRate = 16000.0;
    [session setPreferredSampleRate:sampleRate error:NULL];

    // Active the audio session
    [session setActive:YES error:NULL];
}

- (void) initMic {
    [self configureAudioSession];
    AudioStreamBasicDescription audioStreamBasicDescription = [EZAudioUtilities monoFloatFormatWithSampleRate:16000];
    audioStreamBasicDescription.mFormatID = kAudioFormatLinearPCM;
    audioStreamBasicDescription.mSampleRate = 16000.0;
    audioStreamBasicDescription.mFramesPerPacket = 1;
    audioStreamBasicDescription.mBytesPerPacket = 2;
    audioStreamBasicDescription.mBytesPerFrame = 2;
    audioStreamBasicDescription.mChannelsPerFrame = 1;
    audioStreamBasicDescription.mBitsPerChannel = 16;
    audioStreamBasicDescription.mFormatFlags = kAudioFormatFlagIsSignedInteger | kAudioFormatFlagsNativeEndian | kAudioFormatFlagIsPacked;
    audioStreamBasicDescription.mReserved = 0;

    NSArray *inputs = [EZAudioDevice inputDevices];
    [self.microphone setDevice:[inputs lastObject]];
    self.microphone = [EZMicrophone microphoneWithDelegate:self withAudioStreamBasicDescription:audioStreamBasicDescription];
    [self.microphone startFetchingAudio];
}

-(void) microphone:(EZMicrophone *)microphone
  hasAudioReceived:(float **)buffer
    withBufferSize:(UInt32)bufferSize
withNumberOfChannels:(UInt32)numberOfChannels {
    dispatch_async(dispatch_get_main_queue(),^{
        int result = _snowboyDetect->RunDetection(buffer[0], bufferSize);
        if (result >= 1) {
             NSLog(@"Hotword Detected");
            NSString *modelName = modelsList[result - 1];
            NSDictionary *jsonObj = @{@"message":@"Hotword Detected",@"modelname": modelName};
            [self triggerJSEventData:jsonObj];
           // [self triggerJSEvent:@"Hotword Detected"];
            detection_countdown = 30;
        } else {
            if (detection_countdown == 0){
                NSLog(@"No Hotword Detected");
                [self triggerJSEvent:@"No Hotword Detected"];
            } else {
                detection_countdown--;
            }
        }
    });
}
```

I use EZAudio which is working with Snowboy Hotword Detection.
On Android the Hotword ist recognizing perfectly, even if I speak quiet. With ios the Hotword is not recognized if I speak very quietly. Now my question: Is it possible to increase the microphone level on IOS? I have a feeling it's related to that. Is there any way to increase the mic input level?
Appears after playback
Playing music with EZAudioPlayer, why the music playback stops after the screen is locked.
URGENT!

When I run the PlayFile demo, I get stutters and the program runs very slowly. What's more, the cpu and memory of the virtual machine are overload.

Does anyone know how to deal with it?

urgently!!! I found it is not suppport bluetooth headphone, it had memory problem which will cause program melt
- (void)microphone:(EZMicrophone *)microphone
     hasBufferList:(AudioBufferList *)bufferList
    withBufferSize:(UInt32)bufferSize
withNumberOfChannels:(UInt32)numberOfChannels
{
    //
    // Getting audio data as a buffer list that can be directly fed into the
    // EZRecorder or EZOutput. Say whattt...
    //
    AudioBuffer audioBuffer = bufferList->mBuffers[0];
    
    float* frame = (float*)audioBuffer.mData;
    
    [self apppendAdpcmData:[NSData dataWithBytes:frame length:audioBuffer.mDataByteSize] toFile:@"luck.pcm"];
}
luck.pcm is noise. 
Hello guys,

Is there anyone who converted this library into React Native? 

I would love to use this on React Native...
https://facebook.github.io/react-native/
I've generated a few pure sine tones with Audacity at different frequencies to test with. The issue I'm seeing is that the code is returning the same frequency for two different sine tones that are relatively close in value.

For example: A sine tone generated at 19255Hz will show up from FFT as 19293.750000Hz. So will a sine tone generated at 19330Hz. Something must be off in the calculations.

Any assistance in how I can modify the above code to get a more precise FFT frequency reading for pure sine tones is greatly appreciated. Thank you!

//
// Initialize FFT
//
float maximumBufferSizeBytes = self.maximumBufferSize * sizeof(float);
self.info = (EZAudioFFTInfo *)calloc(1, sizeof(EZAudioFFTInfo));
vDSP_Length log2n = log2f(self.maximumBufferSize);
self.info->fftSetup = vDSP_create_fftsetup(log2n, FFT_RADIX2);
long nOver2 = maximumBufferSizeBytes / 2;
size_t maximumSizePerComponentBytes = nOver2 * sizeof(float);
self.info->complexA.realp = (float *)malloc(maximumSizePerComponentBytes);
self.info->complexA.imagp = (float *)malloc(maximumSizePerComponentBytes);
self.info->outFFTData = (float *)malloc(maximumSizePerComponentBytes);
memset(self.info->outFFTData, 0, maximumSizePerComponentBytes);
self.info->inversedFFTData = (float *)malloc(maximumSizePerComponentBytes);

//
// Calculate real + imaginary components and normalize
//
vDSP_Length log2n = log2f(bufferSize);
long nOver2 = bufferSize / 2;
float mFFTNormFactor = 10.0 / (2 * bufferSize);
vDSP_ctoz((COMPLEX*)buffer, 2, &(self.info->complexA), 1, nOver2);
vDSP_fft_zrip(self.info->fftSetup, &(self.info->complexA), 1, log2n, FFT_FORWARD);
vDSP_vsmul(self.info->complexA.realp, 1, &mFFTNormFactor, self.info->complexA.realp, 1, nOver2);
vDSP_vsmul(self.info->complexA.imagp, 1, &mFFTNormFactor, self.info->complexA.imagp, 1, nOver2);
vDSP_zvmags(&(self.info->complexA), 1, self.info->outFFTData, 1, nOver2);
vDSP_fft_zrip(self.info->fftSetup, &(self.info->complexA), 1, log2n, FFT_INVERSE);
vDSP_ztoc(&(self.info->complexA), 1, (COMPLEX *) self.info->inversedFFTData , 2, nOver2);
self.info->outFFTDataLength = nOver2;

//
// Calculate max freq
//
if (self.sampleRate > 0.0f)
{
    vDSP_maxvi(self.info->outFFTData, 1, &self.info->maxFrequencyMangitude, &self.info->maxFrequencyIndex, nOver2);
   self.info->maxFrequency = [self frequencyAtIndex:self.info->maxFrequencyIndex];

    float nyquistMaxFreq = self.sampleRate / 2.0;
    NSLog(@"FREQ: %f", (((float)self.info->maxFrequencyIndex / (float)self.info->outFFTDataLength) * nyquistMaxFreq));

}
EZAudio code here: https://github.com/syedhali/EZAudio/blob/master/EZAudio/EZAudioFFT.m
Hello guys,

Would you please support Android OS in the future? 
I would love to use this in React Native, Ionic, NativeScript! 
Please publish an android compatible version.

Thanks,