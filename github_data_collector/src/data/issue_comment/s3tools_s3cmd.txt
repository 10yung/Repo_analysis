The most recent commit (b790be56ae6e8f79fa7c7c01bc237edea7e1029d) to run-tests.py changed the  name of the third test bucket from {prefix}-Autotest-3 to all lowercase to match the updated AWS naming rules.  This was a necessary change, but oddly it seems that the "Buckets list" test still checks for the capitalized version, failing as a result when it doesn't find it.

Is it really possible that run-tests.py has been broken for over a year, or am I missing something basic?   Wouldn't this have been caught by running run-tests.py when b790be56ae6e8f79fa7c7c01bc237edea7e1029d was accepted?
Version: 2.0.2 on Mac OS X 10.15.2 (Catalina)

How to Reproduce:
1. Configure a connection to DigitalOcean Spaces using `s3cmd --configure`
This is based on the documentation for setting up s3cmd provided by DO Spaces.
https://www.digitalocean.com/docs/spaces/resources/s3cmd/

```
  Access Key: REDACTED
  Secret Key: REDACTED
  Default Region:
  S3 Endpoint: sgp1.digitaloceanspaces.com
  DNS-style bucket+hostname:port template for accessing a bucket: %(bucket)s.sgp1.digitaloceanspaces.com:443
  Encryption password:
  Path to GPG program: /usr/local/bin/gpg
  Use HTTPS protocol: True
  HTTP Proxy server name:
  HTTP Proxy server port: 0
```

2. The configuration succeeds, and it's possible to list all buckets in the Spaces region using the `s3cmd ls` command.

3. However when trying to create a new bucket the following error occurs - even though this bucket name is available when checking from the DO web dashboard.

```
s3cmd mb s3://s3cmd-bucket-test
ERROR: S3 error: 403 (SignatureDoesNotMatch)
```

4. This seems to be a problem with s3cmd because creating a new bucket using the same Spaces credential and endpoint, using a different S3 compatible client, such as CyberDuck, does work.
So I had a variable in the URL which failed to get set in a backup script... So I accidentally uploaded data to a bucket that isn't mine! I can't tell exactly what happened. It spent time uploading, but then it said failed. So where did the file go?

```
upload: '/tmp/tmpfile-YtibQ0l6px5UkjuIVW2e' -> 's3://database/database-local-2020-01-01T1106.tar.gz'  [1 of 1]
 613131 of 613131   100% in    0s  1779.04 kB/s  done
upload: '/tmp/tmpfile-YtibQ0l6px5UkjuIVW2e' -> 's3://database/database-local-2020-01-01T1106.tar.gz'  [1 of 1]
 613131 of 613131   100% in    0s  1877.15 kB/s  done
ERROR: S3 error: 404 (NoSuchBucket): The specified bucket does not exist
ERROR: S3 error: 403 (AccessDenied): Access Denied
```

Thanks!
On my installation, YAML files (whether `.yaml` or `.yml`) get uploaded with a MIME type of `text/html` when using `--no-mime-magic` (as required because not disabling python-magic will reliably break my site due to its errors in uploading JS and CSS with the wrong MIME type for 7+ years, requiring the `--no-mime-magic` workaround):

```
$ lsb_release -a
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 18.04.3 LTS
Release:	18.04
Codename:	bionic
$ s3cmd --version
s3cmd version 2.0.1
$ s3cmd put --acl-public --guess-mime-type --no-mime-magic  -v -v --add-header="Access-Control-Allow-Origin: *" wiki/static/metadata/custom.yaml s3://www.gwern.net/
INFO: No cache file found, creating it.
INFO: Compiling list of local files...
INFO: Running stat() and reading/calculating MD5 values on 1 files, this may take some time...
INFO: Summary: 1 local files to upload
upload: 'wiki/static/metadata/custom.yaml' -> 's3://www.gwern.net/custom.yaml'  [1 of 1]
 948356 of 948356   100% in    2s   350.22 kB/s  done
Public URL of the object is: http://www.gwern.net.s3.amazonaws.com/custom.yaml
06:55 PM $ curl -I 'http://www.gwern.net.s3.amazonaws.com/custom.yaml'
HTTP/1.1 200 OK
x-amz-id-2: TpknEq9FEt2wMuZsGwq/+N6YLlMvUrh5v53NiQHpgqGY9602vWgYogfhbEYNYuskrPfyCTkEoF0=
x-amz-request-id: 0531B6F45A30440B
Date: Sun, 17 Nov 2019 23:56:00 GMT
Last-Modified: Sun, 17 Nov 2019 23:55:48 GMT
ETag: "3e461b8e641cc36e8834ec23cbddf5bb"
x-amz-meta-s3cmd-attrs: atime:1573927548/ctime:1573927272/gid:1000/gname:gwern/md5:3e461b8e641cc36e8834ec23cbddf5bb/mode:33152/mtime:1573927272/uid:1000/uname:gwern
Accept-Ranges: bytes
Content-Type: text/html
Content-Length: 948356
Server: AmazonS3
$ s3cmd put --acl-public --guess-mime-type --no-mime-magic  -v -v --add-header="Access-Control-Allow-Origin: *" custom.yml s3://www.gwern.net/
INFO: No cache file found, creating it.
INFO: Compiling list of local files...
INFO: Running stat() and reading/calculating MD5 values on 1 files, this may take some time...
INFO: Summary: 1 local files to upload
upload: 'custom.yml' -> 's3://www.gwern.net/custom.yml'  [1 of 1]
 948356 of 948356   100% in    2s   375.11 kB/s  done
Public URL of the object is: http://www.gwern.net.s3.amazonaws.com/custom.yml
06:59 PM $ curl -I 'http://www.gwern.net.s3.amazonaws.com/custom.yml'
HTTP/1.1 200 OK
x-amz-id-2: r6WqkhmDeaE1bWabkYBjLTmpuMjOvVdt0VSaym++p3s9n9gvrkNPCWxsikRTEqLuD3kDTi6H2aw=
x-amz-request-id: 67E6EC4F8AE3364B
Date: Sun, 17 Nov 2019 23:59:14 GMT
Last-Modified: Sun, 17 Nov 2019 23:59:08 GMT
ETag: "3e461b8e641cc36e8834ec23cbddf5bb"
x-amz-meta-s3cmd-attrs: atime:1574035135/ctime:1574035135/gid:1000/gname:gwern/md5:3e461b8e641cc36e8834ec23cbddf5bb/mode:33152/mtime:1574035135/uid:1000/uname:gwern
Accept-Ranges: bytes
Content-Type: text/html
```

Since this is a YAML file using either of the most common and official file extensions for YAML, and there is nothing strange or nonstandard about it, I would expect it to be uploaded with a more correct MIME type. Whatever a YAML file's true MIME type may be, `text/plain` or `application/yaml` or `application/x-yaml` or `text/yaml`, it's definitely not `text/html`, and that causes web browsers to try and fail to render them as HTML, which is annoying. It would be better to be uploaded with a non-HTML mimetype so it gets shown more literally.

--------

Incidentally, this does work if you do not disable MIME magic, but given its unreliability, that's useless:

```
$ s3cmd put --acl-public --guess-mime-type  -v -v --add-header="Access-Control-Allow-Origin: *" wiki/static/metadata/custom.yaml s3://www.gwern.net/static/
INFO: No cache file found, creating it.
INFO: Compiling list of local files...
INFO: Running stat() and reading/calculating MD5 values on 1 files, this may take some time...
INFO: Summary: 1 local files to upload
upload: 'wiki/static/metadata/custom.yaml' -> 's3://www.gwern.net/static/custom.yaml'  [1 of 1]
 948356 of 948356   100% in    3s   268.44 kB/s  done
Public URL of the object is: http://www.gwern.net.s3.amazonaws.com/static/custom.yaml
$ curl -I 'http://www.gwern.net.s3.amazonaws.com/static/custom.yaml'
HTTP/1.1 200 OK
x-amz-id-2: bXlz5NXwyRQP3fk4Z9MB89XtVJXhQZNU2Qt39W7ibX0rFAR3kZVuvsTxNsrI/0HBxg/WU0ng6+Y=
x-amz-request-id: 4474624569376763
Date: Sun, 17 Nov 2019 23:56:30 GMT
Last-Modified: Sun, 17 Nov 2019 23:56:22 GMT
ETag: "3e461b8e641cc36e8834ec23cbddf5bb"
x-amz-meta-s3cmd-attrs: atime:1574034943/ctime:1573927272/gid:1000/gname:gwern/md5:3e461b8e641cc36e8834ec23cbddf5bb/mode:33152/mtime:1573927272/uid:1000/uname:gwern
Accept-Ranges: bytes
Content-Type: text/plain
Content-Length: 948356
Server: AmazonS3
```
version: v2.0.2

when I run the followed cmd(host with 80 port), error has ocuured.
```bash
s3cmd --debug --no-ssl --access_key=AKIAIOSFODNN7EXAMPLE --secret_key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY --host=10.10.40.90:80 --host-bucket= ls s3://cluster-001 
```

[s3.txt](https://github.com/s3tools/s3cmd/files/3844996/s3.txt)

```bash
s3cmd --debug --no-ssl --access_key=AKIAIOSFODNN7EXAMPLE --secret_key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY --host=10.10.40.90 --host-bucket= ls s3://cluster-001 
```
but when I remove 80 port from host, everything is fine.
[success.txt](https://github.com/s3tools/s3cmd/files/3844998/success.txt)



Regardless to specifying -p ,
s3cmd sync DOES NOT preserve original timestamp when dealing with path changes and remote copy
while new uploads DO preserve original timestamp from local file.
How to reproduce:
- run `alpine:3.10.3` docker image
- execute following commands:
```bash
echo '@edge http://nl.alpinelinux.org/alpine/edge/main' >> /etc/apk/repositories
echo '@testing http://nl.alpinelinux.org/alpine/edge/testing' >> /etc/apk/repositories
apk update && apk upgrade
apk add s3cmd@testing
s3cmd --help
```
The output is something like this:
```
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    An unexpected error has occurred.
  Please try reproducing the error using
  the latest s3cmd code from the git master
  branch found at:
    https://github.com/s3tools/s3cmd
  and have a look at the known issues list:
    https://github.com/s3tools/s3cmd/wiki/Common-known-issues-and-their-solutions
  If the error persists, please report the
  following lines (removing any private
  info as necessary) to:
   s3tools-bugs@lists.sourceforge.net

Error loading some components of s3cmd (Import Error)
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

Invoked as: /usr/bin/s3cmd --help
Problem: <class 'ModuleNotFoundError: No module named 'S3'
S3cmd:   unknown version. Module import problem?
python:   3.7.5 (default, Oct 17 2019, 12:25:15) 
[GCC 8.3.0]
environment LANG=None

Traceback (most recent call last):
  File "/usr/bin/s3cmd", line 3070, in <module>
    from S3.ExitCodes import *
ModuleNotFoundError: No module named 'S3'

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    An unexpected error has occurred.
  Please try reproducing the error using
  the latest s3cmd code from the git master
  branch found at:
    https://github.com/s3tools/s3cmd
  and have a look at the known issues list:
    https://github.com/s3tools/s3cmd/wiki/Common-known-issues-and-their-solutions
  If the error persists, please report the
  above lines (removing any private
  info as necessary) to:
   s3tools-bugs@lists.sourceforge.net
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
```

How to fix it???
Hey,

I am testing s3cmd (sync) for backup purposes, with requirement of aws:kms server-side-encryption, but the problem is, files smaller then 15 MiB (default threshold for multipart?) are always uploaded because of md5 hash mismatch:
```
$ s3cmd -d sync --delete-removed  ./<bucketname>/ s3://<bucketname>
[...]
DEBUG: XFER: smallfile (md5 mismatch: src=0640c3aa49049df23805872e7957ed85 dst=92f78c6e45a38e12484ae5fdeceb8adc)
[...]
```

Files uploaded in multipart mode do not have this problem, they are not re-uploaded on every run.
```
$ ls -lash <bucketname>/
total 46M
   0 drwxrwxr-x.  2 user user   89 Nov  4 11:03 .
4.0K drwx------. 28 user user 4.0K Nov  4 10:19 ..
 14M -rw-rw-r--.  1 user user  14M Nov  4 10:11 file14
 16M -rw-rw-r--.  1 user user  16M Nov  4 10:01 file3
4.0M -rw-rw-r--.  1 user user 4.0M Nov  4 10:58 file4
5.0M -rw-rw-r--.  1 user user 5.0M Nov  4 10:59 file5
6.0M -rw-rw-r--.  1 user user 6.0M Nov  4 10:59 file6
4.0K -rw-rw-r--.  1 user user   10 Nov  4 10:07 smallfile
$ s3cmd sync --delete-removed  ./<bucketname>/ s3://<bucketname>
upload: './<bucketname>/file14' -> 's3://<bucketname>/file14'  [1 of 6]
 14680064 of 14680064   100% in    3s     4.33 MB/s  done
upload: './<bucketname>/file3' -> 's3://<bucketname>/file3'  [part 1 of 2, 15MB] [2 of 6]
 15728640 of 15728640   100% in    5s     2.79 MB/s  done
upload: './<bucketname>/file3' -> 's3://<bucketname>/file3'  [part 2 of 2, 1024kB] [2 of 6]
 1048576 of 1048576   100% in    0s  1730.64 kB/s  done
upload: './<bucketname>/file4' -> 's3://<bucketname>/file4'  [3 of 6]
 4194304 of 4194304   100% in    1s     2.14 MB/s  done
upload: './<bucketname>/file5' -> 's3://<bucketname>/file5'  [4 of 6]
 5242880 of 5242880   100% in    2s     2.17 MB/s  done
upload: './<bucketname>/file6' -> 's3://<bucketname>/file6'  [5 of 6]
 6291456 of 6291456   100% in    2s     2.21 MB/s  done
upload: './<bucketname>/smallfile' -> 's3://<bucketname>/smallfile'  [6 of 6]
 10 of 10   100% in    0s    59.34 B/s  done
Done. Uploaded 47185930 bytes in 16.5 seconds, 2.73 MB/s.
$ s3cmd sync --delete-removed  ./<bucketname>/ s3://<bucketname>
upload: './<bucketname>/file14' -> 's3://<bucketname>/file14'  [1 of 5]
 14680064 of 14680064   100% in    2s     4.91 MB/s  done
upload: './<bucketname>/file4' -> 's3://<bucketname>/file4'  [2 of 5]
 4194304 of 4194304   100% in    1s     3.50 MB/s  done
upload: './<bucketname>/file5' -> 's3://<bucketname>/file5'  [3 of 5]
 5242880 of 5242880   100% in    1s     3.17 MB/s  done
upload: './<bucketname>/file6' -> 's3://<bucketname>/file6'  [4 of 5]
 6291456 of 6291456   100% in    1s     3.62 MB/s  done
upload: './<bucketname>/smallfile' -> 's3://<bucketname>/smallfile'  [5 of 5]
 10 of 10   100% in    0s   123.98 B/s  done
Done. Uploaded 30408714 bytes in 7.3 seconds, 3.96 MB/s.
$ s3cmd sync --delete-removed  ./<bucketname>/ s3://<bucketname>
upload: './<bucketname>/file14' -> 's3://<bucketname>/file14'  [1 of 5]
 14680064 of 14680064   100% in    3s     4.31 MB/s  done
upload: './<bucketname>/file4' -> 's3://<bucketname>/file4'  [2 of 5]
 4194304 of 4194304   100% in    1s     3.06 MB/s  done
upload: './<bucketname>/file5' -> 's3://<bucketname>/file5'  [3 of 5]
 5242880 of 5242880   100% in    1s     3.22 MB/s  done
upload: './<bucketname>/file6' -> 's3://<bucketname>/file6'  [4 of 5]
 6291456 of 6291456   100% in    2s     2.95 MB/s  done
upload: './<bucketname>/smallfile' -> 's3://<bucketname>/smallfile'  [5 of 5]
 10 of 10   100% in    0s   126.71 B/s  done
Done. Uploaded 30408714 bytes in 8.2 seconds, 3.52 MB/s.
```

Using '--no-check-md5' workarounds the problem:


's3cmd sync' behaviour is superior to 'aws s3 sync' as it considers also the md5 hash of the source file. 'aws s3 sync' uses only mtime and size attributes to determine what to sync, so it will silently ignore bit-rotted or silently damaged files. Yet, with current behavior, using s3cmd, <15 MiB files will be uploaded every time the command is run, so it will cause extra API calls for upload, extra upload bandwidth and cause extra versions which use extra space one has to pay.
This might be related to: https://github.com/s3tools/s3cmd/issues/463

Version used is the latest packaged in Fedora 31:
```
$ s3cmd --version
s3cmd version 2.0.2
```
I can not use s3cmd to put a file with “--add-header=X-Amz-meta-asd_ad:asdad”, just because of underline.
```
s3cmd put --add-header="X-Amz-meta-asd_ad:asdad" test.go s3://zztest
ERROR: Parameter problem: Invalid character(s) in header name 'X-Amz-meta-asd_ad': "_"
```
Is there any solution?
I cannot configure or use this tool. I get the above error message.

My keys work fine with CrossFTP.