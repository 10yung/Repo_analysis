Is it in general possible to load a list of urls, from which Rcrawler collects urls?
Sometimes when using LinkExtractor function doesnot collect external URLS from the footer of a webpage while the HTML is visible. Bug or wrong use?

For instance:
urls<-LinkExtractor("https://www.partou.nl", ExternalLInks = TRUE, Useragent = "Chrome/41.0.2228.0"))

or 

urls<-LinkExtractor("https://www.kinderopvangoosterhout.nl", ExternalLInks = TRUE, Useragent = "Chrome/41.0.2228.0"))

ps. awesome package, makes scraping and parsing so much easier :)
Hi,

first of all thank you for developing and maintaining this great package!

I ran into a problem yesterday, when crawling a large website. The site contains round about 20k pages. Before analyzing the crawled data I did a few crosschecks manually by comparing the content of an url with e.g. Id = 100 in the browser and the stored html-file with the name 100.html. Doing that i realized that the content doesn't match. Somehow the Ids of the html-files are shuffled and  when i open the file get the content of some other page of the site.

I would really appreciate any help with this!

Best,
cspersonal


hi and good day

as i need to extract comments from a page, as long texts are hided and need to push "read more" link at first, it done by javascript code.

so i need to run a scripts in each page before extracting data.

kindly advice how its possible...

warm regards
I would like to extract e-mails from several domains. Lets say I have only two:

`domains <- c("http://www.aldautomotive.hr", "http://www.bks-leasing.hr")`

Now, I want to extract only e-mail's from this domains. I know I can save htmls and than extract emails, but is it possible to extract it in one step? If this is e-mail regex:

`emailRegex <- "^[a-zA-Z0-9.!#$%&'*+/=?^_`{|}~-]+@[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*$"`

How can I extract only that e-mails, but anything else? It seems KeyWords argument don't accept regex? 



Hi, I tried the following codes, which only returns 30 links.
`Rcrawler(Website = "https://japantoday.com/search?keyword=election", no_cores = 2, no_conn = 2, ExtractXpathPat = ".//h3[@class='media-heading text-xstrong']/a/@href", ManyPerPattern = TRUE, MaxDepth = 1)`

I even changed 'no_cores = 1, no_conn = 1.', the results are the same.
If I open https://japantoday.com/search?keyword=election and click "next", I can find more than 100 results.

Could you please help me solve this problem?
Thanks!!
Hi Salim, 

thank you for this wonderful library! It's really powerful.

I am trying to understand how if there is a way to skip websites that are too big to be crawled.
In the following example I am trying to crawl a list of websites and extract the pages with the email address but I obviously don't want the last website (The Sydney Morning Herald newspaper) to be scraped. 
Is there a way to avoid this or to make the function skip if it's taking too long or the number of pages are above a given threshold?

thank you, 
Ahmed 

```
library(Rcrawler)
sample_web<-c("www.parentskills.com.au",
  "www.huggies.com.au",
  "www.babies2infinity.com.au",
  "www.smh.com.au")  

results<-lapply(sample_web, function(x){
  Rcrawler(Website = x,MaxDepth =1, saveOnDisk=FALSE,
           KeywordsFilter=c("mail","contact","about"),
           KeywordsAccuracy = 70)

    INDEX$Url<-INDEX$Url%>%
    str_extract(.,"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+")
  myINDEX<-na.omit(INDEX$Url)
})

```
Not always I use the crawler, actually more often a content scraper following a predefined link path, so I can be sure the complete site is scraped.
The content scraper includes a sys.sleep(1), which slows him down. Removed it myself to make it faster. Also, the multithreaded scraping as in rcrawler should be available with the contentscraper, as well, I think. 
Hi, 
a nice feature would be an integration with ip shuffeling services. There are numerous, i was thinking about something like https://proxycrawl.com/ 
It is not hard to implement it yourself, but I feel this would be a nice feature for a crawler, i.e. think of using crawlera with scrapy. 
When utilizing the network analysis functionality, only the internal HTML pages identified in the Index file are stored as copies.  This should store a copy of all HTML pages crawled, including those in NetwIndex, correct?

Rcrawler(Website = "https://github.com/salimk/Rcrawler/issues/new", MaxDepth = 2, no_cores = 4, no_conn = 4 , NetworkData = TRUE, NetwExtLinks =TRUE,  statslinks = TRUE)