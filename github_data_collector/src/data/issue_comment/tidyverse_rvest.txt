AFter the IMDb update there was a fix for the readme #225  but not the `html_text()` documentation. Here I have changed it to pull the cast from Wikipedia's listing as it provides an example similar to the previous one before the update.
I am looking at this great answer: https://stackoverflow.com/a/58211397/3502164.

The beginning of the solution includes:

```
library(httr)
library(xml2)

gr <- GET("https://nzffdms.niwa.co.nz/search")
doc <- read_html(content(gr, "text"))

xml_attr(xml_find_all(doc, ".//input[@name='search[_csrf_token]']"), "value")

```

Output is constant across multiple requests:

"59243d3a233492e9461f8f73136118f9"
My Default way so far would have been:

```
doc <- read_html("https://nzffdms.niwa.co.nz/search")
xml_attr(xml_find_all(doc, ".//input[@name='search[_csrf_token]']"), "value")

```

That results differs to the Output above and changes across multiple requests.

Question:

What is the difference in between:

```
read_html(url)
read_html(content(GET(url), "text"))

```

Why does it result in different values and why does only the "GET" solution Returns the csv in the linked question?


What i tried:

Going down the Rabbit hole of function calls:

```
read_html
(ms <- methods("read_html"))
getAnywhere(ms[1])
xml2:::read_html
xml2:::read_html.default
#xml2:::read_html.response

read_xml
(ms <- methods("read_xml"))
getAnywhere(ms[1])

```But that resulted in this Question: Find the correct method

Thoughts:

I dont see that the get request takes any headers or Cookies, that could explain different Responses.

From my understanding both read_html and read_html(content(GET(.), "text")) will return XML/html.


I want to get this table:
url <- https://www.misprofesores.com/escuelas/Tec-de-Monterrey-ITESM-MTY_1004

But when I use read_table only the thead are showing up. Maybe RJSONIO could be the answer, any advices?

tab <- read_html(url) %>%
  html_nodes("table") %>%
  html_table()
Another case where `html_table` fails while `XML::readHTMLTable` works.

Also, check https://github.com/tidyverse/rvest/issues/7

``` r
require(rvest)
#> Loading required package: rvest
#> Loading required package: xml2

wp <- "https://www.moneycontrol.com/financials/bajajfinance/balance-sheetVI/BAF#BAF" %>% read_html()

wp %>% html_nodes("#mc_mainWrapper .table4") %>%
  as.character() %>%  XML::readHTMLTable() %>% .[[3]] %>% head()
#>                             Mar 19  Mar 18  Mar 17  Mar 16  Mar 15
#> 1                             <NA>    <NA>    <NA>    <NA>    <NA>
#> 2                          12 mths 12 mths 12 mths 12 mths 12 mths
#> 3                             <NA>    <NA>    <NA>    <NA>    <NA>
#> 4 EQUITIES AND LIABILITIES                                    <NA>
#> 5      SHAREHOLDER'S FUNDS                                    <NA>
#> 6     Equity Share Capital  115.37  115.03  109.37   53.55   50.00

wp %>% html_nodes("#mc_mainWrapper .table4") %>%
  html_table()
#> Warning in lapply(ncols, as.integer): NAs introduced by coercion
#> Error in if (length(p) > 1 & maxp * n != sum(unlist(nrows)) & maxp * n != : missing value where TRUE/FALSE needed

# below is not related to rvest but kept in case unpivotr may be benefited
wp %>% html_nodes("#mc_mainWrapper .table4") %>%
  .[[3]] %>% unpivotr::as_cells()
#> Warning in ifelse(is.na(colspan), 1, as.integer(colspan)): NAs introduced
#> by coercion
#> Error in if (any(grow)) {: missing value where TRUE/FALSE needed
```

<sup>Created on 2019-09-23 by the [reprex package](https://reprex.tidyverse.org) (v0.3.0)</sup>


I need to webscrape a google analytics table from within a webpage that is behind a user verified entry page.
Any ideas how to go about this using rvest?
This PR adds new vignette on web scraping (closing #244) and corrects pictures and text of the `selectorgadget` article (updating #246).

Please, remember to re-render/update `packagedown` site to include these vignettes. 
This would greatly aid the construction of links as `xml2::url_absolute()` works well with relative, absolute, and external paths.  It may also not be apparent to many rvest users that such a function exists.

If exporting is not an option, I would at least like to import it for an update to the ?html_text help page.

```r
# Possible update of ?html_text
url <- "http://www.imdb.com/title/tt1490017/"
movie <- read_html(url)

cast <- html_nodes(movie, "#titleCast span.itemprop")
html_text(cast)
html_name(cast)
html_attrs(cast)
html_attr(cast, "class")

#  Constructing links from href attributes with xml2::url_abosolute()
all_links <- movie %>%
    html_nodes(xpath = "//a") %>%
    html_attr("href")


full_links <- xml2::url_absolute(all_links, url)

# Preserves external links as well
grep("imdb\\.com", full_links, invert = TRUE, value = TRUE)
```

The following example works when the url was `http://`. When the website forces `https://`, `html_session` no longer works with it.

```r
library(rvest)
#> Loading required package: xml2

url <- "https://www.hkexnews.hk/sdw/search/mutualmarket.aspx?t=sh"
session <- html_session(url)
#> Error in curl::curl_fetch_memory(url, handle = handle): SSL certificate problem: unable to get local issuer certificate
```
Which implements https://chromedevtools.github.io/devtools-protocol/tot/Input


This would be a good TDD project for someone who has significant rvest experience, and some teaching experience.