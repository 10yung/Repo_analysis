merge
I had a general look at the BERT code. If I am not wrong, the BERT model in this folder is simply trained from the data itself, rather than transferred from a huge corpus? Could you help clarify this part?
作者你好，我在运行“a1_seq2seq_attention_train.py”的时候，设置"multi_label_flag=False"，却遇到错误提醒：“You must feed a value for placeholder tensor 'decoder_input' with dtype int32 and shape [?,6]”
在你代码 103 、104行说明，使用FLAGS.multi_label_flag=Flase的时候，不需要 decoder_input 才对。
![image](https://user-images.githubusercontent.com/27938135/67458353-977b9080-f668-11e9-8b35-57430e0d5924.png)
请问这是什么情况？
![image](https://user-images.githubusercontent.com/48819086/64243629-3e319380-cf3a-11e9-8f7b-e0e9652b9f20.png)
作者大大好像忘写了
Thanks for sharing the model. I was interested to run your models with the mentioned data however it was not possible. I spent quite a lot time to get the data. Getting the Baidu app and the data from it was a nightmare. I also tried to preprocess the data as you mentioned but there are many other dependencies. It seems that even in preprocessing, some intermediate/temporary files are used and these files are only available in the Baidu network. I signed up for the Baidu app but it does not recognizes non-chines phone number. Tried a lot and gave up.
Is it possible to host the data somewhere else?
因为我用的python3.5，所以一直报这个错，是
![image](https://user-images.githubusercontent.com/48819086/62779571-26572300-bae6-11e9-848b-394f1e17656b.png)data_util_zhihu.py里面的错误，我debug了半天也没找到到底是哪个数据的类型不对，你们有没有人遇到


When can I see [SpanBert](https://arxiv.org/pdf/1907.10529.pdf) here
i git clone the project failing, how can i clone it
------------------------------------------------
error: RPC failed; curl 18 transfer closed with outstanding read data remaining
fatal: The remote end hung up unexpectedly
fatal: early EOF
fatal: index-pack failed

First of all thanks for your effort to make this repo interesting. I ran the preprocessing notebook and was able to get some of the files, however the other scripts use lot of data files which is not easily accessible. I tried lot of time getting the Baidu storage account but couldn't because of oversees phone number. I was just wondering if you can share the script that generates those data files you used in your scripts.
你好，请问是否缺少数据文件。
data/ieee_zhihu_cup/data.h5
data/ieee_zhihu_cup/vocab_label.pik
question_train_set3.txt
question_topic_train_set3.txt
question_eval_set3.txt