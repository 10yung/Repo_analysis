Hi, I'm trying to use Kamon 2.0 with Prometheus reporting in my Akka 2.5.27 application.

I'm not seeing any 'akka' metrics being published.

my pom...
` ...

        <dependency>
            <groupId>io.kamon</groupId>
            <artifactId>kamon-core_2.12</artifactId>
            <version>2.0.4</version>
        </dependency>
        <dependency>
        	<groupId>io.kamon</groupId>
        	<artifactId>kamon-akka_2.12</artifactId>
        	<version>2.0.1</version>
        	<scope>runtime</scope>
        </dependency>
        <dependency>
               <groupId>io.kamon</groupId>
               <artifactId>kamon-system-metrics_2.12</artifactId>
               <version>2.0.1</version>
               <scope>runtime</scope>
         </dependency>
         <dependency>
                <groupId>io.kamon</groupId>
                <artifactId>kamon-prometheus_2.12</artifactId>
                <version>2.0.1</version>
                <scope>runtime</scope>
       	</dependency>
      	<dependency>
                <groupId>io.kamon</groupId>
                <artifactId>kanela-agent</artifactId>
                <version>1.0.4</version>
                <scope>provided</scope>
         </dependency>
         <dependency>
               <groupId>io.kamon</groupId>
               <artifactId>kamon-logback_2.12</artifactId>
               <version>2.0.2</version>
               <scope>runtime</scope>`
         </dependency>
`
Documentation suggests all I need to do is Kamon.init() but Prometheus scrape does not include akka metrics.

Kamon Status page doen't no anything about them either....
![Kamon](https://user-images.githubusercontent.com/4900853/72237937-1ced5c00-362c-11ea-957a-e10af9f79db9.PNG)

What am I missing??
The kamon-jmx artifact seems to be no longer maintained, so I cannot use it with Scala 2.13

Is there any currently working JMX-Reporter?
`kamon.Kamon.counter("foo")` causes a classloader leak in playframework when using the dev workflow with `run`.

Here's a minimal project with instructions to reproduce the issue: https://github.com/ornicar/play-cdi-leak/tree/kamon

My setup:
- Linux 5.4.2
- openjdk version "13.0.1" 2019-10-15
- scala 2.13.1
- playframework 2.8.0
- sbt 1.3.5

[edit] Same issue on playframework side: https://github.com/playframework/playframework/issues/9935
At some point when we started Kamon we had a single repository with all the code in there. It was convenient to have everything on the same place but for reasons that I really can't remember at the moment we started splitting instrumentation and reporters into dedicated repositories. Funny thing is that somehow we went back to that with the kamon-bundle, not by joining repositories but by putting all dependencies back into a single package that people can reference with a single version number.

Some of the pain points coming up often are:
  - Users constantly complain about different Kamon components having different versions. This got a bit better with the bundle, but I still see examples of people using the same Kamon version variable for all their dependencies and missing important upgrades because versions are not aligned and they are not using the bundle.
  - As a developer, every time a tiny change needs to be done on core or instrumentation-common there is a chain publishing locally and testing changes until things finally work that makes it quite unpleasant. 
  - Similarly to the above, every time there is a release we need to individually publish each one of the independent projects that were affected. Not a big deal, but this is done manually at the moment.
  - Some pieces of instrumentation are very closely related to each other. For example, the Play Framework instrumentation is tied to the Akka HTTP instrumentation which in turn requires the Akka instrumentation and the Common instrumentation to work. This usually means jumping and publishing different artifacts locally for troubleshooting and fixing things.

Rough Idea:
  - Pull all of the instrumentation repositories and the bundle into kamon-io/Kamon an every time we make a release, publish everything together, even if there are no changes in some of the artifacts.
  - Keep separate repositories for all reporters? Or maybe a joint repo with all reporters in there.
  - Keep a separate repository for Kanela.

Benefits:
  - Besides solving everything on the pain points section, having a single repository can also help centralize all issues and pull requests and that can make it more manageable. I constantly find myself frustrated jumping between different issues and PRs on different projects and realizing that I left things unanswered or unattended for long. I'm not saying that this would fix the problem, but will probably help.

The challenges I see here:
  - Running tests on the entire thing. This will definitely increase the time it takes to test stuff. On the other hand, we will always be testing everything together and at one, which eliminates the chance of introducing cross-module compatibility issues.
  - Automating the release process. If a single release process will end up publishing all artifacts at once, that's going to take quite some time and is prone to failures, but I believe that with proper automation this can stop being a pain in the ass.

Looking forward to comments on this, it might be a good Christmas project :smile: /cc @dpsoft @mladens @adriancole 


I want to use use kamon with lagom.

I added the kamon bundle in my application and kanela-plugin, as per the documentation kanela should automatically start and instrumentation should begin but nothing happens.

Moreover I want to use log-reporter but I am not able to find "factory" to provide in the config file to use kamon-log-reporter module.

I added this plugin :-

```
addSbtPlugin("io.kamon" % "sbt-kanela-runner" % "2.0.3")
```
This is my build.sbt file:-
```
val kamon = "io.kamon" %% "kamon-bundle" % "2.0.4"
val logReporter = "io.kamon" %% "kamon-log-reporter" % "0.6.8"

lazy val `root` = (project in file("."))
  .enablePlugins(JavaAgent)
  .aggregate(`root-api`, `root-impl`)
  .settings(libraryDependencies in ThisBuild ++= Seq(macwire,kamon,logReporter))
  .settings(
    credentials += Credentials(Path.userHome / ".sbt" / ".credentials")
  )
  .settings(
    javaOptions in Universal += "-DKamon.auto-start=true",
  )
```
This is my log-reporter conf:-

```
kamon{
  modules{
      enabled = true
      name ="LOG REPORTER"
      description = "Logs the metrics"
      factory = "" //don't know what to add here
      }
     }
```

This is the repo link :- https://github.com/Arpit1596/lagom-kamon-example
Im trying to track internal asynchronous operations duration (potentially hundreds within one trace) where some metric tags are collected along the way.

 Initial approach was to create a span, `tagMetrics` on it and finish at the end resulting in `span.processing-time` tracking duration and being properly tagged while also reporting a span for each execution. Although having subspans for operations is desirable, in some cases i would like to make it configurable. Currently, options are having simple duration tracking via spans but producing and reporting (in case of a positive sampling decision of a parent operation) unneccessary spans (only their metrics are) or manually track start times for each operation since by disabling span creation we are loosing their metrics.

 Couple of solutions that come to mind... 
- a special type of span to be used as a metric "vehicle" to propagate around which is, once finished and metrics recorded, discarded by tracer (irregardles of sampling decision).   `Kamon.tempSpanBuilder()` 
 
- Additional methods to discard regular local span  
`Kamon.spanBuilder("operNameIrrelevant").finishAndDiscard()`

- Make discard decision a parameter to finish 
`span.finish(dicard = true)`

 Wdyt?
 
I just realised there is support in kamon 2.x for providing a custom `Sampler`implementation just by configuring it
```
sampler = "com.area51.CustomSampler"
```
A bit of hidden feature as it doesn't seem to be documented anywhere....:)

Anyways my issue is that the sampler trait only provides the operation name to use as decision point which is kind of pointless for akka-http  client operations as the name contains the actual HTTP operation e.g. GET, POST,...

A simple way to test this:
First a custom sampler
```
class CustomSampler extends Sampler {
  override def decide(operation: Sampler.Operation): Trace.SamplingDecision = {
    println("sample? -> "+operation.operationName())
    Trace.SamplingDecision.Sample
  }
}
```
Then a simple akka-http route
```
private lazy val route: Route =
   pathPrefix("health") {
     get {
       complete(HttpEntity(ContentTypes.`text/plain(UTF-8)`, "OK!"))
     }
   }
```
Using the akka-http client like this
```
Http().singleRequest(HttpRequest(uri = s"http://localhost:$port/health")) 
```
Will render a printout like
```
sample? -> GET
```

If I direct a browser towards my local app I instead get a printout like
```
sample? -> /health
```
So on the server side of the sampling decision I get the URL which makes it much more usable.
Having the custom sampler on the client side is useless as I can't make any decision based purely on the HTTP operation.

The use cases I'm trying to solve_
* apps periodically poll Consul for config and service updates - These lookups URL's I don't want sampled at all.
* An orchestration system periodically polls a `/health`URL on the apps - These I don't want sampled either

Yes there's the new AdaptiveSampler but I want a random sampler with a filtering capability.
This because I want to be able to control the sampling density in runtime just by tweaking probability setting AND have the filtering capability.
Could have been solved in a custom sampler provided I would get more relevant information to make the decision on.




Kinda obvious, at this point we should already have gRPC support!
Heard already a few times in the last week about users missing the client instrumentation for the Mongo Driver. More info in the [official driver page](http://mongodb.github.io/mongo-java-driver-reactivestreams/)

/cc @jtjeferreira 