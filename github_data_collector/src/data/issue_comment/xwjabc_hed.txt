Thank you for your code!
However,my output/epoch-#-train have 7 gray graphs,and the training process can be carried out without error reporting,looking forward to your reply!
In the datasets.py file, the original code is like this:
        image = image - np.array((104.00698793,  # Minus statistics.
                                  116.66876762,
                                  122.67891434))
but How can i get the new statistics number for my own dataset?
Thanks for your code and the model I trained is quite good(ODS=0.788.OIS=0.806)。 I just have a question when i see the log during the training,the  epoch average batch_loss maintains at around 1700 after 5th epoch ,until the end. Does this mean that the model does not converge?
When we evaluate the result，we need to run the EdgeEvalDir.m，but what's the meaning of  maxDist. I am confused about it and there isn't code.Can you explian it or give some papers to explain it?
Thanks a lot！ 
Hi @xwjabc, 

Thanks for your code! 

Could you please tell me if it is possible to have a higher batch-size (>1) for training? I see that when I try this out, I get the following error -
``upsample2 = torch.nn.functional.conv_transpose2d(score_dsn2, self.weight_deconv2, stride=2)``
``Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight'``

Just wondering if you know this already. 

Thanks,
AA
Hello, thanks for sharing! I want to get the five side-outputs however Pytorch is different to caffe. Is 
 there any suggestions? Thanks a lot! Looking forward to your reply!
Thanks for this progect!
My compilation for Caffe or [modified HED](https://github.com/zeakey/hed) doesn't sussess.  I noticed cafffemodel can be converted into pickle with some [tools](https://github.com/pierluigiferrari/caffe_weight_converter). But it still uses Caffe as Dependency. 
Could you upload "hed_pretrained_bsds.py36pickle" file?
Thanks for your help.
Hi @xwjabc 
Thanks for your contribution, I have a question just in the line below:
```
else:
                # Zero initialization following official repository.
                # Reference: hed/docs/tutorial/layers.md
m.weight.data.zero_() 
```
I see that you have used tensorflow too, so this weight initializer is something like
tf.truncated_normal_initializer(mean=0.) or tf.constant_initializer(0.0)

Thanks in advance
xavysp


https://github.com/xwjabc/hed/blob/c8ed5abc4d2b6ad2862b0d61cf6184ce2cdf3cae/hed.py#L100


Thank you for this elegent project !  I am trying to train a HED model using my own dataset. All the hyper-parameters are followed by this project and the results is also reasonable. However, the fifth side outputs of all the test image (shown below) are almost gray, which confuses me greatly. Could you have some ideas about this scenario?

![batch-499-1st-image](https://user-images.githubusercontent.com/24312469/51987652-f3502900-24dd-11e9-9e9e-0b85d5918ca4.png)
 
![batch-499-1st-image](https://user-images.githubusercontent.com/24312469/51987609-dca9d200-24dd-11e9-9e87-5e9ae009f3dd.png)
![batch-499-1st-image](https://user-images.githubusercontent.com/24312469/51987623-e5020d00-24dd-11e9-9e79-8ee6a6476e84.png)


Thanks for sharing your code and it works well.

I have some doubts about the learning rate. Why learning rate for conv5 is 100x of base learning rate, and classification layer(dsns) is 0.01x of base learning rate?  For fune-tuning,  learning rate for classification layer(which is trained from scratch) is usually set 10x compared to the backbone network parameters, right?

Besides, if I insert new layers into hed, how should I set the learning rate for them?