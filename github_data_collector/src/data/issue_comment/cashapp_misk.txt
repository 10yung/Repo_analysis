Implemented the  `scheduleWithFixedDelay` and `scheduleWithFixedRate` functions for the _FakeScheduledExecutorService_.

Since this is a single-threaded executor,  and the service always has to catch up to the clock, fixed delay is no different from fixed rate to my understanding.
... JUnit 5, Kotlin, Mockito, Moshi, OkHttp, Datadog, Retrofit,
ShadowJar, SLF4J.
Adds an identifier to a hibernate session which stays the same across retries. Can be used to determine whether something happened in the same transaction.
For #1330. We need misk.security.ssl for clients
When you switch to the new Android Gradle plugin 3.X.X, you should replace all your compile with the implementation keyword (1*). Then try to compile and test your app. If everything it's ok leave the code as is, if you have problems you probably have something wrong with your dependencies or you used something that now is private and not more accessible. Suggestion by Android Gradle plugin engineer Jerome Dochez (1)*)

If you are a library mantainer you should use api for every dependency which is needed for the public API of your library, while use implementation for test dependencies or dependencies which must not be used by the final users.

https://stackoverflow.com/questions/44413952/gradle-implementation-vs-api-configuration
```kotlin
@Constraint("director.name")
fun directorName(name: String): MovieQuery

queryFactory.newQuery<MovieQuery>()
  .directorName("George Lucas")
  .delete(session)
```

will generate something like `delete from movies cross join directors where name="George Lucas";` which is invalid.
Our cluster abstraction exposes `ClusterResourceMapper` as an instance variable of `Cluster.Snapshot`. This makes sense because the mapper uses only the set of cluster members to generate a mapping from resource ids to cluster members .

However, the Range/Round Robin ResourceMapper described in #1336 requires both the set of cluster members and the set of all resource ids.

This PR separates `ClusterResourceMapper` from `Cluster.Snapshot` and adds a `ClusterResourceMapper.Provider` that provides it based on the current cluster snapshot.

This makes it easy to add a range `ClusterResourceMapper.Provider` that reads both the current cluster members and all leases.




**tl;dr:** consistent hashing is the only way to load balance leases in misk. Although it is a well established way to partition databases, counter intuitively, it is not the best way to load balance event consumers, where the number of total leases is small and each consumer is stateless.

Our team operates a service that consumes an event topic. This topic has 32 partitions and emits 400+ events per second. At some time last week, the service was auto scaled 20 pods. 

Most pods were doing work. But according to the metrics, the load on these pods was not evenly distributed: 4 pods were idle, not consuming any events, while 2 pods were busy, consuming a lot of events.

After doing some math, I found this:
- Assuming the consistent hash ring assigns partitions to a specific pod (letâ€™s call it pod A) 1 time out of 20 (given 20 pods)
- Given 32 partitions, we have the following binomial distribution:

<img width="612" alt="Screen Shot 2019-12-12 at 9 58 22 PM" src="https://user-images.githubusercontent.com/4694896/71133199-ae44ec00-21ae-11ea-8e89-01b8d3d3878d.png">

- the probability that pod A receives 0 partition is 0.19371,
- the probability that pod A receives 1 partitions is 0.32625
- the probability that pod A receives 2 partitions is 0.26615
- the probability that pod A receives 3 partitions is 0.14008
- the probability that pod A receives 4 partitions is 0.05345

This distribution, consistent with the production metrics, is not ideal: 4 pods are idle (consuming 0 partition), 4 pods take twice the load (consuming 3 - 4 partitions). 

As we increase the number of partitions, the distribution becomes more uniform:

**# of partitions = 10000**
- <img width="519" alt="Screen Shot 2019-12-12 at 10 22 32 PM" src="https://user-images.githubusercontent.com/4694896/71133200-ae44ec00-21ae-11ea-893e-52b6aa6454e1.png">

**# of partitions = 100000**
- <img width="522" alt="Screen Shot 2019-12-12 at 10 23 19 PM" src="https://user-images.githubusercontent.com/4694896/71133201-ae44ec00-21ae-11ea-8b58-ade23ba06548.png">

It is common for databases to have more than 1^6 rows. But it is not practical for a topic to have this many partitions.

There are other ways to assign partitions to worker pods that yield more consistent and uniform results. For example,
- kafka client provides a [RoundRobinAssignor](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/RoundRobinAssignor.java) and a [RangeAssignor](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/RangeAssignor.java#L68) 
- kinesis client uses something similar to [RangeAssignor]( https://github.com/awslabs/amazon-kinesis-client/blob/master/amazon-kinesis-client/src/main/java/software/amazon/kinesis/leases/dynamodb/DynamoDBLeaseTaker.java)

Conclusion: we should provide similar options in misk for stateless lease holders. 
This does not fail gracefully and failing to lookup a feature flag
should not impact a production system.
See also #1044 
The misk module currently contains several groups of things:
- Relatively dependency free things: clustering, config, environment, metrics(?), token gen, etc
- Clients: misk.client, including TypedHttpClient, etc
- Server: misk.web.jetty, including some other helpers
- Actions: most of the existing misk module code; anything that touches WebAction. clients also depend on this in tests today.

We should separate the misk module into something like these four pieces:
misk-core
misk-server
misk-client
misk-actions

Additionally we should merge misk-inject into misk-core