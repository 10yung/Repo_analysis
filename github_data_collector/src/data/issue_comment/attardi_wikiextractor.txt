Hi,

I am getting UnicodeDecodeErrors when I try to extract a decompressed .xml dump. For the record, this is how I am using the WikiExtractor:

`WikiExtractor.py wikicorpus_en.xml -b 100M --processes 50 -o /path/to/extraction/folder/`

I am suspecting [this line](https://github.com/attardi/wikiextractor/blob/master/WikiExtractor.py#L2871) to be at the origin of the problem since `fileinput.hook_compressed` seems to result in opening the file using `open(filename, mode)` instead of `open(filename, mode, encoding='utf-8')` which would avoid the decoding error.

Changing the line with this one solved the issue for me:

```input = fileinput.FileInput(input_file, openhook=fileinput.hook_encoded(encoding='utf-8'))```

If the tool wasn't intended to be used with an already decompressed .xml dump you can close this issue. Thank you.

Hicham
Originally reported here: https://github.com/Common-Voice/common-voice-wiki-scraper/issues/44. I'm sure @mozillakab is happy to answer any questions needed.

Python Version: Python 3.6.1 on Windows 7 64bit

## Error

```
C:\tal\extractwiki\wikiextractor>python WikiExtractor.py --json ../kabwiki-latest-pages-articles-multistream.xml
Traceback (most recent call last):
File "WikiExtractor.py", line 3296, in
main()
File "WikiExtractor.py", line 3282, in main
args.compress, args.processes)
File "WikiExtractor.py", line 2874, in process_dump
for line in input:
File "C:\Python36\lib\fileinput.py", line 250, in next
line = self._readline()
File "C:\Python36\lib\fileinput.py", line 364, in _readline
return self._readline()
File "C:\Python36\lib\encodings\cp1252.py", line 23, in decode
return codecs.charmap_decode(input,self.errors,decoding_table)[0]
UnicodeDecodeError: 'charmap' codec can't decode byte 0x8d in position 7365: cha
racter maps to 
```

```
INFO:root:16770 A?e?da
INFO:root:16771 La casa de papel
INFO:root:16772 Imendi
INFO:root:16773 Ta?e?dayt
INFO:root:16774 Idwi
INFO:root:16777 Keikob
INFO:root:16778 Tidwin
...
...
...
INFO:root:18852 Albert camus
INFO:root:18853 Ife?mes
INFO:root:18855 Suduku
INFO:root:18856 I?ed
INFO:root:18857 Awwu
INFO:root:18858 Azwu
INFO:root:18861 Ame?ruy
INFO: Finished 3-process extraction of 4509 articles in 9.3s (484.9 art/s)
INFO: total of page: 7094, total of articl page: 4509; total of used articl page
: 4509
```

## Workaround

> I replaced the content of C:\Python36\lib\encodings\cp1252.py (Western Europe) with the content of C:\Python36\lib\encodings\utf_8.py (all languages) and it works
`cgi.escape` was deprecated and removed in Python 3.8 . Using `html.escape` is recommended. Thanks.

https://github.com/attardi/wikiextractor/blob/3162bb6c3c9ebd2d15be507aa11d6fa818a454ac/WikiExtractor.py#L811
Hi all,

It seems a lot of people have problems with the template expansion function, I found an old version at http://medialab.di.unipi.it/wiki/Wikipedia_Extractor. It works well.

You need to use python2 to run it.

Thanks!
Can I have the bibtex entry for citing this project?
Additional flag that allows to keep section titles hierarchy e.g.:
`Section::::Anarchist schools of thought.:Classical.:Mutualism.` in [Anarchism - Wikipedia](https://en.wikipedia.org/wiki?curid=12).
Consider this fake article:

```
<page>
    <title>bug demo</title>
    <ns>0</ns>
    <id>245544</id>
    <revision>
      <id>71636704</id>
      <parentid>69945883</parentid>
      <timestamp>2019-02-12T11:24:39Z</timestamp>
      <contributor>
        <username>Uuu1996</username>
        <id>1195068</id>
      </contributor>
      <comment>blah</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">

[[Category:one]]
[[Category:two]]</text>
      <sha1>acht3hj6108tk2v3ngv7b05aerdsefc</sha1>
    </revision>
</page>
```

Because of the way the loop that builds the list of categories works (see `catSet` in `pages_from`), the last category is not added to the list of categories here and filtering on categories doesn't work.  If a newline is added after `[[Category:two]]` here then the category is correctly added to `catSet` and will be detected.
The readme says that the package can be installed with setup.py, but there is no setup.py in the repository.
Hi!
I run command 
`python WikiExtractor.py --json enwiki-latest-pages-articles.xml.bz2`
and expected get one json file as output, but got
```
INFO: Finished 7-process extraction of 5940667 articles in 9328.5s (636.8 art/s)
INFO: total of page: 10433470, total of articl page: 5940667; total of used articl page: 5940667
```
and folder **text** with many inner folders like **AA**, **AB**, **AC** ...  
What command should I use to get one json file?I want to use it as described in this manual https://github.com/NVIDIA/Megatron-LM#collecting-wikipedia-training-data
- When `--extract_categories` is given, page categories are extracted and saved along with `title` and `text`.
- Note that [sortkeys](https://en.wikipedia.org/wiki/Help:Category#Sorting_category_pages) are dropped.
  - e.g. `[[Category:Category name|Sortkey]]` -> Only `Category name` is extracted.