## Expected Behavior
I'm Using server_http_ex.cpp example. I just added a line after "set_listening_port" 

`our_web_server.set_listening_ip("111.111.111.111"); //  this IP is model. i added at that time my pc public ip`

## Current Behavior
then i compile with VS 2019 and run in command line.
 gave me a error : 
"error occurred in server::start()
unable to create listener"

but example works perfectly with IP 112.0.0.1 => localhost

Please let us know how should we use dlib http server to public serve.

* **Version**: 19.19
* **Where did you get dlib**: official release
* **Platform**: Windows 10 1909 64bit
* **Compiler**: visual Studio 2019

Dlib is available as a port in VCPKG , documenting the install process here will help users get started by providing a single set of commands to build dlib, ready to be included in their projects.

VCPKG is a C++ library manager that simplifies installation for dlib and other project dependencies, we also test whether our library ports build in various configurations (dynamic, static) on various platforms (OSX, Linux, Windows: x86, x64, UWP, ARM) to keep a wide coverage for users.

I'm a maintainer for vcpkg, and here is what the port script looks like. We try to keep the library maintained as close as possible to the original library.
Hello, I recently used dlib (the python module) for detecting human facial landmarks. I'd like to say dilb's detection is fast and pretty accurate!
However, I'd like to detect more facial landmarks points(e.g. 500 points) to improve the performance. Since dlib's facial landmark detection is based on regression trees, so I'd like to train them by my self.(don't worry for the data, my partner will help me to label some face images). 
So I'd like to know is there any way that I can modify the dlib facial landmarks detector to let it detect 500 points in a face image? And if the answer is yes, how should I train it?

- Add new loss layer for binary loss per pixel (say, for completeness)
- In the instance-segmentation example, resolve possibly overlapping instances using the per-pixel output intensity (and not result rect size as previously); while at it, also make the instance output look "soft":
![image](https://user-images.githubusercontent.com/2297572/72452415-2f030200-37c6-11ea-9c93-2d88d2fe1e9c.png)

Hello

Can i feed face images with boundary boxes and resolutions from other face detectors  into the ``DLIB's face_recognition_model _v1``  to get the embedding vectors ?  
For example `` 40*40 `` face with boundary boxes  from ``OpenCV cv2.dnn.readNetFromCaffe `` ? 
If it is possible , how would the embedding vectors of the same image with different resolutions  i,e ``40*40``   , `` 80*80``  differ? 


As python 3.7 has been released for long time, will dlib support python 3.7? I tried several ways to install dlib in my conda environment, but none of the online solutions works. Thank you.

My install commands are:  conda install -c menpo dlib python=3.7

The conflict messages are:

Package certifi conflicts for:
python=3.7 -> pip -> setuptools -> certifi[version='>=2016.09|>=2016.9.26']
dlib -> python[version='>=3.5,<3.6.0a0'] -> pip -> setuptools -> certifi[version='>=2016.09|>=2016.9.26']
Package ca-certificates conflicts for:
python=3.7 -> openssl[version='>=1.1.1b,<1.1.2a'] -> ca-certificates
dlib -> python[version='>=3.5,<3.6.0a0'] -> ca-certificates
Package pip conflicts for:
dlib -> python[version='>=3.5,<3.6.0a0'] -> pip
python=3.7 -> pip
Package python conflicts for:
python=3.7
dlib -> python[version='2.7.*|3.5.*|>=3.5,<3.6.0a0']
dlib -> numpy -> python[version='>=2.7,<2.8.0a0']
Package setuptools conflicts for:
dlib -> python[version='>=3.5,<3.6.0a0'] -> pip -> setuptools
python=3.7 -> pip -> setuptools
Package wheel conflicts for:
dlib -> python[version='>=3.5,<3.6.0a0'] -> pip -> wheel
python=3.7 -> pip -> wheel
I tried to optimize HOG detector by resizing input image or frame by 32x240 or 240x180, but my original image before resizing was around 1280x720 or it could be 980x720.

Whenever image gets resized from original to 320x240 or 240x180 then below issues are happening:

```
1. Detection is not accurate if face is little FAR from camera , so this detection's output result causing issue for dlib Landmark(LM) prediction. (So dlib LM points are coming outside the face).

2. Once image resized from 1280x720 or 960x720, then face should be closer to camera in order to get a good detection result. (Once detection result is good then dlib LM will be accurate on face). And Face won't be detected if face is little Far from camera.

3.  HOG detector needs image or frame should be of resolution "Standard Screen ratio"(4:3), otherwise detection result is not accurate.

3. Here I pass gray image for HOG detector, So I get only average of 5 to 8 FPS after resizing image from original size.

```

And additional observations: HOG detector is slower, hence I am passing resized image or frame here. I used below code for HOG detection, Is there anyway we can improve speed and accurate detection in HOG.
(FYI: I used opencvHaarCascade(xml) for face detection, it was faster and detects accurately for resized image or frame. It gives good detection result too, So dlib LM works very well with this Haarcascade for all kind of resize). 

Please let me know how it make HOG detection accurate in result for resized image. And How to improve FPS to achieve around 15 to 20(FPS).

```
//passing gray scale image as input
dlib::cv_image<unsigned char> cimg_small(small_frame);
// Detect faces 
// using dlib Hog detector
std::vector<dlib::rectangle> faces = this->detector(cimg_small);
```



Hi @davisking . Thanks a lot for creating a ML/DL tool for training different avenues of models.

I would like to leverage the tool you developed to train my own face recognition model on ~7M images I have collected. I am also willing to open source this if it passes the benchmark. Fingers crossed.

Before I involve myself in this massive training task, here are a couple of questions that seek your attention:

- Were faces aligned for training dlib's face recognition model v1? If yes, what techniques were used?
- Were faces resized for training dlib's face recognition model v1? If yes, what dimensions?
- Does [dnn_metric_learning_on_images_ex.cpp](https://github.com/davisking/dlib/blob/master/examples/dnn_metric_learning_on_images_ex.cpp) suffice to start training?
- Any data augmentation (other than mentioned in the example file above) was used during the original training of dlib's face recognition model v1?
- If there were any changes made to the above example file, would you please share the changes or the original training code template?
- Any other tips/tricks for training face recognition model using dlib tool?

Thank you for your time. :)
##  Expected Behavior
Say, I have two GPUs in the system, how to find out which one is used by dlib? I gues it should be specifiable during the build, as a command line argument of the compiled executable or something. IMHO, it is true for both examples and unit tests, especially for unit tests.

## Current Behavior
GPUs (or just one GPU) are utilized by the samples and unit tests, no idea which one, no way to specify one.

## Steps to Reproduce
Just running the samples and unit tests

* **Version**: 19.19
* **Where did you get dlib**: dlib.net
* **Platform**: Win10 x64 10.0.17763
* **Compiler**: MSVC 19.0.24215.1 (VS2015 Update 3)

##  Expected Behavior
I have compiled dlib test pack using CMake, noticed that CUDA won't be used due to the lack of cuDNN, then I run "dtest --runall" without a single error (using CPU only, no CUDA support).

I have then added cuDNN (7.1.4, last one for CUDA 8.0) an re-compiled test pack, made sure that CUDA will be used:
-- Found CUDA: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0 (found suitable version "8.0", minimum required is "7.5")
-- Looking for cuDNN install...
-- Found cuDNN: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0/lib/x64/cudnn.lib
-- Building a CUDA test project to see if your compiler is compatible with CUDA...
-- Checking if you have the right version of cuDNN installed.
-- Enabling CUDA support for dlib.  DLIB WILL USE CUDA

Expected to have "dtest --runall" without a single error as well (with CUDA support).

## Current Behavior
Three tests (test_cublas, test_dnn and test_ranking) fail with similar error:
Failure message from test: Error while calling cudaOccupancyMaxPotentialBlockSize(&num_blocks,&num_threads,K) in file d:\...\dlib\dlib\cuda\cuda_utils.h:186. code: 8, reason: invalid device function

I believe the kernel function passed to cudaOccupancyMaxPotentialBlockSize is considered as invalid (via dlib::cuda::launch_kernel).

Any ideas how to fix that or what might be the cause? The station I'm working on has one GTX 580 and one Tesla M2090, other CUDA-using software run on those devices properly. I'm stuck with Fermi for a while (which means I'm stuck with CUDA 8.0 and MSVC2015), however, present versions do not seem to vialoate dlib requirements.

## Steps to Reproduce
I believe everything is clear from the Expected Behaviour section

* **Version**: 19.19
* **Where did you get dlib**: dlib.net
* **Platform**: Win10 x64 10.0.17763
* **Compiler**: MSVC 19.0.24215.1 (VS2015 Update 3), CUDA 8.0, cuDNN 7.1.4.
