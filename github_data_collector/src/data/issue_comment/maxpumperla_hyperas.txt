I used the complete example not from GitHub but from http://maxpumperla.com/hyperas/ 
It doesn't work in recent Keras, and only makes confusion, better fix it or remove the website.
Hi, first of all thank you for your work.

I'm trying to use your framework to optimize hiperparameters in my Convolutional Neural Network in order to implement an image classifier.

I'm obtaining the following error:
ValueError: Error when checking input: expected conv2d_1_input to have shape (216, 360, 3) but got array with shape (300, 500, 3)

I have checked my data construction function and the training data returned have the right shape (216, 360, 3) but for some reason the model receive an input with shape (300, 50, 3). 
I really don't know what to do, here is my code:


```
def dataAdv1():
    dfAll = pd.read_csv('filePath')
    labels = pd.read_csv('filePath')

    df = pd.read_csv('filePath')
    
    #this function returns a list of all the images
    imageList = createImageList(df, dfAll)
    imageTmp = imageList
    # this function is used to get the list of labels
    labels_list = clearAdvancingLabel(labels)
    labelTmp = labels_list
    #this function divide the data into train, validation and test set
    x_train, y_train, x_valid, y_valid, x_test, y_test = splitTrainValidationTest(imageTmp, labelTmp)
    
    y_train = y_train.ravel()
    y_valid = y_valid.ravel()
    y_test = y_test.ravel()
    
    return x_train, y_train, x_test, y_test, x_valid, y_valid


def create_modelAdvancing(x_train, y_train, x_test, y_test, x_valid, y_valid):
    model = Sequential()
    model.add(Conv2D({{choice([4, 16, 32, 64])}}, (3, 3), input_shape=(216, 360, 3)))
    model.add(Activation({{choice(['relu', 'tanh'])}}))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    
    num_layers = {{choice(['one', 'two', 'three', 'four'])}}
    
    if num_layers =='two':
        model.add(Conv2D({{choice([4, 16, 32, 64])}}, (3, 3)))
        model.add(Activation({{choice(['relu', 'tanh'])}}))
        model.add(MaxPooling2D(pool_size=(2, 2)))
    elif num_layers == 'three':
        model.add(Conv2D({{choice([4, 16, 32, 64])}}, (3, 3)))
        model.add(Activation({{choice(['relu', 'tanh'])}}))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Conv2D({{choice([4, 16, 32, 64])}}, (3, 3)))
        model.add(Activation({{choice(['relu', 'tanh'])}}))
        model.add(MaxPooling2D(pool_size=(2, 2)))
    elif num_layers == 'four':
        model.add(Conv2D({{choice([4, 16, 32, 64])}}, (3, 3)))
        model.add(Activation({{choice(['relu', 'tanh'])}}))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Conv2D({{choice([4, 16, 32, 64])}}, (3, 3)))
        model.add(Activation({{choice(['relu', 'tanh'])}}))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Conv2D({{choice([4, 16, 32, 64])}}, (3, 3)))
        model.add(Activation({{choice(['relu', 'tanh'])}}))
        model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Flatten())  
    model.add(Dense({{choice([4, 16, 32, 64])}}))
    model.add(Activation({{choice(['relu', 'tanh'])}}))
    model.add(Dropout({{uniform(0, 1)}}))
    model.add(Dense(1))
    model.add(Activation({{choice(['softmax', 'sigmoid'])}}))
    
    chooseOptimizer = {{choice(['adam', 'sgd', 'rmsprop'])}}    
    model.compile(loss='binary_crossentropy', optimizer=chooseOptimizer, metrics=['accuracy'])

    model.fit(x_train, y_train,
              batch_size={{choice([4, 16, 32, 64])}},
              epochs={{choice([10, 30, 50])}},
              verbose=2,
              validation_data=(x_valid, y_valid))
    score, acc = model.evaluate(x_test, y_test, verbose=0)
    print('Test accuracy:', acc)
    return {'loss': -acc, 'status': STATUS_OK, 'model': model.to_yaml()}


# Create Spark context
conf = SparkConf().setAppName('Elephas_Hyperparameter_Optimization').setMaster('local[*]')
sc = SparkContext(conf=conf)

# Define hyper-parameter model and run optimization
hyperparam_model = HyperParamModel(sc)
hyperparam_model.minimize(model=create_modelAdvancing, data=dataAdv1, max_evals=5)
```

Thank you in advance for your help

First of all thank you very much for your work, 

I'm trying to use your framework to optimize hiperparameters in my LSTM network in order to implement a sentiment analysis classifier. 

I used some snippet you posted but I cannot make it work. I think the main issue is how to calculate embedding_matrix (I'm using word embedings) to train the network. I trained separately tokenize to get weights file. 

**I'm getting the following error:**

>>> Hyperas search space:

def get_space():
    return {
        'Dropout': hp.uniform('Dropout', 0, 1),
        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam', 'sgd']),
    }

Traceback (most recent call last):
  File "optim_keras.py", line 132, in <module>
    best_run = optim.minimize(model=keras_model,data=get_data(),algo=tpe.suggest,max_evals=10,trials=Trials())
  File "/usr/bin/anaconda3/lib/python3.7/site-packages/hyperas/optim.py", line 69, in minimize
    keep_temp=keep_temp)
  File "/usr/bin/anaconda3/lib/python3.7/site-packages/hyperas/optim.py", line 98, in base_minimizer
    model_str = get_hyperopt_model_string(model, data, functions, notebook_name, verbose, stack)
  File "/usr/bin/anaconda3/lib/python3.7/site-packages/hyperas/optim.py", line 198, in get_hyperopt_model_string
    data_string = retrieve_data_string(data, verbose)
  File "/usr/bin/anaconda3/lib/python3.7/site-packages/hyperas/optim.py", line 219, in retrieve_data_string
    data_string = inspect.getsource(data)
  File "/usr/bin/anaconda3/lib/python3.7/inspect.py", line 973, in getsource
    lines, lnum = getsourcelines(object)
  File "/usr/bin/anaconda3/lib/python3.7/inspect.py", line 955, in getsourcelines
    lines, lnum = findsource(object)
  File "/usr/bin/anaconda3/lib/python3.7/inspect.py", line 768, in findsource
    file = getsourcefile(object)
  File "/usr/bin/anaconda3/lib/python3.7/inspect.py", line 684, in getsourcefile
    filename = getfile(object)
  File "/usr/bin/anaconda3/lib/python3.7/inspect.py", line 666, in getfile
    type(object).__name__))
TypeError: module, class, method, function, traceback, frame, or code object was expected, got tuple

Thank you in advance for your help 

Here's my code
```

from hyperopt import Trials, STATUS_OK, tpe
from hyperas import optim
from hyperas.distributions import choice, uniform



def get_data():
  import pickle
  from keras.preprocessing import sequence
  from keras.models import Sequential
  from keras.layers.core import Dense, Dropout, Activation
  from keras.layers.embeddings import Embedding
  from keras.layers.recurrent import LSTM
  from keras.datasets import imdb
  from keras.callbacks import EarlyStopping, ModelCheckpoint
  from keras.preprocessing.sequence import pad_sequences
  from keras.utils import to_categorical
  from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
  import classes.filtros as NT
  import classes.data_processing as DP
  import classes.embeddings as EB
  import classes.model as M
  import classes.token as T
  import classes.parameters as Params
  import pandas as pd
  import numpy as np
  import sys
  import time 

  # Carga de datos 
  print('Loading data...')
  #instanciamos objetos necesarios
  text_array = NT.NormalizeText()
  data_processing = DP.DataProcessing()
  word_embedding = EB.ProcessEmbeddings()
  w2v = word_embedding.get_word2vec(Params.W2V_FILE)
  data_set = data_processing.load_data()

  # Separacion de los valores para tener un training set equilibrado
  neutros = [row for row in data_set if 0  == row[2]]
  positiv = [row for row in data_set if 1  == row[2]]
  negativ = [row for row in data_set if -1 == row[2]]


  df_neutros = pd.DataFrame.from_records(neutros)
  df_positiv = pd.DataFrame.from_records(positiv)
  df_negativ = pd.DataFrame.from_records(negativ)

  minimo = np.min([len(df_neutros),len(df_positiv),len(df_negativ)])
  df_final = pd.concat([df_neutros[:minimo], df_positiv[:minimo], df_negativ[:minimo]], ignore_index=True)

    # Cargamos el tokenizer aprendido
  token_path = './models/Tokenizer.pkl'
  t_m = T.TokenizerModel()
  with open(token_path, 'rb') as f:
      t_m.t = pickle.load(f)


  # # Procesado del texto y generación del token list
  filtered = pd.DataFrame(columns=['textos'])
  for row in df_final.itertuples():
    texto_filt = word_embedding.clean_text(row._2)
    filtered.loc[row.Index] = texto_filt

  encoded_docs = t_m.t.texts_to_sequences(filtered['textos'])
  
  # Codificamos los Documentos de entrada
  X = pad_sequences(encoded_docs, maxlen=Params.MAX_SEQUENCE_LENGTH, padding='post')
  y = df_final[2]
  # # Separamos en train y test 
  sss = StratifiedShuffleSplit(n_splits=15,test_size=0.15)

  for train_index, test_index in sss.split(X, y):
      X_train, X_test = X[train_index], X[test_index]
      Y_train, Y_test = y[train_index], y[test_index]

  # # Debemos cambiar a categorical las etiquetas dada la función de pérdida que usamos en el entrenamiento
  y_train_bin = to_categorical(Y_train, num_classes=3, dtype='int32')
  y_test_bin = to_categorical(Y_test, num_classes=3, dtype='int32')

  return X_train,y_train_bin,X_test,y_test_bin


def keras_model(X_train,y_train_bin,X_test,y_test_bin):
  import pickle
# Definición del modelo y entrenamiento

  word_embedding = EB.ProcessEmbeddings()
  w2v = word_embedding.get_word2vec(Params.W2V_FILE)
  text_array = NT.NormalizeText()
  # Cargamos el tokenizer aprendido
  token_path = './models/Tokenizer.pkl'
  t_m = T.TokenizerModel()
  with open(token_path, 'rb') as f:
      t_m.t = pickle.load(f)

  # Generación de la embedding matrix (vocab_size, t, w2v, text_array)
  embedding_matrix = word_embedding.Generate_Matrix(Params.MAX_NB_WORDS, t_m.t, w2v,text_array)

  print('Build model...')
  # Modelo LSTM
  model   = Sequential()
  model.add(Embedding(Params.MAX_NB_WORDS, output_dim=Params.EMBEDDING_DIM, input_length=Params.MAX_SEQUENCE_LENGTH, weights=[embedding_matrix], trainable=False))
  model.add(Bidirectional(LSTM(Params.LSTM_UNITS_1ST,  return_sequences=False)))
  model.add(Dropout({{uniform(0, 1)}}))
  model.add(Dense(3, activation='softmax'))

  model.compile(optimizer={{choice(['rmsprop', 'adam', 'sgd'])}}, loss='categorical_crossentropy', metrics=['categorical_accuracy'])
  model.train(X_train, y_train_bin, X_test, y_test_bin)



  early_stopping = EarlyStopping(monitor='val_loss', patience=4)
  checkpointer = ModelCheckpoint(filepath='keras_weights.hdf5',
                                   verbose=1,
                                   save_best_only=True)

  hist = model.fit(X_train, y_train_bin,
                     nb_epoch=1,
                     validation_split=0.08,
                     show_accuracy=True,
                     callbacks=[early_stopping, checkpointer])

  score, acc = model.evaluate(X_test, y_test_bin, show_accuracy=True, verbose=0)

  print('Test accuracy:', acc)
  return {'loss': -acc, 'status': STATUS_OK}



if __name__ == '__main__':
    best_run = optim.minimize(model=keras_model,data=get_data(),algo=tpe.suggest,max_evals=10,trials=Trials())
    print(best_run)




```
Is it possible, in a multiple GPU scenario, to have each available GPU doing a separate trial? So far it seems that using multi_gpu_model is not accelerating our computer vision deep learning model (U-net / Mask RCNN), so having each trial running on a separate GPU could provide us with great speedups, but I've found no information on the matter.

Thank you.
Before filing an issue, please make sure to tick the following boxes.

- [x] Make sure your issue hasn't been filed already. Use GitHub search or manually check the [existing issues](https://github.com/maxpumperla/hyperas/issues), also the closed ones. Also, make sure to check the FAQ section of our [readme](https://github.com/maxpumperla/hyperas/blob/master/README.md).

- [x] Install latest hyperas from GitHub:
pip install git+git://github.com/maxpumperla/hyperas.git

- [x] Install latest hyperopt from GitHub:
pip install git+git://github.com/hyperopt/hyperopt.git

- [x] We have continuous integration running with Travis and make sure the build stays "green". If, after installing test utilities with `pip install pytest pytest-cov pep8 pytest-pep8`, you can't successfully run `python -m pytest` there's very likely a problem on your side that should be addressed before creating an issue.

- [x] Create a gist containing your complete script, or a minimal version of it, that can be used to reproduce your issue. Also, add your _full stack trace_ to that gist. In many cases your error message is enough to at least give some guidance.

According to issue #204 the number of layers could be tuned by a for-loop 

```
model = ...
num_layers = <result of randint>
for _ in range(num_layers):
        model.add(Dense({{choice([np.power(2,5),np.power(2,9),np.power(2,11)])}}))
        model.add(Activation({{choice(['tanh','relu', 'sigmoid'])}}))
        model.add(Dropout({{uniform(0, 1)}}))
```

But the layers added in the for-loop are missing when you look in the `space `object. This is because `hyperparameter_names(model_string)` is not able to find the string parts in `mode_string`.
Aim is to optimize hyperparameters for a Keras LSTM via Hyperas. While implying the structure of the example like this:

```
def LSTM_Hyperas_Data():
    X_train_augment = pd.read_excel("EscapeKanji_X.xlsx")
    y_train_augment = pd.read_excel("EscapeKanji_y.xlsx")

    X_train_augment = X_train_augment.drop(columns="Del")
    y_train_augment = y_train_augment.drop(columns="Del")
    
    Augmentor=60
    Train_Num=1396

    X_train_augment_R=np.array(X_train_augment).reshape(Augmentor+1,Train_Num,1)
    y_train_augment_R=np.array(y_train_augment).reshape(Augmentor+1,Train_Num,1)

    X_train_Hyperas, X_test_Hyperas, y_train_Hyperas, y_test_Hyperas = train_test_split(X_train_augment_R, y_train_augment_R, test_size=0.045, random_state=618)
    x_train, y_train, x_test, y_test=X_train_Hyperas, y_train_Hyperas,X_test_Hyperas, y_test_Hyperas
    return x_train, y_train, x_test, y_test


def createLSTMModel(x_train, y_train, x_test, y_test):#learning_rate={{choice([0.001,0.01,0.1,0.0001])}},activation={{choice(['relu', 'linear','sigmoid','hard_sigmoid', 'tanh'])}}, loss={{choice(['logcosh', 'mae', 'mse', 'hinge','squared_hinge'])}}, n_jobs=1):
    
#     X_train_R_Grid=X_train_smallLSTM.reshape(4,Train_Num,1)
#     y_train_R_Grid=y_train_smallLSTM.reshape(4,Train_Num,1)
    
    
    #K.clear_session()
    
    model = Sequential()
    #model.add(Reshape((2,Train_Num,), input_shape=(1,2*Train_Num,)))
    model.add(Bidirectional(LSTM(250, return_sequences=True,activation='relu'),input_shape=(Train_Num,1)))
    model.add(Dropout({{uniform(0.05,0.7)}}))
    model.add(Bidirectional(LSTM(170, return_sequences=True,activation='relu')))
    model.add(Dropout({{uniform(0.05,0.7)}}))
    model.add(Bidirectional(LSTM(25, return_sequences=True, activation='relu')))
    model.add(Dropout({{uniform(0.05,0.7)}}))
    #model.add(Flatten())
    model.add(Dense(1))
    model.compile(optimizer='Nadam',loss='logcosh')
    
    model.fit(x_train, y_train,
              batch_size={{choice([64,128,200,256])}},
              epochs=15,
              show_accuracy=True,
              verbose=2,
              validation_split=0.1)
    
    validation_acc = np.amax(result.history['mse']) 
    print('Best validation acc of epoch:', validation_acc)
              
    return {'loss': mse, 'status': STATUS_OK, 'model': model}

if __name__ == '__main__':
    best_run, best_model = optim.minimize(model=createLSTMModel,
                                          data=LSTM_Hyperas_Data,
                                          algo=tpe.suggest,
                                          max_evals=5,
                                          trials=Trials(),
                                          notebook_name='EscapeKanji')
                                         #encoding='utf-8
    x_train, y_train, x_test, y_test = LSTM_Hyperas_Data()
    print("Evalutation of best performing model:")
    print(best_model.evaluate(x_test, y_test))
    print("Best performing model chosen hyper-parameters:")
    print(best_run)

```

Number of hyperparameters to optimize is reduced to Batch_Size only to reduce possible error sources, as following error persists:

```
>>> Imports:
#coding=utf-8

from __future__ import print_function

try:
    import tensorflow as tf
except:
    pass

try:
    import numpy as np
except:
    pass

try:
    import seaborn as sns
except:
    pass

try:
    import scipy.io as sio
except:
    pass

try:
    import joblib
except:
    pass

try:
    import pandas as pd
except:
    pass

try:
    import matplotlib.pyplot as plt
except:
    pass

try:
    import pyexcel as pe
except:
    pass

try:
    from hyperopt import Trials, STATUS_OK, tpe
except:
    pass

try:
    from hyperas import optim
except:
    pass

try:
    from hyperas.distributions import choice, uniform
except:
    pass

try:
    import sys
except:
    pass

try:
    import os
except:
    pass

try:
    import transforms3d as TF3d
except:
    pass

try:
    import statsmodels
except:
    pass

try:
    import random
except:
    pass

try:
    import bezier
except:
    pass

try:
    from imblearn.over_sampling import SMOTE
except:
    pass

try:
    from operator import itemgetter
except:
    pass

try:
    from statsmodels import robust
except:
    pass

try:
    from openpyxl import Workbook
except:
    pass

try:
    from os.path import dirname, join
except:
    pass

try:
    from pylab import rcParams
except:
    pass

try:
    from time import time
except:
    pass

try:
    from numba import cuda
except:
    pass

try:
    import plotly_express as px
except:
    pass

try:
    import holoviews as hv
except:
    pass

try:
    import bokeh.io
except:
    pass

try:
    import bokeh.models
except:
    pass

try:
    import bokeh.palettes
except:
    pass

try:
    import bokeh.plotting
except:
    pass

try:
    from ipywidgets import interact
except:
    pass

try:
    from math import sqrt
except:
    pass

try:
    from bokeh.palettes import Spectral4, Spectral6
except:
    pass

try:
    from bokeh.io import output_file, show, push_notebook, curdoc
except:
    pass

try:
    from bokeh.plotting import figure, output_file, show
except:
    pass

try:
    from bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper, ColorBar
except:
    pass

try:
    from bokeh.transform import transform
except:
    pass

try:
    from bokeh.layouts import widgetbox
except:
    pass

try:
    from bokeh.models.widgets import Dropdown, CheckboxGroup, Select, Slider, TextInput
except:
    pass

try:
    from sklearn import preprocessing
except:
    pass

try:
    from sklearn.ensemble import BaggingRegressor
except:
    pass

try:
    from sklearn import model_selection
except:
    pass

try:
    from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer
except:
    pass

try:
    from sklearn.svm import SVR
except:
    pass

try:
    from sklearn.linear_model import LinearRegression
except:
    pass

try:
    from sklearn.model_selection import train_test_split
except:
    pass

try:
    from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score
except:
    pass

try:
    from sklearn.model_selection import cross_val_score, cross_val_predict, cross_validate, GridSearchCV, RandomizedSearchCV
except:
    pass

try:
    from sklearn.neural_network import MLPRegressor
except:
    pass

try:
    from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor
except:
    pass

try:
    from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, LSTM, Bidirectional, Reshape
except:
    pass

try:
    from keras.models import Model, Sequential
except:
    pass

try:
    from keras.layers import BatchNormalization
except:
    pass

try:
    from keras.optimizers import Adam, RMSprop, SGD, Nadam
except:
    pass

try:
    from keras.regularizers import l2
except:
    pass

try:
    from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard
except:
    pass

try:
    from keras import backend as K
except:
    pass

>>> Hyperas search space:

def get_space():
    return {
        'learning_rate': hp.choice('learning_rate', [0.001,0.01,0.1,0.0001]),
        'activation': hp.choice('activation', ['relu', 'linear','sigmoid','hard_sigmoid', 'tanh']),
        'loss': hp.choice('loss', ['logcosh', 'mae', 'mse', 'hinge','squared_hinge']),
        'Dropout': hp.uniform('Dropout', 0.05,0.7),
        'Dropout_1': hp.uniform('Dropout_1', 0.05,0.7),
        'Dropout_2': hp.uniform('Dropout_2', 0.05,0.7),
        'batch_size': hp.choice('batch_size', [64,128,200,256]),
    }

>>> Data
  1: 
  2: X_train_augment = pd.read_excel("EscapeKanji_X.xlsx")
  3: y_train_augment = pd.read_excel("EscapeKanji_y.xlsx")
  4: 
  5: X_train_augment = X_train_augment.drop(columns="Del")
  6: y_train_augment = y_train_augment.drop(columns="Del")
  7: 
  8: Augmentor=60
  9: Train_Num=1396
 10: 
 11: X_train_augment_R=np.array(X_train_augment).reshape(Augmentor+1,Train_Num,1)
 12: y_train_augment_R=np.array(y_train_augment).reshape(Augmentor+1,Train_Num,1)
 13: 
 14: X_train_Hyperas, X_test_Hyperas, y_train_Hyperas, y_test_Hyperas = train_test_split(X_train_augment_R, y_train_augment_R, test_size=0.045, random_state=618)
 15: x_train, y_train, x_test, y_test=X_train_Hyperas.astype('float32'), y_train_Hyperas.astype('float32'),X_test_Hyperas.astype('float32'), y_test_Hyperas.astype('float32')
 16: 
 17: 
 18: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3:     
   4: #     X_train_R_Grid=X_train_smallLSTM.reshape(4,Train_Num,1)
   5: #     y_train_R_Grid=y_train_smallLSTM.reshape(4,Train_Num,1)
   6:     
   7:     
   8:     #K.clear_session()
   9:     
  10:     model = Sequential()
  11:     #model.add(Reshape((2,Train_Num,), input_shape=(1,2*Train_Num,)))
  12:     model.add(Bidirectional(LSTM(250, return_sequences=True,activation='relu'),input_shape=(Train_Num,1)))
  13:     model.add(Dropout(space['learning_rate']))
  14:     model.add(Bidirectional(LSTM(170, return_sequences=True,activation='relu')))
  15:     model.add(Dropout(space['activation']))
  16:     model.add(Bidirectional(LSTM(25, return_sequences=True, activation='relu')))
  17:     model.add(Dropout(space['loss']))
  18:     #model.add(Flatten())
  19:     model.add(Dense(1))
  20:     model.compile(optimizer='Nadam',loss='logcosh')
  21:     
  22:     model.fit(x_train, y_train,
  23:               batch_size=space['Dropout'],
  24:               epochs=15,
  25:               show_accuracy=True,
  26:               verbose=2,
  27:               validation_split=0.1)
  28:     
  29:     validation_acc = np.amax(result.history['mse']) 
  30:     print('Best validation acc of epoch:', validation_acc)
  31:               
  32:     return {'loss': mse, 'status': STATUS_OK, 'model': model}
  33: 
  0%|                                                                              | 0/5 [00:00<?, ?it/s, best loss: ?]

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-60-033c997d802b> in <module>
     37                                           max_evals=5,
     38                                           trials=Trials(),
---> 39                                           notebook_name='EscapeKanji')
     40                                          #encoding='utf-8
     41     x_train, y_train, x_test, y_test = LSTM_Hyperas_Data()

~\Anaconda3\envs\Tensorflow\lib\site-packages\hyperas\optim.py in minimize(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space, keep_temp)
     67                                      notebook_name=notebook_name,
     68                                      verbose=verbose,
---> 69                                      keep_temp=keep_temp)
     70 
     71     best_model = None

~\Anaconda3\envs\Tensorflow\lib\site-packages\hyperas\optim.py in base_minimizer(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack, keep_temp)
    137              trials=trials,
    138              rstate=np.random.RandomState(rseed),
--> 139              return_argmin=True),
    140         get_space()
    141     )

~\Anaconda3\envs\Tensorflow\lib\site-packages\hyperopt\fmin.py in fmin(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)
    386             catch_eval_exceptions=catch_eval_exceptions,
    387             return_argmin=return_argmin,
--> 388             show_progressbar=show_progressbar,
    389         )
    390 

~\Anaconda3\envs\Tensorflow\lib\site-packages\hyperopt\base.py in fmin(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)
    637             catch_eval_exceptions=catch_eval_exceptions,
    638             return_argmin=return_argmin,
--> 639             show_progressbar=show_progressbar)
    640 
    641 

~\Anaconda3\envs\Tensorflow\lib\site-packages\hyperopt\fmin.py in fmin(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)
    405                     show_progressbar=show_progressbar)
    406     rval.catch_eval_exceptions = catch_eval_exceptions
--> 407     rval.exhaust()
    408     if return_argmin:
    409         return trials.argmin

~\Anaconda3\envs\Tensorflow\lib\site-packages\hyperopt\fmin.py in exhaust(self)
    260     def exhaust(self):
    261         n_done = len(self.trials)
--> 262         self.run(self.max_evals - n_done, block_until_done=self.asynchronous)
    263         self.trials.refresh()
    264         return self

~\Anaconda3\envs\Tensorflow\lib\site-packages\hyperopt\fmin.py in run(self, N, block_until_done)
    225                     else:
    226                         # -- loop over trials and do the jobs directly
--> 227                         self.serial_evaluate()
    228 
    229                     try:

~\Anaconda3\envs\Tensorflow\lib\site-packages\hyperopt\fmin.py in serial_evaluate(self, N)
    139                 ctrl = base.Ctrl(self.trials, current_trial=trial)
    140                 try:
--> 141                     result = self.domain.evaluate(spec, ctrl)
    142                 except Exception as e:
    143                     logger.info('job exception: %s' % str(e))

~\Anaconda3\envs\Tensorflow\lib\site-packages\hyperopt\base.py in evaluate(self, config, ctrl, attach_attachments)
    842                 memo=memo,
    843                 print_node_on_error=self.rec_eval_print_node_on_error)
--> 844             rval = self.fn(pyll_rval)
    845 
    846         if isinstance(rval, (float, int, np.number)):

~\temp_model.py in keras_fmin_fnct(space)

~\Anaconda3\envs\Tensorflow\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     89                 warnings.warn('Update your `' + object_name +
     90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---> 91             return func(*args, **kwargs)
     92         wrapper._original_function = func
     93         return wrapper

~\Anaconda3\envs\Tensorflow\lib\site-packages\keras\layers\core.py in __init__(self, rate, noise_shape, seed, **kwargs)
    100     def __init__(self, rate, noise_shape=None, seed=None, **kwargs):
    101         super(Dropout, self).__init__(**kwargs)
--> 102         self.rate = min(1., max(0., rate))
    103         self.noise_shape = noise_shape
    104         self.seed = seed

TypeError: '>' not supported between instances of 'str' and 'float'
```

Is there any known fix to that. Much THX in advance!

Best regards,
Tobias



First: Lots of thanks for the amazing Hyperas work you already did. Here the issue:

Aim is to optimize hyperparameters for a Keras LSTM via Hyperas. While implying the structure of the example like this:

```
def LSTM_Hyperas_Data():
    X_train_Hyperas, X_test_Hyperas, y_train_Hyperas, y_test_Hyperas = train_test_split(X_train_augment_R, y_train_augment_R, test_size=0.045, random_state=618)
    return X_train_Hyperas, X_test_Hyperas, y_train_Hyperas, y_test_Hyperas

def createLSTMModel():#learning_rate={{choice([0.001,0.01,0.1,0.0001])}},activation={{choice(['relu', 'linear','sigmoid','hard_sigmoid', 'tanh'])}}, loss={{choice(['logcosh', 'mae', 'mse', 'hinge','squared_hinge'])}}, n_jobs=1):
    
#     X_train_R_Grid=X_train_smallLSTM.reshape(4,Train_Num,1)
#     y_train_R_Grid=y_train_smallLSTM.reshape(4,Train_Num,1)
    
    actviation={{choice(['relu', 'linear','sigmoid','hard_sigmoid', 'tanh'])}}

    #K.clear_session()
    
    model = Sequential()
    #model.add(Reshape((2,Train_Num,), input_shape=(1,2*Train_Num,)))
    model.add(Bidirectional(LSTM(250, return_sequences=True,activation=activation),input_shape=(Train_Num,1)))
    model.add(Dropout({{uniform(0.1,0.7)}}))
    model.add(Bidirectional(LSTM(170, return_sequences=True,activation=activation)))
    model.add(Dropout({{uniform(0.1,0.7)}}))
    model.add(Bidirectional(LSTM(25, return_sequences=True, activation=activation)))
    model.add(Dropout({{uniform(0.1,0.7)}}))
    #model.add(Flatten())
    model.add(Dense(1))
    model.compile(optimizer={{choice(['Nadam', 'Adam', 'RMSProp'])}},loss={{choice(['logcosh', 'mae', 'mse', 'hinge','squared_hinge'])}})
    
    model.fit(X_train_Hyperas, y_train_Hyperas,
              batch_size={{choice([64,128,200,256])}},
              epochs={{choice([15,25])}},
              show_accuracy=True,
              verbose=2,
              validation_split=0.1)
    
    validation_acc = np.amax(result.history['val_acc']) 
    print('Best validation acc of epoch:', validation_acc)
              
    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}

if __name__ == '__main__':
    best_run, best_model = optim.minimize(model=createLSTMModel,
                                          data=LSTM_Hyperas_Data,
                                          algo=tpe.suggest,
                                          max_evals=5,
                                          trials=Trials(),
                                          notebook_name='Nihondam')
    X_train_Hyperas, X_test_Hyperas, y_train_Hyperas, y_test_Hyperas = data()
    print("Evalutation of best performing model:")
    print(best_model.evaluate(X_test_Hyperas, y_test_Hyperas))
    print("Best performing model chosen hyper-parameters:")
    print(best_run)

```

the following error call was produced:

```
---------------------------------------------------------------------------
UnicodeDecodeError                        Traceback (most recent call last)
<ipython-input-325-4f9941d8a87c> in <module>
     38                                           max_evals=5,
     39                                           trials=Trials(),
---> 40                                           notebook_name='Nihondam')
     41     X_train_Hyperas, X_test_Hyperas, y_train_Hyperas, y_test_Hyperas = data()
     42     print("Evalutation of best performing model:")

~\Anaconda3\envs\Tensorflow\lib\site-packages\hyperas\optim.py in minimize(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space, keep_temp)
     67                                      notebook_name=notebook_name,
     68                                      verbose=verbose,
---> 69                                      keep_temp=keep_temp)
     70 
     71     best_model = None

~\Anaconda3\envs\Tensorflow\lib\site-packages\hyperas\optim.py in base_minimizer(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack, keep_temp)
     96         model_str = full_model_string
     97     else:
---> 98         model_str = get_hyperopt_model_string(model, data, functions, notebook_name, verbose, stack)
     99     temp_file = './temp_model.py'
    100     write_temp_files(model_str, temp_file)

~\Anaconda3\envs\Tensorflow\lib\site-packages\hyperas\optim.py in get_hyperopt_model_string(model, data, functions, notebook_name, verbose, stack)
    178         notebook_path = os.getcwd() + "/{}.ipynb".format(notebook_name)
    179         with open(notebook_path, 'r') as f:
--> 180             notebook = nbformat.reads(f.read(), nbformat.NO_CONVERT)
    181             exporter = PythonExporter()
    182             source, _ = exporter.from_notebook_node(notebook)

~\Anaconda3\envs\Tensorflow\lib\encodings\cp1252.py in decode(self, input, final)
     21 class IncrementalDecoder(codecs.IncrementalDecoder):
     22     def decode(self, input, final=False):
---> 23         return codecs.charmap_decode(input,self.errors,decoding_table)[0]
     24 
     25 class StreamWriter(Codec,codecs.StreamWriter):

UnicodeDecodeError: 'charmap' codec can't decode byte 0x8f in position 2383145: character maps to <undefined>
```

Is this a known issue? It's struggling with the Notebook name chars, it appears. Hope this is indeed an issue for you and no waste of time.

Best regards,
Tobias


two workstations ，both Windows 10+jupyter notebook+python3.6，run the same code， one is normal,  the other one is getting below error on executing the code：

Traceback (most recent call last):

  File "C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py", line 3326, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)

  File "<ipython-input-1-f26deadceafa>", line 76, in <module>
    notebook_name="my_illusion_deepnet")

  File "C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\hyperas\optim.py", line 70, in minimize
    keep_temp=keep_temp)

  File "C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\hyperas\optim.py", line 99, in base_minimizer
    model_str = get_hyperopt_model_string(model, data, functions, notebook_name, verbose, stack)

  File "C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\hyperas\optim.py", line 190, in get_hyperopt_model_string
    imports = extract_imports(cleaned_source, verbose)

  File "C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\hyperas\utils.py", line 40, in extract_imports
    tree = ast.parse(source)

  File "C:\ProgramData\Anaconda3\envs\tensorflow\lib\ast.py", line 35, in parse
    return compile(source, filename, mode, PyCF_ONLY_AST)

  File "<unknown>", line 548
    if __name__ == '__main__':in
                               ^
SyntaxError: invalid syntax

Before filing an issue, please make sure to tick the following boxes.

- [ ] Make sure your issue hasn't been filed already. Use GitHub search or manually check the [existing issues](https://github.com/maxpumperla/hyperas/issues), also the closed ones. Also, make sure to check the FAQ section of our [readme](https://github.com/maxpumperla/hyperas/blob/master/README.md).

- [ ] Install latest hyperas from GitHub:
pip install git+git://github.com/maxpumperla/hyperas.git

- [ ] Install latest hyperopt from GitHub:
pip install git+git://github.com/hyperopt/hyperopt.git

- [ ] We have continuous integration running with Travis and make sure the build stays "green". If, after installing test utilities with `pip install pytest pytest-cov pep8 pytest-pep8`, you can't successfully run `python -m pytest` there's very likely a problem on your side that should be addressed before creating an issue.

- [ ] Create a gist containing your complete script, or a minimal version of it, that can be used to reproduce your issue. Also, add your _full stack trace_ to that gist. In many cases your error message is enough to at least give some guidance.
