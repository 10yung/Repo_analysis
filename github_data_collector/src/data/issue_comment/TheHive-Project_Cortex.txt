
### Work Environment

| Question              | Answer
|---------------------------|--------------------
| OS version (server)       | Ubuntu
| OS version (client)       | 10
| Cortex version / git hash   | Fresh install from DEB
| Package Type              | DEB
| Browser type & version    | Firefox


### Problem Description
After updating database in Cortex, when the create administrator account pages comes up, nothing happens when I press create after filling up the credentials. I tried refreshing the page but then "Error: user init not found" comes up. The logs show that there is a version conflict, document already exists so tried deleting the Cortex instance and elasticsearch to re-initiate Cortex but still unable to create administrator account. Also tried starting over from the very beginning by installing everything again but still facing the same issue. 

Had done this before and everything was working fine until recently. 

### Steps to Reproduce
1. Install the new instance of Cortex and ElasticSearch


### Complementary information
2019-12-03 06:09:34,717 [INFO] from org.thp.cortex.services.ErrorHandler in application-akka.actor.default-dispatcher-5 - POST /api/user returned 400
org.elastic4play.ConflictError: [doc][cortex]: version conflict, document already exists (current version [1])
        at org.elastic4play.database.DBConfiguration.$anonfun$execute$2(DBConfiguration.scala:146)
        at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
        at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
        at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
        at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91)
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
        at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
        at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)
        at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
        at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)


### Request Type
Bug 

### Work Environment

| Question              | Answer
|---------------------------|--------------------
| OS version (server)       | Debian



### Problem Description
I can't use any analyser, for any of them I try to use I'm geeting:

`"errorMessage": "java.util.concurrent.ExecutionException: javax.ws.rs.ProcessingException: java.io.IOException: No such file or directory",`

### Steps to Reproduce
1.Configure any analyzer
2.Request some data

### Request Type
Feature Request

### Work Environment

| Question              | Answer
|---------------------------|--------------------
| OS version (server)       | Ubuntu 16.04
| OS version (client)       | Ubuntu, ...
| Cortex version / git hash   | 2.x
| Package Type              | From source
| Browser type & version    | ...


### Problem Description
At the moment all jobs are being run through a single queue. If one user, e.g. a machine account of some tool runs a lot of jobs, no other user will be able to run something in parallel.

### Steps to Reproduce
1. Have a one user continuously run a constant amount of jobs.
1. Have other users also wanting to run jobs on demand.
1. The other users will have to wait a long time.

### Possible Solutions
My proposal is to have one job queue per user account and instead of just draining one queue, the job scheduler could go through all queues in a round-robin fashion so that all users handled equally.

### Complementary information
We are using Cortex as a central API gateway for observable enhancement and therefore could really make good use of a better queuing system.

# Analyzer EML-Parser doesn't work with Cortex 3.0.0 Docker version

### Request Type
Bug

### Work Environment

| Question              | Answer
|---------------------------|--------------------
| OS version (server)       | Docker
| Cortex version / git hash   | 3.0.0
| Package Type              | Docker


### Problem Description
Docker Cortex 3.0.0 is build from a Debian 9 whereas Docker Cortex 2.1.3 is build from Debian 10. In Debian 9, the last python version is 3.5.3 : analyzer EML-Parser doesn't work with this version.

### Steps to Reproduce
1. Install Cortex 3.0.0 with Docker
2. Enable EML-Parser
3. Execute several times the analyzer on an eml with as another eml in attachment.
4. Note that the body result is random (because the list order is random).
5. Reproduce those step on Docker Cortex 2.1.3 and it works.

### Possible Solutions
Use a Debian 10 to build the Cortex container, as before.

### Request Type
Bug

### Work Environment

| Question              | Answer
|---------------------------|--------------------
| OS version (server)       | Ubuntu
| Cortex version / git hash   | 3.0.0-1
| Package Type              | Binary
| ElasticSearch version              | 6.7
| Browser type & version    | Chrome


### Problem Description
After updating Cortex and TheHive to the latest versions (respectively 3.0.0-1 and 3.4.0-1) I have noticed a drop in performance in the execution of Cortex analyzers, mainly when I launch several in block from Thehive.
In the Cortex log I receive the following message:
```
[info] o.t.c.s.ErrorHandler - GET /api/job/Xq1zYm0Bdo8Y_Io8dnA8/waitreport?atMost=1%20minute returned 500
akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://application/user/audit#1151591280]] after [60000 ms]. Message of type [org.thp.cortex.services.AuditActor$Register]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
        at akka.pattern.PromiseActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:635)
        at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:650)
        at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)
        at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:874)
        at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113)
        at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
        at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:872)
```
And then the next one:
```
[error] o.e.d.DBConfiguration - ElasticSearch request failure: DELETE:/_search/scroll/?
StringEntity({"scroll_id":["DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAIrlFjdzaTEtXzY1UVgyZFN6SGdqTnBKVncAAAAAAACK5BY3c2kxLV82NVFYMmRTekhnak5wSlZ3AAAAAAAAiuYWN3NpMS1fNjVRWDJkU3pIZ2pOcEpWdwAAAAAAAIrnFjdzaTEtXzY1UVgyZFN6SGdqTnBKVncAAAAAAACK6BY3c2kxLV82NVFYMmRTekhnak5wSlZ3"]},Some(application/json))
 => ElasticError(404,404,None,None,None,List(),None)
```

I have noticed that this problem, I receive it when having activated in several analyzers the AutoExtract flag, when I deactivate it does not fail.
Also (I don't know if it has to see), when this happens the website is inaccessible and you can't navigate through it.

### Steps to Reproduce
1. enable the auto extract flag in the Shodan and VirusTotal analyzers
2. Run this analyzers over several ips (120 in my case)
3. Wait
4. Try to access the website

in previous versions this worked like a lightning for me.

Thank you for all your efforts!!
Feature Request

### Work Environment

| Question              | Answer
|---------------------------|--------------------
| OS version (server)       |  CentOS
| OS version (client)       | 7
| Cortex version / git hash   | cortex-3.0.0-0.1RC3.noarch
| Package Type              | Binary
| Browser type & version    | -

### Problem Description
Cortex cant sent appliaction logs to the logstash because some dependencie is missing.

### Steps to Reproduce
1. Modify logback.xml

Add
```xml
  <appender name="STASH"
        class="net.logstash.logback.appender.LogstashTcpSocketAppender">
        <destination>192.168.99.100:5000</destination>
 </appender>
```

and append the root section with this:
```xml
<appender-ref ref="LOGSTASH" />
```

2. start cortex

3. read the log

`
11:50:05,015 |-ERROR in ch.qos.logback.core.joran.action.AppenderAction - Could not create an Appender of type [net.logstash.logback.appender.LogstashTcpSocketAppender]. ch.qos.logback.core.util.DynamicClassLoadingException: Failed to instantiate type net.logstash.logback.appender.LogstashTcpSocketAppender
	at ch.qos.logback.core.util.DynamicClassLoadingException: Failed to instantiate type net.logstash.logback.appender.LogstashTcpSocketAppender
Caused by: java.lang.ClassNotFoundException: net.logstash.logback.appender.LogstashTcpSocketAppender
`


### Possible Solutions

Maybe this: https://stackoverflow.com/questions/46582135/logstash-failed-to-instantiate-type-net-logstash-logback-appender-logstashtcps



# Cortex try to execute with incorrect Analyzer path

### Request Type
Bug

### Work Environment

| Question              | Answer
|---------------------------|--------------------
| OS version (server)       | Ubuntu 18.04
| OS version (client)       | Windows 10
| Cortex version / git hash   | 3.0.0-1
| Package Type              | Binary
| Browser type & version    | Chrome, Edge Chromium


### Problem Description
The Path of Analyzer that Cortex try to execute is difference with real Analyzer Path.
Cortex Execute: /opt/Cortex-Analyzers/analyzers/McAfeeTIE/mcafeetie.py
Analyzer Path: /opt/Cortex-Analyzers/analyzers/McAfeeTIEAnalyzer/mcafeetie.py

Please see some image bellow to get the detail.
It was a Bug or I miss some config in somewhere ?

### Complementary information
Cortex Result
![image](https://user-images.githubusercontent.com/4435695/65218799-7233b980-dae1-11e9-8170-05e9049fe916.png)

Analyzer Location:
![image](https://user-images.githubusercontent.com/4435695/65218854-8f688800-dae1-11e9-9cee-5e06831ebc33.png)

Analyzer config:
![image](https://user-images.githubusercontent.com/4435695/65218916-ad35ed00-dae1-11e9-8502-1a5ac7e6eedf.png)

Cortex Application.log:
![image](https://user-images.githubusercontent.com/4435695/65221476-4e737200-dae7-11e9-874e-5c230dfa3c9b.png)

### Request Type
Bug

### Work Environment

| Question              | Answer
|---------------------------|--------------------
| OS version (server)       | Ubuntu 18.04
| OS version (client)       | macOS
| Cortex version / git hash   | 3.0.0-1
| Package Type              | Binary
| Browser type & version    | NA

### Problem Description
For some reason, Cortex appears to be expecting to use Docker by default.
I've recently upgraded to 3.0.0-1 and had manually upgrade cortexutils from 1.3.0 to 2.0.0

I'm not entirely sure why this is happening but obviously when we run analysers, Cortex attempts to push jobs to a Docker service that doesn't exist.

### Steps to Reproduce
1. Start up Cortex service
1. Submit job to Analyser (any analyser)
1. Review logs for errors
 

### Complementary information
Cortex Application Log

`2019-09-10 11:02:06,024 [INFO] from org.apache.http.impl.execchain.RetryExec in jersey-client-async-executor-0 - I/O exception (java.io.IOException) caught when processing request to {}->unix://localhost:80: No such file or directory
2019-09-10 11:02:06,025 [INFO] from org.apache.http.impl.execchain.RetryExec in jersey-client-async-executor-0 - Retrying request to {}->unix://localhost:80
2019-09-10 11:02:06,026 [INFO] from org.apache.http.impl.execchain.RetryExec in jersey-client-async-executor-0 - I/O exception (java.io.IOException) caught when processing request to {}->unix://localhost:80: No such file or directory
2019-09-10 11:02:06,026 [INFO] from org.apache.http.impl.execchain.RetryExec in jersey-client-async-executor-0 - Retrying request to {}->unix://localhost:80
2019-09-10 11:02:06,026 [INFO] from org.apache.http.impl.execchain.RetryExec in jersey-client-async-executor-0 - I/O exception (java.io.IOException) caught when processing request to {}->unix://localhost:80: No such file or directory
2019-09-10 11:02:06,027 [INFO] from org.apache.http.impl.execchain.RetryExec in jersey-client-async-executor-0 - Retrying request to {}->unix://localhost:80
2019-09-10 11:02:06,031 [INFO] from org.thp.cortex.services.DockerJobRunnerSrv in application-akka.actor.default-dispatcher-2 - Docker is not available
com.spotify.docker.client.exceptions.DockerException: java.util.concurrent.ExecutionException: javax.ws.rs.ProcessingException: java.io.IOException: No such file or directory`

Add a min_tlp value in Cortex alongside max_tlp. to control what the minimum tlp level is for an analyzer to run. 

### Request Type
Feature Request

### Work Environment

| Question              | Answer
|---------------------------|--------------------
| OS version (server)       | Ubuntu
| OS version (client)       | 10
| Cortex version / git hash   | 2.1.3-1
| Package Type              | Binary
| Browser type & version    | N/A

### Problem Description
We have internal Cortex instances and external Cortex instances. When we add external observables we want those to be sent to the external Cortex instance for analysis by 3rd party threat Intel providers.   When we add an internal IP we want that to be sent to the internal Cortex instance and not to external.

We've set the TLP values on the observables as per the guidance. An external IP is set to tlp:green. An internal IP seen is set to tlp:red. When we set the SecurityCenter max_tlp to red, external IP's get accepted by the analyzer. When we set the max_tlp on the analyzer to white, external IP's aren't accepted but also internal IP's aren't accepted either. 

What we want to do is set the min_tlp of the SecurityCenter analyzer (and other internal analyzers too) to have a min_tlp:red so that it will only accept observables with a tlp:red (essentially internal observables we don't want to share).

### Steps to Reproduce
Enable SecurityCenter Analyzer.
Create a case, and an an external IP as an observable. Set the TLP to red as you don't want to share this externally.
Run the SecurityCenter analyzer. It attempts to scan the external target.

### Possible Solutions
Create a min_tlp value for analyzers get the analyzers to respect that.

Cuckoo Analyser reports unexpected error when job is complete. Report exists in Cuckoo 2.0.7 and has run successfully with a score of 15.6.

### Request Type
Bug

### Work Environment

| Question              | Answer
|---------------------------|--------------------
| OS version (server)       | Ubuntu 18.04
| OS version (client)       | Ubuntu 16.04 (Cuckoo)
| Cortex version / git hash   | 2.1.3-1
| Package Type              | Binary
| Browser type & version    | N/A


### Problem Description
When uploading a file as an observable to Hive and running the Cuckoo analyser, after analysis Cortex reports "Unexpected Error".

### Steps to Reproduce
Open a case
Add an observable
Choose File as type and upload file for analysis.
Select to added observable and Choo Cuckoo Sandbox
 
### Complementary information
<img width="1399" alt="Screenshot 2019-08-25 at 21 11 49" src="https://user-images.githubusercontent.com/43755140/63655186-0594fb80-c77d-11e9-913e-ae07482c0abd.png">
