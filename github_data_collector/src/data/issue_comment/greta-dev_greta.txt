Hi Nick, 

As previously discussed I've begun to work on this. I thought I'd open this issue:

1. To track it, 
2. To host discussion and 
3. so I could ask some questions...

Namely: is the number of samples (both warmup and total) stored anywhere in the `model_info` attribute of a `greta_mcmc_list` object? I thought it would be saved in the `sampler` class but it doesn't appear to be. I wanted to get it to display it at the top the print method; I think that's useful and it's also what `rstan` does. 

Cheers!




1. suppress `check_tf_version()` when running on CI, so installation and tests don't fail

2. reconfigure TF and TFP installation in travis to accept different versions

3. add a matrix of TF and TFP versions

4. add information to a check results readme somewhere
I'm just starting out on a two-month (with some interruptions) coding sprint on greta. There are a bunch of bugfixes, half-finished (and some half-baked) features, and some required refactoring that I have been putting off because 2019 was a pretty busy year. I'm hoping to make some headway on some of these major issues and features I've been promising for a while.

I'll keep chipping away at things on this list, which I will modify as I go. The list is just here for me to keep track of things to do, and for others to follow along and comment if there are features they are particularly interested in. The order here is neither in order of priority nor the order in which I intend to work on things. I'll release new versions as I go, but I'm not sure yet which features and fixes each release will contain.

During this work, I'll also be trying to keep on top of other issues that come up and the forum, which I've been neglecting over the Christmas break. Now's a good time to ask questions [over there](https://forum.greta-stats.org/) :)

## 1. Bugfixes & misc

This is a subset of the open issues that I'm particularly keen to fix, and have a plan for. This doesn't mean the other open issues aren't important as well, and I'll try to get to some of those too.

 - [ ] 1.1 Fix thinning (to do after 2.3) #318 
 - [ ] 1.2 Speed up subsetting #309 
 - [x] 1.3 Array-scalar dispatch #298 
 - [ ] 1.4 Automated decentreing of distributions (requires delayed graph definition or extended representations interface, also applies to greta.gp for Gaussian GPs) #47 
 - [x] 1.5 Fix/exclude linting issues
 - [x] 1.6 Enable memory-safe (batched) prediction in `calculate()` #236
 - [x] 1.7 Make the output of `mcmc()` have its own class, inheriting from coda's mcmc class, and provide methods for post-hoc windowing, thinning etc.
 - [x] 1.8 Fix silently erroring primitive functions #317

## 2. Compatibility

A number of things have changed in the interfaces to TF and TFP (particularly in moving to TF 2.0). greta currently still works with the compatibility functions, but some refactoring is needed to fully support these.

 - [ ] 2.1 Refactor distributions code to be closer to TFP (also enables greta.hmm to use the TFP hmm funcitonality)
 - [ ] 2.2 Refactor internals to use TF functions, not graphs, for full compatibility with TF 2.0
 - [ ] 2.3 Use TFP sampler adaptation (with progress bar updating)

## 3. Marginalisation

There's an incomplete branch `feature/marginalise` that implements an interface for marginalisation of discrete random variables in a greta model. There's some work towards marginalisation of *a priori* multivariate normal variables via the Laplace approximation too. This all needs polishing up, thorough testing, documenting and releasing.

 - [ ] 3.1 Implement marginalisation interface #157 
 - [ ] 3.2 Check discrete marginalisation
 - [ ] 3.3 Check and fix Laplace marginalisation
 - [ ] 3.4 Plan general-purpose variational marginalisation
 - [ ] 3.5 Write documentation that exaplains what this feature is, since it will be unfamiliar to most users of MCMC software.

## 4. Sampling discrete variables

This has been on the to-do list for a long time. It will require a bit of refactoring and redesigning internals, but there's nothing about sampling of discrete random variables that should bee particularly tricky to implemnent.

 - [ ] 4.1 Implement discrete variables
 - [ ] 4.2 Implement discrete-only samplers
 - [ ] 4.3 Implement Gibbs sampling between discrete and continuous parameter spaces.

## 5. Simulation

Random independent sampling from a model object, optionally conditionally on fixed values or posterior samples, is a much-requested feature that needs a surprising amount of engineering in the background, and careful thinking about an intuitive interface. There's some existing work that just needs implementing, polishing up, testing and documenting, along with some examples of postreior predictive checks etc.

 - [ ] 5.1 Implement simulation interface via calculate(), following the discussion and proposed interface [here](https://forum.greta-stats.org/t/feedback-requested-proposed-simulate-interface/154/12?u=nick)

## 6. Continuous integration & TF versions

greta versions are now being tied to specific releases of TF and TFP. I was trying for a while not to do this, because I believe it's best practice not to be overly prescriptive about dependencies. However both TF and TFP are evolving fast and regularly introduce breaking changes. It would be good to catch these changes early with CI testing on the nightly releases of those dependencies.

 - [ ] 6.1 Set up a continuous integration grid that reflects the specific dependencies of all currently available and recent releases of greta
 - [ ] 6.2 Add a table (greta version vs. required TF and TF versions) of badges to the readme with the test results for these versions
 - [ ] 6.3 Add an entry for the dev version of greta agains both stable and nightly versions of TF and TFP
Installing greta in R 3.6 on Ubuntu 18.04 fails and gives the following traceback (also occurs on Ubuntu 19.10):

```r
> install.packages("greta")
Installing package into â€˜/home/username/R/x86_64-pc-linux-gnu-library/3.6â€™
(as â€˜libâ€™ is unspecified)
trying URL 'https://cloud.r-project.org/src/contrib/greta_0.3.1.tar.gz'
Content type 'application/x-gzip' length 882526 bytes (861 KB)
==================================================
downloaded 861 KB

* installing *source* package â€˜gretaâ€™ ...
** package â€˜gretaâ€™ successfully unpacked and MD5 sums checked
** using staged installation
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
*** copying figures
** building package indices
** installing vignettes
** testing if installed package can be loaded from temporary location

 *** caught illegal operation ***
address 0x7fc3e270765f, cause 'illegal operand'

Traceback:
 1: py_module_import(module, convert = convert)
 2: import(module)
 3: doTryCatch(return(expr), name, parentenv, handler)
 4: tryCatchOne(expr, names, parentenv, handlers[[1L]])
 5: tryCatchList(expr, classes, parentenv, handlers)
 6: tryCatch({    import(module)    TRUE}, error = clear_error_handler(FALSE))
 7: reticulate::py_module_available("tensorflow")
 8: have_tf()
 9: check_tf_version("startup")
10: fun(libname, pkgname)
11: doTryCatch(return(expr), name, parentenv, handler)
12: tryCatchOne(expr, names, parentenv, handlers[[1L]])
13: tryCatchList(expr, classes, parentenv, handlers)
14: tryCatch(fun(libname, pkgname), error = identity)
15: runHook(".onLoad", env, package.lib, package)
16: loadNamespace(package, lib.loc)
17: doTryCatch(return(expr), name, parentenv, handler)
18: tryCatchOne(expr, names, parentenv, handlers[[1L]])
19: tryCatchList(expr, classes, parentenv, handlers)
20: tryCatch({    attr(package, "LibPath") <- which.lib.loc    ns <- loadNamespace(package, lib.loc)    env <- attachNamespace(ns, pos = pos, deps, exclude, include.only)}, error = function(e) {    P <- if (!is.null(cc <- conditionCall(e)))         paste(" in", deparse(cc)[1L])    else ""    msg <- gettextf("package or namespace load failed for %s%s:\n %s",         sQuote(package), P, conditionMessage(e))    if (logical.return)         message(paste("Error:", msg), domain = NA)    else stop(msg, call. = FALSE, domain = NA)})
21: library(pkg_name, lib.loc = lib, character.only = TRUE, logical.return = TRUE)
22: withCallingHandlers(expr, packageStartupMessage = function(c) invokeRestart("muffleMessage"))
23: suppressPackageStartupMessages(library(pkg_name, lib.loc = lib,     character.only = TRUE, logical.return = TRUE))
24: doTryCatch(return(expr), name, parentenv, handler)
25: tryCatchOne(expr, names, parentenv, handlers[[1L]])
26: tryCatchList(expr, classes, parentenv, handlers)
27: tryCatch(expr, error = function(e) {    call <- conditionCall(e)    if (!is.null(call)) {        if (identical(call[[1L]], quote(doTryCatch)))             call <- sys.call(-4L)        dcall <- deparse(call)[1L]        prefix <- paste("Error in", dcall, ": ")        LONG <- 75L        sm <- strsplit(conditionMessage(e), "\n")[[1L]]        w <- 14L + nchar(dcall, type = "w") + nchar(sm[1L], type = "w")        if (is.na(w))             w <- 14L + nchar(dcall, type = "b") + nchar(sm[1L],                 type = "b")        if (w > LONG)             prefix <- paste0(prefix, "\n  ")    }    else prefix <- "Error : "    msg <- paste0(prefix, conditionMessage(e), "\n")    .Internal(seterrmessage(msg[1L]))    if (!silent && isTRUE(getOption("show.error.messages"))) {        cat(msg, file = outFile)        .Internal(printDeferredWarnings())    }    invisible(structure(msg, class = "try-error", condition = e))})
28: try(suppressPackageStartupMessages(library(pkg_name, lib.loc = lib,     character.only = TRUE, logical.return = TRUE)))
29: tools:::.test_load_package("greta", "/home/username/R/x86_64-pc-linux-gnu-library/3.6/00LOCK-greta/00new")
An irrecoverable exception occurred. R is aborting now ...
Illegal instruction (core dumped)
ERROR: loading failed
* removing â€˜/home/username/R/x86_64-pc-linux-gnu-library/3.6/gretaâ€™
Warning in install.packages :
  installation of package â€˜gretaâ€™ had non-zero exit status

The downloaded source packages are in
	â€˜/tmp/RtmpkHutkb/downloaded_packagesâ€™

```

This issues is, apprently, related to #241 (also #284). I believe, the root cause of #241 is that (according to docs) when `pb_update <= thin`, `pb_update` is selected to be `thin + 1`. This is extremely poor decision, because, as human beings, users tend to select `thin` divisable by e.g. 5, 10, therefore `pb_udpate + 1` can be a prime number, and not a divisor of `n_sample` or `warmup` in the `mcmc` method. 

Now, different values of `pb_update` cause `greta` to fail at the end of sampling with some `python` TensorArray-zero-size error. I ran some tests for different combinations of `pb_update` and `thin` to check, what values are causing those crashes.

The best thing is that `mcmc(verbose = FALSE)` appears to solve the problem, so `greta` crashes are actually caused by `UI` behaviour. I was unable to locate the source of the problem, but I would say it happens at the last sampling step (maybe, [here](https://github.com/greta-dev/greta/blob/master/R/inference_class.R#L512-L534)). Changes, introduced in #284, might be causing the problem.

In the following table, look at #&#x2060;4 and #&#x2060;7, which update by `10x + 1`. This appears to *almost always* crash, and it is exactly what happens if `pb_update <= thin`.
``` r
reticulate::use_condaenv("r-tensorflow-gpu", required = TRUE)
library(greta, warn.conflicts = FALSE)
library(purrr, warn.conflicts = FALSE)
library(dplyr, warn.conflicts = FALSE)
library(vctrs, warn.conflicts = FALSE)

run_model <- function(n_warm, n_spin, n_thin, n_pb_update) {
    x <- normal(0, 1)
    model <- model(x)
    result <-
        suppressWarnings(
            suppressMessages(
                mcmc(model, n_samples = n_spin, warmup = n_warm,
                    thin = n_thin, pb_update = n_pb_update,
                    chains = 4, n_cores = 8)))

    summary(result)$statistics
}

wrap_call <- function(n_warm, n_spin, n_thin, n_pb_update) {
    pmap(list(n_warm, n_spin, n_thin, n_pb_update),
          function(n_w, n_s, n_t, n_pb) {
              result <- tryCatch(run_model(n_w, n_s, n_t, n_pb),
                                 error = function(e) return(e))
              if (inherits(result, "error"))
                  return(list(Warmup = n_w, Spin = n_s, Thin = n_t, Pb = n_pb,
                              Mean = NA_real_, SD = NA_real_, Status = FALSE,
                              Message = result$message))
              else
                  return(list(Warmup = n_w, Spin = n_s, Thin = n_t, Pb = n_pb,
                              Mean = result["Mean"], SD = result["SD"], Status = TRUE,
                              Message = "OK"))
        }) %>% bind_rows
}

wrap_call(
        n_warm = 1000L,
        n_spin = 1000L,
        n_thin = vec_c(1L, 25L, 30L, 50L, 10L, 10L, 10L, 10L),
        n_pb_update = vec_c(50L, 50L, 50L, 50L, 20L, 23L, 31L, 99L)
    ) ->> temp

print(temp)
#> # A tibble: 8 x 8
#>   Warmup  Spin  Thin    Pb     Mean     SD Status Message                  
#>    <int> <int> <int> <int>    <dbl>  <dbl> <lgl>  <chr>                    
#> 1   1000  1000     1    50  0.00881  1.00  TRUE   OK                       
#> 2   1000  1000    25    50 -0.122    1.11  TRUE   OK                       
#> 3   1000  1000    30    50 -0.245    1.05  TRUE   OK                       
#> 4   1000  1000    50    50 NA       NA     FALSE  "greta hit a tensorflow ~
#> 5   1000  1000    10    20  0.0183   0.987 TRUE   OK                       
#> 6   1000  1000    10    23 -0.0226   0.971 TRUE   OK                       
#> 7   1000  1000    10    31 NA       NA     FALSE  "greta hit a tensorflow ~
#> 8   1000  1000    10    99  0.0791   0.942 TRUE   OK
```
``` r
sessionInfo()
#> R version 3.6.1 (2019-07-05)
#> Platform: x86_64-w64-mingw32/x64 (64-bit)
#> Running under: Windows 10 x64 (build 18362)
#> 
#> Matrix products: default
#> 
#> locale:
#> [1] LC_COLLATE=English_United States.1252 
#> [2] LC_CTYPE=English_United States.1252   
#> [3] LC_MONETARY=English_United States.1252
#> [4] LC_NUMERIC=C                          
#> [5] LC_TIME=English_United States.1252    
#> .....
```
``` r
reticulate::py_config()
#> python:         C:\PROGRA~3\MINICO~1\envs\R-TENS~1\python.exe
#> libpython:      C:/PROGRA~3/MINICO~1/envs/R-TENS~1/python37.dll
#> pythonhome:     C:\PROGRA~3\MINICO~1\envs\R-TENS~1
#> version:        3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]
#> Architecture:   64bit
#> numpy:          C:\PROGRA~3\MINICO~1\envs\R-TENS~1\lib\site-packages\numpy
#> numpy_version:  1.17.0
#> 
#> python versions found: 
#>  C:\PROGRA~3\MINICO~1\envs\R-TENS~1\python.exe
#>  C:\PROGRA~3\MINICO~1\python.exe
#>  C:\ProgramData\Miniconda3\python.exe
#>  C:\ProgramData\Miniconda3\envs\r-tensorflow-gpu\python.exe
```
``` r
reticulate::use_condaenv("r-tensorflow-gpu", required = TRUE)
tensorflow::tf_config()
#> TensorFlow v1.14.0 (C:\PROGRA~3\MINICO~1\envs\R-TENS~1\lib\site-packages\tensorflow\__init__.p)
#> Python v3.7 (C:\ProgramData\Miniconda3\envs\r-tensorflow-gpu\python.exe)
tensorflow::tf_gpu_configured()
#> TensorFlow built with CUDA:  TRUE 
#> GPU device name:  /device:GPU:0
#> [1] TRUE
```
``` r
packageDescription("greta")
#> Package: greta
#> Type: Package
#> Title: Simple and Scalable Statistical Modelling in R
#> Version: 0.3.1.9004
#> Date: 2019-08-25
#> ... 
#> Encoding: UTF-8
#> LazyData: true
#> Depends: R (>= 3.0)
#> Collate: 'package.R' 'utils.R' 'tf_functions.R' 'overloaded.R'
#> .....
#> Imports: R6, tensorflow (>= 1.13.0), reticulate, progress (>=
#>        1.2.0), future, coda, methods
#> Suggests: knitr, rmarkdown, DiagrammeR, bayesplot, lattice,
#>        testthat, mvtnorm, MCMCpack, rmutil, extraDistr, truncdist,
#>        tidyverse, fields, MASS, abind, spelling
#> VignetteBuilder: knitr
#> RoxygenNote: 6.1.1
#> Language: en-GB
#> RemoteType: github
#> RemoteHost: api.github.com
#> RemoteRepo: greta
#> RemoteUsername: greta-dev
#> RemoteRef: master
#> RemoteSha: 4d08524a7e13bc759a1c66fd1657455803c2ebf5
#> GithubRepo: greta
#> GithubUsername: greta-dev
#> GithubRef: master
#> GithubSHA1: 4d08524a7e13bc759a1c66fd1657455803c2ebf5
#> NeedsCompilation: no
#> .....
#> Built: R 3.6.1; ; 2019-09-06 12:11:50 UTC; windows

```

<sup>Created on 2019-09-26 by the [reprex package](https://reprex.tidyverse.org) (v0.3.0)</sup>

```
greta hit a tensorflow error:

Error in py_call_impl(callable, dots$args, dots$keywords): UnimplementedError: 2 root error(s) found.
  (0) Unimplemented: TensorArray has size zero, but element shape [?] is not fully defined. Currently only static shapes are supported when packing zero-size TensorArrays.
	 [[node mcmc_sample_chain/trace_scan/TensorArrayStack_2/TensorArrayGatherV3 (defined at \tensorflow_probability\python\mcmc\internal\util.py:372) ]]
	 [[mcmc_sample_chain/trace_scan/while/smart_for_loop/while/Exit_5/_75]]
  (1) Unimplemented: TensorArray has size zero, but element shape [?] is not fully defined. Currently only static shapes are supported when packing zero-size TensorArrays.
	 [[node mcmc_sample_chain/trace_scan/TensorArrayStack_2/TensorArrayGatherV3 (defined at \tensorflow_probability\python\mcmc\internal\util.py:372) ]]
0 successful operations.
0 derived errors ignored.

Errors may have originated from an input operation.
Input Source operations connected to node mcmc_sample_chain/trace_scan/TensorArrayStack_2/TensorArrayGatherV3:
 mcmc_sample_chain/trace_scan/while/Exit_13 (defined at \tensorflow_probability\python\mcmc\internal\util.py:370)	
 mcmc_sample_chain/trace_scan/TensorArray_3 (defined at \tensorflow_probability\python\mcmc\internal\util.py:355)

Input Source operations connected to node mcmc_sample_chain/trace_scan/TensorArrayStack_2/TensorArrayGatherV3:
 mcmc_sample_chain/trace_scan/while/Exit_13 (defined at \tensorflow_probability\python\mcmc\internal\util.py:370)	
 mcmc_sample_chain/trace_scan/TensorArray_3 (defined at \tensorflow_probability\python\mcmc\internal\util.py:355)

Original stack trace for 'mcmc_sample_chain/trace_scan/TensorArrayStack_2/TensorArrayGatherV3':
  File "\tensorflow_probability\python\mcmc\sample.py", line 361, in sample_chain
    parallel_iterations=parallel_iterations)
  File "\tensorflow_probability\python\mcmc\internal\util.py", line 372, in trace_scan
    stacked_trace = tf.nest.map_structure(lambda x: x.stack(), trace_arrays)
  File "\tensorflow\python\util\nest.py", line 515, in map_structure
    structure[0], [func(*x) for x in entries],
  File "\tensorflow\python\util\nest.py", line 515, in <listcomp>
    structure[0], [func(*x) for x in entries],
  File "\tensorflow_probability\python\mcmc\internal\util.py", line 372, in <lambda>
    stacked_trace = tf.nest.map_structure(lambda x: x.stack(), trace_arrays)
  File "\tensorflow\python\ops\tensor_array_ops.py", line 1205, in stack
    return self._implementation.stack(name=name)
  File "\tensorflow\python\ops\tensor_array_ops.py", line 309, in stack
    return self.gather(math_ops.range(0, self.size()), name=name)
  File "\tensorflow\python\ops\tensor_array_ops.py", line 323, in gather
    element_shape=element_shape)
  File "\tensorflow\python\ops\gen_data_flow_ops.py", line 7167, in tensor_array_gather_v3
    element_shape=element_shape, name=name)
  File "\tensorflow\python\framework\op_def_library.py", line 788, in _apply_op_helper
    op_def=op_def)
  File "\tensorflow\python\util\deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "\tensorflow\python\framework\ops.py", line 3616, in create_op
    op_def=op_def)
  File "\tensorflow\python\framework\ops.py", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()


Detailed traceback: 
  File "C:\PROGRA~3\MINICO~1\envs\R-TENS~1\lib\site-packages\tensorflow\python\client\session.py", line 950, in run
    run_metadata_ptr)
  File "C:\PROGRA~3\MINICO~1\envs\R-TENS~1\lib\site-packages\tensorflow\python\client\session.py", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File "C:\PROGRA~3\MINICO~1\envs\R-TENS~1\lib\site-packages\tensorflow\python\client\session.py", line 1350, in _do_run
    run_metadata)
  File "C:\PROGRA~3\MINICO~1\envs\R-TENS~1\lib\site-packages\tensorflow\python\client\session.py", line 1370, in _do_call
    raise type(e)(node_def, op, message)
```








Tensorflow probability has *just* open sourced it's unrolled* NUTS implementation [here](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/mcmc/nuts.py)

Personally, I think having a NUTS implementation would be a **big** thing for greta. 

*I assume this means that it very parallel(y)

Hi Nick, 

This (single commit)^1 pull request outlines my idea for a sampling interface for distributions in greta. 

I worked a bit a while ago on a substantive pull request which added sampling to almost all distributions. Writing the code to sample the distributions was pretty easy but I was very unsure of the best (simplest, most consistent with other greta internals etc.) interface. Some reasons I was confused about this were:

- Confusion about the arguments to `tf_distrib` which I thought `tf_sample` should try to emulate
  - Why are the parameters passed, not accessed using `self$`? (I instinctively feel that `tf_sample` should have no arguments.)
  - What form are the parameters meant to take?
  - Why is `dag` passed but not used? 

- I wasn't sure what sort of internal juggling of tensorflow tenors would be required to sample a full model and how this might effect the necessary interface

The commit outlines my best idea but I am happy to receive any other ideas. 

Once the interface is decided on I would be very happy to actually go through and implement the sampling for each distribution. 

Jeffrey

 1. Or at least it would be if I knew how to edit commit messages properly 





greta's extraction (e.g. `a <- x[2]`) and replacement (e.g. `x[2] <- 0`) syntax uses the internal tensorflow functions `tf_extract()` and  `tf_replace()` to do the operations on tensors, with a shim to map R's extract/replace syntax onto TensorFlow's. This shim doesn't always use the most efficient operations for common extraction methods.

For example:
`x[2, ]` could use `tf$slice()`, which might be more efficient than the current general approach or reshaping to a vector, using `tf$gather()` and then reshaping to a matrix.

`x[2, ] <- 0` could use `tf$tensor_scatter_nd_update()`, which would probably be much more efficient than the current (`greta:::tf_recombine()`) approach of flattening the vector, breaking it up into vectors, replacing some, and then recombining them with `tf$concat()` before reshaping the vector into a matrix ðŸ˜“.

This would particularly help when people write for loops to that alter elements in matrices (e.g. for timeseries models), and reduce the need for nasty hacks like storing the iterated components in lists.

This issue is about planning ahead to dealing with major TensorFlow API changes when a greta version has to work with TensorFlow 2.0. It doesn't affect anything yet.

### optimisers
Tensorflow's various optimiser interfaces are being unified ([details here](https://github.com/tensorflow/community/blob/master/rfcs/20181016-optimizer-unification.md)). Some of the old versions will still be available in the `compat` compatibility module (though there may be speed benefit to changing to more recent versions). 
In addition, the TF 1.x `contrib` module will no longer be available in TF 2.0, and won't be in `compat`.

There is a tensorflow addons repo with [an 'optimizers' module](https://github.com/tensorflow/addons/tree/master/tensorflow_addons/optimizers) (though with no apparent overlap with existing optimisers) and a [TFP 'optimizer' module](https://www.tensorflow.org/probability/api_docs/python/tfp/optimizer) which has a couple of the methods.

| greta optimiser  | TF 1.x function | TF2.0 function |
| - | - | - |
| `nelder_mead`  | `tf$contrib$opt$ScipyOptimizerInterface` | `tfp$optimizer$nelder_mead_minimize` |
| `powell`  | `tf$contrib$opt$ScipyOptimizerInterface` | ? |
| `cg`  | `tf$contrib$opt$ScipyOptimizerInterface` | ? |
| `bfgs`  | `tf$contrib$opt$ScipyOptimizerInterface` | `tfp$optimizer$bfgs_minimize` |
| `newton_cg`  | `tf$contrib$opt$ScipyOptimizerInterface` | ? |
| `l_bfgs_b`  | `tf$contrib$opt$ScipyOptimizerInterface` | ? |
| `tnc`  | `tf$contrib$opt$ScipyOptimizerInterface` | ? |
| `cobyla`  | `tf$contrib$opt$ScipyOptimizerInterface` | ? |
| `slsqp`  | `tf$contrib$opt$ScipyOptimizerInterface` | ? |
| `gradient_descent`  | `tf$compat$v1$train$GradientDescentOptimizer` | `tf$keras$optimizers$SGD` |
| `adadelta`  | `tf$compat$v1$train$AdadeltaOptimizer` | `tf$keras$optimizers$Adadelta` |
| `adagrad`  | `tf$compat$v1$train$AdagradOptimizer` | `tf$keras$optimizers$Adagrad` |
| `adagrad_da`  | `tf$compat$v1$train$AdagradDAOptimizer` | `tf$compat$v1$train$AdagradDAOptimizer` |
| `momentum`  | `tf$compat$v1$train$MomentumOptimizer` | `tf$keras$optimizers$SGD` |
| `adam`  | `tf$compat$v1$train$AdamOptimizer` | `tf$keras$optimizers$Adam` |
| `ftrl`  | `tf$compat$v1$train$FtrlOptimizer` | `tf$keras$optimizers$FTRL` |
| `proximal_gradient_descent` | `tf$compat$v1$train$ProximalGradientDescentOptimizer` | `tf$compat$v1$train$ProximalGradientDescentOptimizer` |
| `proximal_adagrad`  | `tf$compat$v1$train$ProximalAdagradOptimizer` | `tf$compat$v1$train$ProximalAdagradOptimizer` |
| `rms_prop`  | `tf$compat$v1$train$RMSPropOptimizer` | `tf$keras$optimizers$RMSProp` (though arguments may have changed, so `tf$compat$v1$train$RMSPropOptimizer` as a backup) |

Unless another interface to the Scipy optimisers is developed, the optimisers without replacements will have to be removed from the API. It'll bee a little hard to deprecate them without knowing whether a replacement will be coming.

### ode solvers

The `greta.dynamics` package's `ode_solve()` functionality also depends on `contrib`, using the `tf$contrib$integrate$odeint` and `odeint_fixed` methods. `tfp$math$ode$Solver$solve` may be a viable replacement.