
难道是预测稠密特征的label吗？

感谢解答！
我设置参数时，没有sparse_feature_idx和use_hash_embedding两个参数，
模型实例化时也只传入了feature_idx和feature_dim两个特征相关的参数，请问
现在不支持这两个吗，我想用的话可以在Euler上集成吗
我发现用graphsage use_id后，会增加一个embedding layer. embedding 参数特别大。而且在训练的时候，没有用paritionedVariable造成PS特别慢。想了解一下为何这样？（我打算自己加上partitioned varaiable)
运行的是示例脚本run_dist_euler.sh
节点0日志：
![image](https://user-images.githubusercontent.com/11912425/71791125-bfc33a80-306e-11ea-85b9-f61bcfbb7b3e.png)
节点1日志：
![image](https://user-images.githubusercontent.com/11912425/71791191-fef18b80-306e-11ea-9388-75fac28dd4d5.png)

二者无限输出1 workers have finished。。。

请教下是什么原因导致的呢？SyncExitHook方法中_num_finished_workers不同worker之间是共享的吗？感觉是这个出的问题。谢谢指导！
请教下zookeeper只需要在master上安装就行吗? 我只在hadoop-master节点上装了zookeeper，测试连接正常，用zookeeper自带客户端（zkCli.sh -server localhost:2181）能连接成功并运行create命令。但是运行euler训练的时候报错了，不知道是什么原因，euler_zk_path尝试过zookeeper里头配置的/tmp/zookeeper以及任意的/path/euler的不行，一直报：ZK error when creating meta: no node。麻烦有空指导一下，谢谢。

运行指令如下：
./dist_tf_euler.sh \
  --data_dir hdfs://hadoop-master:9000/user/root/ppi \
  --euler_zk_addr hadoop-master:2181 --euler_zk_path /path/euler \
  --max_id 56944 --feature_idx 1 --feature_dim 50 --label_idx 0 --label_dim 121 \
  --model graphsage_supervised --mode train

错误信息work0(work1也一样)：
I0104 13:37:16.686715  9816 graph_builder.cc:81] Load Done: hdfs://hadoop-master:9000/user/root/ppi/ppi_data_0.dat
I0104 13:37:16.686715  9818 graph_builder.cc:81] Load Done: hdfs://hadoop-master:9000/user/root/ppi/ppi_data_4.dat
I0104 13:37:16.700754  9817 graph_builder.cc:81] Load Done: hdfs://hadoop-master:9000/user/root/ppi/ppi_data_2.dat
I0104 13:37:16.701020  9656 graph_builder.cc:127] Each Thread Load Finish! Node Count:34166 Edge Count:963932
I0104 13:37:16.701040  9656 graph_builder.cc:135] Graph Loading Finish!
I0104 13:37:16.982077  9656 graph_builder.cc:147] Graph Load Finish! Node Count:34166 Edge Count:963932
I0104 13:37:16.988603  9656 graph_builder.cc:152] Done: build node sampler
I0104 13:37:16.988626  9656 graph_builder.cc:162] Graph build finish
I0104 13:37:16.988660  9656 graph_service.cc:179] service init finish
I0104 13:37:16.989260  9656 graph_service.cc:131] bound port: 172.20.0.2:46415
E0104 13:37:16.993388  9656 zk_server_register.cc:77] ZK error when creating root node: no node.
W0104 13:37:16.993427  9656 graph.h:198] global sampler is not ok
E0104 13:37:16.994678  9656 zk_server_register.cc:99] ZK error when creating meta: no node.
I0104 13:37:16.994695  9656 graph_service.cc:146] service start。

hdfs正常，分片也正常。

不好意思，我发现是我没有手动建立这个node。现在运行正常。
在试验分布式node2vec的过程中，如下图所示显示Unable to load ZIP library: /usr/libzip.so。看了下这个文件位于在JAVA_HOME子路径下，不在/usr目录下。但是不知道为什么程序会来查找这个目录，是环境变量配置的问题么。查看了JAVA_HOME=/usr/local/jdk1.8.0_131 以及LD_LIBRARY_PATH=/usr/local/jdk1.8.0_131/jre/lib/amd64:/usr/lib64均已设置。物理机环境为redhat，hadoop版本为cdh版本。还有之前编译的时候需要提供libhdfs.so和libjvm.so文件，提示要在/usr/lib64目录下，这个目录又是根据什么决定的？
谢谢~


![图片1](https://user-images.githubusercontent.com/26162190/71508435-ba256200-28c2-11ea-8b73-a2decf7790d2.png)

我GPU的显存不够，有没有办法，整个embedding在cpu上，每个batch对应的embedding送到GPU上计算？
请问：
单机版，用deepwalk训练，loss和mrr值一直不怎么变是怎么回事啊？
一样的代码，我用python3写了下，效果好很多。
mrr这个评估值，是有效的吗？
在使用graphsage模型的时候，把use_id改成True，不使用任何特征，发现结果基本上是随机的，并且save_embedding之后，发现embedding矩阵没有进行归一化