- Add coauthor, instiution, landing page
- Make titles point of slide (action titles)
- Slide 5: Make title "RF is best classifier"
- Slide 6: What's rotating? More dramatic figure
- Slide 7: Define oblique or use different word or illustrate oblique
- Slide 8: Separate into two slides to empahasize Frank. Make table with columns "Oblique/Axis-aligned", "Sparse/Dense", and "Robust". Show prior art here. Show full table at end.
- Slide 10: Remove words and just illustrate synthetic datasets. Remove mention of benchmark data. Illustrate sparse parity with XOR plot
- Slide 12: SHOW splits in two dimensions for RF and F-RC. Sentence for each why it succeeds and why it fails. Explain what we want (what a good split is)
- Slide 13: Separate classification performance and training time into two slides
- Slide 14: Too much. Choose one transformation. Make Frank black and thick. Others grey and thin
- Slide 15: One colorbar with no numbers. Only three x-axis ticks. Only choose Raw, Affine, and Corrupted?
- Slide 17: Next Steps. Mention RerF?
- Slide 18: Acknowledgements
Suppose the theoretically optimal split direction at an arbitrary node is defined by a vector v*. Assume p is large, and v* has a sparsity of s. For RerF and RR-RF, let the selected split direction be \hat{v}_{rerf} and \hat{v}_{rr-rf}, respectively. Compute the probability that the angle between \hat{v}_{rerf} and v* and \hat{v}_{rr-rf} and v* is less than \delta. 

My intuition says that when sparsity of v* is very close to one, RerF has a reasonable probability of finding a split direction close to the optimal, while RR-RF does not. When sparsity of v* is close to zero, RerF has no chance of getting close because of the sparsity constraint on the split directions, and RR-RF has a finite but very small chance of getting close because it's high dimensional. Even if RR-RF does get close to the right split direction, estimation of the location of the split will be poor if p >> n. Therefore, when p >> n, RerF will do at least as well, and sometimes substantially better than RR-RF because of the "bet on sparsity" principle.
The option/extent of subsampling at large nodes could potentially substantially reduce the amount of sorting while not hurting classification performance.
I'd like to use randomer forests on windows, but for the necessary mex files, such as classregtreeRCcritval, are only provided in Mac and Linux (mexmaci64 and mexa64 format). 

It's understandable that you may not want to share your source code for someone to compile the mexw64 files themselves, but is there any way you could compile the necessary routines for Windows?

Appreciate your code!

if i recall, RerF is 10% better than RF on about 10% of the data?

let's compute, for each dataset:

1. n
2. p
3. p/n
4. sum of singular values
5. sum of squared singular values

let's make a pairs-plot, 5 x 5 panels, color code by much better than RF (eg >7% or so), and not.
and see if we can see anything?

in the PAMI paper, we really should try to answer the questions:

1) which features/subspaces were informative
2) why does RerF > RF (in terms of bias and variance)
3) what properties of data do we expect RerF > RF
in biau08a, they point out in Fig 1, that certain greedily grown trees fail to converge here, 
but breiman's would converge. 

can we think of an example where brieman would NOT be consistent.
their proof suggests such distributions exist.
we should think of one, and show that RerF converges even though RF does not
(at least empirically)
another variant we could explore in this manuscript, in particular, using

Random Features for Large-Scale Kernel Machines

to generate our random A matrix

either pre or post submission, but lots of people ask about it immediately after asking about performance.

i think we could simply count the number of times a feature is used, and/or the number of times a subspace is used.