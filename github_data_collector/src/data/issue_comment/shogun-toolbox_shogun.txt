This PR resolves https://github.com/shogun-toolbox/shogun/issues/4797. Currently I have added the forward pass computation of the Mish Function. 
Fix #4795 
If I go to https://www.shogun-toolbox.org/showroom I get "Internal Server Error".
If I go to https://cloud.shogun.ml/ I get "This site can’t be reached"

Sounds like something exciting should be there. Would be nice to make those links accessible again.
Fix issue #4639 
Building shogun from source (development branch). 
First the following command is passed to cmake:

`cmake -DINTERFACE_PYTHON=ON -DBUILD_META_EXAMPLES=ON -DCMAKE_INSTALL_PREFIX=/home/zeus/shogun_stuff ..`

Then, on running `make`

`[ 37%] Building CXX object src/shogun/CMakeFiles/libshogun.dir/lib/tapkee/tapkee_shogun.cpp.o
In file included from /home/zeus/shogun/src/shogun/lib/tapkee/methods.hpp:17,
                 from /home/zeus/shogun/src/shogun/lib/tapkee/embed.hpp:11,
                 from /home/zeus/shogun/src/shogun/lib/tapkee/tapkee.hpp:10,
                 from /home/zeus/shogun/src/shogun/lib/tapkee/tapkee_shogun.cpp:20:
/home/zeus/shogun/src/shogun/lib/tapkee/routines/locally_linear.hpp: In function ‘tapkee::SparseWeightMatrix tapkee::tapkee_internal::linear_weight_matrix(const RandomAccessIterator&, const RandomAccessIterator&, const Neighbors&, PairwiseCallback, tapkee::ScalarType, tapkee::ScalarType) [with RandomAccessIterator = __gnu_cxx::__normal_iterator<int*, std::vector<int> >; PairwiseCallback = pimpl_kernel_callback<shogun::Kernel>]’:
/home/zeus/shogun/src/shogun/lib/tapkee/routines/locally_linear.hpp:108:15: error: ‘k’ not specified in enclosing ‘parallel’
  108 |   DenseMatrix gram_matrix = DenseMatrix::Zero(k,k);
      |               ^~~~~~~~~~~
/home/zeus/shogun/src/shogun/lib/tapkee/routines/locally_linear.hpp:105:9: error: enclosing ‘parallel’
  105 | #pragma omp parallel shared(begin,end,neighbors,callback,sparse_triplets) default(none)
      |         ^~~
/home/zeus/shogun/src/shogun/lib/tapkee/routines/locally_linear.hpp:133:38: error: ‘trace_shift’ not specified in enclosing ‘parallel’
  133 |    gram_matrix.diagonal().array() += trace_shift*trace;
      |                                      ^~~~~~~~~~~
/home/zeus/shogun/src/shogun/lib/tapkee/routines/locally_linear.hpp:105:9: error: enclosing ‘parallel’
  105 | #pragma omp parallel shared(begin,end,neighbors,callback,sparse_triplets) default(none)
      |         ^~~
/home/zeus/shogun/src/shogun/lib/tapkee/routines/locally_linear.hpp:137:61: error: ‘shift’ not specified in enclosing ‘parallel’
  137 |    SparseTriplet diagonal_triplet(index_iter,index_iter,1.0+shift);
      |                                                             ^~~~~
`
and a bunch of other errors associated with types and variables.
Final lines read:
`make[2]: *** [src/shogun/CMakeFiles/libshogun.dir/build.make:5241: src/shogun/CMakeFiles/libshogun.dir/lib/tapkee/tapkee_shogun.cpp.o] Error 1
make[1]: *** [CMakeFiles/Makefile2:648: src/shogun/CMakeFiles/libshogun.dir/all] Error 2
make: *** [Makefile:152: all] Error 2`

Perhaps an issue with dependencies?
win 10 , i add classpath and can't run run_test.py
Mish is a new novel activation function proposed in this [paper](https://arxiv.org/abs/1908.08681).
It has shown promising results so far and has been adopted in several packages including:

- [TensorFlow-Addons](https://github.com/tensorflow/addons/tree/master/tensorflow_addons/activations)
- [SpaCy (Tok2Vec Layer)](https://github.com/explosion/spaCy)
- [Thinc - SpaCy's official NLP based ML library](https://github.com/explosion/thinc/releases/tag/v7.3.0)
- [Echo AI](https://github.com/digantamisra98/Echo)
- [Eclipse's deeplearning4j](https://github.com/eclipse/deeplearning4j/issues/8417)
- [CNTKX - Extension of Microsoft's CNTK](https://github.com/delzac/cntkx)
- [FastAI-Dev](https://github.com/fastai/fastai_dev/blob/0f613ba3205990c83de9dba0c8798a9eec5452ce/dev/local/layers.py#L441)
- [Darknet](https://github.com/AlexeyAB/darknet/commit/bf8ea4183dc265ac17f7c9d939dc815269f0a213)
- [Yolov3](https://github.com/ultralytics/yolov3/commit/444a9f7099d4ff1aef12783704e3df9a8c3aa4b3)
- [BeeDNN - Library in C++](https://github.com/edeforas/BeeDNN)
- [Gen-EfficientNet-PyTorch](https://github.com/rwightman/gen-efficientnet-pytorch)
- [dnet](https://github.com/umangjpatel/dnet/blob/master/nn/activations.py)

All benchmarks, analysis and links to official package implementations can be found in this [repository](https://github.com/digantamisra98/Mish) 

It would be nice to have Mish as an option within the activation function group.

This is the comparison of Mish with other conventional activation functions in a SEResNet-50 for CIFAR-10:  (Better accuracy and faster than GELU)
![se50_1](https://user-images.githubusercontent.com/34192716/69002745-0de37980-091b-11ea-87da-ac8d17c79e07.png)
While training a random forest, I generate a DenseSubSamplesFeatures<double> from a DenseFeatures<double>.
Compilation works.
At runtime, I get:
```
RuntimeError: Object of type DenseSubSamplesFeatures cannot be converted to type shogun::DenseFeatures<double>.
```

I'm working on commit 84175ba7c8c73dd4028f217daf7a5f4c25c2adaf.
I compile with g++ 9.1.0 on Arch Linux.

Here's the code that generates the DenseSubSamplesFeatures
https://github.com/lejeunel/glia/blob/itk-5.0.1/src/shogun_helpers.hxx#L83

