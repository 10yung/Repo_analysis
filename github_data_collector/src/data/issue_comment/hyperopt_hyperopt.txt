Hey,

I wrote an objective function for a SARIMAX model, because I want to find the model with the best hyperparameters and the best external regressors in one combined step. The objective function takes several integer variables and one categorical variable as an input and returns the loss of a given configuration.

```
# categorical variable (in reality much more regressor combinations)
regressor_combinations = [["regressor1"], ["regressor2"], ["regressor1", "regressor2"]]

```
```
# search space:
search_space_sarimax = {
                        'p': hp.randint('p', 4),
                        'd': hp.randint('d', 2),
                        'q': hp.randint('q', 4),
                        'P': hp.randint('P', 2),
                        'D': hp.randint('D', 2),
                        'Q': hp.randint('Q', 2),
                        's': hp.choice('s', [3, 12]),
                        'regressors' : hp.choice('regressors', regressor_combinations)
                        }  

```
```
# objective function
def objective(search_space_sarimax):
        
        p = search_space_sarimax['p']
        d = search_space_sarimax['d']
        q = search_space_sarimax['q']
        P = search_space_sarimax['P']
        D = search_space_sarimax['D']
        Q = search_space_sarimax['Q']
        s = search_space_sarimax['s']
       regressors = search_space_sarimax['regressors']
      
       # ...create model with hyperparameters and regressors and return loss...
```

If I run hyperopt, I don't receive the names of the regressors that have been chosen but a number:

```
Out[3]: 
{'D': 1,
 'P': 0,
 'Q': 0,
 'd': 1,
 'regressors': 2,
 'p': 2,
 'q': 1,
 's': 0}
```

Is that a bug or am I missing something crucial here? In the obective function, the SARIMAX model takes a list of regressors as a model input, so this shouldn't be the problem.

Thanks in advance!
I've tried some combinations of the algorithms using `mix`, but some resulted in error.
For instance:
* rand + atpe => Pass
* rand + tpe + anneal => Pass
* rand + atpe + anneal => Error
* rand + atpe + tpe => Error

Is this expected? I haven't found documentation about this.
The error is the following:
```
Running  MIX ...                                                        
Traceback (most recent call last):
  File "./example.py", line 45, in <module>                              
    show_progressbar=False)
  File "/home/Documents/pyvirtenvs/envpy3.5/lib/python3.5/site-packages/hyperopt/fmin.py", line 422, in fmin
    rval.exhaust()                                           
  File "/home/Documents/pyvirtenvs/envpy3.5/lib/python3.5/site-packages/hyperopt/fmin.py", line 276, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.asynchronous)
  File "/home/Documents/pyvirtenvs/envpy3.5/lib/python3.5/site-packages/hyperopt/fmin.py", line 222, in run
    self.rstate.randint(2 ** 31 - 1))
  File "/home/Documents/pyvirtenvs/envpy3.5/lib/python3.5/site-packages/hyperopt/mix.py", line 36, in suggest
    seed=int(rng.randint(2 ** 31)))                             
  File "/home/Documents/pyvirtenvs/envpy3.5/lib/python3.5/site-packages/hyperopt/tpe.py", line 892, in suggest
    [d['misc'] for d in docs], keys=list(domain.params.keys()))
  File "/home/Documents/pyvirtenvs/envpy3.5/lib/python3.5/site-packages/hyperopt/base.py", line 210, in miscs_to_idxs_vals
    assert t_idxs == [] or t_idxs == [misc['tid']]
AssertionError

```

Running Hyperopt 0.2.2 and Python 3.5. Here is the code:

```python
from hyperopt import hp, fmin, mix, rand, anneal, atpe, tpe, space_eval
from functools import partial

# define an objective function
def objective(args):
    case, val = args

    #print("obj", case, val)

    if case == 'case 1':
        return val
    else:
        return val ** 2

# define a search space
space = hp.choice('a',
    [
        ('case 1', 1 + hp.lognormal('c1', 0, 1)),
        ('case 2', hp.uniform('c2', -10, 10))
    ])

# minimize the objective over the space
mixsug=partial(mix.suggest,
            p_suggest=[
                (.1, rand.suggest),
                (.2, anneal.suggest),
                (.4, tpe.suggest),
                (.3, atpe.suggest),],
            )

for desc, algo in ('MIX',mixsug),:
    print ("Running ",desc,"...")
    best = fmin(objective, space, algo=algo, max_evals=100,
            show_progressbar=False)

    print(best)
    # -> {'a': 1, 'c2': 0.01420615366247227}
    print(space_eval(space, best))
    # -> ('case 2', 0.01420615366247227}
```

Support dynamic allocation in hyperopt integration. Only throw a warning when current task slots is smaller than requested parallelism.

When user specify parallelism to be greater than `max_num_concurrent_tasks`, print a warning notify user that this will trigger spark executor dynamic allocation (if spark configured with dynamic allocation mode).

When user do not specify parallelism, it will be set to `max(spark_default_parallelism, max_num_concurrent_tasks)` as default behavior, and print a deprecation warning, next released version will remove this default behavior.
The MongoClient call does not forward kwargs:

https://github.com/hyperopt/hyperopt/blob/c903de979e50f844130dcc08107038f95e87a061/hyperopt/mongoexp.py#L277

Might be ideal to just forward the uri string directly rather than trying to parse it and pass it piece by piece. 
@marctorrellas reported some flakiness with Spark tests.  Looking at recent master tests, I see 2 main issues:

(1) The most common failure is:
```
======================================================================
FAIL: test_task_maxFailures_warning (hyperopt.tests.test_spark.FMinTestCase)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/travis/build/hyperopt/hyperopt/hyperopt/tests/test_spark.py", line 657, in test_task_maxFailures_warning
    log_output=log_output
AssertionError: 'spark.task.maxFailures' not found in "trial task 0 started\ntrial task 0 succeeded, result is {'status': 'ok', 'loss': 1.0}\ntrial 0 task thread exits normally and writes results back correctly.\nfmin thread exits normally.\ndispatcher thread exits normally.\nTotal Trials: 1: 1 succeeded, 0 failed, 0 cancelled." :  "spark.task.maxFailures" warning missing from log: 
                    trial task 0 started
trial task 0 succeeded, result is {'status': 'ok', 'loss': 1.0}
trial 0 task thread exits normally and writes results back correctly.
fmin thread exits normally.
dispatcher thread exits normally.
```
E.g.: https://travis-ci.org/hyperopt/hyperopt/jobs/633229910

(2) I saw 1 instance of:
```
FAIL: test_timeout_without_job_cancellation_fmin_timeout (hyperopt.tests.test_spark.FMinTestCase)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/travis/build/hyperopt/hyperopt/hyperopt/tests/test_spark.py", line 479, in test_timeout_without_job_cancellation_fmin_timeout
    self.assertTrue(spark_trials._fmin_cancelled)
AssertionError: False is not true
```
E.g.: https://travis-ci.org/hyperopt/hyperopt/jobs/630275160

This issue is for fixing these instances of flakiness.  (Please add other Spark-related ones if you see any.)
Hello there,

Thanks for this awesome work! I have a quick question wants to clarify, inside TPE, we optimize the ratio of l(x)/g(x) and x is an architecture instead of maximizing EI. So, my question is what algorithm are you using in optimizing l(x)/g(x)? Evolutionary algorithm? Thank you.
I am trying to use Hyperopt to find the best learner for my dataset on Google Colab. The dataset contains both categorical and numerical values but all of them are encoded successfully. While searching for the optimal classifier, I also wanted to get cross-validation scores, which is built-in functionality in hyperopt. However, when I tried to run my code, I got the following error:

> AttributeError: 'NoneType' object has no attribute 'randint'

I could not solve the issue alone, it is related to an attribute defined in the randint fuction defined in stochastic.py. My code and the output are below:

`from sklearn.model_selection import KFold, cross_val_score
from hpsklearn import HyperoptEstimator, any_classifier
from sklearn.model_selection import train_test_split
from hyperopt import tpe, hp, fmin, Trials
import numpy as np
type(aust_predictors)
X_train, X_test, y_train, y_test = train_test_split(aust_predictors, df_aust.target, train_size = 0.75, test_size = 0.25)
y_train = y_train.to_numpy()
y_test = y_test.to_numpy()
estim = HyperoptEstimator(classifier=any_classifier('clf'), algo = tpe.suggest, trial_timeout=150)
estim.fit(X_train, y_train)

print(estim.score(X_test, y_test))
print(estim.best_model())

cv_sc = cross_val_score(estim, X_test, y_test, cv = 10, n_jobs=-1).mean()
print(cv_sc)`

Output:

`100%|██████████| 1/1 [00:00<00:00,  4.48it/s, best loss: 0.375]
100%|██████████| 1/1 [00:00<00:00, 11.45it/s, best loss: 0.375]
100%|██████████| 1/1 [00:00<00:00,  2.03it/s, best loss: 0.1923076923076923]
100%|██████████| 1/1 [00:00<00:00,  8.65it/s, best loss: 0.15384615384615385]
100%|██████████| 1/1 [00:00<00:00,  3.21it/s, best loss: 0.15384615384615385]
100%|██████████| 1/1 [00:00<00:00,  9.92it/s, best loss: 0.15384615384615385]
100%|██████████| 1/1 [00:00<00:00,  5.55it/s, best loss: 0.10576923076923073]
100%|██████████| 1/1 [00:04<00:00,  4.64s/it, best loss: 0.10576923076923073]
100%|██████████| 1/1 [00:00<00:00,  2.11it/s, best loss: 0.10576923076923073]
100%|██████████| 1/1 [00:00<00:00,  8.23it/s, best loss: 0.10576923076923073]
0.8728323699421965
{'learner': GradientBoostingClassifier(criterion='friedman_mse', init=None,
                           learning_rate=0.033985711476027795, loss='deviance',
                           max_depth=4, max_features='sqrt',
                           max_leaf_nodes=None, min_impurity_decrease=0.0,
                           min_impurity_split=None, min_samples_leaf=8,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=109, n_iter_no_change=None,
                           presort='auto', random_state=1,
                           subsample=0.7700933910547965, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), 'preprocs': (StandardScaler(copy=True, with_mean=False, with_std=False),), 'ex_preprocs': ()}
---------------------------------------------------------------------------
Empty                                     Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/joblib/parallel.py in dispatch_one_batch(self, iterator)
    796             try:
--> 797                 tasks = self._ready_batches.get(block=False)
    798             except queue.Empty:

16 frames
/usr/lib/python3.6/queue.py in get(self, block, timeout)
    160                 if not self._qsize():
--> 161                     raise Empty
    162             elif timeout is None:

Empty: 

During handling of the above exception, another exception occurred:

AttributeError                            Traceback (most recent call last)
<ipython-input-12-90d85434dc35> in <module>()
      5 print(estim.best_model())
      6 
----> 7 cv_sc = cross_val_score(estim, X_test, y_test, cv = 10, n_jobs=-1).mean()
      8 print(cv_sc)

/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py in cross_val_score(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)
    389                                 fit_params=fit_params,
    390                                 pre_dispatch=pre_dispatch,
--> 391                                 error_score=error_score)
    392     return cv_results['test_score']
    393 

/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py in cross_validate(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)
    230             return_times=True, return_estimator=return_estimator,
    231             error_score=error_score)
--> 232         for train, test in cv.split(X, y, groups))
    233 
    234     zipped_scores = list(zip(*scores))

/usr/local/lib/python3.6/dist-packages/joblib/parallel.py in __call__(self, iterable)
   1002             # remaining jobs.
   1003             self._iterating = False
-> 1004             if self.dispatch_one_batch(iterator):
   1005                 self._iterating = self._original_iterator is not None
   1006 

/usr/local/lib/python3.6/dist-packages/joblib/parallel.py in dispatch_one_batch(self, iterator)
    806                 big_batch_size = batch_size * n_jobs
    807 
--> 808                 islice = list(itertools.islice(iterator, big_batch_size))
    809                 if len(islice) == 0:
    810                     return False

/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py in <genexpr>(.0)
    230             return_times=True, return_estimator=return_estimator,
    231             error_score=error_score)
--> 232         for train, test in cv.split(X, y, groups))
    233 
    234     zipped_scores = list(zip(*scores))

/usr/local/lib/python3.6/dist-packages/sklearn/base.py in clone(estimator, safe)
     63     for name, param in new_object_params.items():
     64         new_object_params[name] = clone(param, safe=False)
---> 65     new_object = klass(**new_object_params)
     66     params_set = new_object.get_params(deep=False)
     67 

/usr/local/lib/python3.6/dist-packages/hpsklearn/estimator.py in __init__(self, preprocessing, ex_preprocs, classifier, regressor, space, algo, max_evals, loss_fn, verbose, trial_timeout, fit_increment, fit_increment_dump_filename, seed, use_partial_fit)
    534             # self.space = hyperopt.pyll.as_apply(space)
    535             self.space = space
--> 536             evaled_space = space.eval()
    537             if 'ex_preprocs' in evaled_space:
    538                 self.n_ex_pps = len(evaled_space['ex_preprocs'])

/usr/local/lib/python3.6/dist-packages/hyperopt/pyll/base.py in eval(self, memo)
    290         else:
    291             args = [a.eval() for a in self.pos_args]
--> 292             kwargs = dict([(n, a.eval()) for (n, a) in self.named_args])
    293             f = scope._impls[self.name]
    294             memo[id(self)] = rval = f(*args, **kwargs)

/usr/local/lib/python3.6/dist-packages/hyperopt/pyll/base.py in <listcomp>(.0)
    290         else:
    291             args = [a.eval() for a in self.pos_args]
--> 292             kwargs = dict([(n, a.eval()) for (n, a) in self.named_args])
    293             f = scope._impls[self.name]
    294             memo[id(self)] = rval = f(*args, **kwargs)

/usr/local/lib/python3.6/dist-packages/hyperopt/pyll/base.py in eval(self, memo)
    289             return memo[id(self)]
    290         else:
--> 291             args = [a.eval() for a in self.pos_args]
    292             kwargs = dict([(n, a.eval()) for (n, a) in self.named_args])
    293             f = scope._impls[self.name]

/usr/local/lib/python3.6/dist-packages/hyperopt/pyll/base.py in <listcomp>(.0)
    289             return memo[id(self)]
    290         else:
--> 291             args = [a.eval() for a in self.pos_args]
    292             kwargs = dict([(n, a.eval()) for (n, a) in self.named_args])
    293             f = scope._impls[self.name]

/usr/local/lib/python3.6/dist-packages/hyperopt/pyll/base.py in eval(self, memo)
    289             return memo[id(self)]
    290         else:
--> 291             args = [a.eval() for a in self.pos_args]
    292             kwargs = dict([(n, a.eval()) for (n, a) in self.named_args])
    293             f = scope._impls[self.name]

/usr/local/lib/python3.6/dist-packages/hyperopt/pyll/base.py in <listcomp>(.0)
    289             return memo[id(self)]
    290         else:
--> 291             args = [a.eval() for a in self.pos_args]
    292             kwargs = dict([(n, a.eval()) for (n, a) in self.named_args])
    293             f = scope._impls[self.name]

/usr/local/lib/python3.6/dist-packages/hyperopt/pyll/base.py in eval(self, memo)
    292             kwargs = dict([(n, a.eval()) for (n, a) in self.named_args])
    293             f = scope._impls[self.name]
--> 294             memo[id(self)] = rval = f(*args, **kwargs)
    295             return rval
    296 

/usr/local/lib/python3.6/dist-packages/hyperopt/pyll/stochastic.py in randint(upper, rng, size)
    104             assert len(upper) == size[0]
    105             return np.asarray([rng.randint(uu) for uu in upper])
--> 106     return rng.randint(upper, size=size)
    107 
    108 

AttributeError: 'NoneType' object has no attribute 'randint'`
What is the lowest Spark version compatible with running Hyperopt in parallel on Spark? Currently there is no place in the documentation (that I could find). My EMR cluster runs Spark 2.3.2.
`hp.uniformint`function description should be added to wiki at this place:
https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions
This is an idea build on @easyas314159 suggested here: #476 

I suppose this potentially leads into a broader discussion about progress monitoring of the optimization process. For example, there is currently no easy way to cancel the optimization process without access to the `Trials` object, or a mechanism for monitoring no improvement in a while, or even monitoring losses outside the main objective. A more robust callback system in general with hooks into various steps of the optimization process would be extremely useful.

Something kind of like this comes to mind:
```python
class Callback:
	def on_optimization_start(self, trials):
		...

	def on_optimization_end(self, trials):
		...

	def on_trial_start(self, trials, trial):
		...

	def on_trial_end(self, trials, trial):
		...
```

I can think of a handful of things this could be used for:
- Early stopping when there is insufficient improvement
- Early stopping when a threshold is passed
- Monitoring other properties of the trial object that are not the loss
- Custom progress monitoring that sends status information via HTTP to another service
- Custom progress bars with options for more than just a `best loss: %f` postfix

I can technically do all of that myself but it requires calling `fmin` for a single iteration and handling all externally which is not a great option in a distributed context.

This should probably be opened as a separate issue/enhancement? I also don't mind taking the development of this on but it will have to wait until later this month.