In order to product ordered message, can we set the hashcode of primary key to message key? How to do that?
Can provide examples of rabbitmq?
![image](https://cloud.githubusercontent.com/assets/4196781/23738551/ad2d7282-04d4-11e7-90b9-bcfc04c41e3b.png)
this time is very slow, why
can you tell me  how to run mypipe, my english is low
I can not to run mypipe 
"project runner"  "runMain mypipe.runner.KafkaGenericConsoleConsumer $world_$test_generic 127.0.0.1:4180 groupId"
project runner need to use which project
and groupId is what?
thank you very much

1、don't get mysql binary log file position, some error output info like this:
19:29:57 INFO  [m.a.r.ConfigurableFileBasedBinaryLogPositionRepository] Saving binlog position for pipe mysql-bin-position/10.0.46.116-3306 -> null:4
19:29:57 INFO  [m.a.r.ConfigurableMySQLBasedBinaryLogPositionRepository] Saving binlog position for kafka-generic/10.0.46.116-3306 -> Some(null:4)
19:30:07 INFO  [m.a.r.ConfigurableFileBasedBinaryLogPositionRepository] Saving binlog position for pipe mysql-bin-position/10.0.46.116-3306 -> null:4
19:30:07 INFO  [m.a.r.ConfigurableMySQLBasedBinaryLogPositionRepository] Saving binlog position for kafka-generic/10.0.46.116-3306 -> Some(null:4)
19:30:17 INFO  [m.a.r.ConfigurableMySQLBasedBinaryLogPositionRepository] Saving binlog position for kafka-generic/10.0.46.116-3306 -> Some(null:4)
19:30:17 INFO  [m.a.r.ConfigurableFileBasedBinaryLogPositionRepository] Saving binlog position for pipe mysql-bin-position/10.0.46.116-3306 -> null:4
19:30:27 INFO  [m.a.r.ConfigurableFileBasedBinaryLogPositionRepository] Saving binlog position for pipe mysql-bin-position/10.0.46.116-3306 -> null:4
19:30:27 INFO  [m.a.r.ConfigurableMySQLBasedBinaryLogPositionRepository] Saving binlog position for kafka-generic/10.0.46.116-3306 -> Some(null:4)

---

2、this is my configure file: application.conf

include "application.overrides"

mypipe {
  #schema-repo-client = "mypipe.avro.schema.SchemaRepo"

  consumers {
    my_test_mysql {
      source = "10.0.46.116:3306:test:test_123456"
    }
  }

  producers {
    stdout {
      class = "mypipe.kafka.producer.stdout.StdoutProducer"
    }

```
kafka-generic {
  class = "mypipe.kafka.producer.KafkaMutationGenericAvroProducer"
}
```

  }

  pipes {
    stdout {
      consumers = ["my_test_mysql"]

```
  producer {
    stdout {}
  }

  binlog-position-repo {
    class = "mypipe.api.repo.ConfigurableFileBasedBinaryLogPositionRepository"

    config {
      file-prefix = "mysql-bin-position"
      data-dir = "/data/mypipe/out"
    }
  }
}

kafka-generic {
  enabled = true
  consumers = ["my_test_mysql"]

  producer {
    kafka-generic {
      metadata-brokers = "10.0.23.169:9092"
    }
  }

  binlog-position-repo {
    class = "mypipe.api.repo.ConfigurableMySQLBasedBinaryLogPositionRepository"

    config {
      source = "10.0.46.116:3306:test:test_123456"
      database = "mytest"
      table = "binlogpos"
      id = "kafka-generic"
    }
  }
}
```

  }
}

I've a MySql table with a primary key of type Int (unsigned), when the id is bigger than the max Int value the id becomes a negative value, as expected.

I found what I think could be the root of the problem, the query on the information schema done by the MySQLMetadataManager, is using the DATA_TYPE column value, and this value is always int even for the unsigned kind.

I think an idea for a possible solution is to look also at the COLUMN_TYPE that is where the unsigned information is stored and cast this values to long?

ColumnType.scala gives us some errors on VAR_STRING and STRING values. Data stored in these kinds of columns seems to be typically byte[]. Implicit conversion using 

`column.valueOption[String].getOrElse("")`

fails. We now use a (imperfect) patched version which is attached. Imperfect by means that it assumes UTF-8 as encoding, something that should be read from the database column definition in the future.

[ColumnType.scala.zip](https://github.com/mardambey/mypipe/files/341723/ColumnType.scala.zip)

Hi,

I am new to Kafka and have a requirement to read binary log of MySQL and publish it to Kafka. I have followed your "readme" file and have mdoified my.cnf for MySQL.

My application.conf file is as below.

[application_conf.txt](https://github.com/mardambey/mypipe/files/254838/application_conf.txt)

My stdout producer works fine and I am able to see the logs in the console.
But I am not able to see the same in kafka.
I can see that the topics are created. 

I run the following command to see view the same from Kafka

./sbt "project runner" "runMain mypipe.runner.KafkaGenericConsoleConsumer kafka_test_test_user_generic localhost:2181 test-consumer-group"

I get the following output. I am not sure how to proceed further. I believe it has to do with my schema registry address. Please kindly help me

Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=256m; sup                                                                                        port was removed in 8.0
[info] Loading project definition from /home/kafkauser/confluent-2.0.1/mypipe/project
[warn] Multiple resolvers having different access mechanism configured with same name 'sbt-plugin-releases'. To avoid conflict, Remove duplicate project resolvers (`resolvers`) or rename publishing resolver (`publishTo`).
[info] Set current project to mypipe (in build file:/home/kafkauser/confluent-2.0.1/mypipe/)
[info] Set current project to runner (in build file:/home/kafkauser/confluent-2.0.1/mypipe/)
[warn] Multiple main classes detected.  Run 'show discoveredMainClasses' to see the list
[info] Running mypipe.runner.KafkaGenericConsoleConsumer kafka_test_test_user_generic localhost:2181 test-consumer-group
log4j:WARN No appenders could be found for logger (kafka.utils.VerifiableProperties).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
16:36:44 INFO  [org.apache.zookeeper.ZooKeeper          ] Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
16:36:44 INFO  [org.apache.zookeeper.ZooKeeper          ] Client environment:host.name=BIPROSHADOOP2
16:36:44 INFO  [org.apache.zookeeper.ZooKeeper          ] Client environment:java.version=1.8.0_77
16:36:44 INFO  [org.apache.zookeeper.ZooKeeper          ] Client environment:java.vendor=Oracle Corporation
16:36:44 INFO  [org.apache.zookeeper.ZooKeeper          ] Client environment:java.home=/opt/jdk1.8.0_77/jre
16:36:44 INFO  [org.apache.zookeeper.ZooKeeper          ] Client environment:java.class.path=/home/kafkauser/.sbt/launchers/0.13.9/sbt-launch.jar
16:36:44 INFO  [org.apache.zookeeper.ZooKeeper          ] Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
16:36:44 INFO  [org.apache.zookeeper.ZooKeeper          ] Client environment:java.io.tmpdir=/tmp
16:36:44 INFO  [org.apache.zookeeper.ZooKeeper          ] Client environment:java.compiler=<NA>
16:36:44 INFO  [org.apache.zookeeper.ZooKeeper          ] Client environment:os.name=Linux
16:36:44 INFO  [org.apache.zookeeper.ZooKeeper          ] Client environment:os.arch=amd64
16:36:44 INFO  [org.apache.zookeeper.ZooKeeper          ] Client environment:os.version=3.10.0-327.el7.x86_64
16:36:44 INFO  [org.apache.zookeeper.ZooKeeper          ] Client environment:user.name=kafkauser
16:36:44 INFO  [org.apache.zookeeper.ZooKeeper          ] Client environment:user.home=/home/kafkauser
16:36:44 INFO  [org.apache.zookeeper.ZooKeeper          ] Client environment:user.dir=/home/kafkauser/confluent-2.0.1/mypipe
16:36:44 INFO  [org.apache.zookeeper.ZooKeeper          ] Initiating client connection, connectString=localhost:2181 sessionTimeout=400 watcher=org.I0Itec.zkclient.ZkClient@1089e457
16:36:44 INFO  [org.apache.zookeeper.ClientCnxn         ] Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
16:36:44 INFO  [org.apache.zookeeper.ClientCnxn         ] Socket connection established to localhost/127.0.0.1:2181, initiating session
16:36:44 INFO  [org.apache.zookeeper.ClientCnxn         ] Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x1549530daae0006, negotiated timeout = 6000
[WARNING] Avro: Invalid default for field txid: "null" not a ["null",{"type":"fixed","name":"Guid","namespace":"mypipe.avro","size":16}]
[WARNING] Avro: Invalid default for field txid: "null" not a ["null",{"type":"fixed","name":"Guid","namespace":"mypipe.avro","size":16}]
[WARNING] Avro: Invalid default for field txid: "null" not a ["null",{"type":"fixed","name":"Guid","namespace":"mypipe.avro","size":16}]

Hi is it possible to publish to maven for easier use, at least mypipe-api? Thanks.

i have a mysql server on my remote server with row binlog enabled,then i want to implement a data collection bus using kafka(whether source is mysql or other format log file we use these bus to store them and provide other service ),then  i want to consume these topics in kafka to get the exact same mysql database for others to use.
does this make sense?how to realize this
