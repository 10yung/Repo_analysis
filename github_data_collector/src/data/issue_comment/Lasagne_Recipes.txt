Hi, 

I try to run the resnet-32 model on cifar-100 dataset, with only the difference of the training data in "Deep_Residual_Learning_CIFAR-10.py", but it causes the error like this:

```
Starting training...
Traceback (most recent call last):
  File "/home/changchen/anaconda3/lib/python3.6/site-packages/theano/compile/function_module.py", line 903, in __call__
    self.fn() if output_subset is None else\
RuntimeError: error getting worksize: CUDNN_STATUS_BAD_PARAM

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "resnet.py", line 390, in <module>
    main(**kwargs)
  File "resnet.py", line 319, in main
    train_err += train_fn(inputs, targets)
  File "/home/changchen/anaconda3/lib/python3.6/site-packages/theano/compile/function_module.py", line 917, in __call__
    storage_map=getattr(self.fn, 'storage_map', None))
  File "/home/changchen/anaconda3/lib/python3.6/site-packages/theano/gof/link.py", line 325, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File "/home/changchen/anaconda3/lib/python3.6/site-packages/six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "/home/changchen/anaconda3/lib/python3.6/site-packages/theano/compile/function_module.py", line 903, in __call__
    self.fn() if output_subset is None else\
RuntimeError: error getting worksize: CUDNN_STATUS_BAD_PARAM
Apply node that caused the error: GpuDnnConv{algo='small', inplace=True, num_groups=1}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty{dtype='float32', context_name=None}.0, GpuDnnConvDesc{border_mode='half', subsample=(1, 1), dilation=(1, 1), conv_mode='cross', precision='float32', num_groups=1}.0, Constant{1.0}, Constant{0.0})
Toposort index: 399
Inputs types: [GpuArrayType<None>(float32, 4D), GpuArrayType<None>(float32, 4D), GpuArrayType<None>(float32, 4D), <theano.gof.type.CDataType object at 0x7fa464893a20>, Scalar(float32), Scalar(float32)]
Inputs shapes: [(128, 3, 32, 32), (16, 3, 3, 3), (128, 16, 32, 32), 'No shapes', (), ()]
Inputs strides: [(12288, 4096, 128, 4), (108, 36, 12, 4), (65536, 4096, 128, 4), 'No strides', (), ()]
Inputs values: ['not shown', 'not shown', 'not shown', <capsule object NULL at 0x7fa3997c61e0>, 1.0, 0.0]
Outputs clients: [[GpuElemwise{sub,no_inplace}(GpuDnnConv{algo='small', inplace=True, num_groups=1}.0, InplaceGpuDimShuffle{x,0,x,x}.0), GpuContiguous(GpuDnnConv{algo='small', inplace=True, num_groups=1}.0), GpuElemwise{sub,no_inplace}(GpuDnnConv{algo='small', inplace=True, num_groups=1}.0, GpuElemwise{Composite{(((i0 / i1) / i2) / i3)}}[]<gpuarray>.0)]]

Backtrace when the node is created(use Theano flag traceback.limit=N to make it longer):
  File "resnet.py", line 390, in <module>
    main(**kwargs)
  File "resnet.py", line 267, in main
    prediction = lasagne.layers.get_output(network)
  File "/home/changchen/anaconda3/lib/python3.6/site-packages/lasagne/layers/helper.py", line 197, in get_output
    all_outputs[layer] = layer.get_output_for(layer_inputs, **kwargs)
  File "/home/changchen/anaconda3/lib/python3.6/site-packages/lasagne/layers/conv.py", line 352, in get_output_for
    conved = self.convolve(input, **kwargs)
  File "/home/changchen/anaconda3/lib/python3.6/site-packages/lasagne/layers/conv.py", line 650, in convolve
    **extra_kwargs)

HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.
```

Seems something wrong happend in the convolution operation. Could you please give any advice? Thanks a lot. 

I was trying to implement the following piece of code:
https://gist.github.com/Tathagatd96/034ab733279c8b52a571bf9cb145c2ce.js

I am getting an error pertaining to the training function in Theano. I am using the CK+ dataset.
When I hadnt reshaped my input_var ,I got an error:
TypeError: ('Bad input argument to theano function with name "C:/Users/Tathagat Dasgupta/.spyder/fred_test.py:149"  at index 0(0-based)', 'Wrong number of dimensions: expected 4, got 3 with shape (10L, 350L, 350L).')

After reshaping the array by using np.expand_dims, the error changes to :
('Bad input argument to theano function with name "C:/Users/Tathagat Dasgupta/.spyder/fred_test.py:153"  at index 1(0-based)', 'Wrong number of dimensions: expected 1, got 4 with shape (10L, 1L, 350L, 350L).')

Though the input placeholder is a 4d tensor, the error says- expected 1

How is this possible? Why is the error changing?
Pls suggest solutions.
Stuck in a dead end


Presently, Lasagne/Recipes does not seem to provide an example of an
object detector. Our contribution is a high-performance,
low-complexity single-shot detector (SSD). This detector is based on
the SqueezeDet paper (https://arxiv.org/abs/1612.01051) originally
implemented in TensorFlow
(https://github.com/BichenWuUCB/squeezeDet). Our port to Lasagne
includes some additional features for user-friendliness.

Hi
following the Guided Backpropagation recipe/example I had error for:
`if theano.sandbox.cuda.cuda_enabled` because, I guess, I use newer theano / lasagne versions with CuDNN etc. I have no cuda under theano.sandbox. two solutions:
1. skip the if and use the else option: `maybe_to_gpu = lambda x: x`
2. use gpu with gpuarray.dnn
`maybe_to_gpu = theano.gpuarray.dnn.as_gpuarray_variable
x = maybe_to_gpu(x,None)`
and then a few lines below (as above, add None as second input argument):
`outp = maybe_to_gpu(self.nonlinearity(inp),None)`
Thanks for the examples
Unfortunately, this comes a bit late as theano has recently merged a PR adding some bindings to warp-ctc (https://github.com/Theano/Theano/pull/5949). But I wanted to finish this anyway, so here it is :-).

This implementation is:
- written in pure theano
- uses an overriden gradient computation which is more resilient to precision issues
- fairly compact (suggestions for improvements and readability are welcome)
- works in log space for the most parts to prevent precision issues (so does warp-ctc). _Note that I haven't use the rescaling trick though (don't know if warp-ctc uses it)_

I think it can still be useful to anyone who wants to modify the original cost function. And it can run without extra dependency on any plateform where theano runs already.

Notes:
- I haven't battle tested the code, just run tests so far. It seems to give results very close to warp-ctc as it should (10^-7 difference on the gradients).
- The code uses OpFromGraph which is relatively recent in the theano codebase.
- I have no demo so far, contributions are welcome for that. I think the test script is a poor replacement for a real demo.
We can have an example of a Siamese network (similar to Sander Dieleman's [suggestion](https://github.com/Lasagne/Lasagne/issues/168#issuecomment-81134242) ) and convolutional spatial transformer as discussed [here](http://papers.nips.cc/paper/6487-universal-correspondence-network.pdf). From my understanding, the convolutional spatial transformer applies the ST on the feature, patchwise with distinct affine transformation parameters. I have implemented this very recently and will open a PR if you are fine with having these examples in the Recipes @f0k . 

PS : I have a few PRs and issues open in Lasagne which I'm yet to resolve. The past two weeks were a bit hectic for me and I will try to resolve them by this weekend. Apologies 
After training the script prints some segmentation results. These are mostly empty and suggest that the network did not train properly (which is not the case). This is due to a bug in the seg_output definition which I fixed in this pull request.
Cheers,
Fabian
I modified @FabianIsensee implementation of 2D UNet  to 3D UNet using[ the paper's description ](https://arxiv.org/abs/1606.06650) . The implementation is here: https://gist.github.com/mongoose54/c93c113ae195188394a7b363c24e2ac0 
I tried it on a 3D medical image dataset but I cannot say I am getting satisfying results. So I am not sure if my implementation is buggy or in general 3D UNet does not perform well. @FabianIsensee you mentioned you have implemented 3D UNet, what is your experience? Do you get good results?
@FabianIsensee 
I am trying to modify the categorical_crossentropy loss function to dice_coefficient loss function in the Lasagne Unet example. I found [this](https://github.com/jocicmarko/ultrasound-nerve-segmentation/blob/master/train.py#L21-L29) implementation in Keras and I modified it for Theano like below:

def dice_coef(y_pred,y_true):
    smooth = 1.0
    y_true_f = T.flatten(y_true)
    y_pred_f = T.flatten(T.argmax(y_pred, axis=1))
    intersection = T.sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (T.sum(y_true_f) + T.sum(y_pred_f) + smooth)


def dice_coef_loss(y_pred, y_true):
    return dice_coef(y_pred, y_true)

I am not sure if there is problem with my implementation or Dice coefficient is not robust:. See output during training validation. In comparison when I use categorical crossentropy I get like AUC > 0.98. I was wondering if anyone has played with Dice on UNet. 

Started Experiment
Epoch:  0 
train accuracy:  0.129538  train loss:  -0.146859272992
val accuracy:  0.209342  val loss:  -0.282476756789  val AUC score:  0.776537015996
Epoch:  1 
train accuracy:  0.418164  train loss:  -0.110509629949
val accuracy:  0.820385  val loss:  -0.00156800820105  val AUC score:  0.5
Epoch:  2 
train accuracy:  0.375172  train loss:  -0.129266330856
val accuracy:  0.790744  val loss:  -0.00923636992406  val AUC score:  0.5
Epoch:  3 
train accuracy:  0.581028  train loss:  -0.0889976615506
val accuracy:  0.194278  val loss:  -0.279695818208  val AUC score:  0.5


@gyglim 
I was wondering if you could share your code on training the C3D (e.g. training dataset, optimizer). I am trying to [train C3D on my own data but either get NaN or memory problems](https://groups.google.com/forum/#!topic/lasagne-users/PnKVEPXqDsI). How did you manage to train such a massive network? Thanks