Hello,

I am using the following code to set up aggressive caching for the pipeline:

`let dataLoaderConfiguration = DataLoader.defaultConfiguration
        dataLoaderConfiguration.urlCache = nil
        let dataLoader: DataLoader = .init(configuration: dataLoaderConfiguration, validate: DataLoader.validate)

        var configuration = ImagePipeline.Configuration()
        configuration.dataCache = try? DataCache(name: "com.theblueground.guestapp.DataCache")
        configuration.dataLoader = dataLoader
        configuration.imageCache = ImageCache()
        ImagePipeline.shared = ImagePipeline(configuration: configuration)`

I released there is a memory leak at DataLoader.init(configuration:validate:) method. 

See the attached screenshots for more details.

<img width="1437" alt="Screen Shot 2020-01-08 at 11 34 50 AM" src="https://user-images.githubusercontent.com/1173551/72018242-f61bd800-326f-11ea-9e88-caefe7fedc5d.png">
<img width="1440" alt="Screen Shot 2020-01-08 at 11 32 38 AM" src="https://user-images.githubusercontent.com/1173551/72018341-219ec280-3270-11ea-9924-f7b7f3e57166.png">
<img width="1439" alt="Screen Shot 2020-01-08 at 11 32 58 AM" src="https://user-images.githubusercontent.com/1173551/72018317-18adf100-3270-11ea-9af2-2e6a999424db.png">



Hi,
I'm using custom decoder for decoding images' data:
```swift 
struct SVGImageDecoder: Nuke.ImageDecoding {    
    func decode(data: Data, isFinal: Bool) -> Nuke.PlatformImage? {
        //Converting from SVG to Image...
    }
}
```
```swift 
ImageDecoderRegistry.shared.register { imageDecodingContext -> ImageDecoding? in
            if imageDecodingContext.request.urlRequest.url?.isSVG ?? false {
                return SVGImageDecoder()
            } else {
                // it takes a default decoder implicitly
                return nil
            }
        }
```
I noticed that it's firstly getting original data from disk cache (assuming a memory cache is empty) and then it's decoding. My custom decoding takes some time and it's noticeable. Is it any convenient way to store in a disk cache already decoded data in order to speed up loading? I mean, not to convert images every time the app launches (just for the first time).  
Is there a way we can customize DataLoader such that the image can be loaded from S3 using credentials?
Hello I'm getting this error :`Nuke` does not specify a Swift version and none of the targets (`Runner`) integrating it have the `SWIFT_VERSION` attribute set. Please contact the author or set the `SWIFT_VERSION` attribute in at least one of the targets that integrate this pod.

Is it ok you add the SWIFT_VERSION in Nuke.pobspec?
I am building a macOS version of an iOS app using this excellent library. Unfortunately, I am getting errors:
- Type 'ImageLoadingOptions' has no member 'ContentModes'
- Type 'ImageProcessor' has no member 'Circle'

How do you think that I can resolve this obstacle in my macOS app?
I see that SwiftUI integration is coming in January with Nuke 8.5 (on the trello board), but in the meantime can you share the best way right now to load remote images into a SwiftUI Image component using Nuke?
```
let imageView = UIImageView()
let task = ImagePipeline.shared.loadImage(
    with: url,
    progress: { response, _, _ in
        if let response = response {
            imageView.image = response?.image
        }
    },
    completion: { result in
        // Display the final image
    }
)
```
I was wondering how to load multiple URLs or UIImage in a single UIImageView using Nuke.
### Issue
Gaussian blur `CIFilter` naturally creates gray artifacts at the borders of the output image.
A common use case when blurring an image would be avoiding these artifacts.

### Proposed solution
Add a new filter that extends the edge pixels indefinitely and crop the image to its original extend after blurring.

```
/// A Gaussian blur filter that clamps to extent to avoid the gray border artefact.
struct GaussianBlurClampedToExtent: ImageProcessing, Hashable, CustomStringConvertible {    
    private let radius: Int
    
    /// Initializes the receiver with a blur radius.
    public init(radius: Int = 8) {
        self.radius = radius
    }
    
    /// Applies `CIGaussianBlur` filter to the image.
    public func process(image: Image, context: ImageProcessingContext?) -> Image? {
        
        // Get CI image
        let ciImageOptional: CoreImage.CIImage? = {
            if let image = image.ciImage {
                return image
            }
            if let image = image.cgImage {
                return CoreImage.CIImage(cgImage: image)
            }
            return nil
        }()
        
        // Ensure CI image was retrieved
        guard let ciImage = ciImageOptional else { return nil }
        
        // Remember original image extent
        let extent = ciImage.extent
        
        // Create image with infinitely extended border pixels to prevent gray edges from blur filter
        let inputImage: CIImage = ciImage.clampedToExtent()
        
        // Create blur filter
        let filter = CIFilter(name: "CIGaussianBlur", parameters: [kCIInputRadiusKey: radius, kCIInputImageKey: inputImage])
        
        // Get filtered image
        guard let filteredImage = filter?.outputImage else { return nil }
        
        // Get default context shared between all Core Image filters.
        let context = CIContext(options: [.priorityRequestLow: true])
        
        // Create CI image cropped to original extent
        guard let imageRef: CGImage = context.createCGImage(filteredImage, from: extent) else { return nil }
        return UIImage(cgImage: imageRef, scale: image.scale, orientation: image.imageOrientation)
    }

    public var identifier: String {
        return "com.github.kean/nuke/gaussian_blur_clamped_to_extent?radius=\(radius)"
    }

    public var hashableIdentifier: AnyHashable {
        return self
    }

    public var description: String {
        return "GaussianBlurClampedToExtend(radius: \(radius))"
    }
}
```

### Alternative solutions
Extend existing `GaussianBlur` filter with a Boolean `clampToExtend` option:
`"com.github.kean/nuke/gaussian_blur?radius=\(radius)&clampToExtend=\(clampToExtend)"`
Here's an interesting thing, I load many images form the net:

```
Nuke.loadImage(with:data... , options: ILO, into: self)
```

Fantastic.  Eacgh image is loaded many places in the app or on the same screen.  No problem for Nuke.

Often these days you want a "dominant" color for use in shadows or borders. (Good online example: lokeshdhakar.com/projects/color-thief/ )

For this you have CIAreaAverage or perhaps CIAreaHistogram.  Importantly, [CIKMeans](https://cifilter.io/CIKMeans/) is coming soon, too, which will be the go-to choice.

For each of these images, once the image is downloaded (by Nuke), in many places I run some processing:

		if let aac = image?.areaAverageColor {
			shadowLayerColor = aac
		}

(The code for areaAverageColor is below.)

Would there be some way in the Nuke pipeline, that

* For each of the 100s of images,

* I want it to calculate .areaAverageColor

* If I load the same image dozens of times, it knows to calculate .areaAverageColor only once

* and I guess some sort of callback to deliver the calculated value (eg,

	func nuke_display(colorFound: CGColor?) {
		self.image.shadowColor = colorFound
	}

or the like)

Does this already exist in Nuke or would it be a good idea?

Perhaps the ideas of picking out a built-in color should be built-right-in (much as say blur is immediately available).

Nuke == awesome

------------------------------------------------------------------

```
    var areaAverageColor: UIColor? {
        guard let inputImage = CIImage(image: self) else { return nil }
        
        let extentVector = CIVector(
        	x: inputImage.extent.origin.x,
        	y: inputImage.extent.origin.y,
        	z: inputImage.extent.size.width,
        	w: inputImage.extent.size.height)

        guard let filter = CIFilter(
        	name: "CIAreaAverage",
        	parameters: [
        		kCIInputImageKey: inputImage,
        		kCIInputExtentKey: extentVector]
		) else { return nil }
		
        guard let outputImage = filter.outputImage else { return nil }

        var bitmap = [UInt8](repeating: 0, count: 4)
        let context = CIContext(options: [ .workingColorSpace: NSNull() ])
        // consider also , .outputColorSpace: NSNull()
        
        context.render(
        	outputImage,
        	toBitmap: &bitmap,
        	rowBytes: 4,
        	bounds: CGRect(x: 0, y: 0, width: 1, height: 1),
        	format: .RGBA8,
        	colorSpace: nil)

        return UIColor(red: CGFloat(bitmap[0]) / 255, green: CGFloat(bitmap[1]) / 255, blue: CGFloat(bitmap[2]) / 255, alpha: CGFloat(bitmap[3]) / 255)
    }
```

