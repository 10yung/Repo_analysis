When clipping a mesh with the method ```TriangleMesh3DOperations.clip```, one would assume that all the points of the mesh, which satisfy the given predicate are part of the resulting clipped mesh. However, the way it is currently implemented is, that only the points which satisfy the predicate and are part of a triangle are clipped. 

In Scalismo we do not enforce that every point in a mesh is part of a triangle. Indeed, it is perfectly valid in Scalismo to create a mesh with an empty triangle list. Therefore we should not assume or enforce that all points are part of a triangle in the clip mesh function. 

The same problem most likely occurs when clipping tetrahedral meshes.

There seems to be an issue with the PivotedCholesky, in that for many input computeApproximateEig demonstrates chaotic behavior, i.e. a tiny variation in the input produces large variation in the output.  This is not a matter of indeterminacy; the code is deterministic, it is just sensitive to tiny variations of input.  Here is some code to demonstrate this:

`
val p2ds = for (_ <- 0 until 10) yield Point[_2D](DenseVector.rand[Double](2).toArray)
val variancesA = (DenseVector.rand[Double](100) / 10.0).toArray.toIndexedSeq
val variancesB = for (v <- variancesA) yield v + 1e-15  // without the +1e-15, the difference is 0.0
val gk2ds = for (s <- variancesA ++ variancesB) yield GaussianKernel[_2D](s)
val ik2ds = for (k <- gk2ds) yield DiagonalKernel(k, 3)
val tol = PivotedCholesky.RelativeTolerance(1e-12)
val (dms, dvs) = (for (ik <- ik2ds) yield PivotedCholesky.computeApproximateEig(ik, p2ds, 0.5, tol)).unzip
val dmdiffs = for (i <- 0 until 100) yield (dms(i + 100) - dms(i)).toArray.toIndexedSeq
val dvdiffs = for (i <- 0 until 100) yield (dvs(i + 100) - dvs(i)).toArray.toIndexedSeq
println((for (x <- dmdiffs.flatten ++ dvdiffs.flatten) yield math.abs(x)).sum)
`

Note that this test code is random and does not always demonstrate the instability, but usually does.  Furthermore, the instability itself does not necessarily make an error in the result of the computation using PivotedCholesky.  But whether or not the instability directly produces an error in the output, it seems an undesirable property of PivotedCholesky, particularly since it makes the generation of LowRankGaussianProcesses effectively randomized in practice.

Thanks for sharing your code.
Currently, tests are broken when one or more spaces are contained in the absolute path to the project folder. The returned path of the resource then contains the escaped sequence "%20" instead of spaces.

A fix could be to use a URLDecoder to transform the paths such the "%20" are decoded with spaces again.
The default Pivoted Cholesky stopping criterion (PivotedCholesky.RelativeTolerance(0)) can experience a NotConvergedException when using a very small dataset. 

The following code will in most of the runs throw an error when using the default stopping criteria. If the stopping criteria instead is set to be numberOfEigenfunctions = data-1, then it consistently works. 

```scala
val model = StatismoIO.readStatismoMeshModel(new File("model2017-1_bfm_nomouth-2500.h5")).get

val ref = model.referenceMesh

val data = (1 to 3).map(f => model.sample())

println(s"data length: ${data.length}")
val dc = DataCollection.fromMeshSequence(referenceMesh = ref, registeredMeshes = data)._1.get
val dcGpa = DataCollection.gpa(dc)
StatisticalMeshModel.createUsingPCA(dcGpa, NumberOfEigenfunctions.apply(data.length-1)).get
println("pca 1 computed")
StatisticalMeshModel.createUsingPCA(dcGpa).get // NotConvergedException
println("pca 2 computed")
```
Full error message (might need to run multiple times to get the error message - depends on the samples that are drawn from the BFM)
```
Exception in thread "main" breeze.linalg.NotConvergedException: 
	at breeze.linalg.svd$.breeze$linalg$svd$$doSVD_Double(svd.scala:110)
	at breeze.linalg.svd$Svd_DM_Impl$.apply(svd.scala:40)
	at breeze.linalg.svd$Svd_DM_Impl$.apply(svd.scala:39)
	at breeze.generic.UFunc.apply(UFunc.scala:48)
	at breeze.generic.UFunc.apply$(UFunc.scala:46)
	at breeze.linalg.svd$.apply(svd.scala:23)
	at scalismo.numerics.PivotedCholesky$.computeApproximateEigGeneric(PivotedCholesky.scala:193)
	at scalismo.numerics.PivotedCholesky$.computeApproximateEig(PivotedCholesky.scala:216)
	at scalismo.statisticalmodel.DiscreteLowRankGaussianProcess$.createUsingPCA(DiscreteLowRankGaussianProcess.scala:457)
	at scalismo.statisticalmodel.StatisticalMeshModel$.createUsingPCA(StatisticalMeshModel.scala:315)
	at scalismo.statisticalmodel.StatisticalMeshModel$.createUsingPCA(StatisticalMeshModel.scala:300)
	at apps.bfm.PCAtest$.main(PCAtest.scala:36)
	at apps.bfm.PCAtest.main(PCAtest.scala)
```
mesh.triangulation.adjacentTrianglesForPoint() currently results in a IndexOutOfBoundsException when accessed for a pointId not belonging to any cell (same for adjacentPointsForPoint). The reason is that the pointIds used for the computation of this array are obtained by iterating over the cells only.

This comes from the assumption that the meshes read into Scalismo must be "correct", i.e. without any floating vertices. This condition is however not enforced at read time.

Here we should either enforce that meshes are correct at read time (e.g. throw an exception and request to activate a repair flag) or fix the arrays above to return an empty Seq[TriangleId] for a point not belonging to any cell.
As of now SSM only allows for rigid transformations. The suggestion is to extend to also allow for similarity transform. At first, perform a rigid transform and then a scaling transform for the basis vectors according to the computed scaling factor between original and target landmarks (as done in similarity3DLandmarkRegistration).
Reference mesh and mean deformation field should also be updated.

Update the VTK library to the newest version. This will allow running headless mode - thereby avoiding X forwarding.
diceCoefficient from MeshMetrics.scala should be modified, at least, with a grid sampler that does not rely on a random number of samples. 
It would be useful if one could combine some additional attributes, such as the subject's age, weight, height, gender, or even some properties of the shape itself, e.g. length or width of the bone, together with the shape model. 

An example use case of such a combined model would be to complete a fractured bone by conditioning the model, not only on the observed healthy part, but additionally on the patient's attributes: age, weight, length of the contralateral bone , etc., which should improve the completion results.

The method presented in this paper https://link.springer.com/chapter/10.1007/978-3-319-64870-5_5  could for example be used in the implementation (generalized to all shapes, not just faces).
The function

```
PivotedCholesky.computeApproximateEigGeneric
```

takes a parameter `D`. It is from the outside not clear what this parameter does and why 
it is needed. Either it has to be documented, or (if possible) hidden from the interface.
