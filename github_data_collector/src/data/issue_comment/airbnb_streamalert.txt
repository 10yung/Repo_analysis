## Background
While testing the PR #1093, I got ` Error deleting CloudWatch Log Metric Filter: ResourceNotFoundException` errors in the end of `python manage.py destroy` operation. Re-run `python manage.py destroy` will be successful but it is not ideal. Open this issue to track the bug fix progress.

### Description

### Steps to Reproduce
```
python manage.py destroy
```
## Desired Change
I have solution that may fix this issue. It is to add a `depends_on` block to two `aws_cloudwatch_log_metric_filter` resources to prevent the log groups deleted before the metrics. Will work on this and test the idea when get time. Basically, add
```
  depends_on = [
    var.log_group_name,
  ]
```
to https://github.com/airbnb/streamalert/blob/78a29e8a15836564303ba5f8db683c650c03dd07/terraform/modules/tf_metric_filters/main.tf#L2-L14

And add 
```
  depends_on = [
    aws_cloudwatch_log_group.athena.name,
  ]
```
to https://github.com/airbnb/streamalert/blob/78a29e8a15836564303ba5f8db683c650c03dd07/terraform/modules/tf_athena/main.tf#L162-L173
## Background

Whilst running numerous deployments, i ran into issues with the terraform `element(concat())` methods within the modules.

### Description

Whilst running numerous deployments, i ran into issues with the terraform `element(concat())` methods within the modules.

### Steps to Reproduce

- Currently trying to re-produce this error

## Desired Change

All `terraform` modules which contain the `element(concat(thing.*.ATTR), 0)` become `element(concat(thing.*.ATTR, [""]), 0)` to ensure element can always find position 0

Example without:
https://github.com/airbnb/streamalert/blob/ff3aa2183c88e6907a794919aeae573d65eee433/terraform/modules/tf_lambda/output.tf#L21-L28

Example with:
https://github.com/airbnb/streamalert/blob/ff3aa2183c88e6907a794919aeae573d65eee433/terraform/modules/tf_globals/output.tf#L2
## Background

I'm using the release-3-0-0 branch, and I am getting a conflict between Terraform resources (I do not know if the same thing happens on master). Every time I run .`/manage.py build` the configuration switches between two states. I get the same result with `terraform apply` - so the issue is not due to the python code changing the Terraform files.

### Description

The output from Terraform is:
```
# module.cloudwatch_monitoring_prod.aws_cloudwatch_metric_alarm.streamalert_lambda_throttles[0] will be updated in-place
  ~ resource "aws_cloudwatch_metric_alarm" "streamalert_lambda_throttles" {
        actions_enabled           = true
      ~ alarm_actions             = [
          + "arn:aws:sns:eu-north-1:123123123123:sr_streamalert_monitoring",
        ]
        ...
      ~ ok_actions                = [
          + "arn:aws:sns:eu-north-1:123123123123:sr_streamalert_monitoring",
        ]
      ~ period                    = 120 -> 300
        ...
      ~ tags                      = {
          - "Cluster" = "prod" -> null
            "Name"    = "StreamAlert"
        }
        ...
    }
```
If I perform the actions, then re-run the command I get:
```
  # module.classifier_prod_lambda.aws_cloudwatch_metric_alarm.lambda_throttles[0] will be updated in-place
  ~ resource "aws_cloudwatch_metric_alarm" "lambda_throttles" {
        actions_enabled           = true
      ~ alarm_actions             = [
          - "arn:aws:sns:eu-north-1:123123123123:sr_streamalert_monitoring",
        ]
        ...
      ~ ok_actions                = [
          - "arn:aws:sns:eu-north-1:123123123123:sr_streamalert_monitoring",
        ]
      ~ period                    = 300 -> 120
        ...
      ~ tags                      = {
          + "Cluster" = "prod"
            "Name"    = "StreamAlert"
        }
        ...
    }
```
So the state keeps switching between these two settings.
The same thing (with same changes to alarm_actions, ok_actions, period and tags) happens with
```
 # module.cloudwatch_monitoring_prod.aws_cloudwatch_metric_alarm.streamalert_lambda_invocation_errors[0] will be updated in-place
  ~ resource "aws_cloudwatch_metric_alarm" "streamalert_lambda_invocation_errors" {
```

So far I've tracked it down to the configuration in two files, `terraform/modules/tf_lambda/cloudwatch.tf` and `terraform/modules/tf_monitoring/main.tf`. Both of them define resources that have the names above. The default period of the one in `tf_lambda` is 120, and the one in `tf_monitoring` is 300 (that is not changed in my configuration files).

As a work-around I tried to set `"cloudwatch_monitoring"/"lambda_alarms_enabled"` in the cluster configuration to false, then I ran manage.py build, then switched to true again. That resulted in that I now have 4 resources that are switching back and forth with every build:
```
  # module.classifier_prod_lambda.aws_cloudwatch_metric_alarm.lambda_invocation_errors[0] will be updated in-place
  ~ resource "aws_cloudwatch_metric_alarm" "lambda_invocation_errors" {
...
  # module.classifier_prod_lambda.aws_cloudwatch_metric_alarm.lambda_throttles[0] will be updated in-place
  ~ resource "aws_cloudwatch_metric_alarm" "lambda_throttles" {
...
  # module.cloudwatch_monitoring_prod.aws_cloudwatch_metric_alarm.streamalert_lambda_invocation_errors[0] will be updated in-place
  ~ resource "aws_cloudwatch_metric_alarm" "streamalert_lambda_invocation_errors" {
...
  # module.cloudwatch_monitoring_prod.aws_cloudwatch_metric_alarm.streamalert_lambda_throttles[0] will be updated in-place
  ~ resource "aws_cloudwatch_metric_alarm" "streamalert_lambda_throttles" {
```

### Steps to Reproduce
The cluster config (if needed for reproducing this) is:
```
{
  "id": "prod",
  "data_sources": {
    "kinesis": {
      "sr_prod_streamalert": [
        "selling_range"
      ]
    }
  },
  "modules": {
    "cloudwatch_logs_destination": {
      "cross_account_ids": [
        "456456456456",
        "123123123123"
      ],
      "regions": [
        "ap-southeast-1",
        "eu-west-1",
        "us-west-2"
      ],
      "enabled": true
    },
    "kinesis": {
      "streams": {
        "retention": 24,
        "shards": 1
      }
    },
    "kinesis_events": {
      "batch_size": 100,
      "enabled": true
    },
    "cloudwatch_monitoring": {
      "enabled": true,
      "kinesis_alarms_enabled": true,
      "lambda_alarms_enabled": true,
      "settings": {
        "lambda_invocation_error_threshold": 0,
        "lambda_throttle_error_threshold": 0,
        "kinesis_iterator_age_error_threshold": 1000000,
        "kinesis_write_throughput_exceeded_threshold": 10
      }
    },
    "streamalert": {
      "classifier_config": {
        "enable_custom_metrics": true,
        "log_level": "debug",
        "log_retention_days": 14,
        "memory": 128,
        "timeout": 60
      }
    }
  },
  "region": "eu-north-1"
}
```

## Desired Change

A stable Terraform state, or if it is due to a configuration problem a warning/error should be displayed from `./manage.py build`

to: @ryandeivert 
cc: @airbnb/streamalert-maintainers
related to:
resolves:

## Background

Invocation of `./manage.py init` fails with an error when attempting to create the `alerts` table, as the database does not yet exist.

## Changes

* The `alerts` table is created after all other infrastructure

## Testing

* Unit tests all pass. A basic initialization and deployment of a minimal streamalert instance was performed, and the aws console was used to verify that the `alerts` table was created.
## Background

Currently, there is a command to configure the 'prefix', which replaces a preset value in a handful of places. This can be prone to issues if one of these is missed.

## Desired Change

Store the prefix in _only_ one [place](https://github.com/airbnb/streamalert/blob/7d198a3273781f66465420e90886a3ce53ec7559/conf/global.json#L5). Every other place that the prefix is used, like [here](https://github.com/airbnb/streamalert/blob/7d198a3273781f66465420e90886a3ce53ec7559/conf/global.json#L4), should be programmatically interpolated instead of being hard-coded. We should still support using a 'custom' (un-prefix, user-provided) value (like the kms key alias, for example) if the user desires. 
to: @chunyong-lin 
cc: @airbnb/streamalert-maintainers
related to: #1080
resolves: #1080

## Background

Adding support for aws-ses, this is a desired output and one i think others would like.

## Changes

* Added aws-ses to docs
* Implemented the `SESOutput` Class
* Added tests
* Added additional permissions to the `alert_processor`

## Testing

* ran `tests/scripts/test_the_docs.sh` and verified additional info was ther
* ran `tests/scripts/pylint.sh` to verify code score and remediated any issues
* ran `tests/scripts/unit_tests.sh` to verify tests worked

* Tried to deploy the `terraform` for release 3-0-0 (this work is based off that branch) and ran into some issues which i am highlighting on `slack` and `github`
- tl;dr I would like the ability to send alerts via AWS SES (Simple Email Service).

## Background

### Description

Whilst slack/teams acts as the IM version of alerting, having the ability to send alerts via SES, would be great for those not constantly on IM but have the ability to check emails. 

Previously, i used a lambda written in python3 to act as an output which sent the emails as i didn't want to write anything in python2. I would like to add the ability natively to SA.

Also, we should consider the ability to send HTML structured alerts instead of text based alerts via email

## Desired Change

### Permissions (AWS)
- `ses:SendRawEmail` [Docs](https://docs.aws.amazon.com/ses/latest/DeveloperGuide/control-user-access.html)

### Code
- use `email` package to contruct the raw mime email
- use boto3 `ses_client.send_raw_email()`to send the email

to: @chunyong-lin 
cc: @airbnb/streamalert-maintainers
related to: #1078
resolves: #1078

## Background

We use Microsoft Teams instead of Slack, so I wanted to have the ability to use SA and Teams together ðŸ˜„

## Changes

* Implemented `TeamsOutput` (very very similar to the `SlackOutput`)
* Added Tests
* Updated documentation for outputs.rst

## Testing

### Code
* I implemented the output and tested it against an Incoming Webhook on our Teams Instance
* wrote tests in a style similar to the `SlackOutput` tests
* Ran tests and now have 100% Code Coverage for this new addition
* Ran `pylint` against files

### Documentation
* ran `make html` and visually checked addition in browser

## Background

NOTE: Before filing this issue, please consider the following:

Have you tried pinging us on Slack?
https://streamalert.herokuapp.com/

Are you on the latest version of StreamAlert? yes, this is a feature i would like for 3-1-0

### Description
Where i work, we use Microsoft Teams for IM. Therefor, i would like the ability to integrate Microsoft Teams as an output for Streamalert. I have a branch with some work commited currently, but i wanted to see if the community would like this feature?

Current work: [jack1902:output/teams](https://github.com/jack1902/streamalert/blob/output/teams/streamalert/alert_processor/outputs/teams.py)

## Desired Change
The ability to use Microsoft Teams (teams) as an output for alerts within StreamAlert

High level overview of the desired change or outcome.
- Alerts can be sent to teams
## Background
I would like to configure one output on a rule but have it send to different destinations based on something inside the alert. (Eg, account "a" slack channel a, account "b" slack channel "b")
NOTE: Before filing this issue, please consider the following:

Have you tried pinging us on Slack?
https://streamalert.herokuapp.com/

Are you on the latest version of StreamAlert?

### Description
I would like to write one rule, and have that route the alert to an output based on information within the alert. 
The current way i am thinking of implementing this is to use the context field and have a placeholder output, such as "slack:placeholder". 

1. The rule will use a lookup table (new feature in release 3-0-0) to use the account_id on the record and find out which team owns that account (i currently work with alot of AWS Accounts).
2. Use this team name as the descriptor (i plan to add each team as an output, to store the secrets relevant for that output in the s3 bucket) and carry on the output.

This would be amazing, as using matchers is relevant most of the time but not in a case when you want a rule to trigger regardless of the account, but only notify the team that owns it. (I don't like the idea of notifying people who don't need to be notified)

