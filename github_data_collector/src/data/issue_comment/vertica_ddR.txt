I think it make sense to extend S4 method `as` instead of using `as.dlist`, `as.darray` and so on. Correct me if I missed something.
Looking to [fork_driver.R](https://github.com/vertica/ddR/blob/master/ddR/R/fork_driver.R) we can see that `dmapply` is essentially `mcmapply`. So we rely on the fact that every object can be easily   lazily copied from master to worker process. But we are missing the fact, that it is possible that elements of `dlist` can be objects which keep some data out of R's heap in external pointers.
I read the article [dmapply: A functional primitive to express distributed
machine learning algorithms in R](http://www.vldb.org/pvldb/vol9/p1293-ma.pdf) and found following diagram:
<img width="376" alt="screen shot 2017-03-31 at 09 50 48" src="https://cloud.githubusercontent.com/assets/5123805/24538042/cf58a974-15f7-11e7-9f62-fe57b6e57cf6.png">

That was surprise to see peer-to-peer communication. I started to investigated to ddR code and found following - [ddR.R#L278-L285](https://github.com/vertica/ddR/blob/master/ddR/R/ddR.R#L278-L285). So essentially all communications go through master. Do I miss something or this is just misleading diagram?

I'm not that experienced with snow clusters. Can peer-to-peer communication be potentially done on `parallel` package framework?
I am running into a memory issue with drandomForest.  I receive this message which we have all seen before: Error in drandomForest.default(m, y, ..., ntree = ntree, nExecutor = nExecutor,  : 
  cannot allocate vector of size 7.9 Gb

function call:
tree.result <- drandomForest( Target~., data=as.data.frame(signalDF), mtry=predictors, nExecutor = 4)

Executing on a 64-bit machine with 32 Gb memory.  # predictors = 64, and the the dataset contains ~ 4 million rows.  The size of the data is slightly over 2GB.  We are using the function in classification mode ( y is set as a factor).  Predictors are a combination of numeric vars and factors.   

Please advise.  Thanks!
This is a question more than an issue, I would like to create a new backend using Service Fabric Stateful services, can you lead me to some examples or at least where in source I should be looking for the interfaces I need to implement to create my own backend?

Thanks.
Probably the most common operation in R is subsetting, ie. `[` and `$`. While trying to use `ddR` and looking at `ops.R` I notice that the subsetting operators all `collect`.

How would one return a distributed object from subsetting? I.e. remove 10% of the rows of a `dframe` that contain `NA`. The resulting `dframe` will still be large, so it's best to keep it distributed.

One idea is to have `DObject`s closed under subsetting and leave it to the user call `collect()` explicitly.

```
> a = as.dframe(iris)
> colnames(a)
[1] "Sepal.Length" "Sepal.Width"  "Petal.Length" "Petal.Width"  "Species"
> a$Species
Species
   <NA>
Levels: setosa versicolor virginica
> iris$Species
  [1] setosa     setosa     setosa     setosa     setosa     setosa
  [7] setosa     setosa     setosa     setosa     setosa     setosa
 [13] setosa     setosa     setosa     setosa     setosa     setosa
```

Probably the best way to fix this is to include `dframes` in OO model as described in the [design section of the wiki](https://github.com/vertica/ddR/wiki/Design).

This is too slow:

```
> system.time({
+ colnames(ds)
+ })
   user  system elapsed
 16.015  41.366 206.624
```

Here `ds` is a a `dframe` with 4 partitions. Each chunk is about 10 million rows and 8 columns.

Running on a 2016 Macbook pro with a `FORK` cluster.

Following up on #15 here are some ideas for improvements to ddR.
- [x] Makefile to automate testing, Doc build, and examples. See #15 
- [x] Overhaul init and useBackend methods. See #15 
- [x] Internal documentation on ddR's programming model and how to write backends. See [ddR wiki](https://github.com/vertica/ddR/wiki)
- [ ] Rewrite examples for clarity, reproducibility, and best practices
- [ ] Examples of reading / processing / writing actual data
- [ ] Benchmarks for various backends (from @dselivanov )
- [ ] Simplifying do_dmapply
- [ ] Set up continuous integration on Travis (or similar service) - _Probably requires admin access to repo?_
- [ ] Making distributed objects act more like their local counterparts through more OO code

The changes below might require more conversation, since I don't know the reasons behind the design decisions:
- [ ] Allow partitioning dataframes only on rows
- [ ] Make ddR column major order like R
- [ ] Change name from arrays to matrices

The changes in #15 brought this bug out. Global variables are not exported to a PSOCK cluster. This causes the kmeans example to fail. A minimal example:

```
library(ddR)

globvar <- 100
f <- function(x) globvar

useBackend(parallel, type="FORK")

# Works fine
dl <- dlist(1:10, letters)
collect(dlapply(dl, f))

useBackend(parallel, type="PSOCK")

# Fails to find globvar
dl <- dlist(1:10, letters)
collect(dlapply(dl, f))
```

So I think we should make a call for how ddR should work for portability. Here's what I see as the options:
1. Only allow pure functions _The simplest approach_
2. Add a parameter to pass an environment where the function is to be evaluated _Supported by Spark and parallel backends_
3. Programmatically gather function dependencies from the code _SparkR does this_

Right now 2) is the most appealing, because it's clear what's happening. 1) would be not enough- for example I often compose a large function out of several small functions. 3) is appealing, but is significantly more complex.
