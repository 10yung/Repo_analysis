These changes remove some unnecessary trait restrictions on the functions, and also generalize all uses of `PartialEq` (including the set implementation) to arbitrary types implementing `PartialEq<T>`. I think in most cases this introduces no breaking changes (and all tests still run and pass as before), however a larger corpus or test base may better confirm this. The only thing that makes me less sure is things like the change to `sym` which cause the returned parser to be a different type than types that are inferred from input arguments (i.e., the output type now is inferred from usage rather than from arguments, so it may be the case that existing code will need annotations to disambiguate). So to be safe it may be appropriate to put this toward a breaking API release.
Example I'm parsing for supported data_type with:

```rust
    static ref MUST_BE_VALID_DATA_TYPE: String = format!(
        "must have a valid data types are: {}",  SUPPORTED_DATA_TYPE.join(", ")
    );

    (tag("bool").map(|_| DataType::Bool)
        | tag("u8").map(|_| DataType::U8)
        | tag("u16").map(|_| DataType::U16)
        | tag("u32").map(|_| DataType::U32)
    .expect(&MUST_BE_VALID_DATA_TYPE)
```
I want to display show the symbols that are being processed instead of only the first letter.

I'm trying to generalise the approach found in the `whitespace` example.

I think some changes may be needed in pom.

The following function is illegal:

```
fn with_indentation<'a, T>(p: Parser<'a, u8, T>) -> Parser<'a, u8, T>
where
    T: 'a + Clone + Sized,
{
    (
        indented() |
        empty().map(|()| vec![])
    ).repeat(0..).map(
        |lines| lines.into_iter().filter(
            |line| line.len() > 0
        ).fold(
            vec![],
            |accum, line| accum.into_iter().chain(
                line.into_iter().chain(vec![b'\n'].into_iter())
            ).collect()
        )
    ).map(move |lines| {
        p.parse(
            &lines
        ).expect("subparse")
    })
}
```

The error is like this:

```
170 |   fn with_indentation<'a, T>(p: Parser<'a, u8, T>) -> Parser<'a, u8, T>
    |                              - lifetime `'1` appears in the type of `p`
...
189 | /         p.parse(
190 | |             &lines
    | |             ^^^^^^ borrowed value does not live long enough
191 | |         ).expect("subparse")
    | |_________- argument requires that `lines` is borrowed for `'1`
192 |       })
    |       - `lines` dropped here while still borrowed
```

There is a problem: `map` takes `Fn`, not `FnOnce`, so it's not possible to guarantee idempotence
Following up on the discussion in #24, I created one possible implementation which allows chaining parsers without getting a nested tuple result. You can see the result of this at the link below. Note that in this example, I could have removed the call to `map` entirely, but I wanted to leave it to demonstrate that the `(hours, minutes, seconds)` tuple is not longer nested.

https://github.com/J-F-Liu/pom/compare/master...JoshMcguigan:experimental-combinator

Unfortunately, at the moment I'm not sure how this could be extended to tuples of any size without creating `all4`, `all5`.. `allN` for some reasonable value of N.  Another downside of this approach is when the user adds a parser to the chain they'd have to switch which version of the `all` function they are using, from `allN` to `allN+1`. 

The upside to this is the result is a (not nested) tuple of the results of each of the parsers, which means this could nicely replace the use of the `+`, `-`, and `*` combinators, allowing users to write these types of combinations in what I'd consider to be more idiomatic Rust. 

Thanks again for your work on this crate, and feel free to let me know if this isn't something you are interested in.