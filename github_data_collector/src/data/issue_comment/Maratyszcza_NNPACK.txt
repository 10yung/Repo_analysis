Good day, everyone!
Recently, i've noticed that nnp_softmax_output result is buggy(imho) for some inputs.
Examples:
First one : 
`
std::vector<float> input {9.34623, 8.43469, 7.19462, 6.59385, 5.89481, 5.67304};
std::vector<float> output(6);
nnp_softmax_output(1, 6, input.data(), output.data(), nullptr);
`
example's output will be : 
`0.571349, 0.285674, 0.0714186, 0.0357093, 0.0178546, 0.0178546`
And it sum's to : 0.99986 which is not equal to 1 !
Ground truth values for this input:
`0.61010081, 0.24520245, 0.07095275, 0.03890972, 0.01934056, 0.01549371`

Second:
This time i take a part of previous input, for example first three numbers:
`
std::vector<float> input {9.34623, 8.43469, 7.19462};
std::vector<float> output(3);
nnp_softmax_output(1, 3, input.data(), output.data(), nullptr);
`
The output will be : 
`0.658674, 0.264724, 0.0766017`
And the sum is exactly 1

Please help me to find a reason, why i have such behaviour, which have strong relation to the size of the input ?

I use libnnpack compiled for x86 processror with avx2.
Thank you in advance!

It fails on FreeBSD 12 with this error:
```
[43/43] : && /usr/bin/cc -fPIC -O2 -pipe  -fstack-protector-strong -isystem /usr/local/include -fno-strict-aliasing -O2 -pipe  -fstack-protector-strong -isystem /usr/local/include -fno-strict-aliasing  -fstack-protector-strong -L/usr/local/lib -shared -Wl,-soname,libnnpack.so -o libnnpack.so src/x86_64-fma/2d-fourier-8x8.py.o src/x86_64-fma/2d-fourier-16x16.py.o src/x86_64-fma/2d-winograd-8x8-3x3.py.o src/x86_64-fma/blas/s8gemm.py.o src/x86_64-fma/blas/c8gemm.py.o src/x86_64-fma/blas/s4c6gemm.py.o src/x86_64-fma/blas/conv1x1.py.o src/x86_64-fma/blas/sgemm.py.o src/x86_64-fma/max-pooling.py.o src/x86_64-fma/relu.py.o src/x86_64-fma/softmax.py.o src/x86_64-fma/blas/sdotxf.py.o src/x86_64-fma/blas/shdotxf.py.o CMakeFiles/nnpack.dir/src/init.c.o CMakeFiles/nnpack.dir/src/convolution-inference.c.o CMakeFiles/nnpack.dir/src/fully-connected-inference.c.o CMakeFiles/nnpack.dir/src/pooling-output.c.o CMakeFiles/nnpack.dir/src/relu-output.c.o CMakeFiles/nnpack.dir/src/softmax-output.c.o CMakeFiles/nnpack.dir/src/fully-connected-output.c.o CMakeFiles/nnpack.dir/src/relu-input-gradient.c.o CMakeFiles/nnpack.dir/src/convolution-input-gradient.c.o CMakeFiles/nnpack.dir/src/convolution-kernel-gradient.c.o CMakeFiles/nnpack.dir/src/convolution-output.c.o CMakeFiles/nnpack.dir/src/x86_64-fma/softmax.c.o  -Wl,-rpath,/wrkdirs/usr/ports/science/nnpack/work/.build/deps/cpuinfo: deps/cpuinfo/libcpuinfo.so -lpthreadpool && :
FAILED: libnnpack.so
: && /usr/bin/cc -fPIC -O2 -pipe  -fstack-protector-strong -isystem /usr/local/include -fno-strict-aliasing -O2 -pipe  -fstack-protector-strong -isystem /usr/local/include -fno-strict-aliasing  -fstack-protector-strong -L/usr/local/lib -shared -Wl,-soname,libnnpack.so -o libnnpack.so src/x86_64-fma/2d-fourier-8x8.py.o src/x86_64-fma/2d-fourier-16x16.py.o src/x86_64-fma/2d-winograd-8x8-3x3.py.o src/x86_64-fma/blas/s8gemm.py.o src/x86_64-fma/blas/c8gemm.py.o src/x86_64-fma/blas/s4c6gemm.py.o src/x86_64-fma/blas/conv1x1.py.o src/x86_64-fma/blas/sgemm.py.o src/x86_64-fma/max-pooling.py.o src/x86_64-fma/relu.py.o src/x86_64-fma/softmax.py.o src/x86_64-fma/blas/sdotxf.py.o src/x86_64-fma/blas/shdotxf.py.o CMakeFiles/nnpack.dir/src/init.c.o CMakeFiles/nnpack.dir/src/convolution-inference.c.o CMakeFiles/nnpack.dir/src/fully-connected-inference.c.o CMakeFiles/nnpack.dir/src/pooling-output.c.o CMakeFiles/nnpack.dir/src/relu-output.c.o CMakeFiles/nnpack.dir/src/softmax-output.c.o CMakeFiles/nnpack.dir/src/fully-connected-output.c.o CMakeFiles/nnpack.dir/src/relu-input-gradient.c.o CMakeFiles/nnpack.dir/src/convolution-input-gradient.c.o CMakeFiles/nnpack.dir/src/convolution-kernel-gradient.c.o CMakeFiles/nnpack.dir/src/convolution-output.c.o CMakeFiles/nnpack.dir/src/x86_64-fma/softmax.c.o  -Wl,-rpath,/wrkdirs/usr/ports/science/nnpack/work/.build/deps/cpuinfo: deps/cpuinfo/libcpuinfo.so -lpthreadpool && :
/usr/bin/ld: error: src/x86_64-fma/2d-fourier-16x16.py.o: unaligned data
cc: error: linker command failed with exit code 1 (use -v to see invocation)
ninja: build stopped: subcommand failed.
```

Looking into it, it seems to be a very bad idea to compile the python code into binaries. People who write in Python usually believe that Python is "good enough" for most everything. If this isn't true, then this code should be rewritten in a more performant language, like C++ (or C or Rust). Writing in Python first, and then discovering that this was a bad idea and trying to accelerate it using some custom-written compiler isn't a good idea IMO, because it leads to errors like this.

Hi, I am impressive in the code and trying to integrate some possible optimization.

I only found the function of FFT/IFFT transformation for real to complex and vice verse. Have you implemented the related function for complex to complex? Or is it possible for a future plan?

Thank you very much!
There does not appear to be a way to get the nnpack version from nnpack.h
The CMakefile of NNPACK is too complex. How should I build NNPACK for Linux ARM.
Like this...
```shell
SET(TARGET_SYSROOT ${LINUX_BUILD_ROOT}/host/usr/arm-buildroot-linux-gnueabi${HARD_FLOAT}/sysroot)
SET(TARGET_C_COMPILER ${LINUX_TOOLCHAIN}/bin/arm-linux-gnueabi${HARD_FLOAT}-gcc)
SET(TARGET_CXX_COMPILER ${LINUX_TOOLCHAIN}/bin/arm-linux-gnueabi${HARD_FLOAT}-g++)

SET(CMAKE_C_FLAGS "-rdynamic -pipe -marm -mfpu=neon -mcpu=cortex-a7 -Wall -W  -Wno-unused-parameter -Wno-sign-compare -Wno-int-conversion -D_REENTRANT")
SET(CMAKE_CXX_FLAGS "-rdynamic -pipe -marm -mfpu=neon -mcpu=cortex-a7 -Wall -W  -Wno-unused-parameter -Wno-sign-compare -Wno-int-conversion  -D_REENTRANT")


```
Global cache blocking values and thread scheduling don't map optimally to the new big.LITTLE systems with 'global task scheduling' (where all CPU tasks are active at the same time).
Even after integration with the cpuinfo library the init_hwinfo() function still uses fixed hardcoded values for ARM CPUs.
I guess the same cpuinfo-based code should be used for all architectures (although oddly enough, I tried that locally and got no performance improvement on various phones)...
 I wrote small snippet of code to check NNPack convolution inference speed. For some reason nnp_convolution_inference method returns output with zeros. I am not able to figure out the issue with below snippet of code. Can you please help me with the issue.

------------------------------------------
    nnp_status status = nnp_initialize();
    nnp_convolution_transform_strategy transform_strategy = nnp_convolution_transform_strategy_precompute;
	const nnp_convolution_algorithm algorithm = nnp_convolution_algorithm_auto; //nnp_convolution_algorithm_implicit_gemm;

    size_t input_channels = 1;
    size_t output_channels = 3; 
    const nnp_size input_size = {5, 5}; 
    const nnp_size output_size = {3, 3}; 
    const nnp_padding input_padding = { 0, 0, 0, 0 };
	const nnp_size kernel_size = {3, 3}; 
	const nnp_size stride = { 1, 1 };
	
    std::vector<float> input(input_size.width * input_size.height * input_channels);
    std::vector<float> kernel(input_channels * kernel_size.width * kernel_size.height * output_channels);
    std::vector<float> bias(output_channels);
    std::vector<float> output(output_channels * input_size.width * input_size.height);
    
    kernel = {1.0576, -0.0638, -0.3667, 0.2912,  0.9600, -0.2763, 0.4745,  0.0218, -0.4153, -0.2512,  2.2507,  0.3270,
          -0.5482, -0.0241, -0.3120, 0.5434, -2.8615,  0.9707, 1.5259, -0.8924, -0.4584, -0.3262,  1.2160, -0.5744, 1.2048, -1.1605,  0.7418};
   
    input = {1.0163, -1.7396, -0.1464, -1.2687, -2.7988,
               0.1436, -0.0367,  0.0719, -1.0046,  0.7306,
               -0.5130, -1.0900, -0.8827,  0.5993,  0.8043,
               0.6443, -1.7176,  0.5912,  0.2367,  0.5063,
               -1.0304,  1.2539, -1.4350, -2.2669, -0.2690};
     bias = {0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0};
    
    //std::vector<uint8_t, AlignedAllocator<uint8_t, 32>> transformedKernel, workspace_buffer;
    std::vector<float> workspace_buffer;

    pthreadpool_t threadpool = pthreadpool_create(2);

    size_t workspace_size = 0;
    status = nnp_convolution_inference(
			algorithm, nnp_convolution_transform_strategy_precompute,
			input_channels, output_channels,
			input_size, input_padding, kernel_size, stride,
			NULL, NULL, NULL, NULL, NULL, &workspace_size,
			nnp_activation_identity, NULL,
			NULL, NULL);
    
    if (status != nnp_status_success) {
        std::cout << "nnp failure status  " << status << std::endl;
        return -1; 
    } 

    std::cout << "Workspace buffer size  " << workspace_size << std::endl;
    
    workspace_buffer.resize(workspace_size);

    auto begin = chrono::duration_cast<chrono::milliseconds>(chrono::steady_clock::now().time_since_epoch()).count();


            status = nnp_convolution_inference(
            algorithm,
            transform_strategy,
            input_channels,
            output_channels,
            input_size,
            input_padding,
            kernel_size,
            stride,
            input.data(),
            kernel.data(),
            bias.data(),
            output.data(),
            nullptr, //static_cast<void*>(workspace_buffer.data()),
            &workspace_size,
            nnp_activation_identity,
            NULL,
            NULL,
            NULL);
            std::cout << status << std::endl;

    
    auto end = chrono::duration_cast<chrono::milliseconds>(chrono::steady_clock::now().time_since_epoch()).count();
    std::cout << "Use time " << (end - begin) / (times + 0.0) << "\n";

Hi, I want to know how nnpack implement the kernels used in convolution inference. By reading the codes, I find the computation is taken by kernels in src/x86_64. For examples, the direct convolution is computed by the kernel in src/x86_64/blas/conv1x1.py, which is built on the peachpy framework(if my understanding is right).
Most of the codes build on peachypy is straightforward and easy to understand, but some of them are hard. So I'd like to know if there are documents about the peachpy or the kernel implementation details.
Tanks.
I'm trying to test an optimization tool that repeatedly rewrites a function in a native binary and then tests the rewrite for correctness and performance. The structure of the nnpack binaries makes this process extremely complicated and error prone:

1. There is no easy way to determine which kernel will be invoked by a particular set of command-line arguments. For example, if I want to test `nnp_fft8x8_with_offset_and_stream__avx2`, I have to step through the code and figure out which kind of inputs will be directed to that kernel. 

2. There are some preparatory loops and other setup phases that are not isolated from the timed code, so my tool requires special filters to ignore certain kernel invocations.

3. The validation option is not available in the `xxxx-benchmark` binaries. So for example, suppose I get a great speedup in one of my variations of the kernel--now I am facing a mountain of work to validate that my tool did not accidentally break the function. My best solution has been to splice the rewritten function into one of the other binaries that has a validator, but this is very time consuming because it requires patching rip-relative addresses (and there are more complications if my rewrite is larger than the original function). 

At this point it has simply become too difficult to experiment with my tool on nnpack. To continue, what I will need is a single binary that can do the following:

1. Run any kernel in a meaningful context. It doesn't matter to me what data or what kind of use case it chooses to run (even if someone explained it to me, I wouldn't understand). I just want it to do whatever it does: black box, no manual.

2. Provide a single-knob command-line argument to adjust the duration of the test. So if the default configuration runs for 10 seconds, then passing an argument of 2 would make it run for 20 seconds, or .5 for 5 seconds.

3. Automatically evaluate every invocation for correctness. It doesn't matter how, and if there are errors, it doesn't matter what. The domain of nnpack is totally opaque to me, so I will simply step through the validator and find what it did not approve in binary executable terms.

I would be glad to implement this "test driver" and submit a pull request, but it's a bit over my head at this point. If someone can walk me through the steps in extremely concrete, low-level terms, I can probably figure out the details.