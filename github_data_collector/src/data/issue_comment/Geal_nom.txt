Inline attribute is ignored for prototypes so this change removes it. See https://rust-lang.github.io/rust-clippy/master/#inline_fn_without_body
This PR just upgrades jemalloc and criterion to the latest supported versions. It does not require any code change, and tests seem to pass. It updates the two missing dependencies after #1101 and #1073 (or #950) get merged.

The API does not change, so this can be released as a minor version.

As you know better than anyone, with nom 5.0 we seem to be moving from macros toward methods.  There are a set of macros for parsing numbers that take endianness as a parameter.  For example, `u32!(nom::number::Endianness::Big)` is roughly equivalent to `nom::number::complete::be_u32`.

This pull request adds a set of equivalent methods such as `nom::number::complete::u32()` that take endianness as a parameter and delegate to `u32_be` or `u32_le` as appropriate.  This allows for easy migration of older projects that use the macro `u32!()` syntax by instead calling the method `u32()`.

In a separate commit I also marked `nom::number::complete::be_u8` and `le_u8` as deprecated.  It doesn't seem to make sense to have endian-specific methods for handling one-byte integers since there is no byte swapping involved.

Feedback and edits are welcome.  Thank you for considering this pull request.
I am trying to parse utf-8 encoded text with non-ascii alphabetical characters. 
I was using `alpha1` before, but it only accepts ascii characters. Then I tried to parse the strings `char` by char, however I am getting an error `Incomplete(Size(1))` from this test:
```rust
extern crate nom;

#[derive(PartialEq, Debug, Eq, Clone, Hash, Ord, PartialOrd)]
pub struct Word {
    chars: String,
}

impl From<&str> for Word {
    fn from(s: &str) -> Self {
        Self {
            chars: s.into(),
        }
    }
}
use nom::*;
impl Word {
    named!(
        parse(&str) -> Self,
        map!(
            take_while1!(nom::AsChar::is_alpha),
            |s| Self { chars: s.into() }
            )
    );
}


#[test]
fn parse_word() {
    assert_eq!(Word::parse("erfüllen").unwrap().1, Word::from("erfüllen"));
}
fn main() {
    println!("Hello, world!");
}
```
Why does this happen?
Signed-off-by: koushiro <koushiro.cqx@gmail.com>

## What have you changed?

* Fix the `alloc` feature in stable version
* Fix warnings


Patches:
- #1100
- #1080
- #1066
- And a few upstream issues.
Hi, and thanks for all the hard work on Nom.

## Prerequisites

Nom uses a dependency I maintain, lexical-core, which used to define a C-API and Rust API within a single crate. Due to symbol issues and to remove unnecessary functionality from lexical-core, the C-API was separated into a new crate. Other bug fixes have since been implemented, for example, in ensuring full standard compliance in parsing weird edge-cases like ". ", among others.

**Issues**

Other uses have reported issues with older versions of lexical:

[Undefined Symbols when building on Windows](https://github.com/Alexhuszagh/rust-lexical/issues/29)
[Symbol Issues when used in a proc macro](https://github.com/Alexhuszagh/rust-lexical/issues/23)
[Undefined Symbols when building on Windows](https://github.com/Geal/nom/issues/1066)
[Incorrect result with parsing ". "](https://github.com/Geal/nom/issues/1080)

**Versions**

To support Rustc versions 1.31+, as Nom currently does, please use `lexical-core = "0.6"` in Cargo.toml, which supports any version of Rustc >= 1.24.0.

**F64 Complete**

The patch for the complete, f64 parser would be:

```rust
#[cfg(feature = "lexical")]
pub fn double<T, E:ParseError<T>>(input: T) -> IResult<T, f64, E>
where
  T: crate::traits::AsBytes + InputLength + Slice<RangeFrom<usize>>,
{
  match ::lexical_core::parse_partial(input.as_bytes()) {
    Ok((value, processed)) => Ok((input.slice(processed..), value)),
    Err(_) => Err(Err::Error(E::from_error_kind(input, ErrorKind::Float)))
  }
}
```

**F64 Streaming**

The patch for the streaming, f64 parser would be:

```rust
#[cfg(feature = "lexical")]
pub fn double<T, E:ParseError<T>>(input: T) -> IResult<T, f64, E>
where
  T: crate::traits::AsBytes + InputLength + Slice<RangeFrom<usize>>,
{
  match ::lexical_core::parse_partial(input.as_bytes()) {
    Ok((value, processed)) => {
      if (processed == input.input_len()) {
        Err(Err::Incomplete(Needed::Unknown))
      } else {
        Ok((input.slice(processed..), value))
      }
    },
    Err(_) => Err(Err::Error(E::from_error_kind(input, ErrorKind::Float)))
  }
}
```
This PR fixes a minor grammatical error and a minor typo in the docs in src/lib.rs the diff is tiny and should be easy to check for correctness.

* purpose -> purposes
* then -> them
There is the `separated_pair!` macro, but I have already found it useful to parse multiple values separated by the same separator into a tuple.
```
map!(
    separated_tuple!(
        tag!(":"),
        tag!("ab"),
        tag!("xy"),
        tag!("12")
    ),
    |(c1, c2, c3)| println!("{} {} {}", c1, c2, c3)
);
```
```
ab:xy:12 -> ab xy 12
```
A use case was discussed [here](https://users.rust-lang.org/t/how-to-select-a-parser-and-reuse-it-later-in-nom/36280).
I think there’s no obvious reason why `tag` should only work on bytes, as opposed to any kind of slice or vector. However, it currently requires the `InputTake` trait on the type of the input, which allows the input to be sliced. `InputTake`’s `input_take` method specifies that it returns a slice with a specified length in bytes and is only implemented for strings and slices of bytes. I think it could be generalized to arbitrary slices, in which case the length of the returned slice would not necessarily be measured in bytes. `Compare` should work in a similar way, which would allow generalization of `tag`.