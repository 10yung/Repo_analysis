```
top - 17:19:18 up  6:58,  1 user,  load average: 1.18, 1.01, 0.82
Tasks: 143 total,   2 running, 141 sleeping,   0 stopped,   0 zombie
**%Cpu(s): 16.4 us**,  6.6 sy,  0.0 ni, 71.7 id,  4.3 wa,  0.0 hi,  1.1 si,  0.0 st
KiB Mem : 12137936 total,   172000 free, 11748060 used,   217876 buff/cache
KiB Swap:  1048572 total,    59488 free,   989084 used.   107876 avail Mem 

  **PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                  
12184 root      20   0   27.7g  10.7g    528 R  82.7 92.1   4:31.62 ssdb-server**                                                              
   45 root      20   0       0      0      0 S  11.7  0.0   6:08.01 kswapd0                                                                  
12295 root      20   0  162016   1880   1168 R   0.3  0.0   0:00.03 top                                                                      
    1 root      20   0  193564   1592    776 S   0.0  0.0   0:04.03 systemd                                                                  
    2 root      20   0       0      0      0 S   0.0  0.0   0:00.02 
```
ideawu,您好:
对于Redis协议中del命令无法删除除Key-value以外(hash,zset,queue)的数据,在#810与#1178中均有提到.
对于这个问题您在问题#810中的答复为先知道类型再进行删除的方案,这一点我是认同的,从您的源码也能看出这样的设计原则.但是在某些情况下,使用SSDB自定的命令可能不太合适.
如果是单实例的SSDB,客户端直接连接SSDB,直接发送自定义命令hclear/zclear/qclear即可删除对应的数据,但是如果是SSDB集群,并且使用twemproxy进行负载均衡以及容量扩充,由于twemproxy的问题,其本身连一些redis的命令都无法做到支持(比如flushdb,keys之类的,不支持也情有可原),对于SSDB所定义的这些更加特殊的自定义方法并不能很好的进行转发(会导致TwemProxy断开连接).在此场景下,唯一能够对数据库中数据进行删除的命令就只剩del了,而ssdb的del专指(key)del,这将导致在此环境下,SSDB只能增加hash/zset/queue的存储,而不能进行数据的删除.

请您原谅我冒昧提出的Pr.我所更改的部分只是将原本属于key-value的del命令更改为kdel,并将原来del/hclear/zclear/qclear四个函数所做工作集中整合至新的del命令,代码与原来的部分基本一致,所以对于主从部分场景下的功能没有做测试,如果有什么bug在里边,还希望您能斧正,谢谢.
为了方便检测更新，希望能更新 releases页面

https://github.com/ideawu/ssdb/releases
看到好久没更新了
Hello,
I set key / value in cache with phpfastcache and ssdb driver, with php i can see the key is been in cache i can get the value but when i search with phpssdbadmin or command line i can't find the key / value 
I don't understand why ?
Thanks
ssdb好像不支持 nginx转发，twemproxy 作集群的时候，pyssdb操作会报错
ssdb日志：

```
2019-10-07 21:41:48.608 [DEBUG] server.cpp(275): new link from 10.193.98.138:59588, fd: 18, links: 1
2019-10-07 21:41:48.608 [DEBUG] server.cpp(433): fd: 18, read: 0, delete link, s:0.000
```



客户端：
```
Could not connect to Redis at 10.193.98.3:6378: 
not connected>
```


The ssdb-cli nagios checks (-n replication etc) print a relevant result of OK, WARNING, CRITICAL, but the exit code is always 0, leaving nagios to believe the result is always OK. (nagios reads the exit code, not the console output.)

The nagios.cpy routines correctly return 0, 1, 2 respectively for OK, WARNING, CRITICAL , but the wrapper routine ssdb-cli.cpy puts an uncoditional exit(0) after it.

hello, 

there is an issue with the current sync of ssdb which makes it harder to operate a ssdb cluster (sync=mirror). Currently when two masters nodes that use sync=mirror go out of sync, it is needed to completely shutdown both ssdb processes and then remove META directories in order to restore the sync state. This works but requires a downtime. 

In situations when one uses actively only one of the two masters, it'd be very good if there would be a way to let SSDB recreate its META directory without needing to shutdown and restart the server. 
In my case I have two mirrors but I only actively use one of them: master A. So in my case Master A contains all data and master B is a copy of it. I don't actively write to master B. When there is a problem with Master A my ssdb client switches seamless to master B.  When Master A comes back up, then my ssdb client switches back to it . 

This is working well in my case, but I have only this issue that if when I'm using actively MAster A there is a problem in master B, then when i fix master B and i restart it it gets synced to master A but master A goes OUT_OF_SYNC from master B (even though nothing is written to master B). And this forces me to restart Master A and bring down my whole web service. 
When this happens i need to bring down the server and delete  META and DATA in master B and then delete META in Master A... then i restart and MasterB will sync again from master A. This is fine but it requires are  restart of master A.
It'd be really good if there is a way to let master A "forget" about master B state and rebuild its META, without needing to restart it 


thanks



ssdb为什么增加一个setx命令，而不是和redis一样 使用 setex命令？ 
setex命令还是比较常用的， 好奇为什么这么设计，会不方便代码层面直接从redis 迁移到ssdb