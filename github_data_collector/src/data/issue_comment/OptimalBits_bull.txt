## Description

I experiencing problems while trying to gracefully shutdown my queue(s).
My code works (or should work) a bit like this:
- I have multiple queues, with [named jobs](https://optimalbits.github.io/bull/#named-jobs)
- I have one or more processes per queue (one process for each named job obviously)
- I have some logic that listens for interrupt signals so that I know when to try and gracefully shut down my queues
- I loop through my queues and run `.close()` on each of the queues
- I want every processes to finish their current job (if they have any) and then close the queue to prevent any new jobs from being processed
- Once all currently processing jobs have finished I want the Node process to exit

The problem I'm experiencing is that whenever I run `.close()` on a queue, the queue will close even if there's a job currently being processed.

However, this only happens when I have multiple processes for a queue. Which is necessary since I'm using named jobs.

## Minimal, Working Test code to reproduce the issue.
Here's a repo with some code that reproduces this behaviour and (hopefully) explains my problem better.
[https://github.com/ErikMartensson/bull-multiple-processes-named-jobs](https://github.com/ErikMartensson/bull-multiple-processes-named-jobs)

Whenever the second process is removed or commented out, the queue will wait for the current job to end processing and then close as expected.

#### (An easy to reproduce test case will dramatically decrease the resolution time.)

```javascript
const testQueue = new Queue('test');

testQueue.process('jobA', async (job) => {
    console.log(`Process started for Job ${job.id}`);
    return new Promise(resolve => {
        setTimeout(() => {
            console.log(`Process done for Job ${job.id}!`);
            resolve();
        }, 10000); // 10 seconds
    });
});

// Comment this process out to see the difference.
testQueue.process('jobB', async () => {});

// Add a job to the queue
testQueue.add('jobA', {});

process.on('SIGINT', async () => {
    console.log('Closing Queue now...');
    await testQueue.close();
    console.log('Queue closed');
    process.exit(0);
});
```

Run this code and press Ctrl + C to exit the process, within 10 seconds. Notice how the queue closes before reaching the end of the process.

## Bull version
3.12.1

## Additional information
At one point while testing this, there was two jobs being processed at the same time, and then the queue actually did close as expected. Even though there was two processes defined, which kind of contradicts my problem. Unfortunately I can't figure out how to reproduce that scenario.

## Description
I'm noticing that without any options out of the box that I'm seeing what looks to be a solid 10 second delay before my bull worker process acts on the job in the queue.

* Bull startup first job
    * **10 second delay** <-- not sure where this is coming from
    * queue active - 1579197240068
    * 6 bull based redis connections (3 client, 3 subscriber) - 3 queues
    * sending email - 1579197229548
    * queue completed - 1579197229548
* Jobs thereafter
    * add job - 1579197892741
    * queue waiting
    * queue active - 1579197892741
    * sending email - 1579197892741
    * queue completed - 1579197892741
    * queue drained

```
// Server started and running for many minutes

[bull] ⬤  debug     queue active { queue:

[bull]   job:
[bull]    { opts:
[bull]       { attempts: 1,
[bull]         delay: 0,
[bull]         timestamp: 1579198519664,
[bull]         backoff: undefined },
[bull]      name: '__default__',
[bull]      data: { template: 'verify', message: [Object], locals: [Object] },
[bull]      _progress: 0,
[bull]      delay: 0,
[bull]      timestamp: 1579198519664,
[bull]      stacktrace: [],
[bull]      returnvalue: null,
[bull]      attemptsMade: 0,
[bull]      id: '27',
[bull]      processedOn: 1579198527228,
[bull]      failedReason: undefined }

[bull] ⬤  debug     creating bull client { type: 'client',
[bull] ⬤  debug     creating bull client { type: 'subscriber',
[bull] ⬤  debug     creating bull client { type: 'client',
[bull] ⬤  debug     creating bull client { type: 'subscriber',
[bull] ⬤  debug     creating bull client { type: 'client',
[bull] ⬤  debug     creating bull client { type: 'subscriber',

[bull] ⬤  debug     redis connection established { description: '127.0.0.1:6379',
[bull] ⬤  debug     redis connection established { description: '127.0.0.1:6379',
[bull] ⬤  debug     redis connection established { description: '127.0.0.1:6379',
[bull] ⬤  debug     redis connection established { description: '127.0.0.1:6379',
[bull] ⬤  debug     redis connection established { description: '127.0.0.1:6379',
[bull] ⬤  debug     redis connection established { description: '127.0.0.1:6379',

[bull] ⬤  debug     queue drained { queue:

[bull] ⬤  debug     queue completed { queue:

[bull]   job:
[bull]    { opts:
[bull]       { attempts: 1,
[bull]         delay: 0,
[bull]         timestamp: 1579198519664,
[bull]         backoff: undefined },
[bull]      name: '__default__',
[bull]      data: { template: 'verify', message: [Object], locals: [Object] },
[bull]      _progress: 0,
[bull]      delay: 0,
[bull]      timestamp: 1579198519664,
[bull]      stacktrace: [],
[bull]      returnvalue: 0,
[bull]      attemptsMade: 0,
[bull]      id: '27',
[bull]      processedOn: 1579198527228,
[bull]      failedReason: undefined,
[bull]      finishedOn: 1579198541749 }
```

I'm having a tough time debugging this lingering startup issue. I also see from here that sometimes jobs just get stuck and never go into active state until the server is restarted.

## Bull version
^3.0.0

## Additional information


<!--
You may report several types of issues. Bug reports, enhancements or questions.
For bug reports however you are required to provice some information so that the 
issue can be resolved efficiently. The following template should be filled for bugs.

Before submitting the bug just think twice if you really need to submit the bug
or you may have some issue in your own code, remember that handling issues is
time consuming, would you better like that we spend time improving the library
or on non-issues :).
-->

## Description

Hi!

I was trying to figure out how to perform a graceful shutdown. The documentation states that when executing queue.pause(true), a promise is returned that resolves when the queue is paused. It will not process any new jobs and active jobs will continue until they complete. It relies on Queue#whenCurrentJobsFinished, which returns a promise that resolves when all jobs currently being processed by a given worker have finished.

Looking into Queue#whenCurrentJobsFinished, it appears that it does not wait for all active jobs to finish, but rather wait for a single job to complete.

```
/**
 * Returns a promise that resolves when active jobs are finished
 *
 * @returns {Promise}
 */
Queue.prototype.whenCurrentJobsFinished = function() {
  if (!this.bclientInitialized) {
    // bclient not yet initialized, so no jobs to wait for
    return Promise.resolve();
  }

  //
  // Force reconnection of blocking connection to abort blocking redis call immediately.
  //
  const forcedReconnection = redisClientDisconnect(this.bclient).then(() => {
    return this.bclient.connect();
  });

  return Promise.all([this.processing[0]]).then(() => {
    return forcedReconnection;
  });
};
```

Why not wait for all jobs in this.processing to finish before resolving?
I am currently using a concurrency value of 12, meaning that you would need to wait for at most 12 promises to resolve.

```
return Promise.all(this.processing).then(() => {
   return forcedReconnection;
});
```
This change would enable me to perform a perfect shutdown. Looking forward to hearing your thoughts!

Hi, Sorry for asking here, i'm new in Queue Bull and Nodejs

I want to add 5 second before next queue execute, but it seem immediatly executed
i think when i add delay option it can make delay between next queue

How i can achive that?

Here my code
![image](https://user-images.githubusercontent.com/56084255/72260658-ea505d00-3645-11ea-8a06-24aaffa4954f.png)
My bull repeatable job queue has stopped working twice in last two month,
and I checked the document found this "maxStalledCount" I can set.
But how do we manually test how this works?
Is there any method can stall a job?
is there any way to use the job.update inside a processor ?
The lazy client error event handler is never unregistered when the queue is closed. This becomes more problematic in a scenario in which `options.createClient` is specified.
<!--
You may report several types of issues. Bug reports, enhancements or questions.
For bug reports however you are required to provice some information so that the 
issue can be resolved efficiently. The following template should be filled for bugs.

Before submitting the bug just think twice if you really need to submit the bug
or you may have some issue in your own code, remember that handling issues is
time consuming, would you better like that we spend time improving the library
or on non-issues :).
-->

## Description
in [reference](https://github.com/OptimalBits/bull/blob/develop/REFERENCE.md#jobprogress) page said `job.progress` allows a number between 0 and 100
but actually it also allows passing object and other things


<!--
You may report several types of issues. Bug reports, enhancements or questions.
For bug reports however you are required to provice some information so that the 
issue can be resolved efficiently. The following template should be filled for bugs.

Before submitting the bug just think twice if you really need to submit the bug
or you may have some issue in your own code, remember that handling issues is
time consuming, would you better like that we spend time improving the library
or on non-issues :).
-->

## Description
If options are passed as the third argument to the queue constructor but the second argument is undefined the options are ignored. My use case is wanting to provide the redis URL via environment variable in production but connect to local redis in development.

## Minimal, Working Test code to reproduce the issue.

```
const queue = new Queue('task', process.env.BULL_REDIS_URL, {
    prefix: 'my_bull_queue'
  });
```

## Bull version
3.12.1


I have no idea why this happens, but it does.   When the Redis server gets rebooted, and your Queue loses its connection it doesn't reconnect.  We have discussed this before, you believe its an issue with IORedis.  But I am not sure now, after spending several hours playing with this I can consistently repeat the issue now. 

Some observations:

- The only method that throws an exception is queue.add
- All other methods seem to work fine, reconnect to Redis fine and return data fine.
- queue.clien.ping() returns 'PONG' fine, but queue.add fails with an exception.

I created the following rather ugly, horribly ugly code for my healthcheck as a temp workaround until I can find the root cause of this issue.  

`const job = await this.queueService.queue.add(null, {removeOnComplete: true, delay: 9999});
    await job.remove();
`
This code will ALWAYS throw an exception in this scenario, so its a pretty good check to see...

Environment:

Kubernetes cluster, I have tried the following environments and it seems to happen on each with different errors:

Single Redis Instance: Same behavior, but you get an ECCONREFUSED error from IORedis.

Sentinel Redis with 3 instances master/slave, if you kill all of them simultaneously you get ALL SENTINELS are down error from IORedis.

Both appear to behave exactly the same.  If you reboot your app, everything works again.
