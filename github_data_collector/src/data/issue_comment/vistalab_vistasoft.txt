Several changes required for the PRFmodel project:

1. Now vistasoft can handle 1D files
2. Changes in a couple of functions, making function calls explicit (instead of eval() calls). It is a requirement for deployed code
3. Changes to rmSet so that one oval gaussian returns Theta-s
Dear all,

Another question: on my Macbook Mojave, mrMesh does work, but there is an issue with transferring the ROI from the brain's surface in the Mesh window to the Gray volume window.

I figured out a seemingly easy fix, but if someone can confirm that it is a valid fix (and won't cause any problems), that would be great. 

It appears that while Volume{x} updates the Mesh number (e.g., Volume{1}.meshNum3d=2), it does not get updated in the vw variable. mrMesh tries to find the selected mesh in view, and crashes.

If I make the following change in the function meshROI2Volume, it appears to update the variable vw to Volume{1} in my case, but presumably the current volume, and the issue dissappears.
vw now also contains the substructure meshNum3d.
The change: replace "if notDefined('vw'), vw = getSelectedVolume; end" by "vw = getSelectedVolume;". (line 27). 

Does anyone see any issue with this temporary fix?

Thank you
In our lab, I need a lab laptop that can run mrVista. While I have succeeded multiple times in getting most of mrVista to run on both Windows and Ubuntu, I keep having trouble with getting mrMesh to run on Linux. This system has crashed multiple times, despite checking advice online and from personal contacts who use mrMesh as well.

On my personal Macbooks (El Capitan, Mojave), I got mrVista and mrMesh to run without any problems, but I have read about issues with, e.g., High Sierra.
I can most likely not get exactly the same specs for a new lab Macbook. 

My question to fellow Macbook users: what version of Mac OS are you using currently? Are there any known unpublished issues with the newer Mac OS systems (Mojave, Catalina?)? Do you have input on what kind of Macbook specs to look for (or watch out for)? 
Hello experts,
I use MATLAB 2019 on a windows 10 machine.
I'm working on my retinotopy project and trying to perform pRF analysis using vistasoft.
The issue happens when, opening mrVista with the inplane image mounted, I click on "Gray" | Gray/White segmentation | Install or reinstall segmentation.
I install the classification image (NIFTI) but when the process starts I get the an error that obliges me to close and restart MATLAB.
Please, have a look at the file error. It has been saved automatically by MATLAB.

[matlab_crash_dump.4044-1.txt](https://github.com/vistalab/vistasoft/files/4040538/matlab_crash_dump.4044-1.txt)

Thank you in advance for the help! I don't know how to proceed receiving this error.
Marco
I am running dtiInit on a Mac with Matlab R2019a and get several errors with the mex files when running dtiInit. I could easily resolve some my recompiling with the code suggested in dtiCompileMex, but this needs serious updating (see suggestions below) and dtiFitTensor is not running correctly. My current work around is to use the dtiInit docker (https://hub.docker.com/r/vistalab/dtiinit/), but the dtiFitTensor needs to be fixed. Am happy to troubleshoot or test.

----- necessary updates to dtiCompileMex -----
compatibleArrayDims needs to be added to the mex build code:
dtiFiberTracker.c builds & runs correctly with:
`mex -O -compatibleArrayDims -I./jama dtiFiberTracker.cxx`

dtiSplitTensor.c builds/runs correctly with:
`mex -v -compatibleArrayDims dtiSplitTensor.c`

----- error-----
dtiFitTensor.c 
It needs to have zero removed after return from line 594 to compile, but output is not sensible (zeros instead of pdd).

I've made various updates so that vistasoft can handle >1mil fibers in .tck with 30G-60G of memory. The current implementation of some diffusion related functions are very inefficient and require much larger memory than necessary. 

I've made the following updates.

1) updated external/mtrix/external code from the latest version found in mrtrix repo. I am not sure why these are hosted in vistasoft, but they are old and it was missing write_mrtrix_tracks.m altogether.
2) refactored code necessary to parse .tck headers so that this can be used to both import and export. 
3) fileFilters/mrtrix/dtiImportFibersMrtrix.m was updated to load .tck data in batch. Previously, the entire raw data was loaded all at once, before creating copies of the entire data to preprocess data. By doing these process in batch, the function now uses slightly more than the entire size of the .tck. I've also updated so that this function now load the .tck (stored in 32bit float) as single-precision array. This cuts the memory requirement by half and makes it possible to process >1million fibers in 30-60G of memory. To handle larger number of fibers, further memory optimization is necessary.
4) nearpoints.mex was updated to allow handling of both single and double precision data. I've also added various new error checking. For example, destPrt (roi coords) could be empty and nearpoint could crash. Now it displays error message.
5) dtiIntersectFibersWithRoi.m was updated to not crash if fg.fibers is empty
6) dtiRoiNiftiFromMat.m was updated to check if the input refImg is already loaded. I've also replaced the deprecated function niftiGetStruct with niftiCreate as suggested.

All of these changes should be backward compatible and I've been testing these with our various Apps, although there might be some corner cases still (I will fix them if that is the case).
This is not a bug, but I would like to share what I found in dtiInit nowadays... 

conTrack requires pddDispersion.nii.gz. When I simply used least square ("ls") for tensor fitting in dtiInit, this file is automatically created in the process. 

However, when I used RESOTRE method in dtiInit ("rt"), it seems to be that dtiRawFitTensorRobust.m did not create pddDispersion.nii.gz, because the code may not perform a bootstrapping. 

Therefore, I noticed that we can't run conTrack analysis on dt6 structure created using RESTORE algorithm. 

Nothing is wrong about this, but this could be an existing limitation of the current pipeline. 
https://github.com/vistalab/vistasoft/blob/8c402f91ff54a44b0fb532f31550952021ca5f58/fileFilters/mrtrix/dtiImportFibersMrtrix.m#L103

Hypothetically speaking, if the streamline coordinates of the input fg are not in double precision float, doesn't this double the ram usage without any associated gain in precision?

Indeed, at this point (https://github.com/vistalab/vistasoft/blob/8c402f91ff54a44b0fb532f31550952021ca5f58/fileFilters/mrtrix/dtiImportFibersMrtrix.m#L79) its actually single point precision, so it seems that this is necessarily the case.

Also, even in cases where the input is in double point precision, is that level of precision actually useful given the diffusion resolutions we are typically working with? 

Is this line necessary?
I have added a line that writes a timestamp field to the tck file. In my experience this allows visualizing the streamlines in mrview.
Dear all,
        I'm running "dtiInit"with SPM8/Matlab 2018a/latest Vistasoft release in windows10 , and having some problems in getting Brain extraction using BET, the following error occurs:
       "data will be saved to D:\data\dti_asq\cdn\sub001\dti64trilin.
Computing brain mask from average b0...
saving data as int16...

Warning: Brain extraction using BET failed- using a simple threshold method. 
> In dtiRawFitTensorMex (line 198)
  In dtiInit (line 263) 
Fitting the tensor model...
Error using dtiFitTensor
Too many input arguments.

Error in dtiRawFitTensorMex (line 241)
    [dt6,pdd,mdStd,faStd,pddDisp] = dtiFitTensor(d,X,0,bs.permutations,liberalBrainMask);

Error in dtiInit (line 263)
        dt6FileName{1} = dtiRawFitTensorMex(dwRawAligned, dwDir.alignedBvecsFile,..."

Thank you!
Best,
liyanyan