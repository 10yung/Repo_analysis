Can you please tell how doc score is calculated?
If it is tf-idf approach, cosine similarity falls between range of 0 to 1 why does doc score is very high? How can I have threshold in such cases for better retrieval of docs?
When running interactive with `numpy==1.16.3`, I encountered the following error.

It seems the `.npz` file must be loaded with numpy version `<=1.16.2`. When I installed numpy==1.16.2, the problem is gone.

Please update `requirements.txt` or update the `data/wikipedia/docs-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz` file

```
➜ python scripts/pipeline/interactive.py
12/01/2019 04:38:39 PM: [ CUDA enabled (GPU -1) ]
12/01/2019 04:38:39 PM: [ Initializing pipeline... ]
12/01/2019 04:38:39 PM: [ Initializing document ranker... ]
12/01/2019 04:38:39 PM: [ Loading /home/qibin/DrQA/data/wikipedia/docs-tfidf-ngram=2-hash=16777216-tokenizer
=simple.npz ]
Traceback (most recent call last):
  File "scripts/pipeline/interactive.py", line 70, in <module>
    tokenizer=args.tokenizer
  File "/home/qibin/DrQA/drqa/pipeline/drqa.py", line 109, in __init__
    self.ranker = ranker_class(**ranker_opts)
  File "/home/qibin/DrQA/drqa/retriever/tfidf_doc_ranker.py", line 37, in __init__
    matrix, metadata = utils.load_sparse_csr(tfidf_path)
  File "/home/qibin/DrQA/drqa/retriever/utils.py", line 36, in load_sparse_csr
    return matrix, loader['metadata'].item(0) if 'metadata' in loader else None
  File "/data/qibin/anaconda3/envs/alchemy/lib/python3.6/_collections_abc.py", line 666, in __contains__
    self[key]
  File "/data/qibin/anaconda3/envs/alchemy/lib/python3.6/site-packages/numpy/lib/npyio.py", line 262, in __g
etitem__
    pickle_kwargs=self.pickle_kwargs)
  File "/data/qibin/anaconda3/envs/alchemy/lib/python3.6/site-packages/numpy/lib/format.py", line 692, in re
ad_array
    raise ValueError("Object arrays cannot be loaded when "
ValueError: Object arrays cannot be loaded when allow_pickle=False
```
Running `python setup.py develop` on Windows 10 gives the following error:

```
Traceback (most recent call last):
  File "setup.py", line 12, in <module>
    readme = f.read()
  File "E:\WPy-3710\python-3.7.1.amd64\lib\encodings\cp1252.py", line 23, in decode
    return codecs.charmap_decode(input,self.errors,decoding_table)[0]
UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 18175: character maps to <undefined>

```

I also did the following before running it:

```
set LANGUAGE=en_US.UTF-8
set LC_ALL=en_US.UTF-8
set LANG=en_US.UTF-8
set LC_TYPE=en_US.UTF-8

```

`sys.getdefaultencoding()` and `sys.stdout.encoding` both gives:

`'utf-8'`

I am on:

WinPython 3.7.1 (v3.7.1:260ec2c36a, Oct 20 2018, 14:57:15) [MSC v.1915 64 bit (AMD64)] on win32
Windows 10 64-bit 1903


Hello,

I'm trying to run build_db.py on the nested directory of files returned by running WikiExtractor.py on the 2018 English wiki dump, however I keep gettting this error

"UnicodeDecodeError: 'utf-8' codec can't decode byte 0x86 in position 27: invalid start byte"

Could you please let me know if you know how to fix this?

Thanks!
WHEN I RUN THE COMMAND  python scirpts/pipeline/interaction.py 
I GET THE FOLLOWING ERROR

08/13/2019 11:49:58 AM: [ CUDA enabled (GPU 1) ]
08/13/2019 11:49:58 AM: [ Initializing pipeline... ]
08/13/2019 11:49:58 AM: [ Initializing document ranker... ]
08/13/2019 11:49:58 AM: [ Loading /home/zlabs-nlp/ravi/DrQA/data/wikipedia/docs-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz ]
Traceback (most recent call last):
  File "scripts/pipeline/interactive.py", line 70, in <module>
    tokenizer=args.tokenizer
  File "/home/zlabs-nlp/ravi/DrQA/drqa/pipeline/drqa.py", line 109, in __init__
    self.ranker = ranker_class(**ranker_opts)
  File "/home/zlabs-nlp/ravi/DrQA/drqa/retriever/tfidf_doc_ranker.py", line 37, in __init__
    matrix, metadata = utils.load_sparse_csr(tfidf_path)
  File "/home/zlabs-nlp/ravi/DrQA/drqa/retriever/utils.py", line 36, in load_sparse_csr
    return matrix, loader['metadata'].item(0) if 'metadata' in loader else None
  File "/home/zlabs-nlp/miniconda3/envs/drqa/lib/python3.6/_collections_abc.py", line 666, in __contains__
    self[key]
  File "/home/zlabs-nlp/miniconda3/envs/drqa/lib/python3.6/site-packages/numpy/lib/npyio.py", line 262, in __getitem__
    pickle_kwargs=self.pickle_kwargs)
  File "/home/zlabs-nlp/miniconda3/envs/drqa/lib/python3.6/site-packages/numpy/lib/format.py", line 696, in read_array
    raise ValueError("Object arrays cannot be loaded when "
ValueError: Object arrays cannot be loaded when allow_pickle=False

CAN ANYONE HELP ME WHAT THE ERROR IS 
THANKS IN ADVANCE!!!!!!!
scripts/retriever/interactive.py and scripts/reader/interactive.py all works well.
But when I try to run scripts/pipeline/interactive.py.

It shows:
python scripts/pipeline/interactive.py
04/03/2019 05:55:35 PM: [ CUDA enabled (GPU -1) ]
04/03/2019 05:55:35 PM: [ Initializing pipeline... ]
04/03/2019 05:55:35 PM: [ Initializing document ranker... ]
04/03/2019 05:55:35 PM: [ Loading /home/xuelif/Documents/NLP/DrQA/data/wikipedia/docs-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz ]
04/03/2019 05:56:28 PM: [ Initializing document reader... ]
04/03/2019 05:56:28 PM: [ Loading model /home/xuelif/Documents/NLP/DrQA/data/reader/multitask.mdl ]
04/03/2019 05:56:33 PM: [ Initializing tokenizers and document retrievers... ]
Traceback (most recent call last):
  File "scripts/pipeline/interactive.py", line 70, in <module>
    tokenizer=args.tokenizer
  File "/home/xuelif/Documents/NLP/DrQA/drqa/pipeline/drqa.py", line 146, in __init__
    initargs=(tok_class, tok_opts, db_class, db_opts, fixed_candidates)
  File "/home/xuelif/Documents/anaconda3/envs/autopaper/lib/python3.6/multiprocessing/context.py", line 119, in Pool
    context=self.get_context())
  File "/home/xuelif/Documents/anaconda3/envs/autopaper/lib/python3.6/multiprocessing/pool.py", line 175, in __init__
    self._repopulate_pool()
  File "/home/xuelif/Documents/anaconda3/envs/autopaper/lib/python3.6/multiprocessing/pool.py", line 236, in _repopulate_pool
    self._wrap_exception)
  File "/home/xuelif/Documents/anaconda3/envs/autopaper/lib/python3.6/multiprocessing/pool.py", line 255, in _repopulate_pool_static
    w.start()
  File "/home/xuelif/Documents/anaconda3/envs/autopaper/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/xuelif/Documents/anaconda3/envs/autopaper/lib/python3.6/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/xuelif/Documents/anaconda3/envs/autopaper/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/xuelif/Documents/anaconda3/envs/autopaper/lib/python3.6/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
OSError: [Errno 12] Cannot allocate memory

--------------------------------------------------------------------------------------------------------------------
P.S. I have downloaded the latest CoreNLPTokenizer from Stanford. (http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip)
Without the lastest corenlp, running scripts/reader/interactive.py will return TIMEOUT error.
python scripts/reader/interactive.py
04/03/2019 04:53:11 PM: [ CUDA enabled (GPU -1) ]
04/03/2019 04:53:11 PM: [ Initializing model... ]
04/03/2019 04:53:11 PM: [ Loading model /home/xuelif/Documents/NLP/DrQA/data/reader/single.mdl ]
04/03/2019 04:53:12 PM: [ Initializing tokenizer... ]
Traceback (most recent call last):
  File "/home/xuelif/Documents/anaconda3/envs/autopaper/lib/python3.6/site-packages/pexpect/expect.py", line 99, in expect_loop
    incoming = spawn.read_nonblocking(spawn.maxread, timeout)
  File "/home/xuelif/Documents/anaconda3/envs/autopaper/lib/python3.6/site-packages/pexpect/pty_spawn.py", line 462, in read_nonblocking
    raise TIMEOUT('Timeout exceeded.')
pexpect.exceptions.TIMEOUT: Timeout exceeded.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "scripts/reader/interactive.py", line 53, in <module>
    normalize=not args.no_normalize)
  File "/home/xuelif/Documents/NLP/DrQA/drqa/reader/predictor.py", line 84, in __init__
    self.tokenizer = tokenizer_class(annotators=annotators)
  File "/home/xuelif/Documents/NLP/DrQA/drqa/tokenizers/corenlp_tokenizer.py", line 33, in __init__
    self._launch()
  File "/home/xuelif/Documents/NLP/DrQA/drqa/tokenizers/corenlp_tokenizer.py", line 61, in _launch
    self.corenlp.expect_exact('NLP>', searchwindowsize=100)
  File "/home/xuelif/Documents/anaconda3/envs/autopaper/lib/python3.6/site-packages/pexpect/spawnbase.py", line 390, in expect_exact
    return exp.expect_loop(timeout)
  File "/home/xuelif/Documents/anaconda3/envs/autopaper/lib/python3.6/site-packages/pexpect/expect.py", line 107, in expect_loop
    return self.timeout(e)
  File "/home/xuelif/Documents/anaconda3/envs/autopaper/lib/python3.6/site-packages/pexpect/expect.py", line 70, in timeout
    raise TIMEOUT(msg)
pexpect.exceptions.TIMEOUT: Timeout exceeded.
<pexpect.pty_spawn.spawn object at 0x7fac9a550588>
command: /bin/bash
args: ['/bin/bash']
buffer (last 100 chars): b'cuments/NLP/DrQA\x07\x1b[01;32mxuelif@xuelif-HP-Z4-G4-Workstation\x1b[00m:\x1b[01;34m~/Documents/NLP/DrQA\x1b[00m$ '
before (last 100 chars): b'cuments/NLP/DrQA\x07\x1b[01;32mxuelif@xuelif-HP-Z4-G4-Workstation\x1b[00m:\x1b[01;34m~/Documents/NLP/DrQA\x1b[00m$ '
after: <class 'pexpect.exceptions.TIMEOUT'>
match: None
match_index: None
exitstatus: None
flag_eof: False
pid: 7694
child_fd: 15
closed: False
timeout: 60
delimiter: <class 'pexpect.exceptions.EOF'>
logfile: None
logfile_read: None
logfile_send: None
maxread: 100000
ignorecase: False
searchwindowsize: None
delaybeforesend: 0
delayafterclose: 0.1
delayafterterminate: 0.1
searcher: searcher_string:
    0: "b'NLP>'"


when i try to run the code im getting this error my cuda version is  10.0.130 and my pytorch version is 1.0.0a0
some one help me 
paperspace@psochxboa:~/DRQA$ sudo python scripts/pipeline/interactive.py
02/21/2019 11:17:37 PM: [ CUDA enabled (GPU -1) ]
02/21/2019 11:17:37 PM: [ Initializing pipeline... ]
02/21/2019 11:17:37 PM: [ Initializing document ranker... ]
02/21/2019 11:17:37 PM: [ Loading /home/paperspace/DRQA/data/wikipedia/docs-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz ]
02/21/2019 11:18:17 PM: [ Initializing document reader... ]
02/21/2019 11:18:17 PM: [ Loading model /home/paperspace/DRQA/data/reader/multitask.mdl ]
Traceback (most recent call last):
  File "scripts/pipeline/interactive.py", line 70, in <module>
    tokenizer=args.tokenizer
  File "/home/paperspace/DRQA/drqa/pipeline/drqa.py", line 120, in __init__
    self.reader.cuda()
  File "/home/paperspace/DRQA/drqa/reader/model.py", line 469, in cuda
    self.network = self.network.cuda()
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 258, in cuda
    return self._apply(lambda t: t.cuda(device))
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 185, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 185, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 185, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py", line 117, in _apply
    self.flatten_parameters()
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py", line 90, in flatten_parameters
    if not any_param.is_cuda or not torch.backends.cudnn.is_acceptable(any_param):
  File "/usr/local/lib/python3.6/dist-packages/torch/backends/cudnn/__init__.py", line 92, in is_acceptable
    if _libcudnn() is None:
  File "/usr/local/lib/python3.6/dist-packages/torch/backends/cudnn/__init__.py", line 58, in _libcudnn
    'but linked against {}'.format(compile_version, __cudnn_version))
RuntimeError: cuDNN version mismatch: PyTorch was compiled against 7300 but linked against 7402


thank you
While running `python3 scripts/reader/train.py  --embedding-file glove.840B.300d.txt --tune-partial 1000 --gpu 1` 

I am getting error 

`WARN: fix_embeddings set to False as tune_partial > 0.
02/20/2019 05:07:45 PM: [ COMMAND: scripts/reader/train.py --embed-dir /home/atul/DrQA/data/embeddings/ --tune-partial 1000 --gpu 1 ]
02/20/2019 05:07:45 PM: [ ---------------------------------------------------------------------------------------------------- ]
02/20/2019 05:07:45 PM: [ Load data files ]
02/20/2019 05:08:08 PM: [ Num train examples = 86832 ]
02/20/2019 05:08:12 PM: [ Num dev examples = 10570 ]
02/20/2019 05:08:12 PM: [ ---------------------------------------------------------------------------------------------------- ]
02/20/2019 05:08:12 PM: [ Training model from scratch... ]
02/20/2019 05:08:12 PM: [ ---------------------------------------------------------------------------------------------------- ]
02/20/2019 05:08:12 PM: [ Generate features ]
02/20/2019 05:08:20 PM: [ Num features = 62 ]
02/20/2019 05:08:20 PM: [ {'pos=JJR': 35, 'pos=POS': 12, 'pos=NN': 6, 'pos=:': 32, 'pos=WP$': 43, 'pos=RB': 3, 'pos=VBN': 17, 'pos=RBS': 28, 'pos=UH': 44, 'in_question': 0, 'pos=VBD': 23, 'pos=CC': 13, 'pos=JJS': 31, 'pos=.': 9, 'pos=RP': 37, 'pos=VBZ': 7, 'ner=ORGANIZATION': 54, 'pos=WRB': 22, "pos=''": 20, 'pos=WDT': 26, 'ner=MONEY': 59, 'ner=LOCATION': 51, 'tf': 61, 'pos=VBG': 14, 'pos=,': 4, 'pos=NNP': 11, 'pos=LS': 47, 'pos=NNPS': 19, 'ner=PERSON': 50, 'ner=ORDINAL': 57, 'ner=MISC': 49, 'pos=EX': 39, 'ner=TIME': 60, 'ner=DATE': 52, 'ner=DURATION': 56, 'pos=NNS': 16, 'pos=FW': 42, 'pos=TO': 21, 'pos=CD': 24, 'pos=PRP': 15, 'ner=O': 48, 'in_question_uncased': 1, 'pos=``': 18, 'ner=SET': 55, 'pos=$': 36, 'pos=PDT': 45, 'ner=NUMBER': 53, 'pos=WP': 34, 'pos=VB': 30, 'pos=VBP': 29, 'pos=JJ': 8, 'ner=PERCENT': 58, 'pos=SYM': 46, 'pos=#': 40, 'pos=PRP$': 33, 'pos=DT': 5, 'in_question_lemma': 2, 'pos=-LRB-': 25, 'pos=IN': 10, 'pos=-RRB-': 27, 'pos=RBR': 41, 'pos=MD': 38} ]
02/20/2019 05:08:20 PM: [ ---------------------------------------------------------------------------------------------------- ]
02/20/2019 05:08:20 PM: [ Build dictionary ]
02/20/2019 05:08:20 PM: [ Restricting to words in /home/atul/DrQA/data/embeddings/glove.840B.300d.txt ]
02/20/2019 05:08:45 PM: [ Num words in set = 757064 ]
02/20/2019 05:08:54 PM: [ Num words = 88431 ]
02/20/2019 05:08:57 PM: [ Loading pre-trained embeddings for 88429 words from /home/atul/DrQA/data/embeddings/glove.840B.300d.txt ]
02/20/2019 05:09:09 PM: [ WARN: Duplicate embedding found for Kṛṣṇa ]
02/20/2019 05:09:13 PM: [ WARN: Duplicate embedding found for · ]
02/20/2019 05:09:16 PM: [ WARN: Duplicate embedding found for ; ]
02/20/2019 05:09:17 PM: [ WARN: Duplicate embedding found for à ]
02/20/2019 05:09:25 PM: [ WARN: Duplicate embedding found for José ]
Traceback (most recent call last):
  File "scripts/reader/train.py", line 542, in <module>
    main(args)
  File "scripts/reader/train.py", line 402, in main
    model = init_from_scratch(args, train_exs, dev_exs)
  File "scripts/reader/train.py", line 197, in init_from_scratch
    model.load_embeddings(word_dict.tokens(), args.embedding_file)
  File "/home/atul/DrQA/drqa/reader/model.py", line 109, in load_embeddings
    assert(len(parsed) == embedding.size(1) + 1)

AssertionError
`
How to resolve this.?
Thanks
Is there anyway to run this model on low requirement machine.Or on google colaboratory.If anyone tried this or any alternative models similar to this which run's on low requirement machines.
$ python3 scripts/pipeline/interactive.py 
12/08/2018 06:03:16 AM: [ Running on CPU only. ]
12/08/2018 06:03:16 AM: [ Initializing pipeline... ]
12/08/2018 06:03:16 AM: [ Initializing document ranker... ]
12/08/2018 06:03:16 AM: [ Loading /scratch/nikita/DrQA/data/wikipedia/docs-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz ]
12/08/2018 06:03:48 AM: [ Initializing document reader... ]
12/08/2018 06:03:48 AM: [ Loading model /scratch/nikita/DrQA/data/reader/multitask.mdl ]
12/08/2018 06:03:52 AM: [ Initializing tokenizers and document retrievers... ]

Interactive DrQA
>> process(question, candidates=None, top_n=1, n_docs=5)
>> usage()

>>> salloc: Exceeded job memory limit

getting memory error 
using 20 cpu (20gb+)