We are using SparkContext throughout loaders and example pipelines. It makes sense to move these to using SparkSession given that we're relying on Spark 2.0.
The Least Squares Estimator should have the option to use a local least squares solver.

A cost model needs to be built for it, and it needs to be added to the possible options.

Currently transformers are maps from  `A => B`, but if I have some complex type like `(Int, Image)` I can't pass it into an `Image => DenseVector[Double]` Transformer and expect it to do the right thing. We can probably come up with some implicit magic that allows us to apply Transformers to objects like this.

Similar to #249 , this came up in the context of data augmentation in a discussion with @shivaram today.

In addition to the gather operator which concatenates horizontally, it would be nice to have a union operator which concatenates veritcally.

This came up in a discussion with @shivaram today around data augmentation.


We often perform a sweep over lambdas to figure out the right regularization -- it matters a lot. 

@shivaram mentioned that it's possible for the weighted solver to potentially solve for a large number of lambdas with minimal overhead. There was evidently some code in a repo associated with a paper that is an example of this. It might be possible (per Shiv's suggestion) that @tomerk could hack on this? 

Currently so as to be easily chainable with the rest of the code, block operators (such as block solves and block transformers) take a single complete RDD and manually split it into multiple blocks in a way that is hidden from the DAG.

If we add some DAG rewriting rules to detect this and integrate block operators better with the DAG, we should be able to take advantage of optimizations like auto-caching more effectively, and we can allow the block operators to operate on blocks lazily.

Both SIFT and Daisy feature extractors output a `DenseMatrix` of a special shape that contain a bag of image descriptors. These image descriptors also have metadata with them (e.g. original positions) which would allow for things like spatial pooling that we currently discard.

We should create a class for `ImageDescriptor`s that captures this metadata. One downside is that currently things like PCA, GMM, and FV rely on the results of SIFT or Daisy being DenseMatrix, and we'd have to rethink the types there to handle this change in a clean way.

We don't necessarily iterate over images in a couple of common cases in a fast way, e.g.

``` scala
while (x < numRows) {
  while (y < numCols) {
    while(c < numChannels {
      out(x,y,c) = img.get(x,y,c)+1.0
    }
  }
}
```

Is only fast if the image is in channel/column major order (because of cache-locality). This slows down the Pooler and the Convolver by up to an order of magnitude in the case that their inputs are not in exactly the right order.

One fix would be to come up with a nice `Iterator` interface that lets you iterate over images and get back an `Tuple4[Int,Int,Int,Double]` of the image in "natural" order with position and pixel value We need to figure out a good way to do this efficiently. One potential way to do this is to make the iterator `@specialized` on tuples of this sort.

Let's see if we can rewrite the pooler and the convolver to take advantage of an interface like this and push both to respectable FLOP/Memory Throughput levels regardless of Image Representation used.

