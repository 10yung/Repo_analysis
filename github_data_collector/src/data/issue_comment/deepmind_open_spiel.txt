A natural extension of AlphaZero is [MuZero](https://arxiv.org/abs/1911.08265). The pseudocode implementation is very similar to that of AlphaZero, so it would be interesting to extend the AlphaZero implementation #134 to also support MuZero.

Is there any interest in having a MuZero implementation in OpenSpiel? 
This AlphaZero implementation has many design issues that need to be resolved before finalizing the PR (eg. there are no tests, and input validation is not production-ready yet). It trains well on tic-tac-toe (see `examples/tic_tac_toe_alpha_zero.py`), giving confidence that the implementation is correct.

The philosophy of this implementation is to follow the pseudocode of the AlphaZero [Science paper](https://science.sciencemag.org/content/362/6419/1140) as closely as possible, and also fit it into the current OpenSpiel framework with minimal framework changes. But here are some differences with the pseudocode:
- It uses very large batch sizes (`>4000`) and uses gradient accumulation to allow this to fit into memory. However, the AlphaZero ResNet architecture uses batchnorm, which doesn't play well with gradient accumulation. So I've avoided this
- I've used ADAM as the default optimizer, whilst the pseudocode uses SGD with momentum and effectively a `tf.train.piecewise_constant` learning rate scheduler. I've found this very annoying to use for adapting to new games as there are now some extra fiddly hyperparameters that need tweaking.
- The loss was not scaled by the batch size in the pseudocode. This makes it harder to change the batch size as other hyperparameters are now coupled to it (eg learning rate, l2 regularization). So this implementation does normalize by batch size.
- It doesn't support non-vector actions representations (eg what they did for Chess). But the paper does also mention that `We also tried using a flat distribution over moves for chess and shogi; the final result was almost identical although training was slightly slower`. So left this feature out for now.
- Missing support for `max_moves`: this seems like a feature to add to the Game class, but it currently seems hardcoded?
- No clean way of handling almost-Markovian environments (eg Chess and Go) by training on some context window of previous observations. This might be best handled at the game-level? (or am I missing some library feature?)

Some design explanations and issues:
- The main framework-level change is to introduce a `TrainableEvaluator` class that acts like the `Evaluator` class, but also has an `update` method with training logic (doesn't have to be neural net training). It also builds caching into the design due to how common using nets with both value and policy heads is (so subclasses would only implement the `value_and_prior` method).
- More on caching in `mcts.TrainableEvaluator`: The current design of `mcts.Evaluator` is that `evaluate` and `prior` are called separately during mcts search. In the case where these are (expensively) computed jointly as in AlphaZero, this is inefficient. I am well-aware of the general injunction that `OpenSpiel provides reference implementations that are used to learn from and prototype with, rather than fully-optimized / high-performance code`. So I am aware that this caching mechanism might be contentious, but self-play is the bottleneck and makes AlphaZero a lot less pleasant to use.
- The AlphaZero training loop: this implementation ties the number of training iterations to the current buffer size (via `num_training_epochs` in `AlphaZero.update`). This has some virtues: the epoch number to train over is a much more game-independent hyperparameter, and makes more sense than the AlphaZero pseudocode approach (do a huge number of training iterations even when the buffer hasn't been filled up yet).  One option I considered first is to make `AlphaZero.update` do a single training iteration, which gives most flexibility to users. But: the optimizer needs to be reset after all training iterations are done. The user could call a reset method, which is ugly and easy to forget to do. Otherwise, the optimizer reset could be done in `AlphaZero.self_play`. But this side effect of `AlphaZero.self_play` also feels a bit hacky. I'm definitely not confident I've chosen the right design here.
- Device flags for NN: I didn't see any other examples of GPU support in OpenSpiel. So went ahead and allowed the strings `'gpu'`, `'cpu'`, or a `tf.device(...)` object (useful to specify more exotic things, like a particular GPU on a multi-GPU machine. Or maybe some other device type like TPU). 
- Neural nets. In all other algorithms that use neural nets, there is a lack of flexibility in specifying architectures. This could well be intentional (makes it easier to write backend independent implementations as there is a nice backend agnostic interface with only knobs like `hidden_layers_sizes` that can be tweaked). I've gone against this, and allowed any Keras model to be supplied to `AlphaZeroKerasEvaluator`, along with allowing flexible feature extraction. I've also focused on Keras, as its official high-level TF 2.0 API (I assume OpenSpiel will move to TF 2.0 at some stage).
- AlphaZero can probably be extended to more than 2 agents (and the MCTS implementation already supports that). But that can be a future extension, as this implementation only supports 2-player games.

As a devoted gin player, I’d like to submit this gin rummy implementation to OpenSpiel’s game collection. 

Gin is the best representative of the rummy family of games. It’s quite a large game with over 10^85 info states and 10^9 states per info state off the deal. Gin also has two qualities which distinguish it from hold ‘em poker: hidden information is regularly added throughout the game (not just on the deal), and it has a much larger disambiguation factor. 

I’ve made every effort to ensure the correctness of the code. The test file is extensive, and should be instructive to a non gin rummy player. I’ve carefully played through many games myself. And further, I wrote up a simple bot that employs a reasonably effective strategy, which successfully played two million games.

Hi there,
don't know if this is the right place to ask this but yet I don't have any other Contact Details so I'm trying it this way.

Yesterday I got an idea about an algorithm for solving imperfect games. Basically it it a cross between deep_ctr and exploitability decent. The main idea behind it is to train a deep Policy network directly with exploitability decent. I made the implementation yesterday and well here it is:

https://github.com/dennisjay/open_spiel/blob/feature/nn_exploitablit_decent/open_spiel/python/algorithms/deep_exploitability_descent.py

This works very well for Kuhn poker and the exploitability reaches near nash equilibrum (<0.001). But for leduc poker convergence is very slow and gets stuck at approx. 1.3 . Do you have any tips how to improve my algorithm that I can contribute it to open_spiel.

Best regards
Dennis



I am interested in applying, and contributing to, this framework and have some questions and remarks!

My background: I am a data scientist, app developer and a boardgame enthusiast. As a hobby project, last year I have  implemented a web-app, an online version of a multiplayer strategic boardgame, and now I am think about developing an AI for it, so players can choose to play against AI bots instead of against other players. I might implement more games in the future as well. I guess this framework is a great starting point for me. 

Some information on the game. The game is named Ponzi Scheme (more info on: [https://www.boardgamegeek.com/boardgame/180899/ponzi-scheme]()). Some key characteristics:  economic trading game, hidden actions, a few chance events, three or more players.

The implementation of the game is in Python (on the the server side). I think it won't be too difficult to adapt the implementation to the openspiel API. Most important change will be handling the chance events, i.e. adding a chance player instead of handling chance events as game state changes as a result of player actions. 

First question: is there an interest that I add this game to this github? If so, I am happy to do so. I have understood the games here are implemented both in Python in C++. Is a C++ implementation mandatory? Also happy to do that, but it will take time.

I am planning to try different algorithms but maybe someone can give some advice. I am familiar with some algorithms like MCTS, minmax, DQN. But several others in the list here are rather new to me. Which algorithms are most suited for my type of game (i.e. 3+ players, hidden info).

The feature I am most concerned about is the hidden information. I see there are several games with hidden information implemented here, but as far as I can tell most of them deal with information which is hidden at the start of a playthrough (like dealing cards face down), and is possibly revealed during the playthrough. In my game, some actions are hidden to some players (players may make secret trades with each other; the amount of money involved is hidden to players not involved in the trade). So during the whole playthrough there is basically more and more information hidden. In the games listed here, I think only phantom tic-tac-toe has hidden actions, but maybe I have overlooked one or the other game with that characteristic. Anyway, which algorithms are suitable for games with hidden actions?


Unfortunately we cannot import a fix to the current NeuRD (see ( see https://arxiv.org/abs/1906.00190 ) implementation, see https://github.com/deepmind/open_spiel/pull/117 for details.

I'm adding this issiue a reminder to implement a sample-based NeuRD  along with the rest of the policy gradient variants in `policy_gradients.py`.

(If anybody wants to do this, please let us know. Otherwise, I'll add it eventually.)

Hello. When I try to run the Build_and_run_tests.sh file, it could Link and built all the target,but there is a Failed in "Performing Test CMAKE_HAVE_LIBC_PTHREAD, Looking for pthread_create in pthreads - not found".And when Test all the target tests, some test occor the massage," ImportError: /XXX/Deepmind/open_spiel/open_spiel/build/python/pyspiel.so: undefined symbol: PyThread_tss_set ". Below is the building message:

+ CXX=g++
+ NPROC=nproc
+ [[ linux-gnu == \d\a\r\w\i\n* ]]
++ nproc
+ MAKE_NUM_PROCS=56
+ let 'TEST_NUM_PROCS=4*56'
++ python3 -c 'import sys; print(sys.version.split(" ")[0])'
+ PYVERSION=3.6.7
+ BUILD_DIR=build
+ mkdir -p build
+ cd build
+ echo 'Building and testing in /XXX/Deepmind/open_spiel/open_spiel/build using '\''python'\'' (version 3.6.7).'
Building and testing in /XXX/Deepmind/open_spiel/open_spiel/build using 'python' (version 3.6.7).
+ cmake -DPython_TARGET_VERSION=3.6.7 -DCMAKE_CXX_COMPILER=g++ ../
-- The C compiler identification is GNU 4.8.5
-- The CXX compiler identification is GNU 7.3.0
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: /public/software/gcc-7.3/bin/g++
-- Check for working CXX compiler: /public/software/gcc-7.3/bin/g++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed
-- Looking for pthread_create in pthreads
-- Looking for pthread_create in pthreads - not found
-- Looking for pthread_create in pthread
-- Looking for pthread_create in pthread - found
-- Found Threads: TRUE
-- Found Python3: /***/software/anconda3/lib/libpython3.7m.so (found version "3.7.1") found components:  Development
-- Configuring done
-- Generating done
What should I do for this problem?
I saw that a games wrapper for the Ludii General Game System was something that you were looking to add in contributing.md.  It looks like Ludii is written in Java.  Was the idea to be able to call the Ludii jar from C++ in order to interact with it?
First of all, thanks for making this framework open source.

I’m investigating the possibility of making a (simplified) alphaZero implementation using openspiel, and I was looking for some implementation ideas, especially since you already mention this in the contributors guide.

Please note: I am not sure if I will have the time to make the code up to openspiel standards. Also I might not very closely follow the alphazero pseudo-code. Thus, I am uncertain sure whether this effort will eventually result in a pull request. I still think some pointers would be very helpful, since others might be going to work on similar algorithms.

Implementation wise, it seems most logical to me to create an _rl_agent_ implementation called alphaZero. When taking a _step_ however; the agent will perform a MCTS. To do this, a complete game state will have to be reconstructed. The easiest way to do this would be to pass the current environment state as an argument to the _step_() function of the _rl_agent_ and then creating a game with this _state_ internally in the _rl_agent_. This feels  hacky to me: instead of using the _time_step_ argument, which was seemingly designed to provide all available information to the agent, you are feeding it extra with the full game state (of course, in a perfect information game this would be available already anyways). 

What would be your perspective on this topic?
Of course, any other design advice on the implementation would be very welcome as well. 



**tl;dr:** How to do state reconstruction in rl_agent? Do you have design advice on the implementation of an alphaZero-like algorithm?
Thanks for open sourcing open_spiel. I love the aim for completeness in this space!

One thing that I noticed is missing is games with continuous action-spaces and related algorithms (such as Deterministic Policy Gradient). Are there any plans to add support for those?

I'm especially interested in MARL and equilibrium learning in auction games.
As an example, I looked at the included implementation of the discretized first-price sealed-bid auction with integer valuations and actions. On the one hand, continuous-actions are necessary when studying emergent MARL-behavior: While a discrete implementation is pretty straight-forward for symmetric-uniform valuation-distributions, it become a lot less meaningful in settings with asymmetric or non-uniform (e.g. Gaussian) priors, where equilibria are nonlinear.
On the other hand, more involved auction games, that are being studied in economics, simply become all but intractable in a discrete implementation: Take, e.g. combinatorial FPSB auctions, where bundles of multiple items are sold to bidders at the same time. Even in a continuous-action implementation, the representation size of the action space already grows exponentially in the number of items; but in a discrete implementation it grows double-exponentially.

I'm sure there's many other use cases, e.g. in optimal control.