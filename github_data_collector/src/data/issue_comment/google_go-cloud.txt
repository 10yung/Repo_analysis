### Is your feature request related to a problem? Please describe.

I am uploading public files to buckets using blob api. I would like to get the https url (not signed url) for the object from api call.

Fixes #2725.

As described in the bug, a batched set of actions with multiple Create actions would drop all but one of the Creates. This is because an internal grouping operation used to order actions so that Gets and Writes for the same key are in an appropriate order uses `Key` as a map key. Create actions may have a `nil` Key, so only one survived.

To fix this, keep a separate slice of actions with `nil` Keys during the grouping, and append them to the "write" group at the end.
### Describe the bug

Multiple documents cannot be created within single ActionList, only one of the documents would be created. 

### To Reproduce

Steps to reproduce the behavior.

```golang
package docstorecreate

import (
	"context"
	"testing"

	"github.com/stretchr/testify/assert"

	"gocloud.dev/docstore/memdocstore"
)

type Doc struct {
	ID      string
	Payload string
}

func TestMemDocstore(t *testing.T) {
	ctx := context.Background()

	coll, err := memdocstore.OpenCollection("ID", nil)
	assert.NoError(t, err)

	in1 := Doc{Payload: "one"}
	in2 := Doc{Payload: "two"}

	err = coll.Actions().Create(&in1).Create(&in2).Do(ctx)
	assert.NoError(t, err)
	assert.NotEmpty(t, in1.ID)
	assert.NotEmpty(t, in2.ID)
}

```

## Expected behavior

All documents would be created. 

### Version

v0.18.0

### Additional context

The root cause is implementation of [`GroupActions`](https://github.com/google/go-cloud/blob/master/docstore/driver/util.go#L58) function that maps write ops under a key. When documents are provided for `Create` operation, all keys are empty, and therefore only one of the documents would be created. 

This is generic bug that happens for all docstore drivers. 
Please use a title starting with the name of the affected package, or \"all\",
followed by a colon, followed by a short summary of the feature request.
Example: `blob/gcsblob: add support for more blobbing`.

### Is your feature request related to a problem? Please describe.

We would like to use in-memory blob store but something shared across all the Kubernetes replicas of my micro-service.

### Describe the solution you'd like

A blob store interface which stores data in Redis with LRU or time based eviction.

### Describe alternatives you've considered

Write my own caching interface.




[example.zip](https://github.com/google/go-cloud/files/3952167/example.zip)

### Is your feature request related to a problem? Please describe.

The docstore API cannot be used with Google Cloud Firestore in Datastore mode.
The `docstore/gcpfirestore` driver is not sufficient for this -- trying to results in an error:
```
2019/12/11 19:51:32 could not save foo: docstore (code=FailedPrecondition): rpc error: code = FailedPrecondition desc = The Cloud Firestore API is not available for Datastore Mode projects.
```

See the attached example program.
It uses GCP application default credentials.
It expects a GCP project name in the `GCP_PROJECT_NAME` environment variable.

### Describe the solution you'd like

A driver implementation for Google Cloud Firestore in Datastore mode.

### Describe alternatives you've considered

Using a different backing service or cloud provider.

### Additional context

The GCP Firestore documentation mentions Native Mode has a [limit of 10k writes/s](https://cloud.google.com/firestore/docs/firestore-or-datastore#feature_comparison).
This makes it unsuitable for write-heavy workloads at certain scales.
While Docstore Mode does not have an exact figure for it's upper writes/s limit it indicates the limit isn't as low.
### Describe the bug

When attempting to perform a Copy() operation on a blob in an S3 bucket which is larger than the maximum size of 5GB for a single copy operation (see https://docs.aws.amazon.com/AmazonS3/latest/API/API_CopyObject.html), the following error message is displayed.

```
The specified copy source is larger than the maximum allowable size for a copy source: 5368709120
```

### To Reproduce

Place a large file (>5GB) into a bucket and attempt to Copy() it.

## Expected behavior

The object should copy without issue, regardless of size.

### Version

commit: a68836e8e108ad55d26e8e2d21579028090c8aa5

### Is your feature request related to a problem? Please describe.

I'm using the blob List functionality, but it returns an iterator that is a little awkward to use.

In the standard library, we see two examples of iterators: https://golang.org/pkg/database/sql/#Rows and https://golang.org/pkg/bufio/#Scanner. To use them, you structure a loop like so (pseudocode):

```go
for scan.Next() {
    scan.GetNextItem()
}
if scan.Err() {
    // handle error
}
```

The ListIterator, on the other hand, forces you to write a loop with breaks and extra, confusing error handling:

```go
for {
    item, err := iter.Next(ctx context.Context)
    switch err {
    case io.EOF:
        break
    case nil:
        // use the item
    default:
        // in the default case, handle the error; weird!
}
```

With this pattern, you have to handle `io.EOF`, even though we aren't doing IO (at least, not directly) and it isn't an actual error, and we have to write this sort of deeply nested code in a switch or `if.. else if.. else` statement, which is awkward to write and read.

### Describe the solution you'd like

Switch to use the more standard iterator pattern. The main disadvantage as far as I can see is you can't pass a separate context for each call - but I'm not sure why you would want to do that. You don't even know if it is making a network call, so it doesn't really make sense to have a separate context each time. Not specifying a context for each call would also make this match up better with other blob functionality. Writers and Readers in this package use one context for their whole lifetimes, for example.

### Describe alternatives you've considered

It looks like this project is not at 1.0 yet, and I believe you can make this change without breaking. (by just adding the functionality). But, if you don't want to make the change, it would be nice to give people an example of how to write an appropriate `for` loop.

### Additional context
Make sure this is working:
https://github.com/google/go-cloud/blob/master/docstore/gcpfirestore/fs.go#L94

to set the HTTP header correctly.

Secrets version is here:
https://github.com/google/go-cloud/blob/master/secrets/gcpkms/kms.go#L59


### Is your feature request related to a problem? Please describe.

We hit a limitation when processing messages from an AWS SQS FIFO queue with message grouping.

A single batch can contain messages for multiple groups and if a message for one group has a failure there is no way to reject/nack all the messages from that group for the batch. The `Subscription` interface only speaks in messages, not batches.

This meant we were unable to use the `pubsub` package without resorting to limiting batch sizes to 1 message.

### Describe the solution you'd like

Given the description of the `pubsub` package only really mentions "interact[ing] with publish/subscribe systems" maybe a `queue` package for interacting with message queuing systems is more appropriate? The difference being that messages would be persisted, maybe delivered in sequence and/or via streams. This might also apply to `nats-streaming`, `kafka` and others.

### Describe alternatives you've considered

Alternatively, some additional options or interface methods for greater control of how messages are consumed would help but on the surface that feels like it edges towards a mixing of responsibilities.

### Additional context

[This issue](https://github.com/google/go-cloud/issues/1559) might be loosely related in that it talks about "streaming" topics and queues.

EDIT: grammar, formatting.
**pubsub/natspubsub: add nats streaming support**

Fixes #1559 

Adds support for NATS Streaming Server.
