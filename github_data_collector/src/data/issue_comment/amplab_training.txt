Hello, 
On page "Introduction to Scala", command "Source.fromFile(xxx).getLines.toArray" threw an error at me "java.nio.charset.MalformedInputException: Input length = 1". It is an easy fix by setting the encoding to UTF-8. Please consider updating the command on the tutorial page. Thanks!  
      Source.fromFile(xxx)("UTF-8").getLines.toArray

Thanks, 
Zilu 
I followed all the instructions up to step 4 (http://ampcamp.berkeley.edu/3/exercises/launching-a-bdas-cluster-on-ec2.html)  and i already have my key pair on my AWS console as instructed. First i ran this :

`./spark-ec2 -i ampcamp.pem -k ampcamp --copy launch amplab-training 
`
I got the following error: InvalidKeyPair.NotFound. Then i changed my region like this:

`./spark-ec2 -i ampcamp.pem -k ampcamp -r us-west-2 --copy launch amplab-training
`
Now i have the following error: InvalidAMIID.NotFound

Any suggestions?

Following the steps mentioned on - http://ampcamp.berkeley.edu/3/exercises/realtime-processing-with-spark-streaming.html
For Java

Edited the Tutorial.java file along with twitter.txt (setting the corresponding credentials)
However while running the command - 

```
sbt/sbt package run
```

Following error - 

```
: Retrying connect to server: ip-172-31-22-114.ec2.internal/172.31.22.114:9000. Already tried 9 time(s).
[error] (run-main) java.net.ConnectException: Call to ip-172-31-22-114.ec2.internal/172.31.22.114:9000 failed on connection exception: java.net.ConnectException: Connection refused
java.net.ConnectException: Call to ip-172-31-22-114.ec2.internal/172.31.22.114:9000 failed on connection exception: java.net.ConnectException: Connection refused
    at org.apache.hadoop.ipc.Client.wrapException(Client.java:1099)
    at org.apache.hadoop.ipc.Client.call(Client.java:1075)
    at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)
    at com.sun.proxy.$Proxy3.getProtocolVersion(Unknown Source)
    at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:396)
    at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:379)
    at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:119)
    at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:238)
    at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:203)
    at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:89)
    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1386)
    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1404)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)
    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:187)
    at spark.SparkContext.setCheckpointDir(SparkContext.scala:829)
    at spark.streaming.StreamingContext.checkpoint(StreamingContext.scala:171)
    at spark.streaming.api.java.JavaStreamingContext.checkpoint(JavaStreamingContext.scala:589)
    at Tutorial.main(Tutorial.java:41)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
Caused by: java.net.ConnectException: Connection refused
    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
    at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
    at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:489)
    at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:434)
    at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:560)
    at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:184)
    at org.apache.hadoop.ipc.Client.getConnection(Client.java:1206)
    at org.apache.hadoop.ipc.Client.call(Client.java:1050)
    at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)
    at com.sun.proxy.$Proxy3.getProtocolVersion(Unknown Source)
    at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:396)
    at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:379)
    at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:119)
    at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:238)
    at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:203)
    at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:89)
    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1386)
    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1404)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)
    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:187)
    at spark.SparkContext.setCheckpointDir(SparkContext.scala:829)
    at spark.streaming.StreamingContext.checkpoint(StreamingContext.scala:171)
    at spark.streaming.api.java.JavaStreamingContext.checkpoint(JavaStreamingContext.scala:589)
    at Tutorial.main(Tutorial.java:41)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
java.lang.RuntimeException: Nonzero exit code: 1
    at scala.sys.package$.error(package.scala:27)

```

Steps followed
1. Access key ID and Secret Access Key generated for AWS
2. Environment variable set
3. Git clone done
   4.

```
training-scripts]# ./spark-ec2 -i /home/chai/Downloads/td-key-pair-virgina.pem -k td-key-pair-virginio --copy launch amplab-training
[Errno 20] Not a directory

```

It is unable to find the pem file despite giving it the right directory path for the same.

Hi,
I implemented the exercise provided at [spark mllib training](http://ampcamp.berkeley.edu/big-data-mini-course/movie-recommendation-with-mllib.html). But in final part, it is recommended to implement a matrix factorization to improve the algorithm. I could not find any example to do that. Is there anyone who can show me a way to handle this?

Hi I would like to run the training exercises on a Google Compute Engine cluster as I don't have an account on Amazon AWS. I was able to copy the wikipedia pagecounts data successfully to Google Compute Engines equivalent of S3 but I noticed that the data was enhanced to insert the date stamp as the 1st field in the input files. Can you provide me with a pointer to the code that you used to do this, or show me where I can copy the modified pagecounts data from ? 
I copied the raw data from here: 
http://dumps.wikimedia.org/other/pagecounts-raw/2009/

Any help you can provide would be much appreciated.


Running through the exercise code, here are some issues I found:

Data Exploration using Spark SQL page:
1) "parquetFile" has been deprecated and the resulting code should be changed to
 wikiData = sqlCtx.read.parquet("data/wiki_parquet")

Explore In-Memory Data Store Tachyon page:
1) the "tachyon" folder is now a subfolder of spark
2) TACHYON_WORKER_MEMORY_SIZE is already set at 1GB
3) When I try to format the storage using the command "tachyon format", class tachyon.Format cannot be found:
to fix:

```
  export TACHYON_JARS="$TACHYON_HOME/../lib/tachyon-assemblies-${VERSION}-jar-with-dependencies.jar"
```

4) the command "tachyon runTests" fails all the tests
5) In the section "Run Spark on Tachyon", the command " ./bin/spark-shell" is specific to only Scala. Should be generalized for users using other languages, e.g. Python

Querying compressed RDDs with Succinct Spark page:
1) Correct "articleIds.count" to say "articleIdsRDD.count"
2) "val succinctWikiKV = wikiKV.map(t => (t._1, t._2.getBytes).succinctKV" is missing an ending parentheses, i.e. ")".
3) Should combine 

```
val wikiKV2 = sc.textFile("data/succinct/wiki-large.txt")
    .map(_.split('|'))
    .map(t => (t(0).toLong, t(1)))
```

into one line 

```
val wikiKV2 = sc.textFile("data/succinct/wiki-large.txt").map(_.split('|')).map(t => (t(0).toLong, t(1)))
```

4) Change 

```
val wikiSuccinctKV2 = sc.succinctKV[Long]("data/succinct/succinct-wiki-large")
wikiSuccinctKV2.count
```

to

```
val succinctWikiKV2 = sc.succinctKV[Long]("data/succinct/succinct-wiki-large")
succinctWikiKV2.count
```

5) Change "val articleIdsRDD3= succinctWikiKV3.regexSearch("(stanford|berkeley)\.edu")" to "val articleIdsRDD3= succinctWikiKV2.regexSearch("(stanford|berkeley)\.edu")"

Hello,

On https://www.cs.berkeley.edu/~jey/ampcamp6/training/data-exploration-using-spark.html, the pyspark link seems to be broken on section 5. (the java/scala one doesn't load or is very slow as well).

Thanks,
N

http://www.cs.berkeley.edu/~jey/ampcamp6/training/data-exploration-using-spark-sql.html

The exercise at the end ("How many articles contain the word “california”?") requires you to use the "text" field of `wikiData`. Unless there was some explanation of the schema I missed earlier, it's a confusing exercise to do since the "text" field hasn't been mentioned anywhere. It would also suffice to mention some way to explore the schema (e.g., `wikiData.schema.fields`).
