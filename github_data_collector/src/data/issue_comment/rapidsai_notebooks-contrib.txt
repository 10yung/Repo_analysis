This notebook shows how to pass data between `__cuda_array_interface__` supporting libraries (CuPy and Numba, for this demonstration) and both PyTorch and TensorFlow. When using PyTorch, we can simply call:

`torch.as_tensor(foo)` on an existing array that accepts the `__cuda_array_interface__` but for TensorFlow, we leverage the nascent `tfdlpack` package described in [this RFC](https://github.com/tensorflow/community/pull/180).

There is a bug in tfdlpack.to_dlpack() that is documented here: https://github.com/VoVAllen/tf-dlpack/issues/12 and also in this notebook.

This PR adds to notebooks-contrib the ability for developers and/or automation to execute notebooks as benchmark scripts using ASV where benchmarks are indexed by conda build of RapidsAI dependencies.

Usage explained in benchmarks/README.md

Some notebook changes were necessary to enable nbconvert to generate a module that could be run by ASV.
 
weather notebook uses os api calls rather than shelling out to bash
                 uses os.path for path manipulations
                 uses environment variable for base data directory

* This is on hold (WIP) until the condarepo plugin (dependency) finds a public home
Fixes #258 

- [ ] getting_started_notebooks/06_Introduction_to_Supervised_Learning.ipynb - partially blocked by https://github.com/rapidsai/cuml/issues/1501
- [x] getting_started_notebooks/02_Introduction_to_cuDF.ipynb **(cuDF 0.11 tuple changes and update notebook text)**
- [ ] cuml_benchmarks **(AlgorithmPair set up changes)** - awaiting @cjnolet 
- [ ] replace old cuml_quick_benchmarks with working version of cuml_benchmarks **(AlgorithmPair set up changes and removing larger files for smaller memory cards)** - will created after above is complete by @cjnolet 
- [x] Remove cuDatashader.ipynb **(Obsolete now that cuDataShader is in Datashader)**

- [x] Outside of Notebooks Contrib, update **10 minutes to cuDF** with the tuple changes - created, fixed, and PRed, awaiting merge: https://github.com/rapidsai/cudf/pull/3618
A few bugs emerged from CI.  We need to update both the `branch-0.11` and `branch-0.12` branches.

1) getting_started_notebooks/06_Introduction_to_Supervised_Learning.ipynb
```
06_Introduction_to_Supervised_Learning-test.py in <module>
    338 n_neighbors = 3
    339 
--> 340 distances, indices = knn.kneighbors(X_df, k=n_neighbors)
    341 
    342 

cuml/neighbors/nearest_neighbors.pyx in cuml.neighbors.nearest_neighbors.NearestNeighbors.kneighbors()

TypeError: kneighbors() got an unexpected keyword argument 'k'
``` 

2) getting_started_notebooks/02_Introduction_to_cuDF.ipynb
```
TypeError                                 Traceback (most recent call last)
/opt/conda/envs/rapids/lib/python3.6/site-packages/cudf/core/column/column.py in as_column(arbitrary, nan_as_null, dtype, name)
   1384             data = as_column(
-> 1385                 memoryview(arbitrary), dtype=dtype, nan_as_null=nan_as_null
   1386             )

TypeError: memoryview: a bytes-like object is required, not 'tuple'

During handling of the above exception, another exception occurred:

ArrowInvalid                              Traceback (most recent call last)
/opt/conda/envs/rapids/lib/python3.6/site-packages/cudf/core/column/column.py in as_column(arbitrary, nan_as_null, dtype, name)
   1401                 data = as_column(
-> 1402                     pa.array(arbitrary, type=pa_type, from_pandas=nan_as_null),
   1403                     dtype=dtype,

/opt/conda/envs/rapids/lib/python3.6/site-packages/pyarrow/array.pxi in pyarrow.lib.array()

/opt/conda/envs/rapids/lib/python3.6/site-packages/pyarrow/array.pxi in pyarrow.lib._sequence_to_array()

/opt/conda/envs/rapids/lib/python3.6/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()

ArrowInvalid: Cannot mix NumPy dtypes int64 and datetime64

During handling of the above exception, another exception occurred:

ArrowInvalid                              Traceback (most recent call last)
/tmp/02_Introduction_to_cuDF-test.py in <module>
    169 
    170 
--> 171 df = cudf.DataFrame([('id', ids), ('timestamp', timestamps_np)])
    172 print(df)
    173 

/opt/conda/envs/rapids/lib/python3.6/site-packages/cudf/core/dataframe.py in __init__(self, data, index, columns, dtype)
    183 
    184             for i, (col_name, series) in enumerate(data):
--> 185                 self.insert(i, col_name, series, forceindex=index is not None)
    186 
    187         self._add_empty_columns(columns, index)

/opt/conda/envs/rapids/lib/python3.6/site-packages/cudf/core/dataframe.py in insert(self, loc, column, value, forceindex)
   1557             )
   1558         self._cols[column] = self._prepare_series_for_add(
-> 1559             value, forceindex=forceindex, name=column
   1560         )
   1561         keys = list(self._cols.keys())

/opt/conda/envs/rapids/lib/python3.6/site-packages/cudf/core/dataframe.py in _prepare_series_for_add(self, col, forceindex, name)
   1516         if (not SCALAR) and (name is None) and hasattr(col, "name"):
   1517             name = col.name
-> 1518         series = Series(col, name=name) if not SCALAR else col
   1519         self._sanitize_columns(series)
   1520         series = self._sanitize_values(series, SCALAR)

/opt/conda/envs/rapids/lib/python3.6/site-packages/cudf/core/series.py in __init__(self, data, index, name, nan_as_null, dtype)
    122         if not isinstance(data, column.TypedColumnBase):
    123             data = column.as_column(
--> 124                 data, nan_as_null=nan_as_null, dtype=dtype, name=name
    125             )
    126 

/opt/conda/envs/rapids/lib/python3.6/site-packages/cudf/core/column/column.py in as_column(arbitrary, nan_as_null, dtype, name)
   1414                     data = as_column(
   1415                         np.array(arbitrary, dtype=np_type),
-> 1416                         nan_as_null=nan_as_null,
   1417                     )
   1418     if hasattr(data, "name") and (name is not None):

/opt/conda/envs/rapids/lib/python3.6/site-packages/cudf/core/column/column.py in as_column(arbitrary, nan_as_null, dtype, name)
   1224             data = datetime.DatetimeColumn.from_numpy(arbitrary)
   1225         elif arbitrary.dtype.kind in ("O", "U"):
-> 1226             data = as_column(pa.Array.from_pandas(arbitrary))
   1227         else:
   1228             data = as_column(rmm.to_device(arbitrary), nan_as_null=nan_as_null)

/opt/conda/envs/rapids/lib/python3.6/site-packages/pyarrow/array.pxi in pyarrow.lib.Array.from_pandas()

/opt/conda/envs/rapids/lib/python3.6/site-packages/pyarrow/array.pxi in pyarrow.lib.array()

/opt/conda/envs/rapids/lib/python3.6/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()

/opt/conda/envs/rapids/lib/python3.6/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()

ArrowInvalid: only handle 1-dimensional arrays

```
3) cuml_benchmarks 
- Fix error: 'AlgorithmPair' object has no attribute 'set_up_cpu'
4) replace old cuml_quick_benchmarks with working version of cuml_benchmarks
5) Remove cuDatashader.ipynb 
This question is about [intermediate_notebooks/examples/weather.ipynb](https://github.com/rapidsai/notebooks-contrib/blob/branch-0.11/intermediate_notebooks/examples/weather.ipynb).

At the last of section "5. Using Inner Joins to Filter Weather Observations", total length of stations (like `len(atlanta_stations_df)`) is used to calculate average precipitation.
However, the numbers of stations are not the same per year. Therefore, the result values are too small in the case that fewer stations only exist.

Why does this notebook use the total number of stations as a denominator?
Improved the Build.md docs to include clearer, updated directions and how to use the branches 
<!--

Thank you for contributing to RAPIDS Notebooks :)

Here are some guidelines to help the review process go smoothly.

1. Please write a description in this text box of the changes that are being
   made.

2. Please ensure that you have written units tests for the changes made/features
   added.

3. If you are closing an issue please use one of the automatic closing words as
   noted here: https://help.github.com/articles/closing-issues-using-keywords/

4. If your pull request is not ready for review but you want to make use of the
   continuous integration testing facilities please label it with `[WIP]`.

5. If your pull request is ready to be reviewed without requiring additional
   work on top of it, then remove the `[WIP]` label (if present) and replace
   it with `[REVIEW]`. If assistance is required to complete the functionality,
   for example when the C/C++ code of a feature is complete but Python bindings
   are still required, then add the label `[HELP-REQ]` so that others can triage
   and assist. The additional changes then can be implemented on top of the
   same PR. If the assistance is done by members of the rapidsAI team, then no
   additional actions are required by the creator of the original PR for this,
   otherwise the original author of the PR needs to give permission to the
   person(s) assisting to commit to their personal fork of the project. If that
   doesn't happen then a new PR based on the code of the original PR can be
   opened by the person assisting, which then will be the PR that will be
   merged.

6. Once all work has been done and review has taken place please do not add
   features or make changes out of the scope of those requested by the reviewer
   (doing this just add delays as already reviewed code ends up having to be
   re-reviewed/it is hard to tell what is new etc!). Further, please do not
   rebase your branch on master/force push/rewrite history, doing any of these
   causes the context of any comments made by reviewers to be lost. If
   conflicts occur against master they should be resolved by merging master
   into the branch used for making the pull request.

Many thanks in advance for your cooperation!

-->

**Describe the bug**
Hi,

I am reading the following blog, and when I try to reproduce the example notebooks, the results do not match.

https://medium.com/rapids-ai/essential-machine-learning-with-linear-models-in-rapids-part-1-of-a-series-992fab0240da

In my testing, the best result is always MSE_OLS_THEORY, and not MSE_RIDGE, as stated in the blog.

Any ideas why is this happening? Could the code be updated to make reproducible results similar to the blog? 

**Steps/Code to reproduce bug**
I am usign todays rapids nightly build from Dockerhub.

I have git cloned the notebooks-contrib repo (master, v0.11) and run the cells.

**Expected behavior**
Same results as in the blog.

**Environment details (please complete the following information):**
DGX-1

**Additional context**
Add any other context about the problem here.

Due to changes in RMM, there are a few notebooks that need to be updated to the new API.  Alerting those who maintain the respective notebooks.  

Changes Readme: https://github.com/rapidsai/rmm#handling-rmm-options-in-python-code

Notebooks to update:

- [ ] [Spotify Playlist](https://github.com/rapidsai/notebooks-contrib/blob/master/conference_notebooks/ASONAM_2019/Spotify_Playlist.ipynb) @cjnolet 
- [ ] [show me the word count](https://github.com/rapidsai/notebooks-contrib/blob/master/blog_notebooks/nlp/show_me_the_word_count_gutenberg/show_me_the_word_count_gutenberg.ipynb) @VibhuJawa 
- introduce /blazingsql/
- intermediate BlazingSQL demo notebooks
**Describe the bug**
Colab script has several new issues:
1. when installing 0.11 nightlies, cudf is installed as `=.10 `
1. when installed a stable release, like `0.10`, the script fails
1. when installing xgboost, the script fails with a 404 error
1. when all libraries are installed in 0.11, cudf and cugraph throw Attribute errors around features that have been removed for 0.11
- cudf errror: `AttributeError: type object 'cudf._lib.gpuarrow.CudaRecordBatchStreamReader' has no attribute '__reduce_cython__', which was removed in https://github.com/rapidsai/cudf/commit/e5192de83d7d3a568796cadfdd4159236d6ec665`
- cugraph error: `AttributeError: module 'pyarrow.lib' has no attribute 'LargeListType'`

