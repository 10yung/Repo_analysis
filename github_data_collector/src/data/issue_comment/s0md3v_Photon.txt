Hi,

First things first, thank you for creating and sharing so many awesome tools. You 🤘 .

It looks findsubdomains.com no longer exists and is redirected to [Spyse](https://spyse.com/) and hence photon is not exporting any subdomains to a text file when using the flag `--dns`. Can you please review? 

Thanks again for an awesome project!
The issue was being cause by a never-catcheable regex.

dnsdumpster HTML attributes use double quotes whereas this regex
was catching only the expression with simple ones and thus creating
an empty regex object (None Type).
I found at least one example where the links are not being extracted in any of the datasets:

```html
<!DOCTYPE html>

<html>
    <head>
        <title>Test</title>
        <script src="javascriptfile.js"></script>
        <script src="https://externalresource.example.org/javascriptfile.js"></script>
    </head>
    <body>
        Body content
    </body>
</html>
```

In this case, `https://externalresource.example.org/javascriptfile.js` should be left in at least one dataset.

I tried to locate the appropriate point in the code to modify the logic, but I got kind of lost :P
Hi, as shown in the following full dependency graph of  **_Photon_**, **_Photon_** requires _**urllib3**_ (the latest version)， while the installed version of **_requests_**(2.22.0) requires _**urllib3>=1.21.1,<1.26**_.

According to Pip's “first found wins” installation strategy, _**urllib3 1.25.3**_ is the actually installed version.  

Although the first found package version _**urllib3 1.25.3**_ just satisfies the later dependency constraint (_**urllib3>=1.21.1,<1.26**_), it will lead to a build failure once developers release a newer version of **_urllib3_** in the near future, which is greater than 1.26.



### Dependency tree--------
```
Photon(version range:)
| +-requests(version range:)
| | +-chardet(version range:>=3.0.2,<3.1.0)
| | +-idna(version range:>=2.5,<2.9)
| | +-urllib3(version range:>=1.21.1,<1.26)
| | +-certifi(version range:>=2017.4.17)
| +-tld(version range:)
| +-urllib3(version range:)
```

Thanks for your attention.
Best,
Neolith

I have an issue, where Photon gets stuck, when using the _--keys_ option.

Example:
```
python3 photon.py -u "public-sector.mgm-tp.com" -v --keys
      ____  __          __
     / __ \/ /_  ____  / /_____  ____
    / /_/ / __ \/ __ \/ __/ __ \/ __ \
   / ____/ / / / /_/ / /_/ /_/ / / / /
  /_/   /_/ /_/\____/\__/\____/_/ /_/ v1.3.2

[+] URLs retrieved from robots.txt: 2
[+] URLs retrieved from sitemap.xml: 5
[~] Level 1: 8 URLs
[!] Progress: 6/8
```
And then it just stays there forever..

However, not using _--keys_ works great:
```
python3 photon.py -u "public-sector.mgm-tp.com" -v --only-urls --wayback
      ____  __          __
     / __ \/ /_  ____  / /_____  ____
    / /_/ / __ \/ __ \/ __/ __ \/ __ \
   / ____/ / / / /_/ / /_/ /_/ / / / /
  /_/   /_/ /_/\____/\__/\____/_/ /_/ v1.3.2

[~] Fetching URLs from archive.org
[+] Retrieved -1 URLs from archive.org
[+] URLs retrieved from robots.txt: 2
[+] URLs retrieved from sitemap.xml: 5
[~] Level 1: 8 URLs
[!] Progress: 8/8
[~] Level 2: 31 URLs
[!] Progress: 31/31
--------------------------------------------------
[+] Files: 1
[+] Robots: 2
[+] Internal: 86
[+] External: 49
--------------------------------------------------
[!] Total requests made: 40
[!] Total time taken: 0 minutes 21 seconds
[!] Requests per second: 1
[+] Results saved in public-sector.mgm-tp.com directory
```

Note, that this does not happen on every domain, but only on some.



Also: does _--wayback_ work? I always get _-1 URLs from archive.org_
Hi mate, awesome project!  Thank you.

Would be possible to collect js files from wayback and see if they are live ? If so, we add it to the scripts.txt list and look for api keys inside js files too ?

Cheers
I try to use photon as a crawler and save its request to burpsuite History tab. Because Photon failed to verify SSL certificate. Any option to skip this ?
The encode method was used incorrectly, as a result, the files were not written and caused the handles to hang.
The --clone function only clones the seed URL and not all pages crawled as described in the Photon Wiki. 


Saved the logic of auto-detection schemes, all wrapped in an error handler.