
Hi,

 Version: 0.5.1, OS: Windows

Is it possible that the extra arguments "..." for create_vocabulary are not catched?

As I could see in the source code for 0.6:

```

create_vocabulary = function(it, ngram = c("ngram_min" = 1L, "ngram_max" = 1L),

stopwords = character(0), sep_ngram = "_", window_size = 0L)

```

Maybe it should automatically use the class method create_vocabulary.itoken_parallel which has the "...". However I couldnt pass the ".export" in any way to create_vocabulary with an initialized itoken_parallel, and got the "unused argument..." error.

Also for create_dtm I could pass .export to it without any problem.

Just saw the update notes for 0.6 could it be related?

Best regards,
Yiyang
This is related to #283 which was closed. I found seemingly working code able to load pretrained GloVe vectors to text2vec: https://gist.github.com/tjvananne/8b0e7df7dcad414e8e6d5bf3947439a9
I have not run this successfully due to somewhat slow reading of large files, but perhaps it can help.

I would like to know what all the abbreviations mean? Some I can guess, like "PUNCT", but no idea what "X" might be. I want to retain contractions, but hard to choose options without documentation.

Thanks. Great performance code!
https://cacm.acm.org/magazines/2018/4/226373-learning-topic-models-provably-and-efficiently/fulltext

https://dl.acm.org/citation.cfm?id=3186262

[provable-topic-models.pdf](https://github.com/dselivanov/text2vec/files/2061374/provable-topic-models.pdf)

It will be useful to create a comprehensive practical guide for topic modeling. Now we have all components in place:

- POS tags and lemmatization - thanks to `udpipe` package
- `coherence` measures - thanks to Manuel work
- fast LDA, thanks to WarpLDA in text2vec
- fast non-negative matrix factorization, thanks to `rsparse` package
- multi-word phrase extraction - several approaches `text2vec::Collocations`, `udpipe::as_phrasemachine`

### Steps

- [ ] find interesting non-trivial corpus with large number of documents
- [ ] demonstrate how to create tokenizer which only use particular POS
- [ ] create collocation model on top of that
- [ ] create document-term matrix using tokens with multi-word expression
- [ ] fit several topic models (`text2vec::LDA`, `rsparse::WRMF`) with different hyper parameters
- [ ] cross-validate / compare them using different coherence metrics
    - [ ] demonstrate usage of external corpus for `tcm` calculation
    - [ ] check on how coherence metrics are correlated (is perplexity correlated with them? )

There are already good vignettes in udpipe package [topic modeling](https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-usecase-topicmodelling.html) and [phrase extraction](https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-usecase-postagging-lemmatisation.html). They can be used as inspiration.

@manuelbickel @jwijffels anything we can add to the plan above?
This is mostly annoyance. I think it would be logical if the lda_model would also store the resulting  doc_topic_distr as part of the public fields. 

`doc_topic_distr = 
  lda_model$fit_transform(x = dtm, n_iter = 1000, 
                          convergence_tol = 0.001, n_check_convergence = 25, 
                          progressbar = FALSE)`

We can see that topic_word_distribution is already there so having doc_topic_distribution would make sense as well. Or have I misunderstood something. 

`` > lda_model
<WarpLDA>
  Inherits from: <LDA>
 Public:
    clone: function (deep = FALSE) 
    components: active binding
    fit_transform: function (x, n_iter = 1000, convergence_tol = 0.001, n_check_convergence = 10, 
    get_top_words: function (n = 10, topic_number = 1L:private$n_topics, lambda = 1) 
    initialize: function (n_topics = 10L, doc_topic_prior = 50/n_topics, topic_word_prior = 1/n_topics) 
    plot: function (lambda.step = 0.1, reorder.topics = FALSE, doc_len = private$doc_len, 
    topic_word_distribution: active binding
    transform: function (x, n_iter = 1000, convergence_tol = 0.001, n_check_convergence = 5, ``
Seems that LDAvis package doesn't actively maintained and won't be updated on CRAN in near future. In particular we need option to not reorder topics and fixes for NaN in `jensenShannon` (see https://github.com/cpsievert/LDAvis/issues/56):

1. https://github.com/cpsievert/LDAvis/pull/77
1. https://github.com/cpsievert/LDAvis/pull/80
I just saw this work about NBSVM https://www.kaggle.com/jhoward/nb-svm-baseline-and-basic-eda-0-06-lb/

I was wondering if it would make sense to add the NB transformation part of text2vec.
It would be a kind of reweighting weights depending of the data and well know to be a strong base line when combined to SVM (may be other algo too).

Original paper is there:
https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf
Dear Dmitriy,

thank you again for solving issue #218 concerning replacement of terms by multiple synonyms. I now have a question concerning how to best incorporate dictionaries that include information on ngrams/collocations, e.g., city names. A standard solution would be to simply replace all matched patterns in the text by the dashed_version_of_patterns, e.g., via `stri_replace_all`. However, this is slow for large corpora and I am interested how you would solve this task in text2vec.  

As a workaround, I trained a collocation model on a modified dictionary containing all terms bound by dashes leaving the first unigram unbound so the model sees only one prefix and suffix, e.g, "new york_city". Please, see below code example.

I was wondering if you would incorporate such dictionary information differently, e.g., without training a model and manually defining colloaction_stats or so. 

I would appreciate your thoughts. Thank you in advance.

```
library(text2vec)

txt <- c("new york city", "new york city district in new york", "san francisco")

dict_ngrams <- c("new york", "san francisco", "new york city", "the state of new york city", "city district")

#modify dict for limiting to one prefix/suffix
dict_ngrams_dashed <- gsub(" ", "_", dict_ngrams)
dict_ngrams_dashed <- sub("_", " ", dict_ngrams_dashed)

#train model based on dict
cc_model <- Collocations$new(collocation_count_min = 1
                             ,pmi_min = 0
                             ,gensim_min = 0
                             ,lfmd_min = -Inf
)

it_dict_dashed <- itoken(dict_ngrams_dashed, progressbar = FALSE)
cc_model$partial_fit(it_dict_dashed)
# cc_model$collocation_stat
# prefix                 suffix n_i n_j n_ij      pmi      lfmd gensim rank_pmi rank_lfmd rank_gensim
# 1:   city               district   1   1    1 3.321928 -3.321928      0        1         1           3
# 2:    the state_of_new_york_city   1   1    1 3.321928 -3.321928      0        2         2           4
# 3:    san              francisco   1   1    1 3.321928 -3.321928      0        3         3           5
# 4:    new              york_city   2   1    1 2.321928 -4.321928      0        4         4           1
# 5:    new                   york   2   1    1 2.321928 -4.321928      0        5         5           2

it_txt <- itoken(txt, progressbar = FALSE)
it_txt_cc <- cc_model$transform(it_txt)

v <- create_vocabulary(it_txt_cc)
#side effect of cc model that might be interesting: "city_district" is ranked lower than "new_yor_city"
#and thus the latter is preferred over the former , see below
v
# Number of docs: 3 
# 0 stopwords:  ... 
# ngram_min = 1; ngram_max = 1 
# Vocabulary: 
#   term term_count doc_count
# 1:      district          1         1
# 2:            in          1         1
# 3:      new_york          1         1
# 4: san_francisco          1         1
# 5: new_york_city          2         2
```