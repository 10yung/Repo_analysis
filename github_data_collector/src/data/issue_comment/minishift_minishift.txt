### General information

  * Minishift version: minishift-1.34.1-windows-amd64
  * OS: Windows 10 Pro
  * Hypervisor: VirtualBox 6.0


### Steps to reproduce

  1. minishift start --vm-driver virtualbox

### Expected
 Openshift cluster should be up and running in minishift
### Actual
Error during 'cluster up' execution: Error starting the cluster. ssh command error:

### Logs

Please find the log [here](https://gist.github.com/samme4life/dbc7332e39add2a069695ea4a84cae02)


### General information
In the minishift logs, there are connection refused error when connecting to Openshift API server. 

I deployed vault-injector in minishift but the secrets are mounted in the pods. I have admissions-webhook and dynamic-admission-webhook enabled and there is an error connecting to API server in minishift. then, I noticed there are connection refused error for API server in the minishift logs.

This is the default behavior that minishift should talk to all the Openshift components like API server, etcd, etc, etc without issue and there should not be any connection refused errors in the minishift logs.

  * Minishift version:  v1.34.1+c2ff9cb
  * OS: Ubuntu 18.04
  * Hypervisor: KVM 

### Steps to reproduce
  1. start minishift ( just this step should be sufficient to reproduce the connection refused error)
  2. deployed hashicorp vault and vault injector
  3. After that, many connection refused error in the minishift logs
### Expected
minishift should not have connection refused error to minishift components like Openshift API server.

### Actual
k8s$ oc logs pod/vault-injector-7d4bb88bd7-82c2m
2020-01-07T15:40:06.204Z [INFO]  handler: Starting handler..
Updated certificate bundle received. Updating certs...
Listening on ":8080"...
**Error updating MutatingWebhookConfiguration: Patch https://172.30.0.1:443/apis/admissionregistration.k8s.io/v1beta1/mutatingwebhookconfigurations/vault-agent-injector-cfg: dial tcp 172.30.0.1:443: connect: connection refused**

### Logs
Minishift logs can be found at,
k8s$ minishift logs

(truncated the deprecated messages)

Flag --file-check-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cluster-dns has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
I0108 20:22:54.491513    5161 server.go:417] Version: v1.11.0+d4cacc0
I0108 20:22:54.491737    5161 plugins.go:97] No cloud provider specified.
I0108 20:22:54.531350    5161 server.go:657] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
I0108 20:22:54.531874    5161 container_manager_linux.go:243] container manager verified user specified cgroup-root exists: []
I0108 20:22:54.531894    5161 container_manager_linux.go:248] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: ContainerRuntime:docker CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:systemd KubeletRootDir:/var/lib/minishift/base/openshift.local.volumes ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[{Signal:memory.available Operator:LessThan Value:{Quantity:100Mi Percentage:0} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.1} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.inodesFree Operator:LessThan Value:{Quantity:<nil> Percentage:0.05} GracePeriod:0s MinReclaim:<nil>} {Signal:imagefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.15} GracePeriod:0s MinReclaim:<nil>}]} QOSReserved:map[] ExperimentalCPUManagerPolicy:none ExperimentalCPUManagerReconcilePeriod:10s ExperimentalPodPidsLimit:-1 EnforceCPULimits:true}
I0108 20:22:54.532003    5161 container_manager_linux.go:267] Creating device plugin manager: true
I0108 20:22:54.532038    5161 state_mem.go:36] [cpumanager] initializing new in-memory state store
I0108 20:22:54.532354    5161 state_file.go:82] [cpumanager] state file: created new state file "/var/lib/minishift/base/openshift.local.volumes/cpu_manager_state"
I0108 20:22:54.532407    5161 kubelet.go:274] Adding pod path: /var/lib/origin/pod-manifests
I0108 20:22:54.532448    5161 kubelet.go:299] Watching apiserver
E0108 20:22:54.534985    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:54.535357    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:54.536253    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:22:54.545984    5161 client.go:75] Connecting to docker on unix:///var/run/docker.sock
I0108 20:22:54.546012    5161 client.go:104] Start docker client with request timeout=2m0s
W0108 20:22:54.548265    5161 docker_service.go:545] Hairpin mode set to "promiscuous-bridge" but kubenet is not enabled, falling back to "hairpin-veth"
I0108 20:22:54.548291    5161 docker_service.go:238] Hairpin mode set to "hairpin-veth"
W0108 20:22:54.548437    5161 cni.go:172] Unable to update cni config: No networks found in /etc/cni/net.d
I0108 20:22:54.552897    5161 docker_service.go:253] Docker cri networking managed by kubernetes.io/no-op
I0108 20:22:54.563113    5161 docker_service.go:258] Docker Info: &{ID:WF3F:RGWJ:YA4X:E3MA:C3X3:ZDVG:AXGU:53CU:AW5H:4KGS:J5WN:KYW5 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem xfs] [Supports d_type true] [Native Overlay Diff true]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host macvlan null overlay] Authorization:[] Log:[]} MemoryLimit:true SwapLimit:true KernelMemory:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:30 OomKillDisable:true NGoroutines:29 SystemTime:2020-01-08T20:22:54.55585867Z LoggingDriver:journald CgroupDriver:systemd NEventsListener:0 KernelVersion:3.10.0-957.5.1.el7.x86_64 OperatingSystem:CentOS Linux 7 (Core) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc420a536c0 NCPU:2 MemTotal:3153039360 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy:http://10.120.136.40:8080 HTTPSProxy:http://10.120.136.40:8080 NoProxy:localhost,127.0.0.1,172.30.1.1,172.30.1.1,192.168.99.1/16,192.168.42.0/16,10.120.0.0/16,127.0.0.1,localhost,::1,192.168.42.41,vault-vault-demo.192.168.42.41.nip.io,192.168.42.205,vault-vault-demo.192.168.42.205.nip.io,192.168.42.233,vault-vault-demo.192.168.42.233.nip.io Name:minishift Labels:[provider=kvm] ExperimentalBuild:false ServerVersion:1.13.1 ClusterStore: ClusterAdvertise: Runtimes:map[docker-runc:{Path:/usr/libexec/docker/docker-runc-current Args:[]} runc:{Path:docker-runc Args:[]}] DefaultRuntime:docker-runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:0xc420b39400} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID: Expected:aa8187dbd3b7ad67d8e5e3a15115d3eef43a7ed1} RuncCommit:{ID:290a33602b16ff2d1cc5339bc0297f0e094462ce Expected:9df8b306d01f59d3a8029be411de015b7304dd8f} InitCommit:{ID:N/A Expected:949e6facb77383876aeff8a6944dde66b3089574} SecurityOptions:[name=seccomp,profile=default name=selinux]}
I0108 20:22:54.563199    5161 docker_service.go:271] Setting cgroupDriver to systemd
I0108 20:22:54.578046    5161 kuberuntime_manager.go:186] Container runtime docker initialized, version: 1.13.1, apiVersion: 1.26.0
W0108 20:22:54.578643    5161 probe.go:270] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.
I0108 20:22:54.579028    5161 csi_plugin.go:111] kubernetes.io/csi: plugin initializing...
I0108 20:22:54.580283    5161 server.go:995] Started kubelet
E0108 20:22:54.581043    5161 kubelet.go:1244] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data for container /
I0108 20:22:54.581669    5161 fs_resource_analyzer.go:66] Starting FS ResourceAnalyzer
I0108 20:22:54.581691    5161 status_manager.go:152] Starting to sync pod status with apiserver
I0108 20:22:54.581703    5161 kubelet.go:1741] Starting kubelet main sync loop.
I0108 20:22:54.581715    5161 kubelet.go:1758] skipping pod synchronization - [container runtime is down PLEG is not healthy: pleg was last seen active 2562047h47m16.854775807s ago; threshold is 3m0s]
I0108 20:22:54.581907    5161 server.go:129] Starting to listen on 0.0.0.0:10250
I0108 20:22:54.582381    5161 server.go:307] Adding debug handlers to kubelet server.
E0108 20:22:54.583116    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8443: connect: connection refused' (may retry after sleeping)
I0108 20:22:54.583631    5161 volume_manager.go:247] Starting Kubelet Volume Manager
I0108 20:22:54.583912    5161 desired_state_of_world_populator.go:130] Desired state populator starts to run
I0108 20:22:54.683858    5161 kubelet.go:1758] skipping pod synchronization - [container runtime is down]
I0108 20:22:54.684803    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:22:54.689478    5161 kubelet_node_status.go:79] Attempting to register node localhost
E0108 20:22:54.690030    5161 kubelet_node_status.go:103] Unable to register node "localhost" with API server: Post https://localhost:8443/api/v1/nodes: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:22:54.807417    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:22:54.810123    5161 cpu_manager.go:155] [cpumanager] starting with none policy
I0108 20:22:54.810141    5161 cpu_manager.go:156] [cpumanager] reconciling every 10s
I0108 20:22:54.810152    5161 policy_none.go:42] [cpumanager] none policy: Start
I0108 20:22:54.884040    5161 kubelet.go:1758] skipping pod synchronization - [container runtime is down]
I0108 20:22:54.890853    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:22:54.893466    5161 kubelet_node_status.go:79] Attempting to register node localhost
E0108 20:22:54.893925    5161 kubelet_node_status.go:103] Unable to register node "localhost" with API server: Post https://localhost:8443/api/v1/nodes: dial tcp 127.0.0.1:8443: connect: connection refused
Starting Device Plugin manager
W0108 20:22:54.913051    5161 manager.go:496] Failed to retrieve checkpoint for "kubelet_internal_checkpoint": checkpoint is not found
E0108 20:22:54.913495    5161 eviction_manager.go:243] eviction manager: failed to get get summary stats: failed to get node info: node "localhost" not found
E0108 20:22:55.167588    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8443: connect: connection refused' (may retry after sleeping)
I0108 20:22:55.294066    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:22:55.296596    5161 kubelet_node_status.go:79] Attempting to register node localhost
E0108 20:22:55.297042    5161 kubelet_node_status.go:103] Unable to register node "localhost" with API server: Post https://localhost:8443/api/v1/nodes: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:55.535760    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:55.537180    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:55.542416    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:22:55.543359    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:22:55.549618    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:22:55.550412    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
W0108 20:22:55.555771    5161 status_manager.go:482] Failed to get status for pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/master-api-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:22:55.561193    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:22:55.561375    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
W0108 20:22:55.568986    5161 status_manager.go:482] Failed to get status for pod "master-etcd-localhost_kube-system(34b17db69b2b3877c9904b5340f1ae71)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/master-etcd-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:22:55.570051    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:22:55.570823    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
W0108 20:22:55.577032    5161 status_manager.go:482] Failed to get status for pod "kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:22:55.578933    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
W0108 20:22:55.587030    5161 status_manager.go:482] Failed to get status for pod "kube-scheduler-localhost_kube-system(f903f642800a02b87385310221ffe91f)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:22:55.603752    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "master-config" (UniqueName: "kubernetes.io/host-path/34b17db69b2b3877c9904b5340f1ae71-master-config") pod "master-etcd-localhost" (UID: "34b17db69b2b3877c9904b5340f1ae71") 
I0108 20:22:55.603831    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "master-data" (UniqueName: "kubernetes.io/host-path/34b17db69b2b3877c9904b5340f1ae71-master-data") pod "master-etcd-localhost" (UID: "34b17db69b2b3877c9904b5340f1ae71") 
I0108 20:22:55.603865    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "master-cloud-provider" (UniqueName: "kubernetes.io/host-path/dfcadfa6552711112062fbf1121a691c-master-cloud-provider") pod "kube-controller-manager-localhost" (UID: "dfcadfa6552711112062fbf1121a691c") 
I0108 20:22:55.603895    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "master-config" (UniqueName: "kubernetes.io/host-path/f903f642800a02b87385310221ffe91f-master-config") pod "kube-scheduler-localhost" (UID: "f903f642800a02b87385310221ffe91f") 
I0108 20:22:55.603921    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "master-config" (UniqueName: "kubernetes.io/host-path/29e68324ed097a2c36aa5709e9b67154-master-config") pod "master-api-localhost" (UID: "29e68324ed097a2c36aa5709e9b67154") 
I0108 20:22:55.604143    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "master-cloud-provider" (UniqueName: "kubernetes.io/host-path/29e68324ed097a2c36aa5709e9b67154-master-cloud-provider") pod "master-api-localhost" (UID: "29e68324ed097a2c36aa5709e9b67154") 
I0108 20:22:55.604182    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "master-data" (UniqueName: "kubernetes.io/host-path/29e68324ed097a2c36aa5709e9b67154-master-data") pod "master-api-localhost" (UID: "29e68324ed097a2c36aa5709e9b67154") 
I0108 20:22:55.604212    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "master-config" (UniqueName: "kubernetes.io/host-path/dfcadfa6552711112062fbf1121a691c-master-config") pod "kube-controller-manager-localhost" (UID: "dfcadfa6552711112062fbf1121a691c") 
I0108 20:22:55.604244    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "master-cloud-provider" (UniqueName: "kubernetes.io/host-path/f903f642800a02b87385310221ffe91f-master-cloud-provider") pod "kube-scheduler-localhost" (UID: "f903f642800a02b87385310221ffe91f") 
I0108 20:22:56.097222    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:22:56.100009    5161 kubelet_node_status.go:79] Attempting to register node localhost
E0108 20:22:56.100452    5161 kubelet_node_status.go:103] Unable to register node "localhost" with API server: Post https://localhost:8443/api/v1/nodes: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:56.536776    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:56.544956    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:56.545023    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:57.537389    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:57.545613    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:57.546785    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:22:57.700685    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:22:57.703534    5161 kubelet_node_status.go:79] Attempting to register node localhost
E0108 20:22:57.704080    5161 kubelet_node_status.go:103] Unable to register node "localhost" with API server: Post https://localhost:8443/api/v1/nodes: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:58.538022    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:58.546286    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:58.547342    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:59.538692    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:59.547253    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:59.548017    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:00.544958    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:00.547850    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:00.548840    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:23:00.904241    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:23:00.907116    5161 kubelet_node_status.go:79] Attempting to register node localhost
E0108 20:23:00.907516    5161 kubelet_node_status.go:103] Unable to register node "localhost" with API server: Post https://localhost:8443/api/v1/nodes: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:01.545613    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:01.548496    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:01.549431    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:02.546322    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:02.552139    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:02.552197    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:03.547402    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:03.554294    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:03.554362    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:04.549019    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:04.554798    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:04.555966    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:04.913735    5161 eviction_manager.go:243] eviction manager: failed to get get summary stats: failed to get node info: node "localhost" not found
E0108 20:23:05.168314    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8443: connect: connection refused' (may retry after sleeping)
E0108 20:23:05.549658    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:05.555329    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:05.556460    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:06.550419    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:06.556190    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:06.557037    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:23:07.307704    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:23:07.310569    5161 kubelet_node_status.go:79] Attempting to register node localhost
E0108 20:23:07.311028    5161 kubelet_node_status.go:103] Unable to register node "localhost" with API server: Post https://localhost:8443/api/v1/nodes: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:07.551056    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:07.556834    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:07.558259    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:08.551787    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:08.557508    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:08.558867    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:09.552546    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:09.558203    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:09.559467    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:10.553247    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:10.558915    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:10.560070    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:11.553946    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:11.559568    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:11.560572    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:12.554903    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:12.561627    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:12.561692    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:13.555724    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:13.562347    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:13.563330    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:23:14.311343    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:23:14.313740    5161 kubelet_node_status.go:79] Attempting to register node localhost
E0108 20:23:14.314164    5161 kubelet_node_status.go:103] Unable to register node "localhost" with API server: Post https://localhost:8443/api/v1/nodes: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:14.558219    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:14.563119    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:14.563985    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:14.913954    5161 eviction_manager.go:243] eviction manager: failed to get get summary stats: failed to get node info: node "localhost" not found
E0108 20:23:15.169259    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8443: connect: connection refused' (may retry after sleeping)
E0108 20:23:15.559981    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:15.565210    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:15.569927    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:16.560707    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:16.569387    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:16.570638    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:17.563014    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:17.570462    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:17.571956    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:18.564279    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:18.573438    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:18.574028    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:19.564884    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:19.575367    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:19.575442    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:20.565636    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:20.576070    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:20.577333    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:23:21.314380    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:23:21.316848    5161 kubelet_node_status.go:79] Attempting to register node localhost
E0108 20:23:21.317275    5161 kubelet_node_status.go:103] Unable to register node "localhost" with API server: Post https://localhost:8443/api/v1/nodes: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:21.566350    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:21.576753    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:21.578052    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:22.567334    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:22.577514    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:22.578680    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:23.574402    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:23.579341    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:23.584707    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:24.575266    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:24.579984    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:24.585306    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:24.914178    5161 eviction_manager.go:243] eviction manager: failed to get get summary stats: failed to get node info: node "localhost" not found
I0108 20:23:25.027953    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
W0108 20:23:25.031524    5161 status_manager.go:482] Failed to get status for pod "kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:25.170222    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8443: connect: connection refused' (may retry after sleeping)
E0108 20:23:25.583557    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:25.584397    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:25.585837    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:26.584286    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:26.585571    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:26.587112    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:27.584950    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:27.586073    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:27.587721    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:23:28.317479    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:23:28.320010    5161 kubelet_node_status.go:79] Attempting to register node localhost
E0108 20:23:28.320448    5161 kubelet_node_status.go:103] Unable to register node "localhost" with API server: Post https://localhost:8443/api/v1/nodes: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:28.585504    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:28.586589    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:28.588291    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:29.586223    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:29.587450    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:29.589025    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:30.586961    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:30.588252    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:30.589534    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:23:31.066546    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
W0108 20:23:31.073665    5161 status_manager.go:482] Failed to get status for pod "master-etcd-localhost_kube-system(34b17db69b2b3877c9904b5340f1ae71)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/master-etcd-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:23:31.078603    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
W0108 20:23:31.083012    5161 status_manager.go:482] Failed to get status for pod "kube-scheduler-localhost_kube-system(f903f642800a02b87385310221ffe91f)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:31.587667    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:31.589040    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:31.590459    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:23:32.082601    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:23:32.083484    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
E0108 20:23:32.588489    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:32.589701    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:32.591092    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:33.589184    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:33.590267    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:33.592095    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:34.590557    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:34.590911    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:34.593763    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:34.914381    5161 eviction_manager.go:243] eviction manager: failed to get get summary stats: failed to get node info: node "localhost" not found
E0108 20:23:35.172594    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8443: connect: connection refused' (may retry after sleeping)
I0108 20:23:35.320787    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:23:35.324415    5161 kubelet_node_status.go:79] Attempting to register node localhost
E0108 20:23:35.324842    5161 kubelet_node_status.go:103] Unable to register node "localhost" with API server: Post https://localhost:8443/api/v1/nodes: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:35.591328    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:35.592565    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:35.594502    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:36.592564    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:36.593720    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:36.595168    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:37.593352    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:37.594510    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:37.595590    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:38.594610    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:38.595063    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:38.596778    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:39.595306    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:39.596752    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:39.597909    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:40.596076    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:40.597284    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:40.598432    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:41.596851    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:41.598127    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:41.599333    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:23:42.325030    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:23:42.331421    5161 kubelet_node_status.go:79] Attempting to register node localhost
E0108 20:23:42.331750    5161 kubelet_node_status.go:103] Unable to register node "localhost" with API server: Post https://localhost:8443/api/v1/nodes: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:42.597615    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:42.598866    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:42.600048    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:43.600034    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:43.600452    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:43.600464    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:44.600769    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:44.602942    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:44.604047    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:44.914659    5161 eviction_manager.go:243] eviction manager: failed to get get summary stats: failed to get node info: node "localhost" not found
E0108 20:23:45.173443    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8443: connect: connection refused' (may retry after sleeping)
E0108 20:23:45.603759    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:45.603852    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:45.605278    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:46.604635    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:46.605511    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:46.607372    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:47.607318    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:47.607323    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:47.608180    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:48.608457    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:48.608900    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:48.611364    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:23:49.331957    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:23:49.334086    5161 kubelet_node_status.go:79] Attempting to register node localhost
E0108 20:23:49.334485    5161 kubelet_node_status.go:103] Unable to register node "localhost" with API server: Post https://localhost:8443/api/v1/nodes: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:49.609141    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:49.611500    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:49.613790    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:50.609932    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:50.612169    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:50.614445    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:51.610555    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:51.612749    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:51.615034    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:52.611302    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:52.613350    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:52.615597    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:53.614757    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:53.614896    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:53.616495    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:23:54.215076    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
E0108 20:23:54.914911    5161 eviction_manager.go:243] eviction manager: failed to get get summary stats: failed to get node info: node "localhost" not found
I0108 20:23:55.221722    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:23:56.334641    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:23:56.338559    5161 kubelet_node_status.go:79] Attempting to register node localhost
I0108 20:24:03.565802    5161 kubelet_node_status.go:82] Successfully registered node localhost
I0108 20:24:03.587039    5161 reconciler.go:154] Reconciler: start to sync state
E0108 20:24:06.840034    5161 event.go:203] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"localhost.15e803018c7d9572", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"localhost", UID:"localhost", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"Starting", Message:"Starting kubelet.", Source:v1.EventSource{Component:"kubelet", Host:"localhost"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xbf7dac67a295e972, ext:290033945, loc:(*time.Location)(0x90a9e60)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xbf7dac67a295e972, ext:290033945, loc:(*time.Location)(0x90a9e60)}}, Count:1, Type:"Normal", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'namespaces "default" not found' (will not retry!)
E0108 20:24:07.307059    5161 event.go:203] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"localhost.15e8030192ff98db", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"localhost", UID:"localhost", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientDisk", Message:"Node localhost status is now: NodeHasSufficientDisk", Source:v1.EventSource{Component:"kubelet", Host:"localhost"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xbf7dac67a917ecdb, ext:399217774, loc:(*time.Location)(0x90a9e60)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xbf7dac67a917ecdb, ext:399217774, loc:(*time.Location)(0x90a9e60)}}, Count:1, Type:"Normal", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'namespaces "default" not found' (will not retry!)
E0108 20:24:07.499285    5161 event.go:203] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"localhost.15e8030192ffc9e3", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"localhost", UID:"localhost", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node localhost status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"localhost"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xbf7dac67a9181de3, ext:399230324, loc:(*time.Location)(0x90a9e60)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xbf7dac67a9181de3, ext:399230324, loc:(*time.Location)(0x90a9e60)}}, Count:1, Type:"Normal", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'namespaces "default" not found' (will not retry!)
E0108 20:24:07.863065    5161 event.go:203] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"localhost.15e8030192ffe318", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"localhost", UID:"localhost", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node localhost status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"localhost"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xbf7dac67a9183718, ext:399236782, loc:(*time.Location)(0x90a9e60)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xbf7dac67a9183718, ext:399236782, loc:(*time.Location)(0x90a9e60)}}, Count:1, Type:"Normal", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'namespaces "default" not found' (will not retry!)
I0108 20:24:36.160175    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-proxy-token-ncmnd" (UniqueName: "kubernetes.io/secret/e344d540-3254-11ea-b26d-525400ca0151-kube-proxy-token-ncmnd") pod "kube-proxy-qlz8n" (UID: "e344d540-3254-11ea-b26d-525400ca0151") 
I0108 20:24:36.160215    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "node-config" (UniqueName: "kubernetes.io/host-path/e340201c-3254-11ea-b26d-525400ca0151-node-config") pod "kube-dns-mmmt6" (UID: "e340201c-3254-11ea-b26d-525400ca0151") 
I0108 20:24:36.160234    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-dns-token-mrp85" (UniqueName: "kubernetes.io/secret/e340201c-3254-11ea-b26d-525400ca0151-kube-dns-token-mrp85") pod "kube-dns-mmmt6" (UID: "e340201c-3254-11ea-b26d-525400ca0151") 
I0108 20:24:36.160248    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "node-config" (UniqueName: "kubernetes.io/host-path/e344d540-3254-11ea-b26d-525400ca0151-node-config") pod "kube-proxy-qlz8n" (UID: "e344d540-3254-11ea-b26d-525400ca0151") 
I0108 20:24:36.370279    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "openshift-apiserver-token-dg6ph" (UniqueName: "kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-openshift-apiserver-token-dg6ph") pod "openshift-apiserver-6q9ps" (UID: "e3681100-3254-11ea-b26d-525400ca0151") 
I0108 20:24:36.370322    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "master-config" (UniqueName: "kubernetes.io/host-path/e3681100-3254-11ea-b26d-525400ca0151-master-config") pod "openshift-apiserver-6q9ps" (UID: "e3681100-3254-11ea-b26d-525400ca0151") 
I0108 20:24:36.370350    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "master-cloud-provider" (UniqueName: "kubernetes.io/host-path/e3681100-3254-11ea-b26d-525400ca0151-master-cloud-provider") pod "openshift-apiserver-6q9ps" (UID: "e3681100-3254-11ea-b26d-525400ca0151") 
I0108 20:24:36.370375    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "serving-cert" (UniqueName: "kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert") pod "openshift-apiserver-6q9ps" (UID: "e3681100-3254-11ea-b26d-525400ca0151") 
E0108 20:24:36.618632    5161 secret.go:198] Couldn't get secret openshift-apiserver/serving-cert: secrets "serving-cert" not found
E0108 20:24:36.618749    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\" (\"e3681100-3254-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:24:37.118702486 +0000 UTC m=+102.828486447 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"serving-cert\" (UniqueName: \"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\") pod \"openshift-apiserver-6q9ps\" (UID: \"e3681100-3254-11ea-b26d-525400ca0151\") : secrets \"serving-cert\" not found"
I0108 20:24:37.013734    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "serving-cert" (UniqueName: "kubernetes.io/secret/e3ab1aff-3254-11ea-b26d-525400ca0151-serving-cert") pod "openshift-service-cert-signer-operator-6d477f986b-wfzn8" (UID: "e3ab1aff-3254-11ea-b26d-525400ca0151") 
I0108 20:24:37.013843    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "openshift-service-cert-signer-operator-token-58289" (UniqueName: "kubernetes.io/secret/e3ab1aff-3254-11ea-b26d-525400ca0151-openshift-service-cert-signer-operator-token-58289") pod "openshift-service-cert-signer-operator-6d477f986b-wfzn8" (UID: "e3ab1aff-3254-11ea-b26d-525400ca0151") 
I0108 20:24:37.013872    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "config" (UniqueName: "kubernetes.io/configmap/e3ab1aff-3254-11ea-b26d-525400ca0151-config") pod "openshift-service-cert-signer-operator-6d477f986b-wfzn8" (UID: "e3ab1aff-3254-11ea-b26d-525400ca0151") 
E0108 20:24:37.261200    5161 secret.go:198] Couldn't get secret openshift-apiserver/serving-cert: secrets "serving-cert" not found
E0108 20:24:37.261388    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\" (\"e3681100-3254-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:24:38.261269339 +0000 UTC m=+103.971053299 (durationBeforeRetry 1s). Error: "MountVolume.SetUp failed for volume \"serving-cert\" (UniqueName: \"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\") pod \"openshift-apiserver-6q9ps\" (UID: \"e3681100-3254-11ea-b26d-525400ca0151\") : secrets \"serving-cert\" not found"
E0108 20:24:38.344501    5161 secret.go:198] Couldn't get secret openshift-apiserver/serving-cert: secrets "serving-cert" not found
E0108 20:24:38.344597    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\" (\"e3681100-3254-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:24:40.344554192 +0000 UTC m=+106.054338146 (durationBeforeRetry 2s). Error: "MountVolume.SetUp failed for volume \"serving-cert\" (UniqueName: \"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\") pod \"openshift-apiserver-6q9ps\" (UID: \"e3681100-3254-11ea-b26d-525400ca0151\") : secrets \"serving-cert\" not found"
E0108 20:24:40.383040    5161 secret.go:198] Couldn't get secret openshift-apiserver/serving-cert: secrets "serving-cert" not found
E0108 20:24:40.383128    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\" (\"e3681100-3254-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:24:44.383096357 +0000 UTC m=+110.092880324 (durationBeforeRetry 4s). Error: "MountVolume.SetUp failed for volume \"serving-cert\" (UniqueName: \"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\") pod \"openshift-apiserver-6q9ps\" (UID: \"e3681100-3254-11ea-b26d-525400ca0151\") : secrets \"serving-cert\" not found"
E0108 20:24:44.417073    5161 secret.go:198] Couldn't get secret openshift-apiserver/serving-cert: secrets "serving-cert" not found
E0108 20:24:44.417165    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\" (\"e3681100-3254-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:24:52.417130854 +0000 UTC m=+118.126914829 (durationBeforeRetry 8s). Error: "MountVolume.SetUp failed for volume \"serving-cert\" (UniqueName: \"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\") pod \"openshift-apiserver-6q9ps\" (UID: \"e3681100-3254-11ea-b26d-525400ca0151\") : secrets \"serving-cert\" not found"
E0108 20:24:52.463562    5161 secret.go:198] Couldn't get secret openshift-apiserver/serving-cert: secrets "serving-cert" not found
E0108 20:24:52.463654    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\" (\"e3681100-3254-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:25:08.463616045 +0000 UTC m=+134.173400006 (durationBeforeRetry 16s). Error: "MountVolume.SetUp failed for volume \"serving-cert\" (UniqueName: \"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\") pod \"openshift-apiserver-6q9ps\" (UID: \"e3681100-3254-11ea-b26d-525400ca0151\") : secrets \"serving-cert\" not found"
E0108 20:25:10.132139    5161 secret.go:198] Couldn't get secret openshift-apiserver/serving-cert: secrets "serving-cert" not found
E0108 20:25:10.132232    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\" (\"e3681100-3254-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:25:42.132198794 +0000 UTC m=+167.841982752 (durationBeforeRetry 32s). Error: "MountVolume.SetUp failed for volume \"serving-cert\" (UniqueName: \"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\") pod \"openshift-apiserver-6q9ps\" (UID: \"e3681100-3254-11ea-b26d-525400ca0151\") : secrets \"serving-cert\" not found"
I0108 20:25:18.092643    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "signing-key" (UniqueName: "kubernetes.io/secret/fc3b8724-3254-11ea-b26d-525400ca0151-signing-key") pod "service-serving-cert-signer-668c45d5f-l6mc8" (UID: "fc3b8724-3254-11ea-b26d-525400ca0151") 
I0108 20:25:18.092693    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "serving-cert" (UniqueName: "kubernetes.io/secret/fc3b8724-3254-11ea-b26d-525400ca0151-serving-cert") pod "service-serving-cert-signer-668c45d5f-l6mc8" (UID: "fc3b8724-3254-11ea-b26d-525400ca0151") 
I0108 20:25:18.092723    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "config" (UniqueName: "kubernetes.io/configmap/fc3b8724-3254-11ea-b26d-525400ca0151-config") pod "service-serving-cert-signer-668c45d5f-l6mc8" (UID: "fc3b8724-3254-11ea-b26d-525400ca0151") 
I0108 20:25:18.092753    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "service-serving-cert-signer-sa-token-q2lh5" (UniqueName: "kubernetes.io/secret/fc3b8724-3254-11ea-b26d-525400ca0151-service-serving-cert-signer-sa-token-q2lh5") pod "service-serving-cert-signer-668c45d5f-l6mc8" (UID: "fc3b8724-3254-11ea-b26d-525400ca0151") 
I0108 20:25:24.024408    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "config" (UniqueName: "kubernetes.io/configmap/ff5ce502-3254-11ea-b26d-525400ca0151-config") pod "apiservice-cabundle-injector-8ffbbb6dc-hpf6m" (UID: "ff5ce502-3254-11ea-b26d-525400ca0151") 
I0108 20:25:24.024474    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "signing-cabundle" (UniqueName: "kubernetes.io/configmap/ff5ce502-3254-11ea-b26d-525400ca0151-signing-cabundle") pod "apiservice-cabundle-injector-8ffbbb6dc-hpf6m" (UID: "ff5ce502-3254-11ea-b26d-525400ca0151") 
I0108 20:25:24.024492    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "serving-cert" (UniqueName: "kubernetes.io/secret/ff5ce502-3254-11ea-b26d-525400ca0151-serving-cert") pod "apiservice-cabundle-injector-8ffbbb6dc-hpf6m" (UID: "ff5ce502-3254-11ea-b26d-525400ca0151") 
I0108 20:25:24.024510    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "apiservice-cabundle-injector-sa-token-tvtdw" (UniqueName: "kubernetes.io/secret/ff5ce502-3254-11ea-b26d-525400ca0151-apiservice-cabundle-injector-sa-token-tvtdw") pod "apiservice-cabundle-injector-8ffbbb6dc-hpf6m" (UID: "ff5ce502-3254-11ea-b26d-525400ca0151") 
I0108 20:26:50.281215    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "master-config" (UniqueName: "kubernetes.io/host-path/3334e363-3255-11ea-b26d-525400ca0151-master-config") pod "openshift-controller-manager-cp9j7" (UID: "3334e363-3255-11ea-b26d-525400ca0151") 
I0108 20:26:50.281266    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "openshift-controller-manager-token-t74x8" (UniqueName: "kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8") pod "openshift-controller-manager-cp9j7" (UID: "3334e363-3255-11ea-b26d-525400ca0151") 
I0108 20:26:50.281291    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "master-cloud-provider" (UniqueName: "kubernetes.io/host-path/3334e363-3255-11ea-b26d-525400ca0151-master-cloud-provider") pod "openshift-controller-manager-cp9j7" (UID: "3334e363-3255-11ea-b26d-525400ca0151") 
I0108 20:27:16.905229    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "pvdir" (UniqueName: "kubernetes.io/host-path/3567cf6f-3255-11ea-b26d-525400ca0151-pvdir") pod "persistent-volume-setup-5mzds" (UID: "3567cf6f-3255-11ea-b26d-525400ca0151") 
I0108 20:27:16.905305    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "pvinstaller-token-zgfb9" (UniqueName: "kubernetes.io/secret/3567cf6f-3255-11ea-b26d-525400ca0151-pvinstaller-token-zgfb9") pod "persistent-volume-setup-5mzds" (UID: "3567cf6f-3255-11ea-b26d-525400ca0151") 
I0108 20:27:17.621594    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "openshift-web-console-operator-token-l72l6" (UniqueName: "kubernetes.io/secret/437c30e1-3255-11ea-b26d-525400ca0151-openshift-web-console-operator-token-l72l6") pod "openshift-web-console-operator-57986c9c4f-4tpgb" (UID: "437c30e1-3255-11ea-b26d-525400ca0151") 
I0108 20:27:17.621642    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "serving-cert" (UniqueName: "kubernetes.io/secret/437c30e1-3255-11ea-b26d-525400ca0151-serving-cert") pod "openshift-web-console-operator-57986c9c4f-4tpgb" (UID: "437c30e1-3255-11ea-b26d-525400ca0151") 
I0108 20:27:17.621670    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "config" (UniqueName: "kubernetes.io/configmap/437c30e1-3255-11ea-b26d-525400ca0151-config") pod "openshift-web-console-operator-57986c9c4f-4tpgb" (UID: "437c30e1-3255-11ea-b26d-525400ca0151") 
I0108 20:27:24.525031    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "deployer-token-kdk2h" (UniqueName: "kubernetes.io/secret/4735bc66-3255-11ea-b26d-525400ca0151-deployer-token-kdk2h") pod "docker-registry-1-deploy" (UID: "4735bc66-3255-11ea-b26d-525400ca0151") 
I0108 20:27:24.525092    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "deployer-token-kdk2h" (UniqueName: "kubernetes.io/secret/4734816d-3255-11ea-b26d-525400ca0151-deployer-token-kdk2h") pod "router-1-deploy" (UID: "4734816d-3255-11ea-b26d-525400ca0151") 
I0108 20:27:30.031259    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "serving-cert" (UniqueName: "kubernetes.io/secret/4a9f6dcd-3255-11ea-b26d-525400ca0151-serving-cert") pod "webconsole-777b884d95-fvssr" (UID: "4a9f6dcd-3255-11ea-b26d-525400ca0151") 
I0108 20:27:30.031368    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "webconsole-config" (UniqueName: "kubernetes.io/configmap/4a9f6dcd-3255-11ea-b26d-525400ca0151-webconsole-config") pod "webconsole-777b884d95-fvssr" (UID: "4a9f6dcd-3255-11ea-b26d-525400ca0151") 
I0108 20:27:30.031407    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "webconsole-token-mhn7m" (UniqueName: "kubernetes.io/secret/4a9f6dcd-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m") pod "webconsole-777b884d95-fvssr" (UID: "4a9f6dcd-3255-11ea-b26d-525400ca0151") 
I0108 20:27:33.300050    5161 reconciler.go:181] operationExecutor.UnmountVolume started for volume "deployer-token-kdk2h" (UniqueName: "kubernetes.io/secret/4734816d-3255-11ea-b26d-525400ca0151-deployer-token-kdk2h") pod "4734816d-3255-11ea-b26d-525400ca0151" (UID: "4734816d-3255-11ea-b26d-525400ca0151") 
I0108 20:27:33.924005    5161 operation_generator.go:688] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/4734816d-3255-11ea-b26d-525400ca0151-deployer-token-kdk2h" (OuterVolumeSpecName: "deployer-token-kdk2h") pod "4734816d-3255-11ea-b26d-525400ca0151" (UID: "4734816d-3255-11ea-b26d-525400ca0151"). InnerVolumeSpecName "deployer-token-kdk2h". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0108 20:27:33.957008    5161 reconciler.go:301] Volume detached for volume "deployer-token-kdk2h" (UniqueName: "kubernetes.io/secret/4734816d-3255-11ea-b26d-525400ca0151-deployer-token-kdk2h") on node "localhost" DevicePath ""
W0108 20:27:34.826020    5161 pod_container_deletor.go:75] Container "0fcdde98ecd8ea48da4bc507ae32ee40981fffdd6500814c9067f327ec451e7b" not found in pod's containers
I0108 20:27:36.997032    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "registry-token-b8dwv" (UniqueName: "kubernetes.io/secret/4ef4964d-3255-11ea-b26d-525400ca0151-registry-token-b8dwv") pod "docker-registry-1-tlcmc" (UID: "4ef4964d-3255-11ea-b26d-525400ca0151") 
I0108 20:27:36.997089    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "registry-storage" (UniqueName: "kubernetes.io/host-path/4ef4964d-3255-11ea-b26d-525400ca0151-registry-storage") pod "docker-registry-1-tlcmc" (UID: "4ef4964d-3255-11ea-b26d-525400ca0151") 
E0108 20:27:54.817588    5161 fsHandler.go:121] failed to collect filesystem stats - rootDiskErr: du command failed on /rootfs/var/lib/docker/overlay2/7ed74e29e518244187cf0c46bd321874f3c43b0218cdf7e11d656d5e330c7577/diff with output stdout: , stderr: du: cannot access '/rootfs/var/lib/docker/overlay2/7ed74e29e518244187cf0c46bd321874f3c43b0218cdf7e11d656d5e330c7577/diff': No such file or directory
 - exit status 1, rootInodeErr: cmd [ionice -c3 nice -n 19 find /rootfs/var/lib/docker/overlay2/7ed74e29e518244187cf0c46bd321874f3c43b0218cdf7e11d656d5e330c7577/diff -xdev -printf .] failed. stderr: find: '/rootfs/var/lib/docker/overlay2/7ed74e29e518244187cf0c46bd321874f3c43b0218cdf7e11d656d5e330c7577/diff': No such file or directory
; err: exit status 1, extraDiskErr: du command failed on /rootfs/var/lib/docker/containers/f3f7fc0b72cc1f42b8f67e78b73be25bad97b099c776333fdeb97e21a8629f05 with output stdout: , stderr: du: cannot access '/rootfs/var/lib/docker/containers/f3f7fc0b72cc1f42b8f67e78b73be25bad97b099c776333fdeb97e21a8629f05': No such file or directory
 - exit status 1
I0108 20:27:59.340979    5161 reconciler.go:181] operationExecutor.UnmountVolume started for volume "webconsole-token-mhn7m" (UniqueName: "kubernetes.io/secret/4a9f6dcd-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m") pod "4a9f6dcd-3255-11ea-b26d-525400ca0151" (UID: "4a9f6dcd-3255-11ea-b26d-525400ca0151") 
I0108 20:27:59.342031    5161 reconciler.go:181] operationExecutor.UnmountVolume started for volume "serving-cert" (UniqueName: "kubernetes.io/secret/4a9f6dcd-3255-11ea-b26d-525400ca0151-serving-cert") pod "4a9f6dcd-3255-11ea-b26d-525400ca0151" (UID: "4a9f6dcd-3255-11ea-b26d-525400ca0151") 
I0108 20:27:59.342068    5161 reconciler.go:181] operationExecutor.UnmountVolume started for volume "webconsole-config" (UniqueName: "kubernetes.io/configmap/4a9f6dcd-3255-11ea-b26d-525400ca0151-webconsole-config") pod "4a9f6dcd-3255-11ea-b26d-525400ca0151" (UID: "4a9f6dcd-3255-11ea-b26d-525400ca0151") 
I0108 20:27:59.587842    5161 operation_generator.go:688] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/4a9f6dcd-3255-11ea-b26d-525400ca0151-webconsole-config" (OuterVolumeSpecName: "webconsole-config") pod "4a9f6dcd-3255-11ea-b26d-525400ca0151" (UID: "4a9f6dcd-3255-11ea-b26d-525400ca0151"). InnerVolumeSpecName "webconsole-config". PluginName "kubernetes.io/configmap", VolumeGidValue ""
E0108 20:27:59.610158    5161 remote_runtime.go:278] ContainerStatus "e3374a630d8aa4282a7386a8d47eab203e1e32d526fb8328294ae90a6796e158" from runtime service failed: rpc error: code = Unknown desc = Error: No such container: e3374a630d8aa4282a7386a8d47eab203e1e32d526fb8328294ae90a6796e158
I0108 20:27:59.611021    5161 operation_generator.go:688] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/4a9f6dcd-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m" (OuterVolumeSpecName: "webconsole-token-mhn7m") pod "4a9f6dcd-3255-11ea-b26d-525400ca0151" (UID: "4a9f6dcd-3255-11ea-b26d-525400ca0151"). InnerVolumeSpecName "webconsole-token-mhn7m". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0108 20:27:59.667353    5161 reconciler.go:301] Volume detached for volume "webconsole-config" (UniqueName: "kubernetes.io/configmap/4a9f6dcd-3255-11ea-b26d-525400ca0151-webconsole-config") on node "localhost" DevicePath ""
I0108 20:27:59.668063    5161 reconciler.go:301] Volume detached for volume "webconsole-token-mhn7m" (UniqueName: "kubernetes.io/secret/4a9f6dcd-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m") on node "localhost" DevicePath ""
I0108 20:27:59.693781    5161 operation_generator.go:688] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/4a9f6dcd-3255-11ea-b26d-525400ca0151-serving-cert" (OuterVolumeSpecName: "serving-cert") pod "4a9f6dcd-3255-11ea-b26d-525400ca0151" (UID: "4a9f6dcd-3255-11ea-b26d-525400ca0151"). InnerVolumeSpecName "serving-cert". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0108 20:27:59.769321    5161 reconciler.go:301] Volume detached for volume "serving-cert" (UniqueName: "kubernetes.io/secret/4a9f6dcd-3255-11ea-b26d-525400ca0151-serving-cert") on node "localhost" DevicePath ""
I0108 20:28:05.679791    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "serving-cert" (UniqueName: "kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-serving-cert") pod "webconsole-85c9b79fdf-56nvm" (UID: "601081ba-3255-11ea-b26d-525400ca0151") 
I0108 20:28:05.681760    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "webconsole-config" (UniqueName: "kubernetes.io/configmap/601081ba-3255-11ea-b26d-525400ca0151-webconsole-config") pod "webconsole-85c9b79fdf-56nvm" (UID: "601081ba-3255-11ea-b26d-525400ca0151") 
I0108 20:28:05.683546    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "webconsole-token-mhn7m" (UniqueName: "kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m") pod "webconsole-85c9b79fdf-56nvm" (UID: "601081ba-3255-11ea-b26d-525400ca0151") 
W0108 20:28:06.473485    5161 status_manager.go:482] Failed to get status for pod "webconsole-85c9b79fdf-56nvm_openshift-web-console(601081ba-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-web-console/pods/webconsole-85c9b79fdf-56nvm: unexpected EOF
E0108 20:28:06.473565    5161 secret.go:198] Couldn't get secret openshift-web-console/webconsole-token-mhn7m: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: unexpected EOF
E0108 20:28:06.473661    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:06.97362722 +0000 UTC m=+312.683411172 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"webconsole-token-mhn7m\" (UniqueName: \"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: unexpected EOF"
E0108 20:28:06.474040    5161 secret.go:198] Couldn't get secret openshift-web-console/webconsole-serving-cert: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-serving-cert: unexpected EOF
E0108 20:28:06.474116    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-serving-cert\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:06.974085011 +0000 UTC m=+312.683868970 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"serving-cert\" (UniqueName: \"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-serving-cert\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-serving-cert: unexpected EOF"
E0108 20:28:06.474156    5161 configmap.go:199] Couldn't get configMap openshift-web-console/webconsole-config: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: unexpected EOF
E0108 20:28:06.474210    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/configmap/601081ba-3255-11ea-b26d-525400ca0151-webconsole-config\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:06.974186364 +0000 UTC m=+312.683970324 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"webconsole-config\" (UniqueName: \"kubernetes.io/configmap/601081ba-3255-11ea-b26d-525400ca0151-webconsole-config\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: unexpected EOF"
E0108 20:28:06.474275    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/openshift-web-console/events: dial tcp 127.0.0.1:8443: connect: connection refused' (may retry after sleeping)
E0108 20:28:06.474331    5161 reflector.go:253] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to watch *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&resourceVersion=2209&timeoutSeconds=385&watch=true: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:06.474379    5161 reflector.go:253] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to watch *v1.Service: Get https://localhost:8443/api/v1/services?resourceVersion=1535&timeoutSeconds=458&watch=true: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:06.474416    5161 reflector.go:253] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to watch *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&resourceVersion=2189&timeoutSeconds=373&watch=true: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:07.042073    5161 secret.go:198] Couldn't get secret openshift-web-console/webconsole-token-mhn7m: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:07.042176    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:08.042142673 +0000 UTC m=+313.751926631 (durationBeforeRetry 1s). Error: "MountVolume.SetUp failed for volume \"webconsole-token-mhn7m\" (UniqueName: \"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:07.042243    5161 configmap.go:199] Couldn't get configMap openshift-web-console/webconsole-config: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:07.042300    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/configmap/601081ba-3255-11ea-b26d-525400ca0151-webconsole-config\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:08.042274766 +0000 UTC m=+313.752058726 (durationBeforeRetry 1s). Error: "MountVolume.SetUp failed for volume \"webconsole-config\" (UniqueName: \"kubernetes.io/configmap/601081ba-3255-11ea-b26d-525400ca0151-webconsole-config\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:07.042346    5161 secret.go:198] Couldn't get secret openshift-web-console/webconsole-serving-cert: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-serving-cert: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:07.042398    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-serving-cert\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:08.042372083 +0000 UTC m=+313.752156044 (durationBeforeRetry 1s). Error: "MountVolume.SetUp failed for volume \"serving-cert\" (UniqueName: \"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-serving-cert\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-serving-cert: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:07.477981    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:07.483674    5161 status_manager.go:482] Failed to get status for pod "docker-registry-1-deploy_default(4735bc66-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/default/pods/docker-registry-1-deploy: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:07.504091    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:07.504173    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:07.582775    5161 status_manager.go:482] Failed to get status for pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/master-api-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:28:07.670996    5161 reconciler.go:181] operationExecutor.UnmountVolume started for volume "deployer-token-kdk2h" (UniqueName: "kubernetes.io/secret/4735bc66-3255-11ea-b26d-525400ca0151-deployer-token-kdk2h") pod "4735bc66-3255-11ea-b26d-525400ca0151" (UID: "4735bc66-3255-11ea-b26d-525400ca0151") 
I0108 20:28:07.765283    5161 operation_generator.go:688] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/4735bc66-3255-11ea-b26d-525400ca0151-deployer-token-kdk2h" (OuterVolumeSpecName: "deployer-token-kdk2h") pod "4735bc66-3255-11ea-b26d-525400ca0151" (UID: "4735bc66-3255-11ea-b26d-525400ca0151"). InnerVolumeSpecName "deployer-token-kdk2h". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0108 20:28:07.774595    5161 reconciler.go:301] Volume detached for volume "deployer-token-kdk2h" (UniqueName: "kubernetes.io/secret/4735bc66-3255-11ea-b26d-525400ca0151-deployer-token-kdk2h") on node "localhost" DevicePath ""
I0108 20:28:07.905940    5161 kuberuntime_manager.go:513] Container {Name:api Image:openshift/origin-hypershift:v3.11.0 Command:[/bin/bash -c] Args:[#!/bin/bash
set -euo pipefail
if [[ -f /etc/origin/master/master.env ]]; then
  set -o allexport
  source /etc/origin/master/master.env
fi
exec hypershift openshift-kube-apiserver --config=/etc/origin/master/master-config.yaml
] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>} {Name:master-data ReadOnly:false MountPath:/var/lib/origin/ SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:healthz,Port:8443,Host:,Scheme:HTTPS,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:28:07.906071    5161 kuberuntime_manager.go:757] checking backoff for container "api" in pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)"
E0108 20:28:08.116033    5161 secret.go:198] Couldn't get secret openshift-web-console/webconsole-serving-cert: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-serving-cert: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:08.116128    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-serving-cert\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:10.116095947 +0000 UTC m=+315.825879900 (durationBeforeRetry 2s). Error: "MountVolume.SetUp failed for volume \"serving-cert\" (UniqueName: \"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-serving-cert\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-serving-cert: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:08.116186    5161 secret.go:198] Couldn't get secret openshift-web-console/webconsole-token-mhn7m: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:08.116244    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:10.116218748 +0000 UTC m=+315.826002701 (durationBeforeRetry 2s). Error: "MountVolume.SetUp failed for volume \"webconsole-token-mhn7m\" (UniqueName: \"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:08.116307    5161 configmap.go:199] Couldn't get configMap openshift-web-console/webconsole-config: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:08.116382    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/configmap/601081ba-3255-11ea-b26d-525400ca0151-webconsole-config\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:10.116342601 +0000 UTC m=+315.826126559 (durationBeforeRetry 2s). Error: "MountVolume.SetUp failed for volume \"webconsole-config\" (UniqueName: \"kubernetes.io/configmap/601081ba-3255-11ea-b26d-525400ca0151-webconsole-config\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:08.484131    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:08.509850    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:08.509915    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:08.697089    5161 pod_container_deletor.go:75] Container "3b6a9602b5126578d7f7c12c683a2953c505bc9541ce682bb61462b0972f9632" not found in pod's containers
E0108 20:28:09.485580    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:09.510383    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:09.512880    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:10.187406    5161 secret.go:198] Couldn't get secret openshift-web-console/webconsole-serving-cert: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-serving-cert: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:10.187503    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-serving-cert\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:14.187466664 +0000 UTC m=+319.897250623 (durationBeforeRetry 4s). Error: "MountVolume.SetUp failed for volume \"serving-cert\" (UniqueName: \"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-serving-cert\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-serving-cert: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:10.187567    5161 secret.go:198] Couldn't get secret openshift-web-console/webconsole-token-mhn7m: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:10.187632    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:14.187605654 +0000 UTC m=+319.897389605 (durationBeforeRetry 4s). Error: "MountVolume.SetUp failed for volume \"webconsole-token-mhn7m\" (UniqueName: \"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:10.187685    5161 configmap.go:199] Couldn't get configMap openshift-web-console/webconsole-config: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:10.187741    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/configmap/601081ba-3255-11ea-b26d-525400ca0151-webconsole-config\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:14.187714043 +0000 UTC m=+319.897498002 (durationBeforeRetry 4s). Error: "MountVolume.SetUp failed for volume \"webconsole-config\" (UniqueName: \"kubernetes.io/configmap/601081ba-3255-11ea-b26d-525400ca0151-webconsole-config\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:10.489782    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:10.521129    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:10.521196    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:11.490455    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:11.525339    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:11.525926    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:12.493600    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:12.535134    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:12.535201    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:12.877028    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/openshift-web-console/events: dial tcp 127.0.0.1:8443: connect: connection refused' (may retry after sleeping)
E0108 20:28:13.495590    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:13.539746    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:13.539826    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:13.706395    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?resourceVersion=0&timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:13.706730    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:13.707021    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:13.707257    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:13.707477    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:13.707489    5161 kubelet_node_status.go:379] Unable to update node status: update node status exceeds retry count
W0108 20:28:13.858309    5161 status_manager.go:482] Failed to get status for pod "kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:13.894303    5161 status_manager.go:482] Failed to get status for pod "openshift-controller-manager-cp9j7_openshift-controller-manager(3334e363-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/pods/openshift-controller-manager-cp9j7: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:13.944704    5161 secret.go:198] Couldn't get secret openshift-controller-manager/openshift-controller-manager-token-t74x8: Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:13.944838    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\" (\"3334e363-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:14.444777516 +0000 UTC m=+320.154561478 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"openshift-controller-manager-token-t74x8\" (UniqueName: \"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\") pod \"openshift-controller-manager-cp9j7\" (UID: \"3334e363-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused"
I0108 20:28:14.160026    5161 kuberuntime_manager.go:513] Container {Name:controllers Image:openshift/origin-hyperkube:v3.11.0 Command:[hyperkube kube-controller-manager] Args:[--enable-dynamic-provisioning=true --use-service-account-credentials=true --leader-elect-retry-period=3s --leader-elect-resource-lock=configmaps --controllers=* --controllers=-ttl --controllers=-bootstrapsigner --controllers=-tokencleaner --controllers=-horizontalpodautoscaling --pod-eviction-timeout=5m --cluster-signing-key-file= --cluster-signing-cert-file= --experimental-cluster-signing-duration=720h --root-ca-file=/etc/origin/master/ca-bundle.crt --port=10252 --service-account-private-key-file=/etc/origin/master/serviceaccounts.private.key --kubeconfig=/etc/origin/master/openshift-master.kubeconfig --openshift-config=/etc/origin/master/master-config.yaml] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:healthz,Port:10252,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:28:14.160141    5161 kuberuntime_manager.go:757] checking backoff for container "controllers" in pod "kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)"
I0108 20:28:14.195270    5161 kuberuntime_manager.go:513] Container {Name:c Image:openshift/origin-hypershift:v3.11.0 Command:[hypershift openshift-controller-manager] Args:[--config=/etc/origin/master/master-config.yaml --v=0] WorkingDir: Ports:[{Name: HostPort:8444 ContainerPort:8444 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>} {Name:openshift-controller-manager-token-t74x8 ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:8444,Host:,Scheme:HTTPS,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:28:14.195380    5161 kuberuntime_manager.go:757] checking backoff for container "c" in pod "openshift-controller-manager-cp9j7_openshift-controller-manager(3334e363-3255-11ea-b26d-525400ca0151)"
E0108 20:28:14.265994    5161 configmap.go:199] Couldn't get configMap openshift-web-console/webconsole-config: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:14.266093    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/configmap/601081ba-3255-11ea-b26d-525400ca0151-webconsole-config\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:22.266060385 +0000 UTC m=+327.975844344 (durationBeforeRetry 8s). Error: "MountVolume.SetUp failed for volume \"webconsole-config\" (UniqueName: \"kubernetes.io/configmap/601081ba-3255-11ea-b26d-525400ca0151-webconsole-config\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:14.266143    5161 secret.go:198] Couldn't get secret openshift-web-console/webconsole-token-mhn7m: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:14.266151    5161 secret.go:198] Couldn't get secret openshift-web-console/webconsole-serving-cert: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-serving-cert: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:14.266209    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-serving-cert\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:22.266183567 +0000 UTC m=+327.975967518 (durationBeforeRetry 8s). Error: "MountVolume.SetUp failed for volume \"serving-cert\" (UniqueName: \"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-serving-cert\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-serving-cert: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:14.266241    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:22.266216651 +0000 UTC m=+327.976000607 (durationBeforeRetry 8s). Error: "MountVolume.SetUp failed for volume \"webconsole-token-mhn7m\" (UniqueName: \"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:14.524251    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:14.524338    5161 secret.go:198] Couldn't get secret openshift-controller-manager/openshift-controller-manager-token-t74x8: Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:14.524432    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\" (\"3334e363-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:15.524392884 +0000 UTC m=+321.234176848 (durationBeforeRetry 1s). Error: "MountVolume.SetUp failed for volume \"openshift-controller-manager-token-t74x8\" (UniqueName: \"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\") pod \"openshift-controller-manager-cp9j7\" (UID: \"3334e363-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:14.542636    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:14.576304    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:14.592577    5161 status_manager.go:482] Failed to get status for pod "kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:14.593380    5161 status_manager.go:482] Failed to get status for pod "webconsole-85c9b79fdf-56nvm_openshift-web-console(601081ba-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-web-console/pods/webconsole-85c9b79fdf-56nvm: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:14.701956    5161 status_manager.go:482] Failed to get status for pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/master-api-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:14.896394    5161 status_manager.go:482] Failed to get status for pod "openshift-controller-manager-cp9j7_openshift-controller-manager(3334e363-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/pods/openshift-controller-manager-cp9j7: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:15.895459    5161 status_manager.go:482] Failed to get status for pod "docker-registry-1-deploy_default(4735bc66-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/default/pods/docker-registry-1-deploy: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:16.096018    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:16.295514    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:16.496684    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:16.701307    5161 secret.go:198] Couldn't get secret openshift-controller-manager/openshift-controller-manager-token-t74x8: Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:16.701419    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\" (\"3334e363-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:18.701381428 +0000 UTC m=+324.411165388 (durationBeforeRetry 2s). Error: "MountVolume.SetUp failed for volume \"openshift-controller-manager-token-t74x8\" (UniqueName: \"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\") pod \"openshift-controller-manager-cp9j7\" (UID: \"3334e363-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused"
W0108 20:28:16.896296    5161 status_manager.go:482] Failed to get status for pod "openshift-web-console-operator-57986c9c4f-4tpgb_openshift-core-operators(437c30e1-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-core-operators/pods/openshift-web-console-operator-57986c9c4f-4tpgb: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:28:17.137981    5161 kuberuntime_manager.go:513] Container {Name:operator Image:openshift/origin-hypershift:v3.11.0 Command:[hypershift experimental openshift-webconsole-operator] Args:[--config=/var/run/configmaps/config/operator-config.yaml -v=0] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:config ReadOnly:false MountPath:/var/run/configmaps/config SubPath: MountPropagation:<nil>} {Name:openshift-web-console-operator-token-l72l6 ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:28:17.138089    5161 kuberuntime_manager.go:757] checking backoff for container "operator" in pod "openshift-web-console-operator-57986c9c4f-4tpgb_openshift-core-operators(437c30e1-3255-11ea-b26d-525400ca0151)"
I0108 20:28:17.139700    5161 kuberuntime_manager.go:513] Container {Name:scheduler Image:openshift/origin-hyperkube:v3.11.0 Command:[hyperkube kube-scheduler] Args:[--leader-elect=true --leader-elect-resource-lock=configmaps --port=10251 --kubeconfig=/etc/origin/master/openshift-master.kubeconfig --policy-config-file=] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:healthz,Port:10251,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:28:17.139778    5161 kuberuntime_manager.go:757] checking backoff for container "scheduler" in pod "kube-scheduler-localhost_kube-system(f903f642800a02b87385310221ffe91f)"
I0108 20:28:17.139946    5161 kuberuntime_manager.go:513] Container {Name:operator Image:openshift/origin-service-serving-cert-signer:v3.11 Command:[service-serving-cert-signer operator] Args:[--config=/var/run/configmaps/config/operator-config.yaml -v=4] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:config ReadOnly:false MountPath:/var/run/configmaps/config SubPath: MountPropagation:<nil>} {Name:openshift-service-cert-signer-operator-token-58289 ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:28:17.140025    5161 kuberuntime_manager.go:757] checking backoff for container "operator" in pod "openshift-service-cert-signer-operator-6d477f986b-wfzn8_openshift-core-operators(e3ab1aff-3254-11ea-b26d-525400ca0151)"
W0108 20:28:17.500339    5161 status_manager.go:482] Failed to get status for pod "kube-scheduler-localhost_kube-system(f903f642800a02b87385310221ffe91f)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:17.707041    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:17.897316    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:18.096326    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:18.298870    5161 status_manager.go:482] Failed to get status for pod "openshift-service-cert-signer-operator-6d477f986b-wfzn8_openshift-core-operators(e3ab1aff-3254-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-core-operators/pods/openshift-service-cert-signer-operator-6d477f986b-wfzn8: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:18.707741    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:18.805554    5161 secret.go:198] Couldn't get secret openshift-controller-manager/openshift-controller-manager-token-t74x8: Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:18.805660    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\" (\"3334e363-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:22.805620175 +0000 UTC m=+328.515404125 (durationBeforeRetry 4s). Error: "MountVolume.SetUp failed for volume \"openshift-controller-manager-token-t74x8\" (UniqueName: \"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\") pod \"openshift-controller-manager-cp9j7\" (UID: \"3334e363-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:18.898512    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:19.097079    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:19.708390    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:19.899140    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:20.097764    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:20.709149    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:20.899766    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:21.098496    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:21.709930    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:21.909793    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:22.103425    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:22.354230    5161 configmap.go:199] Couldn't get configMap openshift-web-console/webconsole-config: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:22.354343    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/configmap/601081ba-3255-11ea-b26d-525400ca0151-webconsole-config\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:38.354307728 +0000 UTC m=+344.064091686 (durationBeforeRetry 16s). Error: "MountVolume.SetUp failed for volume \"webconsole-config\" (UniqueName: \"kubernetes.io/configmap/601081ba-3255-11ea-b26d-525400ca0151-webconsole-config\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:22.354409    5161 secret.go:198] Couldn't get secret openshift-web-console/webconsole-serving-cert: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-serving-cert: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:22.354487    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-serving-cert\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:38.35446054 +0000 UTC m=+344.064244491 (durationBeforeRetry 16s). Error: "MountVolume.SetUp failed for volume \"serving-cert\" (UniqueName: \"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-serving-cert\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-serving-cert: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:22.354537    5161 secret.go:198] Couldn't get secret openshift-web-console/webconsole-token-mhn7m: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:22.354592    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:38.354566307 +0000 UTC m=+344.064350267 (durationBeforeRetry 16s). Error: "MountVolume.SetUp failed for volume \"webconsole-token-mhn7m\" (UniqueName: \"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:22.715394    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:22.877647    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/openshift-web-console/events: dial tcp 127.0.0.1:8443: connect: connection refused' (may retry after sleeping)
E0108 20:28:22.892667    5161 secret.go:198] Couldn't get secret openshift-controller-manager/openshift-controller-manager-token-t74x8: Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:22.892769    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\" (\"3334e363-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:30.892731894 +0000 UTC m=+336.602515852 (durationBeforeRetry 8s). Error: "MountVolume.SetUp failed for volume \"openshift-controller-manager-token-t74x8\" (UniqueName: \"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\") pod \"openshift-controller-manager-cp9j7\" (UID: \"3334e363-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:22.911386    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:23.106267    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:23.708447    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?resourceVersion=0&timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:23.708780    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:23.709091    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:23.709339    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:23.709569    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:23.709580    5161 kubelet_node_status.go:379] Unable to update node status: update node status exceeds retry count
E0108 20:28:23.716377    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:23.912086    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:24.107009    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:24.587933    5161 status_manager.go:482] Failed to get status for pod "openshift-web-console-operator-57986c9c4f-4tpgb_openshift-core-operators(437c30e1-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-core-operators/pods/openshift-web-console-operator-57986c9c4f-4tpgb: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:24.588294    5161 status_manager.go:482] Failed to get status for pod "kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:24.588562    5161 status_manager.go:482] Failed to get status for pod "kube-scheduler-localhost_kube-system(f903f642800a02b87385310221ffe91f)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:24.588834    5161 status_manager.go:482] Failed to get status for pod "webconsole-85c9b79fdf-56nvm_openshift-web-console(601081ba-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-web-console/pods/webconsole-85c9b79fdf-56nvm: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:24.589084    5161 status_manager.go:482] Failed to get status for pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/master-api-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:24.589330    5161 status_manager.go:482] Failed to get status for pod "openshift-service-cert-signer-operator-6d477f986b-wfzn8_openshift-core-operators(e3ab1aff-3254-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-core-operators/pods/openshift-service-cert-signer-operator-6d477f986b-wfzn8: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:24.589589    5161 status_manager.go:482] Failed to get status for pod "openshift-controller-manager-cp9j7_openshift-controller-manager(3334e363-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/pods/openshift-controller-manager-cp9j7: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:24.589851    5161 status_manager.go:482] Failed to get status for pod "docker-registry-1-deploy_default(4735bc66-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/default/pods/docker-registry-1-deploy: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:24.719438    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:24.914777    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:25.096387    5161 status_manager.go:482] Failed to get status for pod "docker-registry-1-tlcmc_default(4ef4964d-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/default/pods/docker-registry-1-tlcmc: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:25.299338    5161 secret.go:198] Couldn't get secret default/registry-token-b8dwv: Get https://localhost:8443/api/v1/namespaces/default/secrets/registry-token-b8dwv: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:25.299447    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/4ef4964d-3255-11ea-b26d-525400ca0151-registry-token-b8dwv\" (\"4ef4964d-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:25.799410546 +0000 UTC m=+331.509194506 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"registry-token-b8dwv\" (UniqueName: \"kubernetes.io/secret/4ef4964d-3255-11ea-b26d-525400ca0151-registry-token-b8dwv\") pod \"docker-registry-1-tlcmc\" (UID: \"4ef4964d-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/default/secrets/registry-token-b8dwv: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:25.504789    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:25.722423    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:25.895488    5161 secret.go:198] Couldn't get secret default/registry-token-b8dwv: Get https://localhost:8443/api/v1/namespaces/default/secrets/registry-token-b8dwv: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:25.895581    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/4ef4964d-3255-11ea-b26d-525400ca0151-registry-token-b8dwv\" (\"4ef4964d-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:26.895556337 +0000 UTC m=+332.605340288 (durationBeforeRetry 1s). Error: "MountVolume.SetUp failed for volume \"registry-token-b8dwv\" (UniqueName: \"kubernetes.io/secret/4ef4964d-3255-11ea-b26d-525400ca0151-registry-token-b8dwv\") pod \"docker-registry-1-tlcmc\" (UID: \"4ef4964d-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/default/secrets/registry-token-b8dwv: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:26.095477    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:26.505597    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:26.723902    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:27.017057    5161 secret.go:198] Couldn't get secret default/registry-token-b8dwv: Get https://localhost:8443/api/v1/namespaces/default/secrets/registry-token-b8dwv: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:27.017162    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/4ef4964d-3255-11ea-b26d-525400ca0151-registry-token-b8dwv\" (\"4ef4964d-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:29.017125828 +0000 UTC m=+334.726909790 (durationBeforeRetry 2s). Error: "MountVolume.SetUp failed for volume \"registry-token-b8dwv\" (UniqueName: \"kubernetes.io/secret/4ef4964d-3255-11ea-b26d-525400ca0151-registry-token-b8dwv\") pod \"docker-registry-1-tlcmc\" (UID: \"4ef4964d-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/default/secrets/registry-token-b8dwv: dial tcp 127.0.0.1:8443: connect: connection refused"
W0108 20:28:27.145390    5161 status_manager.go:482] Failed to get status for pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/master-api-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:33.173657    5161 fsHandler.go:121] failed to collect filesystem stats - rootDiskErr: du command failed on /rootfs/var/lib/docker/overlay2/08596b42921e7870e1883c2e6819353b00226f3a6cb0fb04e763eac9eb2c2160/diff with output stdout: , stderr: du: cannot access '/rootfs/var/lib/docker/overlay2/08596b42921e7870e1883c2e6819353b00226f3a6cb0fb04e763eac9eb2c2160/diff': No such file or directory
 - exit status 1, rootInodeErr: cmd [ionice -c3 nice -n 19 find /rootfs/var/lib/docker/overlay2/08596b42921e7870e1883c2e6819353b00226f3a6cb0fb04e763eac9eb2c2160/diff -xdev -printf .] failed. stderr: find: '/rootfs/var/lib/docker/overlay2/08596b42921e7870e1883c2e6819353b00226f3a6cb0fb04e763eac9eb2c2160/diff': No such file or directory
; err: exit status 1, extraDiskErr: du command failed on /rootfs/var/lib/docker/containers/f540e0650efd421e1968c2b1dc8d2977f6af99b1b9dea73cf51ecf3e9d4d8650 with output stdout: , stderr: du: cannot access '/rootfs/var/lib/docker/containers/f540e0650efd421e1968c2b1dc8d2977f6af99b1b9dea73cf51ecf3e9d4d8650': No such file or directory
 - exit status 1
E0108 20:28:37.177555    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: net/http: TLS handshake timeout
W0108 20:28:37.404043    5161 status_manager.go:482] Failed to get status for pod "docker-registry-1-tlcmc_default(4ef4964d-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/default/pods/docker-registry-1-tlcmc: net/http: TLS handshake timeout
E0108 20:28:37.516048    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: net/http: TLS handshake timeout
E0108 20:28:37.725073    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: net/http: TLS handshake timeout
E0108 20:28:38.323265    5161 secret.go:198] Couldn't get secret default/registry-token-b8dwv: secrets "registry-token-b8dwv" is forbidden: User "system:node:localhost" cannot get secrets in the namespace "default": no path found to object
no RBAC policy matched
E0108 20:28:38.323357    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/4ef4964d-3255-11ea-b26d-525400ca0151-registry-token-b8dwv\" (\"4ef4964d-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:42.32332111 +0000 UTC m=+348.033105072 (durationBeforeRetry 4s). Error: "MountVolume.SetUp failed for volume \"registry-token-b8dwv\" (UniqueName: \"kubernetes.io/secret/4ef4964d-3255-11ea-b26d-525400ca0151-registry-token-b8dwv\") pod \"docker-registry-1-tlcmc\" (UID: \"4ef4964d-3255-11ea-b26d-525400ca0151\") : secrets \"registry-token-b8dwv\" is forbidden: User \"system:node:localhost\" cannot get secrets in the namespace \"default\": no path found to object\nno RBAC policy matched"
E0108 20:28:48.417037    5161 event.go:203] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"webconsole-85c9b79fdf-56nvm.15e8034a2ac873c1", GenerateName:"", Namespace:"openshift-web-console", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"openshift-web-console", Name:"webconsole-85c9b79fdf-56nvm", UID:"601081ba-3255-11ea-b26d-525400ca0151", APIVersion:"v1", ResourceVersion:"2209", FieldPath:""}, Reason:"FailedMount", Message:"MountVolume.SetUp failed for volume \"webconsole-token-mhn7m\" : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: unexpected EOF", Source:v1.EventSource{Component:"kubelet", Host:"localhost"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xbf7dacb59c3a97c1, ext:312183385943, loc:(*time.Location)(0x90a9e60)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xbf7dacb59c3a97c1, ext:312183385943, loc:(*time.Location)(0x90a9e60)}}, Count:1, Type:"Warning", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "webconsole-85c9b79fdf-56nvm.15e8034a2ac873c1" is forbidden: caches not synchronized' (will not retry!)
E0108 20:28:49.944311    5161 fsHandler.go:121] failed to collect filesystem stats - rootDiskErr: du command failed on /rootfs/var/lib/docker/overlay2/09402e4780d187d94e37dd2158a81eb0c3776a1dcc00610bdd36c84684963053/diff with output stdout: , stderr: du: cannot access '/rootfs/var/lib/docker/overlay2/09402e4780d187d94e37dd2158a81eb0c3776a1dcc00610bdd36c84684963053/diff': No such file or directory
 - exit status 1, rootInodeErr: cmd [ionice -c3 nice -n 19 find /rootfs/var/lib/docker/overlay2/09402e4780d187d94e37dd2158a81eb0c3776a1dcc00610bdd36c84684963053/diff -xdev -printf .] failed. stderr: find: '/rootfs/var/lib/docker/overlay2/09402e4780d187d94e37dd2158a81eb0c3776a1dcc00610bdd36c84684963053/diff': No such file or directory
; err: exit status 1, extraDiskErr: du command failed on /rootfs/var/lib/docker/containers/e3374a630d8aa4282a7386a8d47eab203e1e32d526fb8328294ae90a6796e158 with output stdout: , stderr: du: cannot access '/rootfs/var/lib/docker/containers/e3374a630d8aa4282a7386a8d47eab203e1e32d526fb8328294ae90a6796e158': No such file or directory
 - exit status 1
I0108 20:28:56.857108    5161 kuberuntime_manager.go:513] Container {Name:apiserver Image:openshift/origin-hypershift:v3.11.0 Command:[hypershift openshift-apiserver] Args:[--config=/etc/origin/master/master-config.yaml -v=0] WorkingDir: Ports:[{Name: HostPort:8445 ContainerPort:8445 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:ADDITIONAL_ALLOWED_REGISTRIES Value:registry.centos.org ValueFrom:nil}] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:serving-cert ReadOnly:false MountPath:/var/serving-cert SubPath: MountPropagation:<nil>} {Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>} {Name:openshift-apiserver-token-dg6ph ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:8445,Host:,Scheme:HTTPS,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:28:56.857424    5161 kuberuntime_manager.go:757] checking backoff for container "apiserver" in pod "openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)"
E0108 20:28:57.890397    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/openshift-web-console/events: unexpected EOF; some request body already written' (may retry after sleeping)
E0108 20:28:57.890620    5161 reflector.go:253] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to watch *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&resourceVersion=2261&timeoutSeconds=509&watch=true: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:57.890677    5161 reflector.go:253] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to watch *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&resourceVersion=2246&timeoutSeconds=309&watch=true: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:57.890727    5161 reflector.go:253] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to watch *v1.Service: Get https://localhost:8443/api/v1/services?resourceVersion=2213&timeoutSeconds=588&watch=true: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:58.671567    5161 status_manager.go:482] Failed to get status for pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/master-api-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:58.895872    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:58.895941    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:58.896008    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?resourceVersion=0&timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:58.896292    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:58.896537    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:58.896773    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:58.897058    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:58.897072    5161 kubelet_node_status.go:379] Unable to update node status: update node status exceeds retry count
E0108 20:28:58.897483    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:28:58.971479    5161 kuberuntime_manager.go:513] Container {Name:api Image:openshift/origin-hypershift:v3.11.0 Command:[/bin/bash -c] Args:[#!/bin/bash
set -euo pipefail
if [[ -f /etc/origin/master/master.env ]]; then
  set -o allexport
  source /etc/origin/master/master.env
fi
exec hypershift openshift-kube-apiserver --config=/etc/origin/master/master-config.yaml
] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>} {Name:master-data ReadOnly:false MountPath:/var/lib/origin/ SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:healthz,Port:8443,Host:,Scheme:HTTPS,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:28:58.971583    5161 kuberuntime_manager.go:757] checking backoff for container "api" in pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)"
I0108 20:28:58.971691    5161 kuberuntime_manager.go:767] Back-off 10s restarting failed container=api pod=master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)
E0108 20:28:58.971745    5161 pod_workers.go:186] Error syncing pod 29e68324ed097a2c36aa5709e9b67154 ("master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)"), skipping: failed to "StartContainer" for "api" with CrashLoopBackOff: "Back-off 10s restarting failed container=api pod=master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)"
W0108 20:28:59.703068    5161 status_manager.go:482] Failed to get status for pod "kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:59.897829    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:59.897909    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:59.905411    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:29:00.003468    5161 kuberuntime_manager.go:513] Container {Name:controllers Image:openshift/origin-hyperkube:v3.11.0 Command:[hyperkube kube-controller-manager] Args:[--enable-dynamic-provisioning=true --use-service-account-credentials=true --leader-elect-retry-period=3s --leader-elect-resource-lock=configmaps --controllers=* --controllers=-ttl --controllers=-bootstrapsigner --controllers=-tokencleaner --controllers=-horizontalpodautoscaling --pod-eviction-timeout=5m --cluster-signing-key-file= --cluster-signing-cert-file= --experimental-cluster-signing-duration=720h --root-ca-file=/etc/origin/master/ca-bundle.crt --port=10252 --service-account-private-key-file=/etc/origin/master/serviceaccounts.private.key --kubeconfig=/etc/origin/master/openshift-master.kubeconfig --openshift-config=/etc/origin/master/master-config.yaml] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:healthz,Port:10252,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:29:00.003589    5161 kuberuntime_manager.go:757] checking backoff for container "controllers" in pod "kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)"
I0108 20:29:00.003694    5161 kuberuntime_manager.go:767] Back-off 10s restarting failed container=controllers pod=kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)
E0108 20:29:00.003727    5161 pod_workers.go:186] Error syncing pod dfcadfa6552711112062fbf1121a691c ("kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)"), skipping: failed to "StartContainer" for "controllers" with CrashLoopBackOff: "Back-off 10s restarting failed container=controllers pod=kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)"
E0108 20:29:00.177484    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/openshift-web-console/events: dial tcp 127.0.0.1:8443: connect: connection refused' (may retry after sleeping)
W0108 20:29:00.742038    5161 status_manager.go:482] Failed to get status for pod "openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-apiserver/pods/openshift-apiserver-6q9ps: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:00.751279    5161 status_manager.go:482] Failed to get status for pod "persistent-volume-setup-5mzds_default(3567cf6f-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/default/pods/persistent-volume-setup-5mzds: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:29:00.881497    5161 reconciler.go:181] operationExecutor.UnmountVolume started for volume "pvinstaller-token-zgfb9" (UniqueName: "kubernetes.io/secret/3567cf6f-3255-11ea-b26d-525400ca0151-pvinstaller-token-zgfb9") pod "3567cf6f-3255-11ea-b26d-525400ca0151" (UID: "3567cf6f-3255-11ea-b26d-525400ca0151") 
I0108 20:29:00.881553    5161 reconciler.go:181] operationExecutor.UnmountVolume started for volume "pvdir" (UniqueName: "kubernetes.io/host-path/3567cf6f-3255-11ea-b26d-525400ca0151-pvdir") pod "3567cf6f-3255-11ea-b26d-525400ca0151" (UID: "3567cf6f-3255-11ea-b26d-525400ca0151") 
I0108 20:29:00.881669    5161 operation_generator.go:688] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/3567cf6f-3255-11ea-b26d-525400ca0151-pvdir" (OuterVolumeSpecName: "pvdir") pod "3567cf6f-3255-11ea-b26d-525400ca0151" (UID: "3567cf6f-3255-11ea-b26d-525400ca0151"). InnerVolumeSpecName "pvdir". PluginName "kubernetes.io/host-path", VolumeGidValue ""
E0108 20:29:00.898966    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:00.908417    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:29:00.915790    5161 operation_generator.go:688] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/3567cf6f-3255-11ea-b26d-525400ca0151-pvinstaller-token-zgfb9" (OuterVolumeSpecName: "pvinstaller-token-zgfb9") pod "3567cf6f-3255-11ea-b26d-525400ca0151" (UID: "3567cf6f-3255-11ea-b26d-525400ca0151"). InnerVolumeSpecName "pvinstaller-token-zgfb9". PluginName "kubernetes.io/secret", VolumeGidValue ""
E0108 20:29:00.915962    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:29:00.981817    5161 reconciler.go:301] Volume detached for volume "pvdir" (UniqueName: "kubernetes.io/host-path/3567cf6f-3255-11ea-b26d-525400ca0151-pvdir") on node "localhost" DevicePath ""
I0108 20:29:00.981847    5161 reconciler.go:301] Volume detached for volume "pvinstaller-token-zgfb9" (UniqueName: "kubernetes.io/secret/3567cf6f-3255-11ea-b26d-525400ca0151-pvinstaller-token-zgfb9") on node "localhost" DevicePath ""
W0108 20:29:01.780734    5161 pod_container_deletor.go:75] Container "0431875f86073a1df2c3f4c1dca7e3510b509a4d58d90618903dca1b69971b56" not found in pod's containers
W0108 20:29:01.792932    5161 status_manager.go:482] Failed to get status for pod "openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-apiserver/pods/openshift-apiserver-6q9ps: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:01.899547    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:01.909074    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:01.916533    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:29:02.092639    5161 kuberuntime_manager.go:513] Container {Name:apiserver Image:openshift/origin-hypershift:v3.11.0 Command:[hypershift openshift-apiserver] Args:[--config=/etc/origin/master/master-config.yaml -v=0] WorkingDir: Ports:[{Name: HostPort:8445 ContainerPort:8445 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:ADDITIONAL_ALLOWED_REGISTRIES Value:registry.centos.org ValueFrom:nil}] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:serving-cert ReadOnly:false MountPath:/var/serving-cert SubPath: MountPropagation:<nil>} {Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>} {Name:openshift-apiserver-token-dg6ph ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:8445,Host:,Scheme:HTTPS,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:29:02.092783    5161 kuberuntime_manager.go:757] checking backoff for container "apiserver" in pod "openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)"
I0108 20:29:02.092938    5161 kuberuntime_manager.go:767] Back-off 10s restarting failed container=apiserver pod=openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)
E0108 20:29:02.092977    5161 pod_workers.go:186] Error syncing pod e3681100-3254-11ea-b26d-525400ca0151 ("openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)"), skipping: failed to "StartContainer" for "apiserver" with CrashLoopBackOff: "Back-off 10s restarting failed container=apiserver pod=openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)"
E0108 20:29:02.900203    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:02.910877    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:02.917206    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:03.900913    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:03.911468    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:03.917802    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:04.531572    5161 status_manager.go:482] Failed to get status for pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/master-api-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:04.582898    5161 status_manager.go:482] Failed to get status for pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/master-api-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:04.583222    5161 status_manager.go:482] Failed to get status for pod "persistent-volume-setup-5mzds_default(3567cf6f-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/default/pods/persistent-volume-setup-5mzds: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:04.583486    5161 status_manager.go:482] Failed to get status for pod "kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:04.583737    5161 status_manager.go:482] Failed to get status for pod "openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-apiserver/pods/openshift-apiserver-6q9ps: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:29:04.832049    5161 kuberuntime_manager.go:513] Container {Name:api Image:openshift/origin-hypershift:v3.11.0 Command:[/bin/bash -c] Args:[#!/bin/bash
set -euo pipefail
if [[ -f /etc/origin/master/master.env ]]; then
  set -o allexport
  source /etc/origin/master/master.env
fi
exec hypershift openshift-kube-apiserver --config=/etc/origin/master/master-config.yaml
] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>} {Name:master-data ReadOnly:false MountPath:/var/lib/origin/ SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:healthz,Port:8443,Host:,Scheme:HTTPS,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:29:04.832168    5161 kuberuntime_manager.go:757] checking backoff for container "api" in pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)"
I0108 20:29:04.832275    5161 kuberuntime_manager.go:767] Back-off 10s restarting failed container=api pod=master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)
E0108 20:29:04.832306    5161 pod_workers.go:186] Error syncing pod 29e68324ed097a2c36aa5709e9b67154 ("master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)"), skipping: failed to "StartContainer" for "api" with CrashLoopBackOff: "Back-off 10s restarting failed container=api pod=master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)"
E0108 20:29:04.901605    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:04.912110    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:04.918504    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:05.854701    5161 status_manager.go:482] Failed to get status for pod "openshift-controller-manager-cp9j7_openshift-controller-manager(3334e363-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/pods/openshift-controller-manager-cp9j7: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:05.902339    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:05.913188    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:05.919266    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:06.006321    5161 secret.go:198] Couldn't get secret openshift-controller-manager/openshift-controller-manager-token-t74x8: Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:06.006450    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\" (\"3334e363-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:29:06.506400989 +0000 UTC m=+372.216184953 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"openshift-controller-manager-token-t74x8\" (UniqueName: \"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\") pod \"openshift-controller-manager-cp9j7\" (UID: \"3334e363-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused"
I0108 20:29:06.154581    5161 kuberuntime_manager.go:513] Container {Name:c Image:openshift/origin-hypershift:v3.11.0 Command:[hypershift openshift-controller-manager] Args:[--config=/etc/origin/master/master-config.yaml --v=0] WorkingDir: Ports:[{Name: HostPort:8444 ContainerPort:8444 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>} {Name:openshift-controller-manager-token-t74x8 ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:8444,Host:,Scheme:HTTPS,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:29:06.154744    5161 kuberuntime_manager.go:757] checking backoff for container "c" in pod "openshift-controller-manager-cp9j7_openshift-controller-manager(3334e363-3255-11ea-b26d-525400ca0151)"
I0108 20:29:06.154925    5161 kuberuntime_manager.go:767] Back-off 10s restarting failed container=c pod=openshift-controller-manager-cp9j7_openshift-controller-manager(3334e363-3255-11ea-b26d-525400ca0151)
E0108 20:29:06.154973    5161 pod_workers.go:186] Error syncing pod 3334e363-3255-11ea-b26d-525400ca0151 ("openshift-controller-manager-cp9j7_openshift-controller-manager(3334e363-3255-11ea-b26d-525400ca0151)"), skipping: failed to "StartContainer" for "c" with CrashLoopBackOff: "Back-off 10s restarting failed container=c pod=openshift-controller-manager-cp9j7_openshift-controller-manager(3334e363-3255-11ea-b26d-525400ca0151)"
E0108 20:29:06.507861    5161 secret.go:198] Couldn't get secret openshift-controller-manager/openshift-controller-manager-token-t74x8: Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:06.508004    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\" (\"3334e363-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:29:07.50794785 +0000 UTC m=+373.217731812 (durationBeforeRetry 1s). Error: "MountVolume.SetUp failed for volume \"openshift-controller-manager-token-t74x8\" (UniqueName: \"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\") pod \"openshift-controller-manager-cp9j7\" (UID: \"3334e363-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:29:06.903120    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:06.913880    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:06.919882    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:07.173433    5161 status_manager.go:482] Failed to get status for pod "kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:29:07.473411    5161 kuberuntime_manager.go:513] Container {Name:controllers Image:openshift/origin-hyperkube:v3.11.0 Command:[hyperkube kube-controller-manager] Args:[--enable-dynamic-provisioning=true --use-service-account-credentials=true --leader-elect-retry-period=3s --leader-elect-resource-lock=configmaps --controllers=* --controllers=-ttl --controllers=-bootstrapsigner --controllers=-tokencleaner --controllers=-horizontalpodautoscaling --pod-eviction-timeout=5m --cluster-signing-key-file= --cluster-signing-cert-file= --experimental-cluster-signing-duration=720h --root-ca-file=/etc/origin/master/ca-bundle.crt --port=10252 --service-account-private-key-file=/etc/origin/master/serviceaccounts.private.key --kubeconfig=/etc/origin/master/openshift-master.kubeconfig --openshift-config=/etc/origin/master/master-config.yaml] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:healthz,Port:10252,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:29:07.473524    5161 kuberuntime_manager.go:757] checking backoff for container "controllers" in pod "kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)"
I0108 20:29:07.473633    5161 kuberuntime_manager.go:767] Back-off 10s restarting failed container=controllers pod=kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)
E0108 20:29:07.473665    5161 pod_workers.go:186] Error syncing pod dfcadfa6552711112062fbf1121a691c ("kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)"), skipping: failed to "StartContainer" for "controllers" with CrashLoopBackOff: "Back-off 10s restarting failed container=controllers pod=kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)"
E0108 20:29:07.510969    5161 secret.go:198] Couldn't get secret openshift-controller-manager/openshift-controller-manager-token-t74x8: Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:07.511095    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\" (\"3334e363-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:29:09.511047359 +0000 UTC m=+375.220831329 (durationBeforeRetry 2s). Error: "MountVolume.SetUp failed for volume \"openshift-controller-manager-token-t74x8\" (UniqueName: \"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\") pod \"openshift-controller-manager-cp9j7\" (UID: \"3334e363-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:29:07.905078    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:07.915353    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:07.921389    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:08.901065    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?resourceVersion=0&timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:08.901992    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:08.903159    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:08.903537    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:08.903884    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:08.903897    5161 kubelet_node_status.go:379] Unable to update node status: update node status exceeds retry count
E0108 20:29:08.908253    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:08.916006    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:08.922026    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:09.518035    5161 secret.go:198] Couldn't get secret openshift-controller-manager/openshift-controller-manager-token-t74x8: Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:09.518143    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\" (\"3334e363-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:29:13.518104454 +0000 UTC m=+379.227888413 (durationBeforeRetry 4s). Error: "MountVolume.SetUp failed for volume \"openshift-controller-manager-token-t74x8\" (UniqueName: \"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\") pod \"openshift-controller-manager-cp9j7\" (UID: \"3334e363-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:29:09.909399    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:09.917431    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:09.922631    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:10.178222    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/openshift-web-console/events: dial tcp 127.0.0.1:8443: connect: connection refused' (may retry after sleeping)
E0108 20:29:10.910139    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:10.918075    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:10.926067    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:11.910918    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:11.918743    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:11.933547    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:12.911650    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:12.919370    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:12.934123    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:13.540869    5161 secret.go:198] Couldn't get secret openshift-controller-manager/openshift-controller-manager-token-t74x8: Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:13.540991    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\" (\"3334e363-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:29:21.540953758 +0000 UTC m=+387.250737718 (durationBeforeRetry 8s). Error: "MountVolume.SetUp failed for volume \"openshift-controller-manager-token-t74x8\" (UniqueName: \"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\") pod \"openshift-controller-manager-cp9j7\" (UID: \"3334e363-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused"
W0108 20:29:13.582633    5161 status_manager.go:482] Failed to get status for pod "openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-apiserver/pods/openshift-apiserver-6q9ps: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:29:13.882653    5161 kuberuntime_manager.go:513] Container {Name:apiserver Image:openshift/origin-hypershift:v3.11.0 Command:[hypershift openshift-apiserver] Args:[--config=/etc/origin/master/master-config.yaml -v=0] WorkingDir: Ports:[{Name: HostPort:8445 ContainerPort:8445 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:ADDITIONAL_ALLOWED_REGISTRIES Value:registry.centos.org ValueFrom:nil}] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:serving-cert ReadOnly:false MountPath:/var/serving-cert SubPath: MountPropagation:<nil>} {Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>} {Name:openshift-apiserver-token-dg6ph ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:8445,Host:,Scheme:HTTPS,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:29:13.882746    5161 kuberuntime_manager.go:757] checking backoff for container "apiserver" in pod "openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)"
E0108 20:29:13.912322    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:13.920035    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:13.934704    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:14.582896    5161 status_manager.go:482] Failed to get status for pod "persistent-volume-setup-5mzds_default(3567cf6f-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/default/pods/persistent-volume-setup-5mzds: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:14.583200    5161 status_manager.go:482] Failed to get status for pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/master-api-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:14.583524    5161 status_manager.go:482] Failed to get status for pod "openshift-controller-manager-cp9j7_openshift-controller-manager(3334e363-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/pods/openshift-controller-manager-cp9j7: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:14.583780    5161 status_manager.go:482] Failed to get status for pod "openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-apiserver/pods/openshift-apiserver-6q9ps: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:14.584068    5161 status_manager.go:482] Failed to get status for pod "kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:14.913165    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:14.920730    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:14.935308    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:15.913935    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:15.922071    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:15.935889    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:16.914647    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:16.923749    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:16.936534    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:17.034583    5161 status_manager.go:482] Failed to get status for pod "openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-apiserver/pods/openshift-apiserver-6q9ps: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:17.593246    5161 status_manager.go:482] Failed to get status for pod "openshift-controller-manager-cp9j7_openshift-controller-manager(3334e363-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/pods/openshift-controller-manager-cp9j7: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:29:17.882555    5161 kuberuntime_manager.go:513] Container {Name:c Image:openshift/origin-hypershift:v3.11.0 Command:[hypershift openshift-controller-manager] Args:[--config=/etc/origin/master/master-config.yaml --v=0] WorkingDir: Ports:[{Name: HostPort:8444 ContainerPort:8444 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>} {Name:openshift-controller-manager-token-t74x8 ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:8444,Host:,Scheme:HTTPS,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:29:17.882704    5161 kuberuntime_manager.go:757] checking backoff for container "c" in pod "openshift-controller-manager-cp9j7_openshift-controller-manager(3334e363-3255-11ea-b26d-525400ca0151)"
E0108 20:29:17.915444    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:17.924350    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:17.937200    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:18.056778    5161 status_manager.go:482] Failed to get status for pod "openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-apiserver/pods/openshift-apiserver-6q9ps: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:29:18.356496    5161 kuberuntime_manager.go:513] Container {Name:apiserver Image:openshift/origin-hypershift:v3.11.0 Command:[hypershift openshift-apiserver] Args:[--config=/etc/origin/master/master-config.yaml -v=0] WorkingDir: Ports:[{Name: HostPort:8445 ContainerPort:8445 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:ADDITIONAL_ALLOWED_REGISTRIES Value:registry.centos.org ValueFrom:nil}] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:serving-cert ReadOnly:false MountPath:/var/serving-cert SubPath: MountPropagation:<nil>} {Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>} {Name:openshift-apiserver-token-dg6ph ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:8445,Host:,Scheme:HTTPS,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:29:18.356678    5161 kuberuntime_manager.go:757] checking backoff for container "apiserver" in pod "openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)"
I0108 20:29:18.356849    5161 kuberuntime_manager.go:767] Back-off 20s restarting failed container=apiserver pod=openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)
E0108 20:29:18.356895    5161 pod_workers.go:186] Error syncing pod e3681100-3254-11ea-b26d-525400ca0151 ("openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)"), skipping: failed to "StartContainer" for "apiserver" with CrashLoopBackOff: "Back-off 20s restarting failed container=apiserver pod=openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)"
I0108 20:29:18.882587    5161 kuberuntime_manager.go:513] Container {Name:api Image:openshift/origin-hypershift:v3.11.0 Command:[/bin/bash -c] Args:[#!/bin/bash
set -euo pipefail
if [[ -f /etc/origin/master/master.env ]]; then
  set -o allexport
  source /etc/origin/master/master.env
fi
exec hypershift openshift-kube-apiserver --config=/etc/origin/master/master-config.yaml
] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>} {Name:master-data ReadOnly:false MountPath:/var/lib/origin/ SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:healthz,Port:8443,Host:,Scheme:HTTPS,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:29:18.882727    5161 kuberuntime_manager.go:757] checking backoff for container "api" in pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)"
E0108 20:29:18.904737    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?resourceVersion=0&timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:18.905233    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:18.905566    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:18.905916    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:18.906197    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:18.906210    5161 kubelet_node_status.go:379] Unable to update node status: update node status exceeds retry count
E0108 20:29:18.916152    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:18.925046    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:18.937875    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:19.916995    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:19.925657    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:19.938599    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:20.179039    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/openshift-web-console/events: dial tcp 127.0.0.1:8443: connect: connection refused' (may retry after sleeping)
I0108 20:29:20.889364    5161 kuberuntime_manager.go:513] Container {Name:controllers Image:openshift/origin-hyperkube:v3.11.0 Command:[hyperkube kube-controller-manager] Args:[--enable-dynamic-provisioning=true --use-service-account-credentials=true --leader-elect-retry-period=3s --leader-elect-resource-lock=configmaps --controllers=* --controllers=-ttl --controllers=-bootstrapsigner --controllers=-tokencleaner --controllers=-horizontalpodautoscaling --pod-eviction-timeout=5m --cluster-signing-key-file= --cluster-signing-cert-file= --experimental-cluster-signing-duration=720h --root-ca-file=/etc/origin/master/ca-bundle.crt --port=10252 --service-account-private-key-file=/etc/origin/master/serviceaccounts.private.key --kubeconfig=/etc/origin/master/openshift-master.kubeconfig --openshift-config=/etc/origin/master/master-config.yaml] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:healthz,Port:10252,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:29:20.889480    5161 kuberuntime_manager.go:757] checking backoff for container "controllers" in pod "kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)"
E0108 20:29:20.920353    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:20.926554    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:20.939245    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:21.108850    5161 status_manager.go:482] Failed to get status for pod "openshift-controller-manager-cp9j7_openshift-controller-manager(3334e363-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/pods/openshift-controller-manager-cp9j7: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:21.610270    5161 secret.go:198] Couldn't get secret openshift-controller-manager/openshift-controller-manager-token-t74x8: Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:21.610367    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\" (\"3334e363-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:29:37.610328638 +0000 UTC m=+403.320112598 (durationBeforeRetry 16s). Error: "MountVolume.SetUp failed for volume \"openshift-controller-manager-token-t74x8\" (UniqueName: \"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\") pod \"openshift-controller-manager-cp9j7\" (UID: \"3334e363-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:29:21.921115    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:21.927189    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:21.939867    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:22.923454    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:22.930920    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:22.945953    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:23.147048    5161 status_manager.go:482] Failed to get status for pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/master-api-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:29:32.885197    5161 kuberuntime_manager.go:513] Container {Name:apiserver Image:openshift/origin-hypershift:v3.11.0 Command:[hypershift openshift-apiserver] Args:[--config=/etc/origin/master/master-config.yaml -v=0] WorkingDir: Ports:[{Name: HostPort:8445 ContainerPort:8445 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:ADDITIONAL_ALLOWED_REGISTRIES Value:registry.centos.org ValueFrom:nil}] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:serving-cert ReadOnly:false MountPath:/var/serving-cert SubPath: MountPropagation:<nil>} {Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>} {Name:openshift-apiserver-token-dg6ph ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:8445,Host:,Scheme:HTTPS,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:29:32.885319    5161 kuberuntime_manager.go:757] checking backoff for container "apiserver" in pod "openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)"
I0108 20:29:32.885451    5161 kuberuntime_manager.go:767] Back-off 20s restarting failed container=apiserver pod=openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)
E0108 20:29:32.885481    5161 pod_workers.go:186] Error syncing pod e3681100-3254-11ea-b26d-525400ca0151 ("openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)"), skipping: failed to "StartContainer" for "apiserver" with CrashLoopBackOff: "Back-off 20s restarting failed container=apiserver pod=openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)"
E0108 20:29:42.441294    5161 event.go:203] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"webconsole-85c9b79fdf-56nvm.15e8034a2acfb9d4", GenerateName:"", Namespace:"openshift-web-console", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"openshift-web-console", Name:"webconsole-85c9b79fdf-56nvm", UID:"601081ba-3255-11ea-b26d-525400ca0151", APIVersion:"v1", ResourceVersion:"2209", FieldPath:""}, Reason:"FailedMount", Message:"MountVolume.SetUp failed for volume \"serving-cert\" : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-serving-cert: unexpected EOF", Source:v1.EventSource{Component:"kubelet", Host:"localhost"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xbf7dacb59c41ddd4, ext:312183862637, loc:(*time.Location)(0x90a9e60)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xbf7dacb59c41ddd4, ext:312183862637, loc:(*time.Location)(0x90a9e60)}}, Count:1, Type:"Warning", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "webconsole-85c9b79fdf-56nvm.15e8034a2acfb9d4" is forbidden: caches not synchronized' (will not retry!)
I0108 20:29:44.891378    5161 kuberuntime_manager.go:513] Container {Name:apiserver Image:openshift/origin-hypershift:v3.11.0 Command:[hypershift openshift-apiserver] Args:[--config=/etc/origin/master/master-config.yaml -v=0] WorkingDir: Ports:[{Name: HostPort:8445 ContainerPort:8445 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:ADDITIONAL_ALLOWED_REGISTRIES Value:registry.centos.org ValueFrom:nil}] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:serving-cert ReadOnly:false MountPath:/var/serving-cert SubPath: MountPropagation:<nil>} {Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>} {Name:openshift-apiserver-token-dg6ph ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:8445,Host:,Scheme:HTTPS,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:29:44.891611    5161 kuberuntime_manager.go:757] checking backoff for container "apiserver" in pod "openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)"
E0108 20:29:46.909749    5161 fsHandler.go:121] failed to collect filesystem stats - rootDiskErr: du command failed on /rootfs/var/lib/docker/overlay2/9bf4e5f09bb371d003cd87d90cb071df7d8aa4e916843d299bdb7f1af7955015/diff with output stdout: , stderr: du: cannot access '/rootfs/var/lib/docker/overlay2/9bf4e5f09bb371d003cd87d90cb071df7d8aa4e916843d299bdb7f1af7955015/diff': No such file or directory
 - exit status 1, rootInodeErr: cmd [ionice -c3 nice -n 19 find /rootfs/var/lib/docker/overlay2/9bf4e5f09bb371d003cd87d90cb071df7d8aa4e916843d299bdb7f1af7955015/diff -xdev -printf .] failed. stderr: find: '/rootfs/var/lib/docker/overlay2/9bf4e5f09bb371d003cd87d90cb071df7d8aa4e916843d299bdb7f1af7955015/diff': No such file or directory
; err: exit status 1, extraDiskErr: du command failed on /rootfs/var/lib/docker/containers/86830f935d1d57650e1b1992626083583ca5a8e1629ca63befce6e195258de4f with output stdout: , stderr: du: cannot access '/rootfs/var/lib/docker/containers/86830f935d1d57650e1b1992626083583ca5a8e1629ca63befce6e195258de4f': No such file or directory
 - exit status 1
E0108 20:29:52.449069    5161 event.go:203] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"webconsole-85c9b79fdf-56nvm.15e8034a2ad13e24", GenerateName:"", Namespace:"openshift-web-console", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"openshift-web-console", Name:"webconsole-85c9b79fdf-56nvm", UID:"601081ba-3255-11ea-b26d-525400ca0151", APIVersion:"v1", ResourceVersion:"2209", FieldPath:""}, Reason:"FailedMount", Message:"MountVolume.SetUp failed for volume \"webconsole-config\" : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: unexpected EOF", Source:v1.EventSource{Component:"kubelet", Host:"localhost"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xbf7dacb59c436224, ext:312183962040, loc:(*time.Location)(0x90a9e60)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xbf7dacb59c436224, ext:312183962040, loc:(*time.Location)(0x90a9e60)}}, Count:1, Type:"Warning", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "webconsole-85c9b79fdf-56nvm.15e8034a2ad13e24" is forbidden: caches not synchronized' (will not retry!)
E0108 20:30:02.461020    5161 event.go:203] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"webconsole-85c9b79fdf-56nvm.15e8034a4cab8f77", GenerateName:"", Namespace:"openshift-web-console", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"openshift-web-console", Name:"webconsole-85c9b79fdf-56nvm", UID:"601081ba-3255-11ea-b26d-525400ca0151", APIVersion:"v1", ResourceVersion:"2209", FieldPath:""}, Reason:"FailedMount", Message:"MountVolume.SetUp failed for volume \"webconsole-token-mhn7m\" : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: dial tcp 127.0.0.1:8443: connect: connection refused", Source:v1.EventSource{Component:"kubelet", Host:"localhost"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xbf7dacb5c282e977, ext:312751917848, loc:(*time.Location)(0x90a9e60)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xbf7dacb5c282e977, ext:312751917848, loc:(*time.Location)(0x90a9e60)}}, Count:1, Type:"Warning", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "webconsole-85c9b79fdf-56nvm.15e8034a4cab8f77" is forbidden: caches not synchronized' (will not retry!)
I0108 20:30:05.871003    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "pvinstaller-token-zgfb9" (UniqueName: "kubernetes.io/secret/a7a020dd-3255-11ea-93c0-525400ca0151-pvinstaller-token-zgfb9") pod "persistent-volume-setup-rsqjw" (UID: "a7a020dd-3255-11ea-93c0-525400ca0151") 
I0108 20:30:05.871067    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "pvdir" (UniqueName: "kubernetes.io/host-path/a7a020dd-3255-11ea-93c0-525400ca0151-pvdir") pod "persistent-volume-setup-rsqjw" (UID: "a7a020dd-3255-11ea-93c0-525400ca0151") 
W0108 20:30:09.283992    5161 pod_container_deletor.go:75] Container "ceb2ac2047dfb279fd6097c44e4a01515e23c86357601c40bfa83a90f144c74c" not found in pod's containers
E0108 20:30:12.500345    5161 event.go:203] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"webconsole-85c9b79fdf-56nvm.15e8034a4cad9e2e", GenerateName:"", Namespace:"openshift-web-console", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"openshift-web-console", Name:"webconsole-85c9b79fdf-56nvm", UID:"601081ba-3255-11ea-b26d-525400ca0151", APIVersion:"v1", ResourceVersion:"2209", FieldPath:""}, Reason:"FailedMount", Message:"MountVolume.SetUp failed for volume \"webconsole-config\" : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: dial tcp 127.0.0.1:8443: connect: connection refused", Source:v1.EventSource{Component:"kubelet", Host:"localhost"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xbf7dacb5c284f82e, ext:312752052678, loc:(*time.Location)(0x90a9e60)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xbf7dacb5c284f82e, ext:312752052678, loc:(*time.Location)(0x90a9e60)}}, Count:1, Type:"Warning", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "webconsole-85c9b79fdf-56nvm.15e8034a4cad9e2e" is forbidden: caches not synchronized' (will not retry!)
I0108 20:30:13.438987    5161 reconciler.go:181] operationExecutor.UnmountVolume started for volume "registry-token-b8dwv" (UniqueName: "kubernetes.io/secret/4ef4964d-3255-11ea-b26d-525400ca0151-registry-token-b8dwv") pod "4ef4964d-3255-11ea-b26d-525400ca0151" (UID: "4ef4964d-3255-11ea-b26d-525400ca0151") 
I0108 20:30:13.439677    5161 reconciler.go:181] operationExecutor.UnmountVolume started for volume "registry-storage" (UniqueName: "kubernetes.io/host-path/4ef4964d-3255-11ea-b26d-525400ca0151-registry-storage") pod "4ef4964d-3255-11ea-b26d-525400ca0151" (UID: "4ef4964d-3255-11ea-b26d-525400ca0151") 
I0108 20:30:13.439794    5161 operation_generator.go:688] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/4ef4964d-3255-11ea-b26d-525400ca0151-registry-storage" (OuterVolumeSpecName: "registry-storage") pod "4ef4964d-3255-11ea-b26d-525400ca0151" (UID: "4ef4964d-3255-11ea-b26d-525400ca0151"). InnerVolumeSpecName "registry-storage". PluginName "kubernetes.io/host-path", VolumeGidValue ""
I0108 20:30:13.494867    5161 operation_generator.go:688] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/4ef4964d-3255-11ea-b26d-525400ca0151-registry-token-b8dwv" (OuterVolumeSpecName: "registry-token-b8dwv") pod "4ef4964d-3255-11ea-b26d-525400ca0151" (UID: "4ef4964d-3255-11ea-b26d-525400ca0151"). InnerVolumeSpecName "registry-token-b8dwv". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0108 20:30:13.542945    5161 reconciler.go:301] Volume detached for volume "registry-token-b8dwv" (UniqueName: "kubernetes.io/secret/4ef4964d-3255-11ea-b26d-525400ca0151-registry-token-b8dwv") on node "localhost" DevicePath ""
I0108 20:30:13.542978    5161 reconciler.go:301] Volume detached for volume "registry-storage" (UniqueName: "kubernetes.io/host-path/4ef4964d-3255-11ea-b26d-525400ca0151-registry-storage") on node "localhost" DevicePath ""
E0108 20:30:41.524972    5161 fsHandler.go:121] failed to collect filesystem stats - rootDiskErr: du command failed on /rootfs/var/lib/docker/overlay2/47862618f433eecdcbf51495f269d669c1299ef0749762ef3d016472236f0105/diff with output stdout: , stderr: du: cannot access '/rootfs/var/lib/docker/overlay2/47862618f433eecdcbf51495f269d669c1299ef0749762ef3d016472236f0105/diff': No such file or directory
 - exit status 1, rootInodeErr: cmd [ionice -c3 nice -n 19 find /rootfs/var/lib/docker/overlay2/47862618f433eecdcbf51495f269d669c1299ef0749762ef3d016472236f0105/diff -xdev -printf .] failed. stderr: find: '/rootfs/var/lib/docker/overlay2/47862618f433eecdcbf51495f269d669c1299ef0749762ef3d016472236f0105/diff': No such file or directory
; err: exit status 1, extraDiskErr: du command failed on /rootfs/var/lib/docker/containers/4f6fb72c066a516f9f318de90182a551a4e6d275a088472e8ddbe4d66aec3411 with output stdout: , stderr: du: cannot access '/rootfs/var/lib/docker/containers/4f6fb72c066a516f9f318de90182a551a4e6d275a088472e8ddbe4d66aec3411': No such file or directory
 - exit status 1
I0108 20:31:44.628542    5161 reconciler.go:181] operationExecutor.UnmountVolume started for volume "pvdir" (UniqueName: "kubernetes.io/host-path/a7a020dd-3255-11ea-93c0-525400ca0151-pvdir") pod "a7a020dd-3255-11ea-93c0-525400ca0151" (UID: "a7a020dd-3255-11ea-93c0-525400ca0151") 
I0108 20:31:44.628604    5161 reconciler.go:181] operationExecutor.UnmountVolume started for volume "pvinstaller-token-zgfb9" (UniqueName: "kubernetes.io/secret/a7a020dd-3255-11ea-93c0-525400ca0151-pvinstaller-token-zgfb9") pod "a7a020dd-3255-11ea-93c0-525400ca0151" (UID: "a7a020dd-3255-11ea-93c0-525400ca0151") 
I0108 20:31:44.646723    5161 operation_generator.go:688] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/a7a020dd-3255-11ea-93c0-525400ca0151-pvdir" (OuterVolumeSpecName: "pvdir") pod "a7a020dd-3255-11ea-93c0-525400ca0151" (UID: "a7a020dd-3255-11ea-93c0-525400ca0151"). InnerVolumeSpecName "pvdir". PluginName "kubernetes.io/host-path", VolumeGidValue ""
I0108 20:31:44.729066    5161 reconciler.go:301] Volume detached for volume "pvdir" (UniqueName: "kubernetes.io/host-path/a7a020dd-3255-11ea-93c0-525400ca0151-pvdir") on node "localhost" DevicePath ""
I0108 20:31:44.795262    5161 operation_generator.go:688] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/a7a020dd-3255-11ea-93c0-525400ca0151-pvinstaller-token-zgfb9" (OuterVolumeSpecName: "pvinstaller-token-zgfb9") pod "a7a020dd-3255-11ea-93c0-525400ca0151" (UID: "a7a020dd-3255-11ea-93c0-525400ca0151"). InnerVolumeSpecName "pvinstaller-token-zgfb9". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0108 20:31:44.829330    5161 reconciler.go:301] Volume detached for volume "pvinstaller-token-zgfb9" (UniqueName: "kubernetes.io/secret/a7a020dd-3255-11ea-93c0-525400ca0151-pvinstaller-token-zgfb9") on node "localhost" DevicePath ""
W0108 20:31:45.891550    5161 pod_container_deletor.go:75] Container "ceb2ac2047dfb279fd6097c44e4a01515e23c86357601c40bfa83a90f144c74c" not found in pod's containers
I0108 21:28:54.101247    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "deployer-token-kdk2h" (UniqueName: "kubernetes.io/secret/dda50dcf-325d-11ea-93c0-525400ca0151-deployer-token-kdk2h") pod "docker-registry-2-deploy" (UID: "dda50dcf-325d-11ea-93c0-525400ca0151") 
I0108 21:28:54.321147    5161 reconciler.go:207] 
### General information

  * Minishift version: 
  * OS: Windows
  * Hypervisor: Hyper-V


### Steps to reproduce

  1. minishift ip --set-static

I am not using the default switch, I include info that I think may be helpful in the logs section.
From the source code at **pkg/minishift/network/utils.go** I don't see where they asign a value to VMSwitch (although I have no experience with go), could this be a bug?

### Expected
Success?

### Actual
Not supported for Default switch on HyperV

### Logs
```
C:\>minishift ip --set-static --show-libmachine-logs -v5
-- minishift version: v1.34.2+83ebaab
Found binary path at C:\minishift\minishift.exe
Launching plugin server for driver hyperv
Plugin server listening at address 127.0.0.1:59227
() Calling .GetVersion
Using API Version  1
() Calling .SetConfigRaw
() Calling .GetMachineName
(an6) Calling .GetState
(an6) DBG | [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM an6 ).state
(an6) DBG | [stdout =====>] : Running
(an6) DBG |
(an6) DBG | [stderr =====>] :
Not supported for Default switch on HyperV
```

```
C:\>minishift config view
- hyperv-virtual-switch              : minishift-external
```

```
C:\>powershell Get-VMSwitch

Name               SwitchType NetAdapterInterfaceDescription
----               ---------- ------------------------------
minishift-external External   Intel(R) Dual Band Wireless-AC 3165
Default Switch     Internal
DockerNAT          Internal
```

```
C:\>minishift profile list
- an6           Running         (Active)
- minishift     Stopped
```

```
C:\>minishift status
Minishift:  Running
Profile:    an6
OpenShift:  Running (openshift v3.11.0+7876dd5-361)
DiskUsage:  15% of 19G (Mounted On: /mnt/sda1)
CacheUsage: 507.6 MB (used by oc binary, ISO or cached images)
```

You can start Minishift with `minishift start --show-libmachine-logs -v5` to collect logs.
Please consider posting this on http://gist.github.com/ and post the link in the issue.


### General information

  * Minishift version: v1.24.0+8a904d0
  * OS: macOS
  * Hypervisor: VirtualBox


### Steps to reproduce

  1. oc login -u system:admin
  2. oc new-project demo
  3. oc label ns demo opa-controlled=true
  4. oc adm policy add-scc-to-user privileged -z opa-sa
  5. oc new-project opa
  6. Clone https://github.com/raffaelespazzoli/openshift-opa 
  7. cd inside   
  8. `helm template ./charts/open-policy-agent --namespace opa --set kubernetes_policy_controller.image_tag=2.0 --set kubernetes_policy_controller.image=quay.io/raffaelespazzoli/kubernetes-policy-controller --set caBundle=$CA_BUNDLE --set log_level=debug | oc apply -f  - -n opa`
  9. oc create cm test1 --from-file=./examples/authorization-webhooks/unreadable_secrets.rego -n opa
  10. `minishift ssh` 
  11. cd into /var/lib/minisifhit/openshift.local.config/master
  12. Create **opa-policy-controller.kubeconfig** to current path with 
```
# Kubernetes API version
apiVersion: v1
# kind of the API object
kind: Config
# clusters refers to the remote service.
clusters:
  - name: opa-server
    cluster:
      # CA for verifying the remote service.
      certificate-authority: /var/lib/minishift/openshift.local.config/master/ca-bundle.crt
      # URL of remote service to query. Must use 'https'. May not include parameters.
      server: https://opa.opa.svc

# users refers to the API Server's webhook configuration.
users:
  - name: opa-user
    user:
      client-certificate: /var/lib/minishift/openshift.local.config/master/master.kubelet-client.crt # cert for the webhook plugin to use
      client-key: /var/lib/minishift/openshift.local.config/master/master.kubelet-client.key          # key matching the cert

# kubeconfig files require a context. Provide one for the API Server.
current-context: opa-webhook
contexts:
- context:
    cluster: opa-server
    user: opa-user
  name: opa-webhook
```
  13. `chown docker:docker opa-policy-controller.kubeconfig`
  14. `chmod 7777 opa-policy-controller.kubeconfig`
  15. Create **audit-policy.yaml** to current path
```
apiVersion: audit.k8s.io/v1beta1
kind: Policy
rules:
  - level: RequestResponse
    userGroups: ["system:authenticated"]
    resources:
      - group: ""
        resources: ["serviceaccounts", "secrets"]
    omitStages:
      - "RequestReceived"
```
  16. `chown docker:docker audit-policy.yaml`
  17.  `chmod 7777 audit-policy.yaml`
  18. Update **master-config.yaml** to
```
admissionConfig:
  pluginConfig:
    MutatingAdmissionWebhook:
      configuration:
        apiVersion: apiserver.config.k8s.io/v1alpha1
        kind: WebhookAdmission
        kubeConfigFile: /dev/null
    ValidatingAdmissionWebhook:
      configuration:
        apiVersion: apiserver.config.k8s.io/v1alpha1
        kind: WebhookAdmission
        kubeConfigFile: /dev/null
    GenericAdmissionWebhook:
      configuration:
        apiVersion: v1
        disable: false
        kind: DefaultAdmissionConfig
      location: ""
    openshift.io/ImagePolicy:
      configuration:
        apiVersion: v1
        executionRules:
          - matchImageAnnotations:
              - key: images.openshift.io/deny-execution
                value: "true"
            name: execution-denied
            onResources:
              - resource: pods
              - resource: builds
            reject: true
            skipOnResolutionFailure: true
        kind: ImagePolicyConfig
      location: ""
aggregatorConfig:
  proxyClientInfo:
    certFile: aggregator-front-proxy.crt
    keyFile: aggregator-front-proxy.key
apiLevels:
  - v1
apiVersion: v1
auditConfig:
  enabled: true
  policyFile: audit-policy.yaml
  auditFilePath: audit.log
  logFormat: json
authConfig:
  requestHeader:
    clientCA: front-proxy-ca.crt
    clientCommonNames:
      - aggregator-front-proxy
    extraHeaderPrefixes:
      - X-Remote-Extra-
    groupHeaders:
      - X-Remote-Group
    usernameHeaders:
      - X-Remote-User
controllerConfig:
  controllers:
    - '*'
  election: null
  serviceServingCert:
    signer:
      certFile: service-signer.crt
      keyFile: service-signer.key
controllerLeaseTTL: 0
controllers: '*'
corsAllowedOrigins:
  - //127\.0\.0\.1(:|$)
  - //192\.168\.99\.108:8443$
  - //localhost(:|$)
disabledFeatures: null
dnsConfig:
  allowRecursiveQueries: true
  bindAddress: 0.0.0.0:8053
  bindNetwork: tcp4
etcdClientInfo:
  ca: ca.crt
  certFile: master.etcd-client.crt
  keyFile: master.etcd-client.key
  urls:
    - https://127.0.0.1:4001
etcdConfig:
  address: 127.0.0.1:4001
  peerAddress: 127.0.0.1:7001
  peerServingInfo:
    bindAddress: 0.0.0.0:7001
    bindNetwork: tcp4
    certFile: etcd.server.crt
    clientCA: ca.crt
    keyFile: etcd.server.key
    namedCertificates: null
  servingInfo:
    bindAddress: 0.0.0.0:4001
    bindNetwork: tcp4
    certFile: etcd.server.crt
    clientCA: ca.crt
    keyFile: etcd.server.key
    namedCertificates: null
  storageDirectory: /var/lib/origin/openshift.local.etcd
etcdStorageConfig:
  kubernetesStoragePrefix: kubernetes.io
  kubernetesStorageVersion: v1
  openShiftStoragePrefix: openshift.io
  openShiftStorageVersion: v1
imageConfig:
  format: openshift/origin-${component}:v3.9.0
  latest: false
imagePolicyConfig:
  allowedRegistriesForImport:
    - domainName: docker.io
    - domainName: '*.docker.io'
    - domainName: '*.redhat.com'
    - domainName: gcr.io
    - domainName: quay.io
    - domainName: registry.centos.org
    - domainName: registry.redhat.io
    - domainName: '*.amazonaws.com'
  disableScheduledImport: false
  maxImagesBulkImportedPerRepository: 5
  maxScheduledImageImportsPerMinute: 60
  scheduledImageImportMinimumIntervalSeconds: 900
jenkinsPipelineConfig:
  autoProvisionEnabled: true
  parameters: null
  serviceName: jenkins
  templateName: jenkins-persistent
  templateNamespace: openshift
kind: MasterConfig
kubeletClientInfo:
  ca: ca.crt
  certFile: master.kubelet-client.crt
  keyFile: master.kubelet-client.key
  port: 10250
kubernetesMasterConfig:
  admissionConfig:
    pluginConfig: null
  apiLevels: null
  apiServerArguments:
    authorization-mode:
      - Node
      - Webhook
      - RBAC
    authorization-webhook-config-file:
      - opa-policy-controller.kubeconfig
    runtime-config:
      - apis/admissionregistration.k8s.io/v1alpha1=true
    storage-backend:
      - etcd3
    storage-media-type:
      - application/vnd.kubernetes.protobuf
  controllerArguments: null
  disabledAPIGroupVersions: {}
  masterCount: 1
  masterEndpointReconcileTTL: 15
  masterIP: 127.0.0.1
  podEvictionTimeout: 5m
  proxyClientInfo:
    certFile: master.proxy-client.crt
    keyFile: master.proxy-client.key
  schedulerArguments: null
  schedulerConfigFile: ""
  servicesNodePortRange: 30000-32767
  servicesSubnet: 172.30.0.0/16
  staticNodeNames: null
masterClients:
  externalKubernetesClientConnectionOverrides:
    acceptContentTypes: application/vnd.kubernetes.protobuf,application/json
    burst: 400
    contentType: application/vnd.kubernetes.protobuf
    qps: 200
  externalKubernetesKubeConfig: ""
  openshiftLoopbackClientConnectionOverrides:
    acceptContentTypes: application/vnd.kubernetes.protobuf,application/json
    burst: 600
    contentType: application/vnd.kubernetes.protobuf
    qps: 300
  openshiftLoopbackKubeConfig: openshift-master.kubeconfig
masterPublicURL: https://192.168.99.108:8443
networkConfig:
  clusterNetworkCIDR: 10.128.0.0/14
  clusterNetworks:
    - cidr: 10.128.0.0/14
      hostSubnetLength: 9
  externalIPNetworkCIDRs: null
  hostSubnetLength: 9
  ingressIPNetworkCIDR: 172.29.0.0/16
  networkPluginName: ""
  serviceNetworkCIDR: 172.30.0.0/16
oauthConfig:
  alwaysShowProviderSelection: false
  assetPublicURL: https://192.168.99.108:8443/console/
  grantConfig:
    method: auto
    serviceAccountMethod: prompt
  identityProviders:
    - challenge: true
      login: true
      mappingMethod: claim
      name: anypassword
      provider:
        apiVersion: v1
        kind: AllowAllPasswordIdentityProvider
  masterCA: ca-bundle.crt
  masterPublicURL: https://192.168.99.108:8443
  masterURL: https://127.0.0.1:8443
  sessionConfig:
    sessionMaxAgeSeconds: 300
    sessionName: ssn
    sessionSecretsFile: ""
  templates: null
  tokenConfig:
    accessTokenMaxAgeSeconds: 86400
    authorizeTokenMaxAgeSeconds: 300
pauseControllers: false
policyConfig:
  bootstrapPolicyFile: policy.json
  openshiftInfrastructureNamespace: openshift-infra
  openshiftSharedResourcesNamespace: openshift
  userAgentMatchingConfig:
    defaultRejectionMessage: ""
    deniedClients: null
    requiredClients: null
projectConfig:
  defaultNodeSelector: ""
  projectRequestMessage: ""
  projectRequestTemplate: ""
  securityAllocator:
    mcsAllocatorRange: s0:/2
    mcsLabelsPerProject: 5
    uidAllocatorRange: 1000000000-1999999999/10000
routingConfig:
  subdomain: 192.168.99.108.nip.io
serviceAccountConfig:
  limitSecretReferences: false
  managedNames:
    - default
    - builder
    - deployer
  masterCA: ca-bundle.crt
  privateKeyFile: serviceaccounts.private.key
  publicKeyFiles:
    - serviceaccounts.public.key
servingInfo:
  bindAddress: 0.0.0.0:8443
  bindNetwork: tcp4
  certFile: master.server.crt
  clientCA: ca.crt
  keyFile: master.server.key
  maxRequestsInFlight: 1200
  namedCertificates: null
  requestTimeoutSeconds: 3600
volumeConfig:
  dynamicProvisioningEnabled: true
```
  19. `chown docker:docker master-config.yaml`
  20.  `chmod 7777 master-config.yaml`
  21.  `minishift stop`
  22. `minishift start`
  23. `minishift ssh` 
  24. cd into /var/lib/minisifhit/openshift.local.config/master
  25. `oc login -u system:admin -n demo`  
  26.a `tail -n 100 -F audit.log`
  26.b `oc get secrets` / `oc get secret [samplesecret] -o yaml`

### Expected
1. I should not be able to view secrets
2. Or at least (for debugging) see some logs if authorization webhook mode is properly configured or not (or what's the problem) was ittrying to send it to the webhook or not

### Actual
1. Secrets/tokens are still viewable
2. Can't see 'webhook' or 'SubjectAccessReview' string in the audit.log whenever I trying to oc get/describe secrets in the demo namespace

### Logs
https://gist.github.com/lambotchi/0e9016cc33165dc212caea3f386da84f

You can start Minishift with `minishift start --show-libmachine-logs -v5` to collect logs.
Please consider posting this on http://gist.github.com/ and post the link in the issue.


### General information
In the minishift, having openshift internal docker registry running and exposed using "Edge" connection. The pods and services are running. 

But when I tried to login to the docker internal registry running inside openshift (minishift), i could not login. i am seeing the below error.

/var/lib/docker$ docker login -u chak -p Naqp6NScYF7zOcKN41SuYQ045qR9zBN6lfGVnvxhrUE https://docker-registry-default.192.168.42.186.nip.io
**WARNING! Using --password via the CLI is insecure. Use --password-stdin.
Error response from daemon: Get http://docker-registry-default.192.168.42.186.nip.io/v2/: net/http: request canceled (Client.Timeout exceeded while awaiting headers)**

  * Minishift version:  minishift v1.34.1+c2ff9cb
  * OS: Linux / macOS / Windows
  * Hypervisor: KVM / xhyve / Hyper-V / VirtualBox / hyperkit


### Steps to reproduce

  1. start the minishift
  2. login to minishift console and make sure docker-registry and route are running. The connection shoud be "Edge" route.
  3. add the external name of the internal docker registry to localhost/HOST docker daemon
  4.  Login to docker registry using docker login.

### Expected
To login to Minishift OC internal docker registry.

### Actual
seeing the errors.
What am I doing wrong here, Please suggest. I have many ways to get this working but no luck. Right now, I have to login to minishift vm using minishift ssh and docker pull/push the images to 172.30.1.1:5000 manually everytime. 

![image](https://user-images.githubusercontent.com/18241061/71188112-19e69200-225f-11ea-9862-64052be893be.png)
![image](https://user-images.githubusercontent.com/18241061/71188136-279c1780-225f-11ea-8af4-91601a5c4542.png)



Bumps [rack](https://github.com/rack/rack) from 2.0.5 to 2.0.8.
<details>
<summary>Changelog</summary>

*Sourced from [rack's changelog](https://github.com/rack/rack/blob/master/CHANGELOG.md).*

> # Changelog
> 
> All notable changes to this project will be documented in this file. For info on how to format all future additions to this file please reference [Keep A Changelog](https://keepachangelog.com/en/1.0.0/).
> 
> ## Unreleased
> 
> _Note: There are many unreleased changes in Rack (`master` is around 300 commits ahead of `2-0-stable`), and below is not an exhaustive list. If you would like to help out and document some of the unreleased changes, PRs are welcome._ 
> 
> ### Added
> 
> ### Changed
> 
> - Use `Time#httpdate` format for Expires, as proposed by RFC 7231. ([@&#8203;nanaya](https://github.com/nanaya))
> - Make `Utils.status_code` raise an error when the status symbol is invalid instead of `500`.
> - Rename `Request::SCHEME_WHITELIST` to `Request::ALLOWED_SCHEMES`.
> - Make `Multipart::Parser.get_filename` accept files with `+` in their name.
> - Add Falcon to the default handler fallbacks. ([@&#8203;ioquatix](https://github.com/ioquatix))
> - Update codebase to avoid string mutations in preparation for `frozen_string_literals`. ([@&#8203;pat](https://github.com/pat))
> - Change `MockRequest#env_for` to rely on the input optionally responding to `#size` instead of `#length`. ([@&#8203;janko](https://github.com/janko))
> - Rename `Rack::File` -> `Rack::Files` and add deprecation notice. ([@&#8203;postmodern](https://github.com/postmodern)).
> 
> ### Removed
> 
> ### Documentation
> 
> - Update broken example in `Session::Abstract::ID` documentation. ([tonytonyjan](https://github.com/tonytonyjan))
> - Add Padrino to the list of frameworks implmenting Rack. ([@&#8203;wikimatze](https://github.com/wikimatze))
> - Remove Mongrel from the suggested server options in the help output. ([@&#8203;tricknotes](https://github.com/tricknotes))
> - Replace `HISTORY.md` and `NEWS.md` with `CHANGELOG.md`. ([@&#8203;twitnithegirl](https://github.com/twitnithegirl))
> - Backfill `CHANGELOG.md` from 2.0.1 to 2.0.7 releases. ([@&#8203;drenmi](https://github.com/Drenmi))
> 
> ## [2.0.7] - 2019-04-02
> 
> ### Fixed
> 
> - Remove calls to `#eof?` on Rack input in `Multipart::Parser`, as this breaks the specification. ([@&#8203;matthewd](https://github.com/matthewd))
> - Preserve forwarded IP addresses for trusted proxy chains. ([@&#8203;SamSaffron](https://github.com/SamSaffron))
> 
> ## [2.0.6] - 2018-11-05
> 
> ### Fixed
> 
> - [[CVE-2018-16470](https://nvd.nist.gov/vuln/detail/CVE-2018-16470)] Reduce buffer size of `Multipart::Parser` to avoid pathological parsing. ([@&#8203;tenderlove](https://github.com/tenderlove))
> - Fix a call to a non-existing method `#accepts_html` in the `ShowExceptions` middleware. ([@&#8203;tomelm](https://github.com/tomelm))
> - [[CVE-2018-16471](https://nvd.nist.gov/vuln/detail/CVE-2018-16471)] Whitelist HTTP and HTTPS schemes in `Request#scheme` to prevent a possible XSS attack. ([@&#8203;PatrickTulskie](https://github.com/PatrickTulskie))
</details>
<details>
<summary>Commits</summary>

- [`e7ee459`](https://github.com/rack/rack/commit/e7ee459546d217f32afc83e0b168c5eb9f95d784) Bumping version
- [`f1a79b2`](https://github.com/rack/rack/commit/f1a79b208c4ea877420beee62646e0b146402bd0) Introduce a new base class to avoid breaking when upgrading
- [`5b1cab6`](https://github.com/rack/rack/commit/5b1cab667270d7ad1a4d2088adf5ff4eb9845496) Add a version prefix to the private id to make easier to migrate old values
- [`1e96e0f`](https://github.com/rack/rack/commit/1e96e0f197777458216bb3dfdbcce57a0bbba0c5) Fallback to the public id when reading the session in the pool adapter
- [`3ba123d`](https://github.com/rack/rack/commit/3ba123d278f1085ba78fc000df954e507af2d622) Also drop the session with the public id when destroying sessions
- [`6a04bbf`](https://github.com/rack/rack/commit/6a04bbf6b742c305d3a56f9bd6242e6c943cc2ad) Fallback to the legacy id when the new id is not found
- [`dc45a06`](https://github.com/rack/rack/commit/dc45a06b339c707c1f658c123ec7216151878f7a) Add the private id
- [`73a5f79`](https://github.com/rack/rack/commit/73a5f79f6854eed81ecc3e5fb9f8154e967ccc49) revert conditionals to master
- [`4e32262`](https://github.com/rack/rack/commit/4e322629e0c6698c75a3fb541a42571f8543c34c) remove NullSession
- [`1c7e3b2`](https://github.com/rack/rack/commit/1c7e3b259f0741c869dcfbabeb3e0670c4d3f848) remove || raise and get closer to master
- Additional commits viewable in [compare view](https://github.com/rack/rack/compare/2.0.5...2.0.8)
</details>
<br />

[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=rack&package-manager=bundler&previous-version=2.0.5&new-version=2.0.8)](https://help.github.com/articles/configuring-automated-security-fixes)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot ignore this [patch|minor|major] version` will close this PR and stop Dependabot creating any more for this minor/major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/minishift/minishift/network/alerts).

</details>
### General information

  * Minishift version: v1.34.1+c2ff9cb
  * OS:  Windows 10
  * Hypervisor: KVM / xhyve / Hyper-V / VirtualBox / hyperkit
I am using docker desktop in Windows machine.


### Steps to reproduce

minishift start
-- Starting profile 'minishift'
-- Check if deprecated options are used ... OK
-- Checking if https://github.com is reachable ... OK
-- Checking if requested OpenShift version 'v3.11.0' is valid ... OK
-- Checking if requested OpenShift version 'v3.11.0' is supported ... OK
-- Checking if requested hypervisor 'hyperv' is supported on this platform ... OK
-- Checking if Powershell is available ... OK
-- Checking if Hyper-V driver is installed ... OK
-- Checking if Hyper-V driver is configured to use a Virtual Switch ...
   'Default Switch' ... OK
-- Checking if user is a member of the Hyper-V Administrators group ... FAIL
   See the 'Setting Up the Virtualization Environment' topic (https://docs.okd.io/latest/minishift/getting-started/setting-up-virtualization-environment.html) for more information


### Expected


### Actual


### Logs
minishift start
-- Starting profile 'minishift'
-- Check if deprecated options are used ... OK
-- Checking if https://github.com is reachable ... OK
-- Checking if requested OpenShift version 'v3.11.0' is valid ... OK
-- Checking if requested OpenShift version 'v3.11.0' is supported ... OK
-- Checking if requested hypervisor 'hyperv' is supported on this platform ... OK
-- Checking if Powershell is available ... OK
-- Checking if Hyper-V driver is installed ... OK
-- Checking if Hyper-V driver is configured to use a Virtual Switch ...
   'Default Switch' ... OK
-- Checking if user is a member of the Hyper-V Administrators group ... FAIL
   See the 'Setting Up the Virtualization Environment' topic (https://docs.okd.io/latest/minishift/getting-started/setting-up-virtualization-environment.html) for more information



### General information
I have to create minishift vm everyday and perform the setup that I did previous day inorder to continue. I believe minishift is not made that way. 
Minishift is not working or I am not doing right way inorder to use the exising minishift vm.

  * Minishift version: 
 minishift v1.34.1+c2ff9cb
  * OS: Linux / macOS / Windows
 Linux arun-e470 5.0.0-37-generic #40~18.04.1-Ubuntu SMP Thu Nov 14 12:06:39 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
  * Hypervisor: KVM / xhyve / Hyper-V / VirtualBox / hyperkit
  using kvm

### Steps to reproduce

  1. create the oc cluster using "minishift start --iso-url file:///home/arun/pkgs/minishift-centos7.iso --memory 3072 --vm-driver kvm"
  2. after using the cluster, then using minishift stop
  3. start the stopped minishift vm, using minishift start
  

### Expected
In the minishift status, minishift says it is running. But logging in to the openshift is not working. I am expecting to use the same openshift cluster using oc login. 
ALL i need is, I want to use the previously created openshift cluster by starting minishift start without errors.

### Actual
Everytime I had to delete the vm and create the fresh one since after minishift stop or restart of the machine or next day, the old minishift stopped vm is not working after start.

arun-e470:~$ oc login -u system:admin
error: dial tcp 192.168.42.225:8443: connect: connection refused - verify you have provided the correct host and port and that the server is currently running.
arun-e470:~$ minishift ip
192.168.42.225
arun-e470:~$ oc login -u system:admin https://192.168.42.225:8443/
error: dial tcp 192.168.42.225:8443: connect: connection refused - verify you have provided the correct host and port and that the server is currently running.


### Logs

arun-e470:~$ minishift start
-- Starting profile 'test'
-- Using proxy for the setup
-- Check if deprecated options are used ... OK
-- Checking if https://github.com is reachable ... OK
-- Checking if requested OpenShift version 'v3.11.0' is valid ... OK
-- Checking if requested OpenShift version 'v3.11.0' is supported ... OK
-- Checking if requested hypervisor 'kvm' is supported on this platform ... OK
-- Checking if KVM driver is installed ... 
   Driver is available at /usr/local/bin/docker-machine-driver-kvm ... 
   Checking driver binary is executable ... OK
-- Checking if Libvirt is installed ... OK
-- Checking if Libvirt default network is present ... OK
-- Checking if Libvirt default network is active ... OK
-- Checking the ISO URL ... OK
-- Checking if provided oc flags are supported ... OK
-- Starting the OpenShift cluster using 'kvm' hypervisor ...
-- Starting Minishift VM ........-- Setting proxy information ... .....FAIL
 FAIL E1218 09:58:02.981756   14018 start.go:494] Error starting the VM: Error setting proxy to VM: ssh command error:
command : export HTTP_PROXY=http://myproxy:8080 http_proxy=http://myproxy:8080 HTTPS_PROXY=http://myproxy:8080 https_proxy=http://myproxy:8080 NO_PROXY=localhost,127.0.0.1,172.30.1.1,192.168.99.1/16,192.168.42.0/16,127.0.0.1,localhost,::1,docker-registry-default.127.0.0.1.nip.io,docker-registry-default.192.168.99.101.nip.io,192.168.99.1/24,192.168.42.127,192.168.42.225,192.168.42.1 no_proxy=localhost,127.0.0.1,172.30.1.1,192.168.99.1/16,192.168.42.0/16,127.0.0.1,localhost,::1,docker-registry-default.127.0.0.1.nip.io,docker-registry-default.192.168.99.101.nip.io,192.168.99.1/24,192.168.42.127,192.168.42.225,192.168.42.1
err     : exit status 255
output  : . Retrying.
Error starting the VM: Error setting proxy to VM: ssh command error:
command : export HTTP_PROXY=http://myproxy:8080 http_proxy=http://myproxy:8080 HTTPS_PROXY=http://myproxy:8080 https_proxy=http://myproxy:8080 NO_PROXY=localhost,127.0.0.1,172.30.1.1,192.168.99.1/16,192.168.42.0/16,127.0.0.1,localhost,::1,docker-registry-default.127.0.0.1.nip.io,docker-registry-default.192.168.99.101.nip.io,192.168.99.1/24,192.168.42.127,192.168.42.225,192.168.42.1 no_proxy=localhost,127.0.0.1,172.30.1.1,192.168.99.1/16,192.168.42.0/16,127.0.0.1,localhost,::1,docker-registry-default.127.0.0.1.nip.io,docker-registry-default.192.168.99.101.nip.io,192.168.99.1/24,192.168.42.127,192.168.42.225,192.168.42.1
err     : exit status 255
output  : 

You can start Minishift with `minishift start --show-libmachine-logs -v5` to collect logs.
Please consider posting this on http://gist.github.com/ and post the link in the issue.



minishift start --show-libmachine-logs -v5
-- minishift version: v1.34.1+c2ff9cb
-- Starting profile 'test'
Found binary path at /usr/local/bin/docker-machine-driver-kvm
Launching plugin server for driver kvm
Plugin server listening at address 127.0.0.1:39515
() Calling .GetVersion
Using API Version  1
() Calling .SetConfigRaw
() Calling .GetMachineName
(test) Calling .GetState
(test) DBG | Getting current state...
(test) DBG | Fetching VM...
-- Using proxy for the setup
	Using http proxy: http://myproxy:8080
	Using https proxy: http://myproxy:8080
-- Check if deprecated options are used ... OK
-- Checking if https://github.com is reachable ... OK
-- Checking if requested OpenShift version 'v3.11.0' is valid ... OK
-- Checking if requested OpenShift version 'v3.11.0' is supported ... OK
-- Checking if requested hypervisor 'kvm' is supported on this platform ... OK
-- Checking if KVM driver is installed ... 
   Driver is available at /usr/local/bin/docker-machine-driver-kvm ... 
   Checking driver binary is executable ... OK
-- Checking if Libvirt is installed ... OK
-- Checking if Libvirt default network is present ... OK
-- Checking if Libvirt default network is active ... OK
-- Checking the ISO URL ... OK
-- Checking if provided oc flags are supported ... OK
-- Starting the OpenShift cluster using 'kvm' hypervisor ...
-- Starting Minishift VM ....Found binary path at /usr/local/bin/docker-machine-driver-kvm
Launching plugin server for driver kvm
Plugin server listening at address 127.0.0.1:38775
() Calling .GetVersion
Using API Version  1
() Calling .SetConfigRaw
() Calling .GetMachineName
(test) Calling .GetState
(test) DBG | Getting current state...
(test) DBG | Fetching VM...
(test) Calling .Start
(test) DBG | Starting VM test
...(test) DBG | GetIP called for test
(test) DBG | Failed to retrieve dnsmasq leases from /var/lib/libvirt/dnsmasq/docker-machines.leases
(test) DBG | IP address: 192.168.42.225
(test) DBG | Unable to locate IP address for MAC 52:54:00:91:f5:6a
.(test) Calling .GetConfigRaw
-- Setting proxy information ... (test) Calling .GetIP
(test) DBG | GetIP called for test
(test) DBG | Failed to retrieve dnsmasq leases from /var/lib/libvirt/dnsmasq/docker-machines.leases
(test) DBG | IP address: 192.168.42.225
(test) DBG | Unable to locate IP address for MAC 52:54:00:91:f5:6a
(test) Calling .DriverName
(test) Calling .GetIP
(test) DBG | GetIP called for test
(test) DBG | Failed to retrieve dnsmasq leases from /var/lib/libvirt/dnsmasq/docker-machines.leases
(test) DBG | IP address: 192.168.42.225
(test) DBG | Unable to locate IP address for MAC 52:54:00:91:f5:6a
(test) Calling .GetSSHHostname
(test) DBG | GetIP called for test
(test) DBG | Failed to retrieve dnsmasq leases from /var/lib/libvirt/dnsmasq/docker-machines.leases
(test) DBG | IP address: 192.168.42.225
(test) DBG | Unable to locate IP address for MAC 52:54:00:91:f5:6a
(test) Calling .GetSSHPort
(test) Calling .GetSSHKeyPath
(test) Calling .GetSSHKeyPath
(test) Calling .GetSSHUsername
Using SSH client type: external
Using SSH private key: /home/apurb/.minishift/profiles/test/machines/test/id_rsa (-rw-------)
&{[-F /dev/null -o ConnectionAttempts=3 -o ConnectTimeout=10 -o ControlMaster=no -o ControlPath=none -o LogLevel=quiet -o PasswordAuthentication=no -o ServerAliveInterval=60 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null docker@192.168.42.225 -o IdentitiesOnly=yes -i /home/apurb/.minishift/profiles/test/machines/test/id_rsa -p 22] /usr/bin/ssh <nil>}
About to run SSH command:

FAIL
 FAIL E1218 10:17:54.501679   29184 start.go:494] Error starting the VM: Error setting proxy to VM: ssh command error:
command : export HTTP_PROXY=http://myproxy:8080 http_proxy=http://myproxy:8080 HTTPS_PROXY=http://myproxy:8080 https_proxy=http://myproxy:8080 NO_PROXY=localhost,127.0.0.1,172.30.1.1,192.168.99.1/16,192.168.42.0/16,127.0.0.1,localhost,::1,docker-registry-default.127.0.0.1.nip.io,docker-registry-default.192.168.99.101.nip.io,192.168.99.1/24,192.168.42.127,192.168Search Results.42.225,192.168.42.1 no_proxy=localhost,127.0.0.1,172.30.1.1,192.168.99.1/16,192.168.42.0/16,127.0.0.1,localhost,::1,docker-registry-default.127.0.0.1.nip.io,docker-registry-default.192.168.99.101.nip.io,192.168.99.1/24,192.168.42.127,192.168.42.225,192.168.42.1
err     : exit status 255
output  : . Retrying.
Error starting the VM: Error setting proxy to VM: ssh command error:
command : export HTTP_PROXY=http://myproxy:8080 http_proxy=http://myproxy:8080 HTTPS_PROXY=http://myproxy:8080 https_proxy=http://myproxy:8080 NO_PROXY=localhost,127.0.0.1,172.30.1.1,192.168.99.1/16,192.168.42.0/16,127.0.0.1,localhost,::1,docker-registry-default.127.0.0.1.nip.io,docker-registry-default.192.168.99.101.nip.io,192.168.99.1/24,192.168.42.127,192.168.42.225,192.168.42.1 no_proxy=localhost,127.0.0.1,172.30.1.1,192.168.99.1/16,192.168.42.0/16,127.0.0.1,localhost,::1,docker-registry-default.127.0.0.1.nip.io,docker-registry-default.192.168.99.101.nip.io,192.168.99.1/24,192.168.42.127,192.168.42.225,192.168.42.1
err     : exit status 255
output  : 

### General information

  * Minishift version: 1.34.2
  * OS: Linux / macOS / Windows ; Windows 10
  * Hypervisor: KVM / xhyve / Hyper-V / VirtualBox / hyperkit : Virtualbox


### Steps to reproduce

  1. 
  2. 
  3. 
  4. 

### Expected


### Actual


### Logs

You can start Minishift with `minishift start --show-libmachine-logs -v5` to collect logs.
Please consider posting this on http://gist.github.com/ and post the link in the issue.
[Minishift Logs.txt](https://github.com/minishift/minishift/files/3964974/Minishift.Logs.txt)


CentOS CI at the moment fails to generate the docs combined with minishift docs due to various failures which is are not visible in the console logs of the job run. The issues mostly relates to version mismatch between the ascii_doctor gem and the ruby version.

```
[minishift_ci@n45 openshift-docs]$ rake build
rake aborted!
Gem::ConflictError: Unable to activate ascii_binder-0.1.13, because rake-13.0.1 conflicts with rake (~> 10.0)
/home/minishift_ci/openshift-docs/Rakefile:1:in `<top (required)>'
/home/minishift_ci/.rvm/gems/ruby-2.2.7/gems/rake-13.0.1/exe/rake:27:in `<top (required)>'
/home/minishift_ci/.rvm/gems/ruby-2.2.7/bin/ruby_executable_hooks:15:in `eval'
/home/minishift_ci/.rvm/gems/ruby-2.2.7/bin/ruby_executable_hooks:15:in `<main>'

Caused by:
Gem::ConflictError: Unable to activate ascii_binder-0.1.13, because rake-13.0.1 conflicts with rake (~> 10.0)
/home/minishift_ci/openshift-docs/Rakefile:1:in `<top (required)>'
/home/minishift_ci/.rvm/gems/ruby-2.2.7/gems/rake-13.0.1/exe/rake:27:in `<top (required)>'
/home/minishift_ci/.rvm/gems/ruby-2.2.7/bin/ruby_executable_hooks:15:in `eval'
/home/minishift_ci/.rvm/gems/ruby-2.2.7/bin/ruby_executable_hooks:15:in `<main>'

Caused by:
LoadError: cannot load such file -- ascii_binder/tasks/tasks
/home/minishift_ci/openshift-docs/Rakefile:1:in `<top (required)>'
/home/minishift_ci/.rvm/gems/ruby-2.2.7/gems/rake-13.0.1/exe/rake:27:in `<top (required)>'
/home/minishift_ci/.rvm/gems/ruby-2.2.7/bin/ruby_executable_hooks:15:in `eval'
/home/minishift_ci/.rvm/gems/ruby-2.2.7/bin/ruby_executable_hooks:15:in `<main>'
```