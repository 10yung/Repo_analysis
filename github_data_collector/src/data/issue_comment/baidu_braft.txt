leader节点在一个log entry还未commit时就发生了leader切换，那么新的leader节点在commit该log entry后，会发送响应给client吗？新leader如何知道client的ip，因为log entry里面没有保护request的done。
Adjusting heap profiles for 1-in-524288 sampling rate
Heap version 2
Total: 13021.7 MB
6179.3 47.5% 47.5% 6179.3 47.5% 00000000008ae0d8
1805.7 13.9% 61.3% 1805.7 13.9% 000000000076aa99
943.0 7.2% 68.6% 943.0 7.2% 00000000007af800
888.9 6.8% 75.4% 888.9 6.8% 00000000005ccfdc
884.4 6.8% 82.2% 887.4 6.8% _init
579.5 4.5% 86.6% 579.5 4.5% 00000000005cfce0
341.0 2.6% 89.3% 341.0 2.6% 0000000000610125
294.7 2.3% 91.5% 294.7 2.3% 00000000005e3fc5
206.1 1.6% 93.1% 523.2 4.0% 00000000005ddece
132.5 1.0% 94.1% 132.5 1.0% 00000000005c8413
118.5 0.9% 95.0% 118.5 0.9% 00000000007afa15

操作系统版本是: Centos 7.2.1511, 显示的是函数地址，不显示函数名称，请问这是怎么回事？
https://github.com/baidu/braft/issues/182
OnErrorClousre 应为OnErrorClosure，这个是拼写错了吧
目前有遇到过一个现象：
1.在将一个节点添加到一个raft group中（AddPeer操作）去的时候，发现on_configuration_committed被回调两次，具体情况如下：
2.假设raft group为A B，将C 加入这个raft group中的时候，第一次C节点回调on_configuration_committed时获取到的peer list是A,B，第二次C回调on_configuration_committed的时候，获取到的peer list为ABC。前提是我对butil::Status NodeImpl::list_peers(std::vector<PeerId>* peers)做了一点改造，去掉了是leader的限制，这个是否有影响呢。还有就是两次回调on_configuration_committed是否合理呢
![image](https://user-images.githubusercontent.com/22393698/69537536-4e965f00-0fbb-11ea-9754-c05b371ebf64.png)





另外请问什么情况下会导致iter_impl出现error

_Originally posted by @promoon in https://github.com/brpc/braft/issues/173#issuecomment-546588966_
example中atomic的读请求实现是走apply流程的，但block/counter中的读请求没有走apply流程，理论上是可能stale read的吧。注释写的 // This is the leader and is up-to-date. It's safe to respond client 应该有问题。

看起来`_leader_term`的作用是一个比`read lease`更弱的保证。当其=-1时肯定不是leader，可以在follower上使用来回复redirect，但当其>0时可能只是网络分区中minority里还没step down的leader。不知道这个理解有没有问题。
The Raft protocol is very helpful for distributed state management. 

Can Byzantine Fault Tolerance be added to it please? It would make it more secure. Since you already have a perfectly working RAFT, adding few midifications to the protocol, such as those mentioned in the `Tangaroa` ([Paper](http://www.scs.stanford.edu/14au-cs244b/labs/projects/copeland_zhong.pdf) ) would make it secure.
连续发送请求，每个约16MB，在几百个请求后观察到文件描述符不够用的错误，大致如下：

```
Fail to open /proc/self/stat: Too many open files
Fail to open /proc/self/fd: Too many open files
Fail to open /proc/self/statm: Too many open files
Fail to open /proc/loadavg: Too many open files
Fail to open /proc/self/io: Too many open files
Fail to open /proc/self/fd: Too many open files
Fail to open /proc/self/fd: Too many open files
Fail to open /proc/self/fd: Too many open files
Fail to open /proc/self/fd: Too many open files
Fail to open /proc/self/fd: Too many open files
Fail to close old open_segment or create new open_segment: Too many open files
Fail to append_entries, nappent=0, to_append=1
Timeout when leaving replication group, will force exit
got error={type=LogError, error_code=5, error_text=`Fail to append entries'}
```

并且重新启动服务端也无法恢复。观察到是log的Segment对象仅调用了close而没有析构，所以没有释放fd。这里是否有资源泄露的问题？

想问一下：这个问题有什么可能的解决方法？在Segment对象调用close时就关闭fd可行吗？谢谢！

有两个问题请教下：
1. snapshot和apply的串行化是靠bthread::execution_queue_execute的提交任务的顺序来保证的吗？
2. 如果状态机本身数据很大(比如4G)，每次打snapshot耗时很长，这样是会导致打快照期间日志一直不能提交吗？