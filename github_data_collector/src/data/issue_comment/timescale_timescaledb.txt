Hello.

Lastest `debian` x64 context.

How to install `timescaledb` as pure `postgres` extension on already installed `postgres` db (from official `postgres` sources)?

Can someone help with this question?

Ubuntu 16.04
TimescaleDB version - 1.6
Docker
--------------------
In version 1.6 you've added additional functionality for continuous aggregates that allows us to ignore invalidation and therefore we can keep data in the cagg even if we drop the raw data. This is great but only if we have options to limit the amount of data we keep in the continuous aggregate.

For example - if we use a cagg to rollup to a 1 minute interval. If we have a lot of items we store data for we could quickly start hitting memory limits from having too much 1 minute data.

It would be nice to be able to use drop_chunks() on a continuous aggregate so that we can manage the data retention for rollups of different widths.

An example - if we have 100,000 stock symbols and we collect raw tick data for each into a hypertable we can then rollup to 1m intervals using a continuous agg. We can then drop the raw data after,say,  48 hours.  The problem is that after a couple of weeks we'll have too much 1 minute data

2 weeks * 7 days * 1440 minutes * 100,000 symbols > 2 Billion rows in our continuous agg view. 

**Relevant system information:**
 - OS: CentOS 7.6.1860
 - PostgreSQL version (output of `postgres --version`): 11.6
 - TimescaleDB version (output of `\dx` in `psql`): 1.5.0
 - Installation method: rpm install from repository

**Describe the bug**
A call to drop_chunks() seems to cause a deadlock occasionally. The following locks are present until I kill the drop_chunks task. Both locks are then cleared.

PID 0:
```
SELECT
    typinput='array_in'::regproc,
    typtype
FROM
    pg_catalog.pg_type
    LEFT JOIN (
        select
            ns.oid as nspoid,
            ns.nspname,
            r.r
        from
            pg_namespace as ns
            join (
                select
                    s.r,
                    (current_schemas(false))[s.r] as nspname
                from
                    generate_series(1, array_upper(current_schemas(false), 1)) as s(r)
            ) as r
        using ( nspname )
    ) as sp ON sp.nspoid = typnamespace
WHERE
    typname = $1
ORDER BY
    sp.r,
    pg_type.oid
DESC LIMIT 1
```
Drop chunks command run every hour:
```
select drop_chunks(schema_name => 'metric_v3', table_name => 'counterdata', older_than => interval '37d');
```

**To Reproduce**
This is difficult to reproduce reliably. I have a running system where datapoints are inserted at an average rate of 8000 points per second. The points are generated by 500 clients.

A maintenance task runs every hour to drop old chunks:
```
select drop_chunks(schema_name => 'metric_v3', table_name => 'counterdata', older_than => interval '37d');
```

**Expected behavior**
Most of the times the drop_chunks function returns after 1-2 seconds. I would expect this to always be the case.

**Actual behavior**
Some time drop_chunks locks until the function is cancelled.

**Screenshots**
The following screenshot shows the rate at which points are written to the database. It is clear that the deadlock situation happened a couple of times during the day. In each case the access to the database was locked such that no inserts could be completed. 
<img width="791" alt="Screenshot 2020-01-16 at 11 46 29" src="https://user-images.githubusercontent.com/57727889/72603511-d183c800-3921-11ea-9c7b-ea82cc190aba.png">


**Relevant system information:**
 - OS: Ubuntu 18.04.3 LTS
 - CPU's: 3
 - Memory: 16 GB
 - PostgreSQL version: 11.6
 - TimescaleDB version: 1.6.0
 - Installation method: apt

**Describe the bug**

Our simple hypertable (uuid, time, value) contains 7 billion rows divided over 6220 1-week chunks. The size of the chunks varies wildly, but we've made sure that the largest one comprises no more than 25% of main memory. Apart from an index on (time), an extra index (primary key) on (uuid, time) was created to support our queries. The database was tuned via timescaledb-tune. Table statistics were up to date. What we observe is that the query planning time does not scale well with the number of chunks that have to be touched. In contrast, execution time is fast, even for the largest interval (never exceeds 1 second).
```
EXPLAIN ANALYZE
SELECT time_bucket('4week', time) AS bucket, avg(value)
FROM timeseries
WHERE uuid = '12b3a192-d9d8-497a-8dca-d7caf85e25d0'
AND time BETWEEN '2018-01-01' AND '2019-01-01'
GROUP BY bucket ORDER BY bucket;
```

```
'2018-01-01' and '2019-01-01': planning time =     21.745 ms
'2009-01-01' and '2019-01-01': planning time =    470.588 ms
'1999-01-01' and '2019-01-01': planning time =   2024.340 ms
'1989-01-01' and '2019-01-01': planning time =   5188.943 ms
'1979-01-01' and '2019-01-01': planning time =  13587.168 ms
'1969-01-01' and '2019-01-01': planning time =  19318.124 ms
No interval, uuid filter only: planning time = 149704.720 ms
```

**Expected behavior**

We expected less query planning overhead, but are unsure since we're new to timescaledb. Planning times in your blog for 4000+ chunks are much, much smaller: [Optimizing queries on TimescaleDB hypertables with thousands of partitions](https://blog.timescale.com/blog/optimizing-queries-timescaledb-hypertables-with-partitions-postgresql-6366873a995d/).
Hi devs,

per [Slack discussion](https://timescaledb.slack.com/archives/C4GT3N90X/p1579239896003900), I have a large table with 1 year of data with a Continuous Aggregates view, the view has a missing column so I have to `drop` the view then re-create it. But as Continuous Aggregates always start from day 1, which is quite long time ago, it makes the whole physical view useless for quite a while.

I suggest when creating Continuous Aggregates, allow some option to aggregate the data from latest first. Then backwards to the oldest. This will make the view available sooner.
Hello everyone,

We are considering TSDB for supporting events in our production environment and need some help with the same. On the official site of TSDB I see installers available for ubuntu and redhat, in addition to other platforms. However I don't see one for SLES-15. 
https://docs.timescale.com/latest/getting-started/installation
Could some one help?
NOTE: Installation using source code or on docker is not an option for us.

Any help will be appreciated.

Thanks,
Pooja
**Relevant system information:**
 - OS: CentOS 7
 - PostgreSQL version 11.5
 - TimescaleDB version 1.5.1
 - Installation method: Source

**Describe the bug**
I am following the documentation to compile timescale from the source but I am getting asked about the C99 flag which should be there in the makefile

```
# Bootstrap the build system
./bootstrap
# To build the extension
cd build && make
# To install
make install
```

**Actual behavior**
My steps are:

```
cd contrib/timescaledb-1.5.1 && ./bootstrap -DPG_CONFIG=/usr/local/pgsql/pgsql-11.5/bin/pg_config -DREGRESS_CHECKS=OFF
cd contrib/timescaledb-1.5.1/build && make && make install
```
The output is:

> ../postgresql-11.5/contrib/timescaledb-1.5.1/tsl/src/compression/compression.c: In function 'create_per_compressed_column':
> ../postgresql-11.5/contrib/timescaledb-1.5.1/tsl/src/compression/compression.c:1038:2: error: 'for' loop initial declarations are only allowed in C99 mode
>   for (int16 col = 0; col < in_desc->natts; col++)
>   ^
> ../postgresql-11.5/contrib/timescaledb-1.5.1/tsl/src/compression/compression.c:1038:2: note: use option -std=c99 or -std=gnu99 to compile your code
> ../postgresql-11.5/contrib/timescaledb-1.5.1/tsl/src/compression/compression.c: In function 'populate_per_compressed_columns_from_data':
> ../postgresql-11.5/contrib/timescaledb-1.5.1/tsl/src/compression/compression.c:1090:2: error: 'for' loop initial declarations are only allowed in C99 mode
>   for (int16 col = 0; col < num_cols; col++)
>   ^
> ../postgresql-11.5/contrib/timescaledb-1.5.1/tsl/src/compression/compression.c: In function 'row_decompressor_decompress_row':
> ../postgresql-11.5/contrib/timescaledb-1.5.1/tsl/src/compression/compression.c:1131:3: error: 'for' loop initial declarations are only allowed in C99 mode
>    for (int16 col = 0; col < row_decompressor->num_compressed_columns; col++)
>    ^
> make[3]: *** [tsl/src/CMakeFiles/timescaledb-tsl.dir/compression/compression.c.o] Error 1
> make[2]: *** [tsl/src/CMakeFiles/timescaledb-tsl.dir/all] Error 2
> make[1]: *** [all] Error 2

So why the makefile does not include the -std=c99 flag?

**Relevant system information:**
 - OS: Linux localhost 5.4.1-gentoo
 - PostgreSQL version: 11
 - TimescaleDB version: 1.5.1
 - Installation method: using docker

**Describe the functionality**
Sometimes, a system needs to add support for new data that requires some back-filling to add older pre-existing data to the database (eg. adding support to a new external market pair to a financial solution) or back-fill missing data to an already stored time-series.

Another case would be if your system automatically enables the compression job at startup (to ensure it is running in case it crashed with it disabled) and you were in the middle of a back-filling when the system crashed, in this case, the system would re-enable compression before the new data reached current time data.

To do that correctly, the system will need to disable temporarily compression so it can add old data without TimescaleDB trying to compress it.

The issue arrives when some of this new data is trying to be added in an already compressed chunk. In that case, the documentation shows that we need to manually decompress the chunk and then insert the data.

The problem is that currently there is no way to know what these chunks would be, theoretically `show_chunks` would be used for that, but this only works with only one dimension (the interval timestamp), not more.

For example, consider a case that I have a table with `device_id` and `time` as dimensions.

In this case, I have full `device_id 1` data until current time already in the DB and the system is back-filling data from 2017 to today for `device_id 2`.

In the middle that back-filling, let's say at `2018-12-12`, the system crashed for some reason, and compression was reenabled. This means that now the last row at `2018-12-12` for `device_id 2` is in a compressed chunk, but I don't know which block it is.

The only solution is to run `select show_chunks('table', newer_than => '2018-12-11');` and decompress all the chunks returned. This works but it will decompress a lot of chunks that contain only data for `device_id 1`, especially if I use something like a big `number_partitions` number to force different `device_id`  values to be stored in individual chunks.

**Possible Solutions**

1. Allow multiple dimensions in `show_chunks`:
     eg, `select show_chunks('table', device_id => 2, newer_than => '2018-12-11');`
2. Allow to get a chunk of specific row:
     If I can get some row from the DB with `select * from table where device_id = 2 order by time desc limit 1;` and get this row chunk, I can, then decompress this chunk only and safely continue the back-filling.
3. Return compressed chunk name if the insert fails because it is compressed.
     This already happens in the error message, but it is not a structured message, so I don't think it is safe and wise to parse it to get the chunk name.

This was previously discussed in detail in this thread in Slack:
https://timescaledb.slack.com/archives/C4GT3N90X/p1577201630152000

Thanks for your attention!
**Relevant system information:**
 - OS: Centos 7
 - PostgreSQL 11
 - TimescaleDB 1.5.1
 - Installation method: "yum install"

**Describe the bug**

Considering this hypertable having hundreds of chunks:

```sql
CREATE TABLE public.impact_sound_risk_tiles (
  key_id bigint NOT NULL,
  tile public.raster NOT NULL
);
SELECT create_hypertable('sound_risk_tiles', 'key_id',
  chunk_time_interval => 1250, create_default_indexes => FALSE
);
CREATE INDEX index_sound_risk_tiles_on_tile ON public.sound_risk_tiles USING brin (key_id);
```

and this request:

```sql
EXPLAIN ANALYZE WITH keys AS (
    SELECT 331724 AS id
), route AS (
    SELECT ARRAY[-58.45, 47.10]::DOUBLE PRECISION[] AS coordinates, ST_SetSRID(ST_MakePoint(-58.45, 47.10), 4326) AS waypoint
)
SELECT ST_Value(tile, waypoint)
FROM sound_risk_tiles, keys, route
WHERE sound_risk_tiles.key_id = id AND sound_risk_tiles.tile && waypoint;

```

then, every partition is checked, as seen here:

<details>
  <summary>EXPLAIN ANALYZE</summary>
  <p>

```sql
Nested Loop  (cost=3.38..781459.75 rows=1 width=8) (actual time=16.213..19.015 rows=1 loops=1)
  Join Filter: ((_hyper_2_6_chunk.tile)::geometry && route.waypoint)
  Rows Removed by Join Filter: 103
  CTE keys
    ->  Result  (cost=0.00..0.01 rows=1 width=4) (actual time=0.000..0.001 rows=1 loops=1)
  CTE route
    ->  Result  (cost=0.00..0.01 rows=1 width=64) (actual time=0.002..0.002 rows=1 loops=1)
  ->  CTE Scan on route  (cost=0.00..0.02 rows=1 width=32) (actual time=0.004..0.004 rows=1 loops=1)
  ->  Nested Loop  (cost=3.36..781209.36 rows=328 width=605) (actual time=10.109..13.608 rows=104 loops=1)
        ->  CTE Scan on keys  (cost=0.00..0.02 rows=1 width=4) (actual time=0.001..0.001 rows=1 loops=1)
        ->  Append  (cost=3.36..780611.34 rows=59800 width=613) (actual time=10.073..13.543 rows=104 loops=1)
              ->  Bitmap Heap Scan on _hyper_2_6_chunk  (cost=3.36..1417.71 rows=104 width=577) (actual time=0.026..0.027 rows=0 loops=1)
                    Recheck Cond: (key_id = keys.id)
                    ->  Bitmap Index Scan on _hyper_2_6_chunk_index_sound_risk_tiles_on_tile  (cost=0.00..3.33 rows=1596 width=0) (actual time=0.023..0.023 rows=0 loops=1)
                          Index Cond: (key_id = keys.id)
              ->  Bitmap Heap Scan on _hyper_2_10_chunk  (cost=3.36..1414.70 rows=104 width=578) (actual time=0.017..0.017 rows=0 loops=1)
                    Recheck Cond: (key_id = keys.id)
                    ->  Bitmap Index Scan on _hyper_2_10_chunk_index_sound_risk_tiles_on_tile  (cost=0.00..3.33 rows=1592 width=0) (actual time=0.016..0.016 rows=0 loops=1)
                          Index Cond: (key_id = keys.id)

... hundreds of partitions ...

              ->  Bitmap Heap Scan on _hyper_2_1996_chunk  (cost=3.36..1387.72 rows=104 width=561) (actual time=0.020..0.020 rows=0 loops=1)
                    Recheck Cond: (key_id = keys.id)
                    ->  Bitmap Index Scan on _hyper_2_1996_chunk_index_sound_risk_tiles_on_tile  (cost=0.00..3.33 rows=1425 width=0) (actual time=0.019..0.020 rows=0 loops=1)
                          Index Cond: (key_id = keys.id)
              ->  Bitmap Heap Scan on _hyper_2_1998_chunk  (cost=3.36..1378.32 rows=104 width=561) (actual time=0.016..0.016 rows=0 loops=1)
                    Recheck Cond: (key_id = keys.id)
                    ->  Bitmap Index Scan on _hyper_2_1998_chunk_index_sound_risk_tiles_on_tile  (cost=0.00..3.33 rows=1421 width=0) (actual time=0.016..0.016 rows=0 loops=1)
                          Index Cond: (key_id = keys.id)
Planning Time: 108.322 ms
Execution Time: 27.586 ms
```

</p>
</details>

Otherwise, if the request is against a constant:

```sql
EXPLAIN ANALYZE WITH route AS (
    SELECT ARRAY[-58.45, 47.10]::DOUBLE PRECISION[] AS coordinates, ST_SetSRID(ST_MakePoint(-58.45, 47.10), 4326) AS waypoint
)
SELECT ST_Value(tile, waypoint)
FROM sound_risk_tiles, route
WHERE sound_risk_tiles.key_id = 331724 AND sound_risk_tiles.tile && waypoint;
```

then, only the desired partition is checked, as seen here:

```sql
Nested Loop  (cost=3.37..1412.57 rows=1 width=8) (actual time=7.172..7.286 rows=1 loops=1)
  Join Filter: ((_hyper_2_1416_chunk.tile)::geometry && route.waypoint)
  Rows Removed by Join Filter: 103
  CTE route
    ->  Result  (cost=0.00..0.01 rows=1 width=64) (actual time=0.002..0.002 rows=1 loops=1)
  ->  CTE Scan on route  (cost=0.00..0.02 rows=1 width=32) (actual time=0.004..0.005 rows=1 loops=1)
  ->  Append  (cost=3.36..1332.99 rows=104 width=596) (actual time=0.556..1.356 rows=104 loops=1)
        ->  Bitmap Heap Scan on _hyper_2_1416_chunk  (cost=3.36..1332.47 rows=104 width=596) (actual time=0.555..1.344 rows=104 loops=1)
              Recheck Cond: (key_id = 331724)
              Rows Removed by Index Recheck: 5454
              Heap Blocks: lossy=512
              ->  Bitmap Index Scan on _hyper_2_1416_chunk_index_sound_risk_tiles_on_tile  (cost=0.00..3.33 rows=1351 width=0) (actual time=0.036..0.036 rows=5120 loops=1)
                    Index Cond: (key_id = 331724)
Planning Time: 0.556 ms
Execution Time: 7.331 ms
```

**Expected behavior**
Should be optimized like if it was a constant check.

**Actual behavior**
All partitions are checked, regardless of the narrow value of the partition key.
