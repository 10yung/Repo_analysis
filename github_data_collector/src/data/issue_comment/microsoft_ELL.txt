Hi,

we are trying to integrate Azsure Sphere MCU as IoT edge and wondering if ELL can be applied to Sphere MCU as edge side AI analysis? if yes, is there any instruction or sample? Thanks!

Ken
Hi,

while following steps Win 10from https://microsoft.github.io/ELL/tutorials/Getting-started-with-image-classification-on-the-Raspberry-Pi/ on Win 10 x64 machine, 

C:\azsphere\ELL-master\build\host>python call_model.py
Traceback (most recent call last):
  File "call_model.py", line 8, in <module>
    import model
  File "C:\azsphere\ELL-master\build\host\model.py", line 11, in <module>
    import _model
ModuleNotFoundError: No module named '_model'

my built files under "host" folder are uploaded to https://www.dropbox.com/s/64zg0orbjsmnr4l/host.zip?dl=0, please help. Thanks!

Ken
Hello, 
I followed all the instructions (put aside the fact that the tutorials are not updated), trained a GRU model for 30 and 150 epochs from scratch, however, during the test phase the accuracy is around 10%. 

Sample output is as follows: 
```
FAILED 10.09%: expected stop, got bird in stop/e1469561_nohash_3.wav
FAILED 10.08%: expected bed, got up in bed/cc592808_nohash_3.wav
FAILED 10.07%: expected left, got up in left/d5ca80c6_nohash_0.wav
FAILED 10.05%: expected wow, got nine in wow/bed06fac_nohash_1.wav
FAILED 10.04%: expected cat, got up in cat/b97c9f77_nohash_0.wav
```
During training the accuracy is good: 
```
Epoch 10, Loss 0.576, Validation Accuracy 80.646, Learning Rate 0.001
Epoch 11, Loss 0.501, Validation Accuracy 80.557, Learning Rate 0.001
Epoch 12, Loss 0.685, Validation Accuracy 85.495, Learning Rate 0.001
Epoch 13, Loss 0.532, Validation Accuracy 86.896, Learning Rate 0.001
Epoch 14, Loss 0.358, Validation Accuracy 86.483, Learning Rate 0.001
Epoch 15, Loss 0.470, Validation Accuracy 84.522, Learning Rate 0.001
Epoch 16, Loss 0.334, Validation Accuracy 86.483, Learning Rate 0.001
Epoch 17, Loss 0.424, Validation Accuracy 87.367, Learning Rate 0.001
Epoch 18, Loss 0.266, Validation Accuracy 83.550, Learning Rate 0.001
```
The script I used is:
```
# train
python3 tools/utilities/pythonlibs/audio/training/train_classifier.py --architecture GRU --use_gpu --outdir=out --dataset=audio --epochs=150

# import
python3  $ELL_ROOT/tools/importers/onnx/onnx_import.py out/GRU128KeywordSpotter.onnx

# generate ell
python3 tools/wrap/wrap.py --model_file out/GRU128KeywordSpotter.ell --outdir KeywordSpotter --module_name model

# compile and make spotter
cd KeywordSpotter
mkdir build
cd build
cmake ..
make
cd ..
cd ..

#python3 tools/utilities/pythonlibs/audio/training/train_classifier.py --architecture GRU --use_gpu --outdir=out --dataset=audio --epochs=150

# test
./build/tools/utilities/pythonlibs/audio/training/test_ell_model.py --classifier KeywordSpotter/model --featurizer compiled_featurizer/mfcc --sample_rate 1600$
```

Thanks 
Hi,

I am trying to get my own model to the mxchip following the instructions from:
https://github.com/Microsoft/ELL/wiki/Keyword-Spotting-on-MXCHIP

When I try to use the llvm (6.0) opt tool under ubuntu, I get:

~/tutorial$ opt featurizer.bc -o featurizer.opt.bc -O3
Intrinsic has incorrect argument type!
void (i8*, i8*, i64, i1)* @llvm.memcpy.p0i8.p0i8.i64
Intrinsic has incorrect argument type!
void (i8*, i8*, i32, i1)* @llvm.memcpy.p0i8.p0i8.i32
opt: featurizer.bc: error: input module is broken!

I have no idea how to solve that. Is that an issue with the llvm version?

Best
Nils
General Question: Can we reconstruct the PyTorch models from the ELL models 
Specific Question: Can I get the PyTorch models for the Image Classification ELL models provided in [ILSVRC2012](https://microsoft.github.io/ELL/gallery/ILSVRC2012/) gallery on the Microsoft ELL's website.

This will be helpful for colab users and people who are not able to compile ELL on their system. 

@lovettchris I have one more tutorial planned of Key Word Spotting, in which I have taken care of adding background noise and mixing silent noise with the audio. You can find the colab notebook on this [link](https://colab.research.google.com/drive/1pSjCLZnw6CEptNcCEUKuGJAJfma215dk). I need to format and comment it properly. Which can help people generate KWS models using ELL and deploy it on microcontrollers.
I have followed the *Training an audio keyword spotter with PyTorch* tutorial, on a clean install of Ubuntu 18.04.  

I have found the following minor issues:

- The tutorial did not mention to install `pyaudio`.
- The tutorial did not mention to install `onnx` - I used the following command `conda install -c conda-forge onnx=1.4.1`
- The tutorial has a few typos and other mistakes, but I could easily navigate around them (e.g. "GRUKeywordSpotter.ell" in the tutorial is actually "GRU128KeywordSpotter.ell")

 
Goal: Import a MobileNet reference model from the ONNX Model Zoo.

Repro Steps:

1. Download the mobilenetv2-1.0.onnx model from the ONNX Model Zoo:
  > https://github.com/onnx/models/tree/master/models/image_classification/mobilenet
3. Run the onnx_import.py tool:
  > python E:\ELL\tools\importers\onnx\onnx_import.py mobilenetv2-1.0.onnx

Result: 
**ValueError: cannot reshape array of size 1000 into shape (0,newaxis)**

Updates:
- Same error observed with SqueezeNet:
  > https://github.com/onnx/models/tree/master/models/image_classification/squeezenet
Goal: import an ONNX model exported from CustomVision.ai

Repro Steps
*NOTE: Pre-trained ONNX model attached, if you want to skip training your own

Train a new model:
- Go to [https://www.customvision.ai](https://www.customvision.ai)
- Create an image classifier (mobile) model
- Train the model
- From the "Perfomance" tab, click the download button captioned "export selected iteration"
- Choose "ONNX" and version 1.2, download the model to a local folder

Import ONNX model
- Run ELL's onnx_import.py script:
    > python E:\ELL\tools\importers\onnx\onnx_import.py cv_model.onnx

Result:
**Exception: Unimplemented node type: ImageScaler**

[cv_model.zip](https://github.com/microsoft/ELL/files/3357149/cv_model.zip)
