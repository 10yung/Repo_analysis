Here's the Tweet: https://twitter.com/janl/status/1113015555064201216

Error message:
 ```
2019/11/30 18:04:02 [ERROR][twitter/joonas_fi] Getting latest: getting items from service: processing tweet from API: processing tweet 1113180316510957568: making item from tweet that this tweet (1113180316510957568) is in reply to (1113015555064201216): making item from tweet that this tweet (1113015555064201216) embeds (1112473455650172929): media resource returned HTTP status 403 Forbidden: https://pbs.twimg.com/ext_tw_video_thumb/1112471832232259585/pu/img/ywWGTl09hsnLnMOY.jpg
```

That image URL redirects (when used with browser - different when API use?) to [this DMCA warning](https://pbs.twimg.com/static/dmca/video-preview-img.png).

Timeliner cannot cope with this, and trying to re-run Timeliner always gets me this and cannot continue.
I'm running `$ timeliner -twitter-replies -twitter-retweets get-latest twitter/joonas_fi`:

```
2019/11/30 16:36:03 [ERROR][twitter/joonas_fi] Getting latest:
	getting items from service:
	processing tweet from API:
	processing tweet 123:
	making item from tweet that this tweet (123) is in reply to (456):
	making item from tweet that this tweet (456) is in reply to (789):
	making item from tweet that this tweet (789) is in reply to (AAA):
	getting tweet that this tweet (AAA) is in reply to (BBB):
	HTTP error: https://api.twitter.com/1.1/statuses/show.json?id=BBB&tweet_mode=extended:
	429 Too Many Requests
```

(anonymization and line breaks added by me)

Timeliner configures its Twitter client's rate limit ("with some leeway") to 5 900/h. Bursting is disabled for Twitter, so it's 610ms between requests.

<details><summary>Hidden wrong detours in my thought process</summary><p>

Are all proper requests rate limited?
---------------------------------------

As you can see, Timeliner is digging some considerable reply chains. My first instinct was "are replies counted against the quota or are only my own tweets counted?". Upon further digging, rate limiter is implemented in `http.RoundTripper` level for a HTTP client, so that's not the issue. Nice approach BTW, I might use that idea later in my own projects! :+1:

Upon making the `ezhttp` HTTP suggestion in my other PR I remembered there's a plain `resp, err := http.Get(mediaURL)` call in `twitter/twitter.go` that bypasses the rate limiting HTTP client. That is used for fetching media, and some (or most) media are fetched from `https://pbs.twimg.com/...` which is Twitter's domain - are those counted against the quota? Probably not, because if the quotas are tied to the user or the app (API key) and requests for that domain would not count against **API quotas**..

Does the ratelimiter work properly?
--------------------------------------
I had a hard time using the ratelimiter standalone, so I just plopped a `fmt.Printf(".")` in the `RoundTrip()` and watched the dots appear on the screen as Timeliner chugged along. The dots were appearing calmly so the ratelimiter is working.

</p></details>

What I think is the problem
------------------------------

`twitter/api.go` use three API endpoints:

| Endpoint                       | User limit / 15min | App limit / 15min |
|--------------------------------|------------|-----------|
| `/users/show.json`             | 900        | 900       |
| `/statuses/show.json`          | 900        | 900       |
| `/statuses/user_timeline.json` | 900        | 1500      |

Source for limits: https://developer.twitter.com/en/docs/basics/rate-limits

Timeliner's ratelimit is shared across all of those endpoints. Timeliner theoretically lets me do 1 475 reqs/15 min to `/statuses/show.json` - pushing it over the limit of 900. Now I don't know for real what the ratios of the endpoint call rates are, but if we were to avoid going over the limit with this current "all endpoints have same rate limit" -design, the limit should be re-calculated based on the 900 number.

Another thought: is the 1 500 correct even for user_timeline.json?
------------------------------------------------------------------------

This depends on the authorization model, if Timeliner:

a) uses only the app's credentials to read public data and doesn't get authorization from the user
b) gets authorization from the user and operates on behalf of the user (I don't remember seeing any authorization screen, but that might be because I had an API key laying around which I had authorized way back)

A few quotes:

> Rate limiting of the standard API is primarily on a per-user basis — or more accurately described, per user access token.

> When using application-only authentication, rate limits are determined globally for the entire application. If a method allows for 15 requests per rate limit window, then it allows you to make 15 requests per window — on behalf of your application. This limit is considered completely separately from per-user limits.

Source: https://developer.twitter.com/en/docs/basics/rate-limiting

If I understand correctly, Timeliner is not using "When using application-only authentication", so shouldn't the limit be based on the 900 anyway?

I'm not sure of this. WDYT?


Workaround
----------

This is not a serious issue, since after throttling I can wait a while and continue later.


Code suggestions
-------------------

I don't know if you're interested in code suggestions, but a couple came to mind while kicking the tires:

1. I came across `timeliner.FakeCloser`, just in case you're not aware there's a stdlib implementation for that: https://godoc.org/io/ioutil#NopCloser
2. `ratelimit.go`: you're chucking empty structs (`struct{}`) on the token channel. Usually when a channel is only used for signalling, I've seen `interface{}` used so one can just chuck `nil`'s down the channel. I'm not sure if it's more performant but I think it's more semantic. This might be subjective though.
OS: `macOS (latest)`
GO: `go version go1.12.6 darwin/amd64`

I am getting this error. Everything is done as per README.md

```
2019/06/16 19:58:44 [ERROR][google_photos/xxxxxx@gmail.com] Processing item graph: processing node of item graph: downloading data file: syncing file: sync timeliner_repo/data/2019/06/google_photos/IMG_5773.JPG: operation not supported (item_id=6)
```

Another error:

```
2019/06/16 20:00:21 [ERROR][google_photos/xxxxxx@gmail.com] Processing item graph: processing node of item graph: storing item in database: disk I/O error (item_id=XXXXXXX)
```
Hi all, 
Just came across this project. In the [wiki](https://github.com/mholt/timeliner/wiki/Data-Source:-Google-Photos#caveats) it mentions that location data is stripped when using the Google Photos API?
I am currently using `rclone` to pull and backup my Google Photos account to my personal server through Google Drive. All of my media pulled using `rclone` retains the GPS data (lat/lon/alt).

Two thoughts.
1. I would rather use my own script (or some other program) to sync my Google Photos account. Would it be easy for `timeliner` to import from a local folder?
2. I am not familiar with the Google Photos API, but location data is pretty important (especially if you don't have Location History). It seems alternative methods to import photos **with** the location data would be of high importance.

# Type
Should it have a web interface or a desktop one? Maybe both? Possibly add an API to make it easier to create different types of interfaces. Should it be available from other devices? For example through the web or a phone app. Should import be available through the interface and if you have "external" interfaces (phone, web etc) should you be able to import it from there?

I think an API and a local only interface for import. Import (and other modification) shouldn't be available on other devices unless password protection is added.

# Design (for a graphical interface)
A timeline sounds like a good idea (based of name). Should you be able to sort it by type (video, audio, image, place etc)? Separate more detailed views for each month while only providing a overview on the timeline?

I think a timeline with general month views and then a more detailed monthly view would be good, since it's supposed to be a long term collection then a general yearly view might be good too.

Any more suggestions?
Possible data source in the future.
I get the following error message, and timeliner consistently doesn't download all pictures

2019/05/29 05:02:01 [ERROR][google_photos/xxx@xxx] Processing item graph: processing node of item graph: assembling item for storage: getting item metadata: parsing width as int: strconv.Atoi: parsing "": invalid syntax (width=)


Test data and db with all available features used (images, videos, posts, relationships etc) would be useful for development of an interface as it would allow for development of features even if you don't have your own data with all features.
<!--
This template is specifically for requesting the addition of a new data source (a way to add items to the timeline). Please answer all the questions as completely as possible. Put some effort into it since any implementation is going to require even more effort. If questions are not answered sufficiently, the issue may be closed.

PLEASE NOTE: This project is a community effort. We hope that after posting this issue, you will take the time to implement it and submit a pull request for everyone to use!
-->

## 1. What is the data source you want to add?

[Facebook](https://facebook.com) data exports. Currently there's a date source for data from the API, but a Timeliner user might have already closed their account, leaving behind only their data.




## 2. How are items obtained from the data source?

Data must be manually imported. It may be sourced from some or all of Facebook's different offerings. Documentation is [here](https://www.facebook.com/help/1701730696756992).




### 2a. If authentication is required, how does a user create or obtain credentials for Timeliner to access the data?

N/A




### 2b. If an API is available, what are its rate limits?

N/A




### 2c. If a file is imported, how is the file obtained?

Documentation [here](https://www.facebook.com/help/1701730696756992).




### 2d. If a file is imported, how do we read the file?
<!-- Is the file a compressed archive? How do we get the items out? Is the content and metadata separate? Please link to any documentation or provide a sample file. -->

Exports are a ZIP file containing a structure of folders, JSON files, and various media files. The media files have their EXIF data, but additional metadata from Facebook corresponding to each one is in the associated JSON file.


## 3. What constitutes an "item" from this data source?
<!-- An item is an entry on the timeline. Some data sources have multiple things that are "items" - for example: photos, blog posts, or text messages can all be items. An item must make sense to put on a timeline, and items must have unique IDs. -->

It depends how much of the export is considered useful, but photos, posts, milestones etc. should all be considered.

## 4. How can items from this data source be related?
<!-- Often, items form relationships with other items; for example, an item might be a reply to another item, or an item might contain another item. Think of relationships as uni-or-bi-directional arrows between items, with a label on the arrow. Relationships enrich the data obtained from this source. What kinds of useful relationships can be expressed from this data source? Do the relationships work both ways or just one way? Talk about this. -->

This should be looked into more closely by someone who actually uses Facebook – my memory for it isn't great as I type this.


## 5. What constitutes a "collection" from this data source?
<!-- A collection is a group of items (like a photo album). Note that collections are different from item relationships. Some data sources don't have collections; please explain. -->

Facebook has the concept of photo albums, so they'd constitute a collection. Other than that, I doubt there are many others.


## 6. What might not be trivial, obvious, or straightforward when implementing this data source?
<!-- Most data sources have nuances or caveats, some of which might not be obvious. Please think hard about this and use your experience with this data source to think of things that need special consideration. For example, a data source might only allow the most recent items to be obtained; how could we overcome that, maybe via a data export? See our wiki for "Writing a Data Source" to get ideas about what might be tricky. Ask unanswered questions here, start a discussion. Data sources can't be implemented successfully until these details are figured out. -->

I'm not sure how easy it would be to make the relationship between data from the JSON files and actual media files themselves clearer.


## Bonus: How do you like Timeliner? How much data are you preserving with it? Which existing data sources do you use?
<!-- I want to know! -->

I haven't started using it yet, but I figure a Facebook data export would serve as the foundation for my starting to use this on my VPS.


> I'm closing my Facebook account as I type this, but it'd be good to hang on to the backed-up data in a way I might be able to browse in future. I've observed from the wiki that there's not yet support for loading from a full backup taken from Facebook (whether HTML or JSON). I'd hope there's some support for this thanks to this issue.
> [ERROR][twitter/jacroe] Downloading all: getting items from service: getting next page of tweets: reading response body: json: cannot unmarshal number into Go struct field tweetGeo.coordinates of type string

Unsure what causes this error; this is just immediately after I run `timeliner get-all twitter/jacroe`.