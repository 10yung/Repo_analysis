# Required information

 * Distribution: `Alpine`
 * Distribution version: `3.11`
 * Kernel version: `4.19.80-0-vanilla`
 * LXC version: `3.18`
 * LXD version: `3.18`
 * Storage backend in use: `ext4 on md`

# Issue description

When I attempt to move a container to the cluster node that the container is already on, it crashes the cluster.  Of course there's no reason to do this, but the cluster shouldn't crash if done by accident.  I would expect no output and a success return code.

The following shows the situation pretty well.  I have these lxd nodes behind a load balancer in round robin so it looks like it took a moment for every node to show the failed state.

```
587bab5b1ce9 [~]# lxc cluster list
+--------------+-------------------------+----------+--------+-------------------+
|     NAME     |           URL           | DATABASE | STATE  |      MESSAGE      |
+--------------+-------------------------+----------+--------+-------------------+
| 57c9fae30ca2 | https://10.0.0.180:8443 | YES      | ONLINE | fully operational |
+--------------+-------------------------+----------+--------+-------------------+
| 72182231f55b | https://10.0.0.246:8443 | YES      | ONLINE | fully operational |
+--------------+-------------------------+----------+--------+-------------------+
| a78abd2afeef | https://10.0.0.231:8443 | YES      | ONLINE | fully operational |
+--------------+-------------------------+----------+--------+-------------------+
587bab5b1ce9 [~]# lxc stop test-0
Error: The container is already stopped
587bab5b1ce9 [~]# lxc info test-0 | grep -i locat
Location: 72182231f55b
587bab5b1ce9 [~]# lxc mv test-0 --target=72182231f55b
Error: Migration operation failure: Get https://10.0.0.246:8443/1.0/operations/6483f9e4-a636-4931-ab32-9802cf03379a: Unable to connect to: 10.0.0.246:8443
587bab5b1ce9 [~]# lxc cluster list
+--------------+-------------------------+----------+--------+-------------------+
|     NAME     |           URL           | DATABASE | STATE  |      MESSAGE      |
+--------------+-------------------------+----------+--------+-------------------+
| 57c9fae30ca2 | https://10.0.0.180:8443 | YES      | ONLINE | fully operational |
+--------------+-------------------------+----------+--------+-------------------+
| 72182231f55b | https://10.0.0.246:8443 | YES      | ONLINE | fully operational |
+--------------+-------------------------+----------+--------+-------------------+
| a78abd2afeef | https://10.0.0.231:8443 | YES      | ONLINE | fully operational |
+--------------+-------------------------+----------+--------+-------------------+
587bab5b1ce9 [~]# lxc cluster list
Error: cannot fetch node config from database: driver: bad connection
587bab5b1ce9 [~]# lxc list
Error: cannot fetch node config from database: driver: bad connection
587bab5b1ce9 [~]# lxc list
Error: cannot fetch node config from database: driver: bad connection
```

Rather than the error `Error: storage pool "<pool>" has volumes attached to it` which is given when the pool contains instance volumes from the default project.
Building on issue #5826 to use the new background proceess manager to refactor how dnsmasq and forkdns are processes are started or killed.
```
net16 # lxc --version
3.18

net16 # uname -a
Linux net16.faststablehosting.com 4.15.0-65-generic #74-Ubuntu SMP Tue Sep 17 17:06:04 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux

net16 # lsb_release -a
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 18.04.3 LTS
Release:	18.04
Codename:	bionic
```

After a container creation + container running.

Stopping the container, then attempting a delete produces this...

```
net16 # lxc delete net16-test-itk 
Error: Failed to remove device 'eth0': [Failed to release DHCPv4 lease for instance "net16-test-itk", IP "10.101.55.65", MAC "00:16:3e:43:46:05", write udp 10.101.55.1:36820->10.101.55.1:67: write: operation not permitted]
```

Container is partially removed, so I guess the fix is to somehow force removal of /var/snap/lxd/common/lxd/storage-pools/default/containers/net16-test-itk residue.

Be great if someone can provide the correct way to force container removal, so all cleanup is done correctly.

Thanks.
<!--
Github issues are used for bug reports. For support questions, please use [our forum](https://discuss.linuxcontainers.org).

Please fill the template below as it will greatly help us track down your issue and reproduce it on our side.  
Feel free to remove anything which doesn't apply to you and add more information where it makes sense.
-->

# Required information

 * Distribution: Ubuntu
 * Distribution version: 16.04.6 LTS
 * The output of "lxc info" or if that fails:
   * Kernel version:  4.20.17-042017-generic
   * LXC version: 3.0.3
   * LXD version: 3.0.3.
   * Storage backend in use: dir

# Issue description

I have update lxd/lxc from V2.x to V3.0.3 but now it is not anymore possible to start container. I always get the error "Initialize LXC: Failed to load raw.lxc".

The main issue will be , that my container is privileged and I've had to set set aa_profile=unconfined.
 `lxc config set webserver raw.lxc "lxc.aa_profile=unconfined"`
Then it worked fine under lxd V2.x as expected.
Now i wanted to copy container to a remote one (which has lxd V3.0.3) and I got also this error while copying. Therefore i tried to update my local lxd to Ver. 3.0.3.  but I'm having the same troubles.

No I don't have any clue how to get out this misery, because **any `lxc config set`
leads** to this error for my container.
I think , that this is a migration bug.

If I try to edit config file this way:
`lxc config edit webserver < newconfig.yaml`
overwriting 
aa_profile=unconfined   with the new keyword lxc.apparmor.profile=unconfined
then I receive other errors like i.e.
`Error: The storage pool of the root disk can only be changed through move`

Is there a chance to get my container running fine again?

# Steps to reproduce

 1. Step one
Install lxd V. 2x  and create a pivileged container and set config raw.lxc lxc.aa_profile=unconfined
 2. Step two
Update to lxd V3.0.3
 3. Step three
Try to start container

# Information to attach

 - [ ] Any relevant kernel output (`dmesg`)
 - [ ] Container log (`lxc info NAME --show-log`)
 - [x] Container configuration (`lxc config show NAME --expanded`)
 - [ ] Main daemon log (at /var/log/lxd/lxd.log or /var/snap/lxd/common/lxd/logs/lxd.log)
 - [ ] Output of the client with --debug
 - [ ] Output of the daemon with --debug (alternatively output of `lxc monitor` while reproducing the issue)

```
architecture: x86_64
config:
  environment.http_proxy: ""
  raw.apparmor: mount fstype=cfis,
  raw.idmap: both 1000 1000
  raw.lxc: lxc.aa_profile=unconfined
  security.privileged: "true"
  user.network_mode: ""
  volatile.base_image: f75468c572cc50eca7f76391182e6fdaf58431f84c3d35a2c92e83814e701698
  volatile.eth0.hwaddr: 00:16:3e:cf:f9:a2
  volatile.idmap.base: "0"
  volatile.idmap.next: '[]'
  volatile.last_state.idmap: '[]'
  volatile.last_state.power: STOPPED
devices:
  eth0:
    name: eth0
    nictype: bridged
    parent: lxdbr0
    type: nic
  root:
    path: /
    type: disk
  sharedir:
    path: /var/www/html
    source: /home/sfn/VirtualBox VMs/TBS_Development/Exchange/www
    type: disk
ephemeral: false
profiles:
- default
stateful: false
description: ""


```


One of major problems with remote LXD/LXC containers that prevents their adoption is inability to map service ports running in remote container to local machine with LXC client (mapping other way is also needed).

The common use case for secure port forwarding is [sharing local project directory with remote container](https://github.com/yakshaveinc/linux/issues/32). Doing that requires establishing additional secure channel and routing rule for NFS, SSH or 9p/9p2000 protocol as in the issue above. Using 9p also requires putting encryption layer, and there are no easy solutions like Let's Encrypt for domain names.

LXD already provides a secure channel, so this issue it to reuse that for port forwarding. I asked [the question on the forum](https://discuss.linuxcontainers.org/t/forward-port-to-be-accessible-from-remote-container/6308) 20 days ago and there are still no replies.

The proposal had been raised a year ago in #5414, where is was marked as `isn't something that LXD is meant to do, at least for now.` which was strange, because there was already [proxy device for forwarding tcp connections](#4106). I cam accept if LXD was not meant to support port forwarding, or not meant to support remote containers, but since it is now supports both, it is only logical to add the missing link.

Issues that made LXD is now ready to share its connection for port forwarding on remote containers:

* [presence of `forkproxy`](https://github.com/lxc/lxd/issues?q=is%3Aissue+forkproxy+is%3Aclosed)  executing host side processes for container interaction, which can be used to maintain connection with remote LXD client and do simple channel copy
* [backgroud process manager](https://github.com/lxc/lxd/issues/5826) for managing `forkproxy`, `forkdns` and `dnsmasq`

I could probably do the patch myself, but I need  pointer how to a read and write to connection between LXD client and server.
### Problem

I'm getting sporadic internal server errors on uploading plaintext files to the LXD api.

### Software

```
root@alpha-0001:/# snap list
Name  Version    Rev    Tracking  Publisher   Notes
core  16-2.42.5  8268   stable    canonical✓  core
lxd   3.18       12631  3.18      canonical✓  -
```
```
root@alpha-0001:/# lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 18.04.3 LTS
Release:        18.04
Codename:       bionic
```

### Logs

Info about request:
```
StatusCodeError: 500 - "{\"error\":\"file does not exist\",\"error_code\":500,\"type\":\"error\"}\n\n"                              
// removed for brevity                                       
  name: 'StatusCodeError',                                                                                                          
  statusCode: 500,                                                                                                                  
  message: '500 - "{\\"error\\":\\"file does not exist\\",\\"error_code\\":500,\\"type\\":\\"error\\"}\\n\\n"',                     
  error: '{"error":"file does not exist","error_code":500,"type":"error"}\n\n',                                                     
  options: {                                                                                                                        
// removed for brevity  
    },                                                                    
    ecdhCurve: 'secp384r1',                          
    json: false,                                                  
    method: 'POST',                                         
    url: 'https://10.254.0.4:8443/1.0/containers/tree-9a763b55-51a2-47e5-aa35-af7238a90290/files',
    qs: { path: '/etc/environment' },   
    headers: { 'X-LXD-type': 'file', 'Content-Type': 'plain/text' },
    body: '\n' +                   
      'ENVIRONMENT_VARIABLE=value\n' +                                                    
      '\n' +                                                
```                          

I can't find anything in the lxd daemon logs.

Also, i haven't reported this for quite some time as i'd found this: https://github.com/lxc/lxd/issues/5240 which seems like the same problem to me as i did not set the `Content-Type` header manually. Now i'm passing the header since a few days though and i'm still encountering this bug.

Also, a lot of containers get set up, but this problem occurs only sporadically.
An attempt in implementing passwordless PKI mode. 
Addressing issue #3364

Implements basic filtering on the 1.0/instance and 1.0/images endpoints. Users can pass in a filter query through the API call, and it will be applied to the results. Supports filtering on basic attributes and some nested objects (config, devices...), and basic logic (AND/OR/NOT) following OData conventions. Additional details in doc/rest-api.md.