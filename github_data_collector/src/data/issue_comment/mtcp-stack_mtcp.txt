Add mTCP support for SPDK.

1. add README.spdk for change summary.
2. Add mTCP separated mtcp_run_instance API.
3. Fix some issues about cross-compilation for arm64.
4. Fix RSS issue for brcm stingray.
5. Add a cross-compile script for arm64.

For spdk changes, we are preparing for upstream and when it's ready, will introduce submodule for spdk and give test case steps of nvmeotcp.

Thanks a lot.
Qingmin
Hi, I am running epserver and epwget for single-process, multi-threaded usage. The client always has an ARP request timeout error, while the server does not receive any packets. 

So how could I can solve this problem?
And I would like to know if two hosts using DPDK NICa can ping each other.
Existing ab lack of SSL load test support. Add the apache benchmark
configure script with --enable-ssl and --with-ssl option. Adopt
openssl crypto/bio/bss_sock.c as example to enable ab to create SSL
connection over mTCP socket.

Example:

$./ab  -N 1 -c 2 -n 4 https://10.1.72.68/
Configuration updated by mtcp_setconf().
This is ApacheBench, Version 2.3 <$Revision: 655654 $>
Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
Licensed to The Apache Software Foundation, http://www.apache.org/

---------------------------------------------------------------------------------
Loading mtcp configuration from : config/mtcp.conf
Loading interface setting
EAL: Detected 16 lcore(s)
EAL: Detected 2 NUMA nodes
EAL: Auto-detected process type: PRIMARY
EAL: Multi-process socket /var/run/dpdk/rte/mp_socket
EAL: Probing VFIO support...
EAL: PCI device 0000:0b:00.0 on NUMA socket -1
EAL:   Invalid NUMA socket, default to 0
EAL:   probe driver: 15ad:7b0 net_vmxnet3
Total number of attached devices: 1
Interface name: dpdk0
EAL: Auto-detected process type: PRIMARY
Configurations:
Number of CPU cores available: 1
Number of CPU cores to use: 1
Number of TX ring descriptor: 512
Number of RX ring descriptor: 0
Number of source ip to use: 0
Maximum number of concurrency per core: 16000
Maximum number of preallocated buffers per core: 16000
Receive buffer size: 8192
Send buffer size: 8192
TCP timeout seconds: 30
TCP timewait seconds: 0
NICs to print statistics: dpdk0
---------------------------------------------------------------------------------
Interfaces:
name: dpdk0, ifindex: 0, hwaddr: 00:50:56:86:10:76, ipaddr: 10.1.72.28, netmask: 255.255.0.0
Number of NIC queues: 1
---------------------------------------------------------------------------------
Loading routing configurations from : config/route.conf
Routes:
Destination: 10.1.0.0/16, Mask: 255.255.0.0, Masked: 10.1.0.0, Route: ifdx-0
Destination: 10.1.72.0/24, Mask: 255.255.255.0, Masked: 10.1.72.0, Route: ifdx-0
Destination: 10.2.72.0/24, Mask: 255.255.255.0, Masked: 10.2.72.0, Route: ifdx-0
Destination: 10.169.0.0/16, Mask: 255.255.0.0, Masked: 10.169.0.0, Route: ifdx-0
---------------------------------------------------------------------------------
Loading ARP table from : config/arp.conf
ARP Table:
IP addr: 10.1.72.68, dst_hwaddr: 00:50:56:86:22:BA
---------------------------------------------------------------------------------
Initializing port 0... Ethdev port_id=0 tx_queue_id=0, new added offloads 0x8011 must be within pre-queue offload capabilities 0x0 in rte_eth_tx_queue_setup()

done:
rte_eth_dev_flow_ctrl_get: Function not supported
[dpdk_load_module: 765] Failed to get flow control info!
rte_eth_dev_flow_ctrl_set: Function not supported
[dpdk_load_module: 772] Failed to set flow control info!: errno: -95

Checking link statusdone
Port 0 Link Up - speed 10000 Mbps - full-duplex
Benchmarking 10.1.72.68 (be patient)...
CPU0 connecting to port 443
CPU 0: initialization finished.
[mtcp_create_context:1359] CPU 0 is now the master thread.
[CPU 0] dpdk0 flows:      0, RX:       9(pps) (err:     0),  0.00(Gbps), TX:       0(pps),  0.00(Gbps)
[ ALL ] dpdk0 flows:      0, RX:       9(pps) (err:     0),  0.00(Gbps), TX:       0(pps),  0.00(Gbps)
rte_eth_stats_reset: Function not supported
..done

Server Software:        BigIP
Server Hostname:        10.1.72.68
Server Port:            443
SSL/TLS Protocol:       TLSv1.2,ECDHE-RSA-AES128-GCM-SHA256,2048,128

Document Path:          /
Document Length:        13 bytes

Number of Cores:        1
Concurrency Level:      2
Time taken for tests:   0.715 seconds
Complete requests:      4
Failed requests:        0
Write errors:           0
Total transferred:      344 bytes
HTML transferred:       52 bytes
Requests per second:    5.60 [#/sec] (mean)
Time per request:       357.421 [ms] (mean)
Time per request:       178.710 [ms] (mean, across all concurrent requests)
Transfer rate:          0.47 [Kbytes/sec] received

Connection Times (ms)
              min  mean[+/-sd] median   max
Connect:        5    7   1.2      7       8
Processing:     1    1   0.3      1       2
Waiting:        1    1   0.3      1       2
Total:          6    8   1.3      9       9

Percentage of the requests served within a certain time (ms)
  50%      9
  66%      9
  75%      9
  80%      9
  90%      9
  95%      9
  98%      9
  99%      9
 100%      9 (longest request)
[RunMainLoop: 876] MTCP thread 0 finished.
[mtcp_free_context:1405] MTCP thread 0 joined.
[mtcp_destroy:1685] All MTCP threads are joined.
this patch adds a user-level thread support to mTCP
(enabled by running the configure script with `--enable-uctx` option)
I am testing mTCP in KVM guest with IXGBE SR-IOV VF provisioned, the epwget runs fine with one core, but if I give it two cores, asymmetric flow direction happens, for example:

<pre>
./apps/example/epwget 10.0.0.2 8 -f ./apps/example/epwget.conf -N 2 -c 2
Configuration updated by mtcp_setconf().
Application configuration:
URL: /
# of total_flows: 8
# of cores: 2
Concurrency: 2
---------------------------------------------------------------------------------
Loading mtcp configuration from : ./apps/example/epwget.conf
Loading interface setting
[probe_all_rte_devices: 128] Could not find pci info on dpdk device: =. Is it a dpdk-attached interface?
EAL: Detected 4 lcore(s)
EAL: Detected 1 NUMA nodes
EAL: Auto-detected process type: PRIMARY
EAL: Multi-process socket /var/run/dpdk/rte/mp_socket
EAL: No free hugepages reported in hugepages-1048576kB
EAL: Probing VFIO support...
EAL: WARNING: cpu flags constant_tsc=yes nonstop_tsc=no -> using unreliable clock cycles !
EAL: PCI device 0000:00:10.0 on NUMA socket -1
EAL:   Invalid NUMA socket, default to 0
EAL:   probe driver: 8086:10ed net_ixgbe_vf
Total number of attached devices: 1
Interface name: dpdk0
EAL: Auto-detected process type: PRIMARY
Configurations:
Number of CPU cores available: 2
Number of CPU cores to use: 2
Maximum number of concurrency per core: 10000
Maximum number of preallocated buffers per core: 10000
Receive buffer size: 8192
Send buffer size: 8192
TCP timeout seconds: 30
TCP timewait seconds: 0
NICs to print statistics: dpdk0
---------------------------------------------------------------------------------
Interfaces:
name: dpdk0, ifindex: 0, hwaddr: 52:54:00:7D:36:0C, ipaddr: 10.0.0.1, netmask: 255.255.255.0
Number of NIC queues: 2
---------------------------------------------------------------------------------
Loading routing configurations from : config/route.conf
Routes:
Destination: 10.0.0.0/24, Mask: 255.255.255.0, Masked: 10.0.0.0, Route: ifdx-0
Destination: 10.0.0.0/24, Mask: 255.255.255.0, Masked: 10.0.0.0, Route: ifdx-0
---------------------------------------------------------------------------------
Loading ARP table from : config/arp.conf
ARP Table:
IP addr: 10.0.0.2, dst_hwaddr: 90:E2:BA:93:26:0E
---------------------------------------------------------------------------------
Initializing port 0... ixgbevf_dev_configure(): VF can't disable HW CRC Strip
Ethdev port_id=0 tx_queue_id=0, new added offloads 0x8011 must be within pre-queue offload capabilities 0x0 in rte_eth_tx_queue_setup()

Ethdev port_id=0 tx_queue_id=1, new added offloads 0x8011 must be within pre-queue offload capabilities 0x0 in rte_eth_tx_queue_setup()

done: 
[dpdk_load_module: 761] Failed to get flow control info!
[dpdk_load_module: 768] Failed to set flow control info!: errno: -95

Checking link statusdone
Port 0 Link Up - speed 10000 Mbps - full-duplex
Configuration updated by mtcp_setconf().
CPU 1: initialization finished.
[mtcp_create_context:1359] CPU 1 is now the master thread.
Thread 1 handles 4 flows. connecting to 10.0.0.2:80
[CPU 1] dpdk0 flows:      0, RX:       0(pps) (err:     0),  0.00(Gbps), TX:       0(pps),  0.00(Gbps)
[ ALL ] dpdk0 flows:      0, RX:       0(pps) (err:     0),  0.00(Gbps), TX:       0(pps),  0.00(Gbps)
[CPU 1] dpdk0 flows:      0, RX:       0(pps) (err:     0),  0.00(Gbps), TX:       0(pps),  0.00(Gbps)
[ ALL ] dpdk0 flows:      0, RX:       0(pps) (err:     0),  0.00(Gbps), TX:       0(pps),  0.00(Gbps)
CPU 0: initialization finished.
Response size set to 978
Thread 0 handles 4 flows. connecting to 10.0.0.2:80
[CPU 0] dpdk0 flows:      1, RX:       5(pps) (err:     0),  0.00(Gbps), TX:       7(pps),  0.00(Gbps)
[CPU 1] dpdk0 flows:      1, RX:       5(pps) (err:     0),  0.00(Gbps), TX:       7(pps),  0.00(Gbps)
[ ALL ] dpdk0 flows:      2, RX:      10(pps) (err:     0),  0.00(Gbps), TX:      14(pps),  0.00(Gbps)
[CPU 0] dpdk0 flows:      1, RX:       1(pps) (err:     0),  0.00(Gbps), TX:       2(pps),  0.00(Gbps)
[CPU 1] dpdk0 flows:      1, RX:       1(pps) (err:     0),  0.00(Gbps), TX:       2(pps),  0.00(Gbps)
[ ALL ] dpdk0 flows:      2, RX:       2(pps) (err:     0),  0.00(Gbps), TX:       4(pps),  0.00(Gbps)
[CPU 0] dpdk0 flows:      1, RX:       1(pps) (err:     0),  0.00(Gbps), TX:       2(pps),  0.00(Gbps)
[CPU 1] dpdk0 flows:      1, RX:       1(pps) (err:     0),  0.00(Gbps), TX:       2(pps),  0.00(Gbps)

</pre>

here is the debug log for core 0 log_0, note the tcp source port 1028 connection (SYN) is initiated by core 1, but the SYN+ACK is received by core 0, no existing TCP stream matches the flow on core 0, thus the [CreateNewFlowHTEntry: 725] Weird packet comes.

core 0 log_0:

<pre>

[MTCPRunThread:1238] CPU 0: initialization finished.
[STREAM: CreateTCPStream: 372] CREATED NEW TCP STREAM 0: 10.0.0.1(1026) -> 10.0.0.2(80) (ISS: 1012484)
[ STATE: mtcp_connect: 807] Stream 0: TCP_ST_SYN_SENT
[RunMainLoop: 773] CPU 0: mtcp thread running.
IN 0 1827801836 10.0.0.2(80) -> 10.0.0.1(1028) IP_ID=0 TTL=64 TCP S A seq 191461752 ack 1716955680 WDW=65160 len=74
[CreateNewFlowHTEntry: 725] Weird packet comes.
10.0.0.2(80) -> 10.0.0.1(1028) IP_ID=0 TTL=64 TCP S A seq 191461752 ack 1716955680 WDW=65160 len=60
IN 0 1827801837 10.0.0.2(80) -> 10.0.0.1(1026) IP_ID=0 TTL=64 TCP S A seq 3479957166 ack 1012485 WDW=65160 len=74
[ STATE: Handle_TCP_ST_SYN_SENT: 812] Stream 0: TCP_ST_ESTABLISHED
IN 0 1827801849 10.0.0.2(80) -> 10.0.0.1(1026) IP_ID=4245 TTL=64 TCP A seq 3479957167 ack 1012586 WDW=509 len=66
IN 0 1827801849 10.0.0.2(80) -> 10.0.0.1(1026) IP_ID=4246 TTL=64 TCP F A seq 3479957167 ack 1012586 WDW=509 len=1044
[ STATE: Handle_TCP_ST_ESTABLISHED: 942] Stream 0: TCP_ST_CLOSE_WAIT
[STREAM: CreateTCPStream: 372] CREATED NEW TCP STREAM 1: 10.0.0.1(1027) -> 10.0.0.2(80) (ISS: 1716955679)
[ STATE: mtcp_connect: 807] Stream 1: TCP_ST_SYN_SENT
[ STATE: HandleApplicationCalls: 599] Stream 0: TCP_ST_LAST_ACK
IN 0 1827801849 10.0.0.2(80) -> 10.0.0.1(1026) IP_ID=0 TTL=64 TCP A seq 3479958146 ack 1012587 WDW=509 len=66
[ STATE: Handle_TCP_ST_LAST_ACK:1015] Stream 0: TCP_ST_CLOSED
[STREAM: DestroyTCPStream: 413] DESTROY TCP STREAM 0: 10.0.0.1(1026) -> 10.0.0.2(80) (CLOSED)
[STREAM: DestroyTCPStream: 569] Destroyed. Remaining flows: 1
IN 0 1827802327 10.0.0.2(80) -> 10.0.0.1(1028) IP_ID=0 TTL=64 TCP S A seq 199272686 ack 1716955680 WDW=65160 len=74
[CreateNewFlowHTEntry: 725] Weird packet comes.
10.0.0.2(80) -> 10.0.0.1(1028) IP_ID=0 TTL=64 TCP S A seq 199272686 ack 1716955680 WDW=65160 len=60
IN 0 1827803327 10.0.0.2(80) -> 10.0.0.1(1028) IP_ID=0 TTL=64 TCP S A seq 214897367 ack 1716955680 WDW=65160 len=74
[CreateNewFlowHTEntry: 725] Weird packet comes.
10.0.0.2(80) -> 10.0.0.1(1028) IP_ID=0 TTL=64 TCP S A seq 214897367 ack 1716955680 WDW=65160 len=60
IN 0 1827805327 10.0.0.2(80) -> 10.0.0.1(1028) IP_ID=0 TTL=64 TCP S A seq 246146665 ack 1716955680 WDW=65160 len=74
[CreateNewFlowHTEntry: 725] Weird packet comes.
10.0.0.2(80) -> 10.0.0.1(1028) IP_ID=0 TTL=64 TCP S A seq 246146665 ack 1716955680 WDW=65160 len=60


</pre>

core 1 log_1:

<pre>

[MTCPRunThread:1238] CPU 1: initialization finished.
[RunMainLoop: 773] CPU 1: mtcp thread running.
[STREAM: CreateTCPStream: 372] CREATED NEW TCP STREAM 0: 10.0.0.1(1025) -> 10.0.0.2(80) (ISS: 1012484)
[ STATE: mtcp_connect: 807] Stream 0: TCP_ST_SYN_SENT
IN 0 1827801826 10.0.0.2(80) -> 10.0.0.1(1025) IP_ID=0 TTL=64 TCP S A seq 2092968279 ack 1012485 WDW=65160 len=74
[ STATE: Handle_TCP_ST_SYN_SENT: 812] Stream 0: TCP_ST_ESTABLISHED
IN 0 1827801826 10.0.0.2(80) -> 10.0.0.1(1025) IP_ID=55151 TTL=64 TCP A seq 2092968280 ack 1012586 WDW=509 len=66
IN 0 1827801827 10.0.0.2(80) -> 10.0.0.1(1025) IP_ID=55152 TTL=64 TCP F A seq 2092968280 ack 1012586 WDW=509 len=1044
[ STATE: Handle_TCP_ST_ESTABLISHED: 942] Stream 0: TCP_ST_CLOSE_WAIT
[STREAM: CreateTCPStream: 372] CREATED NEW TCP STREAM 1: 10.0.0.1(1028) -> 10.0.0.2(80) (ISS: 1716955679)
[ STATE: mtcp_connect: 807] Stream 1: TCP_ST_SYN_SENT
[ STATE: HandleApplicationCalls: 599] Stream 0: TCP_ST_LAST_ACK
IN 0 1827801827 10.0.0.2(80) -> 10.0.0.1(1025) IP_ID=0 TTL=64 TCP A seq 2092969259 ack 1012587 WDW=509 len=66
[ STATE: Handle_TCP_ST_LAST_ACK:1015] Stream 0: TCP_ST_CLOSED
[STREAM: DestroyTCPStream: 413] DESTROY TCP STREAM 0: 10.0.0.1(1025) -> 10.0.0.2(80) (CLOSED)
[STREAM: DestroyTCPStream: 569] Destroyed. Remaining flows: 1
IN 0 1827801849 10.0.0.2(80) -> 10.0.0.1(1027) IP_ID=0 TTL=64 TCP S A seq 509557464 ack 1716955680 WDW=65160 len=74
[CreateNewFlowHTEntry: 725] Weird packet comes.
10.0.0.2(80) -> 10.0.0.1(1027) IP_ID=0 TTL=64 TCP S A seq 509557464 ack 1716955680 WDW=65160 len=60
IN 0 1827802349 10.0.0.2(80) -> 10.0.0.1(1027) IP_ID=0 TTL=64 TCP S A seq 517358188 ack 1716955680 WDW=65160 len=74
[CreateNewFlowHTEntry: 725] Weird packet comes.
10.0.0.2(80) -> 10.0.0.1(1027) IP_ID=0 TTL=64 TCP S A seq 517358188 ack 1716955680 WDW=65160 len=60
IN 0 1827803349 10.0.0.2(80) -> 10.0.0.1(1027) IP_ID=0 TTL=64 TCP S A seq 532982841 ack 1716955680 WDW=65160 len=74
[CreateNewFlowHTEntry: 725] Weird packet comes.
10.0.0.2(80) -> 10.0.0.1(1027) IP_ID=0 TTL=64 TCP S A seq 532982841 ack 1716955680 WDW=65160 len=60
IN 0 1827805349 10.0.0.2(80) -> 10.0.0.1(1027) IP_ID=0 TTL=64 TCP S A seq 564232134 ack 1716955680 WDW=65160 len=74
[CreateNewFlowHTEntry: 725] Weird packet comes.
10.0.0.2(80) -> 10.0.0.1(1027) IP_ID=0 TTL=64 TCP S A seq 564232134 ack 1716955680 WDW=65160 len=60
IN 0 1827809349 10.0.0.2(80) -> 10.0.0.1(1027) IP_ID=0 TTL=64 TCP S A seq 626730718 ack 1716955680 WDW=65160 len=74
[CreateNewFlowHTEntry: 725] Weird packet comes.
10.0.0.2(80) -> 10.0.0.1(1027) IP_ID=0 TTL=64 TCP S A seq 626730718 ack 1716955680 WDW=65160 len=60
[RunMainLoop: 871] MTCP thread 1 out of main loop.
[MTCPRunThread:1238] CPU 1: initialization finished.

</pre>

is this something need to be addressed in mtcp/src/rss.c ?

<pre>

/*-------------------------------------------------------------------*/
/* RSS redirection table is in the little endian byte order (intel)  */
/*                                                                   */
/* idx: 0 1 2 3 | 4 5 6 7 | 8 9 10 11 | 12 13 14 15 | 16 17 18 19 ...*/
/* val: 3 2 1 0 | 7 6 5 4 | 11 10 9 8 | 15 14 13 12 | 19 18 17 16 ...*/
/* qid = val % num_queues */
/*-------------------------------------------------------------------*/
/*
 * IXGBE (Intel X520 NIC) : (Rx queue #) = (7 LS bits of RSS hash) mod N
 * I40E (Intel XL710 NIC) : (Rx queue #) = (9 LS bits of RSS hash) mod N
 */
#define RSS_BIT_MASK_IXGBE              0x0000007F
#define RSS_BIT_MASK_I40E               0x000001FF



</pre>




After bind my `ens4` (which is AWS-ENA nic) to `vfio-pci` driver, the system cannot see the device anymore. I can successfully complete the compiling process. But don't know how to run epserver through that `ens4` interface. 

```
Network devices using DPDK-compatible driver
============================================
0000:00:04.0 'Elastic Network Adapter (ENA) ec20' drv=vfio-pci unused=ena

Network devices using kernel driver
===================================
0000:00:03.0 'Elastic Network Adapter (ENA) ec20' if=ens3 drv=ena unused=vfio-pci *Active*
```
I noticed that TCPCalcChecksum is consuming CPU in my workload profile. I want to offload this calculation to hardware/NIC. I noticed DISABLE_HWCSUM flag in tcp_in.c etc. Where should I disable this flag to enable hardware checksum calculation? Or is there any other way to do this.
Thanks
Hello,

I am trying to write an application with mTCP which repeatedly sends multiple requests to the server without waiting for a response. In short, we are not following the request response model directly. However, I have observed that I am only able to send as much packets as specified by the RECV_BUFF variable. For example, if RECV_BUFF was set 2048 and my single request's size is 1024, then I am only able to send two packets. After that mtcp_write throws an EAGAIN error. According to mtcp_write [link](http://shader.kaist.edu/mtcp/man/mtcp_write.html) documentation, EAGAIN means that I need to send the message again until buffer is available. Which in my case means I have to repeatedly call mtcp_write until it succeeds which in essence means wasted CPU cycles.

Can you please guide what will be the best way to achieve what I need with mTCP? Is there a way to "flush" the packets without waiting for the buffer?

Regards
I have a distributed storage software that wants to use mtcp. I need to start 2 processes (metanode and datanode) on each machine. I have 8 machines. Each machine needs to start metanode and datanode, and each process needs Communicate with each other, then I use openNetVM. Can I meet my needs?

Hello,

Let say I have two mtcp threads, running on two different cores, which are reading packets from the NIC. After receiving a packet, the mtcp thread puts it in a ring buffer. Another thread (non-mtcp thread, running on a third core using `rte_eal_remote_launch`) reads the packet from that ring buffer, does some processing, and generates a response. That response is then send to the mtcp thread, which received the packet initially from NIC, using a different ring buffer. Finally, mtcp sends the response to the NIC using its own TX queue.

My question is this can I have a thread in my program which doesn't directly communicate with NIC (non-mtcp thread)?

Thanks,