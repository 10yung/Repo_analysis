Hi - I'm new to Kafka so perhaps I'm not understanding something. First, thanks for providing this dockerized Kafka image. It works great. I have the Kafka logs volume mounted and using `docker-compose stop` and `docker-compose up -d --no-recreate` seem to be doing the trick. I am curious how one should handle data persistence a container is torn down (e.g. `docker-compose down`). It seems that all of the necessary data is still there on the docker host, but when starting the Kafka container runs into an issue where the Cluster ID does not match the id specified in meta.properties, and the Kafka container exits. Is there a way around this? Perhaps more environment variables that need to be specified in the compose file? Thanks in advance for any guidance and/or suggestions you may have.
Hi,
I am running one jar file in Linux, which has Kafka listeners spring boot annotation. While I am running that jar no data is coming to the topic until I press any key from keyboard. After pressing any key all the data is coming which are already there in that topic. 
Can anyone please let me know what is the root cause?

```
if [[ -n "$KAFKA_HEAP_OPTS" ]]; then
    sed -r -i 's/(export KAFKA_HEAP_OPTS)="(.*)"/\1="'"$KAFKA_HEAP_OPTS"'"/g' "$KAFKA_HOME/bin/kafka-server-start.sh"
    unset KAFKA_HEAP_OPTS
fi
```
Since the official script `kafka-server-start.sh` already checks and well handles the environment variable `KAFKA_HEAP_OPTS`, the block above is redundant. I think it should be removed. 
Kafka supports now java-11 https://issues.apache.org/jira/browse/KAFKA-7264.
As java-11 brings better ergonomics when running in docker I think it would be useful to have java-11 supported in this image
I wanted to run kafka-docker in k8s for testing purposes, even a single node deployment is good enough. Can we provide a guide for k8s deployment?
see https://github.com/docker/compose/issues/6316#issuecomment-442666873 

fixes https://github.com/wurstmeister/kafka-docker/issues/537
Host ports should not be limited to 2181.
I called command `docker-compose up -d` to run zookeeper and kafka but I got error:

> ERROR: Service 'kafka' failed to build: The command '/bin/sh -c apk add --no-cache bash curl jq docker  && chmod a+x /tmp/*.sh  && mv /tmp/start-kafka.sh /tmp/broker-list.sh /tmp/create-topics.sh /tmp/versions.sh /usr/bin  && sync && /tmp/download-kafka.sh  && tar xfz /tmp/kafka_${SCALA_VERSION}-${KAFKA_VERSION}.tgz -C /opt  && rm /tmp/kafka_${SCALA_VERSION}-${KAFKA_VERSION}.tgz  && ln -s /opt/kafka_${SCALA_VERSION}-${KAFKA_VERSION} ${KAFKA_HOME}  && rm /tmp/*  && wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/${GLIBC_VERSION}/glibc-${GLIBC_VERSION}.apk  && apk add --no-cache --allow-untrusted glibc-${GLIBC_VERSION}.apk  && rm glibc-${GLIBC_VERSION}.apk' returned a non-zero code: 2

My docker-compose.yml file looks:
```
version: '2'
services:
  zookeeper:
    image: wurstmeister/zookeeper
    ports:
      - "2181:2181"
  kafka:
    build: .
    ports:
      - "9092"
    environment:
      KAFKA_ADVERTISED_HOST_NAME: kafka
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
```
I'm using docker on Windows 10. How can I fix it?
With this you will not need to remember to run `docker-compose -up` after restarting **Docker** or the whole machine.