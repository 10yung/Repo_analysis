Thanks for your sharing. I have read your paper, and I am confused about how to make sure the number of the kernels in "conv" operation? 
In ```operations.py``` it seems like 'c_in==c_out', but I wonder why. I always think that NAS is to determine not only the TYPE of the operations but also the NUMBERs, like the number of the kernels, which makes it difficult. I don't know if there is anything that I misunderstand?
Hi,

Genotype concatenation calculation and printing has some bug.

Printing
----------
During printing of genotype in search process, the main function calls genotype function, which is member function of class Network.
There is a line in genotype function, "concat = range(2+self._steps-self._multiplier, self._steps+2)".
This line produces [2,4] and not [2,3,4], which should have been the case.
So the line should be replaced with something like
      concat = []
      for i in range(2+self._steps-self._multiplier, self._steps+2):
            concat.append(i)

Calculation
---------------
According to the paper,  only the feature maps of those nodes, whose outputs are not used as inputs to any other nodes in the cell, are concatenated to form the output of the cell.
But according to the code in genotype function in Network class, it seems the code is blindly concatenating all internal nodes of the cell.
So, according to me there should be a logic to check if top 2 most probable operations doesnt have a node in any of the nodes of the cell, then include the node in concatenation.

Does the above claims make sense?
Hi, I want to ask if I want to apply it to the time series, what should I modify? I have modified the final output and loss functions, but a few epochs later, Nan will appear
@quark0 Hi, I have some question about the base framework in Darts. How did you choose the framework, just follow NasNet? I found that in this framework, ‘Relu’ is placed at  the front end of each module, before the conv rather than after it. This arrangement might result in more relu operations and is not friendly to some hardware. I'm confused about this, could you give me some explanation?
thank you
The alpha variable's value of "avg_pool" and "skip_connect" is the mostly biggest over the others, and "none" alpha is very big to 0.6, when search the normal_cell. And it is the same with reduce_cell.
Hi, I have tried to repeat the architecture search on CIFAR10 for 10 times using exactly the same code with the following command. 
`python train_search.py --unrolled`

But unfortunately none of the experiments generates the exact same cells published in the paper and every experiment produced different cell structure. 

Any advice on why is that the case? Thanks.
Thanks for your great work!
I find some inconsistency  about the architecture parameters initialization between your paper and your code. In your paper, you said using zero initialization for architecture variables, which implies equal amount attention over all possible ops. However, in your code,  you initialize the architecture variables randomly:  ' Variable(1e-3*torch.randn(k, num_ops).cuda(), requires_grad=True)'  in your code.  Could you please explain why you use this random initialization? 

@quark0  hi, firstly, thanks for your work , I'm really interested in Darts. 
But I have some confusion about my reproduce result. I found that the network I searched was too big compared to the network released in Darts. The param_size of my result model are generally 8M， while released mode‘s is nearly 3M.
Could you tell me the reason why the results are different. Or I just should continue to search for more networks.
Thanks
In rnn/utils.py

The below code is meaning add a mask to embeding. But it run error at torch 1.12. Does anyone know to realize it at torch 1.12? Thank you!

X = embed._backend.Embedding.apply(words, masked_embed_weight,
        padding_idx, embed.max_norm, embed.norm_type,
        embed.scale_grad_by_freq, embed.sparse
    )

