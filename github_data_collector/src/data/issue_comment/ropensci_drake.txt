## Prework

- [x] Read and abide by `drake`'s [code of conduct](https://github.com/ropensci/drake/blob/master/CODE_OF_CONDUCT.md).
- [x] Search for duplicates among the [existing issues](https://github.com/ropensci/drake/issues), both open and closed.

## Proposal

Certainly not a new idea, but I think we are now ready to try. We just need to send individual target data files to the cloud, e.g. Amazon S3. The rest of the `storr` cache can stay local. The big data can live on the cloud, and the swarm of tiny metadata files can still live locally. That way, local `drake` caches are highly portable and shareable, and we can more easily trust the big data not to break when we move around caches.

What makes this idea possible now? Why not earlier? Because of specialized data formats: https://books.ropensci.org/drake/plans.html#special-data-formats-for-targets. Due to the mechanics of the implementation, `drake` can bypass the `storr` for big files, while letting `storr` keep taking care of the small files. That means we should be able to shuffle around big data when it counts, while avoiding unnecessary network transactions (e.g. https://github.com/richfitz/storr/pull/72 would send metadata to the cloud as well, which would severely slow down data processing).

## API

I am thinking about a new argument to `make()` and `drake_config()`

```r
make(plan, storage = "amazon_s3")
```

and target-specific configurable storage

```r
plan <- drake_plan(
  small_data = get_small_data() # not worth uploading to the cloud,
  large_data = target(
    get_large_data(),
    storage = "amazon_s3"
  )
)
```
