Whats the status of this project?  I was curious about it but don't want to invest time if its abandoned.   Commit log looks like it slowed down.

As discussed on Gitter it would be great Ballista could read and write from HDFS.

Ideally Ballista would have a common way of accessing data from various sources such as S3 or other object stores.

What would the timeline for support here look like? I guess the initial steps would be to design the storage API, which arbitrary sources could access?

In terms of Rust support for HDFS, I haven't seen any particularly mature implementations. Do you see this (or a child project) as the place for that work to go? Or would that be more in the realm of Arrow?

Sorry if I've missed anything here or looked over something obvious, still in the grokking phase of Ballista but like what it does for distributed compute with rust. HDFS Support would really unblock things for me wrt to using rust in a professional capacity.
In order to demonstrate how K8s would help manage client interaction,  create example CRD schemas and a controller that has stub functionality for query submission and execution planning. 

Deliverables: 
BallistaQuery CRD
BallistaPlan CRD
BallistaOperator controller implementation and deployment. 
This is to show how the CLI module may look if structopt is used instead of base clap.

This is based on #55 and so shouldn't be merged until that PR is (it should be simple to change this PR in case #55 isn't merged).

Fixes #60.
These are turning out to be a pain because of the dependency on arrow github repo
There are so many sources of errors and we'll need to provide context to the user for each one. [snafu](https://docs.rs/snafu/0.4.3/snafu/) seems to provide a nice way of doing this in a single place, so could be worth investigating.
If other tools in this space are anything to go by, we'll end up with a lot of options in the CLI, so having arg parsing handled automatically would be great.
The spark benchmark fails with:

```
Caused by: java.net.UnknownHostException: io-andygrove-ballista-spark-main-1564066103519-driver-svc.default.svc
```

This is probably some minikube DNS issue that needs to be figured out and documented.
The example relies on data being located at /mnt/ssd/nyc_taxis/csv currently and we at least need to document this so others can run the example