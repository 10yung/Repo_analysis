@macanv 
为什么服务调用超过一定的次数之后，我在自己的电脑上测的当调用超过14352次后，服务就访问不通了？
报错信息如下：
{'description': 'Too many concurrent connections!Try to increase the value of "max_concurrency", currently =10', 'status': 400, 'type': 'RuntimeError'}
我试过在起服务之前把max_concurrency改成50，到41000多的时候，又会出现上面的情况

大佬帮我看看QAQ

I:?[33mWORKER-0?[0m:use device gpu: 0, load graph from F:\学习资料\毕设\code\bert-master\bert-master\output\classification_
model.pb
WARNING:tensorflow:From d:\anaconda\lib\site-packages\bert_base-0.0.9-py3.7.egg\bert_base\server\helper.py:161: The name tf
.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

WARNING:tensorflow:From d:\anaconda\lib\site-packages\bert_base-0.0.9-py3.7.egg\bert_base\server\helper.py:161: The name tf
.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.

Process BertWorker-3:
Traceback (most recent call last):
  File "D:\Anaconda\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
  File "d:\anaconda\lib\site-packages\bert_base-0.0.9-py3.7.egg\bert_base\server\__init__.py", line 490, in run
    self._run()
  File "d:\anaconda\lib\site-packages\zmq\decorators.py", line 75, in wrapper
    return func(*args, **kwargs)
  File "d:\anaconda\lib\site-packages\bert_base-0.0.9-py3.7.egg\bert_base\server\zmq_decor.py", line 27, in wrapper
    return func(*args, **kwargs)
  File "d:\anaconda\lib\site-packages\bert_base-0.0.9-py3.7.egg\bert_base\server\__init__.py", line 508, in _run
    for r in estimator.predict(input_fn=self.input_fn_builder(receivers, tf), yield_single_examples=False):
  File "d:\anaconda\lib\site-packages\tensorflow_estimator\python\estimator\estimator.py", line 622, in predict
    features, None, ModeKeys.PREDICT, self.config)
  File "d:\anaconda\lib\site-packages\tensorflow_estimator\python\estimator\estimator.py", line 1149, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File "d:\anaconda\lib\site-packages\bert_base-0.0.9-py3.7.egg\bert_base\server\__init__.py", line 466, in classification_
model_fn
    pred_probs = tf.import_graph_def(graph_def, name='', input_map=input_map, return_elements=['pred_prob:0'])
  File "d:\anaconda\lib\site-packages\tensorflow_core\python\util\deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "d:\anaconda\lib\site-packages\tensorflow_core\python\framework\importer.py", line 405, in import_graph_def
    producer_op_list=producer_op_list)
  File "d:\anaconda\lib\site-packages\tensorflow_core\python\framework\importer.py", line 535, in _import_graph_def_interna
l
    ', '.join(missing_unused_input_keys))
**ValueError: Attempted to map inputs that were not found in graph_def: [segment_ids:0]**

我将12层的BERT模型改为6层或者3层，预测有时会出现X的情况，另外预测序列长度小于输入序列长度，请问是怎么回事？
请问如果我想减少BERT的层数，应该在哪些参数上进行调整那？
请问针对句子对相似度计算的模型，如何部署呢？敬请回复！
使用 1个cpu  12核心，一个1000个字的文章，推理速度在1000ms，这个速度正常吗，GPU，大概是100毫秒。一般什么速度比较正常？
使用自己的数据用run.py训练NER模型，之后想部署服务，压缩ckpt为pb格式，却一直提示这个错误，还请大佬赐教~感谢！

Key output_bias not found in checkpoint
         [[node save/RestoreV2 (defined at freeze_graph.py:191)  = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device="/job:localhost/replica:0/task:0/device:CPU:0"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]


训练过程没有报错。尝试用terminal_predict.py做在线预测，结果提示output路径里没有label_list.pkl文件。
猜测原因可能是把Bert_lstm_ner.py里get_labels语句改了。
原本是
  def get_labels(self, labels=None):
        if labels is not None:
            try:
                # 支持从文件中读取标签类型
                if os.path.exists(labels) and os.path.isfile(labels):
                    with codecs.open(labels, 'r', encoding='utf-8') as fd:
                        for line in fd:
                            self.labels.append(line.strip())
                else:
                    # 否则通过传入的参数，按照逗号分割
                    self.labels = labels.split(',')
                self.labels = set(self.labels) # to set
            except Exception as e:
                print(e)
        # 通过读取train文件获取标签的方法会出现一定的风险。
        if os.path.exists(os.path.join(self.output_dir, 'label_list.pkl')):
            with codecs.open(os.path.join(self.output_dir, 'label_list.pkl'), 'rb') as rf:
                self.labels = pickle.load(rf)
        else:
            if len(self.labels) > 0:
                self.labels = self.labels.union(set(["X", "[CLS]", "[SEP]"]))
                with codecs.open(os.path.join(self.output_dir, 'label_list.pkl'), 'wb') as rf:
                    pickle.dump(self.labels, rf)
            else:
                self.labels = ["O", 'B-TIM', 'I-TIM', "B-PER", "I-PER", "B-ORG", "I-ORG", "B-LOC", "I-LOC", "X", "[CLS]", "[SEP]"]
        return self.labels
后面改成了
    def get_labels(self):
        return ["O", "B-PER", "I-PER", "B-ORG", "I-ORG", "B-LOC", "I-LOC", "X", "[CLS]", "[SEP]"]
请问是这个原因吗？

另外，对于第一种get_labels语句，是完全从train data读取标签对吗，所以无论train data怎样都不需要改动对吗？
bert-base-ner-train \
    -data_dir /home/lef/Desktop/\
    -output_dir /home/lef/Desktop/output\
    -init_checkpoint /home/lef/Desktop/bert_chinese/bert_model.ckpt\
    -bert_config_file /home/lef/Desktop/bert_chinese/bert_config.json \
    -vocab_file /home/lef/Desktop/bert_chinese/vocab.txt \
    -max_seq_length 128 \
    -num_train_epochs 5 \
    -do_train FALSE \
    -do_eval FALSE
已经训练好了一个模型，现在想在一个新的test集检测下，但是用了如上语句后，程序显示train和eval还是true，请问需要如何修改呢？
您好，我问一下如果我做词性标注应该做哪些地方的改动呢