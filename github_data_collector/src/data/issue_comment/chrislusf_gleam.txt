hi, chris, this time i run a standalone kafkareader connecting to a remote kafka cluster. no error reported, but no print after i put into messages into the "test" topic by kafka-console-consumer.sh. i changed the %x to %s, still no print.
This looks like an awesome tool but I'm curious how it stacks up to various Apache big data projects. 

It would be awesome to have some benchmarks and/or comparison to Beam, Flink, Hadoop, etc.

Thanks!
directory: 

```text
github.com/chrislusf/gleam/examples/word_count_in_go
```

Environment:

```text
MacOS
```

command:

```bash
GOOS=linux GOARCH=amd64 go build -o word_count_in_go_linux word_count_in_go.go
```

Error Message:

```text
go build github.com/DataDog/zstd: build constraints exclude all Go files in go/src/github.com/DataDog/zstd
```

One solution: https://github.com/DataDog/zstd/issues/25
Any other alternatives except xgo?



Setup Gleam Cluster Locally, MacOS sudo needed.
tidb now has a column storage and so is a good match for GLeam.

https://www.pingcap.com/blog/tidb-3.0-announcement/

https://www.percona.com/live/19/sites/default/files/slides/Making%20HTAP%20Real%20with%20TiFlash%20--%20A%20TiDB%20Native%20Columnar%20Extension%20-%20FileId%20-%20174070.pdf
This is a first attempt at adding this functionality for reading from zip files. The idea is that each file in the zip file is returned as a row in the form of a byte array, the user an choose to do with the array as they see fit.
Hello, I am trying to figure out if gleam is the right tool for the job.

My inquiry is similar to #27 , I have a huge amount of data that I want to keep in memory to run a certain operation on it several times in a distributed fashion.

For the sake of clarity, I am reporting a simplified pseudo-code example of my use case.

```
data = seq(1, 1024*3)
mapping = (n) -> n*56
filter_op = (n) -> n < 123456
```

My goal is to have 3 nodes, each with `1024` numbers. Each node would run `mapping`, discarding results that do not satisfy the `filter_op` predicate.

I would like to be able to run this multiple times, changing `mapping` or `filter_op` but keeping the `1024` numbers in the nodes' memory.

Thank you in advance for your time and help!

If I run `flow.Sort(..., flow.OrderBy(2, true))` on the row "1 2 3 4", the order of the keys and values gets changed to "2 1 3 4". However, when I run `flow.LocalSort(..., flow.OrderBy(2, true))`, the order stays the same. Is this intentional behaviour? Ideally, any operations in the flow shouldn't change the order of the keys and values unless explicitly asked for right?