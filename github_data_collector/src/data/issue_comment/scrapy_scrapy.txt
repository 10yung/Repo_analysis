This solves #4276. All tests passed in my computer. Any questions or modifications let me know. Thanks.
### Versions Affected:

All versions after PR #2082 when flags were added to `Request`. (>1.4.0)

### Response.follow() method not consistent with Request.__init__(). Missing `flags`.

No sure how this got missed since PR #2082 was merged, but looking at the parameters for `Request.__init__` and `Response.follow` it appears `flags` wasn't added to `Response.follow` to keep it in line with how creating new `Request` instances works. I'm not sure if this was just overlooked or was intentional, when `flags` support was added to requests. It's also missing for the subclasses of `Response` as well (`TextResponse` will need updated, the rest currently use inheritance).

I was looking to be able to add custom `flags` to certain `Response.follow()` calls but realized looking at the source they wouldn't be carried over to the new responses, but this can easily be worked around by manually added the flag after Request creation. This seems like an easy fix to include in 1.8.1 or later but doesn't seem too high priority and I just thought you guys should know or that it should at least be noted somewhere on this repo since the [docs](https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Response.follow) still say, "It accepts the same arguments as `Request.__init__` method...".

<!--

Thanks for taking an interest in Scrapy!

If you have a question that starts with "How to...", please see the Scrapy Community page: https://scrapy.org/community/.
The GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.

Keep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md

The following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs

-->

### Description

The static variables set in `scrapy.Items` class are not registered in the internal `_values` dict and therefore I cannot access them via `self['field_name']`, i.e., the `__getattr__` function.

### Steps to Reproduce
In my items_error.py file I have this. (the actual code is larger but I isolate the code causing the error)
```
from scrapy import Item, Field

class BaseItem(Item):
    url=Field()
    id=Field()
    type=Field()

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if not self['type']: #<--- error here. Raises KeyError
            raise NotImplementedError('type of ad must be set as static class variable')
        #use url hash as id to avoid duplicates
        self['id']=hash(self.get('url'))

class ChildItem(BaseItem):
    type='estate'
    other_fields=Field()
```

When I ran it this raises:
```
C:\Users\ivangonzalez\Documents\Code\scrady>python
Python 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 21:48:41) [MSC v.1916 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
>>> from scrady.items_error import *
>>> ChildItem()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ivangonzalez\Documents\Code\scrady\scrady\items_error.py", line 10, in __init__
    if not self['type']: #<--- error here. Raises KeyError
  File "C:\Users\ivangonzalez\Anaconda3\lib\site-packages\scrapy\item.py", line 91, in __getitem__
    return self._values[key]
KeyError: 'type'
>>> BaseItem()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "C:\Users\ivangonzalez\Documents\Code\scrady\scrady\items_error.py", line 10, in __init__
    if not self['type']: #<--- error here. Raises KeyError
  File "C:\Users\ivangonzalez\.virtualenvs\scrady-d4yjsu4b\lib\site-packages\scrapy\item.py", line 91, in __getitem__       return self._values[key]
KeyError: 'type'
>>>
```

**Expected behavior:** `self['field_name']` should return the `Field()` attribute.

**Actual behavior:** static fields are not registered by `scrapy.Items`  in `scrapy.Items._values[key]`.

**Reproduces how often:** Ran in pipenv, so 100%.

### Versions

Please paste here the output of executing `scrapy version --verbose` in the command line:
```
C:\Users\ivangonzalez\Documents\Code\scrady>scrapy version --verbose
Scrapy       : 1.8.0
lxml         : 4.4.2.0
libxml2      : 2.9.5
cssselect    : 1.1.0
parsel       : 1.5.2
w3lib        : 1.21.0
Twisted      : 19.10.0
Python       : 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 21:48:41) [MSC v.1916 64 bit (AMD64)]
pyOpenSSL    : 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019)
cryptography : 2.8
Platform     : Windows-10-10.0.18362-SP0
```

### Additional context

In scrapy/item.py : ItemMeta class the attributes should be registered by this code:

```
class ItemMeta(ABCMeta):
    """Metaclass_ of :class:`Item` that handles field definitions.

    .. _metaclass: https://realpython.com/python-metaclasses
    """

    def __new__(mcs, class_name, bases, attrs):
        classcell = attrs.pop('__classcell__', None)
        new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))
        _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)

        fields = getattr(_class, 'fields', {})
        new_attrs = {}
        for n in dir(_class):
            v = getattr(_class, n)
            if isinstance(v, Field):
                fields[n] = v
            elif n in attrs:
                new_attrs[n] = attrs[n] #<--- here

        new_attrs['fields'] = fields
        new_attrs['_class'] = _class
        if classcell is not None:
            new_attrs['__classcell__'] = classcell
        return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs)
```

But they aren't. I don't know why.


@Gallaecio created an issue #4264, in which I had to add in the backward-incompatible changes that scrapy 1.7.0 version breaks custom schedulers. 
I have added it to **news.rst**, please check it out and let me know if it needs to be more thorough.

I have ran all the tests using **tox**

(edit) Fixes #4264
Fixes #4260.

Evaluates the output iterable right after the spider callback, as it's currently being done in the `process_spider_output` chain.

(Plus some minor styling adjustments)

This is not very useful yet, as it doesn't allow `yield` in callbacks, but it's a first step.
On some test runs `CrawlTestCase.test_fixed_delay`  fails with
```
>       yield self._test_delay(total=3, delay=0.1)
E   twisted.trial.unittest.FailTest: twisted.trial.unittest.FailTest: True is not false : test total or delay values are too small
```
I have noticed that the offcial documentation suggests using [brotlipy](https://pypi.org/project/brotlipy/). While this works, the project seems to be stale or abandoned. Most recent release was in 2017 and most recent commit - in 2018.

I suggest replacing that with a more recent and seemingly more actively supported [Brotli](https://pypi.org/project/Brotli) by google. It's most recent release was in 2018 and most recent commit - this month. This library has same API and judging by my experience, has proved to work seamlessly.
