## update the code to fastai>=1.0.48

Because of update, the code cell cannot run in newest version of fastai

Hi Sylvain, thanks for the awesome exposition in your DeepPainterlyHarmonization notebook.  I am however running into an issue during the second training phase.  Not changing anything, was able to reproduce the code up until In[84].  
With the following warning:

IndexError: too many indices for tensor of dimension 1

Here's the detailed warning: 

---------------------------------------------
IndexError  Traceback (most recent call last)
<ipython-input-191-eb68f305f4df> in <module>()
      1 n_iter=0
----> 2 while n_iter <= max_iter: optimizer.step(partial(step,final_loss))

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lbfgs.py in step(self, closure)
    101 
    102         # evaluate initial f(x) and df/dx
--> 103         orig_loss = closure()
    104         loss = float(orig_loss)
    105         current_evals = 1

<ipython-input-190-872e726eba91> in step(loss_fn)
      2     global n_iter
      3     optimizer.zero_grad()
----> 4     loss = loss_fn(opt_img_v)
      5     loss.backward()
      6     n_iter += 1

<ipython-input-186-01d4a9cbf1c9> in final_loss(opt_img_v)
      4     c_loss = content_loss(out_ftrs[-1])
      5     s_loss = style_loss(out_ftrs)
----> 6     h_loss = hist_loss([out_ftrs[0], out_ftrs[3]])
      7     t_loss = tv_loss(opt_img_v[0])
      8     return c_loss + w_s * s_loss + w_h * h_loss + w_tv * t_loss

<ipython-input-181-d0c2943ea4a4> in hist_loss(out_ftrs)
      6         mask = V(torch.Tensor(mf).contiguous().view(1, -1), requires_grad=False)
      7         of_masked = of * mask
----> 8         of_masked = torch.cat([of_masked[i][mask>=0.1].unsqueeze(0) for i in range(of_masked.size(0))])
      9         loss += F.mse_loss(of_masked, V(remap_hist(of_masked, sh), requires_grad=False))
     10     return loss / 2

<ipython-input-181-d0c2943ea4a4> in <listcomp>(.0)
      6         mask = V(torch.Tensor(mf).contiguous().view(1, -1), requires_grad=False)
      7         of_masked = of * mask
----> 8         of_masked = torch.cat([of_masked[i][mask>=0.1].unsqueeze(0) for i in range(of_masked.size(0))])
      9         loss += F.mse_loss(of_masked, V(remap_hist(of_masked, sh), requires_grad=False))
     10     return loss / 2

IndexError: too many indices for tensor of dimension 1
Hey, 

I was implementing 1 cycle policy as an exercise. And I have a few observations from my experiments.
I have a 
__Model__ : Resnet18.
__Batch size for training__ = 128
__Batch size for testing__ = 100

Optimser :  ```optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)```
Total number of epochs 26

__1 cycle policy__ : Learning rate goes from __0.01 to 0.1__ and back till __24__ epochs

Then model is trained for __2 epochs at 0.001 learning rate__.

No cyclic momentum used or adamw. 

I achieved a test set accuracy of __93.4%in 26__ epochs.

_This seems like a big difference from the 70 epochs at 512 batch size that is quoted in your blog post._

Am I doing something wrong ? _Is the number of epochs a good metric to base your results on, as those are dependant on the batch size_ ? .

The whole point of using super convergence is using high learning rates to converge quicker , but it seems like using low learning rates (0.01- 0.1 < 0.8-3) is __faster__ to train.  

