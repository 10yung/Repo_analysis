Created a scala program and trying to run the streaming code. it is filing with below error

19/11/09 07:25:24 ERROR StreamExecution: Query customer_purchases_2 [id = 6863c8d1-fd1c-49eb-a454-92825f0d2782, runId = 8adbd1e6-04e1-4b79-9fad-05950f359e77] terminated with error
java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
spark_guide.chapter3.StructuredStreaming$.delayedEndpoint$spark_guide$chapter3$StructuredStreaming$1(StructuredStreaming.scala:12)
spark_guide.chapter3.StructuredStreaming$delayedInit$body.apply(StructuredStreaming.scala:9)
scala.Function0$class.apply$mcV$sp(Function0.scala:34)
scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)
scala.App$$anonfun$main$1.apply(App.scala:76)
scala.App$$anonfun$main$1.apply(App.scala:76)
scala.collection.immutable.List.foreach(List.scala:392)
scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)
scala.App$class.main(App.scala:76)
spark_guide.chapter3.StructuredStreaming$.main(StructuredStreaming.scala:9)
spark_guide.chapter3.StructuredStreaming.main(StructuredStreaming.scala)
…alytics_and_Machine_Learning.py

fix print() and add some missing code lines
Value format is not a member of org.apache.spark.sql.DataFrame
I'm studying spark advanced RDD API and got a little bit confused by one example.
`// in Scala
import org.apache.spark.Partitioner

class DomainPartitioner extends Partitioner {
def numPartitions = 3
def getPartition(key: Any): Int = {
val customerId = key.asInstanceOf[Double].toInt
if (customerId == 17850.0 || customerId == 12583.0) {
return 0
} else {
return new java.util.Random().nextInt(2) + 1
}
}
}`
As far as I can see in code documentation, partitioner must return the same partition id given the same partition key. That is not true for the example in the code above. Isn't "random" id for key break the Partitioner interface ? 
 
https://github.com/databricks/Spark-The-Definitive-Guide/blob/38e881406cd424991a624dddb7e68718747b626b/code/Structured_APIs-Chapter_6_Working_with_Different_Types_of_Data.py#L313

It seems SCALA version, same as #L319

https://github.com/databricks/Spark-The-Definitive-Guide/blob/38e881406cd424991a624dddb7e68718747b626b/code/Structured_APIs-Chapter_7_Aggregations.scala#L171

First, the column `Quantity` is parsed as `String`. It should be  `IntegerType`.

Second `orderBy(CustomerId)` should be `orderBy(desc(Quantity))`
Hi,
I'm really stuck with this section of Spark book.
staticDataFrame = spark.read.format("csv")\
.option("header", "true")\
.option("inferSchema", "true")\
**.load("/mnt/defg/retail-data/by-day/*.csv")**

1) I'm not able to understand the "load("/mnt/...") section. 
I have downloaded the data to my local drive. But now the issue is on loading the data. How to load the data ?

2) Is the mnt/defg being done via S3 ? or by any other method !




교재 정보, 자료
Hi,

I start learning **Apache Spark** by reading that book. I'm now at ``chapter 3 - Streaming part``.
For snippet code, I choose ``python3``
My problem is that nothing is displayed in console, as said in the book, from that code
```python
purchaseByCustomerPerHour.writeStream \
    .format("console") \
    .queryName("customer_purchases_3") \
    .outputMode("complete") \
    .start() \
    .show() 
```

I don't know if I'm doing something wrong but tell me how to display result in console at start and at every update too
Hello,

I tried to follow the Transfer learning example in pyspark on page 534-535. However, when I try to do p_model = p.fit(train_df) I get a 'Number of source Raster bands and source color space components do not match'. I tried googling what to do, but i wasn't sure what to do? The trace includes references to java.awt.image.ColorConvertOp.filter, com.sun.imageio.plugins.jpeg.JPEGImageReader.... and javax.imageio.ImageIO.read. Note that I used Spark ml's ImageSchema read function because the sparkdl readImages function seems to be deprecated.

Thanks in advance,
Nick