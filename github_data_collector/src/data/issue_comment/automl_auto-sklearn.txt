```
 running build_ext
    building 'pyrfr._regression' extension
    swigging pyrfr/regression.i to pyrfr/regression_wrap.cpp
    swig -python -c++ -modern -features nondynamic -I./include -o pyrfr/regression_wrap.cpp pyrfr/regression.i
    unable to execute 'swig': No such file or directory
    error: command 'swig' failed with exit status 1
 
```   



I've tried to install and reinstall swig 3.0 - nothing. Please help!
ENV:
linux mint 19.2
gcc 7.4
swig 3
python 3.7
Remove warning "No models better than random - using Dummy Score!" since this error message seems to be very confusing. fix #739 
**I'm confused with below outputs where trainin score is around 71% and testing score is around69%. But Dont see 71% or 69% is the show_models() output.** 
![image](https://user-images.githubusercontent.com/35065303/72041552-e339ef80-32d1-11ea-9b28-38e92d71f594.png)

**what is the first value in  automl.show_models() out put. What is this 0.26000 value ? it is accuracy of the model?**
[(0.260000, SimpleRegressionPipeline({'categorical_encoding:__choice__': 'one_hot_encoding', 'imputation:strategy': 'median', 'preprocessor:__choice__': 'polynomial', 'regressor:__choice__': 'liblinear_svr', 'rescaling:__choice__': 'none', 'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'False', 'preprocessor:polynomial:degree': 2, 'preprocessor:polynomial:include_bias': 'True', 'preprocessor:polynomial:interaction_only': 'False', 'regressor:liblinear_svr:C': 103.11030493878376, 'regressor:liblinear_svr:dual': 'False', 'regressor:liblinear_svr:epsilon': 0.3011904329496452, 'regressor:liblinear_svr:fit_intercept': 'True', 'regressor:liblinear_svr:intercept_scaling': 1, 'regressor:liblinear_svr:loss': 'squared_epsilon_insensitive', 'regressor:liblinear_svr:tol': 1.0974773655165751e-05},
dataset_properties={
  'task': 4,
  'sparse': False,
  'multilabel': False,
  'multiclass': False,
  'target_type': 'regression',
  'signed': False})),

i want to know whether autosklearn support  multilabel task, thank you!
Good evening; 

I have used AutoSklearn for a binary classification of tabular data and got 0.99 as best validation score. 
When I read cv_results, there are numerous pipelines that indeed have given such a score. 
However, when I explicitely implement one of them with scikit-learn (say passive_aggressive with the same parameters as in CV_results) and train it, I get only 0.5 ! 

Why ? Is there something I do no get with those 'Best validation score' and 'Mean score' ?

It is very important. There is almost 3 monthes of work at stake. 

Thank you. 
Hi,
Thanks for developing and helping maintain this package.

I am developing a text classifier and will like to ´translate´ the best ensemble model to sklearn model to allow for further processing, such as to run LIME TextExplainer to explore the important feature. I am unable to do so with the auto-sklearn:

Here is my model:
```
# load Pkgs
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
from sklearn.metrics import accuracy_score 
from sklearn.base import TransformerMixin 
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.svm import SVC
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

from autosklearn.classification import AutoSklearnClassifier
from autosklearn.metrics import make_scorer
from autosklearn import classification, metrics

from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from imblearn.over_sampling import RandomOverSampler
from imblearn.metrics import classification_report_imbalanced

from mlxtend.plotting import plot_confusion_matrix

import matplotlib.pyplot as plt

import autosklearn.metrics
import sklearn.metrics
df = pd.read_csv('df.csv')
df.head()



X = df['sentence'] 
ylabels = df['label']

X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.2, random_state=42)

# Using Tfidf
tdf = TfidfVectorizer(
    stop_words = 'english', 
    ngram_range=(1,2)

X_train_vec = tdf.fit_transform(X_train)
X_test_vec = tdf.transform(X_test)

ros  = RandomOverSampler(random_state=42)
X_train_res, y_train_res = ros.fit_resample(X_train_vec, y_train)

recall_scorer = make_scorer('recall', metrics.recall, needs_proba=True, optimum = 1)
automl = AutoSklearnClassifier(
    delete_tmp_folder_after_terminate=False,
    resampling_strategy='cv',
    resampling_strategy_arguments={'folds': 5},
    seed=31,
    )

automl.fit(
    X_train_res.copy(), 
    y_train_res.copy(), 
    metric = recall_scorer,
    dataset_name='cvd_nma', )

automl.refit(X_train_res.copy(), y_train_res.copy())

print(automl.show_models())
``` 

And the best model is:
```
[(0.380000, SimpleClassificationPipeline({'balancing:strategy': 'none', 'categorical_encoding:__choice__': 'one_hot_encoding', 'classifier:__choice__': 'random_forest', 'imputation:strategy': 'mean', 'preprocessor:__choice__': 'no_preprocessing', 'rescaling:__choice__': 'standardize', 'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True', 'classifier:random_forest:bootstrap': 'True', 'classifier:random_forest:criterion': 'gini', 'classifier:random_forest:max_depth': 'None', 'classifier:random_forest:max_features': 0.5, 'classifier:random_forest:max_leaf_nodes': 'None', 'classifier:random_forest:min_impurity_decrease': 0.0, 'classifier:random_forest:min_samples_leaf': 1, 'classifier:random_forest:min_samples_split': 2, 'classifier:random_forest:min_weight_fraction_leaf': 0.0, 'classifier:random_forest:n_estimators': 100, 'categorical_encoding:one_hot_encoding:minimum_fraction': 0.01},
dataset_properties={
  'task': 1,
  'sparse': True,
  'multilabel': False,
  'multiclass': False,
  'target_type': 'classification',
  'signed': False})),
(0.320000, SimpleClassificationPipeline({'balancing:strategy': 'none', 'categorical_encoding:__choice__': 'one_hot_encoding', 'classifier:__choice__': 'random_forest', 'imputation:strategy': 'median', 'preprocessor:__choice__': 'no_preprocessing', 'rescaling:__choice__': 'standardize', 'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True', 'classifier:random_forest:bootstrap': 'False', 'classifier:random_forest:criterion': 'entropy', 'classifier:random_forest:max_depth': 'None', 'classifier:random_forest:max_features': 0.4207539425382141, 'classifier:random_forest:max_leaf_nodes': 'None', 'classifier:random_forest:min_impurity_decrease': 0.0, 'classifier:random_forest:min_samples_leaf': 9, 'classifier:random_forest:min_samples_split': 5, 'classifier:random_forest:min_weight_fraction_leaf': 0.0, 'classifier:random_forest:n_estimators': 100, 'categorical_encoding:one_hot_encoding:minimum_fraction': 0.008543536861725461},
dataset_properties={
  'task': 1,
  'sparse': True,
  'multilabel': False,
  'multiclass': False,
  'target_type': 'classification',
  'signed': False})),
(0.300000, SimpleClassificationPipeline({'balancing:strategy': 'none', 'categorical_encoding:__choice__': 'no_encoding', 'classifier:__choice__': 'passive_aggressive', 'imputation:strategy': 'median', 'preprocessor:__choice__': 'liblinear_svc_preprocessor', 'rescaling:__choice__': 'normalize', 'classifier:passive_aggressive:C': 0.02604047653930501, 'classifier:passive_aggressive:average': 'True', 'classifier:passive_aggressive:fit_intercept': 'True', 'classifier:passive_aggressive:loss': 'squared_hinge', 'classifier:passive_aggressive:tol': 0.003862604895731395, 'preprocessor:liblinear_svc_preprocessor:C': 12510.70278007016, 'preprocessor:liblinear_svc_preprocessor:dual': 'False', 'preprocessor:liblinear_svc_preprocessor:fit_intercept': 'True', 'preprocessor:liblinear_svc_preprocessor:intercept_scaling': 1, 'preprocessor:liblinear_svc_preprocessor:loss': 'squared_hinge', 'preprocessor:liblinear_svc_preprocessor:multi_class': 'ovr', 'preprocessor:liblinear_svc_preprocessor:penalty': 'l1', 'preprocessor:liblinear_svc_preprocessor:tol': 0.08749303356614886},
dataset_properties={
  'task': 1,
  'sparse': True,
  'multilabel': False,
  'multiclass': False,
  'target_type': 'classification',
  'signed': False})),
]

``` 

Thanks a lot 
See here https://scikit-learn.org/dev/whats_new/v0.21.html#multiple-modules
Dear all,

When working with autosklearn I run into the following problem:
- When running autosklearn.fit, I want to pickle.load all models, which have automatically been pickled by autosklearn ( ....model) and use them to predict on a given dataset.
- for this I have an observer who observes the directory into which autosklearn automatically stores pickled models, as soon as a new model is pickled, it pickle.loads this model and runs model.predict(X).
- But: this then throws an error (see below) and crashed
- if I, after I pickle.load the model run model.fit(X_train, y_train), the error no longer appears
- this makes me think that for some reason the model is not fitted before being pickled (which is actually not the case when reading autosklearn documentation)

Has anybody encountered this problem yet? If so, what is the reason and is there a workaround?

(I use autosklearn version 0.5.2 and tried it  with scikit learn 0.19.2 and 0.21.3)

> type of model:  <class 'autosklearn.pipeline.regression.SimpleRegressionPipeline'>
Exception in thread Thread-1:
Traceback (most recent call last): ...
  File " .../lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File " .../lib/python3.7/site-packages/watchdog/observers/api.py", line 199, in run
    self.dispatch_events(self.event_queue, self.timeout)
  File " .../lib/python3.7/site-packages/watchdog/observers/api.py", line 368, in dispatch_events
    handler.dispatch(event)
  File " .../lib/python3.7/site-packages/watchdog/events.py", line 330, in dispatch
    _method_map[event_type](event)
  File " .../_helper.py", line 102, in on_moved
    self._cback(event.dest_path) # note that this may result in callbacks with a file outside of models_dir, if it was moved somewhere else
  File  .../_helper.py", line 93, in _cback
    else: self.callback(path, self.ref)
  File " .../auto_sklearn.py", line 272, in _directory_changed
    pickle_model.predict(X_train)
  File " .../lib/python3.7/site-packages/autosklearn/pipeline/regression.py", line 96, in predict
    y = super().predict(X, batch_size=batch_size)
  File " .../lib/python3.7/site-packages/autosklearn/pipeline/base.py", line 141, in predict
    return super().predict(X).astype(self._output_dtype)
  File " .../lib/python3.7/site-packages/sklearn/utils/metaestimators.py", line 115, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
  File " .../lib/python3.7/site-packages/sklearn/pipeline.py", line 306, in predict
    Xt = transform.transform(Xt)
  File " .../lib/python3.7/site-packages/autosklearn/pipeline/components/data_preprocessing/one_hot_encoding/__init__.py", line 97, in transform
    return self.choice.transform(X)
  File " .../lib/python3.7/site-packages/autosklearn/pipeline/components/data_preprocessing/one_hot_encoding/one_hot_encoding.py", line 60, in transform
    if self.preprocessor is None:
AttributeError: 'OneHotEncoder' object has no attribute 'preprocessor'

> 
I'm working on a project that needs me to pass a mask of label into the scorer. For example, if y is a vector (100,), then I will have a boolean mask with the same size.

After passing this mask, I found that the resampling strategy used hold-out set then this mask won't be able to match with the original vector size. I'm just wondering how can I fix this? Also, if someone can point out which part of code does the splitting, it would be definitely helpful. 

Thanks!
PR #735 implements data preprocessing of categorical and numerical features as two parallel pipelines, each with its own independent hyperparameters.
Ideally, if the dataset is made just of features of the same kind, then just the hyperparameters of the corresponding pipeline should be considered for optimization.
Nevertheless, this is not the current behavior. At the moment, the hyperparameters of both pipelines remain active regardless of the dataset.