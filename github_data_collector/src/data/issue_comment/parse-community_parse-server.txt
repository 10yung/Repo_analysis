In the Dockerfile, there is a line which defines two volumes:
https://github.com/parse-community/parse-server/blob/3ed6535b233cf20c6cf853df88796f4b7f6edb99/Dockerfile#L18

Without specifying a volume in the run command, these volumes aren't easily accessible to the end user. I believe `-v` options need to be added in the sample run command:

https://github.com/parse-community/parse-server/blob/3ed6535b233cf20c6cf853df88796f4b7f6edb99/README.md#L102
Tracking issue #6351
Previously requested with #4251, #4045

```js
// PUT http://localhost:1337/schemas/:className
{
  "classLevelPermissions":
  {
    "get"/"find"/"count"/"update"/"delete"/"addField": { 
      "*": true, // Public access (set by default) 
      [objectId]: true, // by id of `_User`
      "role:admin": true, // by role (`role:${role_name}`)
      "requiresAuthentication": true, // only authenticated users
      "pointerFields": ["field_name", ...] // fields of type Pointer<_User> or Pointer<_User>[]
    },
    "readUserFields":["field_name",...], // same as 'pointerFields' for get+find+count
    "writeUserFields":["field_name",...] // same as 'pointerFields' for update+delete+addField
    ...
  }
}
```
Pointer permissions for:
- `create` has no effect
- `addField` accounted only on updating object, ignored when creating

### When set for an operation:
```js
{
  "classLevelPermissions":{
    [operation]: {
      pointerFields: [‘field’]
      // default * Public removed
      // no other rules set
    }
  }
}
```

|Operation | user not pointed by field | user is pointed by field |
| - | - | - | 
|get |	101: Object not found	| ok |
|find	| Limited results |	ok |
|count |	Limited count |	ok |
|create |	Permission denied	| Permission denied |
|update |	101: Object not found |	ok |
|delete	| 101: Object not found	| ok |
|addField|	Permission denied | ok |

**Is your feature request related to a problem? Please describe.**
As of now it's impossible to set fine-grained pointer permissions in CLP.  

Was previously requested:  #4045 and #4251.

I'll track progress here.

This involves:
- [ ] server implementation #6352 
- [ ] dashboard update
- [ ] docs update  parse-community/docs#700



Previously i was using PFFacebookUtils to login/signup in app using user's Facebook but somehow PFFacebookUtils stopped working in my app due to clash with other libraries. So i moved to actual Facebook login. 

Now all working fine here when user signed up/login using facebook SDK and then after searching in Parse server am logging the user in using username and password.

Now the issue is how to login the previously added user which was logged in using PFFacebookUtils through simple parse method "logInWithUsername and password". I can't find user's password in parse "User" class.

Any help would be much appreciated.
Thanks

## The dependency [mongodb](https://github.com/mongodb/node-mongodb-native) was updated from `3.4.1` to `3.5.0`.
This version is **not covered** by your **current version range**.

If you don’t accept this pull request, your project will work just like it did before. However, you might be missing out on a bunch of new features, fixes and/or performance improvements from the dependency update.

---

**Publisher:** [mbroadst](https://www.npmjs.com/~mbroadst)
**License:** Apache-2.0

<details>
<summary>Release Notes for v3.5.0</summary>

<p>The MongoDB Node.js team is pleased to announce version 3.5.0 of the driver</p>
<h2>Release Highlights</h2>
<h3>CMAP-compliant Connection Pool</h3>
<p>This release introduces a modern replacement for the driver's connection pool, available only with the<br>
unified topology. A major effort was made in early 2019 to fully specifiy connection pools for MongoDB<br>
drivers (see: <a href="https://urls.greenkeeper.io/mongodb/specifications/blob/master/source/connection-monitoring-and-pooling/connection-monitoring-and-pooling.rst">CMAP specification</a>), and this release brings the Node.js driver in line with that<br>
specification.</p>
<h4>Traceability</h4>
<p>The new pool supports monitoring for all aspects of its behavior. This allows deep introspection into<br>
the operation of the connection pool, as well as an ability to profile the lifetime of an operation<br>
when used in conjunction with command monitoring.</p>
<h4>Stream-first Connection Design</h4>
<p>The <code>Connection</code> class was completely rewritten for the new pool adopting a stream-first mentality. All<br>
wire message processing and compression is handled in a duplex stream called the <code>MessageStream</code>, and<br>
that stream is connected bidirectionally to the underlaying TCP socket. The result is a connection which<br>
gains the general benefit of streams: better performance, less memory pressure, backpressure support. It<br>
also opens the possiblity of supporting non-TCP/UDP streams as a transport for the driver.</p>
<h4>waitQueueTimeoutMS</h4>
<p>The new connection pool has a concept of a "wait queue", which allows operation requests to buffer waiting<br>
for a connection to execute against. There is no timeout by default, but users can now specify a new value<br>
<code>waitQueueTimeoutMS</code> in their connection string or <code>MongoClient</code> options to proactively cancel operations<br>
that have waited too long.</p>
<p>Remember that the new connection pool is only available for the "Unified Topology", so remember to pass<br>
<code>useUnifiedTopology: true</code> to your <code>MongoClient</code> constructor to use it!</p>
<h3>Dedicated monitoring thread</h3>
<p>Both the legacy and unified SDAM implementations have until now executed monitoring checks as priority<br>
messages in the legacy Pool implementation. This means that monitoring (<code>ismaster</code>) operations were<br>
prioritized over other queued operations, but also means that monitoring could be indefinitely blocked,<br>
in particular during failover or blackhole scenarios. The default socket timeout is <code>null</code> (read: Infinity),<br>
so if the pool was completely saturated with operations, there may be no ability to execute a monitoring<br>
check and determine that the connection to a server was no longer valid. This version of the driver<br>
introduces a new <code>Monitor</code> class which manages its own dedicated monitoring socket to each known<br>
node.</p>
<h3>Server selection errors</h3>
<p>In v3.3.0 of the driver we introduced a new <code>MongoTimeoutError</code> for all errors covered by the server<br>
selection loop, leading to a spike in bug reports with a title similar to <code>Server selection timed out after 30000ms</code>.<br>
Even though the error type itself had an attached <code>reason</code> field, we still feel it was easy to miss why<br>
the selection had failed. As a result we have introduced a new type <code>MongoServerSelectionError</code> which<br>
will use the originating error (<code>reason</code>) for its message, better informing users what caused a<br>
selection error, while still also conveying it is an error in server selection.</p>
<h2>Release Notes</h2>
<h2>        New Feature
</h2>
<ul>
<li>[<a href="https://jira.mongodb.org/browse/NODE-1742" rel="nofollow">NODE-1742</a>] -         Implement Connection Monitoring and Pooling spec
</li>
<li>[<a href="https://jira.mongodb.org/browse/NODE-2386" rel="nofollow">NODE-2386</a>] -         Use a dedicated monitoring thread
</li>
</ul>
<h2>        Bug
</h2>
<ul>
<li>[<a href="https://jira.mongodb.org/browse/NODE-2400" rel="nofollow">NODE-2400</a>] -         Synchronous errors are swallowed by executeOperation
</li>
<li>[<a href="https://jira.mongodb.org/browse/NODE-2417" rel="nofollow">NODE-2417</a>] -         Server descriptions with me mismatch from primary response should be removed
</li>
<li>[<a href="https://jira.mongodb.org/browse/NODE-2418" rel="nofollow">NODE-2418</a>] -         client platform not sent in metadata for CMAP connections
</li>
</ul>
<h2>        Improvement
</h2>
<ul>
<li>[<a href="https://jira.mongodb.org/browse/NODE-1619" rel="nofollow">NODE-1619</a>] -         Remove wasteful empty Buffer allocations in `Connection`
</li>
<li>[<a href="https://jira.mongodb.org/browse/NODE-2049" rel="nofollow">NODE-2049</a>] -         Add "connectionError" as a valid "reason" for a ConnectionCheckOutFailedEvent when connection set up fails
</li>
<li>[<a href="https://jira.mongodb.org/browse/NODE-2397" rel="nofollow">NODE-2397</a>] -         Make server selection errors more informative
</li>
<li>[<a href="https://jira.mongodb.org/browse/NODE-2402" rel="nofollow">NODE-2402</a>] -         Integrate CMAP connection pool into unified topology
</li>
<li>[<a href="https://jira.mongodb.org/browse/NODE-2419" rel="nofollow">NODE-2419</a>] -         Improve traceability of CMAP events
</li>
<li>[<a href="https://jira.mongodb.org/browse/NODE-2033" rel="nofollow">NODE-2033</a>] -         Ignore ConnectionReadyEvent in CMAP pool creation test
</li>
</ul>
</details>

<details>
<summary>Commits</summary>
<p>The new version differs by 131 commits.</p>
<ul>
<li><a href="https://urls.greenkeeper.io/mongodb/node-mongodb-native/commit/899128580b5a1a42019c7fc103489c72b87ec279"><code>8991285</code></a> <code>chore(release): 3.5.0</code></li>
<li><a href="https://urls.greenkeeper.io/mongodb/node-mongodb-native/commit/ea56625e4ddb4ea30ec80410bfee8703883c9422"><code>ea56625</code></a> <code>chore: <code>waitQueueTimeoutMS</code> is a valid connection string option</code></li>
<li><a href="https://urls.greenkeeper.io/mongodb/node-mongodb-native/commit/a630389bef2cc2a360dacd84d67cf514ed321fdd"><code>a630389</code></a> <code>refactor: wait until server destroyed before stopping event relay</code></li>
<li><a href="https://urls.greenkeeper.io/mongodb/node-mongodb-native/commit/c04a52050ae6959bc4eec8d82168f9870168146b"><code>c04a520</code></a> <code>doc: add basic documentation for CMAP event monitoring</code></li>
<li><a href="https://urls.greenkeeper.io/mongodb/node-mongodb-native/commit/c01bf5065bc4f31fbf6c0b93d408e4a51af701af"><code>c01bf50</code></a> <code>refactor: don't encode type name into public CMAP event types</code></li>
<li><a href="https://urls.greenkeeper.io/mongodb/node-mongodb-native/commit/1aea4debda51b0ca62270a921b4b85935780e4e6"><code>1aea4de</code></a> <code>feat: relay all CMAP events to MongoClient</code></li>
<li><a href="https://urls.greenkeeper.io/mongodb/node-mongodb-native/commit/ed8c9d41e28eb29f6f70b05412874dd160b8f738"><code>ed8c9d4</code></a> <code>refactor: warn on use of deprecated SDAM events in unified mode</code></li>
<li><a href="https://urls.greenkeeper.io/mongodb/node-mongodb-native/commit/7e64df72f631b981e5172e238a9f5c1cf1d64be7"><code>7e64df7</code></a> <code>test: reduce flakiness of objectid test which checks by time</code></li>
<li><a href="https://urls.greenkeeper.io/mongodb/node-mongodb-native/commit/0715a36ae3e6f371d06e2b78ad4f8cf6cf740bcd"><code>0715a36</code></a> <code>doc: add documentation for CMAP events and errors</code></li>
<li><a href="https://urls.greenkeeper.io/mongodb/node-mongodb-native/commit/9bd360c63cba8fbe23fa8ec2882d65caadb7ce93"><code>9bd360c</code></a> <code>feat: include <code>connectionId</code> for APM with new CMAP connection pool</code></li>
<li><a href="https://urls.greenkeeper.io/mongodb/node-mongodb-native/commit/95414108f134968f7783ee75e9cf7052486c761a"><code>9541410</code></a> <code>test: ignore ismaster events in change streams spec tests</code></li>
<li><a href="https://urls.greenkeeper.io/mongodb/node-mongodb-native/commit/ec3d87b57db242ed88997ec48c0b41eb4cc1201c"><code>ec3d87b</code></a> <code>test: allow all test files to use custom chai mongodb spec matcher</code></li>
<li><a href="https://urls.greenkeeper.io/mongodb/node-mongodb-native/commit/35d02747e5daca6bbeb0fbec7b0192b77ccc5ac9"><code>35d0274</code></a> <code>fix: report the correct platform in client metadata</code></li>
<li><a href="https://urls.greenkeeper.io/mongodb/node-mongodb-native/commit/c528a6695e2c23e99e8692102717c3e6880924a9"><code>c528a66</code></a> <code>Revert "fix: remove servers with me mismatch in <code>updateRsFromPrimary</code>"</code></li>
<li><a href="https://urls.greenkeeper.io/mongodb/node-mongodb-native/commit/903208f473e8416183f95f9a54b6416ee6e65706"><code>903208f</code></a> <code>Revert "test: include auth information in generated test connection string"</code></li>
</ul>
<p>There are 131 commits in total.</p>
<p>See the <a href="https://urls.greenkeeper.io/mongodb/node-mongodb-native/compare/bc93598a6ee2ea24bb1aa08e4293dc763c478ea8...899128580b5a1a42019c7fc103489c72b87ec279">full diff</a></p>
</details>

---

<details>
  <summary>FAQ and help</summary>

  There is a collection of [frequently asked questions](https://greenkeeper.io/faq.html). If those don’t help, you can always [ask the humans behind Greenkeeper](https://github.com/greenkeeperio/greenkeeper/issues/new).
</details>

---


Your [Greenkeeper](https://greenkeeper.io) bot :palm_tree:


<!--- 

**We use GitHub Issues for reporting bugs with parse-server.**

If you have a *question*, you should join the [Parse Community's Discourse forum](https://community.parseplatform.org/c/parse-server).

If you have a vulnerability disclosure, please follow our policy available here https://github.com/parse-community/parse-server/blob/master/SECURITY.md

You may also search through existing issues before opening a new one: https://github.com/parse-community/parse-server/issues?utf8=%E2%9C%93&q=is%3Aissue 

--- Please use this template. If you don't use this template, your issue may be closed without comment. ---
--->
### Issue Description

<!--- Describe your issue in as much detail as possible. -->
The ParseObject returned by LiveQuery events gives Date in String format, instead of Date format. Problem seem to specifically pertain to "Update" events. Strangely, some Date fields are returned in Date format while some are returned in String format.

I have tried several Parse-Server versions and with the exception of version 3.1.1, all other versions 3.7.2, 3.8.0, and 3.9.0 seem to have this problem.

P.S. I am aware that special fields like updatedAt and createdAt return String values. I was referring to custom Date fields created by me.

Note: I am using Back4App, which is a backend-as-a-service built on Parse.

### Steps to reproduce
Codes in Android client
```
ParseQuery<ParseObject> liveQuery = ParseQuery.getQuery("QueueGP");
liveQuery.whereEqualTo("clinic", clinicId);

SubscriptionHandling<ParseObject> subscription = parseLiveQueryClient.subscribe(liveQuery);
subscription.handleEvents(new SubscriptionHandling.HandleEventsCallback<ParseObject>() {
    @Override
    public void onEvents(ParseQuery<ParseObject> query, SubscriptionHandling.Event event, ParseObject parseObject) {
        if(event == SubscriptionHandling.Event.UPDATE){
            Date registeredAt = parseObject.getDate("registeredAt");
        } ...
    }
});
```

<!--- Please include a detailed list of steps that reproduce the issue. Include curl commands when applicable.  --->
 
### Expected Results

<!--- What you expected to happen. --->
onEvent from ParseLiveQueryClient (seen in verbose in my logs)
> ... Socket onMessage {"op":"update", ... "registeredAt":{"__type":"**Date**","**iso**":"2020-01-15T12:19:07.175Z"}}

Expected to see "Date" and "iso", and `parseObject.getDate("registeredAt");` should contain date value

### Actual Outcome

<!--- What is happening instead. --->
No "Date" or "iso" seen. 
> "registeredAt":"2020-01-15T12:19:19.759Z"

In actual, `parseObject.getDate("registeredAt")` gives null value, while `parseObject.get("registeredAt");` gives value of String type

### Environment Setup

- **Server**
  - parse-server version (Be specific! Don't say 'latest'.) : [3.72 / 3.8.0/ 3.9.0]
  - Operating System:     [Handled by back4app]
  - Hardware:             [Handled by back4app]
  - Localhost or remote server? (AWS, Heroku, Azure, Digital Ocean, etc): [Handled by back4app]

- **Database**
  - MongoDB version: [Handled by back4app]
  - Storage engine:  [Handled by back4app]
  - Hardware:        [Handled by back4app]
  - Localhost or remote server? (AWS, mLab, ObjectRocket, Digital Ocean, etc): [Handled by back4app]

### Logs/Trace

<!--- Include all relevant logs. You can turn on additional logging by configuring VERBOSE=1 in your environment. --->


Sourcefile: https://github.com/parse-community/parse-server/blob/master/src/Adapters/Storage/Mongo/MongoStorageAdapter.js

Function name: _parseAggregateArgs()

The purpose of the function is to  

> recursively traverse the pipeline and convert any Pointer or Date columns

But it can only recognize existing fields defined in the collection schema. If we create a new Data field in project stage, it can not recognize it and the query won't work.

Examples: any match on startTime will work, but not on endTime, which is generated in project stage.

`var pipeline = [`
`    {match: {_p_user: "_User$"+userId},},`
`    {`
`      project: {`
`                _p_user: 1,`
`                startTime: 1,`
`                "endTime": {$add: ["$startTime", {$multiply: ["$duration",60000]}]}`
`                }`
`    },`
`    // startTime can be recognized as Date() type, thus the query will work`
`    {match: {"startTime": {$gt: new Date()}}},`
`    // endTime can not be recognized as Date type, any query on endTime will fail`
`    //  {match: {"endTime": {$gt: new Date()}}},  `
`    {limit: 10},`
`  ];`
**Is your feature request related to a problem? Please describe.**
Currently when saving files, the only thing you can set is the data itself and the file name. The ability to add additional [meta-data](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-object-metadata.html) and [tags](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-object-metadata.html) (specifically with S3) would be a great feature add

**Describe the solution you'd like**
Being able to pass in a "metaData" and/or "tags" key to the options object when saving a file. These two values would be sent to S3 along with the file data. Also, if support for beforeSaveFile and afterSaveFile are implemented (#6340), these metaData and tag values could be set there too. This would make it possible for things like deleting all files a user created once their account is deleted, etc.

**Describe alternatives you've considered**
None

**Additional context**
I have no problem adding this along with #6340 if the community will accept the changes.

**Is your feature request related to a problem? Please describe.**
Currently there is zero authentication or customization of files via hooks.

**Describe the solution you'd like**
Just like we have beforeSave, afterSave, etc. for Objects. Having the ability for beforeSaveFile, afterSaveFile, etc. would allow for different authentication, validations, file name changes, meta data, etc. when saving files. This would allow the ability to implement a file tracking class that would then allow for easily deleting files from file storage when they're no longer needed.

**Describe alternatives you've considered**
building my own middleware in front of Parse

**Additional context**
I have no problem building this!
