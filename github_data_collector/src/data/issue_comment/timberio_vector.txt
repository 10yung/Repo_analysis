```
root@ubuntu-test:~# vector --version
vector 0.7.0-nightly (g41be43d x86_64-unknown-linux-musl 2020-01-18)
root@ubuntu-test:~# lsb_release -a
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu Focal Fossa (development branch)
Release:	20.04
Codename:	focal
root@ubuntu-test:~# vector --version
vector 0.7.0-nightly (g41be43d x86_64-unknown-linux-musl 2020-01-18)
root@ubuntu-test:~# vector test 1.toml 
Running 1.toml tests
root@ubuntu-test:~# cat 1.toml 
data_dir = "/tmp/vector"

[sources.ss11]
  # REQUIRED
  type = "journald" # example, must be: "journald"
  # units = []
  # current_boot_only = true # default

[sinks.out]
    inputs = ["ss11"]
    type = "console"
    encoding = "json"
root@ubuntu-test:~# vector --config 1.toml 
Jan 19 04:17:15.137  INFO vector: Log level "info" is enabled.
Jan 19 04:17:15.137  INFO vector: Loading config. path="1.toml"
Jan 19 04:17:15.139  INFO vector: Vector is starting. version="0.7.0" git_version="v0.6.0-155-g41be43d" released="Sat, 18 Jan 2020 11:24:13 +0000" arch="x86_64"
Jan 19 04:17:15.140  INFO vector::topology: Running healthchecks.
Jan 19 04:17:15.140  INFO vector::topology: Starting source "ss11"
Jan 19 04:17:15.140  INFO vector::topology: Starting sink "out"
Jan 19 04:17:15.140  INFO vector::topology::builder: Healthcheck: Passed.
Jan 19 04:17:15.140  INFO source{name=ss11 type=journald}: vector::sources::journald: Starting journald server.
Failed to parse timestamp: 1970-01-01
^CJan 19 04:17:16.682  INFO vector: Shutting down.
root@ubuntu-test:~# journalctl
-- Logs begin at Tue 2020-01-14 03:18:38 +0330, end at Sun 2020-01-19 04:17:01 +0330. --
Jan 14 03:18:38 ubuntu-test kernel: Linux version 5.4.0-9-generic (buildd@lcy01-amd64-023) (gcc version 9.2.1 20191130 (Ubuntu 9.2.1-21ubuntu1)) #12-Ubuntu SMP Mon Dec 16 22:34:19 UTC 2019 (Ubuntu 5.4.0-9.12-ge>
Jan 14 03:18:38 ubuntu-test kernel: Command line: BOOT_IMAGE=/vmlinuz-5.4.0-9-generic root=UUID=1072eb2b-fdbc-4049-8c24-10c535d75bc1 ro splash quiet
Jan 14 03:18:38 ubuntu-test kernel: KERNEL supported cpus:
Jan 14 03:18:38 ubuntu-test kernel:   Intel GenuineIntel
Jan 14 03:18:38 ubuntu-test kernel:   AMD AuthenticAMD
Jan 14 03:18:38 ubuntu-test kernel:   Hygon HygonGenuine
Jan 14 03:18:38 ubuntu-test kernel:   Centaur CentaurHauls
Jan 14 03:18:38 ubuntu-test kernel:   zhaoxin   Shanghai  
Jan 14 03:18:38 ubuntu-test kernel: x86/fpu: Supporting XSAVE feature 0x001: 'x87 floating point registers'
Jan 14 03:18:38 ubuntu-test kernel: x86/fpu: Supporting XSAVE feature 0x002: 'SSE registers'
Jan 14 03:18:38 ubuntu-test kernel: x86/fpu: Supporting XSAVE feature 0x004: 'AVX registers'
Jan 14 03:18:38 ubuntu-test kernel: x86/fpu: xstate_offset[2]:  576, xstate_sizes[2]:  256
Jan 14 03:18:38 ubuntu-test kernel: x86/fpu: Enabled xstate features 0x7, context size is 832 bytes, using 'standard' format.
Jan 14 03:18:38 ubuntu-test kernel: BIOS-provided physical RAM map:
Jan 14 03:18:38 ubuntu-test kernel: BIOS-e820: [mem 0x0000000000000000-0x000000000009fbff] usable
Jan 14 03:18:38 ubuntu-test kernel: BIOS-e820: [mem 0x000000000009fc00-0x000000000009ffff] reserved
Jan 14 03:18:38 ubuntu-test kernel: BIOS-e820: [mem 0x00000000000f0000-0x00000000000fffff] reserved
Jan 14 03:18:38 ubuntu-test kernel: BIOS-e820: [mem 0x0000000000100000-0x000000003ffeffff] usable
Jan 14 03:18:38 ubuntu-test kernel: BIOS-e820: [mem 0x000000003fff0000-0x000000003fffffff] ACPI data
Jan 14 03:18:38 ubuntu-test kernel: BIOS-e820: [mem 0x00000000fec00000-0x00000000fec00fff] reserved
```

And more important think i think you will need to add specific Ubuntu testing environment. That's required as most important distro.
Currently [all parser transforms](https://vector.dev/components/?functions%5B%5D=parse) accept a `types` table for field value coercion. The `timestamp` type is unique in that it requires an explicit strptime format to be specified:

```toml
[transforms.coerce]
  type = "coercer"

  [transforms.coerce.types]
    timestamp_field = "timestamp|%s"
```

I propose that we offer a best-effort timestamp parsing option that does not require a strptime format:

```toml
[transforms.coerce]
  type = "coercer"

  [transforms.coerce.types]
    timestamp_field = "timestamp"
```

The formats would include:

- [ ] IS08601
- [ ] RFC3339
- [ ] A generic time only format: `HH:MM[:SS][.SSS][SSS][Z]`, where `[...]` represents an optional successive fragment. The date is assumed to be the local date.

I'm open to other formats I might be missing here.
Currently, the `coerce` transform, and parsing transforms, [offer a `types` table](https://vector.dev/docs/reference/transforms/regex_parser/#types) that allows users to explicitly coercer field values. This is ideal for pipelines that must guarantee consistent field typing, but there are use cases where dynamic typing might be preferred. For these, I propose that we add a `types.auto` option. Specifically:

- [ ] Add a boolean `auto` option the `types` table for [all parser transforms](https://vector.dev/components/).
- [ ] Explicit types, as they exist currently, always take priority and should override automatic typing.
- [ ] Integer strings should automatically be coerced to integers.
- [ ] Float strings should automatically be coerced to floats.
- [ ] Boolean strings as defined by [this comment](https://github.com/timberio/vector/blob/fff92728c9490824ff9d0ae76669adc901bb5499/src/types.rs#L164-L168) should automatically coerce to boolean.
- [ ] ISO8601 time formats should automatically coerce to timestamps. I'm open to other popular formats within reason.

I'm curious what others think about this before we proceed with the work.
Closes #1482 

Adds a pretty conservative logfmt parser. It will basically scan and extract any logfmt-style `foo=bar` pairs in the given field (handling quoting and such), and add them as fields on the event. If there are no such pairs, nothing is added and the message is passed through normally.
Nothing too fancy, just accepts text bodies over HTTP where each line is roughly syslog-formatted. We do some basic parsing but fall back to forwarding the raw line if anything seems off.
This change introduces some improved docs on the process for proposing large PRs. We've in the past run into issues with PRs living for a while because they are large and complex. This should hopefully clear some of that up.
Vector is fast approaching 1.0 and we want to be as transparent as possible with our development and planning efforts.

## 1.0 Roadmap

- [X] `[complete]` Initial logs support ([milestone #2](https://github.com/timberio/vector/milestone/2))
- [X] `[complete]` Securely process data ([milestone #11](https://github.com/timberio/vector/milestone/11))
- [X] `[complete]` Best-in-class `file` source ([milestone #16](https://github.com/timberio/vector/milestone/16))
- [X] `[complete]` Initial metrics support ([milestone #15](https://github.com/timberio/vector/milestone/15))
- [X] `[complete]` Initial containers support ([milestone #14](https://github.com/timberio/vector/milestone/14))
- [X] `[complete]` Initial AWS support ([milestone #2](https://github.com/timberio/vector/milestone/2))
- [X] `[complete]` Support popular targets and CPU archs ([milestone #18](https://github.com/timberio/vector/milestone/18))
- [X] `[complete]` Config testing ([milestone #19](https://github.com/timberio/vector/milestone/19))
- [X] `[complete]` Config consistency & cleanup ([milestone #25](https://github.com/timberio/vector/milestone/25))
- [ ] `[in-progress]` Enrich data with environment context ([milestone #20](https://github.com/timberio/vector/milestone/20))
- [ ] `[in-progress]` Initial GCP support ([milestone #17](https://github.com/timberio/vector/milestone/17))
- [ ] `[in-progress]` Data processing & scripting ([milestone #23](https://github.com/timberio/vector/milestone/23))
- [ ] `[pending]` Initial tracing support
- [ ] `[pending]` Server level metrics collection ([milestone #22](https://github.com/timberio/vector/milestone/22))
- [ ] `[pending]` Vector observability ([milestone #24](https://github.com/timberio/vector/milestone/24))
- [ ] `[pending]` Formalize regression control ([milestone #12](https://github.com/timberio/vector/milestone/12))
- [ ] `[pending]` End-to-end event acknowledgement
- [ ] `[pending]` Improved high concurrency performance
- [ ] `[pending]` Load balancing support
- [ ] `[pending]` Vector to vector application level acknowledgement

## What does 1.0 mean to us?

* **A stable API.** [As defined by semantic versioning](https://semver.org/#spec-item-5). This includes any user-facing part of Vector: the CLI, configuration schema, and everything exposed through Vector's [components](https://vector.dev/components/).
* **Production readiness.** High confidence that Vector will perform well in large demanding production environments. You can read more about he we define "production readiness" [here](https://vector.dev/docs/about/guarantees/#prod-ready).
* **Achieves Vector's vision.** A single tool that collects _all_ observability data and gets it to its destination.

## What does pre-1.0 mean to us?

* Possible API changes. Although, we do not take this lightly and will try to bundle breaking changes into a single release to reduce the migration burden.
*  Possible roadmap changes. We work closely with many Vector users and learn a lot in that process. These findings are likely to alter the 1.0 roadmap in non-substantial ways.

## What about post 1.0?

We will be focusing heavily on end-to-end use cases and going deeper on quality & innovation within the context of these use cases. For example:

* A constant focus on performance, reliability, and usability
* Application observability
* Infrastructure monitoring
* Security & compliance
* Improved parsing & structuring
* Extending Vector with plugins

Closes https://github.com/timberio/vector/issues/1534.

The option is added only to `json` encoding, as `text` encoding was not using `fields` previously.

I decided to not implement dropping the indexed fields from `events` mentioned in #1534 for now as I'm worried about unexpected results for existing users. It can be easily changed.
GCP Big Query is a powerful service for analyzing large amounts of structured data. If used correctly, it can be a cost-effective storage for log data. I would like to see Vector support this service as a sink, but it'll require careful planning due to the many different ways Big Query can be used.

## Context

Big Query is flexible and we should consider the following features for our Big Query sink:

1. Storing JSON data in a single column. Users can use Big Query's [JSON functions](https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functions).
    1. This type of querying is slower for obvious reasons (fetching more data, parsing, etc).
    2. This type of querying is more expensive because each query must fetch and scan the entire JSON payload as opposed to individual columns.
2. Mapping structured data to individual Big Query table columns.
3. Automatic schema maintenance.
4. [Streaming inserts](https://cloud.google.com/bigquery/streaming-data-into-bigquery) vs [batching inserts](https://cloud.google.com/bigquery/docs/loading-data).
    1. Streaming inserts [have a cost](https://cloud.google.com/bigquery/pricing) ($0.010 per 200 MB).
    2. Note the variety of methods to batch insert. This can be done through the API directly or through other GCP services (cloud storage, stackdriver, etc).

This, of course, is not inclusive of all factors we should consider for Big Query, but it helps to demonstrate the variety of options.

## Starting simple

v1 of this sink should solve the simplest implementation:

- [ ] Use the [streaming inserts API]() and stream records 1 by 1 (no batching).
- [ ] Assume we are writing to a table with 2 columns: `timestamp` and `json_event`.
    - [ ] The `timestamp` column should map to our own internal `timestamp` column.
    - [ ] The `json_event` column should contain a JSON encoded representation of our event.
    - [ ] Both of these column names should be configurable. It is worth thinking about a generic column mapping configuration scheme so that users could map other custom fields to Big Query columns.
- [ ] Include documentation on how to create a properly structured table. Ideally this table would be partitioned by the `timestamp` day.

## Long Term

We might consider the following features for long-term development:

1. Support for using the batching API since it does not incur a cost.
2. Dynamic schema maintenance. Although, I think this might be better solved with a transform or something separate.
Blocked by #1530 and #1526.

We need to cut the `0.7.0` release. This is the "Great Breaking Change" release. We've purposely rolled up various breaking changes to consolidate them. As such, this release will require a little extra work:

- [ ] Run `make release` and manually curate the commits in `.meta/releases/0.7.0.toml`. Make sure everything has the proper type and scope. `make generate` pauses for manual curation of this data.
- [ ] Manually add upgrade guides for all breaking changes in `.meta/releases/0.7.0.toml`. You can see an example [here](https://github.com/timberio/vector/blob/fe0e43a60b0ccb4dc7980b8b1edfa5e7b074b189/.meta/releases/0.6.0.toml#L252). Note, we do not need individual guides for every single change, we can combine them into a single guide if it makes sense and provides for a better UX.
- [ ] Proceed with `make generate` to tag and finalize the reasons.

Note: `make generate` is designed to be idempotent, you can exit and re-run it as much as you want. It is an interactive process with prompts.