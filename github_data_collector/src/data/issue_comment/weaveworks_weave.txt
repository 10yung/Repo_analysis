Is there a chance to use weave CNI plugin with podman?

I tried to start weaveexec with `sudo podman --log-level debug run --rm --privileged --net host --pid host -v /:/host -e HOST_ROOT=/host -e DOCKERHUB_USER=weaveworks -e WEAVE_VERSION -e WEAVE_DEBUG -e WEAVE_DOCKER_ARGS -e WEAVE_PASSWORD -e WEAVE_PORT -e WEAVE_HTTP_ADDR -e WEAVE_STATUS_ADDR -e WEAVE_CONTAINER_NAME -e WEAVE_MTU -e WEAVE_NO_FASTDP -e WEAVE_NO_BRIDGED_FASTDP -e DOCKER_BRIDGE -e DOCKER_CLIENT_HOST= -e DOCKER_CLIENT_ARGS -e PROXY_HOST=127.0.0.1 -e COVERAGE -e CHECKPOINT_DISABLE -e AWSVPC weaveworks/weaveexec:2.5.2 --local launch`

Got `Failed to get netdev for "docker0" bridge: Link not found`

Here an excerpt of the debug log:

```bash
INFO[0000] Running conmon under slice machine.slice and unitName libpod-conmon-04a0eac0f55ed48d5e8cb5bc79f9852dce09a40101024ce5fa77478aa95774de.scope 
DEBU[0000] Received: 581                                
INFO[0000] Got Conmon PID as 553                        
DEBU[0000] Created container 04a0eac0f55ed48d5e8cb5bc79f9852dce09a40101024ce5fa77478aa95774de in OCI runtime 
DEBU[0000] Attaching to container 04a0eac0f55ed48d5e8cb5bc79f9852dce09a40101024ce5fa77478aa95774de 
DEBU[0000] connecting to socket /var/run/libpod/socket/04a0eac0f55ed48d5e8cb5bc79f9852dce09a40101024ce5fa77478aa95774de/attach 
DEBU[0000] Starting container 04a0eac0f55ed48d5e8cb5bc79f9852dce09a40101024ce5fa77478aa95774de with command [/home/weave/sigproxy /home/weave/weave --local launch] 
DEBU[0000] Started container 04a0eac0f55ed48d5e8cb5bc79f9852dce09a40101024ce5fa77478aa95774de 
DEBU[0000] Enabling signal proxying                     
Failed to get netdev for "docker0" bridge: Link not found
DEBU[0000] Cleaning up container 04a0eac0f55ed48d5e8cb5bc79f9852dce09a40101024ce5fa77478aa95774de 
DEBU[0000] Network is already cleaned up, skipping...   
DEBU[0000] unmounted container "04a0eac0f55ed48d5e8cb5bc79f9852dce09a40101024ce5fa77478aa95774de" 
DEBU[0000] Successfully cleaned up container 04a0eac0f55ed48d5e8cb5bc79f9852dce09a40101024ce5fa77478aa95774de 
DEBU[0000] Container 04a0eac0f55ed48d5e8cb5bc79f9852dce09a40101024ce5fa77478aa95774de storage is already unmounted, skipping... 
DEBU[0000] [graphdriver] trying provided driver "overlay" 
DEBU[0000] cached value indicated that overlay is supported 
DEBU[0000] cached value indicated that metacopy is not being used 
DEBU[0000] backingFs=extfs, projectQuotaSupported=false, useNativeDiff=true, usingMetacopy=false
```

Here my images:

```bash
$ sudo podman images
REPOSITORY                       TAG      IMAGE ID       CREATED        SIZE
docker.io/library/alpine         latest   e9a72a7c189c   3 weeks ago    5.61 MB
docker.io/weaveworks/weavedb     latest   6898eac75586   2 months ago   6.42 kB
docker.io/weaveworks/weaveexec   2.5.2    429ac05cb8ee   8 months ago   167 MB
docker.io/weaveworks/weave       2.5.2    d2dc0122f4e3   8 months ago   115 MB
k8s.gcr.io/pause  
```
Here the defined rootful networks:

```bash
$ sudo podman network ls
NAME     VERSION   PLUGINS
weave    0.3.0     weave-net,portmap
podman   0.4.0     bridge,portmap,firewall
```

Here the content of /etc/cni/net.d/10-weave.conflist:

```bash
$ cat 10-weave.conflist 
{
    "cniVersion": "0.3.0",
    "name": "weave",
    "plugins": [
        {
            "name": "weave",
            "type": "weave-net",
            "hairpinMode": true
        },
        {
            "type": "portmap",
            "capabilities": {"portMappings": true},
            "snat": true
        }
    ]
}
```

Here the libpod config file:

> NOTE: I changed the container runtime runc with crun (!)

```bash
$ cat /home/podman/.config/containers/libpod.conf
volume_path = "/home/podman/.local/share/containers/storage/volumes"                                                                                           
image_default_transport = "docker://"                                                                                                                          
runtime = "crun"                                                                                                                                               
runtime_supports_json = ["crun"]                                                                                                                               
runtime_supports_nocgroups = ["crun"]                                                                                                                          
conmon_path = ["/usr/libexec/podman/conmon", "/usr/local/libexec/podman/conmon", "/usr/local/lib/podman/conmon", "/usr/bin/conmon", "/usr/sbin/conmon", "/usr/local/bin/conmon", "/usr/local/sbin/conmon", "/run/current-system/sw/bin/conmon"]                                                                               
conmon_env_vars = ["PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"]                                                                        
cgroup_manager = "cgroupfs"                                                                                                                                    
init_path = ""                                                                                                                                                 
static_dir = "/home/podman/.local/share/containers/storage/libpod"                                                                                             
tmp_dir = "/run/user/1000/libpod/tmp"                                                                                                                          
max_log_size = -1                                                                                                                                              
no_pivot_root = false                                                                                                                                          
cni_config_dir = "/etc/cni/net.d/"                                                                                                                             
cni_plugin_dir = ["/usr/libexec/cni", "/usr/lib/cni", "/usr/local/lib/cni", "/opt/cni/bin"]                                                                    
infra_image = "k8s.gcr.io/pause:3.1"                                                                                                                           
infra_command = "/pause"                                                                                                                                       
enable_port_reservation = true                                                                                                                                 
label = true                                                                                                                                                   
network_cmd_path = ""                                                                                                                                          
num_locks = 2048                                                                                                                                               
lock_type = "shm"                                                                                                                                              
events_logger = "journald"                                                                                                                                     
events_logfile_path = ""                                                                                                                                       
detach_keys = "ctrl-p,ctrl-q"                                                                                                                                  
SDNotify = false                                                                                                                                               
                                                                                                                                                               
[runtimes]                                                                                                                                                     
  crun = ["/usr/bin/crun", "/usr/local/bin/crun"]
```

Should I define a custom bridge called `docker0`?

This PR is adding an additional field in the IPAM config for Weave - `ips`.

It allows for specifying an IP address through the CNI plugin, similar to `weave attach` and claim that IP.

1. I am not sure if it makes sense to configure also `checkAlive` or to set it to `true` at all times - currently it is set to `false`.

2. Since the IPAM configuration is specific to each plugin, I think this is allowed per the CNI spec. I am not sure if the choice for `ips` type is right though - this type is used in the `Result`, but it seemed appropriate for the input values of the IPAM as well.

## What you expected to happen?

So I have 2 servers, let's call them A1 (behind NAT) and A2 (has publicly available IP address).
My goal is to achieve 100% peers compatibility as if the server A1 had public IP.

## How to reproduce it?

I setup a script on A1 which forwards TCP port though ssh connection
```
A1# ssh -R 16783:127.0.0.1:6783 A2
```

So that I can reach A1 from A2 now:
```
A2# telnet 127.0.0.1 16783
Trying 127.0.0.1...
Connected to 127.0.0.1.
Escape character is '^]'.
weave^]
```

Great, let's add A1 server to the A2 peers:
```
cat /etc/sysconfig/weave
PEERS="127.0.0.1:16783"
```

Let's see if A2 sees A1 as a peer:
```
A2# weave status peers
22:bb:2e:ae:0b:62(A2)
   -> 127.0.0.1:16783       3e:f5:a1:0d:22:4a(A1)        pending
3e:f5:a1:0d:22:4a(A1)
   <- 127.0.0.1:51520       22:bb:2e:ae:0b:62(A2)               pending

A2# weave status connections
-> 127.0.0.1:16783       pending     encrypted   fastdp 3e:f5:a1:0d:22:4a(A1) encrypted=truemtu=1376

```


So far it seems to be working. Let's finally test pings:


```
A1# docker run -ti --net=weave -e WEAVE_CIDR=192.168.0.1/8 --ip=192.168.0.1 -h a1.weave.local $(weave dns-args) weaveworks/ubuntu
A2# docker run -ti --net=weave -e WEAVE_CIDR=192.168.0.2/8 --ip=192.168.0.2 -h a2.weave.local $(weave dns-args) weaveworks/ubuntu

A2# ping a1.weave.local
PING a1.weave.local (192.168.0.1) 56(84) bytes of data.
From a2.weave.local (192.168.0.2) icmp_seq=1 Destination Host Unreachable
From a2.weave.local (192.168.0.2) icmp_seq=2 Destination Host Unreachable
From a2.weave.local (192.168.0.2) icmp_seq=3 Destination Host Unreachable
^C

#vice versa
A1# ping a2.weave.local
PING a2.weave.local (192.168.0.1) 56(84) bytes of data.
From a1.weave.local (192.168.0.2) icmp_seq=1 Destination Host Unreachable
From a1.weave.local (192.168.0.2) icmp_seq=2 Destination Host Unreachable
^C
```



Here are the questions I have:

1. What other ports I need to forward? 6783, 6784 for TCP and UDP?

>Weave uses ports 6783 TCP / 6784 UDP for fastdp and 6783 TCP / 6783 UDP for sleeve

I'm going to forward UDP/6783 using this approach http://zarb.org/~gc/html/udp-in-ssh-tunneling.html but the question 2 stops me.


2. I was able to specify a single port in `PEERS="127.0.0.1:16783"`. How can I specify the other forwarded ports?

3. I understand that A2->A1 ping requires UDP traffic forwarding. But why A1->A2 ping doesn't work? A2 server has public IP and ports are not restricted.



Please, answer my questions or push me to the right direction.




## Versions:

$ weave version
weave script 2.6.0

$ docker version
18.09.3

$ uname -a
Linux 4.4.127-mainline-rev1 #1 SMP Sun Apr 8 10:38:32 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
```

## Logs:
A2 logs: https://gist.github.com/karser/f323ffb3ac3d875b62c571a46ec5206e
There are some issues using weave on k8s together with shorewall.
I understand my setup is a playground, and the k8s shouldn't be installed on a bare metal with a firewall, so this is for reference in case someone else stumble on those issues, really.

1) if the "iface" extension is installed on the node, shorewall uses it, but the weave pod doesn have it, resulting in `Can't find library for match 'iface'`
2) weave does a `iptables -A INPUT -i weave -j WEAVE-NPC-EGRESS`, resulting in the rule ending up *after* the reject from shorewall, ie
```
Chain INPUT (policy DROP)
num  target     prot opt source               destination
1    KUBE-FIREWALL  all  --  anywhere             anywhere
2    KUBE-SERVICES  all  --  anywhere             anywhere             ctstate NEW /* kubernetes service portals */
3    KUBE-EXTERNAL-SERVICES  all  --  anywhere             anywhere             ctstate NEW /* kubernetes externally-visible service portals */
[zone related rules]
12   ACCEPT     all  --  anywhere             anywhere
13   Reject     all  --  anywhere             anywhere
14   LOG        all  --  anywhere             anywhere             LOG level info prefix "Shorewall:INPUT:REJECT:"
15   reject     all  --  anywhere             anywhere            [goto]
16   WEAVE-NPC-EGRESS  all  --  anywhere             anywhere
```

Solutions:
1) I ended up removing the extension from my system. Shorewall seems fine without it.
2) I manually add a `iptables -I INPUT 4 -i weave -j WEAVE-NPC-EGRESS` to duplicate the rule *before* the reject
<!--
Hi, thank you for opening an issue!
Before hitting the button...



** Is this a REQUEST FOR HELP? **
If so, please have a look at:
- our FAQ: https://www.weave.works/docs/net/latest/faq/
- our troubleshooting page: https://www.weave.works/docs/net/latest/troubleshooting/
- our help page, to choose the best channel (Slack, etc.) to reach out: https://www.weave.works/help/

** Is this a FEATURE REQUEST? **
If so, please search existing feature requests, and if you find a similar one, up-vote it and/or add your comments to it instead.
If you did not find a similar one, please describe in details:
- why: your use-case, specific constraints you may have, etc.
- what: the feature/behaviour/change you would like to see in Weave Net
Do not hesitate, when appropriate, to share the exact commands or API you would like, and/or to share a diagram (e.g.: asciiflow.com): "a picture is worth a thousand words".

** Is this a BUG REPORT? **

we are noticing a random issue with weave cni deamons. we have a Running a K8S cluster on azure VMSS(Scale sets) with following versions. we have 16 workers in the cluster and about 8 of them seems to experience similar problem and the pods are in a crashloop state. 


All pods that are in bad state have the following error. 

[boltDB] Unable to open /weavedb/weave-netdata.db: timeout”

any idea how to investigate this further or cause of this issue. 

Thank you!
-->

## What you expected to happen?

## What happened?
<!-- Error message, actual behaviour, etc. -->
we are noticing a random issue with weave cni deamons. we have a Running a K8S cluster on azure VMSS(Scale sets) with following versions. we have 16 workers in the cluster and about 8 of them seems to experience similar problem and the pods are in a crashloop state. 


All pods that are in bad state have the following error. 

[boltDB] Unable to open /weavedb/weave-netdata.db: timeout”

any idea how to investigate this further or cause of this issue. 


## How to reproduce it?
We have a similar setup and were able to reproduce the issue

## Anything else we need to know?
<!-- Cloud provider? Hardware? How did you configure your cluster? Kubernetes YAML, KOPS, etc. -->

## Versions:
<!-- Please paste in the output of these commands; 'kubectl' only if using Kubernetes -->
```

Kubernetes: 1.10
docker:18.9.3
weave: 2.4.1
Centos7
Client Version: version.Info{Major:"1", Minor:"11", GitVersion:"v1.11.2", GitCommit:"bb9ffb1654d4a729bb4cec18ff088eacc153c239", GitTreeState:"clean", BuildDate:"2018-08-07T23:17:28Z", GoVersion:"go1.10.3", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"10", GitVersion:"v1.10.0", GitCommit:"fc32d2f3698e36b93322a3465f63a14e9f0eaead", GitTreeState:"clean", BuildDate:"2018-03-26T16:44:10Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}


```

## Logs:
```
$ docker logs weave
```
or, if using Kubernetes:
```
$ kubectl logs -n kube-system <weave-net-pod> weave
kubectl logs -n kube-system weave-net-mdl5r weave
[boltDB] Unable to open /weavedb/weave-netdata.db: timeout

kubectl logs -n kube-system weave-net-ntrjs weave
Error from server: Get https://hostname:10250/containerLogs/kube-system/weave-net-ntrjs/weave: x509: certificate specifies an incompatible key usage
```
<!-- (If output is long, please consider a Gist.) -->
<!-- Anything interesting or unusual output by the below, potentially relevant, commands?
$ journalctl -u docker.service --no-pager
$ journalctl -u kubelet --no-pager
$ kubectl get events
-->

## Network:
<!-- If your problem has anything to do with one network endpoint not being able to contact another, please run the following commands -->
```
$ ip route
$ ip -4 -o addr
$ sudo iptables-save
```

## What you expected to happen?
We use kops and weave-net cni for Kubernetes networking in the production cluster. At times we get UnknownHost exceptions, for a couple of external urls. Eg: our database urls, which is a RDS url, for which we have created an ExternalName service and apps talks through the ExternalName service. Our entire cluster runs on AWS. We found that Restarting the weave-net pods helped to fix this issue, every time we face this issue. When we checked, we could not find any specific error logs from Weave nor from CoreDNS. But observed that the weave pods are restarted couple of times, though the Error persist, even when the weave-net pods are running, but after manual kill and start, they have gone.

The issue seems to be only happening on certain nodes in the cluster, at a particular time, and remaining nodes works. 


## What happened?
<!-- Error message, actual behaviour, etc. -->

## How to reproduce it?
<!-- Specific steps, as minimally and precisely as possible. -->

## Anything else we need to know?
<!-- Cloud provider? Hardware? How did you configure your cluster? Kubernetes YAML, KOPS, etc. -->

## Versions:
<!-- Please paste in the output of these commands; 'kubectl' only if using Kubernetes -->
```
$ weave 2.5.2
$ kubectl version 
Client Version: version.Info{Major:"1", Minor:"14", GitVersion:"v1.14.0", GitCommit:"641856db18352033a0d96dbc99153fa3b27298e5", GitTreeState:"clean", BuildDate:"2019-03-25T15:53:57Z", GoVersion:"go1.12.1", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"14", GitVersion:"v1.14.6", GitCommit:"96fac5cd13a5dc064f7d9f4f23030a6aeface6cc", GitTreeState:"clean", BuildDate:"2019-08-19T11:05:16Z", GoVersion:"go1.12.9", Compiler:"gc", Platform:"linux/amd64"}
```

I couldn't collect any specific logs that show this error yet.

## What you expected to happen?
Weave should form a fastdp for the new nodes that have the been joined to the cluster.

## What happened?
Weave falls back to sleeve, however, other nodes remain connected via fastdp.

## How to reproduce it?
Issue only appears to be related to the Broadcom BCM57416 NetXtreme-E Dual-Media 10G NIC.  Connecting the same node with the Broadcom BCM5720 NetXtreme Gigabit forms a fastdp network.  No other changes where made to the node.

## Anything else we need to know?
NIC details:
(BCM57416)
driver: bnxt_en
version: 1.8.0
firmware-version: 214.0.253.1/pkg 21.40.25.31
expansion-rom-version: 
bus-info: 0000:18:00.1
supports-statistics: yes
supports-test: yes
supports-eeprom-access: yes
supports-register-dump: no
supports-priv-flags: no

(BCM5720)
driver: tg3
version: 3.137
firmware-version: FFV20.8.4 bc 5720-v1.39
expansion-rom-version: 
bus-info: 0000:01:00.0
supports-statistics: yes
supports-test: yes
supports-eeprom-access: yes
supports-register-dump: yes
supports-priv-flags: no


## Versions:
<!-- Please paste in the output of these commands; 'kubectl' only if using Kubernetes -->
```
$ weave version 2.6.0
$ docker version 18.09.7
$ uname -a Linux pm-dell-01 4.15.0-72-generic #81-Ubuntu SMP Tue Nov 26 12:20:02 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
$ kubectl version v1.16.3
```

## Logs:
First 1000 lines for failing node:
[pm-dell-01 weave-net debug log](https://gist.github.com/yatsura/4a800f8f2606bda2e28456275a697c04)

## What you expected to happen?
When adding new nodes into a weave network I expect them to be assigned a unique IP for the weave bridge interface. I also expect to be able to perform weave reset from within a weavenet container. 

## What happened?
This is on a kubernetes cluster consisting of 4 nodes (1 control and 3 workers). weavenet is installed as the CNI.
I had a node that already had a weave interface defined and I added it to an existing weave network cluster, weave reached quorum and made links successfully between all nodes, however it did not notice that two nodes shared the same IP address. This meant that routing traffic to any pods living on either of those nodes resulted in 50/50 failure rate. 

## How to reproduce it?
Create a single node K8S cluster (node 1), by installing kubernetes, and installing weave as the CNI.
   kubeadm init
   kubeadm apply -f (deployment file for weave: image: weaveworks/weave-kube:2.5.0 & host network : true)
All pods go to RUNNING state and see the weave interface when you run 'ifconfig weave'
Repeat on another node (node 2)
Note: Both nodes default to weave bridge IP: 10.32.0.1
Bring node 2 down and clean it:
   kubeadm reset
Use the join command on node 2 to join the existing cluster.
On node 1:
   kubeadm token create --print-join-command 
On node 2:
  <use the join command received from the previous step>
kubectl get nodes - will confirm that node 2 has joined the cluster.
ifconfig weave on both nodes confirms that they both have the same IP address still.

Log in to the weave container to verify that weavenet connections have been made:
$ sudo kubectl exec -ti -nkube-system weave-net-stfns -c weave sh
/home/weave # ./weave --local status

        Version: 2.5.0 (version 2.6.0 available - please upgrade!)

        Service: router
       Protocol: weave 1..2
           Name: 72:a0:a9:f7:52:26(node1)
     Encryption: disabled
  PeerDiscovery: enabled
        Targets: 1
    Connections: 2 (1 established, 1 failed)
          Peers: 2 (with 2 established connections)
 TrustedSubnets: none

        Service: ipam
         Status: ready
          Range: 10.32.0.0/12
  DefaultSubnet: 10.32.0.0/12

/home/weave # ./weave --local status connections
<- 10.250.130.7:56006    established fastdp a2:37:ac:ae:1d:ef(node2) mtu=1376
-> 10.250.130.2:6783     failed      cannot connect to ourself, retry: never

From this point onwards you can validate (using something like kuberang) that connectivity into pods within the weave network from outside the network (e.g. node 1 or node 2, or any upstream network connected node) is unstable and will result in timeouts/failures. Eventually it causes coredns to crash.

## Anything else we need to know?
Tests have been run on VMWare and AWS EC2. In both cases this is a Kubernetes cluster.
We do not install weave binary into the command line of the node. It should only run as a container. Installing weave and running weave reset from the command line of the node itself is a workaround we shouldn't be using. Note: I can not seem to find a way of running weave reset from within the weave-net container - otherwise I might use that as part of process of decommissioning a node.

## Versions:
/home/weave # ./weave --local version
weave 2.5.0

$ sudo docker version
Client:
 Version:           18.09.5
 API version:       1.39
 Go version:        go1.10.8
 Git commit:        e8ff056
 Built:             Thu Apr 11 04:43:34 2019
 OS/Arch:           linux/amd64
 Experimental:      false

Server: Docker Engine - Community
 Engine:
  Version:          18.09.5
  API version:      1.39 (minimum version 1.12)
  Go version:       go1.10.8
  Git commit:       e8ff056
  Built:            Thu Apr 11 04:13:40 2019
  OS/Arch:          linux/amd64
  Experimental:     false

$ uname -a
Linux node2 3.10.0-862.el7.x86_64 #1 SMP Wed Mar 21 18:14:51 EDT 2018 x86_64 x86_64 x86_64 GNU/Linux

$ sudo kubectl version
Client Version: version.Info{Major:"1", Minor:"15", GitVersion:"v1.15.0", GitCommit:"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529", GitTreeState:"clean", BuildDate:"2019-06-19T16:40:16Z", GoVersion:"go1.12.5", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"15", GitVersion:"v1.15.0", GitCommit:"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529", GitTreeState:"clean", BuildDate:"2019-06-19T16:32:14Z", GoVersion:"go1.12.5", Compiler:"gc", Platform:"linux/amd64"}
```

## Logs:
```
$ sudo kubectl logs -n kube-system weave-net-zqd49 weave
DEBU: 2019/12/05 11:27:55.957155 [kube-peers] Checking peer "a2:37:ac:ae:1d:ef" against list &{[{72:a0:a9:f7:52:26 node1}]}
Peer not in list; removing persisted data
INFO: 2019/12/05 11:27:56.404358 Command line options: map[docker-api: nickname:node2 no-dns:true datapath:datapath ipalloc-range:10.32.0.0/12 http-addr:127.0.0.1:6784 ipalloc-init:consensus=2 name:a2:37:ac:ae:1d:ef conn-limit:100 db-prefix:/weavedb/weave-net metrics-addr:0.0.0.0:6782 port:6783 expect-npc:true host-root:/host]
INFO: 2019/12/05 11:27:56.404670 weave  2.5.0
INFO: 2019/12/05 11:27:56.412281 failed to create weave-test-commentf7c03267; disabling comment support
INFO: 2019/12/05 11:27:56.495587 Re-exposing 10.32.0.1/12 on bridge "weave"
INFO: 2019/12/05 11:27:56.523625 Bridge type is bridged_fastdp
INFO: 2019/12/05 11:27:56.523699 Communication between peers is unencrypted.
INFO: 2019/12/05 11:27:56.540895 Our name is a2:37:ac:ae:1d:ef(node2)
INFO: 2019/12/05 11:27:56.541031 Launch detected - using supplied peer list: [10.250.130.2 10.250.130.7]
INFO: 2019/12/05 11:27:56.576822 Checking for pre-existing addresses on weave bridge
INFO: 2019/12/05 11:27:56.577274 weave bridge has address 10.32.0.1/12
INFO: 2019/12/05 11:27:56.604256 [allocator a2:37:ac:ae:1d:ef] No valid persisted data
INFO: 2019/12/05 11:27:56.623474 [allocator a2:37:ac:ae:1d:ef] Initialising via deferred consensus
INFO: 2019/12/05 11:27:56.623763 Sniffing traffic on datapath (via ODP)
INFO: 2019/12/05 11:27:56.624668 ->[10.250.130.2:6783] attempting connection
INFO: 2019/12/05 11:27:56.625159 ->[10.250.130.7:6783] attempting connection
INFO: 2019/12/05 11:27:56.625840 ->[10.250.130.7:54862] connection accepted
INFO: 2019/12/05 11:27:56.628366 ->[10.250.130.7:6783|a2:37:ac:ae:1d:ef(node2)]: connection shutting down due to error: cannot connect to ourself
INFO: 2019/12/05 11:27:56.628842 ->[10.250.130.7:54862|a2:37:ac:ae:1d:ef(node2)]: connection shutting down due to error: cannot connect to ourself
INFO: 2019/12/05 11:27:56.629407 ->[10.250.130.2:6783|72:a0:a9:f7:52:26(node1)]: connection ready; using protocol version 2
INFO: 2019/12/05 11:27:56.629738 overlay_switch ->[72:a0:a9:f7:52:26(node1)] using fastdp
INFO: 2019/12/05 11:27:56.629862 ->[10.250.130.2:6783|72:a0:a9:f7:52:26(node1)]: connection added (new peer)
INFO: 2019/12/05 11:27:56.630495 Listening for HTTP control messages on 127.0.0.1:6784
INFO: 2019/12/05 11:27:56.631031 Listening for metrics requests on 0.0.0.0:6782
INFO: 2019/12/05 11:27:56.649937 ->[10.250.130.2:6783|72:a0:a9:f7:52:26(node1)]: connection fully established
ERRO: 2019/12/05 11:27:56.649985 [allocator] address 10.32.0.1/12 is owned by other peer 72:a0:a9:f7:52:26 (node1)
INFO: 2019/12/05 11:27:56.674291 EMSGSIZE on send, expecting PMTU update (IP packet was 60028 bytes, payload was 60020 bytes)
INFO: 2019/12/05 11:27:56.675557 sleeve ->[10.250.130.2:6783|72:a0:a9:f7:52:26(node1)]: Effective MTU verified at 1438
INFO: 2019/12/05 11:27:57.165663 [kube-peers] Added myself to peer list &{[{72:a0:a9:f7:52:26 node1} {a2:37:ac:ae:1d:ef node2}]}
DEBU: 2019/12/05 11:27:57.195294 [kube-peers] Nodes that have disappeared: map[]
10.44.0.0
INFO: 2019/12/05 11:27:57.347081 Weave version 2.6.0 is available; please update at https://github.com/weaveworks/weave/releases/download/v2.6.0/weave
10.250.130.2
10.250.130.7
DEBU: 2019/12/05 11:27:57.462869 registering for updates for node delete events
```

<!-- Anything interesting or unusual output by the below, potentially relevant, commands?
$ journalctl -u docker.service --no-pager | grep -vi info
-- Logs begin at Thu 2019-12-05 10:26:21 UTC, end at Thu 2019-12-05 11:48:59 UTC. --
Dec 05 10:26:39 node1 systemd[1]: Starting Docker Application Container Engine...
Dec 05 10:26:42 node1 dockerd[1455]: time="2019-12-05T10:26:42.678121878Z" level=warning msg="Using pre-4.0.0 kernel for overlay2, mount failures may require kernel update" storage-driver=overlay2
Dec 05 10:26:48 node1 systemd[1]: Started Docker Application Container Engine.
Dec 05 10:27:18 node1 systemd[1]: Stopping Docker Application Container Engine...
Dec 05 10:27:18 node1 systemd[1]: Stopped Docker Application Container Engine.
Dec 05 10:27:18 node1 systemd[1]: Starting Docker Application Container Engine...
Dec 05 10:27:19 node1 dockerd[3858]: time="2019-12-05T10:27:19.039945350Z" level=warning msg="Using pre-4.0.0 kernel for overlay2, mount failures may require kernel update" storage-driver=overlay2
Dec 05 10:27:21 node1 systemd[1]: Started Docker Application Container Engine.

$ journalctl -u kubelet --no-pager
Last message in journal is just as node is joining (and get pods now shows everything as running).
Dec 05 11:27:54 node2 kubelet[30502]: W1205 11:27:54.919085   30502 cni.go:213] Unable to update cni config: No networks found in /etc/cni/net.d
Dec 05 11:27:55 node2 kubelet[30502]: W1205 11:27:55.349741   30502 watcher.go:87] Error while processing event ("/sys/fs/cgroup/devices/libcontainer_31035_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/devices/libcontainer_31035_systemd_test_default.slice: no such file or directory
Dec 05 11:27:55 node2 kubelet[30502]: E1205 11:27:55.818583   30502 kubelet.go:2169] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized

$ kubectl get events
- Nothing unusual. 
25m         Normal   NodeReady                 node/node2   Node node2 status is now: NodeReady
$ date
Thu Dec  5 11:53:51 UTC 2019
-->

## Network:
```
node 1
$ sudo ip route
default via 10.250.130.1 dev eth0 proto static metric 100
10.32.0.0/12 dev weave proto kernel scope link src 10.32.0.1
10.250.0.0/16 dev eth0 proto kernel scope link src 10.250.130.2 metric 100
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1

$ sudo ip -4 -o addr
1: lo    inet 127.0.0.1/8 scope host lo\       valid_lft forever preferred_lft forever
2: eth0    inet 10.250.130.2/16 brd 10.250.255.255 scope global noprefixroute eth0\       valid_lft forever preferred_lft forever
3: docker0    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0\       valid_lft forever preferred_lft forever
6: weave    inet 10.32.0.1/12 brd 10.47.255.255 scope global weave\       valid_lft forever preferred_lft forever

$ sudo iptables-save
# Generated by iptables-save v1.4.21 on Thu Dec  5 11:43:24 2019
*nat
:PREROUTING ACCEPT [0:0]
:INPUT ACCEPT [0:0]
:OUTPUT ACCEPT [25:1516]
:POSTROUTING ACCEPT [25:1516]
:DOCKER - [0:0]
:KUBE-MARK-DROP - [0:0]
:KUBE-MARK-MASQ - [0:0]
:KUBE-NODEPORTS - [0:0]
:KUBE-POSTROUTING - [0:0]
:KUBE-SEP-3DU66DE6VORVEQVD - [0:0]
:KUBE-SEP-PNP333KAQDYB2H5R - [0:0]
:KUBE-SEP-S4MK5EVI7CLHCCS6 - [0:0]
:KUBE-SEP-SWLOBIBPXYBP7G2Z - [0:0]
:KUBE-SEP-SZZ7MOWKTWUFXIJT - [0:0]
:KUBE-SEP-UJJNLSZU6HL4F5UO - [0:0]
:KUBE-SEP-ZCHNBYOGFZRFKYMA - [0:0]
:KUBE-SERVICES - [0:0]
:KUBE-SVC-ERIFXISQEP7F7OF4 - [0:0]
:KUBE-SVC-JD5MR3NA4I4DYORP - [0:0]
:KUBE-SVC-NPX46M4PTMTKRN6Y - [0:0]
:KUBE-SVC-TCOU7JCQXEZGVUNU - [0:0]
:OUTPUT_direct - [0:0]
:POSTROUTING_ZONES - [0:0]
:POSTROUTING_ZONES_SOURCE - [0:0]
:POSTROUTING_direct - [0:0]
:POST_public - [0:0]
:POST_public_allow - [0:0]
:POST_public_deny - [0:0]
:POST_public_log - [0:0]
:PREROUTING_ZONES - [0:0]
:PREROUTING_ZONES_SOURCE - [0:0]
:PREROUTING_direct - [0:0]
:PRE_public - [0:0]
:PRE_public_allow - [0:0]
:PRE_public_deny - [0:0]
:PRE_public_log - [0:0]
:WEAVE - [0:0]
-A PREROUTING -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER
-A OUTPUT -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER
-A POSTROUTING -m comment --comment "kubernetes postrouting rules" -j KUBE-POSTROUTING
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
-A POSTROUTING -j WEAVE
-A DOCKER -i docker0 -j RETURN
-A KUBE-MARK-DROP -j MARK --set-xmark 0x8000/0x8000
-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000
-A KUBE-POSTROUTING -m comment --comment "kubernetes service traffic requiring SNAT" -m mark --mark 0x4000/0x4000 -j MASQUERADE
-A KUBE-SEP-3DU66DE6VORVEQVD -s 10.32.0.3/32 -j KUBE-MARK-MASQ
-A KUBE-SEP-3DU66DE6VORVEQVD -p udp -m udp -j DNAT --to-destination 10.32.0.3:53
-A KUBE-SEP-PNP333KAQDYB2H5R -s 10.250.130.2/32 -j KUBE-MARK-MASQ
-A KUBE-SEP-PNP333KAQDYB2H5R -p tcp -m tcp -j DNAT --to-destination 10.250.130.2:6443
-A KUBE-SEP-S4MK5EVI7CLHCCS6 -s 10.32.0.3/32 -j KUBE-MARK-MASQ
-A KUBE-SEP-S4MK5EVI7CLHCCS6 -p tcp -m tcp -j DNAT --to-destination 10.32.0.3:53
-A KUBE-SEP-SWLOBIBPXYBP7G2Z -s 10.32.0.2/32 -j KUBE-MARK-MASQ
-A KUBE-SEP-SWLOBIBPXYBP7G2Z -p tcp -m tcp -j DNAT --to-destination 10.32.0.2:9153
-A KUBE-SEP-SZZ7MOWKTWUFXIJT -s 10.32.0.2/32 -j KUBE-MARK-MASQ
-A KUBE-SEP-SZZ7MOWKTWUFXIJT -p udp -m udp -j DNAT --to-destination 10.32.0.2:53
-A KUBE-SEP-UJJNLSZU6HL4F5UO -s 10.32.0.2/32 -j KUBE-MARK-MASQ
-A KUBE-SEP-UJJNLSZU6HL4F5UO -p tcp -m tcp -j DNAT --to-destination 10.32.0.2:53
-A KUBE-SEP-ZCHNBYOGFZRFKYMA -s 10.32.0.3/32 -j KUBE-MARK-MASQ
-A KUBE-SEP-ZCHNBYOGFZRFKYMA -p tcp -m tcp -j DNAT --to-destination 10.32.0.3:9153
-A KUBE-SERVICES -d 10.96.0.10/32 -p udp -m comment --comment "kube-system/kube-dns:dns cluster IP" -m udp --dport 53 -j KUBE-SVC-TCOU7JCQXEZGVUNU
-A KUBE-SERVICES -d 10.96.0.10/32 -p tcp -m comment --comment "kube-system/kube-dns:dns-tcp cluster IP" -m tcp --dport 53 -j KUBE-SVC-ERIFXISQEP7F7OF4
-A KUBE-SERVICES -d 10.96.0.1/32 -p tcp -m comment --comment "default/kubernetes:https cluster IP" -m tcp --dport 443 -j KUBE-SVC-NPX46M4PTMTKRN6Y
-A KUBE-SERVICES -d 10.96.0.10/32 -p tcp -m comment --comment "kube-system/kube-dns:metrics cluster IP" -m tcp --dport 9153 -j KUBE-SVC-JD5MR3NA4I4DYORP
-A KUBE-SERVICES -m comment --comment "kubernetes service nodeports; NOTE: this must be the last rule in this chain" -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS
-A KUBE-SVC-ERIFXISQEP7F7OF4 -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-UJJNLSZU6HL4F5UO
-A KUBE-SVC-ERIFXISQEP7F7OF4 -j KUBE-SEP-S4MK5EVI7CLHCCS6
-A KUBE-SVC-JD5MR3NA4I4DYORP -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-SWLOBIBPXYBP7G2Z
-A KUBE-SVC-JD5MR3NA4I4DYORP -j KUBE-SEP-ZCHNBYOGFZRFKYMA
-A KUBE-SVC-NPX46M4PTMTKRN6Y -j KUBE-SEP-PNP333KAQDYB2H5R
-A KUBE-SVC-TCOU7JCQXEZGVUNU -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-SZZ7MOWKTWUFXIJT
-A KUBE-SVC-TCOU7JCQXEZGVUNU -j KUBE-SEP-3DU66DE6VORVEQVD
-A WEAVE -s 10.32.0.0/12 -d 224.0.0.0/4 -j RETURN
-A WEAVE ! -s 10.32.0.0/12 -d 10.32.0.0/12 -j MASQUERADE
-A WEAVE -s 10.32.0.0/12 ! -d 10.32.0.0/12 -j MASQUERADE
COMMIT
# Completed on Thu Dec  5 11:43:24 2019
# Generated by iptables-save v1.4.21 on Thu Dec  5 11:43:24 2019
*mangle
:PREROUTING ACCEPT [482665:278017281]
:INPUT ACCEPT [482649:278013893]
:FORWARD ACCEPT [4:490]
:OUTPUT ACCEPT [471570:90210128]
:POSTROUTING ACCEPT [471574:90210618]
:FORWARD_direct - [0:0]
:INPUT_direct - [0:0]
:OUTPUT_direct - [0:0]
:POSTROUTING_direct - [0:0]
:PREROUTING_ZONES - [0:0]
:PREROUTING_ZONES_SOURCE - [0:0]
:PREROUTING_direct - [0:0]
:PRE_public - [0:0]
:PRE_public_allow - [0:0]
:PRE_public_deny - [0:0]
:PRE_public_log - [0:0]
-A PREROUTING -j PREROUTING_direct
-A PREROUTING -j PREROUTING_ZONES_SOURCE
-A PREROUTING -j PREROUTING_ZONES
-A INPUT -j INPUT_direct
-A FORWARD -j FORWARD_direct
-A OUTPUT -j OUTPUT_direct
-A POSTROUTING -j POSTROUTING_direct
-A PREROUTING_ZONES -i eth0 -g PRE_public
-A PREROUTING_ZONES -g PRE_public
-A PRE_public -j PRE_public_log
-A PRE_public -j PRE_public_deny
-A PRE_public -j PRE_public_allow
COMMIT
# Completed on Thu Dec  5 11:43:24 2019
# Generated by iptables-save v1.4.21 on Thu Dec  5 11:43:24 2019
*security
:INPUT ACCEPT [482651:278016725]
:FORWARD ACCEPT [4:490]
:OUTPUT ACCEPT [471584:90211104]
:FORWARD_direct - [0:0]
:INPUT_direct - [0:0]
:OUTPUT_direct - [0:0]
-A INPUT -j INPUT_direct
-A FORWARD -j FORWARD_direct
-A OUTPUT -j OUTPUT_direct
COMMIT
# Completed on Thu Dec  5 11:43:24 2019
# Generated by iptables-save v1.4.21 on Thu Dec  5 11:43:24 2019
*raw
:PREROUTING ACCEPT [482665:278017281]
:OUTPUT ACCEPT [471570:90210128]
:OUTPUT_direct - [0:0]
:PREROUTING_ZONES - [0:0]
:PREROUTING_ZONES_SOURCE - [0:0]
:PREROUTING_direct - [0:0]
:PRE_public - [0:0]
:PRE_public_allow - [0:0]
:PRE_public_deny - [0:0]
:PRE_public_log - [0:0]
-A PREROUTING -j PREROUTING_direct
-A PREROUTING -j PREROUTING_ZONES_SOURCE
-A PREROUTING -j PREROUTING_ZONES
-A OUTPUT -j OUTPUT_direct
-A PREROUTING_ZONES -i eth0 -g PRE_public
-A PREROUTING_ZONES -g PRE_public
-A PRE_public -j PRE_public_log
-A PRE_public -j PRE_public_deny
-A PRE_public -j PRE_public_allow
COMMIT
# Completed on Thu Dec  5 11:43:24 2019
# Generated by iptables-save v1.4.21 on Thu Dec  5 11:43:24 2019
*filter
:INPUT ACCEPT [2623:467665]
:FORWARD DROP [0:0]
:OUTPUT ACCEPT [2730:559392]
:DOCKER - [0:0]
:DOCKER-ISOLATION-STAGE-1 - [0:0]
:DOCKER-ISOLATION-STAGE-2 - [0:0]
:DOCKER-USER - [0:0]
:FORWARD_IN_ZONES - [0:0]
:FORWARD_IN_ZONES_SOURCE - [0:0]
:FORWARD_OUT_ZONES - [0:0]
:FORWARD_OUT_ZONES_SOURCE - [0:0]
:FORWARD_direct - [0:0]
:FWDI_public - [0:0]
:FWDI_public_allow - [0:0]
:FWDI_public_deny - [0:0]
:FWDI_public_log - [0:0]
:FWDO_public - [0:0]
:FWDO_public_allow - [0:0]
:FWDO_public_deny - [0:0]
:FWDO_public_log - [0:0]
:INPUT_ZONES - [0:0]
:INPUT_ZONES_SOURCE - [0:0]
:INPUT_direct - [0:0]
:IN_public - [0:0]
:IN_public_allow - [0:0]
:IN_public_deny - [0:0]
:IN_public_log - [0:0]
:KUBE-EXTERNAL-SERVICES - [0:0]
:KUBE-FIREWALL - [0:0]
:KUBE-FORWARD - [0:0]
:KUBE-SERVICES - [0:0]
:OUTPUT_direct - [0:0]
:WEAVE-NPC - [0:0]
:WEAVE-NPC-DEFAULT - [0:0]
:WEAVE-NPC-EGRESS - [0:0]
:WEAVE-NPC-EGRESS-ACCEPT - [0:0]
:WEAVE-NPC-EGRESS-CUSTOM - [0:0]
:WEAVE-NPC-EGRESS-DEFAULT - [0:0]
:WEAVE-NPC-INGRESS - [0:0]
-A INPUT -m conntrack --ctstate NEW -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A INPUT -m conntrack --ctstate NEW -m comment --comment "kubernetes externally-visible service portals" -j KUBE-EXTERNAL-SERVICES
-A INPUT -j KUBE-FIREWALL
-A INPUT -i weave -j WEAVE-NPC-EGRESS
-A FORWARD -i weave -m comment --comment "NOTE: this must go before \'-j KUBE-FORWARD\'" -j WEAVE-NPC-EGRESS
-A FORWARD -o weave -m comment --comment "NOTE: this must go before \'-j KUBE-FORWARD\'" -j WEAVE-NPC
-A FORWARD -o weave -m state --state NEW -j NFLOG --nflog-group 86
-A FORWARD -o weave -j DROP
-A FORWARD -i weave ! -o weave -j ACCEPT
-A FORWARD -o weave -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -m comment --comment "kubernetes forwarding rules" -j KUBE-FORWARD
-A FORWARD -m conntrack --ctstate NEW -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A FORWARD -j DOCKER-USER
-A FORWARD -j DOCKER-ISOLATION-STAGE-1
-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -o docker0 -j DOCKER
-A FORWARD -i docker0 ! -o docker0 -j ACCEPT
-A FORWARD -i docker0 -o docker0 -j ACCEPT
-A OUTPUT -m conntrack --ctstate NEW -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A OUTPUT -j KUBE-FIREWALL
-A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2
-A DOCKER-ISOLATION-STAGE-1 -j RETURN
-A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP
-A DOCKER-ISOLATION-STAGE-2 -j RETURN
-A DOCKER-USER -j RETURN
-A KUBE-FIREWALL -m comment --comment "kubernetes firewall for dropping marked packets" -m mark --mark 0x8000/0x8000 -j DROP
-A KUBE-FORWARD -m conntrack --ctstate INVALID -j DROP
-A KUBE-FORWARD -m comment --comment "kubernetes forwarding rules" -m mark --mark 0x4000/0x4000 -j ACCEPT
-A WEAVE-NPC -m state --state RELATED,ESTABLISHED -j ACCEPT
-A WEAVE-NPC -d 224.0.0.0/4 -j ACCEPT
-A WEAVE-NPC -m physdev --physdev-out vethwe-bridge -j ACCEPT
-A WEAVE-NPC -m state --state NEW -j WEAVE-NPC-DEFAULT
-A WEAVE-NPC -m state --state NEW -j WEAVE-NPC-INGRESS
-A WEAVE-NPC-DEFAULT -m set --match-set weave-P.B|!ZhkAr5q=XZ?3}tMBA+0 dst -m comment --comment "DefaultAllow ingress isolation for namespace: kube-system" -j ACCEPT
-A WEAVE-NPC-DEFAULT -m set --match-set weave-Rzff}h:=]JaaJl/G;(XJpGjZ[ dst -m comment --comment "DefaultAllow ingress isolation for namespace: kube-public" -j ACCEPT
-A WEAVE-NPC-DEFAULT -m set --match-set weave-]B*(W?)t*z5O17G044[gUo#$l dst -m comment --comment "DefaultAllow ingress isolation for namespace: kube-node-lease" -j ACCEPT
-A WEAVE-NPC-DEFAULT -m set --match-set weave-;rGqyMIl1HN^cfDki~Z$3]6!N dst -m comment --comment "DefaultAllow ingress isolation for namespace: default" -j ACCEPT
-A WEAVE-NPC-EGRESS -m state --state RELATED,ESTABLISHED -j ACCEPT
-A WEAVE-NPC-EGRESS -m physdev --physdev-in vethwe-bridge -j RETURN
-A WEAVE-NPC-EGRESS -d 224.0.0.0/4 -j RETURN
-A WEAVE-NPC-EGRESS -m state --state NEW -j WEAVE-NPC-EGRESS-DEFAULT
-A WEAVE-NPC-EGRESS -m state --state NEW -m mark ! --mark 0x40000/0x40000 -j WEAVE-NPC-EGRESS-CUSTOM
-A WEAVE-NPC-EGRESS -m state --state NEW -m mark ! --mark 0x40000/0x40000 -j NFLOG --nflog-group 86
-A WEAVE-NPC-EGRESS -m mark ! --mark 0x40000/0x40000 -j DROP
-A WEAVE-NPC-EGRESS-ACCEPT -j MARK --set-xmark 0x40000/0x40000
-A WEAVE-NPC-EGRESS-DEFAULT -m set --match-set weave-E1ney4o[ojNrLk.6rOHi;7MPE src -m comment --comment "DefaultAllow egress isolation for namespace: kube-system" -j WEAVE-NPC-EGRESS-ACCEPT
-A WEAVE-NPC-EGRESS-DEFAULT -m set --match-set weave-E1ney4o[ojNrLk.6rOHi;7MPE src -m comment --comment "DefaultAllow egress isolation for namespace: kube-system" -j RETURN
-A WEAVE-NPC-EGRESS-DEFAULT -m set --match-set weave-41s)5vQ^o/xWGz6a20N:~?#|E src -m comment --comment "DefaultAllow egress isolation for namespace: kube-public" -j WEAVE-NPC-EGRESS-ACCEPT
-A WEAVE-NPC-EGRESS-DEFAULT -m set --match-set weave-41s)5vQ^o/xWGz6a20N:~?#|E src -m comment --comment "DefaultAllow egress isolation for namespace: kube-public" -j RETURN
-A WEAVE-NPC-EGRESS-DEFAULT -m set --match-set weave-sui%__gZ}{kX~oZgI_Ttqp=Dp src -m comment --comment "DefaultAllow egress isolation for namespace: kube-node-lease" -j WEAVE-NPC-EGRESS-ACCEPT
-A WEAVE-NPC-EGRESS-DEFAULT -m set --match-set weave-sui%__gZ}{kX~oZgI_Ttqp=Dp src -m comment --comment "DefaultAllow egress isolation for namespace: kube-node-lease" -j RETURN
-A WEAVE-NPC-EGRESS-DEFAULT -m set --match-set weave-s_+ChJId4Uy_$}G;WdH|~TK)I src -m comment --comment "DefaultAllow egress isolation for namespace: default" -j WEAVE-NPC-EGRESS-ACCEPT
-A WEAVE-NPC-EGRESS-DEFAULT -m set --match-set weave-s_+ChJId4Uy_$}G;WdH|~TK)I src -m comment --comment "DefaultAllow egress isolation for namespace: default" -j RETURN
COMMIT
# Completed on Thu Dec  5 11:43:24 2019

Node 2
$ sudo ip route
default via 10.250.130.1 dev eth0 proto static metric 100
10.32.0.0/12 dev weave proto kernel scope link src 10.32.0.1
10.250.0.0/16 dev eth0 proto kernel scope link src 10.250.130.7 metric 100
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1

$ sudo ip -4 -o addr
1: lo    inet 127.0.0.1/8 scope host lo\       valid_lft forever preferred_lft forever
2: eth0    inet 10.250.130.7/16 brd 10.250.255.255 scope global noprefixroute eth0\       valid_lft forever preferred_lft forever
3: docker0    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0\       valid_lft forever preferred_lft forever
6: weave    inet 10.32.0.1/12 brd 10.47.255.255 scope global weave\       valid_lft forever preferred_lft forever
6: weave    inet 10.44.0.0/12 brd 10.47.255.255 scope global secondary weave\       valid_lft forever preferred_lft forever

$ sudo iptables-save
# Generated by iptables-save v1.4.21 on Thu Dec  5 11:46:34 2019
*nat
:PREROUTING ACCEPT [0:0]
:INPUT ACCEPT [0:0]
:OUTPUT ACCEPT [1:60]
:POSTROUTING ACCEPT [1:60]
:DOCKER - [0:0]
:KUBE-MARK-DROP - [0:0]
:KUBE-MARK-MASQ - [0:0]
:KUBE-NODEPORTS - [0:0]
:KUBE-POSTROUTING - [0:0]
:KUBE-SEP-3DU66DE6VORVEQVD - [0:0]
:KUBE-SEP-PNP333KAQDYB2H5R - [0:0]
:KUBE-SEP-S4MK5EVI7CLHCCS6 - [0:0]
:KUBE-SEP-SWLOBIBPXYBP7G2Z - [0:0]
:KUBE-SEP-SZZ7MOWKTWUFXIJT - [0:0]
:KUBE-SEP-UJJNLSZU6HL4F5UO - [0:0]
:KUBE-SEP-ZCHNBYOGFZRFKYMA - [0:0]
:KUBE-SERVICES - [0:0]
:KUBE-SVC-ERIFXISQEP7F7OF4 - [0:0]
:KUBE-SVC-JD5MR3NA4I4DYORP - [0:0]
:KUBE-SVC-NPX46M4PTMTKRN6Y - [0:0]
:KUBE-SVC-TCOU7JCQXEZGVUNU - [0:0]
:OUTPUT_direct - [0:0]
:POSTROUTING_ZONES - [0:0]
:POSTROUTING_ZONES_SOURCE - [0:0]
:POSTROUTING_direct - [0:0]
:POST_public - [0:0]
:POST_public_allow - [0:0]
:POST_public_deny - [0:0]
:POST_public_log - [0:0]
:PREROUTING_ZONES - [0:0]
:PREROUTING_ZONES_SOURCE - [0:0]
:PREROUTING_direct - [0:0]
:PRE_public - [0:0]
:PRE_public_allow - [0:0]
:PRE_public_deny - [0:0]
:PRE_public_log - [0:0]
:WEAVE - [0:0]
-A PREROUTING -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER
-A OUTPUT -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER
-A POSTROUTING -m comment --comment "kubernetes postrouting rules" -j KUBE-POSTROUTING
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
-A POSTROUTING -j WEAVE
-A DOCKER -i docker0 -j RETURN
-A KUBE-MARK-DROP -j MARK --set-xmark 0x8000/0x8000
-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000
-A KUBE-POSTROUTING -m comment --comment "kubernetes service traffic requiring SNAT" -m mark --mark 0x4000/0x4000 -j MASQUERADE
-A KUBE-SEP-3DU66DE6VORVEQVD -s 10.32.0.3/32 -j KUBE-MARK-MASQ
-A KUBE-SEP-3DU66DE6VORVEQVD -p udp -m udp -j DNAT --to-destination 10.32.0.3:53
-A KUBE-SEP-PNP333KAQDYB2H5R -s 10.250.130.2/32 -j KUBE-MARK-MASQ
-A KUBE-SEP-PNP333KAQDYB2H5R -p tcp -m tcp -j DNAT --to-destination 10.250.130.2:6443
-A KUBE-SEP-S4MK5EVI7CLHCCS6 -s 10.32.0.3/32 -j KUBE-MARK-MASQ
-A KUBE-SEP-S4MK5EVI7CLHCCS6 -p tcp -m tcp -j DNAT --to-destination 10.32.0.3:53
-A KUBE-SEP-SWLOBIBPXYBP7G2Z -s 10.32.0.2/32 -j KUBE-MARK-MASQ
-A KUBE-SEP-SWLOBIBPXYBP7G2Z -p tcp -m tcp -j DNAT --to-destination 10.32.0.2:9153
-A KUBE-SEP-SZZ7MOWKTWUFXIJT -s 10.32.0.2/32 -j KUBE-MARK-MASQ
-A KUBE-SEP-SZZ7MOWKTWUFXIJT -p udp -m udp -j DNAT --to-destination 10.32.0.2:53
-A KUBE-SEP-UJJNLSZU6HL4F5UO -s 10.32.0.2/32 -j KUBE-MARK-MASQ
-A KUBE-SEP-UJJNLSZU6HL4F5UO -p tcp -m tcp -j DNAT --to-destination 10.32.0.2:53
-A KUBE-SEP-ZCHNBYOGFZRFKYMA -s 10.32.0.3/32 -j KUBE-MARK-MASQ
-A KUBE-SEP-ZCHNBYOGFZRFKYMA -p tcp -m tcp -j DNAT --to-destination 10.32.0.3:9153
-A KUBE-SERVICES -d 10.96.0.10/32 -p udp -m comment --comment "kube-system/kube-dns:dns cluster IP" -m udp --dport 53 -j KUBE-SVC-TCOU7JCQXEZGVUNU
-A KUBE-SERVICES -d 10.96.0.10/32 -p tcp -m comment --comment "kube-system/kube-dns:dns-tcp cluster IP" -m tcp --dport 53 -j KUBE-SVC-ERIFXISQEP7F7OF4
-A KUBE-SERVICES -d 10.96.0.10/32 -p tcp -m comment --comment "kube-system/kube-dns:metrics cluster IP" -m tcp --dport 9153 -j KUBE-SVC-JD5MR3NA4I4DYORP
-A KUBE-SERVICES -d 10.96.0.1/32 -p tcp -m comment --comment "default/kubernetes:https cluster IP" -m tcp --dport 443 -j KUBE-SVC-NPX46M4PTMTKRN6Y
-A KUBE-SERVICES -m comment --comment "kubernetes service nodeports; NOTE: this must be the last rule in this chain" -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS
-A KUBE-SVC-ERIFXISQEP7F7OF4 -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-UJJNLSZU6HL4F5UO
-A KUBE-SVC-ERIFXISQEP7F7OF4 -j KUBE-SEP-S4MK5EVI7CLHCCS6
-A KUBE-SVC-JD5MR3NA4I4DYORP -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-SWLOBIBPXYBP7G2Z
-A KUBE-SVC-JD5MR3NA4I4DYORP -j KUBE-SEP-ZCHNBYOGFZRFKYMA
-A KUBE-SVC-NPX46M4PTMTKRN6Y -j KUBE-SEP-PNP333KAQDYB2H5R
-A KUBE-SVC-TCOU7JCQXEZGVUNU -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-SZZ7MOWKTWUFXIJT
-A KUBE-SVC-TCOU7JCQXEZGVUNU -j KUBE-SEP-3DU66DE6VORVEQVD
-A WEAVE -s 10.32.0.0/12 -d 224.0.0.0/4 -j RETURN
-A WEAVE ! -s 10.32.0.0/12 -d 10.32.0.0/12 -j MASQUERADE
-A WEAVE -s 10.32.0.0/12 ! -d 10.32.0.0/12 -j MASQUERADE
COMMIT
# Completed on Thu Dec  5 11:46:34 2019
# Generated by iptables-save v1.4.21 on Thu Dec  5 11:46:34 2019
*mangle
:PREROUTING ACCEPT [380540:260841992]
:INPUT ACCEPT [380522:260838121]
:FORWARD ACCEPT [4:490]
:OUTPUT ACCEPT [369211:68123116]
:POSTROUTING ACCEPT [369215:68123606]
:FORWARD_direct - [0:0]
:INPUT_direct - [0:0]
:OUTPUT_direct - [0:0]
:POSTROUTING_direct - [0:0]
:PREROUTING_ZONES - [0:0]
:PREROUTING_ZONES_SOURCE - [0:0]
:PREROUTING_direct - [0:0]
:PRE_public - [0:0]
:PRE_public_allow - [0:0]
:PRE_public_deny - [0:0]
:PRE_public_log - [0:0]
-A PREROUTING -j PREROUTING_direct
-A PREROUTING -j PREROUTING_ZONES_SOURCE
-A PREROUTING -j PREROUTING_ZONES
-A INPUT -j INPUT_direct
-A FORWARD -j FORWARD_direct
-A OUTPUT -j OUTPUT_direct
-A POSTROUTING -j POSTROUTING_direct
-A PREROUTING_ZONES -i eth0 -g PRE_public
-A PREROUTING_ZONES -g PRE_public
-A PRE_public -j PRE_public_log
-A PRE_public -j PRE_public_deny
-A PRE_public -j PRE_public_allow
COMMIT
# Completed on Thu Dec  5 11:46:34 2019
# Generated by iptables-save v1.4.21 on Thu Dec  5 11:46:34 2019
*security
:INPUT ACCEPT [380520:260840697]
:FORWARD ACCEPT [4:490]
:OUTPUT ACCEPT [369224:68124016]
:FORWARD_direct - [0:0]
:INPUT_direct - [0:0]
:OUTPUT_direct - [0:0]
-A INPUT -j INPUT_direct
-A FORWARD -j FORWARD_direct
-A OUTPUT -j OUTPUT_direct
COMMIT
# Completed on Thu Dec  5 11:46:34 2019
# Generated by iptables-save v1.4.21 on Thu Dec  5 11:46:34 2019
*raw
:PREROUTING ACCEPT [380540:260841992]
:OUTPUT ACCEPT [369211:68123116]
:OUTPUT_direct - [0:0]
:PREROUTING_ZONES - [0:0]
:PREROUTING_ZONES_SOURCE - [0:0]
:PREROUTING_direct - [0:0]
:PRE_public - [0:0]
:PRE_public_allow - [0:0]
:PRE_public_deny - [0:0]
:PRE_public_log - [0:0]
-A PREROUTING -j PREROUTING_direct
-A PREROUTING -j PREROUTING_ZONES_SOURCE
-A PREROUTING -j PREROUTING_ZONES
-A OUTPUT -j OUTPUT_direct
-A PREROUTING_ZONES -i eth0 -g PRE_public
-A PREROUTING_ZONES -g PRE_public
-A PRE_public -j PRE_public_log
-A PRE_public -j PRE_public_deny
-A PRE_public -j PRE_public_allow
COMMIT
# Completed on Thu Dec  5 11:46:34 2019
# Generated by iptables-save v1.4.21 on Thu Dec  5 11:46:34 2019
*filter
:INPUT ACCEPT [62:15062]
:FORWARD DROP [0:0]
:OUTPUT ACCEPT [46:7368]
:DOCKER - [0:0]
:DOCKER-ISOLATION-STAGE-1 - [0:0]
:DOCKER-ISOLATION-STAGE-2 - [0:0]
:DOCKER-USER - [0:0]
:FORWARD_IN_ZONES - [0:0]
:FORWARD_IN_ZONES_SOURCE - [0:0]
:FORWARD_OUT_ZONES - [0:0]
:FORWARD_OUT_ZONES_SOURCE - [0:0]
:FORWARD_direct - [0:0]
:FWDI_public - [0:0]
:FWDI_public_allow - [0:0]
:FWDI_public_deny - [0:0]
:FWDI_public_log - [0:0]
:FWDO_public - [0:0]
:FWDO_public_allow - [0:0]
:FWDO_public_deny - [0:0]
:FWDO_public_log - [0:0]
:INPUT_ZONES - [0:0]
:INPUT_ZONES_SOURCE - [0:0]
:INPUT_direct - [0:0]
:IN_public - [0:0]
:IN_public_allow - [0:0]
:IN_public_deny - [0:0]
:IN_public_log - [0:0]
:KUBE-EXTERNAL-SERVICES - [0:0]
:KUBE-FIREWALL - [0:0]
:KUBE-FORWARD - [0:0]
:KUBE-SERVICES - [0:0]
:OUTPUT_direct - [0:0]
:WEAVE-NPC - [0:0]
:WEAVE-NPC-DEFAULT - [0:0]
:WEAVE-NPC-EGRESS - [0:0]
:WEAVE-NPC-EGRESS-ACCEPT - [0:0]
:WEAVE-NPC-EGRESS-CUSTOM - [0:0]
:WEAVE-NPC-EGRESS-DEFAULT - [0:0]
:WEAVE-NPC-INGRESS - [0:0]
-A INPUT -m conntrack --ctstate NEW -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A INPUT -m conntrack --ctstate NEW -m comment --comment "kubernetes externally-visible service portals" -j KUBE-EXTERNAL-SERVICES
-A INPUT -j KUBE-FIREWALL
-A INPUT -i weave -j WEAVE-NPC-EGRESS
-A FORWARD -i weave -m comment --comment "NOTE: this must go before \'-j KUBE-FORWARD\'" -j WEAVE-NPC-EGRESS
-A FORWARD -o weave -m comment --comment "NOTE: this must go before \'-j KUBE-FORWARD\'" -j WEAVE-NPC
-A FORWARD -o weave -m state --state NEW -j NFLOG --nflog-group 86
-A FORWARD -o weave -j DROP
-A FORWARD -i weave ! -o weave -j ACCEPT
-A FORWARD -o weave -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -m comment --comment "kubernetes forwarding rules" -j KUBE-FORWARD
-A FORWARD -m conntrack --ctstate NEW -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A FORWARD -j DOCKER-USER
-A FORWARD -j DOCKER-ISOLATION-STAGE-1
-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -o docker0 -j DOCKER
-A FORWARD -i docker0 ! -o docker0 -j ACCEPT
-A FORWARD -i docker0 -o docker0 -j ACCEPT
-A OUTPUT -m conntrack --ctstate NEW -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A OUTPUT -j KUBE-FIREWALL
-A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2
-A DOCKER-ISOLATION-STAGE-1 -j RETURN
-A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP
-A DOCKER-ISOLATION-STAGE-2 -j RETURN
-A DOCKER-USER -j RETURN
-A KUBE-FIREWALL -m comment --comment "kubernetes firewall for dropping marked packets" -m mark --mark 0x8000/0x8000 -j DROP
-A KUBE-FORWARD -m conntrack --ctstate INVALID -j DROP
-A KUBE-FORWARD -m comment --comment "kubernetes forwarding rules" -m mark --mark 0x4000/0x4000 -j ACCEPT
-A WEAVE-NPC -m state --state RELATED,ESTABLISHED -j ACCEPT
-A WEAVE-NPC -d 224.0.0.0/4 -j ACCEPT
-A WEAVE-NPC -m physdev --physdev-out vethwe-bridge -j ACCEPT
-A WEAVE-NPC -m state --state NEW -j WEAVE-NPC-DEFAULT
-A WEAVE-NPC -m state --state NEW -j WEAVE-NPC-INGRESS
-A WEAVE-NPC-DEFAULT -m set --match-set weave-P.B|!ZhkAr5q=XZ?3}tMBA+0 dst -m comment --comment "DefaultAllow ingress isolation for namespace: kube-system" -j ACCEPT
-A WEAVE-NPC-DEFAULT -m set --match-set weave-Rzff}h:=]JaaJl/G;(XJpGjZ[ dst -m comment --comment "DefaultAllow ingress isolation for namespace: kube-public" -j ACCEPT
-A WEAVE-NPC-DEFAULT -m set --match-set weave-]B*(W?)t*z5O17G044[gUo#$l dst -m comment --comment "DefaultAllow ingress isolation for namespace: kube-node-lease" -j ACCEPT
-A WEAVE-NPC-DEFAULT -m set --match-set weave-;rGqyMIl1HN^cfDki~Z$3]6!N dst -m comment --comment "DefaultAllow ingress isolation for namespace: default" -j ACCEPT
-A WEAVE-NPC-EGRESS -m state --state RELATED,ESTABLISHED -j ACCEPT
-A WEAVE-NPC-EGRESS -m physdev --physdev-in vethwe-bridge -j RETURN
-A WEAVE-NPC-EGRESS -d 224.0.0.0/4 -j RETURN
-A WEAVE-NPC-EGRESS -m state --state NEW -j WEAVE-NPC-EGRESS-DEFAULT
-A WEAVE-NPC-EGRESS -m state --state NEW -m mark ! --mark 0x40000/0x40000 -j WEAVE-NPC-EGRESS-CUSTOM
-A WEAVE-NPC-EGRESS -m state --state NEW -m mark ! --mark 0x40000/0x40000 -j NFLOG --nflog-group 86
-A WEAVE-NPC-EGRESS -m mark ! --mark 0x40000/0x40000 -j DROP
-A WEAVE-NPC-EGRESS-ACCEPT -j MARK --set-xmark 0x40000/0x40000
-A WEAVE-NPC-EGRESS-DEFAULT -m set --match-set weave-E1ney4o[ojNrLk.6rOHi;7MPE src -m comment --comment "DefaultAllow egress isolation for namespace: kube-system" -j WEAVE-NPC-EGRESS-ACCEPT
-A WEAVE-NPC-EGRESS-DEFAULT -m set --match-set weave-E1ney4o[ojNrLk.6rOHi;7MPE src -m comment --comment "DefaultAllow egress isolation for namespace: kube-system" -j RETURN
-A WEAVE-NPC-EGRESS-DEFAULT -m set --match-set weave-41s)5vQ^o/xWGz6a20N:~?#|E src -m comment --comment "DefaultAllow egress isolation for namespace: kube-public" -j WEAVE-NPC-EGRESS-ACCEPT
-A WEAVE-NPC-EGRESS-DEFAULT -m set --match-set weave-41s)5vQ^o/xWGz6a20N:~?#|E src -m comment --comment "DefaultAllow egress isolation for namespace: kube-public" -j RETURN
-A WEAVE-NPC-EGRESS-DEFAULT -m set --match-set weave-sui%__gZ}{kX~oZgI_Ttqp=Dp src -m comment --comment "DefaultAllow egress isolation for namespace: kube-node-lease" -j WEAVE-NPC-EGRESS-ACCEPT
-A WEAVE-NPC-EGRESS-DEFAULT -m set --match-set weave-sui%__gZ}{kX~oZgI_Ttqp=Dp src -m comment --comment "DefaultAllow egress isolation for namespace: kube-node-lease" -j RETURN
-A WEAVE-NPC-EGRESS-DEFAULT -m set --match-set weave-s_+ChJId4Uy_$}G;WdH|~TK)I src -m comment --comment "DefaultAllow egress isolation for namespace: default" -j WEAVE-NPC-EGRESS-ACCEPT
-A WEAVE-NPC-EGRESS-DEFAULT -m set --match-set weave-s_+ChJId4Uy_$}G;WdH|~TK)I src -m comment --comment "DefaultAllow egress isolation for namespace: default" -j RETURN
COMMIT
# Completed on Thu Dec  5 11:46:34 2019
```
## What happened?

`weave-npc` threw a fatal error and restarted

## Anything else we need to know?
TBC

## Versions:
<!-- Please paste in the output of these commands; 'kubectl' only if using Kubernetes -->
```
$ weave version   2.5.1
$ docker version  19.03.1
$ uname -a
Linux ncn-w001 4.12.14-197.21-default #1 SMP Mon Oct 7 12:41:58 UTC 2019 (8ef2efd) x86_64 x86_64 x86_64 GNU/Linux
$ kubectl version
v1.14.3
```

Log excerpt (full log supplied offline):
```
INFO: 2019/11/22 19:35:26.919571 EVENT AddNetworkPolicy {"metadata":{"creationTimestamp":"2019-11-22T19:35:26Z","generation":1,"labels":{"app":"cfs-8b2e1a5c-4fa9-48c6-8ed2-26b999b0f977","cfsession":"REDACTED","session-id":"8b2e1a5c-4fa9-48c6-8ed2-26b999b0f977"},"name":"cfs-8b2e1a5c-4fa9-48c6-8ed2-26b999b0f977","namespace":"services","resourceVersion":"15153096","selfLink":"/apis/networking.k8s.io/v1/namespaces/services/networkpolicies/cfs-8b2e1a5c-4fa9-48c6-8ed2-26b999b0f977","uid":"3c006073-0d5f-11ea-9b49-b42e993b70ae"},"spec":{"ingress":[{"from":[{"podSelector":{"matchLabels":{"job-name":"cfs-8b2e1a5c-4fa9-48c6-8ed2-26b999b0f977"}}},{"podSelector":{"matchLabels":{"app":"REDACTED"}}}]}],"podSelector":{"matchLabels":{"app":"cfs-8b2e1a5c-4fa9-48c6-8ed2-26b999b0f977"}},"policyTypes":["Ingress"]}}
INFO: 2019/11/22 19:35:26.919655 creating ipset: &npc.selectorSpec{key:"app=cfs-8b2e1a5c-4fa9-48c6-8ed2-26b999b0f977", selector:labels.internalSelector{labels.Requirement{key:"app", operator:"=", strValues:[]string{"cfs-8b2e1a5c-4fa9-48c6-8ed2-26b999b0f977"}}}, policyTypes:[]npc.policyType{0x0}, ipsetType:"hash:ip", ipsetName:"weave-vV!?#opipYlkU!ZcG~mRVDB|=", nsName:"services"}
INFO: 2019/11/22 19:35:26.920605 creating ipset: &npc.selectorSpec{key:"job-name=cfs-8b2e1a5c-4fa9-48c6-8ed2-26b999b0f977", selector:labels.internalSelector{labels.Requirement{key:"job-name", operator:"=", strValues:[]string{"cfs-8b2e1a5c-4fa9-48c6-8ed2-26b999b0f977"}}}, policyTypes:[]npc.policyType(nil), ipsetType:"hash:ip", ipsetName:"weave-pUZWsLv@phb_a*3/REL_zzqTE", nsName:"services"}
INFO: 2019/11/22 19:35:26.921555 adding rule [-m set --match-set weave-pUZWsLv@phb_a*3/REL_zzqTE src -m set --match-set weave-vV!?#opipYlkU!ZcG~mRVDB|= dst -m comment --comment pods: namespace: services, selector: job-name=cfs-8b2e1a5c-4fa9-48c6-8ed2-26b999b0f977 -> pods: namespace: services, selector: app=cfs-8b2e1a5c-4fa9-48c6-8ed2-26b999b0f977 (ingress) -j ACCEPT] to "WEAVE-NPC-INGRESS" chain
...
INFO: 2019/11/22 19:51:43.428091 EVENT DeleteNetworkPolicy {"metadata":{"creationTimestamp":"2019-11-22T19:35:26Z","generation":1,"labels":{"app":"cfs-8b2e1a5c-4fa9-48c6-8ed2-26b999b0f977","cfsession":"REDACTED","session-id":"8b2e1a5c-4fa9-48c6-8ed2-26b999b0f977"},"name":"cfs-8b2e1a5c-4fa9-48c6-8ed2-26b999b0f977","namespace":"services","resourceVersion":"15159684","selfLink":"/apis/networking.k8s.io/v1/namespaces/services/networkpolicies/cfs-8b2e1a5c-4fa9-48c6-8ed2-26b999b0f977","uid":"3c006073-0d5f-11ea-9b49-b42e993b70ae"},"spec":{"ingress":[{"from":[{"podSelector":{"matchLabels":{"job-name":"cfs-8b2e1a5c-4fa9-48c6-8ed2-26b999b0f977"}}},{"podSelector":{"matchLabels":{"app":"REDACTED"}}}]}],"podSelector":{"matchLabels":{"app":"cfs-8b2e1a5c-4fa9-48c6-8ed2-26b999b0f977"}},"policyTypes":["Ingress"]}}
INFO: 2019/11/22 19:51:43.428226 deleting rule [-m set --match-set weave-pUZWsLv@phb_a*3/REL_zzqTE src -m set --match-set weave-vV!?#opipYlkU!ZcG~mRVDB|= dst -m comment --comment pods: namespace: services, selector: job-name=cfs-8b2e1a5c-4fa9-48c6-8ed2-26b999b0f977 -> pods: namespace: services, selector: app=cfs-8b2e1a5c-4fa9-48c6-8ed2-26b999b0f977 (ingress) -j ACCEPT] from "WEAVE-NPC-INGRESS" chain
FATA: 2019/11/22 19:51:43.430600 delete network policy: running [/sbin/iptables -t filter -D WEAVE-NPC-INGRESS -m set --match-set weave-pUZWsLv@phb_a*3/REL_zzqTE src -m set --match-set weave-vV!?#opipYlkU!ZcG~mRVDB|= dst -m comment --comment pods: namespace: services, selector: job-name=cfs-8b2e1a5c-4fa9-48c6-8ed2-26b999b0f977 -> pods: namespace: services, selector: app=cfs-8b2e1a5c-4fa9-48c6-8ed2-26b999b0f977 (ingress) -j ACCEPT --wait]: exit status 1: iptables: Bad rule (does a matching rule exist in that chain?).
```

From @abuehrle 

> We should add a section to the Weave Net docs (possibly a FAQ item or even a whole topic) about multicasting and Weave Net:
> 
>     cluster to cluster multicasting limitations
>     pod to pod what's supported and what's not and caveats
>     ideal situation for multicast
> 
> -maybe encourage PRs from the community to update it
> 
> from @bboreham:
> 
> Our multicast support is a behaviour that falls out of faithfully emulating a layer 2 switch.
> Within the Weave network it just works; you’d have to make special efforts to stop it.
> However what some people want is to route multicast traffic in and out of the cluster. We don’t have any special support for that; it would need some external bridging technology.