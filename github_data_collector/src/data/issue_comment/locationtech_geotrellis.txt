I reported this on Gitter thinking it was a data issue but I just ran the included `VectorizeSpec.scala` test with a similar failure. The test in question is
`it("should handle a user-submitted problem tile")`.

Vectorization with `EightNeighbors` fails for many of our tiles while succeeding for a few. There seems to be some connection to the tile size. With tile size 512 or greater all tiles fail vectorization. With tile size 256, 4 out 156 succeed and the rest fail.
The included test with a tile size of 44 fails when changing `regionConnectivity` to `EightNeighbors`.

The error from the included test is
```
-40
java.lang.ArrayIndexOutOfBoundsException: -40
	at geotrellis.raster.IntConstantNoDataArrayTile.apply(IntArrayTile.scala:117)
	at geotrellis.raster.ArrayTile.get(ArrayTile.scala:339)
	at geotrellis.raster.regiongroup.RegionGroup$.apply(RegionGroup.scala:165)
	at geotrellis.raster.vectorize.Vectorize$.apply(Vectorize.scala:67)
	at geotrellis.raster.vectorize.TileVectorizeMethods.toVector(TileVectorizeMethods.scala:25)
	at geotrellis.raster.vectorize.TileVectorizeMethods.toVector$(TileVectorizeMethods.scala:24)
	at geotrellis.raster.vectorize.Implicits$withSinglebandVectorizeMethods.toVector(Implicits.scala:25)
	at geotrellis.raster.vectorize.VectorizeSpec.$anonfun$new$57(VectorizeSpec.scala:625)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSpecLike$$anon$1.apply(FunSpecLike.scala:455)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.FunSpec.withFixture(FunSpec.scala:1630)
	at org.scalatest.FunSpecLike.invokeWithFixture$1(FunSpecLike.scala:453)
	at org.scalatest.FunSpecLike.$anonfun$runTest$1(FunSpecLike.scala:465)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:286)
	at org.scalatest.FunSpecLike.runTest(FunSpecLike.scala:465)
	at org.scalatest.FunSpecLike.runTest$(FunSpecLike.scala:447)
	at org.scalatest.FunSpec.runTest(FunSpec.scala:1630)
	at org.scalatest.FunSpecLike.$anonfun$runTests$1(FunSpecLike.scala:498)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:393)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:381)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:370)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:407)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:381)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:376)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:458)
	at org.scalatest.FunSpecLike.runTests(FunSpecLike.scala:498)
	at org.scalatest.FunSpecLike.runTests$(FunSpecLike.scala:497)
	at org.scalatest.FunSpec.runTests(FunSpec.scala:1630)
	at org.scalatest.Suite.run(Suite.scala:1124)
	at org.scalatest.Suite.run$(Suite.scala:1106)
	at org.scalatest.FunSpec.org$scalatest$FunSpecLike$$super$run(FunSpec.scala:1630)
	at org.scalatest.FunSpecLike.$anonfun$run$1(FunSpecLike.scala:502)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:518)
	at org.scalatest.FunSpecLike.run(FunSpecLike.scala:502)
	at org.scalatest.FunSpecLike.run$(FunSpecLike.scala:501)
	at org.scalatest.FunSpec.run(FunSpec.scala:1630)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13(Runner.scala:1349)
	at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13$adapted(Runner.scala:1343)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1343)
	at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24(Runner.scala:1033)
	at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24$adapted(Runner.scala:1011)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1509)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1011)
	at org.scalatest.tools.Runner$.run(Runner.scala:850)
	at org.scalatest.tools.Runner.run(Runner.scala)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:133)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:27)
```
# Overview

Fixes issue described at #3168 
The original implementation blindly assumed same partition indexing, causing empty join when using custom partitioning with one of the RDDs being joined.
This PR adds tests for this use case and fixes the issue by adding an additional check that same PartitionIndexer is used, so that reshuffle is triggered when that is not the case.
PR also fixed some inactive asserts in same file.
It also addresses partitioning mismatch in ReorderedSpaceRDD

## Checklist

- [x] [docs/CHANGELOG.rst](https://github.com/locationtech/geotrellis/blob/master/CHANGELOG.md) updated, if necessary
-  <del>[Module Hierarcy](https://github.com/locationtech/geotrellis/blob/master/docs/guide/module-hierarchy.rst) updated, if necessary </del>
-  <del>`docs` guides update, if necessary</del>
-  <del>New user API has useful Scaladoc strings</del>
- [x] Unit tests added for bug-fix or new feature

Closes #3168

GT version: 2.3.1

When i ingest epsg:4490 tif into catalog, i found that the crs info is `"crs": "+proj=longlat +ellps=GRS80 +no_defs"` in metadata.json file. But in [proj4 epsg config file,](https://github.com/locationtech/proj4j/blob/master/src/main/resources/proj4/nad/epsg) there are many epsgcodes corresponding to that crs info. 
When reading metadata from attribute, it will choose the first epsgcode 4019 in the epsg file, which is not expected one. 
I suggest to add more crs info in metadata, like `"crs": "<4490> +proj=longlat +ellps=GRS80 +no_defs"`.
Related encode code:
```
geotrellis.spark.io.json.Implicits

implicit object CRSFormat extends RootJsonFormat[CRS] {
// Perhaps `crs.toWKT()` is better?
def write(crs: CRS) = JsString(crs.toProj4String) // change to JsString(s"<${crs.epsgcode.getOrElse(defaultValue)}> ${crs.toProj4String}"
```
Decode in readTileMetaData should also change based that convention.


In 95 rows
`if(c + 1 < cols && doesFlow(flowDirrection.get(n._1, n._2), 16))`
 and 106 rows
`if(c + 1 < cols && r + 1 < rows && doesFlow(flowDirrection.get(n._1, n._2), 32))`
if the grid meets the conditions and is not processed, it will be pushed into the stack. When the grid in the stack is processed later, the grid number here does not move with it, because the map set is not updated, which can lead to a dead cycle.
The recommended forms are unified:
1. Using 117 lines in the form of direct calculation with C and R
`if(r + 1 < rows && doesFlow(flowDirrection.get(c, r + 1), 64))`
2. After lines 187 and 188 update C and R, update the correct map set with the new C and R
`c = t._1
 r = t._2
map = Map[Int, (Int, Int)](
      16 -> (c+1, r),
      32 -> (c+1, r+1),
      64 -> (c, r+1),
      128 -> (c-1, r+1),
      1 -> (c-1, r),
      2 -> (c-1, r-1),
      4 -> (c, r-1),
      8 -> (c+1, r-1))`
there is a piece of sample code.
```
val c = a.convert(IntCellType).combine(b) {...}
```
as `a` is `ShortConstantNoDataArrayTile` with 16bits cellType `ShortConstantNoDataCellType`,
`b` is `CroppedTile(ShortUserDefinedNoDataArrayTile)` with 16bits cellType `ShortUserDefinedNoDataCellType`.
After `convert`, it will be `IntRawArrayTile.combine(b) {...}` which `IntRawArrayTile` contains 32bits `IntCellType`.
Finally, the result cellType of `c` is `ShortUserDefinedNoDataCellType` instead of expected `IntCellType`.
i think the `CroppedTile.combine` op should union other tile's cellType. Like below code.

```
  def combine(other: Tile)(f: (Int, Int) => Int): Tile = {
    (this, other).assertEqualDimensions

    val tile = ArrayTile.alloc(cellType, cols, rows)
    cfor(0)(_ < rows, _ + 1) { row =>
      cfor(0)(_ < cols, _ + 1) { col =>
        tile.set(col, row, f(get(col, row), other.get(col, row)))
      }
    }

    tile
  }
```
should be 
```
  def combine(other: Tile)(f: (Int, Int) => Int): Tile = {
    (this, other).assertEqualDimensions

    val tile = ArrayTile.alloc(cellType.union(other.cellType), cols, rows)
    cfor(0)(_ < rows, _ + 1) { row =>
      cfor(0)(_ < cols, _ + 1) { col =>
        tile.set(col, row, f(get(col, row), other.get(col, row)))
      }
    }

    tile
  }
```
same like ArrayTile.
CompositeTile also has this issue.
We're doing a spatial join of 2 rdd's, but have defined a custom partitioner index:

```
 implicit object SpaceTimeByMonthPartitioner extends  PartitionerIndex[SpaceTimeKey] {
    private def toZ(key: SpaceTimeKey): Z3 = Z3(key.col >> 4, key.row >> 4, 13*key.time.getYear + key.time.getMonthValue)

    def toIndex(key: SpaceTimeKey): BigInt = toZ(key).z

    def indexRanges(keyRange: (SpaceTimeKey, SpaceTimeKey)): Seq[(BigInt, BigInt)] =
      Z3.zranges(toZ(keyRange._1), toZ(keyRange._2))
  }
```

If one of the RDD's in the join already has a SpacePartitioner, with a default partitioner index, the resulting rdd is empty.
This is caused by the fact that geotrellis.spark.partition.ReorderedSpaceRDD assumes that all space partitioners use the same index.

Adding a require in ReorderedSpaceRDD to compare the index would help to make the user aware of this assumption.

The following code
```
val p = Point(1, 1)
println(GeometryCollection(multiPoints = Seq(MultiPoint(p, p))).toGeoJson)
```
prints
```
{
  "type" : "GeometryCollection",
  "geometries" : [
    {"type" : "MultiPoint", "coordinates" : [[1.0, 1.0], [1.0, 1.0]]},
    {"type" : "GeometryCollection", "geometries" : [
      {"type" : "Point", "coordinates" : [1.0, 1.0]},
      {"type" : "Point", "coordinates" : [1.0, 1.0]}
    ]}
  ]
}
```
The same duplication happens with `MultiPolygons` and `MultiLines` when nested into `GeometryCollection`. It appears that this is because a `MultiPoint` IS a `GeometryCollection`, so the `Encoder` retrieves it two times - first by `getAll[MultiPoint]`, then by `getAll[GeometryCollection]`.
So that `MVTFeature` has the same capabilities as `geotrellis.vector.Feature`: https://github.com/locationtech/geotrellis/blob/master/vector/src/main/scala/geotrellis/vector/Feature.scala#L30-L42
Investigate would it improve our performance or not. Spark tests (in the Apache Spark github repo) are build around [SparkFunSuite](https://github.com/apache/spark/blob/master/core/src/test/scala/org/apache/spark/SparkFunSuite.scala) and [SharedSparkContext](https://github.com/apache/spark/blob/master/core/src/test/scala/org/apache/spark/SharedSparkContext.scala). Probably it makes sense to adopt their code and use it in our codebase. 
Signed-off-by: Andrey Tararaksin <atararaksin@gmail.com>

# Overview

This PR fixes `PolygonRasterizer` crash described in #3160. Consider a scanline crossing two edges of a polygon in two separate (non-vertex) points. If x-coords of these points are calculated as identical Doubles (due to a Double-related error), the crash occurs.
This PR fixes this issue by recalculating such coordinates using precise `spire.math.Rational`. This recalculation is performed for a given row if it contains two or more matching Double x-coords. It is a very rare case and for a typical polygon occurs zero to little times, so the general-case performance is not affected.

## Checklist

- [x] [docs/CHANGELOG.rst](https://github.com/locationtech/geotrellis/blob/master/CHANGELOG.md) updated, if necessary
- [ ] [Module Hierarcy](https://github.com/locationtech/geotrellis/blob/master/docs/guide/module-hierarchy.rst) updated, if necessary
- [ ] `docs` guides update, if necessary
- [ ] New user API has useful Scaladoc strings
- [x] Unit tests added for bug-fix or new feature


Closes #3160
