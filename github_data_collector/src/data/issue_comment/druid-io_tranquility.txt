I added a section on readme to inform users about the state of the project. Druid docs say that it is not the recommended way to ingest data. See [here](https://github.com/apache/druid/releases/tag/druid-0.16.0-incubating#docs) and [here](https://druid.staged.apache.org/docs/latest/ingestion/tranquility.html). We should let people now that when they stumble upon this repository that there might be a better way to ingest data.
tranquility 0.8.2
druid 0.16.0

run: 
bin/tranquility server -configFile conf/server.json -Ddruid.extensions.loadList='["druid-datasketches"]' -Ddruid.extensions.directory=/usr/local/druid-0.16.0/extensions

in  server.json, i add:

"metricsSpec": [
         {
         "name" : "count",
         "type" : "count"
         },
         {
         "name":"sketch_uid",
         "type":"thetaSketch",
         "fieldName":"uid",
         "isInputThetaSketch":"false",
         "size":"1048576"
          }
]



output:
2019-10-14 09:19:46,634 [main] INFO  i.d.initialization.Initialization - Loading extension [druid-datasketches] for class [io.druid.initialization.DruidModule]
2019-10-14 09:19:46,701 [main] INFO  i.d.initialization.Initialization - added URL[file:/usr/local/druid-0.16.0/extensions/druid-datasketches/commons-math3-3.6.1.jar]
2019-10-14 09:19:46,701 [main] INFO  i.d.initialization.Initialization - added URL[file:/usr/local/druid-0.16.0/extensions/druid-datasketches/druid-datasketches-0.16.0-incubating.jar]
2019-10-14 09:19:46,701 [main] INFO  i.d.initialization.Initialization - added URL[file:/usr/local/druid-0.16.0/extensions/druid-datasketches/memory-0.12.2.jar]
2019-10-14 09:19:46,702 [main] INFO  i.d.initialization.Initialization - added URL[file:/usr/local/druid-0.16.0/extensions/druid-datasketches/sketches-core-0.13.4.jar]
2019-10-14 09:19:46,702 [main] INFO  i.d.initialization.Initialization - added URL[file:/usr/local/druid-0.16.0/extensions/druid-datasketches/slf4j-api-1.7.25.jar]
2019-10-14 09:19:47,070 [main] INFO  c.metamx.emitter.core.LoggingEmitter - Start: started [true]
2019-10-14 09:19:47,545 [main] WARN  io.druid.segment.indexing.DataSchema - No metricsSpec has been specified. Are you sure this is what you want?
2019-10-14 09:19:47,547 [main] WARN  io.druid.segment.indexing.DataSchema - No metricsSpec has been specified. Are you sure this is what you want?
2019-10-14 09:19:47,550 [main] INFO  c.metamx.emitter.core.LoggingEmitter - Start: started [true]
2019-10-14 09:19:47,552 [main] WARN  io.druid.segment.indexing.DataSchema - No metricsSpec has been specified. Are you sure this is what you want?
java.lang.IllegalArgumentException: Could not resolve type id 'thetaSketch' into a subtype of [simple type, class io.druid.query.aggregation.AggregatorFactory]
 at [Source: N/A; line: -1, column: -1] (through reference chain: Object[][1])
	at com.fasterxml.jackson.databind.ObjectMapper._convert(ObjectMapper.java:2774)
	at com.fasterxml.jackson.databind.ObjectMapper.convertValue(ObjectMapper.java:2700)
	at com.metamx.tranquility.druid.DruidBeams$.makeFireDepartment(DruidBeams.scala:433)
	at com.metamx.tranquility.druid.DruidBeams$.fromConfigInternal(DruidBeams.scala:299)
	at com.metamx.tranquility.druid.DruidBeams$.fromConfig(DruidBeams.scala:204)
	at com.metamx.tranquility.server.http.ServerMain$$anonfun$2.apply(ServerMain.scala:118)
	at com.metamx.tranquility.server.http.ServerMain$$anonfun$2.apply(ServerMain.scala:98)
	at com.metamx.common.scala.collection.package$MapLikeOps$$anonfun$strictMapValues$1.apply(package.scala:143)
	at com.metamx.common.scala.collection.package$MapLikeOps$$anonfun$strictMapValues$1.apply(package.scala:143)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
	at scala.collection.immutable.Map$Map3.foreach(Map.scala:161)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:245)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at com.metamx.common.scala.collection.package$MapLikeOps.strictMapValues(package.scala:143)
	at com.metamx.tranquility.server.http.ServerMain$.createServlet(ServerMain.scala:98)
	at com.metamx.tranquility.server.http.ServerMain$.main(ServerMain.scala:77)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.twitter.app.App$$anonfun$nonExitingMain$3.apply(App.scala:168)
	at com.twitter.app.App$$anonfun$nonExitingMain$3.apply(App.scala:167)
	at scala.Option.foreach(Option.scala:257)
	at com.twitter.app.App$class.nonExitingMain(App.scala:167)
	at com.metamx.tranquility.server.http.ServerMain$.nonExitingMain(ServerMain.scala:49)
	at com.twitter.app.App$class.main(App.scala:133)
	at com.metamx.tranquility.server.http.ServerMain$.main(ServerMain.scala:49)
	at com.metamx.tranquility.distribution.DistributionMain$.main(DistributionMain.scala:34)
	at com.metamx.tranquility.distribution.DistributionMain.main(DistributionMain.scala)
Caused by: com.fasterxml.jackson.databind.JsonMappingException: Could not resolve type id 'thetaSketch' into a subtype of [simple type, class io.druid.query.aggregation.AggregatorFactory]
 at [Source: N/A; line: -1, column: -1] (through reference chain: Object[][1])
	at com.fasterxml.jackson.databind.JsonMappingException.from(JsonMappingException.java:148)
	at com.fasterxml.jackson.databind.DeserializationContext.unknownTypeException(DeserializationContext.java:862)
                        "format": "json"
	at com.fasterxml.jackson.databind.jsontype.impl.TypeDeserializerBase._findDeserializer(TypeDeserializerBase.java:167)
	at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:99)
	at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:84)
	at com.fasterxml.jackson.databind.deser.AbstractDeserializer.deserializeWithType(AbstractDeserializer.java:132)
	at com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:151)
	at com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:17)
	at com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:538)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:344)
	at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1064)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:264)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:124)
	at com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:538)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:344)
	at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1064)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:264)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:124)
	at com.fasterxml.jackson.databind.ObjectMapper._convert(ObjectMapper.java:2769)

I use the 0.15.0 version sql query,
e.q.
select a.uuid from db a where a.uuid not in (select b.uuid from db b where b.status = '0' )
it always return "LIMIT 5000 / org.apache.druid.java.util.common.ISE" but use "in" that ok.Am I using the error?

thanks~
Could anyone point me in the right direction to use tranquility with BeamRDD in JAVA?

Basically I would like to see a basic working example with default values just as in:

https://github.com/druid-io/tranquility/blob/master/docs/spark.md

But in java.
My error is like follows:
It seems caused by I have too many process want to write the same druid datasource.
But why it need zookeeper lock when write to druid?

java.lang.IllegalStateException: Failed to create merged beam: druid:overlord/dwarch_ad_local_abtest_lit
	at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$sendAll$2$$anonfun$26.apply(ClusteredBeam.scala:383)
	at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$sendAll$2$$anonfun$26.apply(ClusteredBeam.scala:379)
	at com.twitter.util.Promise$Transformer.liftedTree1$1(Promise.scala:112)
	at com.twitter.util.Promise$Transformer.k(Promise.scala:112)
	at com.twitter.util.Promise$Transformer.apply(Promise.scala:122)
	at com.twitter.util.Promise$Transformer.apply(Promise.scala:103)
	at com.twitter.util.Promise$$anon$1.run(Promise.scala:366)
	at com.twitter.concurrent.LocalScheduler$Activation.run(Scheduler.scala:178)
	at com.twitter.concurrent.LocalScheduler$Activation.submit(Scheduler.scala:136)
	at com.twitter.concurrent.LocalScheduler.submit(Scheduler.scala:207)
	at com.twitter.concurrent.Scheduler$.submit(Scheduler.scala:92)
	at com.twitter.util.Promise.runq(Promise.scala:350)
	at com.twitter.util.Promise.updateIfEmpty(Promise.scala:721)
	at com.twitter.util.ExecutorServiceFuturePool$$anon$2.run(FuturePool.scala:107)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalStateException: Failed to save new beam for identifier[druid:overlord/dwarch_ad_local_abtest_lit] timestamp[2019-07-19T09:00:00.000Z]
	at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$2.applyOrElse(ClusteredBeam.scala:289)
	at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$2.applyOrElse(ClusteredBeam.scala:286)
	at com.twitter.util.Future$$anonfun$rescue$1.apply(Future.scala:924)
	at com.twitter.util.Future$$anonfun$rescue$1.apply(Future.scala:922)
	... 17 more
Caused by: java.lang.IllegalStateException: instance must be started before calling this method
	at org.apache.curator.shaded.com.google.common.base.Preconditions.checkState(Preconditions.java:176)
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.create(CuratorFrameworkImpl.java:351)
	at org.apache.curator.framework.recipes.locks.StandardLockInternalsDriver.createsTheLock(StandardLockInternalsDriver.java:54)
	at org.apache.curator.framework.recipes.locks.LockInternals.attemptLock(LockInternals.java:217)
	at org.apache.curator.framework.recipes.locks.InterProcessMutex.internalLock(InterProcessMutex.java:232)
	at org.apache.curator.framework.recipes.locks.InterProcessMutex.acquire(InterProcessMutex.java:89)
	at org.apache.curator.framework.recipes.locks.InterProcessSemaphoreV2.internalAcquire1Lease(InterProcessSemaphoreV2.java:351)
	at org.apache.curator.framework.recipes.locks.InterProcessSemaphoreV2.acquire(InterProcessSemaphoreV2.java:284)
	at org.apache.curator.framework.recipes.locks.InterProcessSemaphoreV2.acquire(InterProcessSemaphoreV2.java:212)
	at org.apache.curator.framework.recipes.locks.InterProcessSemaphoreMutex.acquire(InterProcessSemaphoreMutex.java:46)
	at com.metamx.tranquility.beam.ClusteredBeam$$anon$1$$anonfun$modify$1.apply(ClusteredBeam.scala:153)
	at com.metamx.tranquility.beam.ClusteredBeam$$anon$1$$anonfun$modify$1.apply(ClusteredBeam.scala:152)
	at com.twitter.util.Try$.apply(Try.scala:13)
	... 6 more
java.lang.IllegalStateException: Failed to create merged beam: druid:overlord/dwarch_ad_local_abtest_lit
	at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$sendAll$2$$anonfun$26.apply(ClusteredBeam.scala:383)
	at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$sendAll$2$$anonfun$26.apply(ClusteredBeam.scala:379)
	at com.twitter.util.Promise$Transformer.liftedTree1$1(Promise.scala:112)
	at com.twitter.util.Promise$Transformer.k(Promise.scala:112)
	at com.twitter.util.Promise$Transformer.apply(Promise.scala:122)
	at com.twitter.util.Promise$Transformer.apply(Promise.scala:103)
	at com.twitter.util.Promise$$anon$1.run(Promise.scala:366)
	at com.twitter.concurrent.LocalScheduler$Activation.run(Scheduler.scala:178)
	at com.twitter.concurrent.LocalScheduler$Activation.submit(Scheduler.scala:136)
	at com.twitter.concurrent.LocalScheduler.submit(Scheduler.scala:207)
	at com.twitter.concurrent.Scheduler$.submit(Scheduler.scala:92)
	at com.twitter.util.Promise.runq(Promise.scala:350)
	at com.twitter.util.Promise.updateIfEmpty(Promise.scala:721)
	at com.twitter.util.ExecutorServiceFuturePool$$anon$2.run(FuturePool.scala:107)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalStateException: Failed to save new beam for identifier[druid:overlord/dwarch_ad_local_abtest_lit] timestamp[2019-07-19T09:00:00.000Z]
	at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$2.applyOrElse(ClusteredBeam.scala:289)
	at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$2.applyOrElse(ClusteredBeam.scala:286)
	at com.twitter.util.Future$$anonfun$rescue$1.apply(Future.scala:924)
	at com.twitter.util.Future$$anonfun$rescue$1.apply(Future.scala:922)
	... 17 more
Caused by: java.lang.NullPointerException
	at org.apache.curator.framework.recipes.locks.InterProcessSemaphoreV2.acquire(InterProcessSemaphoreV2.java:213)
	at org.apache.curator.framework.recipes.locks.InterProcessSemaphoreMutex.acquire(InterProcessSemaphoreMutex.java:46)
	at com.metamx.tranquility.beam.ClusteredBeam$$anon$1$$anonfun$modify$1.apply(ClusteredBeam.scala:153)
	at com.metamx.tranquility.beam.ClusteredBeam$$anon$1$$anonfun$modify$1.apply(ClusteredBeam.scala:152)
	at com.twitter.util.Try$.apply(Try.scala:13)
	... 6 more
java.lang.IllegalStateException: Failed to create merged beam: druid:overlord/dwarch_ad_local_abtest_lit
	at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$sendAll$2$$anonfun$26.apply(ClusteredBeam.scala:383)
	at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$sendAll$2$$anonfun$26.apply(ClusteredBeam.scala:379)
	at com.twitter.util.Promise$Transformer.liftedTree1$1(Promise.scala:112)
	at com.twitter.util.Promise$Transformer.k(Promise.scala:112)
	at com.twitter.util.Promise$Transformer.apply(Promise.scala:122)
	at com.twitter.util.Promise$Transformer.apply(Promise.scala:103)
	at com.twitter.util.Promise$$anon$1.run(Promise.scala:366)
	at com.twitter.concurrent.LocalScheduler$Activation.run(Scheduler.scala:178)
	at com.twitter.concurrent.LocalScheduler$Activation.submit(Scheduler.scala:136)
	at com.twitter.concurrent.LocalScheduler.submit(Scheduler.scala:207)
	at com.twitter.concurrent.Scheduler$.submit(Scheduler.scala:92)
	at com.twitter.util.Promise.runq(Promise.scala:350)
	at com.twitter.util.Promise.updateIfEmpty(Promise.scala:721)
	at com.twitter.util.ExecutorServiceFuturePool$$anon$2.run(FuturePool.scala:107)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalStateException: Failed to save new beam for identifier[druid:overlord/dwarch_ad_local_abtest_lit] timestamp[2019-07-19T09:00:00.000Z]
	at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$2.applyOrElse(ClusteredBeam.scala:289)
	at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$2.applyOrElse(ClusteredBeam.scala:286)
	at com.twitter.util.Future$$anonfun$rescue$1.apply(Future.scala:924)
	at com.twitter.util.Future$$anonfun$rescue$1.apply(Future.scala:922)
	... 17 more
Caused by: java.lang.NullPointerException
	at org.apache.curator.framework.recipes.locks.InterProcessSemaphoreV2.acquire(InterProcessSemaphoreV2.java:213)
	at org.apache.curator.framework.recipes.locks.InterProcessSemaphoreMutex.acquire(InterProcessSemaphoreMutex.java:46)
	at com.metamx.tranquility.beam.ClusteredBeam$$anon$1$$anonfun$modify$1.apply(ClusteredBeam.scala:153)
	at com.metamx.tranquility.beam.ClusteredBeam$$anon$1$$anonfun$modify$1.apply(ClusteredBeam.scala:152)
	at com.twitter.util.Try$.apply(Try.scala:13)
	... 6 more
java.lang.IllegalStateException: Failed to create merged beam: druid:overlord/dwarch_ad_local_abtest_lit
	at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$sendAll$2$$anonfun$26.apply(ClusteredBeam.scala:383)
	at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$sendAll$2$$anonfun$26.apply(ClusteredBeam.scala:379)
	at com.twitter.util.Promise$Transformer.liftedTree1$1(Promise.scala:112)
	at com.twitter.util.Promise$Transformer.k(Promise.scala:112)
	at com.twitter.util.Promise$Transformer.apply(Promise.scala:122)
	at com.twitter.util.Promise$Transformer.apply(Promise.scala:103)
	at com.twitter.util.Promise$$anon$1.run(Promise.scala:366)
	at com.twitter.concurrent.LocalScheduler$Activation.run(Scheduler.scala:178)
	at com.twitter.concurrent.LocalScheduler$Activation.submit(Scheduler.scala:136)
	at com.twitter.concurrent.LocalScheduler.submit(Scheduler.scala:207)
	at com.twitter.concurrent.Scheduler$.submit(Scheduler.scala:92)
	at com.twitter.util.Promise.runq(Promise.scala:350)
	at com.twitter.util.Promise.updateIfEmpty(Promise.scala:721)
	at com.twitter.util.ExecutorServiceFuturePool$$anon$2.run(FuturePool.scala:107)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalStateException: Failed to save new beam for identifier[druid:overlord/dwarch_ad_local_abtest_lit] timestamp[2019-07-19T09:00:00.000Z]
	at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$2.applyOrElse(ClusteredBeam.scala:289)
	at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$2.applyOrElse(ClusteredBeam.scala:286)
	at com.twitter.util.Future$$anonfun$rescue$1.apply(Future.scala:924)
	at com.twitter.util.Future$$anonfun$rescue$1.apply(Future.scala:922)
	... 17 more
Caused by: java.lang.NullPointerException
	at org.apache.curator.framework.recipes.locks.InterProcessSemaphoreV2.acquire(InterProcessSemaphoreV2.java:213)
	at org.apache.curator.framework.recipes.locks.InterProcessSemaphoreMutex.acquire(InterProcessSemaphoreMutex.java:46)
	at com.metamx.tranquility.beam.ClusteredBeam$$anon$1$$anonfun$modify$1.apply(ClusteredBeam.scala:153)
	at com.metamx.tranquility.beam.ClusteredBeam$$anon$1$$anonfun$modify$1.apply(ClusteredBeam.scala:152)
	at com.twitter.util.Try$.apply(Try.scala:13)
	... 6 more
java.lang.IllegalStateException: Failed to create merged beam: druid:overlord/dwarch_ad_local_abtest_lit
	at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$sendAll$2$$anonfun$26.apply(ClusteredBeam.scala:383)
	at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$sendAll$2$$anonfun$26.apply(ClusteredBeam.scala:379)
	at com.twitter.util.Promise$Transformer.liftedTree1$1(Promise.scala:112)
	at com.twitter.util.Promise$Transformer.k(Promise.scala:112)
	at com.twitter.util.Promise$Transformer.apply(Promise.scala:122)
	at com.twitter.util.Promise$Transformer.apply(Promise.scala:103)
	at com.twitter.util.Promise$$anon$1.run(Promise.scala:366)
	at com.twitter.concurrent.LocalScheduler$Activation.run(Scheduler.scala:178)
	at com.twitter.concurrent.LocalScheduler$Activation.submit(Scheduler.scala:136)
	at com.twitter.concurrent.LocalScheduler.submit(Scheduler.scala:207)
	at com.twitter.concurrent.Scheduler$.submit(Scheduler.scala:92)
	at com.twitter.util.Promise.runq(Promise.scala:350)
	at com.twitter.util.Promise.updateIfEmpty(Promise.scala:721)
	at com.twitter.util.ExecutorServiceFuturePool$$anon$2.run(FuturePool.scala:107)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalStateException: Failed to save new beam for identifier[druid:overlord/dwarch_ad_local_abtest_lit] timestamp[2019-07-19T09:00:00.000Z]
	at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$2.applyOrElse(ClusteredBeam.scala:289)
	at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$2.applyOrElse(ClusteredBeam.scala:286)
	at com.twitter.util.Future$$anonfun$rescue$1.apply(Future.scala:924)
	at com.twitter.util.Future$$anonfun$rescue$1.apply(Future.scala:922)
	... 17 more
Caused by: java.lang.NullPointerException
	at org.apache.curator.framework.recipes.locks.InterProcessSemaphoreV2.acquire(InterProcessSemaphoreV2.java:213)
	at org.apache.curator.framework.recipes.locks.InterProcessSemaphoreMutex.acquire(InterProcessSemaphoreMutex.java:46)
	at com.metamx.tranquility.beam.ClusteredBeam$$anon$1$$anonfun$modify$1.apply(ClusteredBeam.scala:153)
	at com.metamx.tranquility.beam.ClusteredBeam$$anon$1$$anonfun$modify$1.apply(ClusteredBeam.scala:152)
	at com.twitter.util.Try$.apply(Try.scala:13)
	... 6 more

I need 
select avg(latencyMs),min(latencyMs),sum(latencyMs) from mytable

so make config
I can only do that
```
 "metricsSpec": [
       {"name": "views", "type": "count"},
       {"name": "latencyMs", "type": "doubleSum", "fieldName": "latencyMs"}
  ]
```
or 
I should do this
```
 "metricsSpec": [
       {"name": "views", "type": "count"},
       {"name": "latencyMs", "type": "doubleSum", "fieldName": "latencyMs"},
       {"name": "latencyMs", "type": "doubleMin", "fieldName": "latencyMs"},
  ]
```
and what about avg?
Exception in thread "main" org.apache.flink.api.common.InvalidProgramException: The implementation of the RichSinkFunction is not serializable. The object probably contains or references non serializable fields.
	at org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:99)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.clean(StreamExecutionEnvironment.java:1558)
	at org.apache.flink.streaming.api.datastream.DataStream.clean(DataStream.java:185)
	at org.apache.flink.streaming.api.datastream.DataStream.addSink(DataStream.java:1227)
	at com.ke.bigdata.dtarch.flink.druid.sink.DruidSink.emitDataStream(DruidSink.java:78)
	at org.apache.flink.table.api.StreamTableEnvironment.writeToSink(StreamTableEnvironment.scala:347)
	at org.apache.flink.table.api.TableEnvironment.insertInto(TableEnvironment.scala:879)
	at org.apache.flink.table.api.TableEnvironment.sqlUpdate(TableEnvironment.scala:817)
	at org.apache.flink.table.api.TableEnvironment.sqlUpdate(TableEnvironment.scala:777)
	at com.ke.bigdata.dtarch.flink.Main.executeSql(Main.java:168)
	at com.ke.bigdata.dtarch.flink.Main.main(Main.java:85)
Caused by: java.io.NotSerializableException: io.druid.query.aggregation.CountAggregatorFactory
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1184)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:576)
	at org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:81)
	... 10 more
Every time a new connection is established in IndexService, there is a DruidTaskResolver submitting a runnable to a new thread (poll thread) bind to the connection and periodically check the task running status, but when the beam is closed, the poll runnable cannot be notified and cancled properly. This caused a thread leakage in a long-last running jvm.
After upgrading all plugins to newer versions and changing certain dependencies-
**_diff-_**
diff --git a/build.sbt b/build.sbt
index f4151c0..940eae5 100644
--- a/build.sbt
+++ b/build.sbt
@@ -1,4 +1,4 @@
-scalaVersion in ThisBuild := "2.11.8"
+scalaVersion in ThisBuild := "2.12.1"
 
 // Disable parallel execution, the various Druid oriented tests need to claim ports
 parallelExecution in ThisBuild := false
@@ -11,7 +11,7 @@ concurrentRestrictions in Global += Tags.limitAll(1)
 val jacksonOneVersion = "1.9.13"
 // See https://github.com/druid-io/druid/pull/1669, https://github.com/druid-io/tranquility/pull/81 before upgrading Jackson
 val jacksonTwoVersion = "2.4.6"
-val jacksonTwoModuleScalaVersion = "2.4.5"
+val jacksonTwoModuleScalaVersion = "2.9.9"
 val druidVersion = "0.9.2"
 val curatorVersion = "2.12.0"
 val guiceVersion = "4.0"
@@ -45,11 +45,11 @@ def dependOnDruid(artifact: String) = {
 }
 
 val coreDependencies = Seq(
-  "com.metamx" %% "scala-util" % "1.13.6"
+  "com.metamx" %% "scala-util" % "1.14.1"
     exclude("log4j", "log4j")
     exclude("mysql", "mysql-connector-java") // Not needed, unwanted GPLv2 license
     force(),
-  "com.metamx" % "java-util" % "0.28.2" exclude("log4j", "log4j") force(),
+  "com.metamx" % "java-util" % "1.3.6" exclude("log4j", "log4j") force(),
   "io.netty" % "netty" % "3.10.5.Final" force(),
   "org.apache.curator" % "curator-client" % curatorVersion force(),
   "org.apache.curator" % "curator-framework" % curatorVersion force(),
@@ -139,7 +139,7 @@ val kafkaDependencies = Seq(
 ) ++ loggingDependencies
 
 val coreTestDependencies = Seq(
-  "org.scalatest" %% "scalatest" % "2.2.5" % "test",
+  "org.scalatest" %% "scalatest" % "3.0.8" % "test",
   dependOnDruid("druid-services") % "test",
   "org.apache.curator" % "curator-test" % curatorVersion % "test" exclude("log4j", "log4j") force(),
   "com.sun.jersey" % "jersey-servlet" % "1.17.1" % "test" force(),
@@ -184,7 +184,7 @@ lazy val commonSettings = Seq(
 
   // Target Java 7
   scalacOptions += "-target:jvm-1.7",
-  javacOptions in compile ++= Seq("-source", "1.7", "-target", "1.7"),
+  javacOptions in compile ++= Seq("-source", "1.7", "-target", "11"),
 
   // resolve-term-conflict:object since storm-core has a package and object with the same name
   scalacOptions := Seq("-feature", "-deprecation", "-Yresolve-term-conflict:object"),
@@ -220,7 +220,7 @@ lazy val commonSettings = Seq(
 lazy val root = project.in(file("."))
   .settings(commonSettings: _*)
   .settings(publishArtifact := false)
-  .aggregate(core, flink, storm, samza, spark, server, kafka)
+  .aggregate(core)
 
 lazy val core = project.in(file("core"))
   .settings(commonSettings: _*)
@@ -228,52 +228,52 @@ lazy val core = project.in(file("core"))
   .settings(publishArtifact in(Test, packageBin) := true)
   .settings(libraryDependencies ++= (coreDependencies ++ coreTestDependencies))
 
-lazy val flink = project.in(file("flink"))
-  .settings(commonSettings: _*)
-  .settings(name := "tranquility-flink")
-  .settings(libraryDependencies ++= (flinkDependencies ++ flinkTestDependencies))
-  .dependsOn(core % "test->test;compile->compile")
-
-lazy val spark = project.in(file("spark"))
-  .settings(commonSettings: _*)
-  .settings(name := "tranquility-spark")
-  .settings(libraryDependencies ++= sparkDependencies)
-  .dependsOn(core % "test->test;compile->compile")
-
-lazy val storm = project.in(file("storm"))
-  .settings(commonSettings: _*)
-  .settings(name := "tranquility-storm")
-  .settings(resolvers += "clojars" at "http://clojars.org/repo/")
-  .settings(libraryDependencies ++= stormDependencies)
-  .dependsOn(core % "test->test;compile->compile")
-
-lazy val samza = project.in(file("samza"))
-  .settings(commonSettings: _*)
-  .settings(name := "tranquility-samza")
-  .settings(libraryDependencies ++= (samzaDependencies ++ samzaTestDependencies))
-  .settings(publishArtifact in Test := false)
-  .dependsOn(core % "test->test;compile->compile")
-
-lazy val server = project.in(file("server"))
-  .settings(commonSettings: _*)
-  .settings(name := "tranquility-server")
-  .settings(libraryDependencies ++= (serverDependencies ++ serverTestDependencies))
-  .settings(publishArtifact in Test := false)
-  .dependsOn(core % "test->test;compile->compile")
-
-lazy val kafka = project.in(file("kafka"))
-  .settings(commonSettings: _*)
-  .settings(name := "tranquility-kafka")
-  .settings(libraryDependencies ++= (kafkaDependencies ++ kafkaTestDependencies))
-  .settings(publishArtifact in Test := false)
-  .dependsOn(core % "test->test;compile->compile")
-
-lazy val distribution = project.in(file("distribution"))
-  .settings(commonSettings: _*)
-  .settings(name := "tranquility-distribution")
-  .settings(publishArtifact in Test := false)
-  .settings(mainClass in Compile := Some("com.metamx.tranquility.distribution.DistributionMain"))
-  .settings(executableScriptName := "tranquility")
-  .settings(bashScriptExtraDefines += """addJava "-Dlogback.configurationFile=${app_home}/../conf/logback.xml"""")
-  .enablePlugins(JavaAppPackaging)
-  .dependsOn(kafka, server)
+// lazy val flink = project.in(file("flink"))
+//   .settings(commonSettings: _*)
+//   .settings(name := "tranquility-flink")
+//   .settings(libraryDependencies ++= (flinkDependencies ++ flinkTestDependencies))
+//   .dependsOn(core % "test->test;compile->compile")
+// 
+// lazy val spark = project.in(file("spark"))
+//   .settings(commonSettings: _*)
+//   .settings(name := "tranquility-spark")
+//   .settings(libraryDependencies ++= sparkDependencies)
+//   .dependsOn(core % "test->test;compile->compile")
+// 
+// lazy val storm = project.in(file("storm"))
+//   .settings(commonSettings: _*)
+//   .settings(name := "tranquility-storm")
+//   .settings(resolvers += "clojars" at "http://clojars.org/repo/")
+//   .settings(libraryDependencies ++= stormDependencies)
+//   .dependsOn(core % "test->test;compile->compile")
+// 
+// lazy val samza = project.in(file("samza"))
+//   .settings(commonSettings: _*)
+//   .settings(name := "tranquility-samza")
+//   .settings(libraryDependencies ++= (samzaDependencies ++ samzaTestDependencies))
+//   .settings(publishArtifact in Test := false)
+//   .dependsOn(core % "test->test;compile->compile")
+// 
+// lazy val server = project.in(file("server"))
+//   .settings(commonSettings: _*)
+//   .settings(name := "tranquility-server")
+//   .settings(libraryDependencies ++= (serverDependencies ++ serverTestDependencies))
+//   .settings(publishArtifact in Test := false)
+//   .dependsOn(core % "test->test;compile->compile")
+// 
+// lazy val kafka = project.in(file("kafka"))
+//   .settings(commonSettings: _*)
+//   .settings(name := "tranquility-kafka")
+//   .settings(libraryDependencies ++= (kafkaDependencies ++ kafkaTestDependencies))
+//   .settings(publishArtifact in Test := false)
+//   .dependsOn(core % "test->test;compile->compile")
+// 
+// lazy val distribution = project.in(file("distribution"))
+//   .settings(commonSettings: _*)
+//   .settings(name := "tranquility-distribution")
+//   .settings(publishArtifact in Test := false)
+//   .settings(mainClass in Compile := Some("com.metamx.tranquility.distribution.DistributionMain"))
+//   .settings(executableScriptName := "tranquility")
+//   .settings(bashScriptExtraDefines += """addJava "-Dlogback.configurationFile=${app_home}/../conf/logback.xml"""")
+//   .enablePlugins(JavaAppPackaging)
+//   .dependsOn(kafka, server)
diff --git a/project/build.properties b/project/build.properties
index 9cc0dea..e326436 100644
--- a/project/build.properties
+++ b/project/build.properties
@@ -17,4 +17,4 @@
-sbt.version=0.13.13
+sbt.version=1.2.8
diff --git a/project/plugins.sbt b/project/plugins.sbt
index 1363b1a..f0e8230 100644
--- a/project/plugins.sbt
+++ b/project/plugins.sbt
@@ -2,10 +2,10 @@ resolvers ++= Seq(
   "Central" at "https://oss.sonatype.org/content/repositories/releases/"
 )
 
-addSbtPlugin("net.virtual-void" % "sbt-dependency-graph" % "0.8.2")
+addSbtPlugin("net.virtual-void" % "sbt-dependency-graph" % "0.9.2")
 
-addSbtPlugin("com.github.gseitz" % "sbt-release" % "1.0.4")
+addSbtPlugin("com.github.gseitz" % "sbt-release" % "1.0.11")
 
-addSbtPlugin("com.jsuereth" % "sbt-pgp" % "1.0.0")
+addSbtPlugin("com.jsuereth" % "sbt-pgp" % "2.0.0-M1")
 
-addSbtPlugin("com.typesafe.sbt" % "sbt-native-packager" % "1.0.5")
+addSbtPlugin("com.typesafe.sbt" % "sbt-native-packager" % "1.3.22")








**_logs-_**
**[info] Loading settings for project tranquility-build from plugins.sbt ...
[info] Loading project definition from /home/suryansh/workspace/sonar-test/tranquility/project
[info] Loading settings for project root from build.sbt,version.sbt ...
[info] Set current project to root (in build file:/home/suryansh/workspace/sonar-test/tranquility/)
[info] Setting Scala version to 2.12.1 on 2 projects.
[info] Reapplying settings...
[info] Set current project to root (in build file:/home/suryansh/workspace/sonar-test/tranquility/)
[info] Compiling 54 Scala sources to /home/suryansh/workspace/sonar-test/tranquility/core/target/scala-2.12/classes ...
[warn] /home/suryansh/workspace/sonar-test/tranquility/core/src/main/scala/com/metamx/tranquility/druid/DruidBeams.scala:166:39: non-variable type argument java.nio.ByteBuffer in type io.druid.data.input.impl.InputRowParser[java.nio.ByteBuffer] is unchecked since it is eliminated by erasure
[warn]               trialParser.isInstanceOf[InputRowParser[ByteBuffer]],
[warn]                                       ^
[warn] /home/suryansh/workspace/sonar-test/tranquility/core/src/main/scala/com/metamx/tranquility/druid/DruidBeams.scala:175:39: non-variable type argument java.nio.ByteBuffer in type io.druid.data.input.impl.InputRowParser[java.nio.ByteBuffer] is unchecked since it is eliminated by erasure
[warn]               trialParser.isInstanceOf[InputRowParser[ByteBuffer]],
[warn]                                       ^
[error] /home/suryansh/workspace/sonar-test/tranquility/core/src/main/scala/com/metamx/tranquility/druid/DruidGuicer.scala:71:20: ambiguous reference to overloaded definition,
[error] both method putAll in class Properties of type (x$1: java.util.Map[_, _])Unit
[error] and  method putAll in class Hashtable of type (x$1: java.util.Map[_ <: Object, _ <: Object])Unit
[error] match argument types (java.util.Properties)
[error]           theProps.putAll(props)
[error]                    ^
[error] /home/suryansh/workspace/sonar-test/tranquility/core/src/main/scala/com/metamx/tranquility/druid/DruidGuicer.scala:72:20: ambiguous reference to overloaded definition,
[error] both method putAll in class Properties of type (x$1: java.util.Map[_, _])Unit
[error] and  method putAll in class Hashtable of type (x$1: java.util.Map[_ <: Object, _ <: Object])Unit
[error] match argument types (java.util.Properties)
[error]           theProps.putAll(System.getProperties)
[error]                    ^
[warn] two warnings found
[error] two errors found
[error] (core / Compile / compileIncremental) Compilation failed
[error] Total time: 13 s, completed 20-Jun-2019, 4:19:29 PM**

From this report: https://snyk.io/test/github/druid-io/tranquility, there are 36 high severity CVEs and 31 medium severity CVEs in tranquility dependencies. Most of these are related to Jackson and Jetty. 

It would be great to see these upgraded for a new release.