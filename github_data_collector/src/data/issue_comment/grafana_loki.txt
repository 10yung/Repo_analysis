**Describe the bug**

Loki & promtail daemons crash eventually.

**To Reproduce**

Steps to reproduce the behavior:
1. Started Loki (SHA or version): 1.2.0
1. Started Promtail (SHA or version): 1.2.0

I have a very basic setup — everything is stored on `localhost`.

When I look at the logs of loki, I see lots of messages about out of order events. I believe this happens as I currently have persistence configured for `journald` (to `/var/log/journal`) and whenever it starts up, it tries to ingest the entire directory, not just the most recent/current journal.

Another thing I noticed is that eventually the ingester is gone. But I haven't been able to track as to why this happened. Maybe the amount of journal is the problem? It's roughly 700 MB, the VM has plenty of RAM (8 GiB) and 4 vCPUs.

Promtail crashes eventually as well, after — I think — too many "400" errors from loki, which I guess is related, but also undesired behaviour. Should I make another ticket for that?

**Expected behavior**

I expect them to be running and working.

**Environment:**
 - Infrastructure: VM (on bare-metal HV)
 - Deployment tool: Ansible
 - OS: CentOS7 (`CentOS Linux release 7.7.1908 (Core)`, kernel: `3.10.0`)

**Screenshots, Promtail config, or terminal output**

Loki config:

```
auth_enabled: True

server:
  http_listen_address: 127.0.0.1
  http_listen_port: 3100
  grpc_listen_address: 127.0.0.1
  log_level: debug

ingester:
  lifecycler:
    address: 127.0.0.1
    ring:
      kvstore:
        store: inmemory
      replication_factor: 1
    final_sleep: 0s
  chunk_idle_period: 5m
  chunk_retain_period: 30s

schema_config:
  configs:
  - from: 2018-04-15
    store: boltdb
    object_store: filesystem
    schema: v9
    index:
      prefix: loki_index_
      period: 168h

storage_config:
  boltdb:
    directory: /tmp/loki/index

  filesystem:
    directory: /tmp/loki/chunks

limits_config:
  enforce_metric_name: false
  reject_old_samples: true
  reject_old_samples_max_age: 168h

chunk_store_config:
  max_look_back_period: 840h

table_manager:
  chunk_tables_provisioning:
    inactive_read_throughput: 0
    inactive_write_throughput: 0
    provisioned_read_throughput: 0
    provisioned_write_throughput: 0
  index_tables_provisioning:
    inactive_read_throughput: 0
    inactive_write_throughput: 0
    provisioned_read_throughput: 0
    provisioned_write_throughput: 0
  retention_deletes_enabled: true
  retention_period: 840h
```

Promtail config:

```
server:
  http_listen_adress: 127.0.0.1
  http_listen_port: 9080
  grpc_listen_address: 127.0.0.1
  grpc_listen_port: 0
  log_level: debug

# Positions
positions:
  filename: /tmp/positions.yaml

# Loki Server URL
clients:
  - url: http://127.0.0.1:3100/loki/api/v1/push
    tenant_id: till
    external_labels:
      node: fully.qualified.domain.name
      customer: till

scrape_configs:
  - job_name: journal
    journal:
      path: /var/log/journal
      labels:
        job: journal
    relabel_configs:
      - source_labels: ['__journal__systemd_unit']
        target_label: 'unit'
```
I'd love to use Loki in a distributed system easier and without being forced relatively high cardinality labels based on something like process ID. This goes double for systems like AWS Lambda. 

This main obstacle to this for me is being unable to submit 'out of order' log lines, it would be great if loki could have a feature that would enable this.

At one point I found an old issue relating to this request but it was closed with "not something we need before releasing". Perhaps it is time to revisit this?

Cheers
A common way to match multiple simple words in either label matching or log text is to use a regex filter, `|~ "first-term|second-term"` for example. This is expensive to compute as the regex engine can be slow.

Specifically for this case where the regex is a simple string contains another string, break the query into n queries where n is the number of terms provided, using the faster `byte.contains` method to match, then combine the results. This would be as if the user searched first on `|= "first-term"` and then on `|= "second-term"` and then manually combined the results, but automatically converted and computed when executing the original `|~ "first-term|second-term"`.
If Promtail happens to crash when it has a batch of logs queued to send to Loki, the logs will be lost. While the file and journal targets have their last read positions saved in the positions file, this file is updated before Promtail guarantees the batch was sent. 

One potential implementation to solve this problem is a WAL.
Promtail is unable to install using helm charts.    I am getting the error below in logs of all pods. 

 `Not ready: Unable to find any logs to tail. Please verify permissions, volumes, scrape_config, etc.\\n\" ws: false; Accept-Encoding: gzip; Connection: close; User-Agent: kube-probe/1.12+`

I was trying to install it on rancher kubernetes. its creating the pod but not initializing it. 
<img width="576" alt="Capture" src="https://user-images.githubusercontent.com/46532977/72556754-23f6b300-3854-11ea-9ece-a2e1490931c5.PNG">


Getting a warning message on kubernetes dashboard like 
`Search Line limits were exceeded, some search paths have been omitted, the applied search line is:default.svc.cluster.local svc.cluster.local cluster.local rancher.internal 
Readiness probe failed: HTTP probe failed with statuscode: 500`
**Is your feature request related to a problem? Please describe.**
I want to use the Loki Docker driver to get my logs from [Amazon ECS](https://aws.amazon.com/ecs/) into Loki, but there isn't an easy way to do so.

**Describe the solution you'd like**
The Loki Docker driver is easily usable with Amazon ECS.

**Describe alternatives you've considered**
The solution suggested by Amazon in the [LogConfiguration documentation](https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_LogConfiguration.html) is to fork the ECS agent and run a custom build.  You can also work around it by setting the Loki driver as the default Docker logging driver ([docs](https://docs.docker.com/config/containers/logging/configure/)) on your EC2 instance, then providing no logging configuration in your ECS task.

**Additional context**
This isn't really a problem that can be fixed within Loki, but this issue provides a reference point for people who are interested in using Loki with ECS or attempting to find more information.

It may make sense for a Loki maintainer to open a PR to add support to the [ECS agent](https://github.com/aws/amazon-ecs-agent).

This issue would also be resolved by the completion of https://github.com/aws/containers-roadmap/issues/435, which allows the usage of arbitrary custom log drivers with ECS.

The documentation added in #1531 should also be updated once a better solution is available.

* Make Promtail's Helm chart support passing extra env variables
* bump charts versions

<!--  Thanks for sending a pull request!  Before submitting:

1. Read our CONTRIBUTING.md guide
2. Name your PR as `<Feature Area>: Describe your change`
3. Rebase your PR if it gets out of sync with master
4. If changing the Helm chart, please ensure the chart version is increased per semantic versioning (https://semver.org)
-->

**What this PR does / why we need it**:
This PR enables developers to pass environment variables to the `promtail` containers via helm similarly to how it's done for [loki](https://github.com/grafana/loki/blob/master/production/helm/loki/values.yaml#L219)

**Which issue(s) this PR fixes**:
Fixes #<issue number>

**Special notes for your reviewer**:

**Checklist**
- [ ] Documentation added
- [ ] Tests updated


**Describe the bug**
I see a lot of 500 `status_codes` in s3 metrics (`cortex_s3_request_duration_seconds_bucket`). Strange thing is that we do not see those requests on the storage backend. I see that timeout for s3 is not configurable but I think there is some default which reports as 500 in this metric. 

**Expected behavior**
I would like to be able to set storage timeout

**Environment:**
K8S + Rados Gateway (Ceph)


**Describe the bug**
Got error below when running tk apply environments/loki
The content of "tk show" shows that there is no selector in promtail daemonset part

```shell
tk apply environments/loki
The DaemonSet "promtail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"name":"promtail"}: `selector` does not match template `labels`
Warning: There are no differences. Your apply may not do anything at all.
Applying to namespace 'loki' of cluster 'cluster.local' at 'https://xxx:6443' using context 'kubernetes-admin@cluster.local'.
Please type 'yes' to confirm: no
aborted by user
```

**Describe the bug**
When turning on AutoKubernetesLabels, fluent-bit log returns panic as follow



**Expected behavior**
Successfully see all logs in Loki

**Environment:**
 - Infrastructure: minikube v1.6.2
 - Deployment tool: kustomize

**Screenshots, Promtail config, or terminal output**

kubectl logs output
```
[2020/01/15 08:28:55] [ info] [filter_kube] https=1 host=kubernetes.default.svc port=443
[2020/01/15 08:28:55] [ info] [filter_kube] local POD info OK
[2020/01/15 08:28:55] [ info] [filter_kube] testing connectivity with API server...
[2020/01/15 08:28:55] [ info] [filter_kube] API server connectivity OK
[2020/01/15 08:28:55] [ info] [sp] stream processor started
panic: interface conversion: interface {} is map[string]interface {}, not map[interface {}]interface {}
```

fluent-bit.conf:

```
[SERVICE]
    Flush        1
    Daemon       Off
    Log_Level    info
    Parsers_File parsers.conf

[INPUT]
  Name tail
  Path /var/log/containers/*.log
  Parser docker
  Tag kube.*
  Refresh_Interval 5
  Mem_Buf_Limit 5MB
  Skip_Long_Lines On
  DB /run/fluent-bit/flb_kube.db

[INPUT]
  Name systemd
  Tag host.*
  Systemd_Filter _SYSTEMD_UNIT=docker.service
  Systemd_Filter _SYSTEMD_UNIT=kubelet.service
  Systemd_Filter _SYSTEMD_UNIT=node-problem-detector.service
  Max_Entries 1000
  Read_From_Tail true
  Strip_Underscores false

[FILTER]
  Name kubernetes
  Match kube.*
  Kube_Tag_Prefix kube.var.log.containers.
  Kube_URL https://kubernetes.default.svc:443
  Kube_CA_File /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
  Kube_Token_File /var/run/secrets/kubernetes.io/serviceaccount/token
  Merge_Log On
  K8S-Logging.Parser Off

[OUTPUT]
  Name loki
  Match kube.*
  Url http://loki.gohub-gohub.svc:3100/api/prom/push
  # Labels {job="fluent-bit"}
  # RemoveKeys kubernetes,stream
  AutoKubernetesLabels true
  # LabelMapPath /fluent-bit/etc/labelmap.json
  # LineFormat json
  # LogLevel warn
```

parsers.conf:

```
[PARSER]
  Name         docker
  Format       json
  Time_Key     time
  Time_Format  %Y-%m-%dT%H:%M:%S.%L
```