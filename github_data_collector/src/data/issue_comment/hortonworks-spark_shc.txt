â€¦-alphabetic splits can be easily supported.

## What changes were proposed in this pull request?

Adds two properties that allow the first and last split to be set by the user. This allows the use of more complex split preferences, as well as enables scenarios such as numeric-only keys, which were underserved in earlier versions.

## How was this patch tested?

Two unit tests were added. One tests that for alphabetic keys, the rows are dispersed between regions correctly. The other uses numeric keys and sets the first and last splits accordingly. This also checks that the rows have been dispersed appropriately across regions.

If you have keys that start exclusively with numeric values, all rows will be put into a single region. That region will be split as necessary based on the settings in HBase, but the other regions will remain empty. For example, if I request 100 regions, it will create 100 regions between "aaaaaa" and "zzzzzz". If I then add 1 million rows with numeric keys, all of them will go into the first region. If I add more rows, then the HBase cluster may decide to split the first region, but the other regions will remain empty. 

I've included some source code below that I've written which demonstrates this issue easily. Increasing the number of samples allows you to see the new regions were created, but even at 1 million, you wouldn't expect the minimum and maximum keys to be in the same region if you asked for 100 regions.

```
val numValues = 1000000
val startValue = 48000000

val df = spark.range(startValue, startValue + numValues).select($"id".cast("string") as "pat_id", lit("RUL_ID_VALUE") as "rul_id", lit("ROW_KEY_VALUE") as "rowkey")

/* Cast DataTypes */

val cir_fitered_data_DF = df.select(
  $"pat_id" as "fullrowkey",
  $"rul_id",
  $"rowkey" as "ROW_KEY");

//Schema to load into HBase perf table

def mbr_cir_load_hbase_catalog =
  s"""{
     |"table":{"namespace": "u405317_sandbox", "name":"PerfTest"},
     |"rowkey":"key",
     |"columns":{
     |"fullrowkey":{"cf":"rowkey", "col":"key", "type":"string"},
     |"rul_id":{"cf":"P1", "col":"rul_id", "type":"string"},
     |"ROW_KEY":{"cf":"P1", "col":"rowkey", "type":"string"}
     |}
     |}""".stripMargin

cir_fitered_data_DF.show(false);

cir_fitered_data_DF.write.options(Map(HBaseTableCatalog.tableCatalog -> mbr_cir_load_hbase_catalog, HBaseTableCatalog.newTable -> "100")).format("org.apache.spark.sql.execution.datasources.hbase").save();
```


 I find jar in https://repo.hortonworks.com/content/groups/public/com/hortonworks/shc/

I don't know which one is for the spark 2.3.2 hbase 2.0.2  can be used can you make a list
![image](https://user-images.githubusercontent.com/8144089/69122783-a96f1880-0ada-11ea-8aed-bcdd03188c68.png)

```
        <dependency>
            <groupId>com.hortonworks</groupId>
            <artifactId>shc-core</artifactId>
            <version>1.1.1-2.1-s_2.11</version>
        </dependency>

```

Is this the latest version?
in thread "main" java.lang.NoSuchMethodError: org.apache.spark.sql.internal.SQLConf$.LEGACY_PASS_PARTITION_BY_AS_OPTIONS()Lorg/apache/spark/internal/config/ConfigEntry;
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:277)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)
Guys,

Here's what I did - 
1. Forked the package, made adjustments in HBaseRelation.scala so that while using this for Google BigTable the class must not call methods of Namespaces viz. getNamespaceDescriptor, createNamespaces etc. [Check this](https://cloud.google.com/bigtable/docs/hbase-differences)
2. Compiled my version of package, without big fat jar file, it's become hardly 600 KB
3. Bundled this package in my code where I'm using SHC for DF write operations as HBase format
4. If I run the job I see issues like - 
java.lang.NoClassDefFoundError: org/apache/hadoop/hbase/client/TableDescriptor
at org.apache.spark.sql.execution.datasources.hbase.DefaultSource.createRelation(HBaseRelation.scala:61)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.client.TableDescriptor

If I have compiled SHC on my local, and if I want to use it in my spark job without obstacles like above, what should I do ? Where am I making mistake ?

I want to push data into an already existing table, single column family, no records.
I am using shc-core:1.1.1-2.1-s_2.11 on a windows machine. I have hbase 1.2.6 installed and use scala 2.11.8.

When I try to push data I got first the following error: `org.apache.spark.sql.execution.datasources.hbase.InvalidRegionNumberException: Number of regions specified for new table must be greater than 3.`

After following the advice of this link https://github.com/hortonworks-spark/shc/issues/249#issue-318285217, I added: `HBaseTableCatalog.newTable -> "5"` to my options.
It still failed but with: `java.lang.IllegalArgumentException: Can not create a Path from a null string`.

Following this link: `https://github.com/hortonworks-spark/shc/issues/151#issuecomment-313800739`, I added to my catalog: `, "tableCoder":"PrimitiveType"`.
Still facing the same error.

I saw people are expecting some clarification about that issue (https://github.com/hortonworks-spark/shc/issues/249#issuecomment-463528032).

It is known issue and apparently it seems fixed (https://github.com/hortonworks-spark/shc/issues/155#issuecomment-315236736).
I do not know what to do next.
Is there a solution about this?
Not able to store data in hbase, It seems to to some jar file isse. I am using Spark 2.3

using :spark-submit --packages com.hortonworks:shc-core:1.1.0-2.1-s_2.11 --repositories http://repo.hortonworks.com/content/groups/public/
Is it possible to provide spark.hbase.host and spark.hbase.port in code?  I tried following

`val sparkConf = new SparkConf().setAppName("Spark-HBase").setMaster("local[4]")

    sparkConf.set("spark.hbase.host", "hbase_dev.myorg.net");

    val spark: SparkSession = SparkSession.builder()
      .config(sparkConf)
      .getOrCreate()`

but getting error as shc tries to connect to localhost:

`19/09/25 21:16:34 INFO ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
19/09/25 21:16:34 WARN ReadOnlyZKClient: 0x1a785fd5 to localhost:2181 failed for get of /hbase/hbaseid, code = CONNECTIONLOSS, retries = 3
19/09/25 21:16:34 WARN ClientCnxn: Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1125)`