This is a version of #373 that tries to remove some duplicated commits. The original PR adds Log Linear Model based weighting to the disambiguation procedure. It also adds some new features, such as a bias term and a lexical similarity measure of the surface form and candidate entity name. 

This is the PR for the second part of my GSoC project, adding LLM-based weighting to the disambiguation model. I've also added some new features, such as a bias term and a lexical similarity measure of the surface form and candidate entity name. 

This is an attempt on cleaning the rest module:
- Tried to get rid of repeated code/methods follwing DRY as much as I could
- `confidence` param refers to `disambiguationConfidence`
- added a new parameter `spotterConfidence`
- Got rid of `spotlightInterface`
- Server has a `SpotlightModel`
- Endpoints call methods of `spotlight model`
  - `spot` calls  `spot`
  - `annotate` calls `firstBest`
  - `candidates` calls `nBest`
- I couldnt see any difference between `disambiguate` and `annotate` so I merged both endpoints in the same jersey resource
- Resources only gather parameters and send them to spotlightModel and then to the output manager to get the correct output, this resulted in lots of logic being removed from server.java and the resources themselves.

---

ToDo:
- [ ]  Properly test this branch. 
  - [x] Test Json, Xml request
  - [ ] Test form requests

---

Other nice things to do ( probably in another PR): 
- [x] add swagger
- [ ] bump to jersey 2.x
- [ ] replace `@queryparam`s  for `@beanparam` 

---
- It would be nice to update to jersey 2.x in such way we could create a bean with all the params fields and avoid naming them over and over in every endpoint. The problem is when I tried to bump jersey version I found errors in other modules (uima..) and this was already a big PR.

the entity topic model implementation is very simple and preliminary, but fast and working at the moment. It relies on the statistical backend for training (spotter+stores). It needs a lot of memory at the moment, though. Probabilistic count stores might be a nice idea to reduce those requirements.

I also migrated the whole project to 2.10, further testing would be nice, e.g., creating spotlight model (I don't have the raw counts).

Currently, disambiguation on CSAW has a precision of around 0.81 after 150 iterations of training, which is compared to our common sense baseline with 0.83 a little worse, which means that the context model actually hurts performance, compared to only using surface-form to resource probabilities.

Edit: Includes now new LinearChainCRFSpotter and a new spotter for pretrained factorie ner-models: see [PretrainedFactorieNerSpotter](https://github.com/dirkweissenborn/dbpedia-spotlight/blob/entity_topic_clean/core/src/main/scala/org/dbpedia/spotlight/spot/factorie/PretrainedFactorieNerSpotter.scala) (e.g., SimpleNerSpotter, BilouConllNerSpotter)

This PR adds a feature to dbpedia-spotlight, namely, weights associated with the annotations extracted.To do so, all you need to do is add a line containing `relevance_scoring=default` to your `model.properties` file in the model folder. If that line is not present, spotlight behaves as usual.

We use:
- the context vector overlap
- number a times a topic is spotted in the text
- the overlap among context words for the topics. For example "Microsoft" being a context words for the topics.
- We normalize the output score using min-max normalization (0-1) ( this step could be improved)

Given some toy data that we had manually annotated, we realized that this method gives results close to human judgements than those given by other topic extractors such as Zemanta or Alchemy. If you think this is a good idea, we could manually annotate some establish dataset, like the milne witten, and write a paper as an attempt to reproduce the results in a more formal way. 

This is meant to be a review/feedback pr :) : 

verbose explanation and details :
https://gist.github.com/dav009/67cfc2787d07761a55d5
