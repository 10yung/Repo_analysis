Dear friends,

I am a beginner of spark.Because I think the spark-shell is inconvenience to use, so I want to use spark-shell(scala) with jupyter ipython notebook.

But my scala version is 2.10.4,and my spark version is 1.2.0.

what do you think about it if I git your jupyter scala for the spark work?

can you give me some advice on the IDE with spark-shell(scala)?

Thanks in advance!

Hong Cheng

email:kwchenghong@gmail.com
Has auto completion been implemented (e.g. ctrl-space)?

There is a binary package of jupyter-scala for 2.10, so I tried to run your code:
var lines = sc.textFile("sotu/2009-2014-BO.txt")
val wordCountBO = lines
  .flatMap(_.split(" ")
    .map(_.toLowerCase.trim)
    .map(clean)
    .map(word => (word, 1)))
  .reduceByKey(_ + _)
wordCountBO.count()

and got:
java.io.NotSerializableException: org.apache.spark.SparkContext

This is probably because one of the closures references sc. 

Do you know why?

I can't load spark from the repo.
This is what I'm getting when I execute `load.ivy("org.apache.spark" %% "spark-assembly" % "2.11")`:
`module not found: org.apache.spark#spark-assembly_2.11;2.11`

Any idea what is the problem?

Sorry if this qustione seems to be mundane. I am new to the JVM world.
