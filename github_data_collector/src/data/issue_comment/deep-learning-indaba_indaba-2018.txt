
There was a typo in the code. The code was trying to plot the `sin_t_noise` variable but the variable that was defined was called `sin_t_noisy`. Thus, the code produced a runtime error and halted the flow of the notebook.
I hope this isn't too late. I thought it would be cool to add a visualisation of the policy for the random agent and for the trained agent - to really drive home what the policy is. I also added a repeat of the exploratory task, with the policy visualisation put in, after the agent is trained. This lets the attendees play the game again with the agent's suggestions for actions (see image at https://groups.google.com/forum/#!topic/dli-practicals/laQcxlJSYUg).