just as above
In file mmdet/core/bbox/sampler/ohem_sample.py at line 46:

_, topk_loss_inds = loss.topk(num_expected)

when I do not change, just add ohem_sample in config, it error out  like list:
RuntimeError: invalid argument 2: k not in range for dimension
 so i debug in it and search for the func topk
i modify it like this
_, topk_loss_inds = loss.topk(num_expected, dim=0)

it runs well, so i just confuse,and make the issue.

thanks, best wish.

Hi all,
After a discussion with @hellock and @yhcao6 , we decide to encapsulate box and mask heads into RoI head to reduce the codebase's complexity. This refactoring has several advantages as follows.

- The refactoring reduces most of the duplicate code in the detectors. For example, when adding new methods like Double head R-CNN and Cascade R-CNN, the previous implementation always need to create a new file in the detectors directory and copy the same code for RPN. Since most of these modifications are about the RCNN process, it seems to be better to encapsulate the second stage (including box head, mask head, mask scoring head, and potentially key point head, etc.) into the RoI heads to reduce the duplicate codes and logics.

- The refactoring only creates box assigners and samplers once in RoI heads. The previous implementation creates assigner and sampler during each iteration in the training process, this is unnecessary since they are the same during the training process. Now, the assigner and sampler are created once as a sub-module of RoI heads to reduce the unnecessary overhead.

- The refactoring will ease the processes when code refactoring and adding new methods as it reduces duplicate code and logic and seems to be more conceptually simple than before.

**Notice**: The structure of the detectors are changed, as all configs of two-stage detectors are changed. Thus, it might have compatibility issues with the previous model zoo. We will re-benchmark all the models in the configs when all the refactorings are finished, which will be released in early February.


# Problems description

Hello!
When I wanted to do the training with the following code:

`sudo docker run --rm -it -v $(pwd):/$(pwd) -w $(pwd) --name cascademask cascademask python tools/train.py configs/mask_rcnn_r50_fpn_1x.py`

a RuntimeError happened as below:

`RuntimeError: DataLoader worker (pid 37) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.`

However, as I checked my server, it still had much plenty memory as the project was under training. I am confused why this error happened with free memory. I am very grateful for any help.

The details of my nvidia-smi is:
![image](https://user-images.githubusercontent.com/56009567/72595966-600e3e80-394e-11ea-9ac4-5481084e4ca5.png)

# Traceback
2020-01-17 07:32:35,054 - INFO - Distributed training: False
2020-01-17 07:32:35,054 - INFO - MMDetection Version: 1.0rc1+e032ebb
2020-01-17 07:32:35,054 - INFO - Config:
model settings
model = dict(
    type='MaskRCNN',
    pretrained='torchvision://resnet50',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        style='pytorch'),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_scales=[8],
        anchor_ratios=[0.5, 1.0, 2.0],
        anchor_strides=[4, 8, 16, 32, 64],
        target_means=[.0, .0, .0, .0],
        target_stds=[1.0, 1.0, 1.0, 1.0],
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),
    bbox_roi_extractor=dict(
        type='SingleRoIExtractor',
        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),
        out_channels=256,
        featmap_strides=[4, 8, 16, 32]),
    bbox_head=dict(
        type='SharedFCBBoxHead',
        num_fcs=2,
        in_channels=256,
        fc_out_channels=1024,
        roi_feat_size=7,
        num_classes=81,
        target_means=[0., 0., 0., 0.],
        target_stds=[0.1, 0.1, 0.2, 0.2],
        reg_class_agnostic=False,
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),
    mask_roi_extractor=dict(
        type='SingleRoIExtractor',
        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),
        out_channels=256,
        featmap_strides=[4, 8, 16, 32]),
    mask_head=dict(
        type='FCNMaskHead',
        num_convs=4,
        in_channels=256,
        conv_out_channels=256,
        num_classes=81,
        loss_mask=dict(
            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))
model training and testing settings
train_cfg = dict(
    rpn=dict(
        assigner=dict(
            type='MaxIoUAssigner',
            pos_iou_thr=0.7,
            neg_iou_thr=0.3,
            min_pos_iou=0.3,
            ignore_iof_thr=-1),
        sampler=dict(
            type='RandomSampler',
            num=256,
            pos_fraction=0.5,
            neg_pos_ub=-1,
            add_gt_as_proposals=False),
        allowed_border=0,
        pos_weight=-1,
        debug=False),
    rpn_proposal=dict(
        nms_across_levels=False,
        nms_pre=2000,
        nms_post=2000,
        max_num=2000,
        nms_thr=0.7,
        min_bbox_size=0),
    rcnn=dict(
        assigner=dict(
            type='MaxIoUAssigner',
            pos_iou_thr=0.5,
            neg_iou_thr=0.5,
            min_pos_iou=0.5,
            ignore_iof_thr=-1),
        sampler=dict(
            type='RandomSampler',
            num=512,
            pos_fraction=0.25,
            neg_pos_ub=-1,
            add_gt_as_proposals=True),
        mask_size=28,
        pos_weight=-1,
        debug=False))
test_cfg = dict(
    rpn=dict(
        nms_across_levels=False,
        nms_pre=1000,
        nms_post=1000,
        max_num=1000,
        nms_thr=0.7,
        min_bbox_size=0),
    rcnn=dict(
        score_thr=0.05,
        nms=dict(type='nms', iou_thr=0.5),
        max_per_img=100,
        mask_thr_binary=0.5))
dataset settings
dataset_type = 'CocoDataset'
data_root = 'data/coco/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='Normalize', **img_norm_cfg),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(type='Normalize', **img_norm_cfg),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img']),
        ])
]
data = dict(
    imgs_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/instances_train2017.json',
        img_prefix=data_root + 'train2017/',
        pipeline=train_pipeline),
    val=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/instances_val2017.json',
        img_prefix=data_root + 'val2017/',
        pipeline=test_pipeline),
    test=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/instances_val2017.json',
        img_prefix=data_root + 'val2017/',
        pipeline=test_pipeline))
optimizer
optimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
learning policy
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=1.0 / 3,
    step=[8, 11])
checkpoint_config = dict(interval=1)
yapf:disable
log_config = dict(
    interval=50,
    hooks=[
        dict(type='TextLoggerHook'),
        # dict(type='TensorboardLoggerHook')
    ])
yapf:enable
evaluation = dict(interval=1)
runtime settings
total_epochs = 12
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './work_dirs/mask_rcnn_r50_fpn_1x'
load_from = None
resume_from = None
workflow = [('train', 1)]

2020-01-17 07:32:35,466 - INFO - load model from: torchvision://resnet50
Downloading: "https://download.pytorch.org/models/resnet50-19c8e357.pth" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth
100%|###################################################| 97.8M/97.8M [00:08<00:00, 11.6MB/s]
2020-01-17 07:32:44,674 - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: fc.weight, fc.bias

loading annotations into memory...
Done (t=17.53s)
creating index...
index created!
2020-01-17 07:33:11,633 - INFO - Start running, host: root@0798a02fad8d, work_dir: /home/ChutianXu0922/instance/cascademask/work_dirs/mask_rcnn_r50_fpn_1x
2020-01-17 07:33:11,633 - INFO - workflow: [('train', 1)], max: 12 epochs
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 Traceback (most recent call last):
  File "tools/train.py", line 124, in <module>
    main()
  File "tools/train.py", line 120, in main
    timestamp=timestamp)
  File "/mmdetection/mmdet/apis/train.py", line 133, in train_detector
    timestamp=timestamp)
  File "/mmdetection/mmdet/apis/train.py", line 319, in _non_dist_train
    runner.run(data_loaders, cfg.workflow, cfg.total_epochs)
  File "/opt/conda/lib/python3.6/site-packages/mmcv/runner/runner.py", line 364, in run
    epoch_runner(data_loaders[i], **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/mmcv/runner/runner.py", line 268, in train
    self.model, data_batch, train_mode=True, **kwargs)
  File "/mmdetection/mmdet/apis/train.py", line 100, in batch_processor
    losses = model(**data)
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 150, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File "/mmdetection/mmdet/core/fp16/decorators.py", line 49, in new_func
    return old_func(*args, **kwargs)
  File "/mmdetection/mmdet/models/detectors/base.py", line 138, in forward
    return self.forward_train(img, img_meta, **kwargs)
  File "/mmdetection/mmdet/models/detectors/two_stage.py", line 166, in forward_train
    x = self.extract_feat(img)
  File "/mmdetection/mmdet/models/detectors/two_stage.py", line 92, in extract_feat
    x = self.backbone(img)
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File "/mmdetection/mmdet/models/backbones/resnet.py", line 522, in forward
    x = self.conv1(x)
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 345, in forward
    return self.conv2d_forward(input, self.weight)
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 342, in conv2d_forward
    self.padding, self.dilation, self.groups)
  File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 37) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.
here is my inference script
`import mmcv
import os
import numpy as np
from tqdm import tqdm
import json
import cv2

def show_result(result):
    bbox_result = result
    bboxes = np.vstack(bbox_result)
    labels = [
        np.full(bbox.shape[0], i, dtype=np.int32)
        for i, bbox in enumerate(bbox_result)
    ]
    labels = np.concatenate(labels)
    return bboxes,labels



config_file = '/data/lzy_intern/mmdetection/configs/my_config/image_ann_cascade_rcnn_x101_64x4d_fpn_1x.py'
checkpoint_file = '/data/lzy_intern/mmdetection/models/cascade_rcnn_x101_64x4d_fpn_1x.pth'
model = init_detector(config_file, checkpoint_file, device = 'cuda:2')

image = mmcv.imread('/data/lzy_intern/dataset/coco/train2017/000000174601.jpg')
result = inference_detector(model,image)
bbox,label = show_result(result)
print(len(bbox))
print(len(label))`

the checkpoint model is download from [https://s3.ap-northeast-2.amazonaws.com/open-mmlab/mmdetection/models/cascade_rcnn_x101_64x4d_fpn_1x_20181218-e2dc376a.pth](url)
and the config file is the deafult version of mmdetection,
but why is output of len(bbox) and len(label) is 0, Is the inference is not work?
#### Added `random` classmethods to SamplingResult and AssignResult. 

These allow for the simple creation of "demo data" that can be use for testing, debugging, and potentially more (although I don't think these are as useful as random boxes might be). 

#### Fix issue in `SamplingResult.__init__`

I've been hitting issues where sometimes `gt_bboxes` looks like its had its dimensions squashed. This causes an error when doing the index selection. Adding two checks fixes it. 

#### Add rng as attribute of RandomSampler

To ensure that the new random methods could be seeded to consistently reproduce an issue if needed, I had to ensure that the `RandomSampler` class could be seeded. I used `ensure_rng`, which I added in a previous PR to accomplish this simplify. 

#### Misc 

I also added a `to` function to `SamplingResult`, where it moves the data onto a different device. I wrote this to test something that turned out not to be an issue, but I figure its a nice API to have so I left it in.

I also added `info` properties to SamplingResult and RandomSampler as well as implementing the `__nice__` debugging string for SamplingResult. 

I did end up using my [ubelt](https://github.com/Erotemic/ubelt) library to accomplish some of these tasks. The core fix and the random clasmethods don't rely on ubelt, so if there is any resistance to adding a dependency I can remove the features that depend on ubelt and still have a functionally useful PR. However, I've been maintaining and keeping ubelt stable for ~2 years, it has 100% test coverage, it itself is fairly small, and its own dependencies are minimal. 
I set the model savedir google drive dir when using google colaboratory.But I met os error.Why?How to solve that bug?I always lost model weight due to the very short time keeping the connection
Hi, guys!
I want to know how to get the batch size of the model in MMDetection?
Because I did not see it in a config file.

Any answer would be appreciated!
As the [GETTING_STARTED.md#train-with-multiple-gpus](https://github.com/open-mmlab/mmdetection/blob/master/docs/GETTING_STARTED.md#train-with-multiple-gpus) says, we can launch single machine multi-gpu training using [tools/dist_train.sh](https://github.com/open-mmlab/mmdetection/blob/master/tools/dist_train.sh) and the training goes well.

I have two questions:

- In single machine multi-gpu training, does the argument `gpus` in [tools/train.py](https://github.com/open-mmlab/mmdetection/blob/master/tools/train.py) become useless?

I try command 
`CUDA_VISIBLE_DEVICES="0,1,2,3" PORT="29500" bash tools/dist_train.sh configs/ms_rcnn/ms_rcnn_r50_caffe_fpn_1x.py 4 --validate --gpus 2`
`nvidia-smi` shows 4 gpus are running
I try command
`CUDA_VISIBLE_DEVICES="0,1,2,3" PORT="29500" bash tools/dist_train.sh configs/ms_rcnn/ms_rcnn_r50_caffe_fpn_1x.py 2 --validate --gpus 4`
`nvidia-smi` shows 2 gpus are running

- Is [tools/dist_train.sh](https://github.com/open-mmlab/mmdetection/blob/master/tools/dist_train.sh) the only way for single machine multi-gpu training ?

I try command 
`python3 tools/train.py configs/ms_rcnn/ms_rcnn_r50_caffe_fpn_1x.py --gpus 4`
for multi-gpu training, but encounter error like this
```
Traceback (most recent call last):
  File "tools/train.py", line 124, in <module>
    main()
  File "tools/train.py", line 120, in main
    timestamp=timestamp)
  File "/local/mmdetection/mmdet/apis/train.py", line 133, in train_detector
    timestamp=timestamp)
  File "/local/mmdetection/mmdet/apis/train.py", line 319, in _non_dist_train
    runner.run(data_loaders, cfg.workflow, cfg.total_epochs)
  File "/usr/local/lib/python3.6/dist-packages/mmcv/runner/runner.py", line 363, in run
    epoch_runner(data_loaders[i], **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/mmcv/runner/runner.py", line 267, in train
    self.model, data_batch, train_mode=True, **kwargs)
  File "/local/mmdetection/mmdet/apis/train.py", line 100, in batch_processor
    losses = model(**data)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/data_parallel.py", line 153, in forward
    return self.gather(outputs, self.output_device)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/data_parallel.py", line 165, in gather
    return gather(outputs, output_device, dim=self.dim)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/scatter_gather.py", line 68, in gather
    res = gather_map(outputs)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/scatter_gather.py", line 62, in gather_map
    for k in out))
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/scatter_gather.py", line 62, in <genexpr>
    for k in out))
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/scatter_gather.py", line 55, in gather_map
    return Gather.apply(target_device, dim, *outputs)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py", line 67, in forward
    ctx.input_sizes = tuple(map(lambda i: i.size(ctx.dim), inputs))
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py", line 67, in <lambda>
    ctx.input_sizes = tuple(map(lambda i: i.size(ctx.dim), inputs))
IndexError: dimension specified as 0 but tensor has no dimensions
```
I noticed that there is a function get_points_single( ) in fcos_head.py that maps the positions on multi-level feature maps to the points on the image plane. I was wondering if there is an existing 'inverse' function that can map the point on the image plane to the position on the feature map? 
Thanks.