I append `headerOnFirstFile` option. 
If this option enabled, it will generate 1-header only. ( according to #317 )
If this option disabled, spark-csv will generate header per file. Defaut is false.
Added the comments for csv file paths
On line 150, the code actually defaults to `false` for the `header` option.
This agrees with the documentation in the project README.
Currently running the codes below:

```scala
val schema = StructType(
  StructField("bool", BooleanType, true) ::
  StructField("nullcol", IntegerType, true) ::
  StructField("nullcol1", IntegerType, true) :: Nil)

new CsvParser()
  .withSchema(schema)
  .withUseHeader(true)
  .withParserLib(parserLib)
  .withParseMode(ParseModes.PERMISSIVE_MODE)
  .csvFile(sqlContext, boolFile)
  .select("bool", "nullcol")
  .collect()
```

with the data below:

```csv
bool
"True"
"False"

"true"
```

throws an exception as below:

```
java.lang.NumberFormatException: null
	at java.lang.Integer.parseInt(Integer.java:542)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:30)
	at com.databricks.spark.csv.util.TypeCast$.castTo(TypeCast.scala:57)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$6.apply(CsvRelation.scala:196)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$6.apply(CsvRelation.scala:174)
```

This is related with https://github.com/apache/spark/pull/15767. Please refer the description there.



This is the change that allows an option to render errors when parsing such as number format exceptions as nulls. 
It was in this pull request, https://github.com/databricks/spark-csv/pull/298 but I thought it would be cleaner to create a new one. 
Adds Relation, LineReader and BulkReader traits to avoid duplicated code. Largely derived from https://github.com/quartethealth/spark-csv and https://github.com/quartethealth/spark-fixedwidth.

This is in response to the following PR (created by @blrnw3) being closed without a merge:

https://github.com/databricks/spark-csv/pull/259

A fixed-width parser is a very common use case that I think several users would enjoy using. We plan to use this in our current production environment. 


I don't know Scala (at all!) so there's almost certainly cleaner ways - my apologies.  The logging at the moment is sometimes unhelpful as it's hard to see the real issue - with DROPMALFORMED you see the line, with another parsing mode you get the error but not the line. 

For the context and discussion on this, please refer to https://github.com/databricks/spark-csv/pull/244.
