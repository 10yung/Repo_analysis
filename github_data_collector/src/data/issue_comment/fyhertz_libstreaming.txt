I was writing a program that could pack RTP header before h264 & aac. but I was confused with RTP timestamp field.
If the video codec is h264, the timestamp could be added by 90000/fps with each frame. 
I have no idea about the aac.
My aac sample rate is 8000HZ,config=1588 and each frame is 250 ~ 520. 
How to calculate the audio rtp timestamp?
libstreaming rtsp server does not support digest authentication.Do you have any plansï¼ŸAs you knows,Basic authentication security is low.
I know little about RTSP and I think it's similar to some C/S architecture application.
Wish that someone tell me if the description below is right:
With libstreaming, I can build an RTSP Server in an Android device(let's call it device A), capturing video and audio. Then I run an RTSP Client in another Android device(let's call it device B). Device B can easily play media which provided by Device A.

If that is true, then I am confused. It seems that examples do not behave like that (all devices are in the same LAN).

I am trying to stream Video using the rtp and should stream video simultaniously from other devices connected to same Wifi Network .

I used [this github link][1] to implement the above concept but i am facing an issue for how to configure the session bilder and get the video transmitted so that we can use vlc media player through .sdp file to pull to display the video. 


  [1]: https://github.com/fyhertz/libstreaming
I'm setting MediaFormat.KEY_I_FRAME_INTERVAL value as 1 (below given code) but it's not generating Key frame every second but it is generating every 66 second. I'm using Libstreaming library. can anyone please help why is not working ?

`mMediaCodec = MediaCodec.createByCodecName(debugger.getEncoderName());
  MediaFormat mediaFormat =MediaFormat.createVideoFormat("video/avc",
               640,480);
        mediaFormat.setInteger(MediaFormat.KEY_BIT_RATE, 1000000);
        mediaFormat.setInteger(MediaFormat.KEY_FRAME_RATE, 15);
        mediaFormat.setInteger(MediaFormat.KEY_COLOR_FORMAT,
                debugger.getEncoderColorFormat());
        mediaFormat.setInteger(MediaFormat.KEY_I_FRAME_INTERVAL, 1); //here
        mMediaCodec.configure(mediaFormat, null, null,
                MediaCodec.CONFIGURE_FLAG_ENCODE);
        mMediaCodec.start();`
I want to use another video source and stream it using libstreaming but as I understand there is no way to change the source video and libstreaming automatically use device camera as source video.
to be more specific I have an external camera that I can get video as a raw `ByteBuffer` and I wanted to stream it over network. 

I call this function in `onCreate` 

```java
private void initListener() {

        hdCameraManager = (HDCameraManager) getUnitManager(FuncConstant.HDCAMERA_MANAGER);

        Log.i(TAG, "Now in initListener start");

        hdCameraManager.setMediaListener(new MediaStreamListener() {
            @Override
            public void getVideoStream(int i, byte[] bytes, int i1, int i2) {
                drawVideoSample(ByteBuffer.wrap(bytes));
            }

            @Override
            public void getAudioStream(int i, byte[] bytes) {

            }
        });
    }
```

it's not important what is `HdCameraManager` and other stuff the only important thing is in `getVideoStream` function I get the raw video data as `bytes` and wrap it up in a `ByteBuffer` and pass it to another function.
```java
@Override
 public void getVideoStream(int i, byte[] bytes, int i1, int i2) {
        drawVideoSample(ByteBuffer.wrap(bytes));
}
```
so How can I be able to send this raw stream to external encoder like Wowza or FFmpeg using Libstreaming?
Should I use `MediaCodec` and other stuff to make it through? 
Should I change the libstreaming source code to make it happen? 
Thanks a lot.  
H264Packetizer.java 

`if (type == 5 && sps != null && pps != null) {`
`            buffer = socket.requestBuffer();`
`            socket.markNextPacket();`
`            socket.updateTimestamp(ts);`
`            System.arraycopy(stapa, 0, buffer, rtphl, stapa.length);`
`            super.send(rtphl + stapa.length);`
`        }`

Above code executing every 65-66 seconds in my device I want to execute it every 10 seconds. I tried by adding interval manually but it not working because packets not there for pps and sps at that time. So anybody please tell me  how can I achieve this. 

Thanks.
Brother, the delay is a bit serious, about 2 to 3 seconds, this effect is too bad, simply can not be used, is there any solution?
Let me explain:

I'm new in RTSP world. All I can say is that I have a camera and it connect well. I'm receiving all the frames in byte[] format. Just don't know how to finally display it. I should use this library or there's a native method? 

Hope u guys help me!


