Hi,

I've been using syuzhet and got to say it's just awesome.  What I would like to add is my own dictionary in a NRC kind of way with my own sentiments and values.

I'm kind of new to R and I tried to replicate the get_nrc_sentiment() and get_nrc_values in order to acomplish getting my own dictionary. I came up with something like this:

 `get_custom_sentiment <- function(char_v, cl=NULL,  lowercase = TRUE, myDictionary){
  if (!is.character(char_v)) stop("Data must be a character vector.")
  if(!is.null(cl) && !inherits(cl, 'cluster')) stop("Invalid Cluster")
  #lexicon <- dplyr::filter_(nrc, ~lang == language) #I commented this since I wanted my own dictionary
  if(lowercase){
    char_v <- tolower(char_v)
  }
  word_l <- strsplit(char_v, "[^A-Za-z']+")
  
  if(is.null(cl)){
    nrc_data <- lapply(word_l, get_custom_values, lexicon = myDictionary)
  }
  else{
    nrc_data <- parallel::parLapply(cl=cl, word_l, lexicon = myDictionary, get_custom_values(myDictionary))
  }
  
  result_df <- as.data.frame(do.call(rbind, nrc_data), stringsAsFactors=F)
  #reorder the columns
  my_col_order <- c(
    "anger", 
    "anticipation", 
    "disgust", 
    "fear", 
    "joy", 
    "sadness", 
    "surprise", 
    "trust", 
    "negative", 
    "positive"
  )
  result_df[, my_col_order]
}`

And for getting the values:

`get_custom_values <- function(word_vector,lexicon = NULL){
  #if (is.null(lexicon)) {
  #lexicon <- dplyr::filter_(nrc, ~lang == language)
  #} Again, commented this since I want my own Dictionary
   if (! all(c("word", "sentiment", "value") %in% names(lexicon)))
      stop("lexicon must have a 'word', a 'sentiment' and a 'value' field")
  
  data <- dplyr::filter_(myDictionary, ~word %in% word_vector)
  data <- dplyr::group_by_(data, ~sentiment)
  data <- dplyr::summarise_at(data, "value", sum)
  
  all_sent <- unique(myDictionary$sentiment)
  sent_present <- unique(data$sentiment)
  sent_absent  <- setdiff(all_sent, sent_present)
  if (length(sent_absent) > 0) {
    missing_data <- dplyr::data_frame(sentiment = sent_absent, value = 0)
    data <- rbind(data, missing_data)
  }
  tidyr::spread_(data, "sentiment", "value")
}`

If I pass the dictionary directly into the function code (not as a variable) the code works, but if I pass it as a variable I get this error: 

_Error in get_custom_sentiment(DB$text, feelings) : 
  Invalid Cluster_

My guess is there is something in how I format de custom dictionary that doesn't let the function find the clusters in this line 
` if(!is.null(cl) && !inherits(cl, 'cluster')) stop("Invalid Cluster")` 

But also if that was the case why does it work if I put the dictionary directly inside the function. 
I hope this makes sense and that you could point me in the right direction to work on this.



I realized that the NRC dictionary in spanish has multiple appearances for the same word in the same sentiment. I look into the structure for the original translated dictionaries at http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm and figured out that this has a same word in spanish being used to translate several words.

For instance, the word "asesino" serves as translation for the words "assassin", "cutthroat", "murderer", "murderous" and "slayer" which in turn, due to data structure, returns incorrect data when being used with the packages functions.

The output for `get_nrc_sentiment(char_v = c("mira, un asesino"), language = "spanish")`
is anger = 5, disgust = 3, fear = 5, sadness = 4, surprise = 2 and negative = 5; while the output for `get_nrc_sentiment(char_v = c("look, an assassin"), language = "english")` is anger = 1, fear = 1, sadness = 1 and negative = 1.

I think it'd be look if you could fix this, for the package is quite useful. I would expect this same issue to appear in other translation.

currently, only languages with Latin alphabets are supported by NRC and custom methods. I have made a fork which supports Unicode alphabets by altering the regex used, and by using the CRAN package ‘tokenizers’ for unicode tokenisation .

The regex has been altered from "[^A-Za-z']+" to "\\s+", looking only at whitespace to split the strings. There are more complex alternatives using perl style regex to include only non unicode alphabet characters if this causes issues.
I tried to use get_nrc_sentiment with lang = "portuguese" and NRC method but it aways returns zero for all sentiments. I was wondering if I shoud do something more to use it.