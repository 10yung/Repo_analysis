It would be fantastic to have gpu support for all those functions. 
How much work/time is required to rewrite the matlab code so it could be used on GPU ?
Hi Ben,

I have been using your HCTSA toolbox for sometime and had a query. First of all thank you for working on the package it has been really useful for my work.

I often face the problem that once I have performed feature extraction on a dataset,normalized the features trained a classifier based on the normalized features and then when I have a new data on which I want to make prediction after feature extraction I do not have the normalization parameters originally used. 

So I have to combine the new and trained data features, normalize together and then separate out the new data. I wish there was something similar to python implementation of fit_transform(). Please let me know if you have any provision in your code for dealing with this.

Best Regards,
Avi
Perhaps it's time to update the terminology, since `Feature` is more common usage than `Operation`.
Could consider changing the name of the `Operations` data object to `Features`...
Hello,

This might be considered as a feature request or seeking advice how to hack it.

The gist of the problem is, I do not have all the toolboxes available. Seemingly producing errors and calling the functions adds an unnecessary overhead to my computation time, which I would want to minimize.

What I would like to have the option to do, is to exclude operations that belong to certain toolboxes, as to avoid even calling functions I know will fail.

The TS_ops.txt does not seem to contain information on which toolbox something belongs to.

So, if anyone has any ideas, how I could perform, this to speed up my computations, I would be very happy.

And thank you for a nice software suite! :)

PS: I have many more questions, and I have a feeling this is more a bug reporting place so where would be a more appropriate place to ask those.
Are you aware of any R bindings for hctsa?

Best regards,
Sebastian
Many features rely on similar intermediate calculations (such as the first zero-crossing of the ACF, or the embedding dimension estimated using fnn, etc.)
Rather than repeating these time consuming calculations again and again, would be more efficient to compute them once and have that information accessible by the functions.
This would come at the cost of making some functions rely on a particular input structure, but this could take the form of an optional argument, which is a structure with fields containing frequently used quantities. An alternative is to make the first input to all operations take either a vector as currently the case, or a structure that contains the time series data as well as some intermediate calculations that can be extracted (or recomputing if not available). This is potentially a more major change...
