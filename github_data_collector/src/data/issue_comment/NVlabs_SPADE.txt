Hi, 
1-  I want to generate infrared images from visual images , I have paired infrared-visual images for training (FLIR and one other dataset), I have attached sample. 
2- Is this project appropriate for this work? i already tried pix2pix, pix2pixHD and currently trying MUNIT but it do not generate good output.
3- I only have paired images, is there any method to generate labels  and instances for better training? Right now i just want to focus on 3 categories (person, vehicle, background). 
 
I am waiting for your kind suggestions. 


![4440](https://user-images.githubusercontent.com/28862708/72309018-a8143380-36b8-11ea-9281-a0b4a8aad51d.png)
![4784](https://user-images.githubusercontent.com/28862708/72309419-e1996e80-36b9-11ea-9a4a-0a98f0d6a923.png)

Hi, author!
I have a question about the quantitative evaluation. 
For Cityscapes dataset, I run evaluation scripts and the result is that mIoU=62.2, accuracy=93.5, FID=63.36. It has some difference with results shown in your paper, especially the accuracy is obviously higher than your 81.9. I use the bilinear downsample method for label maps as you answered in #issue39

Current versions of scipy doesn't have `toimage`, so spade repo needs to be pinned to an older scipy version to work.

Dataset: [coco-stuff dataset](https://github.com/nightrome/cocostuff)
training script: 
```
python train.py --dataset_mode coco --dataroot ../../sky/data_spade/ --use_vae --batchSize 16 --gpu_ids 5 --checkpoints_dir temp --print_freq 100 --save_latest_freq 40000 --save_epoch_freq
1 
```
training GPU: V100 32G
screen print:
```
(epoch: 23, iters: 38128, time: 0.131) KLD: 0.228 GAN: 1.110 GAN_Feat: 14.514 VGG: 9.875 D_Fake: 0.514 D_real: 0.383 
(epoch: 23, iters: 38224, time: 0.130) KLD: 0.223 GAN: 1.397 GAN_Feat: 15.611 VGG: 11.365 D_Fake: 0.349 D_real: 0.682 
(epoch: 23, iters: 38320, time: 0.132) KLD: 0.131 GAN: 1.541 GAN_Feat: 15.706 VGG: 11.025 D_Fake: 0.184 D_real: 0.832 
(epoch: 23, iters: 38416, time: 0.133) KLD: 0.133 GAN: 1.392 GAN_Feat: 15.577 VGG: 11.413 D_Fake: 0.320 D_real: 0.506 
(epoch: 23, iters: 38528, time: 0.131) KLD: 0.134 GAN: 3.051 GAN_Feat: 14.533 VGG: 11.742 D_Fake: 0.007 D_real: 0.919 
(epoch: 23, iters: 38624, time: 0.130) KLD: 0.119 GAN: 0.305 GAN_Feat: 15.251 VGG: 11.495 D_Fake: 1.092 D_real: 0.133 
(epoch: 23, iters: 38720, time: 0.133) KLD: 0.277 GAN: 1.254 GAN_Feat: 15.551 VGG: 11.702 D_Fake: 0.463 D_real: 0.362 
(epoch: 23, iters: 38816, time: 0.131) KLD: 0.140 GAN: 0.055 GAN_Feat: 15.785 VGG: 11.672 D_Fake: 1.173 D_real: 0.103 
(epoch: 23, iters: 38928, time: 0.131) KLD: 0.159 GAN: 0.716 GAN_Feat: 14.472 VGG: 10.877 D_Fake: 0.803 D_real: 0.540 
(epoch: 23, iters: 39024, time: 0.130) KLD: 0.238 GAN: 2.033 GAN_Feat: 14.876 VGG: 10.564 D_Fake: 0.068 D_real: 0.193 
(epoch: 23, iters: 39120, time: 0.130) KLD: 0.168 GAN: 1.427 GAN_Feat: 15.235 VGG: 11.779 D_Fake: 0.335 D_real: 0.419 
(epoch: 23, iters: 39216, time: 0.131) KLD: 0.170 GAN: 0.170 GAN_Feat: 15.228 VGG: 10.682 D_Fake: 1.107 D_real: 0.185 
(epoch: 23, iters: 39328, time: 0.131) KLD: 0.198 GAN: 1.319 GAN_Feat: 15.483 VGG: 11.348 D_Fake: 0.322 D_real: 0.780 
(epoch: 23, iters: 39424, time: 0.129) KLD: 0.175 GAN: 2.136 GAN_Feat: 14.712 VGG: 11.150 D_Fake: 0.110 D_real: 0.444 
(epoch: 23, iters: 39520, time: 0.129) KLD: 0.204 GAN: 1.191 GAN_Feat: 15.792 VGG: 11.135 D_Fake: 0.371 D_real: 0.427 
(epoch: 23, iters: 39616, time: 0.130) KLD: 0.222 GAN: 1.365 GAN_Feat: 15.121 VGG: 11.219 D_Fake: 0.336 D_real: 0.877 
(epoch: 23, iters: 39728, time: 0.131) KLD: 0.193 GAN: 0.222 GAN_Feat: 14.594 VGG: 10.041 D_Fake: 1.011 D_real: 0.425 
(epoch: 23, iters: 39824, time: 0.130) KLD: 0.176 GAN: 0.844 GAN_Feat: 14.419 VGG: 10.047 D_Fake: 0.514 D_real: 0.778 
(epoch: 23, iters: 39920, time: 0.131) KLD: 0.150 GAN: 0.905 GAN_Feat: 15.275 VGG: 10.872 D_Fake: 0.445 D_real: 0.362 
(epoch: 23, iters: 40016, time: 0.131) KLD: 0.295 GAN: 1.670 GAN_Feat: 15.155 VGG: 10.790 D_Fake: 0.248 D_real: 0.998 
(epoch: 23, iters: 40128, time: 0.129) KLD: 0.157 GAN: 2.210 GAN_Feat: 14.544 VGG: 11.130 D_Fake: 0.121 D_real: 0.452 
(epoch: 23, iters: 40224, time: 0.131) KLD: 0.252 GAN: 1.712 GAN_Feat: 15.156 VGG: 10.957 D_Fake: 0.321 D_real: 0.789 
(epoch: 23, iters: 40320, time: 0.131) KLD: 0.214 GAN: -0.240 GAN_Feat: 15.196 VGG: 10.799 D_Fake: 1.577 D_real: 0.123 
(epoch: 23, iters: 40416, time: 0.130) KLD: 0.312 GAN: 2.588 GAN_Feat: 14.627 VGG: 10.129 D_Fake: 0.016 D_real: 0.894 
(epoch: 23, iters: 40528, time: 0.132) KLD: 0.195 GAN: 1.023 GAN_Feat: 15.504 VGG: 12.202 D_Fake: 0.515 D_real: 0.401 
(epoch: 23, iters: 40624, time: 0.131) KLD: 0.170 GAN: 1.492 GAN_Feat: 15.240 VGG: 11.308 D_Fake: 0.267 D_real: 0.566 
(epoch: 23, iters: 40720, time: 0.141) KLD: 0.181 GAN: 1.855 GAN_Feat: 15.906 VGG: 11.838 D_Fake: 0.218 D_real: 0.098 
(epoch: 23, iters: 40816, time: 0.132) KLD: 0.160 GAN: 0.869 GAN_Feat: 15.950 VGG: 11.365 D_Fake: 0.666 D_real: 0.457 
(epoch: 23, iters: 40928, time: 0.131) KLD: 0.199 GAN: 1.359 GAN_Feat: 14.625 VGG: 11.319 D_Fake: 0.006 D_real: 0.923 
(epoch: 23, iters: 41024, time: 0.131) KLD: 0.171 GAN: -0.199 GAN_Feat: 15.155 VGG: 9.773 D_Fake: 1.790 D_real: 0.227 
(epoch: 23, iters: 41120, time: 0.131) KLD: 0.219 GAN: 0.208 GAN_Feat: 15.184 VGG: 11.473 D_Fake: 1.065 D_real: 0.329 
(epoch: 23, iters: 41216, time: 0.130) KLD: 0.168 GAN: 2.242 GAN_Feat: 14.358 VGG: 10.455 D_Fake: 0.063 D_real: 0.474 
(epoch: 23, iters: 41328, time: 0.146) KLD: 0.254 GAN: 0.968 GAN_Feat: 14.831 VGG: 10.917 D_Fake: 0.362 D_real: 0.915 
(epoch: 23, iters: 41424, time: 0.133) KLD: 0.131 GAN: 2.295 GAN_Feat: 14.657 VGG: 10.332 D_Fake: 0.043 D_real: 0.637 

```
Loss is as the same as epoch 3. 
Why this network does not convergence?
Hello, 

I used pretrained model for coco stuff to test on val .
I generated instances for val dataset. 
so to generate images , the model take val instance as input,
but why the path for val_img also needed 

Hello 
I would like to ask if SPADE takes fixed dimension maps because 
I am trying to use your pre trained model for COCO Stuff dataset to generate RGB Images for my own Sementic maps but i got that error 
We got that error complaining about the dimensions which really different form the input that SPADE use
**RuntimeError: Given groups=1, weight of size [1024, 184, 3, 3], expected input[1, 186, 8, 8] to have 184 channels, but got 186 channels instead.**

The dimension of the semantic maps that i am using : 256*256.
I would like to know the properties of the semantic maps that SPADE used to generate RGB Images. and if you can recommend me any way to solve this problem.

Thanks 
I am interested in fine-tuning the pre-trained version of this network network which is based on ADE20K. However, when experimenting with this model by running the following code:

```bash
python train.py\
    --name='ade20k_pretrained'\
    --dataset_mode='ade20k'\
    --dataroot='../data/ade20k'\
    --no_pairing_check\
    --continue_train\
    --checkpoints_dir='../data/checkpoints/ade20k_pretrained/'
```

I receive the following error: 

```
FileNotFoundError: [Errno 2] No such file or directory: '../data/checkpoints/ade20k_pretrained/latest_net_D.pth'
```

Checking the files in the [Google Drive link](https://drive.google.com/file/d/12gvlTbMvUcJewQlSEaZdeb2CdOB-b8kQ/view) linked to in the paper, I see that, indeed, for all of the pre-trained versions of this network whose weights have been distributed, only the generator network weights, `*_g.pth`, are available.

Are the discriminator weights not publicly available?
Hello,
I hope to compare with your experimental results in my dissertation. May I ask if it is convenient for you to provide val generated pictures on ADE20K, COCO-stuff, cityscapes. We will make accurate citations in the paper, thank you very much!