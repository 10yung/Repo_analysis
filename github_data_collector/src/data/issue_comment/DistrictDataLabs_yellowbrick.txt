This PR addresses issue #960.

I have made the following changes:

1. Added show hyperparameter to manifold.
2. Updated test for the quick method.
3. Updated documentation.

### Sample Code and Plot

    from yellowbrick.features.Manifold import manifold_embedding
    from yellowbrick.datasets import load_concrete

    # Load the regression dataset
    X, y = load_concrete()

    # Instantiate the visualizer
    manifold_embedding(X, y, manifold="isomap", n_neighbors=10)


### CHECKLIST

<!--
Here's a handy checklist to go through before submitting a PR, note that you can check a checkbox in Markdown by changing `- [ ]` to `- [x]` or you can create the PR and check the box manually.
-->

- [x] _Is the commit message formatted correctly?_
- [ ] _Have you noted the new functionality/bugfix in the release notes of the next release?_

<!-- If you've changed any code -->

- [x] _Included a sample plot to visually illustrate your changes?_
- [x] _Do all of your functions and methods have docstrings?_
- [x] _Have you added/updated unit tests where appropriate?_
- [x] _Have you updated the baseline images if necessary?_
- [x] _Have you run the unit tests using `pytest`?_
- [x] _Is your code style correct (are you using PEP8, pyflakes)?_
- [x] _Have you documented your new feature/functionality in the docs?_


> FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.
This PR refers to issue #604. It adds effect plots to 'yellowbrick/regressors'. 
It has the following features:
1. Auto identification of discrete and continuous columns.
2. Hyperparams for custom plots.

### Sample Code and Plot
```python
from yellowbrick.regressor.effect import EffectPlot
from yellowbrick.datasets import load_bikeshare
from sklearn.linear_model import LinearRegression
X, y = load_bikeshare()
viz = EffectPlot(LinearRegression(), colormap='cool')
viz.fit(X, y)
viz.poof(outpath = 'Effect.png')
```

![Effect](https://user-images.githubusercontent.com/43993586/68571698-941a3e80-0489-11ea-8224-9bf45220233e.png)


### TODOs and questions

<!--
If this is a work-in-progress (WIP), list the changes you still need to make and/or questions or the Yellowbrick team. You can also mention extensions to your work that might be added as an issue to work on after the PR.
-->

Still to do:

- [ ] Provide parameter which allows users to define discrete and continuous features.
- [ ] Add `test_effect.py`
- [ ] Add Documentation.

Questions for the @DistrictDataLabs/team-oz-maintainers:


### CHECKLIST

<!--
Here's a handy checklist to go through before submitting a PR, note that you can check a checkbox in Markdown by changing `- [ ]` to `- [x]` or you can create the PR and check the box manually.
-->

- [ ] _Is the commit message formatted correctly?_
- [ ] _Have you noted the new functionality/bugfix in the release notes of the next release?_

<!-- If you've changed any code -->

- [ ] _Included a sample plot to visually illustrate your changes?_
- [ ] _Do all of your functions and methods have docstrings?_
- [ ] _Have you added/updated unit tests where appropriate?_
- [ ] _Have you updated the baseline images if necessary?_
- [ ] _Have you run the unit tests using `pytest`?_
- [ ] _Is your code style correct (are you using PEP8, pyflakes)?_
- [ ] _Have you documented your new feature/functionality in the docs?_

<!-- If you've added to the docs -->

- [ ] _Have you built the docs using `make html`?_
Addressing issue #960.
I have made the following changes:

1. Added show kwarg to Postag.
2. Added test for the quick method
3. Updated documentation

### Sample Code

```python
import nltk
from yellowbrick.text.postag import postag
from yellowbrick.datasets import load_hobbies
docs = load_hobbies().data
tagged_stanzas = [nltk.pos_tag(nltk.word_tokenize(sent)) for sent in docs]
tag = [tagged_stanzas]
viz = postag(tag)
```


### CHECKLIST

<!--
Here's a handy checklist to go through before submitting a PR, note that you can check a checkbox in Markdown by changing `- [ ]` to `- [x]` or you can create the PR and check the box manually.
-->

- [ ] _Is the commit message formatted correctly?_
- [ ] _Have you noted the new functionality/bugfix in the release notes of the next release?_

<!-- If you've changed any code -->

- [ ] _Included a sample plot to visually illustrate your changes?_
- [ ] _Do all of your functions and methods have docstrings?_
- [ ] _Have you added/updated unit tests where appropriate?_
- [ ] _Have you updated the baseline images if necessary?_
- [ ] _Have you run the unit tests using `pytest`?_
- [ ] _Is your code style correct (are you using PEP8, pyflakes)?_
- [ ] _Have you documented your new feature/functionality in the docs?_

<!-- If you've added to the docs -->

- [ ] _Have you built the docs using `make html`?_

I have made the following changes:
1. Rearrange quick method function parameters to match docstring
1. Reformat docstring
1. Add show kwarg to quick method
1. Remove unnecessary args (random_state, legend) from test
1. Update docs with link, example of quick method, and add to automodule

Questions for the @DistrictDataLabs/team-oz-maintainers:

- [x] I'm not a data science expert so the example I created for the kelbow quickmethod may be nonsense. Can someone please review the example?
- [x] I did basically nothing regarding the test since the quick method uses `xfail` due to some non-determinism in the output graph. Other tests seem to have have the same problem and fix. Is this fine?
- [ ] In the docs I included what I expect to be the hyperlink to the quick method (`kelbow_visualizer() <https://www.scikit-yb.org/en/latest/api/cluster/elbow.html#yellowbrick.cluster.elbow.kelbow_visualizer>`_), though I believe I have no way to test this until it actually gets merged and hosted. Is there a correct way to handle this?
After the visualizer audit we believe that the quick methods are ready to be made prime time. 

Quick methods must:

1. Have a function signature identical to the Visualizer constructor with additional data for fit/score
2. Call `finalize()` and not `poof()`/`show()`
3. Return the fitted visualizer instance
4. Some visualizers must also insure that they call transform/fit_transform/score 

For each visualizer found in the list below we must:

- [x] refactor the quick method to the new API 
- [ ] document the quick method 
- [ ] ensure there is an associated test or tests for the quick method

See #600 for more details on this issue

- [ ] class_prediction_error
- [ ] classification_report
- [ ] confusion_matrix
- [ ] precision_recall_curve
- [x] roc_auc
- [ ] discrimination_threshold
- [ ] alphas
- [ ] cooks_distance
- [ ] prediction_error
- [ ] residuals_plot
- [ ] kelbow_visualizer
- [ ] intercluster_distance
- [x] silhouette_visualizer
- [ ] balanced_binning_reference
- [ ] class_balance
- [ ] feature_correlation
- [x] cv_scores
- [ ] learning_curve
- [ ] validation_curve
- [ ] explained_variance_visualizer
- [ ] feature_importances
- [ ] rank1d
- [ ] rank2d
- [x] rfecv
- [ ] joint_plot
- [ ] mainfold_embedding
- [x] pca_decomposition
- [x] parallel_coordinates
- [x] radviz
- [ ] dispersion
- [ ] freqdist
- [x] postag
- [x] tsne
- [ ] umap
- [ ] gridsearch_color_plot
This PR fixes #498 and adds support for `Pipelines` in most `ScoreVisualizers`. It also adds some automatic checking extending functionality described in #180. 

I have made the following changes:

1. Added an `is_pipeline` type check
2. Added functionality to `ModelVisualizer` to access the final estimator in a pipeline

### TODOs and questions

<!--
If this is a work-in-progress (WIP), list the changes you still need to make and/or questions or the Yellowbrick team. You can also mention extensions to your work that might be added as an issue to work on after the PR.
-->

Still to do:

- [ ] modify visualizers to use new code
- [ ] create tests for pipelines with model visualizers
- [ ] document pipelines in visualizers 

### CHECKLIST

<!--
Here's a handy checklist to go through before submitting a PR, note that you can check a checkbox in Markdown by changing `- [ ]` to `- [x]` or you can create the PR and check the box manually.
-->

- [ ] _Is the commit message formatted correctly?_
- [ ] _Have you noted the new functionality/bugfix in the release notes of the next release?_

<!-- If you've changed any code -->

- [ ] _Included a sample plot to visually illustrate your changes?_
- [ ] _Do all of your functions and methods have docstrings?_
- [ ] _Have you added/updated unit tests where appropriate?_
- [ ] _Have you updated the baseline images if necessary?_
- [ ] _Have you run the unit tests using `pytest`?_
- [ ] _Is your code style correct (are you using PEP8, pyflakes)?_
- [ ] _Have you documented your new feature/functionality in the docs?_

<!-- If you've added to the docs -->

- [ ] _Have you built the docs using `make html`?_

<!--
# Welcome Contributor!

Thank you for contributing to Yellowbrick, please follow the instructions below to get
your PR started off on the right foot.

## First Steps

1. Are you merging from a feature branch into develop?

    _If not, please create a feature branch and change your PR to merge from that branch
    into the Yellowbrick `develop` branch._

2. Does your PR have a title?

    _Please ensure your PR has a short, informative title, e.g. "Enhances ParallelCoordinates with new andrews_curve parameter" or "Corrects bug in WhiskerPlot that causes index error"_

3. Summarize your PR (HINT: See CHECKLIST/TEMPLATE below!)
-->

This PR fixes #316. I added the following features :

1. Added a parameter for showing cumulative explained variance. 
2. Added a parameter which takes percentage of variance needed in the components and shows corresponding principal components.

### Sample Code and Plot

```python
from yellowbrick.features.decomposition import ExplainedVariance

from sklearn.datasets import load_digits
digits = load_digits()
X = digits.data

viz = ExplainedVariance(cumulative=True, cutoff=85)
viz.fit(X)
viz.transform(X)
viz.poof("Varaince.png")
```

![abc](https://user-images.githubusercontent.com/43993586/63538753-2d465280-c536-11e9-879b-8f828b6b0f7c.png)

Still to do:
- [ ] Add more features.
- [ ] Add tests.
- [ ] Add documentation

More Todos:

- [x] _Is the commit message formatted correctly?_
- [ ] _Have you noted the new functionality/bugfix in the release notes of the next release?_

<!-- If you've changed any code -->

- [x] _Included a sample plot to visually illustrate your changes?_
- [ ] _Do all of your functions and methods have docstrings?_
- [ ] _Have you added/updated unit tests where appropriate?_
- [ ] _Have you updated the baseline images if necessary?_
- [ ] _Have you run the unit tests using `pytest`?_
- [ ] _Is your code style correct (are you using PEP8, pyflakes)?_
- [ ] _Have you documented your new feature/functionality in the docs?_

<!-- If you've added to the docs -->

- [ ] _Have you built the docs using `make html`?_

I am suggesting trying migrating Miniconda builds to Travis. I have determined how to resolve the issue on Windows CI that resulted in #894, but Travis would offer better overall build times for each successful Travis+Appveyor build than continuing with Miniconda on Appveyor.

#### Why Appveyor total build times are longer than on Travis

- Jobs sequential
- Also seems to be additional time needed between jobs in each build

Sample times from recent commits are below.

Having the ability to use either Appveyor or Travis for Windows in general should remain to remain resilient to issues on either CI service, but the discrepancy in total build times between them can be adjusted.

#### Impact of migrating Miniconda greater per job than regular Python

- Each Miniconda job taking longer by up to 2min, 45s.

#### Impact on successful build times

- Potential 12-21 minute reduction in successful build times, depending on whether both 3.6 and 3.7 considered
- Sample Appveyor time now: 25 minutes with Miniconda 3.6 (~35 with Miniconda 3.6 and 3.7)
- Expected Appveyor times after migration: 13-14 minutes without Miniconda
- Expected Travis time after migration: 9:30-10:30 (unknown precisely)

#### Potential small uptick in failed build times, but improvement likely overall

Given a reduction in successful Appveyor build times of about 12 minutes, and a possible (but not known) increase in failing Travis build time of 1-2 minutes, there would need to be about 6-12 times as many failing builds as successful builds in order to avoid at least testing this migration.

### Passing builds

#### Appveyor

| Job | Time elapsed | Result |      
|-----|--------------|--------|          
| Python 3.6 | 6:44  |  PASS  |
| Python 3.7 | 6:49  |  PASS  | 
| Miniconda 3.6 | 9:26 | PASS |
| **Setup and all jobs** | **25:04** | **PASS** |

Note that the sum of the job times was just under 23 minutes, but the total time was about 2 minutes longer.

https://ci.appveyor.com/project/districtdatalabs/yellowbrick/builds/25707894

#### Travis
 | Job | Time elapsed | Result |
|-----|---------------|-------|
| Python 3.6 | 7:29    |   PASS   |
| Python 3.7 |  7:29   | PASS |
| Miniconda 3.6 |  8:41   | PASS |
| Miniconda 3.7 |  9:30    | PASS |
| **All jobs** | **9:30** | **PASS** |

https://travis-ci.com/DistrictDataLabs/yellowbrick/builds/117746928   

### Failing builds

#### Appveyor

| Job | Time elapsed | Result |
|-----|---------------|-------|
| Python 3.6 | 6:50  |    FAIL   |
| **Total time** |  **6:50** | **FAIL** |

In the case of this failed build, the total time equals the time for this single job. Based on this and the total time for the successful build, there seems to be additional time between jobs in a build that are not included in the Appveyor job times, but impact the overall time.

https://ci.appveyor.com/project/districtdatalabs/yellowbrick/builds/25793515

#### Travis

 | Job | Time elapsed | Result |
|-----|---------------|-------|
| Python 3.6 | 7:33    |    FAIL   |
| Python 3.7 |  7:34   | FAIL |
| Miniconda 3.6 |  9:13   | FAIL |
| Miniconda 3.7 |  9:17    | FAIL |
| **Total time** | **9:17** | **FAIL** |

https://travis-ci.com/DistrictDataLabs/yellowbrick/builds/118174237

<!-- This line alerts the Yellowbrick maintainers, feel free to use this
     @ address to alert us directly in follow up comments -->
@DistrictDataLabs/team-oz-maintainers

I would be glad to work on a PR so the impact can be validated and realized.

We've recently been having [some issues](https://github.com/DistrictDataLabs/yellowbrick/issues/902#issuecomment-508462537) ensuring that all of our dependencies are passing tests since our CI only tests the latest version. Currently, our tests have good coverage but do take a while to run, which is slowing down the PR process. Instead of updating our tests with a larger test matrix, we are proposing an advanced matrix that only runs if the version has been bumped, e.g. on a release. This matrix would include all major versions of our dependencies and potentially could expand the number of python versions and implementations (e.g. miniconda), reducing these from our day-to-day test requirements. 

Also for discussion, @jklymak suggests that we [test matplotlib from master](https://github.com/DistrictDataLabs/yellowbrick/pull/914#issuecomment-509009169) so that we can better catch changes in matplotlib dependencies before a release requires a hotfix. 