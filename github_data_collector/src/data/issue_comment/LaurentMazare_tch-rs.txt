
I am implementing the AdamW optimizer for a project. The implementation can benefit from in-place computations, in particular `add_`, `addcmul_`, and `addcdiv_`. The tensor, multiplied tensors, and divided tensors need to be scaled before addition. Unfortunately, the corresponding `Tensor` methods do not take a scalar, instead the default (in the C++ API) of `1.0` is used. So, instead, some temporaries are needed.

It would be nice if the 'scaling scalar' was also exposed as an argument.
Hello, thanks for this awesome lib, I love it!

I could not find a way to define the negative slope of the leaky relu. The bindings appear to not generate the version which takes a slope parameter such as: https://pytorch.org/docs/stable/nn.functional.html#leaky-relu . https://github.com/LaurentMazare/tch-rs/blob/2d94d6b7cf8fc9e179515a021ee18c40a310a712/src/wrappers/tensor_generated.rs#L3835-L3842 

Is this indeed missing or am I not looking in the right place.






Current tch-rs IValue which is used as the input to TorchScript models covers the following input:
- ``Tensor(crate::Tensor)``
- ``Int(i64)``
- ``Double(f64)``
- ``Tuple(Vec<IValue>)``

Taking a look at [TorchScript supported types](https://pytorch.org/docs/stable/jit.html#supported-type) we still need to support:
- [x] `bool` | A boolean value: (https://github.com/LaurentMazare/tch-rs/commit/0e4ecff8fbb73e6151ce2bade5a0857a324d8fe5)
- [x] `str` | A string (https://github.com/LaurentMazare/tch-rs/commit/7d3e9cb445a72608789271c0d3735ca43eaf6edf)
- [x] `List[T]` | A list of which all members are type T
- [ ] `Optional[T]` | A value which is either None or type T
- [x] `Dict[K, V]` | A dict with key type K and value type V. Only str, int, and float are allowed as key 
types.
- [ ] `NamedTuple[T0, T1, ...]`

Are there plans to add support for these types anytime soon?


I'm getting this error : 

```powershell
error: process didn't exit successfully: `target\debug\neural1.exe` (exit code: 0xc0000138, STATUS_ORDINAL_NOT_FOUND)
```

From what i understood it might come from my installation of LibTorch so here's the steps i've followed : 

1. Download LibTorch for c++, CPU, release
2. Unzip the libtorch folder somewhere
3. Added an environment variable called LIBTORCH with the path of the libtorch folder
4. Added the libtorch folder to the PATH
Hello, thank you a lot for this lib.

I've been using it to run inference on the YoloV3 example. However, after inference the thread local variables seem to not be cleaned up. 

To reproduce the issue, just run Yolo and check that the thread now uses around ~200MB of memory, even if it is just sleeping.

The issue seem to be the same as https://github.com/pytorch/pytorch/issues/24237#issuecomment-558882706 . I wonder if there is something we can do about it. Anyway, a warning on the README would be nice to warn people about this.
Could you add benchmark of speed training and interference resnet50 on your rust library and python library on this dataset https://www.dropbox.com/s/ffzfpbwzwdo9qp8/ants_bees_cleared_190806.tar.gz. 

Hi!

Thanks for this great package! I'm looking into using an existing libtorch C++ module with the package and haven't found an example or info on how to do this. I'm fairly new to Rust, so maybe I'm also just not seeing that this is trivial. What's the most idiomatic way of doing this with tch-rs?
I'm trying to port over some of my python code; however coming across some issues due to the way a Sequential currently works.

The current way it's programmed is doesn't allow to add tensors after it's initialization. I'm wanting to dynamically create a model via loops, so I have no idea of the input size or number of layers as it's set dynamically. A bare-bones example:

```
let mut model = tch::nn::seq();

model.add(tch::nn::linear(var_store / "layer_final", input_feature_size, output_size, Default::default()));;
model.add_fn(|xs| xs.sigmoid());
```

Comes up with the error:

```
error[E0382]: use of moved value: `model`
  --> src/main.rs:20:5
   |
17 |     let mut model = tch::nn::seq();
   |         --------- move occurs because `model` has type `tch::nn::sequential::Sequential`, which does not implement the `Copy` trait
18 | 
19 |     model.add(tch::nn::linear(var_store / "layer_final", input_feature_size, output_size, Default::default()));;
   |     ----- value moved here
20 |     model.add_fn(|xs| xs.sigmoid());
   |     ^^^^^ value used here after move
```

Looking at the [source](https://github.com/LaurentMazare/tch-rs/blob/master/src/nn/sequential.rs), the reason this happens is that due to Copy not being implemented, when it returns 'Self' that moves everything meaning that it is no longer accessible further down the line if you're using it this way.

My first thoughts are to either:

- Make Sequential copyable (probably not a valid option, due to variable stores etc.)
- Adding separate non-builder functions that don't return 'Self'

I'm happy to work on putting something together and submit a PR, but thought it would be worth the discussion first as to the best way forward (if it's even wanted). I would think it would be better to add separate functions but I don't know how one would best differentiate them by name. I know some libraries have separate builder classes but that would create quite a breaking change to the API.
Inspired by [Tensor Considered Harmful](http://nlp.seas.harvard.edu/NamedTensor), I'm wondering if it's possible to build compile-time checked tensor type. There prior works to make things possible.

- [typenum](https://github.com/paholg/typenum) for typed numbers. It's necessary for CUDA device ordinal and dimension size.
- [frunk](https://github.com/lloydmeta/frunk) for HList. Each dimension component can be named by a unit struct. HList acts as a `Vec` of distinct types that help us build the type-level dimension.
-  The last is const generics. Though it still unstable and panics the compiler. It would replace typenum in the long run. 

I made a little [demo](https://github.com/jerry73204/rust-typed-tensor-demo/blob/master/tests/test.rs) to demonstrate this idea. May we push it further:

- Introduce tensor operations like `expand`, `multiply`, `flatten` and more into type semantics.
- Since tch is backed on libtorch, a thorough test is required to verify the type design.