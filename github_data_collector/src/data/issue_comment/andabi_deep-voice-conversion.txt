new to python, having a heck of time getting this to work. Was wondering if there is a prebuild windows release available for this kind of like the one there is for DeepFakeLab?

https://github.com/iperov/DeepFaceLab/blob/master/doc/doc_prebuilt_windows_app.md

if not is anyone working on one?
![loss](https://user-images.githubusercontent.com/54697554/69516482-998c8400-0f73-11ea-8d64-1d963f7fa5e2.JPG)
I trained the net2 for almost 3 days and the loss value was 0.0063382 on arctic dataset. Now when I am using the model, the quality of the sound is not good and not clearly audible. Can you please suggest what I can do to improve it ? @andabi 
Who has the TIMIT daraset which this project use? Please contact me.
emil: 2290085762@qq.com
Thanks a lot!!
Using the `Dockerfile` in `scripts` folder I managed to start `train1.py`

```
[1010 11:14:00 @base.py:158] Setup callbacks graph ...
[1010 11:14:00 @summary.py:34] Maintain moving average summary of 0 tensors.
[1010 11:14:02 @base.py:174] Creating the session ...
2019-10-10 11:14:02.528831: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-10-10 11:14:02.660072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-10 11:14:02.661434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:01:00.0
totalMemory: 11.91GiB freeMemory: 11.29GiB
2019-10-10 11:14:02.661929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2019-10-10 11:14:03.584727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-10-10 11:14:03.584761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2019-10-10 11:14:03.584779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2019-10-10 11:14:03.585635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 12072 MB memory) -> physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0, compute capability: 6.1)
[1010 11:14:04 @base.py:182] Initializing the session ...
[1010 11:14:04 @base.py:189] Graph Finalized.
2019-10-10 11:14:06.635234: W tensorflow/core/kernels/queue_base.cc:285] _0_QueueInput/input_queue: Skipping cancelled dequeue attempt with queue not closed
[1010 11:14:06 @concurrency.py:36] Starting EnqueueThread QueueInput/input_queue ...
[1010 11:14:06 @graph.py:70] Running Op sync_variables_from_main_tower ...
[1010 11:14:07 @base.py:209] Start Epoch 1 ...
 12%|########2                                                            |12/100[00:32<01:59, 0.73it/s]
```

However, you can see how it's extremely slow. Even though my GPU is recognised and the memory allocated, this is the actual usage from `nvidia-smi`

```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 430.14       Driver Version: 430.14       CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  TITAN X (Pascal)    Off  | 00000000:01:00.0  On |                  N/A |
| 26%   49C    P2    55W / 250W |   1884MiB / 12194MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
```

ideas?
hello ~ I wonder if default params  is suitable for  ljspeech dataset ? could anyone give some tips for other dataset training.
the result is really different between cmu_slt and other dataset with default params
When I Running train1.py using timit data,  encountered the following error:
```
InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'NcclAllReduce' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

[[{{node AllReduceGrads/NcclAllReduce}} = NcclAllReduce[T=DT_FLOAT, num_devices=2, reduction="sum", shared_name="c0", _device="/device:GPU:0"](tower0/gradients/tower0/net1/prenet/dense1/Tensordot/transpose_1_grad/transpose)]]


```
the folloing the the command line:
```
python train1.py timit
```
but how to register the kernel?






I am trying to see what an output might be like. I have net1 weights and only need net2 weights so that I can put in my own choice of data and see what it would be like on the arctic slm set.
What format and directory layout are the samples in, and do I need to provide a script too?
I am hoping to train Arnold Schwarzenegger
(venv) vyshnavi@vyshnavi-Inspiron-3558:~/Desktop/deep-voice-conversion-master$ python train1.py firsttrain
/home/vyshnavi/Desktop/deep-voice-conversion-master/hparam.py:11: YAMLLoadWarning: calling yaml.load_all() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  for doc in docs:
Traceback (most recent call last):
  File "train1.py", line 78, in <module>
    train(args, logdir=logdir_train1)
  File "train1.py", line 32, in train
    logger.set_logger_dir(logdir)
  File "/home/vyshnavi/venv/local/lib/python2.7/site-packages/tensorpack/utils/logger.py", line 149, in set_logger_dir
    mkdir_p(dirname)
  File "/home/vyshnavi/venv/local/lib/python2.7/site-packages/tensorpack/utils/fs.py", line 29, in mkdir_p
    raise e
OSError: [Errno 13] Permission denied: '/data'
**I am not able to understand what the issue is over here**