Thanks so much for writing this code! I am working on a possible application for my research but need C++ and so have been translating pieces of your code (with proper attribution, of course).

I ran across something odd at [this line of maxvol2](https://github.com/oseledets/TT-Toolbox/blob/master/core/maxvol2.m#L69) It looks like there is an iteration variable and a while loop, but the iteration variable never gets incremented. For the simple tests I've tried, it looks like the loop is always broken anyway, but I can't tell if that was the purpose. If it is intentional, a comment explaining why would be helpful. If not, I think a simple `iter = iter + 1` would avoid a possible infinite loop.
I tried to replicate some results from the "Approximation of 2^d x 2^d matrices" paper. There were results given for the approximate inverse of a 2D Laplacian with the Newton method. However, with my code it already takes more than 20GB of RAM to calculate this Newton iteration for a 2D Laplacian with d=9. What are the reasons for this?  Am I even supposed to minimize the _absolute_ as opposed to the _relative_ residual?
The code:
```
% Approximate inversion of 2D Laplace operator for varying n
d=[5,6,7,8,9,10,11,12];

max_rank=75;
eps=1e-5;

for i=1:8

A=tt_qlaplace_dd([d(i),d(i)]);
I = tt_eye(2,(d(i)*2)); I = tt_matrix(I);
X = 0.2*I;
err=2;
j = 0;

while norm(err,'F') > 1    
    Z = round(2*I-A*X,eps,max_rank);
    X = round(X*Z,eps,max_rank);
    err = I - A*X;
    j = j+1;
end
disp("n="+num2str(2^d(i))+"²" + " Iters: "+num2str(j) + " Error (Frob):" + num2str(norm(err,'F'),'%10.1e\n')); 
end
```
And the output:
```
n=32² Iters: 7 Error (Frob):8.0e-01
n=64² Iters: 9 Error (Frob):7.8e-01
n=128² Iters: 11 Error (Frob):7.7e-01
n=256² Iters: 13 Error (Frob):7.7e-01
Out of memory. Type "help memory" for your options.

Error in tt_tensor/round (line 55)
cr=cr(1:pos1); %Truncate storage if required

Error in tt_matrix/round (line 24)
tt.tt=round(tt.tt,eps,rmax);

Error in testinv (line 16)
    X = round(X*Z,eps,max_rank);
```
Consider the following example
```
M = 192;
N = 64;
tt_x = tt_tensor(reshape(randn(N,1), factor(N)), 1e-3);
tt_P = tt_matrix(rand(M,N), 1e-3, factor(M), factor(N));
tt_P2 = tt_matrix(rand(M,N), 1e-3, [4, 2,2,2,2,3], factor(N)); % mode lengths are the same
res2 = tt_P2 * tt_x; % works ok
res = tt_P * tt_x; % fails
```
I am not an expert in (Q)TT formats, and don't know if it is an expected behavior. If yes, I  suggest you add notes on it to the documentation or matrix multiplication header.

Thanks!
It seems that you forgot to run a command to generate documentation.
Please run 
>> m2html('mfiles','tt2', 'htmldir','tt2/doc', 'recursive', 'on', 'global', 'on');


Line 16 of `tt_dot2.m` on branch TT2.2 contains the following

```
%---------------------------d=size(tt1,1);
```

It should be split into two lines like so -

```
%---------------------------
d=size(tt1,1);
```

That is the same branch linked in the [webpage](http://spring.inm.ras.ru/osel/?page_id=24).

So Ivan believes that it's a distinctive bug in MATLAB version

so on the same matrix, with the same inputs:

x=amen_solve2(A,b,tol,'max_full_size',15000,'kickrank',10,'x0',b,'nswp',15,...
'resid_damp',1 ,'rmax',100);

ttpy
amen_solve: swp=1, max_dx= 1.999E+00, max_res= 1.999E+00, max_rank=12
amen_solve: swp=2, max_dx= 1.939E+00, max_res= 4.579E-01, max_rank=22
amen_solve: swp=3, max_dx= 7.191E-01, max_res= 5.247E+00, max_rank=32
amen_solve: swp=4, max_dx= 4.561E-01, max_res= 2.237E+00, max_rank=42
amen_solve: swp=5, max_dx= 5.151E-01, max_res= 3.403E+00, max_rank=52
amen_solve: swp=6, max_dx= 4.186E-01, max_res= 3.818E+00, max_rank=62
amen_solve: swp=7, max_dx= 4.798E-01, max_res= 2.816E+00, max_rank=72
amen_solve: swp=8, max_dx= 4.060E-01, max_res= 1.799E+00, max_rank=82
amen_solve: swp=9, max_dx= 1.037E-02, max_res= 7.897E-02, max_rank=87
amen_solve: swp=10, max_dx= 7.175E-04, max_res= 1.934E-03, max_rank=97
amen_solve: swp=11, max_dx= 9.770E-05, max_res= 7.761E-04, max_rank=92
amen_solve: swp=12, max_dx= 8.662E-06, max_res= 2.994E-05, max_rank=96
amen_solve: swp=13, max_dx= 1.226E-05, max_res= 2.031E-05, max_rank=97
amen_solve: swp=14, max_dx= 5.315E-06, max_res= 1.115E-05, max_rank=90
amen_solve: swp=15, max_dx= 7.059E-06, max_res= 1.252E-05, max_rank=86

TT-Toolbox
=amen_solve= sweep 1, max_dx: 1.999e+00, max_res: 1.999e+00, max_rank: 12
=amen_solve= sweep 2, max_dx: 1.158e+00, max_res: 7.469e-01, max_rank: 22
=amen_solve= sweep 3, max_dx: 2.800e+00, max_res: 6.261e+01, max_rank: 32
=amen_solve= sweep 4, max_dx: 6.230e-01, max_res: 4.422e+00, max_rank: 42
=amen_solve= sweep 5, max_dx: 5.247e-01, max_res: 1.826e+00, max_rank: 52
=amen_solve= sweep 6, max_dx: 3.155e-01, max_res: 1.740e+00, max_rank: 62
=amen_solve= sweep 7, max_dx: 5.078e-01, max_res: 1.809e+00, max_rank: 72
=amen_solve= sweep 8, max_dx: 3.439e-01, max_res: 2.840e+00, max_rank: 82
=amen_solve= sweep 9, max_dx: 5.985e-01, max_res: 8.537e-01, max_rank: 92
=amen_solve= sweep 10, max_dx: 3.544e-01, max_res: 6.569e-01, max_rank: 102
=amen_solve= sweep 11, max_dx: 3.274e-01, max_res: 4.083e-01, max_rank: 110
=amen_solve= sweep 12, max_dx: 2.944e-01, max_res: 5.549e-01, max_rank: 110
=amen_solve= sweep 13, max_dx: 3.590e-01, max_res: 4.755e-01, max_rank: 110
=amen_solve= sweep 14, max_dx: 3.269e-01, max_res: 1.476e+00, max_rank: 110
=amen_solve= sweep 15, max_dx: 2.507e-01, max_res: 4.531e-01, max_rank: 110

Dear Alexey,

somehow I lost track of changes in tt_tensor slicing.
If you try the following code,
  a = tt_rand([10;20;30], 3, 5);
  b=a(1:6,:,:)
you will see that b is a 2D tensor, containing a(1,:,:) instead of what is expected.
Is there any non-standard syntax for slicing only one dimension?
It seems that the full indexation a(1:6,1:20,1:30) works correctly.

Implement an (approximate) tensor by tensor product over some indices;  
tentative syntax is

``` matlab
c = tenmul(a,b,ind1,ind2,1e-8);
```

where ind1,ind2 are the indices over which the summation is done. 
