I have K3s (release v1.17.0+k3s.1) installed on my Raspberry Pi cluster, running on Raspbian Buster.

I used the standard configuration for installation, and Traefik is being used as the ingress controller.

I would like to disable TLS verification in Traefik by setting the "insecureSkipVerify" setting to "true".
Note. I am running Kubernetes Dashboard with a self-signed certificate. This is on my home network and I'm not too concerned about verifying the validity of the certificate.

K3s appears to install Traefik using a Helm Chart, and I can see the Traefik chart manifest is installed in:
/var/lib/rancher/k3s/server/manifests/traefik.yaml

I have updated the traefik.yaml in this folder to include the additional setting:
```
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: traefik
  namespace: kube-system
spec:
  chart: https://%{KUBERNETES_API}%/static/charts/traefik-1.81.0.tgz
  set:
    rbac.enabled: "true"
    ssl.enabled: "true"
    metrics.prometheus.enabled: "true"
    kubernetes.ingressEndpoint.useDefaultPublishedService: "true"
    # New TLS skip verify certificate setting
    ssl.insecureSkipVerify: "true"
```

How can I apply the updated Traefik settings from my chart manifest file?
**Is your feature request related to a problem? Please describe.**
Automating a k3s ha setup is a bit difficult because if you start k3s using the cluster-init parameter the server will run forever. It would be better to do a brief startup and exit the system once some self check is sucessful. A subsequent start of the first server should just be without the parameter. If provided it should just exit with an error like cluster already initialized. The --server parameter should also be similar it should be use only for the first startup. It should just do a inital sync and exit.

Right now my Vagrant provision scripts looks like this:
```
# first server
K3S_TOKEN=SECRET timeout 60s k3s server --cluster-init --flannel-iface=eth1
K3S_TOKEN=SECRET k3s server --flannel-iface=eth1 &
# other servers
K3S_TOKEN=SECRET timeout 120s k3s server --server https://192.168.10.51:6443 --flannel-iface=eth1
K3S_TOKEN=SECRET k3s server --flannel-iface=eth1 &
```

In a perfect world after cluster init all systemd service files should be the same. Furthermore I should be able to reboot one server at a time without a cluster breakdown. Stopping all servers and starting them one by one after a power outage should also be possible.

**Version:**
k3s version v1.0.1 (e94a3c60)

**Describe the bug**
The external datastore usage is too high while nothing is being deployed in the cluster except the base containers

**To Reproduce**
Run a cluster with two masters and two workers connected to a single mysql database

**Expected behavior**
Database should not grow indefinitely 

**Actual behavior**
K3s is adding 5 rows per second, causing the entire cluster to fail when the datastore is full

**Additional context**
Is there a way to limit the size of the external datastore? I do see that the kine table has binary blobs on each row and its currently around 2M rows with approx 2G of disk used. The cluster is for home use, running on a PI 4 and 3 x86 VMs in esxi. 

We should document the --debug flag and it's environment variable, K3S_DEBUG.

We should have perhaps a Troubleshooting section in our docs that could discuss the debug flag/env var and perhaps we mention Logging here as well. Logging should be separate but we could link to it from this Troubleshooting section I propose.
Is there any way to disable auto deployment for multiple target at once with `--no-deploy`?
Opened via https://github.com/rancher/docs/issues/2178 we use rancher/k3s for tracking docs currently as it's easier to manage on our boards.

What exactly does disable network policy do? From a user standpoint, I don’t see any docs on what the network policy is to begin with (google “k3s network policy” or search the docs page). So, I'm not not sure what is being disabled when I use the `--disable-network-policy` flag or how it correlates to other flags such as the group for networking:
```
--cluster-cidr value                       (networking) Network CIDR to use for pod IPs (default: "10.42.0.0/16")
--service-cidr value                       (networking) Network CIDR to use for services IPs (default: "10.43.0.0/16")
--cluster-dns value                        (networking) Cluster IP for coredns service. Should be in your service-cidr range (default: 10.43.0.10)
--cluster-domain value                     (networking) Cluster Domain (default: "cluster.local")
--flannel-backend value                    (networking) One of 'none', 'vxlan', 'ipsec', or 'flannel' (default: "vxlan")
 ```
This is coming from @ibuildthecloud:

 we really need a someway to have stable and testing releases.  like v1.17.0 shouldn't be considered stable.
 not saying they are low quality, but there's got to be some bake in time.  i think conservative users should not be jumping on the .0 release the day it comes out.

rancher/k3s:v1.17.0+k3s.1

k3s chart: https://github.com/rancher/eio-charts/tree/master/charts/k3s/0.0.5
answer overrides can be found at bottom of this issue. 

k3s is installed as an app inside a Rancher cluster. 
DNS resolution is not working -- a Helm chart fails to deploy from manifest with error: 
```
+ helm_v3 repo add rancher http://releases.rancher.com/server-charts/latest
Error: looks like "http://releases.rancher.com/server-charts/latest" is not a valid chart repository or cannot be reached: Get http://releases.rancher.com/server-charts/latest/index.yaml: dial tcp: i/o timeout
```

Logs / troubleshooting:
```
/ # kubectl apply -f https://k8s.io/examples/admin/dns/dnsutils.yaml
/ # kubectl exec -ti dnsutils -n default -- nslookup rancher.releases.com
;; connection timed out; no servers could be reached

/ # kubectl exec -ti dnsutils -n default -- nslookup google.com
;; connection timed out; no servers could be reached

/ # kubectl get pods -n kube-system
NAME                                      READY   STATUS             RESTARTS   AGE
local-path-provisioner-58fb86bdfd-f2dlp   1/1     Running            0          3h4m
metrics-server-6d684c7b5-l9z82            1/1     Running            0          3h4m
coredns-d798c9dd-ftm4m                    1/1     Running            0          3h4m
helm-install-rancher-x6nsz                0/1     CrashLoopBackOff   31         149m



/ # kubectl logs coredns-d798c9dd-ftm4m -n kube-system
.:53
[INFO] plugin/reload: Running configuration MD5 = 4665410bf21c8b272fcfd562c482cb82
   ______                ____  _   _______
  / ____/___  ________  / __ \/ | / / ___/      ~ CoreDNS-1.6.3
 / /   / __ \/ ___/ _ \/ / / /  |/ /\__ \       ~ linux/amd64, go1.12.9, 37b9550
/ /___/ /_/ / /  /  __/ /_/ / /|  /___/ / 
\____/\____/_/   \___/_____/_/ |_//____/  
[INFO] Reloading
[INFO] plugin/reload: Running configuration MD5 = 15f82cc5e1140623fb92d49a82a77e45
[INFO] Reloading complete
[INFO] Reloading
[INFO] 127.0.0.1:55526 - 23474 "HINFO IN 2281408888788472974.5907971783182429936. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.01187075s
[INFO] plugin/reload: Running configuration MD5 = 10478660745616d3fc72c4df2f9c7a5a
[INFO] Reloading complete



/ # kubectl run nginx --image nginx -n default
kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
deployment.apps/nginx created


/ # kubectl exec -it -n default nginx-6db489d4b7-7xpt2 /bin/bash
root@nginx-6db489d4b7-7xpt2:/# apt-get update
0% [Connecting to deb.debian.org] [Connecting to security.debian.org]^C
# stuck here



root@nginx-6db489d4b7-7xpt2:/# cat etc/resolv.conf 
search default.svc.cluster.local svc.cluster.local cluster.local k3s.svc.cluster.local us-west-2.compute.internal
nameserver 192.169.0.10
options ndots:5
exit



/ # kubectl get svc,ep  -n kube-system
NAME                     TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE
service/kube-dns         ClusterIP   192.169.0.10     <none>        53/UDP,53/TCP,9153/TCP   3h10m
service/metrics-server   ClusterIP   192.169.147.42   <none>        443/TCP                  3h10m

NAME                              ENDPOINTS                                        AGE
endpoints/metrics-server          192.168.2.2:443                                  3h9m
endpoints/kube-dns                192.168.2.3:53,192.168.2.3:53,192.168.2.3:9153   3h9m
endpoints/rancher.io-local-path   <none>                                           3h9m




/ # kubectl get pods -n kube-system -o wide
NAME                                      READY   STATUS             RESTARTS   AGE     IP            NODE                          NOMINATED NODE   READINESS GATES
local-path-provisioner-58fb86bdfd-f2dlp   1/1     Running            0          3h10m   192.168.1.2   k3s-agent-849c48bb76-2z86x    <none>           <none>
metrics-server-6d684c7b5-l9z82            1/1     Running            0          3h10m   192.168.2.2   k3s-agent-849c48bb76-9rxgp    <none>           <none>
coredns-d798c9dd-ftm4m                    1/1     Running            0          3h10m   192.168.2.3   k3s-agent-849c48bb76-9rxgp    <none>           <none>
helm-install-rancher-x6nsz                0/1     CrashLoopBackOff   32         155m    192.168.0.4   k3s-server-7b9c97cbdb-dwgjd   <none>           <none>

/ # kubectl get nodes -o wide
NAME                          STATUS   ROLES    AGE     VERSION         INTERNAL-IP    EXTERNAL-IP   OS-IMAGE   KERNEL-VERSION    CONTAINER-RUNTIME
k3s-agent-849c48bb76-9rxgp    Ready    <none>   3h11m   v1.17.0+k3s.1   10.42.17.202   <none>        Unknown    4.15.0-1040-aws   containerd://1.3.0-k3s.5
k3s-agent-849c48bb76-2z86x    Ready    <none>   3h11m   v1.17.0+k3s.1   10.42.0.62     <none>        Unknown    4.15.0-1043-aws   containerd://1.3.0-k3s.5
k3s-server-7b9c97cbdb-dwgjd   Ready    master   3h11m   v1.17.0+k3s.1   10.42.18.111   <none>        Unknown    4.15.0-1039-aws   containerd://1.3.0-k3s.5

/ # kubectl get pods -A -o wide
NAMESPACE       NAME                                      READY   STATUS             RESTARTS   AGE     IP             NODE                          NOMINATED NODE   READINESS GATES
kube-system     local-path-provisioner-58fb86bdfd-f2dlp   1/1     Running            0          3h11m   192.168.1.2    k3s-agent-849c48bb76-2z86x    <none>           <none>
ingress-nginx   default-http-backend-84cdffc75-gd5cs      1/1     Running            0          3h11m   192.168.1.3    k3s-agent-849c48bb76-2z86x    <none>           <none>
kube-system     metrics-server-6d684c7b5-l9z82            1/1     Running            0          3h11m   192.168.2.2    k3s-agent-849c48bb76-9rxgp    <none>           <none>
kube-system     coredns-d798c9dd-ftm4m                    1/1     Running            0          3h11m   192.168.2.3    k3s-agent-849c48bb76-9rxgp    <none>           <none>
ingress-nginx   nginx-ingress-controller-9klvl            1/1     Running            0          3h11m   10.42.0.62     k3s-agent-849c48bb76-2z86x    <none>           <none>
ingress-nginx   nginx-ingress-controller-qgqpw            1/1     Running            0          3h11m   10.42.17.202   k3s-agent-849c48bb76-9rxgp    <none>           <none>
ingress-nginx   nginx-ingress-controller-hvn5b            1/1     Running            0          3h11m   10.42.18.111   k3s-server-7b9c97cbdb-dwgjd   <none>           <none>
default         dnsutils                                  1/1     Running            2          160m    192.168.0.3    k3s-server-7b9c97cbdb-dwgjd   <none>           <none>
default         nginx-6db489d4b7-7xpt2                    1/1     Running            0          5m52s   192.168.1.5    k3s-agent-849c48bb76-2z86x    <none>           <none>
kube-system     helm-install-rancher-x6nsz                0/1     CrashLoopBackOff   32         156m    192.168.0.4    k3s-server-7b9c97cbdb-dwgjd   <none>           <none>
/ # 
```

Answer overrides for k3s app: 
```
image:
  repository: rancher/k3s
  tag: v1.17.0-k3s.1
server:
  enableTraefik: false
  manifests:
    nginx-ns.yaml: |-
      apiVersion: v1
      kind: Namespace
      metadata:
        name: ingress-nginx
    nginx-conf.yaml: |-
      kind: ConfigMap
      apiVersion: v1
      metadata:
        name: nginx-configuration
        namespace: ingress-nginx
        labels:
          app: ingress-nginx
    nginx-tcp.yaml: |-
      kind: ConfigMap
      apiVersion: v1
      metadata:
        name: tcp-services
        namespace: ingress-nginx
    nginx-udp.yaml: |-
      kind: ConfigMap
      apiVersion: v1
      metadata:
        name: udp-services
        namespace: ingress-nginx
    nginx-sa.yaml: |-
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: nginx-ingress-serviceaccount
        namespace: ingress-nginx
    nginx-rbac.yaml: |-
      apiVersion: rbac.authorization.k8s.io/v1beta1
      kind: ClusterRole
      metadata:
        name: nginx-ingress-clusterrole
      rules:
        - apiGroups:
            - ""
          resources:
            - configmaps
            - endpoints
            - nodes
            - pods
            - secrets
          verbs:
            - list
            - watch
        - apiGroups:
            - ""
          resources:
            - nodes
          verbs:
            - get
        - apiGroups:
            - ""
          resources:
            - services
          verbs:
            - get
            - list
            - watch
        - apiGroups:
            - "extensions"
            - "networking.k8s.io"
          resources:
            - ingresses
            - daemonsets
          verbs:
            - get
            - list
            - watch
        - apiGroups:
            - ""
          resources:
              - events
          verbs:
              - create
              - patch
        - apiGroups:
            - "extensions"
          resources:
            - ingresses/status
          verbs:
            - update
    nginx-role.yaml: |-
      apiVersion: rbac.authorization.k8s.io/v1beta1
      kind: Role
      metadata:
        name: nginx-ingress-role
        namespace: ingress-nginx
      rules:
        - apiGroups:
            - ""
          resources:
            - configmaps
            - pods
            - secrets
            - namespaces
          verbs:
            - get
        - apiGroups:
            - ""
          resources:
            - configmaps
          resourceNames:
            # Defaults to "<election-id>-<ingress-class>"
            # Here: "<ingress-controller-leader>-<nginx>"
            # This has to be adapted if you change either parameter
            # when launching the nginx-ingress-controller.
            - "ingress-controller-leader-nginx"
          verbs:
            - get
            - update
        - apiGroups:
            - ""
          resources:
            - configmaps
          verbs:
            - create
        - apiGroups:
            - ""
          resources:
            - endpoints
          verbs:
            - get
    nginx-rb.yaml: |-
      apiVersion: rbac.authorization.k8s.io/v1beta1
      kind: RoleBinding
      metadata:
        name: nginx-ingress-role-nisa-binding
        namespace: ingress-nginx
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: Role
        name: nginx-ingress-role
      subjects:
        - kind: ServiceAccount
          name: nginx-ingress-serviceaccount
          namespace: ingress-nginx
    nginx-crb.yaml: |-
      apiVersion: rbac.authorization.k8s.io/v1beta1
      kind: ClusterRoleBinding
      metadata:
        name: nginx-ingress-clusterrole-nisa-binding
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: nginx-ingress-clusterrole
      subjects:
        - kind: ServiceAccount
          name: nginx-ingress-serviceaccount
          namespace: ingress-nginx
    nginx-ds.yaml: |-
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        name: nginx-ingress-controller
        namespace: ingress-nginx
      spec:
        selector:
          matchLabels:
            app: ingress-nginx
        template:
          metadata:
            labels:
              app: ingress-nginx
            annotations:
              prometheus.io/port: '10254'
              prometheus.io/scrape: 'true'
          spec:
            affinity:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                    - matchExpressions:
                      - key: beta.kubernetes.io/os
                        operator: NotIn
                        values:
                          - windows
            hostNetwork: true
            serviceAccountName: nginx-ingress-serviceaccount
            tolerations:
            - effect: NoExecute
              operator: Exists
            - effect: NoSchedule
              operator: Exists
            containers:
              - name: nginx-ingress-controller
                image: rancher/nginx-ingress-controller:nginx-0.25.1-rancher1
                args:
                  - /nginx-ingress-controller
                  - --default-backend-service=$(POD_NAMESPACE)/default-http-backend
                  - --configmap=$(POD_NAMESPACE)/nginx-configuration
                  - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
                  - --udp-services-configmap=$(POD_NAMESPACE)/udp-services
                  - --annotations-prefix=nginx.ingress.kubernetes.io
                securityContext:
                  capabilities:
                      drop:
                      - ALL
                      add:
                      - NET_BIND_SERVICE
                  runAsUser: 33
                env:
                  - name: POD_NAME
                    valueFrom:
                      fieldRef:
                        fieldPath: metadata.name
                  - name: POD_NAMESPACE
                    valueFrom:
                      fieldRef:
                        fieldPath: metadata.namespace
                ports:
                - name: http
                  containerPort: 80
                - name: https
                  containerPort: 443
                livenessProbe:
                  failureThreshold: 3
                  httpGet:
                    path: /healthz
                    port: 10254
                    scheme: HTTP
                  initialDelaySeconds: 10
                  periodSeconds: 10
                  successThreshold: 1
                  timeoutSeconds: 1
                readinessProbe:
                  failureThreshold: 3
                  httpGet:
                    path: /healthz
                    port: 10254
                    scheme: HTTP
                  periodSeconds: 10
                  successThreshold: 1
                  timeoutSeconds: 1
    nginx-dep.yaml: |-
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: default-http-backend
        labels:
          app: default-http-backend
        namespace: ingress-nginx
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: default-http-backend
        template:
          metadata:
            labels:
              app: default-http-backend
          spec:
            terminationGracePeriodSeconds: 60
            tolerations:
            - effect: NoExecute
              operator: Exists
            - effect: NoSchedule
              operator: Exists
            containers:
            - name: default-http-backend
              # Any image is permissable as long as:
              # 1. It serves a 404 page at /
              # 2. It serves 200 on a /healthz endpoint
              image: rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1
              livenessProbe:
                httpGet:
                  path: /healthz
                  port: 8080
                  scheme: HTTP
                initialDelaySeconds: 30
                timeoutSeconds: 5
              ports:
              - containerPort: 8080
              resources:
                limits:
                  cpu: 10m
                  memory: 20Mi
                requests:
                  cpu: 10m
                  memory: 20Mi
    nginx-svc.yaml: |-
      apiVersion: v1
      kind: Service
      metadata:
        name: default-http-backend
        namespace: ingress-nginx
        labels:
          app: default-http-backend
      spec:
        ports:
        - port: 80
          targetPort: 8080
        selector:
          app: default-http-backend
    rancher.yaml: |-
      apiVersion: helm.cattle.io/v1
      kind: HelmChart
      metadata:
        name: rancher
        namespace: kube-system
      spec:
        chart: rancher-latest/rancher 
        repo: http://releases.rancher.com/server-charts/latest
        version: 2.3.3
        targetNamespace: cattle-system
        set:
          tls: external
          ingress.tls.source: secret
          rancherImagePullPolicy: Always
          rancherImageTag: master-head

k3sClusterSecret: <randomsecret>
```
We should document what we will be supporting/maintaining so this is clearly defined.
The containerd flag was accidentally added to kubelet and is
deprecated, but needed for cadvisor to properly connect with
the k3s containerd socket, so adding for now.

For #473