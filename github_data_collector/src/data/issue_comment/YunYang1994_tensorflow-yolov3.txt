各位前辈好！
请问运行train.py的时候，比如已经运行了一段时间了，比如说上午10:00的时候需要暂停训练，到了下午14:00的时候想要继续训练，应该怎么操作？我发现终止train.py运行后，再重新运行train.py的时候，,没有接着中断的地方开始训练，所以我应该怎么做才能从中断的地方接着训练？
我试着修改config.py,把`__C.YOLO.ORIGINAL_WEIGHT        = "./checkpoint/yolov3_coco.ckpt"`改成自己的，比如改成`__C.YOLO.ORIGINAL_WEIGHT        = "./checkpoint/yolov3_test_loss=16.4555.ckpt"`，但是再运行train.py的时候，输出的信息没变，依然是：
```
=> Restoring weights from: ./checkpoint/yolov3_coco_demo.ckpt ... 
  0%|          | 0/4014 [00:00<?, ?it/s]2020-01-11 15:11:44.567005: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.73GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-01-11 15:11:45.392378: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.14GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-01-11 15:11:45.420473: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.13GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
train loss: 4195.52:   0%|          | 1/4014 [00:07<8:15:17,  7.41s/it]2020-01-11 15:11:46.625862: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.13GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-01-11 15:11:46.704219: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.13GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
train loss: 1974.33:   0%|          | 2/4014 [00:08<6:09:40,  5.53s/it]2020-01-11 15:11:47.816559: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.14GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-01-11 15:11:47.841658: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.13GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
train loss: 3769.86:   0%|          | 3/4014 [00:09<4:44:16,  4.25s/it]2020-01-11 15:11:49.013124: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.13GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-01-11 15:11:49.089715: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.13GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
train loss: 1902.35:   0%|          | 4/4014 [00:10<3:41:04,  3.31s/it]2020-01-11 15:11:50.022054: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.13GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
train loss: 13.42: 100%|██████████| 4014/4014 [11:29<00:00,  5.82it/s]
=> Epoch:  1 Time: 2020-01-11 15:23:59 Train loss: 546.57 Test loss: 30.60 Saving ./checkpoint/yolov3_test_loss=30.6046.ckpt ...
```
说明还是从头开始训练了，所以怎么样才能接着训练，不从头开始？
从checkpoint文件夹来看，第一次训练生成的文件是：
```
yolov3_test_loss=30.1690.ckpt-1.data-00000-of-00001
yolov3_test_loss=30.1690.ckpt-1.index
yolov3_test_loss=30.1690.ckpt-1.meta
```
而第二次生成的文件是：
```
yolov3_test_loss=30.6046.ckpt-1.data-00000-of-00001
yolov3_test_loss=30.6046.ckpt-1.index
yolov3_test_loss=30.6046.ckpt-1.meta
```
这个ckpt-1出现了2次，上面的是第一次生成的，下面的是第二次生成的。只是每次的loss不一样，然后每组文件的时间戳不一样，所请请问如何接着训练？

Thanks for your sharing!
when I evaluate my data with  yolov3_coco_demo.ckpt(ReadMe providede the link),I meet some errors that I can not solve by adding tf.global_variables_initializer() and some other ops.

the config file that I have set,and my data just 1 class. the "./data/dataset/tfyolo_test_all.txt"  format refer to the readme.md.

__C.YOLO.CLASSES                = "./data/classes/coco.names"
__C.YOLO.ANCHORS                = "./data/anchors/coco_anchors.txt"

__C.TEST.ANNOT_PATH             = "./data/dataset/tfyolo_test_all.txt"
__C.TEST.WEIGHT_FILE            = './ckpt0/yolov3_coco_demo.ckpt' 

the errors as follow：

Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1356, in _do_call
    return fn(*args)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1341, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1429, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [1,1,512,255] rhs shape= [1,1,512,18]
	 [[{{node save/Assign_93}}]]
	 [[save/RestoreV2/_344]]
  (1) Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [1,1,512,255] rhs shape= [1,1,512,18]
	 [[{{node save/Assign_93}}]]
0 successful operations.
0 derived errors ignored.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py", line 1286, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 950, in run
    run_metadata_ptr)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1350, in _do_run
    run_metadata)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [1,1,512,255] rhs shape= [1,1,512,18]
	 [[node save/Assign_93 (defined at evaluate.py:50) ]]
	 [[save/RestoreV2/_344]]
  (1) Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [1,1,512,255] rhs shape= [1,1,512,18]
	 [[node save/Assign_93 (defined at evaluate.py:50) ]]
0 successful operations.
0 derived errors ignored.

Errors may have originated from an input operation.
Input Source operations connected to node save/Assign_93:
 conv_mbbox/weight (defined at /mnt/MountVolume1/xielu/code_hub/codes/gitsfile/tensorflow-yolov3-master/core/common.py:31)

Input Source operations connected to node save/Assign_93:
 conv_mbbox/weight (defined at /mnt/MountVolume1/xielu/code_hub/codes/gitsfile/tensorflow-yolov3-master/core/common.py:31)

Original stack trace for 'save/Assign_93':
  File "evaluate.py", line 168, in <module>
    if __name__ == '__main__': YoloTest().evaluate()
  File "evaluate.py", line 50, in __init__
    self.saver = tf.train.Saver() #ema_obj.variables_to_restore()
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py", line 825, in __init__
    self.build()
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py", line 837, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py", line 875, in _build
    build_restore=build_restore)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py", line 508, in _build_internal
    restore_sequentially, reshape)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py", line 350, in _AddRestoreOps
    assign_ops.append(saveable.restore(saveable_tensors, shapes))
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saving/saveable_object_util.py", line 72, in restore
    self.op.get_shape().is_fully_defined())
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/state_ops.py", line 227, in assign
    validate_shape=validate_shape)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_state_ops.py", line 66, in assign
    use_locking=use_locking, name=name)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py", line 788, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py", line 3616, in create_op
    op_def=op_def)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "evaluate.py", line 168, in <module>
    if __name__ == '__main__': YoloTest().evaluate()
  File "evaluate.py", line 51, in __init__
    self.saver.restore(self.sess, self.weight_file)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py", line 1322, in restore
    err, "a mismatch between the current graph and the graph")
tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

2 root error(s) found.
  (0) Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [1,1,512,255] rhs shape= [1,1,512,18]
	 [[node save/Assign_93 (defined at evaluate.py:50) ]]
	 [[save/RestoreV2/_344]]
  (1) Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [1,1,512,255] rhs shape= [1,1,512,18]
	 [[node save/Assign_93 (defined at evaluate.py:50) ]]
0 successful operations.
0 derived errors ignored.

Errors may have originated from an input operation.
Input Source operations connected to node save/Assign_93:
 conv_mbbox/weight (defined at /mnt/MountVolume1/xielu/code_hub/codes/gitsfile/tensorflow-yolov3-master/core/common.py:31)

Input Source operations connected to node save/Assign_93:
 conv_mbbox/weight (defined at /mnt/MountVolume1/xielu/code_hub/codes/gitsfile/tensorflow-yolov3-master/core/common.py:31)

Original stack trace for 'save/Assign_93':
  File "evaluate.py", line 168, in <module>
    if __name__ == '__main__': YoloTest().evaluate()
  File "evaluate.py", line 50, in __init__
    self.saver = tf.train.Saver() #ema_obj.variables_to_restore()
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py", line 825, in __init__
    self.build()
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py", line 837, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py", line 875, in _build
    build_restore=build_restore)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py", line 508, in _build_internal
    restore_sequentially, reshape)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py", line 350, in _AddRestoreOps
    assign_ops.append(saveable.restore(saveable_tensors, shapes))
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saving/saveable_object_util.py", line 72, in restore
    self.op.get_shape().is_fully_defined())
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/state_ops.py", line 227, in assign
    validate_shape=validate_shape)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_state_ops.py", line 66, in assign
    use_locking=use_locking, name=name)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py", line 788, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py", line 3616, in create_op
    op_def=op_def)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()

if I set the config as this:
__C.YOLO.CLASSES                = "./data/classes/tfyolo.names"
__C.YOLO.ANCHORS                = "./data/anchors/tfyolo_anchors.txt"

__C.TEST.ANNOT_PATH             = "./data/dataset/tfyolo_test_all.txt"
__C.TEST.WEIGHT_FILE            = './ckpt0/yolov3_coco_demo.ckpt' 

No error in evaluate.py,but the map=0.0%
是不能修改分类数量吗？必须使用coco的80？
想问下 在运行 train.py 的时候 出现这样的问题 是什么原因呢？
54:21.558613: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
train loss: 53.78: 100%|████████████████████| 7889/7889 [07:29<00:00, 17.56it/s]
Traceback (most recent call last):
File "train.py", line 184, in 
if name == 'main': YoloTrain().train()
File "train.py", line 161, in train
for test_data in self.testset:
File "/home/administrator/m1/yolo/人脸数据集/tensorflow-yolov3-master/core/dataset.py", line 82, in next
label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes = self.preprocess_true_boxes(bboxes)
File "/home/administrator/m1/yolo/人脸数据集/tensorflow-yolov3-master/core/dataset.py", line 244, in preprocess_true_boxes
label[best_detect][yind, xind, best_anchor, :] = 0
IndexError: index 41 is out of bounds for axis 1 with size 40

请各位大佬指导一下
Hi，我自己训练了一个数据集，然后调用convert_weight.py的时候报下面的错误，请问是什么原因啊？
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-1-2ed498e8931e> in <module>()
     57 cur_weights_num = len(cur_weights_mess)
     58 if cur_weights_num != org_weights_num:
---> 59     raise RuntimeError
     60 
     61 print('=> Number of weights that will rename:\t%d' % cur_weights_num)

RuntimeError:
it would be great to have the .pb in the release files. Not very sure if tf 2.0 is supported here, would be great to have it
请教两个问题哈。
1，如果用GPU做训练的话，需要配置哪些参数？
2，如果有多个GPU，比如4块，需要怎么配置做平行训练呢？
请问在coco 测试集的evalute什么时候能发布？谢谢
作者实验结果大家有复现出来的？？？求助  目前的复现的test loss 只有30%， mAP值 为0；有同样遇到这类型问题的？

请教一下前辈，我在训练自己的数据集时，程序会自动停止。
此时使用训练结果去检测，效果不理想。
同样的训练集使用darknet去训练、检测效果是可以的