```
OpenCV Error: Unspecified error (The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Carbon support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script) in cvShowImage, file /io/opencv/modules/highgui/src/window.cpp, line 583
Traceback (most recent call last):
  File "play_radar.py", line 59, in <module>
    cv2.imshow(title, vis * 2.)  # The data is doubled to improve visualisation
cv2.error: /io/opencv/modules/highgui/src/window.cpp:583: error: (-2) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Carbon support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function cvShowImage
```

I use python3.5.2,  The libgtk2.0-dev and pkg-config have been already installed. Thanks!
Regarding bumblebee, I noticed https://github.com/ori-mrg/robotcar-dataset-sdk/tree/master/models only has Pinhole intrinsic calibration for stereo_narrow_left. stereo_narrow_right, stereo_wide_left, stereo_wide_right.
However, the camera images are stored as stereo/left stereo/right and stereo/centre.

According to the paper:
`stereo_narrow_<left/right>.txt` are for the the `stereo/centre` and `stereo/right` images.
`stereo_wide_<left/right>.txt` are for the the stereo/left and stereo/right images.

So are there two intrinsic calibrations for stereo/right (stereo_narrow_right.txt and stereo_wide_right.txt)? What about intrinsic info for treating stereo/right as a monocular camera?

If for instance I feed images from one of these lenses to a visual odometry algorithm that works on monocular images, which `Pinhole fx fy cx cy` values would be correct?
The mono left camera extrinsic:
-0.0905 1.6375 0.2803 0.2079 -0.2339 1.2321
It seems from the car sensor location image that the translation should be close to:
-1.55 -0.59 -0.16.
It seems there is also a mistake with the mono right camera
Hi, RobotCar Team,

Firstly, thank you for your great dataset!

I followed the https://robotcar-dataset.robots.ox.ac.uk/documentation/#pointcloud-projection-into-images to project pointcloud into images using the Python API provided by the SDK. However, the result seems incorrect as the attachment shows. The code is also in the attachment. Do you have any suggestions?

![screenshot from 2019-01-16 20-37-21](https://user-images.githubusercontent.com/25477396/51249647-de986f00-19ce-11e9-987e-54a9b4f496ec.png)

The code is:
```python
################################################################################
#
# Copyright (c) 2017 University of Oxford
# Authors:
#  Geoff Pascoe (gmp@robots.ox.ac.uk)
#
# This work is licensed under the Creative Commons
# Attribution-NonCommercial-ShareAlike 4.0 International License.
# To view a copy of this license, visit
# http://creativecommons.org/licenses/by-nc-sa/4.0/ or send a letter to
# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.
#
################################################################################

import os
import re
import numpy as np
import matplotlib.pyplot as plt
import argparse

sys.path.append('../dataset_loaders')
from robotcar_sdk.build_pointcloud import build_pointcloud
from robotcar_sdk.transform import build_se3_transform
from robotcar_sdk.image import load_image
from robotcar_sdk.camera_model import CameraModel

image_dir='../data/deepslam_data/RobotCar/loop_ldmrs/2014-06-26-09-24-58/stereo/centre/'
laser_dir='../data/deepslam_data/RobotCar/loop_ldmrs/2014-06-26-09-24-58/ldmrs'
poses_file='../data/deepslam_data/RobotCar/loop_ldmrs/2014-06-26-09-24-58/vo/vo.csv'  # or  '../data/deepslam_data/RobotCar/loop_ldmrs/2014-06-26-09-24-58/gps/ins.csv'  # 
models_dir='../data/robotcar_camera_models'
extrinsics_dir='/mnt/lustre/sunjiankai/App/robotcar-dataset-sdk/extrinsics/'
save_dir='../data/deepslam_data/RobotCar/loop_ldmrs/2014-06-26-09-24-58/depth/'

# args = parser.parse_args()

model = CameraModel(models_dir, image_dir)

extrinsics_path = os.path.join(extrinsics_dir, model.camera + '.txt')
with open(extrinsics_path) as extrinsics_file:
    extrinsics = [float(x) for x in next(extrinsics_file).split(' ')]

G_camera_vehicle = build_se3_transform(extrinsics)
G_camera_posesource = None

poses_type = re.search('(vo|ins)\.csv', poses_file).group(1)
if poses_type == 'ins':
    with open(os.path.join(extrinsics_dir, 'ins.txt')) as extrinsics_file:
        extrinsics = next(extrinsics_file)
        G_camera_posesource = G_camera_vehicle * build_se3_transform([float(x) for x in extrinsics.split(' ')])
else:
    # VO frame and vehicle frame are the same
    G_camera_posesource = G_camera_vehicle

print('G_camera_posesource: ', G_camera_posesource)

timestamps_path = os.path.join(image_dir, os.pardir, model.camera + '.timestamps')
if not os.path.isfile(timestamps_path):
    timestamps_path = os.path.join(image_dir, os.pardir, os.pardir, model.camera + '.timestamps')

timestamp_list = []
with open(timestamps_path) as timestamps_file:
    for i, line in enumerate(timestamps_file):
        timestamp_list.append(int(line.split(' ')[0]))

for timestamp in timestamp_list[1000:]:
    pointcloud, reflectance = build_pointcloud(laser_dir, poses_file, extrinsics_dir,
                                               timestamp - 1e7, timestamp + 1e7, timestamp)
    
    pointcloud = np.dot(G_camera_posesource, pointcloud)
    print('pointcloud.shape: ', pointcloud[:, 0].max(), pointcloud[:, 1].max(), pointcloud[:, 2].max(), pointcloud[:, 0].min(), pointcloud[:, 1].min(), pointcloud[:, 2].min())
    
    image_path = os.path.join(image_dir, str(timestamp) + '.png')
    image = load_image(image_path, model)

    uv, depth = model.project(pointcloud, image.shape)

    plt.imshow(image)
    plt.hold(True)
    plt.scatter(np.ravel(uv[0, :]), np.ravel(uv[1, :]), s=2, c=depth, edgecolors='none', cmap='jet')
    plt.xlim(0, image.shape[1])
    plt.ylim(image.shape[0], 0)
    plt.xticks([])
    plt.yticks([])
    plt.show()
```
Thank you!
I've recently started using this dataset and I've notices some strange misalignments between the various timestamps. 
The images below show the global pose, relative pose (obtained from VO) and stereo frame at the same (or closest) timestamp. As seem, the INS and VO poses are completely different, with the stereo following the VO.
Do the timestamps need to be modified according to the start time of each sensor? 

![INS1](https://user-images.githubusercontent.com/43554262/49728394-8b622c80-fc6a-11e8-9784-800945524f1f.png) ![VO1](https://user-images.githubusercontent.com/43554262/49728402-8f8e4a00-fc6a-11e8-8506-91a6d4a36be6.png) ![Stereo1](https://user-images.githubusercontent.com/43554262/49728403-91f0a400-fc6a-11e8-9913-105a7252c280.png)

![INS2](https://user-images.githubusercontent.com/43554262/49731213-886b3a00-fc72-11e8-9247-706b37873607.png)
![VO2](https://user-images.githubusercontent.com/43554262/49731225-8d2fee00-fc72-11e8-9ef4-f4d99f8f3ff9.png)
![Stereo2](https://user-images.githubusercontent.com/43554262/49731229-8f924800-fc72-11e8-9027-569bf7b9540e.png)





Hello,

I ran ProjectLaserIntoCamera.m function but I got some erroneous results. For example, these are the results I get for timestamp=1400076055044636 (2014-05-14-13-59-05) for the centre camera:
![image](https://user-images.githubusercontent.com/44470694/47503849-b8ea4680-d873-11e8-9b34-a3f08b172591.png)

And after zoom-in:
![image 1](https://user-images.githubusercontent.com/44470694/47503860-bdaefa80-d873-11e8-8a3c-e8b2fe54f743.png)



It is clear that the samples over the pedestrians are not coherent, and also on the background.

Can you please check if you get the same output, and If not, can you please tell me what did I do wrong?

Thanks,
Adam
I've found a few sequences don't have their timestamps of GPS/INS and fisheye camera synchronized. For example the sequence http://robotcar-dataset.robots.ox.ac.uk/datasets/2014-06-23-15-36-04/. It seems that in the sequence when GPS/INS has already turned around at the end of the sequence, the fisheye camera is still a few seconds from the turn. But most of the sequences are well synchronized.
This is just a boundary error, but it is annoying since it happens for the last a few hundred frames of many drives when I try to generate the point cloud for each frame. I though many people might use your SDK, and it may be useful to point it out.

---

### Error:

The error message will be "Matrix dimensions must agree" at the following [line 79](https://github.com/oxford-ori/robotcar-dataset-sdk/blob/master/matlab/RelativeToAbsolutePoses.m#L79) of in file `RelativeToAbsolutePoses.m` 
```
  fractions = (pose_timestamps - vo_timestamps(lower_indices)')./...
    (vo_timestamps(lower_indices+1)'-vo_timestamps(lower_indices)');
```
I figured out that it is caused by the [`pose_timestamps` it not bounded by `vo_timestamps`](https://github.com/oxford-ori/robotcar-dataset-sdk/blob/master/matlab/RelativeToAbsolutePoses.m#L43). If the max `pose_timestamp` is greater than the max `vo_timestamps`,  `upper_index` in line 43 will be empty and `upper_index` in line 46 will be 0.

### Solution:

We could add one function to bound the `pose_timestamp` with `vo_timestamps`, and avoid any changes in your original code. 
 
Add the following line before [line 98 at `BuildPointcloud.m`](https://github.com/oxford-ori/robotcar-dataset-sdk/blob/master/matlab/BuildPointcloud.m#L98)

```
laser_timestamps = boundLaserTimestamps(laser_timestamps, ins_file);
```

Added function is as follows:

```
function [ bounded_timestamps ] = boundLaserTimestamps( laser_timestamps, vo_file )
% Bound the laser timestamps by the max vo timestamps
% Usage:  [ bounded_timestamps ] = boundLaserTimestamps( laser_timestamps, ins_file )
%
% INPUTS:
%   laser_timestamps: array of UNIX timestamps from laser
%   vo_file: csv file containing relative pose data
%
% OUTPUTS:
%   bounded_timestamps: is the bounded by max vo_timestamps
%
% Author: Junsheng Fu (junsheng.fu@tut.fi)


vo_file_id = fopen(vo_file);
headers = textscan(vo_file_id, '%s', 8, 'Delimiter',',');
vo_data = textscan(vo_file_id, '%u64 %u64 %f %f %f %f %f %f','Delimiter',',');
fclose(vo_file_id);

vo_timestamps = vo_data{1};

% only need to handle the up bound
end_timestamp = min(vo_timestamps(end), laser_timestamps(end,1));   
end_index = find(laser_timestamps(:,1) <= end_timestamp, 1, 'last');

bounded_timestamps = laser_timestamps(1:end_index, :);

end
```
