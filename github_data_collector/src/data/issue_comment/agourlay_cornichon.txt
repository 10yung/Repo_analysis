The test for the Kafka DSL is really flaky which makes the whole projects build red.

I tried to understand the root issue but made not progress so far.

```
Starting scenario 'write and read arbitrary Strings to/from topic'

Starting scenario 'use cornichon jsonAssertions on the message value'

- **failed** write and read arbitrary Strings to/from topic [2634 ms]

  Scenario 'write and read arbitrary Strings to/from topic' failed:

  

  at step:

  When I reading the last 1 messages from topic=cornichon

  

  with error(s):

  Not enough messages polled, expected at least 1 but got 0

  

  

  seed for the run was '1576850996627'

  replay only this scenario with the command:

  testOnly *KafkaExample -- "write and read arbitrary Strings to/from topic" "--seed=1576850996627"

   Scenario : write and read arbitrary Strings to/from topic

      main steps

      Given I put message=I am a plain string with key=success to topic=cornichon [206 ms]

      When I reading the last 1 messages from topic=cornichon [798 ms]

      *** FAILED ***

      Not enough messages polled, expected at least 1 but got 0


```






`RandomContext` is encapsulating a `scala.util.Random` which in turn contains `java.util.Random`.

Every time the `RandomContext` is used, its internal are mutated under the cover.

This programming style does not fit at all with the current architecture of the library.

A couple of places rely on this mutation happening behind the scene which makes things more complicated to understand.

What we want instead:
- a `RandomContext` instance is immutable
- the `RandomContext` API is referentially transparent
- `RandomContext` instances passed explicitly everywhere it is needed
Are there any example of tests where events from an event stream are collected asynchronously?

For example:

1. Open stream
2. make one or more api calls to related service
3. terminate event collection, provide collected events as part of scenario context

For (3.), it would be great if we could terminate the stream in the following ways:
* Immediately
* After a fixed amount of time
* After the collected items matches a matcher spec

Please let us know if this has been discussed anywhere that we might reference. If this is possible, we might be able to help by creating examples around this for the docs. If it's not possible, but you can envision a solution that fits well within the current framework, please let us know, and maybe we can help implement support around this.

Great stuff going on here. Thanks for you contribution.
Actually the `release` of an `ResourceStep` is called at the end of the scenario.
It can make the release process complex if it needs some other information, like a special OAuthToken.

Idea: a `ResourceStep` can collect steps.
for example: let's see I need an admin user that is deleted after the tests.
This admin user can create projects.
A scenario could be:
```
{
  WithAdminUser("root") {
    WithProject("project1") {
      When I get("/projects)
      Then assert body.path("count").is(1)

      WithProject("project2") {
        When I get("/projects)
        Then assert body.path("count").is(2)
      } // `project2` is deleted here

      When I get("/projects)
      Then assert body.path("count").is(1)
    } // `project1` is deleted here
  } // `root` is deleted here
}
```

And maybe introduce a way to compose such `ResourceStep` together.

`val GivenAdminUserAndProject = WithAdminUser("root").andThen(WithProject("project1"))`

```
{
  GivenAdminUserAndProject {
    When I get("/projects)
    Then assert body.path("count").is(1)
  } // `project1` and `root` are deleted here
}
```

This could replace `beforeEachScenario` and `Â afterEachScenario`.
As a user of test framework
I need ability to tag specific scenario/feature 
So that I can only include/exclude specific scenarios/features to test run

**Acceptance Criteria**
Scenario A: specify tags on scenario and feature level to include or exclude in the test run
WHEN I run tests with an option to include/exclude given tags
THEN only specified scenarios and/or features are running

As a test framework user
I need to have integrated reporting system
So that there is an overview on test run success and its details.

**Acceptance criteria.**
Scenario A: test reports (.json, .html) are created after a test run
WHEN I run tests
THEN .json and .html files are created as an output in a project folder
AND there is a stack trace for failed tests

This is a must for testing framework to generate test reports. I'm still using scalatest implementation of given framework and would love to change it to the most updated one but I miss reports generation.

Please review the possibility to integrate with such reporter as allure http://allure.qatools.ru/, https://github.com/allure-examples/allure-scalatest-example, https://www.npmjs.com/package/mochawesome or any other reporter that proved its reliability on market.

Thank you! 
When sending or receiving big payloads, if the step fails, then the output is really really long.
Maybe it should be a good idea, depending on the # of lines, to cut the display of some assertions.
Integrating with `Scalatest` helped a lot at the beginning to bootstrap the library.

Currently the better way to use cornichon is through `cornichon-test-framework`.

It is more lightweight and offers more controls on the implementation side.

However it currently lacks  a couple of features to be a complete replacement for `cornichon-scalatest`.

The missing pieces:

- [ ] a main runner to be able to run without SBT (useful when tests are packaged and run separately)
- [ ] generate HTML reports
- [ ] run tests from IDEA - requires the creation of plugin I guess
- [ ] integration with other build tools

Hi,

Would be awesome to have an ability to attach a file while calling certain endpoint.
Example:

```scala
Then assert body.path("errors[0].message").is(
  """Using different values is not valid for field 'color'. Given values are '<undefined>, "red"'. """)
```

It would be nice to be able to have `<undefined>` not being resolved as a placeholder but used literally.

This could probably be done with either another implementation of `is` called e.g. `isRaw` or maybe more flexible by introducing a way to mark `<undefined>` in some way, e.g. `<<undefined>>` which would tell `is` to not evaluate the occurrence and replace it by `<undefined>`.