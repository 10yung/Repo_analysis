It seems to be a little bit confused that the evaluation on classification tasks uses the probabilities output directly in calculating the AUC.  
For example, in [6-xgboost.R#L39](https://github.com/szilard/benchm-ml/blob/4131bc45eed147c69e3ece16aacefaf5bd157af3/3-boosting/6-xgboost.R#L39), 
Will it be better to do that with \(phat>0.5\)?
Did you consider using more datasets? 

And how about regression problems?

There is for example this benchmarking suite, accessible via the OpenML packages: [https://arxiv.org/abs/1708.03731](https://arxiv.org/abs/1708.03731)
As we've discussed in Slack, H2O has recently released some very interesting AutoML functionality. In this case, the leader is the StackedEnsemble generated from a GBM grid, a DL grid, a DRF and an XRT model. On 100k records it trained for a while on some small cloud hardware, and generated a respectable AUC of 0.7284624

```
> md
An object of class "H2OAutoML"
Slot "project_name":
[1] "<default>"

Slot "leader":
Model Details:
==============

H2OBinomialModel: stackedensemble
Model ID:  StackedEnsemble_model_1496028880431_2818 
NULL


H2OBinomialMetrics: stackedensemble
** Reported on training data. **

MSE:  0.06495612
RMSE:  0.2548649
LogLoss:  0.2435769
Mean Per-Class Error:  0.07056041
AUC:  0.9872952
Gini:  0.9745905

Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
           N     Y    Error         Rate
N      54777  1849 0.032653  =1849/56626
Y       1450 11918 0.108468  =1450/13368
Totals 56227 13767 0.047133  =3299/69994

Maximum Metrics: Maximum metrics at their respective thresholds
                        metric threshold    value idx
1                       max f1  0.299564 0.878423 218
2                       max f2  0.243801 0.912848 242
3                 max f0point5  0.362489 0.896238 193
4                 max accuracy  0.313673 0.953653 213
5                max precision  0.974294 1.000000   0
6                   max recall  0.132957 1.000000 309
7              max specificity  0.974294 1.000000   0
8             max absolute_mcc  0.299564 0.849339 218
9   max min_per_class_accuracy  0.253667 0.943118 237
10 max mean_per_class_accuracy  0.247323 0.944984 240

Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`
H2OBinomialMetrics: stackedensemble
** Reported on validation data. **

MSE:  0.1327237
RMSE:  0.3643127
LogLoss:  0.4226191
Mean Per-Class Error:  0.3271404
AUC:  0.7433911
Gini:  0.4867822

Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
           N    Y    Error         Rate
N       9287 2974 0.242558  =2974/12261
Y       1166 1666 0.411723   =1166/2832
Totals 10453 4640 0.274299  =4140/15093

Maximum Metrics: Maximum metrics at their respective thresholds
                        metric threshold    value idx
1                       max f1  0.196506 0.445931 257
2                       max f2  0.114152 0.591573 329
3                 max f0point5  0.307013 0.439652 188
4                 max accuracy  0.579457 0.822434  82
5                max precision  0.950060 1.000000   0
6                   max recall  0.048541 1.000000 396
7              max specificity  0.950060 1.000000   0
8             max absolute_mcc  0.272812 0.299325 207
9   max min_per_class_accuracy  0.165504 0.672539 281
10 max mean_per_class_accuracy  0.156244 0.677032 289

Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`


Slot "leaderboard":
                                             model_id      auc  logloss
1            StackedEnsemble_model_1496028880431_2818 0.742023 0.424990
2  GBM_grid__a70036165806366cd146a852765f4af0_model_3 0.724540 0.472045
3  GBM_grid__a70036165806366cd146a852765f4af0_model_1 0.722181 0.438297
4  GBM_grid__a70036165806366cd146a852765f4af0_model_0 0.720750 0.475918
5                           DRF_model_1496028880431_4 0.718733 0.471836
6                         XRT_model_1496028880431_366 0.718564 0.439938
7   DL_grid__a70036165806366cd146a852765f4af0_model_0 0.715729 0.453427
8   DL_grid__a70036165806366cd146a852765f4af0_model_1 0.715312 0.453516
9  GBM_grid__a70036165806366cd146a852765f4af0_model_8 0.712989 0.443795
10 GBM_grid__a70036165806366cd146a852765f4af0_model_4 0.711725 0.457926
11  DL_grid__a70036165806366cd146a852765f4af0_model_2 0.711247 0.472706
12 GLM_grid__a70036165806366cd146a852765f4af0_model_0 0.709769 0.443991
13 GLM_grid__a70036165806366cd146a852765f4af0_model_1 0.709769 0.443991
14 GBM_grid__a70036165806366cd146a852765f4af0_model_6 0.705461 0.468157
15 GBM_grid__a70036165806366cd146a852765f4af0_model_2 0.703969 0.444650
16 GBM_grid__a70036165806366cd146a852765f4af0_model_5 0.697802 0.483724
17  DL_grid__a70036165806366cd146a852765f4af0_model_4 0.691404 0.497545
18 GBM_grid__a70036165806366cd146a852765f4af0_model_7 0.668311 0.897990
19  DL_grid__a70036165806366cd146a852765f4af0_model_3 0.658246 0.647369
```
Great initiative, thanks for making this public!
You might be interested in extending your benchmarking to the auto-sklearn. https://github.com/automl/auto-sklearn 
I have created a script that can take in a sparse dataset in the pandas HDFS dataframe .h5 format and run a binary classification on it on multiprocessing cluster with auto-sklearn. https://github.com/Motorrat/autosklearn-zeroconf Myself I will try to duplicate your benchmark, but just in case you are on it you might want to try out yourself.
Hey Szilard,

I'd like to replicate your code from beginning to end perhaps on Google Compute Engine (GCE), mainly to test out GCE with Vagrant.  Do you know have a sense of how long the entire process would take assuming a similar server size as what you used on EC2?

Is there a convenient way to run all your scripts in from folder 0 to 4? That is, is there a master script that executes them all?

I notice that the results are written out to the console.  Do you have a script that scrapes all the AUC's for your comparison analysis?

Thanks!

Thanks for great work! We have an open source machine learning library called SMILE (https://github.com/haifengl/smile). We have incorporated your benchmark (https://github.com/haifengl/smile/blob/master/benchmark/src/main/scala/smile/benchmark/Airline.scala). We found that our system is much faster for this data set. For 100K training data on a 4 core machine, we can train a random forest with 500 trees in 100 seconds, and gradient boost trees of 300 trees in 180 seconds. Projected to 32 cores, I think that we will be much faster than all the tools you tested. You can try it out by cloning our project. Then

sbt benchmark/run

This also includes benchmark on USPS data, which you may ignore. Thanks!

Motivation: I can't run mxnet on the 10M records airline set https://github.com/szilard/benchm-ml/issues/29 because `model.matrix` crashes out of RAM (on g2.8xlarge with 60GB or RAM - largest available for GPU instances).

Using `Matrix::sparse.model.matrix` to encode the categorical data would be great (uses <2GB RAM), but I get:

```
Error in asMethod(object) : 
  Cholmod error 'problem too large' at file ../Core/cholmod_dense.c, line 105
```

Strangely on the 1M dataset I get another error:

```
Error: io.cc:50: Seems X, y was passed in a Row major way, MXNetR adopts a column major convention.
```

I know from @glouppe that "RFs in sklearn now support sparse matrices too"
https://twitter.com/glouppe/status/660012865554903040

It would be interesting to see the results with sparse for RF and for logistic regression too. We should see  lower memory footprint and perhaps faster runs. Anyone wants to help w the code (PR)?

This is to collaborate on some issues with Spark RF also addressed by @jkbradley in comments to this post http://datascience.la/benchmarking-random-forest-implementations/ (see comments by Joseph Bradley). cc: @mengxr 

Please see “Absolute Minimal Benchmark” for random forests https://github.com/szilard/benchm-ml/tree/master/z-other-tools and let's use the 1M row training set and the test set linked in from there.

@jkbradley says: One-hot encoder: Spark 1.4 includes this, plus a lot more feature transformers. Preprocessing should become ever-easier, especially using DataFrames (Spark 1.3+).

Yes, indeed. Can you please provide code that reads in the original dataset (pre- 1-hot encoding) and does the 1-hot encoding in Spark. Also, if random forest 1.4 API can use data frames, I guess we should use that for the training. Can you please provide code for that too.

@jkbradley says: AUC/accuracy: The AUC issue appears to be caused by MLlib tree ensembles aggregating votes, rather than class probabilities, as you suggested. I re-ran your test using class probabilities (which can be aggregated by hand), and then got the same AUC as other libraries. We’re planning on including this fix in Spark 1.5 (and thanks for providing some evidence of its importance!).

Fantastic. Can you please share code that does that already? I would be happy to check it out.

http://www.libfm.org/
Factorization machines (FM) are a generic approach that allows to mimic most factorization models by feature engineering. 

http://www.csie.ntu.edu.tw/~cjlin/libffm/
LIBFFM is an open source tool for field-aware factorization machines (FFM). For the formulation of FFM, please see these slides. It has been used to win two recent click-through rate prediction competitions (Criteo's and Avazu's).

They are also very interesting tools. 
