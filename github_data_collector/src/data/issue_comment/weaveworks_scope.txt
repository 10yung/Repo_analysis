<!--
Hi, thank you for opening an issue!
Before hitting the button...

** Is this a REQUEST FOR HELP? **
If so, please have a look at:
- How WeaveScope works : https://www.weave.works/docs/scope/latest/how-it-works/ 
- our troubleshooting page: https://www.weave.works/docs/scope/latest/building/
- our help page, to choose the best channel (Slack, etc.) to reach out: https://weave-community.slack.com/messages/scope/

** Is this a FEATURE REQUEST? **
If so, please search existing feature requests, and if you find a similar one, up-vote it and/or add your comments to it instead.
If you did not find a similar one, please describe in details:
- why: your use-case, specific constraints you may have, etc.
- what: the feature/behaviour/change you would like to see in Weave Scope
Do not hesitate, when appropriate, to share the exact commands or API you would like, and/or to share a diagram (e.g.: asciiflow.com): "a picture is worth a thousand words".

** Is this a BUG REPORT? **
Please fill in as much of the template below as you can.

Thank you!
-->

## What you expected to happen?
Kubernetes to show the hosts of all nodes from Kubernetes cluster and to show all namespaces

## What happened?
It shows only the master node details and its system pods details.

## How to reproduce it?
$ kubectl apply -f "https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl version | base64 | tr -d '\n')"

$ kubectl port-forward -n weave "$(kubectl get -n weave pod --selector=weave-scope-component=app -o jsonpath='{.items..metadata.name}')" 4040

## Anything else we need to know?
<!-- Cloud provider? Hardware? How did you configure your cluster? Kubernetes YAML, KOPS, etc. -->

## Versions:
<!-- Please paste in the output of these commands; 'kubectl' only if using Kubernetes -->
```
$ scope version
scope is not installed on this kubernetes
$ docker version
1.13.1
$ uname -a
Linux XXXXX 3.10.0-1062.4.3.el7.x86_64 #1 SMP Wed Nov 13 23:58:53 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux

$ kubectl version
1.16
```

## Logs:
```
$ docker logs weavescope
```
or, if using Kubernetes:
```
$ kubectl logs <weave-scope-pod> -n <namespace> 
```
time="2020-01-16T08:59:02Z" level=info msg="publishing to: weave-scope-app.weave.svc.cluster.local:80"
<probe> INFO: 2020/01/16 08:59:02.698144 Basic authentication disabled
<probe> INFO: 2020/01/16 08:59:52.705396 command line args: --mode=probe --probe-only=true --probe.docker=true --probe.docker.bridge=docker0 --probe.kubernetes.role=host --probe.publish.interval=4.5s --probe.spy.interval=2s weave-scope-app.weave.svc.cluster.local:80
<probe> INFO: 2020/01/16 08:59:52.705473 probe starting, version 1.12.0, ID 2073a6f658f8315e
<probe> WARN: 2020/01/16 08:59:52.710795 Cannot resolve 'scope.weave.local.': dial tcp 172.17.0.1:53: connect: connection refused
<probe> WARN: 2020/01/16 08:59:52.783117 Error setting up the eBPF tracker, falling back to proc scanning: kernel not supported: got kernel 3.10.0-1062.4.3.el7.x86_64 but need kernel >=4.4
<probe> WARN: 2020/01/16 08:59:52.807210 Error collecting weave status, backing off 10s: Get http://127.0.0.1:6784/report: dial tcp 127.0.0.1:6784: connect: connection refused. If you are not running Weave Net, you may wish to suppress this warning by launching scope with the `--weave=false` option.
<probe> WARN: 2020/01/16 09:00:02.808038 Error collecting weave status, backing off 20s: Get http://127.0.0.1:6784/report: dial tcp 127.0.0.1:6784: connect: connection refused. If you are not running Weave Net, you may wish to suppress this warning by launching scope with the `--weave=false` option.
<probe> WARN: 2020/01/16 09:00:22.808885 Error collecting weave status, backing off 40s: Get http://127.0.0.1:6784/report: dial tcp 127.0.0.1:6784: connect: connection refused. If you are not running Weave Net, you may wish to suppress this warning by launching scope with the `--weave=false` option.
<probe> ERRO: 2020/01/16 09:00:22.844881 Error checking version: Get https://checkpoint-api.weave.works/v1/check/scope-probe?arch=amd64&flag_kernel-version=3.10.0-1062.4.3.el7.x86_64&flag_kubernetes_enabled=true&flag_os=linux&os=linux&signature=BwL9A0iTwtjEKlXj32GZv3PyESFpb5XCcvBf6wvPR1s%3D&version=1.12.0: dial tcp: i/o timeout
<probe> WARN: 2020/01/16 09:00:42.707989 Cannot resolve 'weave-scope-app.weave.svc.cluster.local': lookup weave-scope-app.weave.svc.cluster.local on 10.96.0.10:53: read udp 10.105.208.63:40260->10.96.0.10:53: i/o timeout
<probe> ERRO: 2020/01/16 09:00:52.926855 Error checking version: Get https://checkpoint-api.weave.works/v1/check/scope-probe?arch=amd64&flag_kernel-version=3.10.0-1062.4.3.el7.x86_64&flag_kubernetes_enabled=true&flag_os=linux&os=linux&signature=BwL9A0iTwtjEKlXj32GZv3PyESFpb5XCcvBf6wvPR1s%3D&version=1.12.0: dial tcp: i/o timeout
<probe> ERRO: 2020/01/16 14:23:01.198075 Error checking version: Get https://checkpoint-api.weave.works/v1/check/scope-probe?arch=amd64&flag_kernel-version=3.10.0-1062.4.3.el7.x86_64&flag_kubernetes_enabled=true&flag_os=linux&os=linux&signature=BwL9A0iTwtjEKlXj32GZv3PyESFpb5XCcvBf6wvPR1s%3D&version=1.12.0: dial tcp: i/o timeout

<!-- (If output is long, please consider a Gist.) -->
<!-- Anything interesting or unusual output by the below, potentially relevant, commands?
$ journalctl -u docker.service --no-pager

Jan 16 09:32:18 xxxxx dockerd-current[1927]: 2020-01-16 09:32:18.026, 7342.7361, Information, connection_manager:667: Sent msgtype=1 len=24415 to collector
Jan 16 09:32:18 xxxxx dockerd-current[1927]: 2020-01-16 09:32:18.587, 7342.7370, Information, Reached max process lookup number, duration=0ms
Jan 16 09:32:18 xxxxx dockerd-current[1927]: 2020-01-16 09:32:18.587, 7342.7370, Information, Reached max socket lookup number, tid=22493, duration=0ms
Jan 16 09:32:19 xxxxx dockerd-current[1927]: 2020-01-16 09:32:19.027, 7342.7370, Warning, statsd metrics over limit, giving up
Jan 16 09:32:19 xxxxx dockerd-current[1927]: 2020-01-16 09:32:19.027, 7342.7370, Information, Added 100 statsd metrics for container=k8s_sysdig-agent_sysdig-agent-lkvtj_sysdig-agent_0b1edfb6-4980-4b1a-aaf1-f10b496b80b6_2 (id=b6d3488e55e7)
Jan 16 09:32:19 xxxxx dockerd-current[1927]: 2020-01-16 09:32:19.030, 7342.7370, Warning, statsd metrics over limit, giving up
Jan 16 09:32:19 xxxxx dockerd-current[1927]: 2020-01-16 09:32:19.030, 7342.7370, Information, Added 100 statsd metrics for host=xxxxx
Jan 16 09:32:19 xxxxx dockerd-current[1927]: 2020-01-16 09:32:19.030, 7342.7370, Information, ts=1579167139, ne=30106, de=0, c=4.02, fp=0.39, sr=1, st=0, fl=19
Jan 16 09:32:19 xxxxx dockerd-current[1927]: 2020-01-16 09:32:19.032064 I | embed: rejected connection from "127.0.0.1:49234" (error "tls: first record does not look like a TLS handshake", ServerName "")
Jan 16 09:32:19 xxxxx dockerd-current[1927]: 2020-01-16 09:32:19.035, 7342.7361, Information, connection_manager:667: Sent msgtype=1 len=24394 to collector
Jan 16 09:32:19 xxxxx dockerd-current[1927]: 2020-01-16 09:32:19.565, 7342.7370, Information, Reached max process lookup number, duration=0ms
Jan 16 09:32:19 xxxxx dockerd-current[1927]: 2020-01-16 09:32:19.566, 7342.7370, Information, Reached max socket lookup number, tid=22594, duration=0ms
Jan 16 09:32:20 xxxxx dockerd-current[1927]: 2020-01-16 09:32:20.024, 7342.7370, Warning, statsd metrics over limit, giving up
Jan 16 09:32:20 xxxxx dockerd-current[1927]: 2020-01-16 09:32:20.024, 7342.7370, Information, Added 100 statsd metrics for container=k8s_sysdig-agent_sysdig-agent-lkvtj_sysdig-agent_0b1edfb6-4980-4b1a-aaf1-f10b496b80b6_2 (id=b6d3488e55e7)
Jan 16 09:32:20 xxxxx dockerd-current[1927]: 2020-01-16 09:32:20.027, 7342.7370, Warning, statsd metrics over limit, giving up
Jan 16 09:32:20 xxxxx dockerd-current[1927]: 2020-01-16 09:32:20.028, 7342.7370, Information, Added 100 statsd metrics for host=xxxxx
Jan 16 09:32:20 xxxxx dockerd-current[1927]: 2020-01-16 09:32:20.028, 7342.7370, Information, ts=1579167140, ne=35135, de=0, c=4.33, fp=0.40, sr=1, st=0, fl=19
Jan 16 09:32:20 xxxxx dockerd-current[1927]: 2020-01-16 09:32:20.029414 I | embed: rejected connection from "127.0.0.1:49238" (error "tls: first record does not look like a TLS handshake", ServerName "")
Jan 16 09:32:20 xxxxx dockerd-current[1927]: 2020-01-16 09:32:20.032, 7342.7361, Information, connection_manager:667: Sent msgtype=1 len=24410 to collector
Jan 16 09:32:20 xxxxx dockerd-current[1927]: 2020-01-16 09:32:20.563, 7342.7370, Information, Reached max process lookup number, duration=0ms
Jan 16 09:32:20 xxxxx dockerd-current[1927]: 2020-01-16 09:32:20.563, 7342.7370, Information, Reached max socket lookup number, tid=22697, duration=0ms
Jan 16 09:32:21 xxxxx dockerd-current[1927]: 2020-01-16 09:32:21.035, 7342.7370, Warning, statsd metrics over limit, giving up
Jan 16 09:32:21 xxxxx dockerd-current[1927]: 2020-01-16 09:32:21.035, 7342.7370, Information, Added 100 statsd metrics for container=k8s_sysdig-agent_sysdig-agent-lkvtj_sysdig-agent_0b1edfb6-4980-4b1a-aaf1-f10b496b80b6_2 (id=b6d3488e55e7)
Jan 16 09:32:21 xxxxx dockerd-current[1927]: 2020-01-16 09:32:21.035, 7342.7370, Information, k8s_deleg: found our node: xxxxx
Jan 16 09:32:21 xxxxx dockerd-current[1927]: 2020-01-16 09:32:21.035, 7342.7370, Information, k8s_deleg: delegated node xxxxx ips:  xxxxx id: 8b65c5f6-95e6-416a-a91e-44dc602bbd93 (this node)
Jan 16 09:32:21 xxxxx dockerd-current[1927]: 2020-01-16 09:32:21.035, 7342.7370, Information, k8s_deleg: delegated node xxxxx2. ips:  xxxxx2. id: 4468c33f-4ef2-4e92-a621-f8b00c01351c
Jan 16 09:32:21 xxxxx dockerd-current[1927]: 2020-01-16 09:32:21.035, 7342.7370, Information, k8s_deleg: This node is delegated
Jan 16 09:32:21 xxxxx dockerd-current[1927]: 2020-01-16 09:32:21.038, 7342.7370, Warning, statsd metrics over limit, giving up
Jan 16 09:32:21 xxxxx dockerd-current[1927]: 2020-01-16 09:32:21.038, 7342.7370, Information, Added 100 statsd metrics for host=xxxxx
Jan 16 09:32:21 xxxxx dockerd-current[1927]: 2020-01-16 09:32:21.038, 7342.7370, Information, ts=1579167141, ne=32554, de=0, c=4.21, fp=0.39, sr=1, st=0, fl=19
Jan 16 09:32:21 xxxxx dockerd-current[1927]: 2020-01-16 09:32:21.040132 I | embed: rejected connection from "127.0.0.1:49244" (error "tls: first record does not look like a TLS handshake", ServerName "")

$ journalctl -u kubelet --no-pager
Jan 16 08:57:06 xxxxx kubelet[2657]: E0116 08:57:06.309160    2657 kubelet.go:2267] node "xxxxx" not found
Jan 16 08:57:06 xxxxx kubelet[2657]: E0116 08:57:06.409303    2657 kubelet.go:2267] node "xxxxx" not found
Jan 16 08:57:06 xxxxx kubelet[2657]: E0116 08:57:06.494472    2657 reflector.go:123] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://xxxxx:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dxxxxx&limit=500&resourceVersion=0: dial tcp xxxxx:6443: connect: connection refused
Jan 16 08:57:06 xxxxx kubelet[2657]: E0116 08:57:06.509448    2657 kubelet.go:2267] node "xxxxx" not found
Jan 16 08:57:06 xxxxx kubelet[2657]: E0116 08:57:06.609603    2657 kubelet.go:2267] node "xxxxx" not found
Jan 16 08:57:06 xxxxx kubelet[2657]: E0116 08:57:06.694375    2657 reflector.go:123] k8s.io/kubernetes/pkg/kubelet/kubelet.go:459: Failed to list *v1.Node: Get https://xxxxx:6443/api/v1/nodes?fieldSelector=metadata.name%3Dxxxxx&limit=500&resourceVersion=0: dial tcp xxxxx:6443: connect: connection refused
Jan 16 08:57:06 xxxxx kubelet[2657]: E0116 08:57:06.709726    2657 kubelet.go:2267] node "xxxxx" not found
Jan 16 08:57:06 xxxxx kubelet[2657]: E0116 08:57:06.809843    2657 kubelet.go:2267] node "xxxxx" not found
Jan 16 08:57:06 xxxxx kubelet[2657]: E0116 08:57:06.894149    2657 reflector.go:123] k8s.io/client-go/informers/factory.go:134: Failed to list *v1beta1.CSIDriver: Get https://xxxxx:6443/apis/storage.k8s.io/v1beta1/csidrivers?limit=500&resourceVersion=0: dial tcp xxxxx:6443: connect: connection refused
Jan 16 08:57:06 xxxxx kubelet[2657]: E0116 08:57:06.909999    2657 kubelet.go:2267] node "xxxxx" not found
Jan 16 08:57:07 xxxxx kubelet[2657]: E0116 08:57:07.010142    2657 kubelet.go:2267] node "xxxxx" not found
Jan 16 08:57:07 xxxxx kubelet[2657]: E0116 08:57:07.094201    2657 reflector.go:123] k8s.io/client-go/informers/factory.go:134: Failed to list *v1beta1.RuntimeClass: Get https://xxxxx:6443/apis/node.k8s.io/v1beta1/runtimeclasses?limit=500&resourceVersion=0: dial tcp xxxxx:6443: connect: connection refused
Jan 16 08:57:07 xxxxx kubelet[2657]: E0116 08:57:07.110312    2657 kubelet.go:2267] node "xxxxx" not found
Jan 16 08:57:07 xxxxx kubelet[2657]: E0116 08:57:07.114392    2657 eviction_manager.go:246] eviction manager: failed to get summary stats: failed to get node info: node "xxxxx" not found
Jan 16 08:57:07 xxxxx kubelet[2657]: E0116 08:57:07.210470    2657 kubelet.go:2267] node "xxxxx" not found
Jan 16 08:57:07 xxxxx kubelet[2657]: E0116 08:57:07.294892    2657 reflector.go:123] k8s.io/kubernetes/pkg/kubelet/kubelet.go:450: Failed to list *v1.Service: Get https://xxxxx:6443/api/v1/services?limit=500&resourceVersion=0: dial tcp xxxxx:6443: connect: connection refused
Jan 16 08:57:07 xxxxx kubelet[2657]: E0116 08:57:07.310636    2657 kubelet.go:2267] node "xxxxx" not found
Jan 16 08:57:07 xxxxx kubelet[2657]: E0116 08:57:07.410840    2657 kubelet.go:2267] node "xxxxx" not found
Jan 16 08:57:07 xxxxx kubelet[2657]: E0116 08:57:07.494997    2657 reflector.go:123] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://xxxxx:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dxxxxx&limit=500&resourceVersion=0: dial tcp xxxxx:6443: connect: connection refused
Jan 16 08:57:07 xxxxx kubelet[2657]: E0116 08:57:07.510939    2657 kubelet.go:2267] node "xxxxx" not found
Jan 16 08:57:07 xxxxx kubelet[2657]: E0116 08:57:07.611131    2657 kubelet.go:2267] node "xxxxx" not found
Jan 16 08:57:07 xxxxx kubelet[2657]: E0116 08:57:07.694982    2657 reflector.go:123] k8s.io/kubernetes/pkg/kubelet/kubelet.go:459: Failed to list *v1.Node: Get https://xxxxx:6443/api/v1/nodes?fieldSelector=metadata.name%3Dxxxxx&limit=500&resourceVersion=0: dial tcp xxxxx:6443: connect: connection refused
Jan 16 08:57:07 xxxxx kubelet[2657]: E0116 08:57:07.711294    2657 kubelet.go:2267] node "xxxxx" not found
Jan 16 08:57:07 xxxxx kubelet[2657]: E0116 08:57:07.811453    2657 kubelet.go:2267] node "xxxxx" not found
Jan 16 08:57:07 xxxxx kubelet[2657]: E0116 08:57:07.901701    2657 reflector.go:123] k8s.io/client-go/informers/factory.go:134: Failed to list *v1beta1.CSIDriver: Get https://xxxxx:6443/apis/storage.k8s.io/v1beta1/csidrivers?limit=500&resourceVersion=0: dial tcp xxxxx:6443: connect: connection refused
Jan 16 08:57:07 xxxxx kubelet[2657]: E0116 08:57:07.911591    2657 kubelet.go:2267] node "xxxxx" not found
Jan 16 08:57:08 xxxxx kubelet[2657]: E0116 08:57:08.011758    2657 kubelet.go:2267] node "xxxxx" not found
Jan 16 08:57:08 xxxxx kubelet[2657]: E0116 08:57:08.094725    2657 reflector.go:123] k8s.io/client-go/informers/factory.go:134: Failed to list *v1beta1.RuntimeClass: Get https://xxxxx:6443/apis/node.k8s.io/v1beta1/runtimeclasses?limit=500&resourceVersion=0: dial tcp xxxxx:6443: connect: connection refused
Jan 16 08:57:08 xxxxx kubelet[2657]: E0116 08:57:08.111911    2657 kubelet.go:2267] node "xxxxx" not found
Jan 16 08:57:08 xxxxx kubelet[2657]: E0116 08:57:08.182214    2657 cni.go:379] Error deleting _/13385c77546d9157fe463da2704f657a93bd1f25929a3a36be8dfb7100173763 from network calico/k8s-pod-network: error getting ClusterInformation: Get https://[10.96.0.1]:443/apis/crd.projectcalico.org/v1/clusterinformations/default: dial tcp 10.96.0.1:443: i/o timeout
Jan 16 08:57:08 xxxxx kubelet[2657]: W0116 08:57:08.183079    2657 cni.go:328] CNI failed to retrieve network namespace path: Error: No such container: 8c5fd6577cd5cb161712556c346333279b482e80e207324fbb53fde8ba5a9558
Jan 16 08:57:08 xxxxx kubelet[2657]: E0116 08:57:08.183398    2657 remote_runtime.go:128] StopPodSandbox "13385c77546d9157fe463da2704f657a93bd1f25929a3a36be8dfb7100173763" from runtime service failed: rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod "_" network: error getting ClusterInformation: Get https://[10.96.0.1]:443/apis/crd.projectcalico.org/v1/clusterinformations/default: dial tcp 10.96.0.1:443: i/o timeout
Jan 16 08:57:08 xxxxx kubelet[2657]: E0116 08:57:08.183458    2657 kuberuntime_manager.go:878] Failed to stop sandbox {"docker" "13385c77546d9157fe463da2704f657a93bd1f25929a3a36be8dfb7100173763"}
Jan 16 08:57:08 xxxxx kubelet[2657]: E0116 08:57:08.183511    2657 kuberuntime_manager.go:658] killPodWithSyncResult failed: failed to "KillPodSandbox" for "a1a1da6b0be8db05b675c252e777d252" with KillPodSandboxError: "rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \"_\" network: error getting ClusterInformation: Get https://[10.96.0.1]:443/apis/crd.projectcalico.org/v1/clusterinformations/default: dial tcp 10.96.0.1:443: i/o timeout"
Jan 16 08:57:08 xxxxx kubelet[2657]: E0116 08:57:08.183559    2657 pod_workers.go:191] Error syncing pod a1a1da6b0be8db05b675c252e777d252 ("kube-apiserver-xxxxx_kube-system(a1a1da6b0be8db05b675c252e777d252)"), skipping: failed to "KillPodSandbox" for "a1a1da6b0be8db05b675c252e777d252" with KillPodSandboxError: "rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \"_\" network: error getting ClusterInformation: Get https://[10.96.0.1]:443/apis/crd.projectcalico.org/v1/clusterinformations/default: dial tcp 10.96.0.1:443: i/o timeout"
Jan 16 08:57:08 xxxxx kubelet[2657]: E0116 08:57:08.212075    2657 kubelet.go:2267] node "xxxxx" not found
Jan 16 08:57:08 xxxxx kubelet[2657]: 2020-01-16 08:57:08.224 [INFO][4148] k8s.go 228: Using Calico IPAM
Jan 16 08:57:08 xxxxx kubelet[2657]: E0116 08:57:08.295452    2657 reflector.go:123] k8s.io/kubernetes/pkg/kubelet/kubelet.go:450: Failed to list *v1.Service: Get https://xxxxx:6443/api/v1/services?limit=500&resourceVersion=0: dial tcp xxxxx:6443: connect: connection refused
Jan 16 08:57:08 xxxxx kubelet[2657]: E0116 08:57:08.312246    2657 kubelet.go:2267] node "xxxxx" not found
Jan 16 08:57:08 xxxxx kubelet[2657]: E0116 08:57:08.412397    2657 kubelet.go:2267] node "xxxxx" not found
Jan 16 08:57:08 xxxxx kubelet[2657]: E0116 08:57:08.495590    2657 reflector.go:123] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://xxxxx:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dxxxxx&limit=500&resourceVersion=0: dial tcp xxxxx:6443: connect: connection refused
Jan 16 08:57:08 xxxxx kubelet[2657]: E0116 08:57:08.512563    2657 kubelet.go:2267] node "xxxxx" not found
Jan 16 08:57:08 xxxxx kubelet[2657]: E0116 08:57:08.612746    2657 kubelet.go:2267] node "xxxxx" not found
Jan 16 08:57:08 xxxxx kubelet[2657]: E0116 08:57:08.695654    2657 reflector.go:123] k8s.io/kubernetes/pkg/kubelet/kubelet.go:459: Failed to list *v1.Node: Get https://xxxxx:6443/api/v1/nodes?fieldSelector=metadata.name%3Dxxxxx&limit=500&resourceVersion=0: dial tcp xxxxx:6443: connect: connection refused
Jan 16 08:57:08 xxxxx kubelet[2657]: E0116 08:57:08.712920    2657 kubelet.go:2267] node "xxxxx" not found


$ kubectl get events
-->


Dockershim has added a label `io.kubernetes.docker.type` for at least four years, where the pause container is of type `podsandbox`.  This should be more reliable than trying to keep up with everyone's name for the pause container.

I don't like the way we do this in two places (one probe, one app), but it seemed a bit complicated to unwind that right now.

I left in the old name-based code just in case; I don't think it is necessary any more.
The number is 2104, so "2.10400k" is not absolutely wrong but it looks weird.

![image](https://user-images.githubusercontent.com/8125524/72269025-c88db080-361a-11ea-979c-e2f20d794877.png)

Scope does not use Ruby, but we (apparently) need this file to make the Netlify website build go.

And Netlify builds have been failing recently saying the version had to be at least 2.5.2.

There are some comments about checking what is still in use every 30 seconds, but no implementation.

An entry is created each time a pipe is opened, but they are never removed.
There are no calls to `Delete()` in the code.

<!--
Hi, thank you for opening an issue!
Before hitting the button...

** Is this a REQUEST FOR HELP? **
If so, please have a look at:
- How WeaveScope works : https://www.weave.works/docs/scope/latest/how-it-works/ 
- our troubleshooting page: https://www.weave.works/docs/scope/latest/building/
- our help page, to choose the best channel (Slack, etc.) to reach out: https://weave-community.slack.com/messages/scope/

** Is this a FEATURE REQUEST? **
If so, please search existing feature requests, and if you find a similar one, up-vote it and/or add your comments to it instead.
If you did not find a similar one, please describe in details:
- why: your use-case, specific constraints you may have, etc.
- what: the feature/behaviour/change you would like to see in Weave Scope
Do not hesitate, when appropriate, to share the exact commands or API you would like, and/or to share a diagram (e.g.: asciiflow.com): "a picture is worth a thousand words".

** Is this a BUG REPORT? **
Please fill in as much of the template below as you can.

Thank you!
-->

## What you expected to happen?
can scope support misps64le
## What happened?
<!-- Error message, actual behaviour, etc. -->

## How to reproduce it?
<!-- Specific steps, as minimally and precisely as possible. -->

## Anything else we need to know?
<!-- Cloud provider? Hardware? How did you configure your cluster? Kubernetes YAML, KOPS, etc. -->

## Versions:
<!-- Please paste in the output of these commands; 'kubectl' only if using Kubernetes -->
```
$ scope version
$ docker version
$ uname -a
$ kubectl version
```

## Logs:
```
$ docker logs weavescope
```
or, if using Kubernetes:
```
$ kubectl logs <weave-scope-pod> -n <namespace> 
```
<!-- (If output is long, please consider a Gist.) -->
<!-- Anything interesting or unusual output by the below, potentially relevant, commands?
$ journalctl -u docker.service --no-pager
$ journalctl -u kubelet --no-pager
$ kubectl get events
-->


We got these messages:
```
<app> ERRO: 2019/12/04 10:36:06.456145 Error 500: NoCredentialProviders: no valid providers in chain
<app> WARN: 2019/12/04 10:36:06.456209 GET /api/probes?sparse (500) 2m9.249389377s Response: "\"NoCredentialProviders: no valid providers in chain\"false" ws: false; ...
```

and it took a while to figure out it was AWS credentials where the problem lay.
Also we shouldn't send that back to the caller - it should be a more opaque "internal error" type of message.
Currently Scope can open a shell in a container, if the container image contains a shell.

[Ephemeral containers](https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/) is a new feature in Kubernetes, alpha as of 1.16, where you can inject a whole container into a pod for troubleshooting.

If we turned on process namespace sharing it would be close to being in the same container.  Not sure what to do about volumes.
For example by adding a boolean flag option.

* For users who know they want to use a fixed version and not get distracted by the upgrades
* For demo purposes when that information is not useful
