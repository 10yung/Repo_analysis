This replaces the waterfall workflow with dumping a serialized DDSketch instead. It's not meant to be integrated into rpc-perf proper but instead shows how rpc-perf can be used to generate latency graphs.

The graph generation parts within this PR are specialized to my end-of-internship presentation so they probably won't generalize to anything beyond comparing two latency distributions. However, it should be enough to show how to draw these graphs.

Hello, I was looking at testing more complex commands for redis such as mget, mset, eval, evalsha which were supported in the previous version, but not in the recent rewrite. I've tried to prototype how I think it could be possible to support commands that have additional options such as the number of keys, a script, script arguments, and so on. 

I've prototyped for mget and mset, and was wondering if my approach was correct here ðŸ˜„ If so, I'll continue to add more commands since we are looking at heavily benchmarking different workloads for redis.

Thanks!
On client systems that have multiple NICs/IP addresses, it would be useful to be able to specify which local interface (either ip address or interface name, e.g. eth0) to use for the connection to the server.

An example...I have a 10Gb NIC and a 25Gb NIC on my client system. It is not clear which will be used by rpc-perf.
the current warmup behavior is to spin endlessly on the warmup workload(s) until the target hit rate is attained. however, there is the possibility that the target hit rate will never be reached, and we should detect this condition by monitoring the hit rate to see if it levels off over some number of windows.
A few engineers on our team reported that without specifying the request connect timeout setting, reconnect on timeout has the old behavior without exponential back off. We should make exponential back off the default, since it is the most sensible reconnect policy.
My current use case of rpc-perf with explicitly sized dataset involves first configuring rpc-perf to send write heavy traffic to warm up the cache, then restarting rpc-perf with the workload being tested. It would be nice if we had a "warm up" option that would systematically write all keys, then automatically transition to the configured workload. 
connect is a fairly expensive operation for the cache server, at large volume it can easily overwhelm the server thread accepting new connections temporarily, leading to test instabilities. Since it's currently impossible to coordinate different rpcperf instances at a fine granularity, I think we should introduce a few features into the connect behavior to reduce the chance of "reconnect storms":
- allow connect to be rate-limited, and the rate can be set in the config;
- allow timeouts to be retried with an exponential backoff, with a max cap that can be configured.
Let's add integration tests. Initial thought is to have a mock server for each protocol. Open to other ideas on this. 
