Hello, Dears,

Thank you for a super good tuber package. I love all of it. Though one thing missing for my Christmas happiness is to find out how to make get_captions function work. 
Now I am getting these errors:
 get_captions(video_id="N708P-A45D0")
Error in get_captions(video_id = "N708P-A45D0") : 
  Must specify a valid id.
> get_captions(id = "y3ElXcEME3lSISz6izkWVT5GvxjPu8pA")
Auto-refreshing stale OAuth token.
Error: HTTP failure: 403
> get_captions(id = "McIW_Nn8xf8", lang = "en")
Error: HTTP failure: 404

Can anybody advise? 

Best wishes,
Tija 

<div class="row">
  <div class="contents col-md-9">
    <div class="page-header">
      <h1>contentDetails Issue</h1>
    </div>


<p>I am having an issue with get_video_details(). When I try to save the video details to my workspace I am losing the information that I am interested (e.g., video duration); however, when I just run the function without assigning to my environment, I get all the correct information in my console. See below for the example of what I mean. </p>


<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="co">#
 df<- get_video_details(video_id ='8ZVX17ZIhyI', part = 'contentDetails') 
#does not work for me and the df includes none of the contentDetails information 
image of what my output looks like is here: https://imgur.com/a/LZmLP8o
<p>
get_video_details(video_id ='8ZVX17ZIhyI', part = 'contentDetails') 
#does work for me but is only in the console. 

</span></a></code></pre></div>


[](url)


Thank you for a very useful package!
I was wondering however if the 50 max results limit can be extended? I have 400+ subscriptions which I wanted to periodically back up using R and I thought tuber could do it, but with the max_results limit I see no way out.

Added `yt_authorized` for a logical check to get whether you have a token or not, including extracting/fetching the token with `yt_token`.  

Also added `read_sbv` for a way to read `sbv` captions, and added an argument in `get_captions` to return a character vector vs. raw vector.

Maybe we should consider new `gargle` way of doing things: https://gargle.r-lib.org/articles/gargle-auth-in-client-package.html
If I include the query argument it throws an error below:

Error in f(init, x[[i]]) : is.request(y) is not TRUE

In my checking of the code, the tuber_GET is already using the argument 'query' but exclusively for some specific things. According to the docu, the query (q=) could be delivered through "..." at the end of the argument list.

The original purpose of this function might not accommodate the keyword search. But I think it should be necessary as the function only retrieves max 50 playlists of the given channel. Many channels are growing adding more than 50 playlists. Thanks for your contribution!
Hi, I am having a problem with the list_channel_resources function with the username filter. I am interested in giving username and getting channel id. The code is generating the desired result for some usernames but not others. For some usernames, tuber will return incomplete results (omit channel id). It does not seem that there is something wrong about the specific usernames I am giving, however. When I run the corresponding function of list_channel_resources with the youtube provided python code (https://google-developers.appspot.com/youtube/v3/code_samples/code_snippet_instructions#toggle-code-snippets-and-full-samples), I get results without NA values.

Following code is what I tried with Tuber, and the csv file is attached below. So the tuber command works for userids$userid[1] but not for userids$userid[2] or userids$userid[3] (userids$userid[2] worked once but not for 20+ other attempts). By not working, I mean, they give empty list for [items] in tmp, unlike userids$userid[1]'s result giving [[1]] that consists of [kind], [etag], and [id]. Both of them works in python code and gives me channel ids.
[Username.zip](https://github.com/soodoku/tuber/files/2984797/Username.zip)

userids<-read.csv('clean_user_id.csv')
user_channelid<-list()
#loop
for(i in 1:69){
  k=userids$userid[i]
  tmp<-list_channel_resources(filter=c(username=k), part="id")
  user_channelid[i]<-tmp[["items"]][[1]][["id"]]
  print(i)
}


#individual
k=userids$userid[3]
k
tmp<-list_channel_resources(filter=c(username=k), part="id")
"list_channel_resources"
tmp[["items"]][[1]][["id"]]

<!-- language-all: lang-r -->
<sup>Created on 2019-03-19 by the [reprex package](https://reprex.tidyverse.org) (v0.2.1)</sup>

i am able to pass authentication with yt_oauth but when I try to do get_channel_stats or list_channel_resources it returns: "Error: HTTP failure: 403"

get_channel_stats(channel_id="UCMtFAi84ehTSYSE9XoHefig")

Can you help?
ty

Just a heads up:
> search1 <- yt_search(term="drone")
> nrow(search1)
[1] 572
> 
> search2 <- yt_search(term="drone",max_results=50)
> nrow(search2)
[1] 572

Also, if there is the length of returned searches always constrained near 500? Some searches that I expect 1000's of hits ("wildlife, or "cats") only seem to produce 450, or 550 results?
I've installed tuber on my ubuntu RStudio Server. As I'm running R in my browser, there's of course no possibility that I'm able to use the httpuv generated `.httr-oauth` file as my browser isn't able to redirect to my server via localhost:1410.

But the first time I called the `yt_oauth` function, muscle memory kicked in and I selected option 1 (use the .httr-oauth file) instead of 2. (out-of-band option). 

Somehow I'm not able to find the possibility to enable option 2 after I selected option 1. I tried removing the httpuv package, but tuber is dependent on it, wether or not I've selected option 1 or 2.

I removed the existing .httr-oauth file manually and with yt_oath(keys, remove_old_oauth = TRUE) but I can't "reselect" option 2 instead of 1. Am I missing something?

Hi, first of all thank you for your awesome R package to scrape for comments at youtube. I am using your package to analyse some comments, but I came up with the problem that not all comments can be collected. I think the issue have been mentioned in other issues (get_comment_threads) as well but this problem focuses on the method "get_all_comments". The original video has 3040 comments, the function returns only 2335 records, so approximately 30% get lost. The bigger problem in my opinion is the returning of the replies. Looking at the user in "top comment" category it can be seen that the original video counts 34 different replies, the function returns only 5, so the communication between different users will be lost.

`comments <- get_all_comments(video_id = "zz-RpiUFY-I")`