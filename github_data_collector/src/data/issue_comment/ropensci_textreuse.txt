I am encountering an issue using the TextReuseCorpus function where I feed in a vector of texts (using the "text = " option in the function, and: (1) receive a warning of skipped texts due to insufficient length on character strings that should be long enough; and (2) get a different number of skip warnings each time. I am reading in a large vector (>300,000) of texts, ranging from 155 to 9900 characters, and usually 30k to 150k are skipped for being too short. I can take these same skipped strings, run TextReuseCorpus on them, and they'll be fine this time around. Perhaps I'm simply doing something wrong?
When using Windows `readLines` will expect text files to be encoded in Windows-1252.

This would add an optional encoding argument to `TextReuseCorpus` as well as  `TextReuseTextDocument` which can be used to explicitly specify the encoding of the input files (mostly UTF-8).

As it defaults to "unknown" which is the default for `readLines` this should maintain backward compatability.

Edit: I forgot to mention that this specific issue can be worked around by setting `options(encoding = "UTF-8")` before creating the corpus however this has some side effects so I still think having an encoding argument is the better way to deal with this.
As I am reading it, the TextReuseCorpus function has some safety check in order not to run tokenizers on documents that are too short, "too short" being documents too small to generate two ngrams of the requested size. In addition to that, the tokenizers seem to have their own assertions to prevent running with too short documents.

However, I have run into problems with skipgrams. First, the safety check in TextReuseCorpus lets documents pass that the assertion in tokenize_skip_ngrams then bails out on, because the latter assertion assumes a larger minimum document length. Second, I don't quite understand why the assertion would require this in the first place. IIUC, it's `n + n * k - k <= length(words)`, but why would I not be able to generate skipgrams with the same document length as that of the ngram tokenizer (`n < length(words)`).

FWIW, I am trying to build large skipgrams, say, with `n=15` and `k = 3`.

https://github.com/ropensci/textreuse/blob/35f8421d16ed4348d5784a2cbf4a42067e8813b2/R/tokenizers.R#L59

Thanks for any pointers or insights.
Hello from rOpenSci!

The official [rOpenSci docs server](https://docs.ropensci.org) which we [announced](https://ropensci.org/technotes/2019/06/07/ropensci-docs/) in June is fully ready for production now. Our server automatically builds and hosts pkgdown sites for all ropensci packages.

 - Official documentation URL: https://docs.ropensci.org/textreuse
 - Website build logs: https://dev.ropensci.org/job/textreuse (click "last build" -> "console output")

If all seems good, we strongly suggest to add the URL to the package DESCRIPTION file and include this in the next CRAN update. This has two major benefits:

  - Pkgdown does automatic [cross-linking](https://pkgdown.r-lib.org/dev/articles/linking.html) to other pkgdown sites that can be found via CRAN. This means that if another package refers to your package in an example or vignette, their pkgdown site automatically hyperlinks those functions to your pkgdown site (if your pkgdown URL has been published on CRAN!)
  - Because all our documentation is hosted under `docs.ropensci.org` this will accumulate a higher pagerank than when a site are hosted on various custom domains. This should make it easier to find these documentation sites on Google and other search engines.

We hope that this service will make it easy to maintain high quality and visible documentation for your packages! If something is unclear or not working, feel free to ask questions here or on slack.

## FAQ

### 1. What do I need to do to maintain documentation at docs.ropensci.org?

Absolutely nothing, everything is done automatically.

### 2. How can I customize my docs.ropensci.org site?

You can use all standard pkgdown options in your `pkgdown.yml` file, except for the template (we use the [rotemplate pkgdown theme](https://github.com/ropensci/rotemplate) to render). 

### 3. Can I help to improve the template?

Of course! You can send pull requests to [ropensci/rotemplate](https://github.com/ropensci/rotemplate).

### 4. Why are the images from the readme.md not showing in my pkgdown site?

pkgdown [only supports local images](https://github.com/ropensci/rotemplate/issues/19) in a few locations since only a few locations also work with CRAN's rendering of readme's.  The recommended path for static images is `man/figures`.

### 5. I already had a site. How to create a redirect from my old pkgdown site?
  
Simply push an `index.html` file to your `gh-pages` branch which redirects to the new site, see for example [here](https://github.com/ropensci/magick/blob/gh-pages/index.html).


**Background**: In order to trace Shakespearean Intertextuality, I am tokenizing Shakespeare texts (hypotexts) as 9grams and align each ngram (align_local) with other texts, e.g. by Terry Pratchett or Charles Dickens (hypertexts). I loop through all the ngrams and only return alignments that are above a certain alignment score. To speed the process up a little bit, I only use every third ngram, which should still be sufficient overlap to not miss any potential quotes ([WyrdSisters_Macbeth_minimal.R.zip](https://github.com/ropensci/textreuse/files/1624263/WyrdSisters_Macbeth_minimal.R.zip)). 

**Problem:** However, I am occasionally getting the following error message:

`Error in b_out[out_i] <- b_orig[row_i - 1] : replacement has length zero
`

Here is some more context via a screenshot from my console:

![screenshot_error](https://user-images.githubusercontent.com/2189545/34849537-f7fc9202-f722-11e7-9d74-895c195884e0.png)

I cannot really reproduce the error, but it seems to depend on how I set the count-variable, which has an effect on the ngram I start with. I assume the error has something to do with how the Smith-Waterman algorithm builds up its matrix of values, or – looking into the [TextReuse code](https://github.com/ropensci/textreuse/blob/master/R/align_local.R) – more concretely with the output vector construction ...
```
  # Place our first known values in the output vectors
  b_out[out_i] <- b_orig[row_i - 1]
  a_out[out_i] <- a_orig[col_i - 1]
out_i = out_i + 1L # Advance the out vector position
```
I assume a related problem is described in [StackOverflow](https://stackoverflow.com/questions/47657593/errors-with-the-align-local-function-in-r), but with no real solution.

Since the overall approach seems to work pretty well when it comes to discovering verbatim or near verbatim Shakespeare text reuse in other hypertexts, I would be really happy to understand what is happening here, and how I can possibly fix it. 



For very large corpora, it would be good to have a database backend so things don't have to stay in memory.

Create an index of n-grams and documents containing those n-grams, with ways to remove rare (unique?) or common n-grams, and to extract pairs of matches from that index.


Implement earth mover distances as described in this paper. There is a gensim implementation. If possible, do this using the word2vec package's GloVe implementation from @dselivanov.

http://jmlr.org/proceedings/papers/v37/kusnerb15.html
