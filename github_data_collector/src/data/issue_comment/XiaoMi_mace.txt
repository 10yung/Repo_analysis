Before you open an issue, please make sure you have tried the following steps:

1. Make sure your **environment** is the same with (https://mace.readthedocs.io/en/latest/installation/env_requirement.html).
2. Have you ever read the document for your usage?
3. Check if your issue appears in [HOW-TO-DEBUG](https://mace.readthedocs.io/en/latest/development/how_to_debug.html) or [FAQ](https://mace.readthedocs.io/en/latest/faq.html).
4. The form below must be filled.

------------------------

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **NDK version(e.g., 15c)**: 18b
- **GCC version(if compiling for host, e.g., 5.4.0)**: 5.4.0
- **MACE version (Use the command: git describe --long --tags)**: 0.11.0-rc0
- **Python version(2.7)**: 2.7
- **Bazel version (e.g., 0.13.0)**: 0.16.0

### Model deploy file (*.yml)
```yaml
# The name of library
library_name: model
target_abis: [arm64-v8a]
model_graph_format: file
model_data_format: file
models:
  sp: # model tag, which will be used in model loading and must be specific.
    platform: caffe
    # path to your tensorflow model's pb file. Support local path, http:// and https://
    model_file_path: /models/sp/model-nofc.prototxt
    weight_file_path: /models/sp/model-nofc.caffemodel
    # sha256_checksum of your model's pb file.
    # use this command to get the sha256_checksum --> sha256sum path/to/your/pb/file
    model_sha256_checksum: 54479f5ec821884f5bfcc03cb1f4558275541c6e80d9f33f65cc58562fffe91b 
    weight_sha256_checksum: e9599be0e9d5a5f08b85f9b98d2a76b55463ecb6820efc3bcdbc3ea0050f62a0 
    subgraphs:
      - input_tensors:
          - data
        input_shapes:
          - 1,3,112,112
        input_data_formats:
          - NCHW
        output_tensors:
          - pre_fc1
        output_shapes:
          - 1,512,1,1
        output_data_formats:
          - NCHW
    obfuscate: 0
    quantize: 1
    quantize_range_file: /mace/overall_range
    runtime: cpu # cpu, gpu or cpu+gpu or dsp
    winograd: 0
```

### Describe the problem
Segmentation fault happens when running quantized depthwise conv2d.

### To Reproduce
Steps to reproduce the problem:
```bash
1. cd /path/to/mace
2. python tools/converter.py convert --config_file=/path/to/your/model_deployment_file
2. python tools/converter.py run --config_file=/path/to/your/model_deployment_file
```

### Error information / logs
Please include the **full** log and/or traceback here.
https://gist.github.com/gasgallo/619eb23800d7caf46e6e97ed23bfc38a

### Additional context
Models runs fine w/o quantization.
Before you open an issue, please make sure you have tried the following steps:

1. Make sure your **environment** is the same with (https://mace.readthedocs.io/en/latest/installation/env_requirement.html).
2. Have you ever read the document for your usage?
3. Check if your issue appears in [HOW-TO-DEBUG](https://mace.readthedocs.io/en/latest/development/how_to_debug.html) or [FAQ](https://mace.readthedocs.io/en/latest/faq.html).
4. The form below must be filled.

------------------------

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **NDK version(e.g., 15c)**:
- **GCC version(if compiling for host, e.g., 5.4.0)**:
- **MACE version (Use the command: git describe --long --tags)**:
- **Python version(2.7)**: 
- **Bazel version (e.g., 0.13.0)**:

### Model deploy file (*.yml)
```yaml
......
```

### Describe the problem
A clear and concise description of what the bug is.

### To Reproduce
Steps to reproduce the problem:
```bash
1. cd /path/to/mace
2. python tools/converter.py convert --config_file=/path/to/your/model_deployment_file
```

### Error information / logs
Please include the **full** log and/or traceback here.
```bash
LOGs
```

### Additional context
Add any other context about the problem here, e.g., what you have modified about the code.

Before you open an issue, please make sure you have tried the following steps:

1. Make sure your **environment** is the same with (https://mace.readthedocs.io/en/latest/installation/env_requirement.html).
2. Have you ever read the document for your usage?
3. Check if your issue appears in [HOW-TO-DEBUG](https://mace.readthedocs.io/en/latest/development/how_to_debug.html) or [FAQ](https://mace.readthedocs.io/en/latest/faq.html).
4. The form below must be filled.

------------------------

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **NDK version(e.g., 15c)**:
- **GCC version(if compiling for host, e.g., 5.4.0)**:
- **MACE version (Use the command: git describe --long --tags)**:
- **Python version(2.7)**: 
- **Bazel version (e.g., 0.13.0)**:

### Model deploy file (*.yml)
```yaml
......
```

### Describe the problem
A clear and concise description of what the bug is.

### To Reproduce
Steps to reproduce the problem:
```bash
1. cd /path/to/mace
2. python tools/converter.py convert --config_file=/path/to/your/model_deployment_file
```

### Error information / logs
Please include the **full** log and/or traceback here.
```bash
LOGs
```

### Additional context
Add any other context about the problem here, e.g., what you have modified about the code.

I apologies for posting my question as an issue.

I'm trying to run `Qualcomm/Hexagon_SDK/3.4.3/examples/hexagon_nn/tutorials` on Xiaomi Mi9 phone SM8150 (SDM855).

Example: `001-nop.c` (using libs/hexagon_nn/2.6)

I added `#pragma weak remote_session_control` and `hexnn_controller_request_unsigned_pd()` to `001-nop.c`. 

Tutorial execution in examples/hexagon_nn/tutorials shows:
```
python tutorials_walkthrough.py -T sm8150 -N
...
---- Run Examples on cDSP ----
---- Runing 001-nop		----
adb wait-for-device push /root/3.4.3/libs/hexagon_nn/2.6/hexagon_Debug_dynamic_toolv83_v66/ship/libhexagon_nn_skel.so /data/local/tmp/vendor/lib/rfsa/adsp/
/root/3.4.3/libs/hexagon_nn/2.6/hexagon_Debug_dynamic_toolv83_v66/ship/libhexagon_nn_skel.so: 1 file pushed. 1.4 MB/s (5007704 bytes in 3.443s)
adb wait-for-device push /root/3.4.3/examples/hexagon_nn/tutorials/android_Debug_aarch64/ship/001-nop /data/local/tmp/vendor/bin
/root/3.4.3/examples/hexagon_nn/tutorials/android_Debug_aarch64/ship/001-nop: 1 file pushed. 0.6 MB/s (118624 bytes in 0.176s)
adb wait-for-device shell chmod 777 /data/local/tmp/vendor/bin/001-nop
adb wait-for-device shell ADSP_LIBRARY_PATH="/data/local/tmp/vendor/lib/rfsa/adsp" /data/local/tmp/vendor/bin/001-nop
***************** remote_session_control is TRUE ****************
***************** remote_session_control returned 0 ****************
fastrpc_setup Done
Trying to hexagon_nn_config
hexagon_nn_config Done
hexagon_nn_init Done
hexagon_nn_append_node Done
hexagon_nn_prepare Done
Trying hexagon_nn_execute
Whoops... run failed: -1
Test Failed, err=-1
```

logcat shows that remote_handle_open for libhexagon_nn_skel.so was successfull, but remote_handle_invoke failed
```
12-21 23:50:18.501  3565  3565 V /data/local/tmp/vendor/bin/001-nop: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1724: Successfully opened fastrpc_shell_unsigned_3
12-21 23:50:18.521  3565  3565 V /data/local/tmp/vendor/bin/001-nop: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1870: Successfully created user PD on domain 3 (attrs 0x8)
12-21 23:50:18.545  3565  3565 V /data/local/tmp/vendor/bin/001-nop: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1006: remote_handle_open: Successfully opened handle 0xed2620 for hexagon_nn on domain 3
12-21 23:50:18.557  3565  3565 D /data/local/tmp/vendor/bin/001-nop: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:925: Error 0xffffffff: remote_handle_invoke failed for handle 0xed2620, method 12 on domain 3 (sc 0xc020200)
12-21 23:50:18.557  3565  3566 D /data/local/tmp/vendor/bin/001-nop: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:925: Error 0xffffffff: remote_handle_invoke failed for handle 0x3, method 4 on domain 3 (sc 0x4020200)
12-21 23:50:18.558  3565  3566 E /data/local/tmp/vendor/bin/001-nop: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/listener_android.c:244:listener protocol failure ffffffff
12-21 23:50:18.558  3565  3565 D /data/local/tmp/vendor/bin/001-nop: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:925: Error 0x27: remote_handle_invoke failed for handle 0xed2620, method 13 on domain 3 (sc 0xd010000)
12-21 23:50:18.558  3565  3566 D /data/local/tmp/vendor/bin/001-nop: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:925: Error 0x27: remote_handle_invoke failed for handle 0x3, method 4 on domain 3 (sc 0x4020200)
12-21 23:50:18.558  3565  3566 E /data/local/tmp/vendor/bin/001-nop: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/listener_android.c:251::error: 39: 0 == (nErr = __QAIC_HEADER(adsp_listener_next2)( ctx, nErr, 0, 0, &ctx, &handle, &sc, inBufs, inBufsLen, &inBufsLenReq))
12-21 23:50:18.558  3565  3566 E /data/local/tmp/vendor/bin/001-nop: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/listener_android.c:333:Error 0x27: listener2 thread exited
```

Full example code `001-nop.c`:
```
#include "remote.h"

#pragma weak remote_session_control

// hexagon_nn.h includes most of the things you'll need to create and run graphs.
// Its most important includes are nn_graph.h, which includes nn_graph_if.h.
// Together, these provide the data-types for input/output tensors,
//   and the API you'll use for initializing, building, preparing and running
//   your graphs.
// NOTE: hexagon_nn.h redefines malloc(), alloc(), etc. so
//   they become a compile-error "OOPS MALLOC".  This is because you should
//   use rpcmem_alloc() instead.
#include <hexagon_nn.h>

// hexagon_nn_ops.h defines the various graph operations (e.g. "MatMul", "NOP"
//   and "Relu") which you can do.  Internally, it just expands
//   interface/ops.def into a usable format.  ops.def contains the list of
//   all implemented ops.
#include "hexagon_nn_ops.h"

// For printf, etc.
#include <stdio.h>

// If you're already familiar with SDK programming for the DSP,
//   you've probably used fastRPC.  There's already lots of examples
//   documenting its use, and the purpose of this tutorial is to
//   expose the hexagon_nn_* API, so we'll ignore the fastRPC details.
// For these tutorials, we create a couple functions
//   fastrpc_setup() and fastrpc_teardown(), and some required includes.
// FastRPC allows our code running on the ARM to call functions located
//   on the DSP, quite seamlessly.
// To enable this ARM/DSP communication, we need to open a channel.
//   We'll also need to be careful later how we call functions that cross
//   the ARM/DSP partition, e.g. sending pointers, to ensure the ARM and
//   DSP see the same data.
#include "sdk_fastrpc.h"




// The structure of our NOP network looks like this.
//   It's really just a NOP floating in space, with no inputs or outputs.
//
//
//                   ==============
//    ?????????      ||    NOP   ||      ?????????
//   ??nothing??     || id=0x1000||     ??nothing??
//    ?????????      ==============      ?????????
//

int hexnn_controller_request_unsigned_pd() {
  int ret = -1;
  if (remote_session_control) {
    printf("***************** remote_session_control is TRUE ****************\n");
    struct remote_rpc_control_unsigned_module data;
    data.enable = 1;
    data.domain = CDSP_DOMAIN_ID;
    ret = remote_session_control(DSPRPC_CONTROL_UNSIGNED_MODULE, (void *) &data, sizeof(data));
    printf("***************** remote_session_control returned %d ****************\n", ret);
  } else {
    printf("***************** remote_session_control is FALSE ****************\n");
  }
  return ret;
}


int main(int argc, char **argv) {
        int err = 0;
        hexnn_controller_request_unsigned_pd();
        // Start the ARM/DSP communications channel so we can call
        //   library functions that execute on the dsp.
        if (fastrpc_setup() != 0) return 1;
        printf("fastrpc_setup Done\n");
        // The nnlib API consists of functions that begin "hexagon_nn_*"
        // This prefix indicates that the function will actually run on the DSP.
        // To run a neural network we'll use this basic API:
        //   1) hexagon_nn_config            - Start nnlib, preparing globals
        //   2) hexagon_nn_init              - Initialize a new graph
        //   3) hexagon_nn_set_debug_level   - Enable debug
        //   4) hexagon_nn_append_node       - Add nodes to the graph
        //   5) hexagon_nn_append_const_node - Add constants (pure data, not ops)
        //                                     (we won't need any for now)
        //   6) hexagon_nn_prepare           - Allocate memory, strategize,
        //                                     and optimize the graph for speed
        //   7) hexagon_nn_execute           - Run an inference
        //   8) hexagon_nn_teardown          - Destroy the graph, free resources

        // Ensures that nnlib is ready to start working.
        printf("Trying to hexagon_nn_config\n");
        hexagon_nn_config();
        printf("hexagon_nn_config Done\n");

        // Initialize a fresh, empty graph.  Return a graph-handle by reference.
        hexagon_nn_nn_id graph_id;
        if (hexagon_nn_init(&graph_id)) {
                printf("Whoops... Cannot init\n");
				return 2;
        }
        printf("hexagon_nn_init Done\n");

        // Set power level (to max/turbo)
        if ((err = hexagon_nn_set_powersave_level(0)) != 0) {
                printf("Whoops... Cannot set power level: %d\n", err);
                goto TEARDOWN;
        }

        // Select our debug level.  0=none, >4=max
        // When creating new graphs, it's nice to have max debug
        //   even if you don't think you need it.
        //hexagon_nn_set_debug_level(graph_id, 100);

        // Append a node to the graph.
        // We need to provide a unique-id so other nodes can connect.
        // The operation can be any of the ops found in interface/ops.def,
        //   prefixed with "OP_" (e.g. OP_MatMul_f, OP_Relu_f, OP_MaxPool_f)
        // Our NOP node doesn't need any padding, because it won't do anything.
        // Our input/output lists will be NULL in this example,
        //   but for real graphs we'll need to connect nodes using these lists.
        hexagon_nn_append_node(
                graph_id,           // Graph handle we're appending into
                0x1000,             // Node identifier (any unique uint32)
                OP_Nop,             // Operation of this node (e.g. Concat, Relu)
                NN_PAD_NA,          // Padding type for this node
                NULL,               // The list of inputs to this node
                0,                  //   How many elements in input list?
                NULL,               // The list of outputs from this node
                0                   //   How many elements in output list?
                );
        printf("hexagon_nn_append_node Done\n");
        // Prepare the graph for execution by optimizing it, allocating storage,
        //   connecting all the input/output pointers between nodes, and
        //   doing some basic checks, like number of input/output tensors and
        //   sizing for each node.
        if (hexagon_nn_prepare(graph_id)) {
                printf("Whoops... Cannot prepare\n");
        }
        printf("hexagon_nn_prepare Done\n");


        // Execute an inference on our input data.
        // Real graphs require input and output buffers, but we'll
        //   just use zero-size NULL pointers for this NOP example.
        uint32_t out_batches, out_height, out_width, out_depth, out_data_size;
        printf("Trying hexagon_nn_execute\n");
        if ((err = hexagon_nn_execute(
                     graph_id,
                     0, 0, 0, 0,             // Our input has 0-dimension
                     NULL,                   // Pointer to input data
                     0,                      // How many total bytes of input?
                     (unsigned int *) &out_batches,
                     (unsigned int *) &out_height,
                     (unsigned int *) &out_width,
                     (unsigned int *) &out_depth,
                     (uint8_t *)NULL,        // Pointer to output buffer
                     0,                      // Max size of output buffer
                     (unsigned int*) &out_data_size)         // Actual size used for output
                    ) != 0) {

                printf("Whoops... run failed: %d\n",err);
        }
        

TEARDOWN:
    // Free the memory, especially if we want to build subsequent graphs
    hexagon_nn_teardown(graph_id);

    // Stop fastRPC
    fastrpc_teardown();

    if (!err) printf("Test Passed!\n");
    else printf ("Test Failed, err=%d\n", err);

    return err;
}
```

Before you open an issue, please make sure you have tried the following steps:

1. Make sure your **environment** is the same with (https://mace.readthedocs.io/en/latest/installation/env_requirement.html).
2. Have you ever read the document for your usage?
3. Check if your issue appears in [HOW-TO-DEBUG](https://mace.readthedocs.io/en/latest/development/how_to_debug.html) or [FAQ](https://mace.readthedocs.io/en/latest/faq.html).
4. The form below must be filled.

------------------------

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **NDK version(e.g., 15c)**:
- **GCC version(if compiling for host, e.g., 5.4.0)**:
- **MACE version (Use the command: git describe --long --tags)**:
- **Python version(2.7)**: 
- **Bazel version (e.g., 0.13.0)**:

### Model deploy file (*.yml)
```yaml
library_name: shufflenetv2
target_abis: [arm64-v8a]
model_graph_format: code 
model_data_format: code
models:
  shufflenetv2:
    platform: "onnx"
    # model_file_path : ./models/lml/shufflenetv2_ssd_change_v2_opt.onnx
    model_file_path : ./models/lml/shufflenetv2_ssd_change_v3__opt.onnx
    # model_sha256_checksum : b7fcb6949ff3145d9a093b40cc9e383f3e5cc9ac1831e28fa2a98e5cebf10200
    model_sha256_checksum : 396f7432fc329ac2c28eae4d2d8b926523607d750f8c9dd7e376fa94d2696570 
    data_types: "fp32_fp32"
    subgraphs:
      - input_tensors:
          - '0'
        input_shapes:
          - 1,320,320,3
        output_tensors:
          - '711'
          - '712'
          - '713'
          - '714'
          - '715'
          - '716'
          - '717'
          - '718'
          - '719'
          - '720'
          - '721'
          - '722'
        output_shapes:
          - 1,40,40,8
          - 1,40,40,16
          - 1,10,10,12
          - 1,10,10,24
          - 1,5,5,12
          - 1,5,5,24
          - 1,3,3,12
          - 1,3,3,24
          - 1,2,2,8
          - 1,2,2,16
          - 1,1,1,8
          - 1,1,1,16
        #onnx backend framwork for validation. Suppport pytorch/caffe/tensorflow. Default is tensorflow.
        backend: pytorch
    #cpu, gpu or cpu+gpu
    runtime: cpu+gpu
    obfuscate: 0
    winograd: 0
```

### Describe the problem
A clear and concise description of what the bug is.
我将shufflenetv2转onnx后，模型中的slice算子中的end值为9223372036854775807。虽然在mace中转模型没有出错，但是部署在安卓上就不能build成功，

### To Reproduce
Steps to reproduce the problem:
```bash
1. cd /path/to/mace
2. python tools/converter.py convert --config_file=/path/to/your/model_deployment_file
```

### Error information / logs
Please include the **full** log and/or traceback here.
```bash
LOGs
```

### Additional context
Add any other context about the problem here, e.g., what you have modified about the code.

Before you open an issue, please make sure you have tried the following steps:

1. Make sure your **environment** is the same with (https://mace.readthedocs.io/en/latest/installation/env_requirement.html).
2. Have you ever read the document for your usage?
3. Check if your issue appears in [HOW-TO-DEBUG](https://mace.readthedocs.io/en/latest/development/how_to_debug.html) or [FAQ](https://mace.readthedocs.io/en/latest/faq.html).
4. The form below must be filled.

------------------------

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **NDK version(e.g., 15c)**:
- **GCC version(if compiling for host, e.g., 5.4.0)**:
- **MACE version (Use the command: git describe --long --tags)**:
- **Python version(2.7)**: 
- **Bazel version (e.g., 0.13.0)**:

### Model deploy file (*.yml)
```yaml
......
```

### Describe the problem
A clear and concise description of what the bug is.

### To Reproduce
Steps to reproduce the problem:
```bash
1. cd /path/to/mace
2. python tools/converter.py convert --config_file=/path/to/your/model_deployment_file
```

### Error information / logs
Please include the **full** log and/or traceback here.
```bash
LOGs
```

### Additional context
Add any other context about the problem here, e.g., what you have modified about the code.
Hi
I have kaldi model and I want to deploy asr on iphone and android phone.
I successfully deploy the Mobilenet example on my phone but I still don't know how to do with the kaldi model.
Could you give me an example of speech recognition ?

Thanks a lot.






增加出错详细信息
Before you open an issue, please make sure you have tried the following steps:

1. Make sure your **environment** is the same with (https://mace.readthedocs.io/en/latest/installation/env_requirement.html).
2. Have you ever read the document for your usage?
3. Check if your issue appears in [HOW-TO-DEBUG](https://mace.readthedocs.io/en/latest/development/how_to_debug.html) or [FAQ](https://mace.readthedocs.io/en/latest/faq.html).
4. The form below must be filled.

------------------------

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **NDK version(e.g., 15c)**: 18b
- **GCC version(if compiling for host, e.g., 5.4.0)**: 5.4.0
- **MACE version (Use the command: git describe --long --tags)**: 0.11.0-rc0
- **Python version(2.7)**: 2.7
- **Bazel version (e.g., 0.13.0)**: 0.16.0

### Model deploy file (*.yml)
```yaml
# The name of library
library_name: FD
target_abis: [arm64-v8a]
target_socs: [rk3399]
model_graph_format: file
model_data_format: file
models:
  RF: # model tag, which will be used in model loading and must be specific.
    platform: caffe
    # path to your tensorflow model's pb file. Support local path, http:// and https://
    model_file_path: /models/model.prototxt
    weight_file_path: /models/model.caffemodel
    # sha256_checksum of your model's pb file.
    # use this command to get the sha256_checksum --> sha256sum path/to/your/pb/file
    model_sha256_checksum: 81c388e812da37e499da8272eff0d7d140e8ae50dcb8d7e124dbd4e98462ad24
    weight_sha256_checksum: 2250beffe1bc13f96f60b95fa37f48848bb31f567ae9eb763c86496a4ae29c9b
    subgraphs:
      - input_tensors:
          - data
        input_shapes:
          - 1,3,640,480
        input_data_formats:
          - NCHW
        output_tensors:
          - face_rpn_cls_prob_stride128
          - face_rpn_bbox_pred_stride128
          - face_rpn_landmark_pred_stride128
          - face_rpn_cls_prob_stride64
          - face_rpn_bbox_pred_stride64
          - face_rpn_landmark_pred_stride64
          - face_rpn_cls_prob_stride32
          - face_rpn_bbox_pred_stride32
          - face_rpn_landmark_pred_stride32
          - face_rpn_cls_prob_stride16
          - face_rpn_bbox_pred_stride16
          - face_rpn_landmark_pred_stride16
          - face_rpn_cls_prob_stride8
          - face_rpn_bbox_pred_stride8
          - face_rpn_landmark_pred_stride8
          - face_rpn_cls_prob_stride4
          - face_rpn_bbox_pred_stride4
          - face_rpn_landmark_pred_stride4
        output_shapes:
          - 1,2,5,5
          - 1,4,5,5
          - 1,10,5,5
          - 1,2,10,10
          - 1,4,10,10
          - 1,10,10,10
          - 1,2,20,20
          - 1,4,20,20
          - 1,10,20,20
          - 1,2,40,40
          - 1,4,40,40
          - 1,10,40,40
          - 1,2,80,80
          - 1,4,80,80
          - 1,10,80,80
          - 1,2,160,160
          - 1,4,160,160
          - 1,10,160,160
        output_data_formats:
          - NCHW
          - NCHW
          - NCHW
          - NCHW
          - NCHW
          - NCHW
          - NCHW
          - NCHW
          - NCHW
          - NCHW
          - NCHW
          - NCHW
          - NCHW
          - NCHW
          - NCHW
          - NCHW
          - NCHW
          - NCHW
    obfuscate: 0
    runtime: cpu+gpu # cpu, gpu or cpu+gpu or dsp
    winograd: 4
```

### Describe the problem
Inference time on MALI GPUs is very slow compared to other frameworks and a lot slower than the same model running on Adreno GPUs.

### To Reproduce
Steps to reproduce the problem:
```bash
1. cd /path/to/mace
2. python tools/converter.py convert --config_file=/path/to/your/model_deployment_file
2. python tools/converter.py benchmark --config_file=/path/to/your/model_deployment_file
```

### Error information / logs
Please include the **full** log and/or traceback here.
```bash
LOGs
```

### Additional context
For example, the model running with the above yml file takes:
- 530ms on a MALI T864
- 18ms on Adreno 640
- 233ms on MALI T864 (using Alibaba/MNN)
Before you open an issue, please make sure you have tried the following steps:

1. Make sure your **environment** is the same with (https://mace.readthedocs.io/en/latest/installation/env_requirement.html).
2. Have you ever read the document for your usage?
3. Check if your issue appears in [HOW-TO-DEBUG](https://mace.readthedocs.io/en/latest/development/how_to_debug.html) or [FAQ](https://mace.readthedocs.io/en/latest/faq.html).
4. The form below must be filled.

------------------------

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux ubuntu 5.0.0-31-generic
- **NDK version(e.g., 15c)**: r15c
- **GCC version(if compiling for host, e.g., 5.4.0)**: 
- **MACE version (Use the command: git describe --long –tags)**: v0.11.0-rc0-0-g2d650b67
- **Python version(2.7)**: 3.7.3
- **Bazel version (e.g., 0.13.0)**: 0.13.1

### Model deploy file (*.yml)
```yaml
https://github.com/XiaoMi/mace-models/blob/master/kaldi-models/nnet3/tedlium.yml
```

### Describe the problem
A clear and concise description of what the bug is.
When I converted a pre-trained kaldi-onnx model (https://github.com/XiaoMi/mace-models/blob/master/kaldi-models/nnet3/callhome.yml) to MACE format model, I got an error like: Exception: Unexpected fc input ndim. A dimension of ```shape_a = self._graph_shapes_dict[node.inputs[0]]``` for GEMM at ```convert_gemm@onnx_converter.py```  is 3 while the program assumes its dimension as 4.
Thank you.

### To Reproduce
Steps to reproduce the problem:
```bash
1. cd /path/to/mace
2. python tools/converter.py convert --config_file=https://github.com/XiaoMi/mace-models/blob/master/kaldi-models/nnet3/callhome.yml
```

### Error information / logs
Please include the **full** log and/or traceback here.
```bash
Transform model to one that can better run on device
onnx model IR version:  5
constains ops domain:  ai.kaldi.dnn version: 7
Traceback (most recent call last):
  File "//software/mace/bazel-bin/mace/python/tools/converter.runfiles/mace/mace/python/tools/converter.py", line 414, in <module>
    main(unused_args=[sys.argv[0]] + unparsed)
  File "//software/mace/bazel-bin/mace/python/tools/converter.runfiles/mace/mace/python/tools/converter.py", line 227, in main
    output_graph_def = converter.run()
  File "//software/mace/bazel-bin/mace/python/tools/converter.runfiles/mace/mace/python/tools/converter_tool/onnx_converter.py", line 451, in run
    self.convert_ops(graph_def)
  File "//software/mace/bazel-bin/mace/python/tools/converter.runfiles/mace/mace/python/tools/converter_tool/onnx_converter.py", line 528, in convert_ops
    self._op_converters[node.op_type](node)
  File "//software/mace/bazel-bin/mace/python/tools/converter.runfiles/mace/mace/python/tools/converter_tool/onnx_converter.py", line 1070, in convert_gemm
    "Unexpected fc input ndim.")
  File "//software/mace/bazel-bin/mace/python/tools/converter.runfiles/mace/mace/python/tools/convert_util.py", line 20, in mace_check
    raise Exception(msg)
Exception: Unexpected fc input ndim.
tools/converter.py:242: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  configs = yaml.load(f)
#[92m---------------------------------------------------
                Common Configuration               
---------------------------------------------------
|        key        |            value            |
===================================================
|       library_name|                     callhome|
---------------------------------------------------
|        target_abis| ['armeabi-v7a', 'arm64-v8a']|
---------------------------------------------------
|        target_socs|                           []|
---------------------------------------------------
| model_graph_format|                         file|
---------------------------------------------------
|  model_data_format|                         file|
---------------------------------------------------
#[0m
#[95m******************************************
          Convert callhome model          
******************************************
#[0m
#[32mLoading:#[0m

#[1A#[K#[32mLoading:#[0m 0 packages loaded

#[1A#[K#[32mINFO: #[0mAnalysed target //mace/python/tools:converter (0 packages loaded).
#[32mBuilding:#[0m no action

#[1A#[K#[32mINFO: #[0mFound 1 target...
#[32mBuilding:#[0m no action

#[1A#[K#[32m[0 / 1]#[0m [-----] BazelWorkspaceStatusAction stable-status.txt

#[1A#[KTarget //mace/python/tools:converter up-to-date:
#[32m[1 / 1]#[0m no action

#[1A#[K  bazel-bin/mace/python/tools/converter
#[32m[1 / 1]#[0m no action

#[1A#[K#[32mINFO: #[0mElapsed time: 0.204s, Critical Path: 0.04s
#[32m[1 / 1]#[0m no action

#[1A#[K#[32mINFO: #[0m0 processes.
#[32m[1 / 1]#[0m no action

#[1A#[K#[32mINFO:#[0m Build completed successfully, 1 total action
#[0m
Traceback (most recent call last):
  File "tools/converter.py", line 1346, in <module>
    flags.func(flags)
  File "tools/converter.py", line 853, in convert_func
    convert_model(configs, flags.cl_mem_type)
  File "tools/converter.py", line 782, in convert_model
    ",".join(model_config.get(YAMLKeyword.graph_optimize_options, [])))
  File "//software/mace/tools/sh_commands.py", line 549, in gen_model_code
    _fg=True)
  File "//.local/lib/python3.6/site-packages/sh.py", line 1413, in __call__
    raise exc
sh.ErrorReturnCode_1: 

  RAN: //anaconda3/envs/py36/bin/python bazel-bin/mace/python/tools/converter -u --platform=onnx --model_file=builds/downloads/1d5537265e4a166511a4ec69cbd12a84.pb --weight_file= --model_checksum=91f6f4088f35ffd6b2e1e8e852136b403a05693310e86c1201c78c1fc7397c1a --weight_checksum= --input_node=input --input_data_types=float32 --input_data_formats=NONE --output_node=output.log-softmax --output_data_types=float32 --output_data_formats=NONE --check_node= --runtime=cpu --template=mace/python/tools --model_tag=callhome --input_shape=1,20,23 --input_range= --output_shape=1,20,5164 --check_shape= --dsp_mode=0 --embed_model_data=False --winograd=0 --quantize=0 --quantize_range_file= --change_concat_ranges=0 --obfuscate=0 --output_dir=mace/codegen/models/callhome --model_graph_format=file --data_type=fp32_fp32 --graph_optimize_options= --cl_mem_type=image

  STDOUT:


  STDERR:

```

### Additional context
Add any other context about the problem here, e.g., what you have modified about the code.

### Additional context
It would help performance if we could directly pass OpenGL textures to MACE for GPU inference without first downloading those to CPU. Currently, we spend about 10% of our inference time in downloading opengl textures to CPU and pre-processing, which can be saved upon if MACE adds support for opencl/opengl interop.
