It would be nice to add a tor proxy to the results.
Weboproxy for example, gives the option.

TOR proxy:
https://tor.weboproxy.com/index.php?url=https://www.facebook.com
Canada Proxy:
https://ca.weboproxy.com/index.php?url=https://www.facebook.com

regards
Py2 is no longer supported officially. Do you want to discontinue the support of py2 compatibility in searx?
Please vote with :+1: to abandon and :-1: to keep it
Often we hear, that searx results of public instances are pure, it is common that this mainly due to two aspects: 

1. a bot blocker
2. up to date instance

To give users a chance to detect if a searx instance is maintained, we should more often increase the version (set a v-tag). By this https://searx.space/ can warn about a unmaintained (old version) of searx. For me this is also related to https://github.com/dalf/searx-stats2/issues/7
This is my first attempt to (partial) address #1795, #1785, #1759.

WIP: I will add more patches in the next days .. remarks are welcome!

A preview of the docs is here (don't bookmark) https://return42.github.io/searx/admin/filtron.html#filtron-go
In the result parsing, engines parse the HTML using lxml (most of the time). If an XPath request doesn't return at least one result, it may be fine or trigger an error later. In the later case, it is difficult to know exactly what is going on without looking at the downloaded HTML.

This issue suggests:
* to add new exception classes.
* to add two optional parameters to [eval_xpath](https://github.com/asciimoo/searx/blob/85b37233458c21b775bf98568c0a5c9260aa14fe/searx/utils.py#L465) function to check the result count.

It may help to know when an engine starts to be broken if the engine codes says which XPath request should not fail (?).

I'm not sure if it is useful and /or a privacy problem if searx makes statistics about broken XPath ?

### Class hierarchy

```
SearxException
	SearxParameterException
	SearxEngineException
		SearxEngineCaptchaException (instead of RuntimeWarning in google.py)
		SearxEngineXPathException
```

### eval_xpath
```python	
def eval_xpath(element, xpath_str, eq=None, gte=None):
    xpath = get_xpath(xpath_str)
    result = xpath(element)
    # new code: check result count now
    if eq is not None and len(result) != eq:
	raise SearxEngineXPathException(xpath, eq=eq)
    if gte is not None and len(result) < gte:
	raise SearxEngineXPathException(xpath, gte=gte)
    return result
```

### usage examples

#### extract_url
https://github.com/asciimoo/searx/blob/master/searx/engines/xpath.py#L53
```python
def extract_url(xpath_results, search_url):
    if xpath_results == []:
        raise Exception('Empty url resultset')	
```
--> Make the check before calling extract_url


#### bing engine
```python
	...
    for result in eval_xpath(dom, '//div[@class="sa_cc"]'):
        link = eval_xpath(result, './/h3/a', eq=1)[0]
	...
    for result in eval_xpath(dom, '//li[@class="b_algo"]'):
        link = eval_xpath(result, './/h2/a', eq=1)[0]
	...
```

#### google engine
```python
	title = extract_text(eval_xpath(result, title_xpath, eq=1)[0])
	url = parse_url(extract_url(eval_xpath(result, url_xpath, eq=1), google_url), google_hostname)
```
The huge try/catch to ignore all the parsing errors would be able to display the XPath in the logs.

Another way without try/catch and without modification to the eval_xpath function:
```python
	title_xpr = eval_xpath(result, title_xpath)
	url_xpr = eval_xpath(result, url_xpath)
	if len(title_xpr) > 0 and len(url_xpr) > 0:
		title = extract_text(title_xpr[0])
		url = parse_url(extract_url(url_xpr, google_url), google_hostname)
		...
```

The eq and gte parameters can't help much for the [result count](https://github.com/asciimoo/searx/blob/4cddb829f9f3718933f16346383fe989effc07e3/searx/engines/google.py#L228-L233).

Using eq:
```python
    try:
        results_num = int(eval_xpath(dom, '//div[@id="resultStats"]//text()', eq=1)[0]
                          .split()[1].replace(',', ''))
        results.append({'number_of_results': results_num})
    except:
        pass
```

Without eq, with more checking:
```python
	results_num_xpath = eval_xpath(dom, '//div[@id="resultStats"]//text()')
	if len(results_num_xpath) > 0:
		results_num_text = results_num_xpath[0]
		results_num_text_first = results_num_text.split()[1].replace(',', '') 
		try:
			results_num = int(results_num_text_first)
			results.append({'number_of_results': results_num})
		except ValueError:
			pass
```

We should stop C&P the license text into every file.  We have one https://github.com/asciimoo/searx/blob/master/LICENSE in the root folder and the source files should be tagged with a [SPDX License Identifier](https://spdx.org/using-spdx-license-identifier):

     SPDX-License-Identifier: AGPL-3.0-or-later

SPDX reduces redundant work by providing a common format for companies and communities to share important data about software licenses, copyrights, and security references, thereby streamlining and improving compliance.
Similar to #419

Installation: current master commit
How to reproduce? Search for "kek" on https://search.snopyta.org/ and click on "Images"

```
ERROR:flask.app:Exception on / [POST]
Traceback (most recent call last):
  File "/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/app.py", line 2292, in wsgi_app
    response = self.full_dispatch_request()
  File "/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/app.py", line 1815, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/app.py", line 1718, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File "/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/app.py", line 1813, in full_dispatch_request
    rv = self.dispatch_request()
  File "/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/app.py", line 1799, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File "/usr/local/searx/searx/webapp.py", line 544, in index
    result['title'] = highlight_content(escape(result['title'] or u''), search_query.query)
  File "/usr/local/searx/searx/utils.py", line 79, in highlight_content
    if content.lower().find(query.lower()) > -1:
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 1: ordinal not in range(128)
```
Here is a way to install searx in 5 minutes and wanted to post this somewhere. This is not a bug, this is a **guide**.

### Requirements
1. You have docker running already
2. You have several websites already being served by Apache (we will be using a subdomain)
3. Apache2 version >= 2.4.17 < 2.4.36
4. You are using certbot for ssl certs
5. Need to use TLS1.2 (TLS1.3 not implemented until Apache2 >= 2.4.36)

#### Updated for @dalf comment below
1. Add ability not to log anything (recommended reverse proxy setup)
2. I used docker repo `wonderfall/searx` as that was stated in the install [here](https://asciimoo.github.io/searx/admin/installation.html#docker). but is out of date so updated to `searx/searx`
3. Removed old CSP sha-256 as it was not needed for updated docker image
4. Added more "default" requirements
5. Removed `<FilesMatch "\.(cgi|shtml|phtml|php)$">` and `<Directory /usr/lib/cgi-bin>` as they are not needed for using docker.

### Guide
Grab the docker code
1. `sudo docker pull searx/searx`
Start searx docker image listening on port 9999 (note, you can change this to whatever you want!)
2. `sudo docker run -i -t -d --restart=always -p 127.0.0.1:9999:8888 searx/searx`
3. Open up a browser and navigate to http://localhost:9999/
If you see the searx logo, you may continue. If not, check to see if anything is running on port 9999 with `sudo netstat -peanut | grep :9999`
4. Create a new apache config file called searx.conf with the following content:
Note: if you are not using port 9999, change it in the apache conf file!
```apache
    DEFINE searx_url 127.0.0.1
    DEFINE searx_port 9999
    DEFINE public_url searx.sub.domain
    DEFINE email webadmin@searx.sub.domain
    ServerTokens Prod
    SSLStaplingCache "shmcb:${APACHE_LOG_DIR}/stapling-cache(150000)"
    SSLSessionCache "shmcb:${APACHE_LOG_DIR}/ssl_scache(512000)"
    SSLSessionCacheTimeout 300
### If you have Google's Mod PageSpeed, disable it
    ModPagespeed Off
<VirtualHost *:80>
    ServerName ${public_url}
    DocumentRoot /var/www/offline
    ServerAdmin ${email}
    ErrorLog ${APACHE_LOG_DIR}/web.error.log
    CustomLog ${APACHE_LOG_DIR}/web.access.log combined
 ###If you don't want logs
    #ErrorLog /dev/null
    #CustomLog /dev/null combined
    RewriteEngine On
    RewriteCond %{SERVER_NAME}=${public_url}
    RewriteCond %{HTTPS} off
    RewriteRule ^ https://%{SERVER_NAME}%{REQUEST_URI} [END,NE,R=permanent]
</VirtualHost>
<VirtualHost *:443>
    ServerName ${public_url}
    DocumentRoot /var/www/offline
    ServerAdmin ${email}
    ErrorLog ${APACHE_LOG_DIR}/web.error.log
    CustomLog ${APACHE_LOG_DIR}/web.access.log combined
 ###If you don't want logs
    #ErrorLog /dev/null
    #CustomLog /dev/null combined
    SSLEngine On
    SSLCertificateFile /etc/letsencrypt/live/${public_url}/fullchain.pem
    SSLCertificateKeyFile /etc/letsencrypt/live/${public_url}/privkey.pem
    #Include /etc/letsencrypt/options-ssl-apache.conf
### Forbid the http1.0 protocol ###
    Protocols h2 http/1.1
    Timeout 360
    ProxyRequests Off
    ProxyPreserveHost On
    ProxyTimeout 600
    ProxyReceiveBufferSize 4096
    SSLProxyEngine On
    RequestHeader set Front-End-Https "On"
    ServerSignature Off
    SSLCompression Off
    SSLUseStapling On
    SSLStaplingResponderTimeout 5
    SSLStaplingReturnResponderErrors Off
    SSLSessionTickets Off
    RequestHeader set X-Forwarded-Proto 'https' env=HTTPS
    Header always set Strict-Transport-Security "max-age=15552000; preload"
    Header always set X-Content-Type-Options nosniff
    Header always set X-Robots-Tag none
    Header always set X-XSS-Protection "1; mode=block"
    Header always set X-Frame-Options "SAMEORIGIN"
    Header always set Referrer-Policy "strict-origin-when-cross-origin"
    Header always set Content-Security-Policy "default-src 'self' https:; font-src 'self' data: ${searx_url} ${public_url}; media-src 'self' blob: data: https: ${searx_url} ${public_url}; script-src 'self' ${searx_url} ${public_url}; img-src 'self' data: https: blob: ${searx_url} ${public_url}"
    Header always set Feature-Policy "geolocation 'self'; midi 'self'; sync-xhr 'self'; microphone 'self'; camera 'self'; magnetometer 'self'; gyroscope 'self'; speaker 'self'; fullscreen 'self'; payment 'self'"
    SSLHonorCipherOrder Off
### Use next two for very secure connections ###
    SSLCipherSuite EECDH+AESGCM:EDH+AESGCM:AES256+EECDH:AES256+EDH:!aNULL:!eNULL:!EXPORT:!DES:!MD5:!PSK:!RC4
    SSLProtocol All -SSLv2 -SSLv3 -TLSv1 -TLSv1.1
### Use next two for secure connections and supports more endpoints ###
    #SSLCipherSuite EECDH+AESGCM:EDH+AESGCM:AES256+EECDH:AES256+EDH:ECDHE-RSA-AES128-GCM-SHA384:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA128:DHE-RSA-AES128-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES128-GCM-SHA128:ECDHE-RSA-AES128-SHA384:ECDHE-RSA-AES128-SHA128:ECDHE-RSA-AES128-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA128:DHE-RSA-AES128-SHA128:DHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:ECDHE-RSA-DES-CBC3-SHA:EDH-RSA-DES-CBC3-SHA:AES128-GCM-SHA384:AES128-GCM-SHA128:AES128-SHA128:AES128-SHA128:AES128-SHA:AES128-SHA:DES-CBC3-SHA:HIGH:!aNULL:!eNULL:!EXPORT:!DES:!MD5:!PSK:!RC4
    #SSLProtocol All -SSLv2 -SSLv3 -TLSv1 -TLSv1.1
### Actually proxy the traffic and really the only important part ###
    ProxyPass / http://${searx_url}:${searx_port}/
    ProxyPassReverse / http://${searx_url}:${searx_port}/
### Additional suggestions: https://github.com/asciimoo/searx/wiki/How-to-create-and-configure-SSL ###
    BrowserMatch "MSIE [2-6]" nokeepalive ssl-unclean-shutdown downgrade-1.0 force-response-1.0
    BrowserMatch "MSIE [17-9]" ssl-unclean-shutdown
</VirtualHost>
```
5. Enable the new site: `sudo a2ensite searx.conf`
6. Reload and restart apache service: `sudo service apache2 reload && sudo service apache2 restart`

## Congrats!
Wasn't that easy?

### Comments? Questions? Complaints? Grievances?
Go ahead and leave a comment below.
**Type:** Feature Request

This enhancement proposal is a continuation of @dalf's [feedback](https://github.com/asciimoo/searx/pull/1781#pullrequestreview-338364659) on PR https://github.com/asciimoo/searx/pull/1781 to track proposal.

### Description
After accepting PR https://github.com/asciimoo/searx/pull/1781, along with Info box, Suggestion, and Link box moved to top of page in low width devices, so replacing Link box with dropdown button has following advantages

- Cleaner UI
- No issue with infinite scrolling

Currently, the [documentation](https://asciimoo.github.io/searx/admin/installation.html) (even if it requires a huge update), describes how to setup:
```[reverse proxy]-->[uwsgi]-->[searx]```

I think:
- this should be removed to avoid unsafe installations (because it is vulnerable to bots), or at least put aside with a warning.
- Filtron should be a first citizen, not an [optional layer](https://asciimoo.github.io/searx/admin/filtron.html). The documentation should explain how to setup step by step
```[reverse proxy]-->[filtron]-->[uwsgi]-->[searx]```

[EDIT] Here I have pick [filtron](https://github.com/asciimoo/filtron) as bot shield, but there is also [antibot-proxy](https://github.com/unixfox/antibot-proxy).
Also the comments of https://github.com/asciimoo/searx/pull/1771