Using the .net CLI I'm trying to run MSpec tests, this works perfectly outside of Docker:

dotnet test MyMspecTestProject.csproj

No problem there, but if I try the same command inside a Docker container I get:

Test run in progress. No test is available in bin/Debug/netcoreapp2.2/MyMspecTestProject.dll. Make sure that installed test discoverers & executors, platform & framework version settings are appropriate and try again.

I figured this was related to it not having an appropriate package, but I've restored it inside the container with all the following:

Microsoft.NET.Test.Sdk" Version="15.0.0" 
Machine.Specifications.Runner.Console" Version="1.0.0" 
Machine.Specifications.Runner.VisualStudio" Version="2.9.0" 
Machine.Specifications" Version="1.0.0" 

Anything else I might be missing?
Cloning this repo is around 160mb, which is not overly burdensome but is not ideal either. 

I propose a destructive cleanup with something like [BFG](https://rtyley.github.io/bfg-repo-cleaner/) to reduce old copies of DLLs that have long since been deleted from the repo but still exist in the `.git` folder. 

**This will require any forks to either re-clone or re-base (somehow) so that everything is in sync again.**
In projects I've worked with we've noticed that we don't deviate from one flavor of `Machine.Fakes` (in our case Moq). For the rare cases where we've needed additional control over the mocking, we just reference `Moq` directly and use it as normal. 

In addition to the above, the overhead in supporting multiple frameworks is a bit overbearing, and also prevents, to varying degrees, those frameworks from progressing. Also RhinoMocks is basically abandoned, and will never support `netstandard`.

On a picky note, it's always irked me that the package and namespace does not line up with `Machine.Specifications`. One benefit of aligning Fakes to Mspec would be that we have verified packages on Nuget for `Machine.Specifications.*`.

To solve the above, I propose a new package, `Machine.Specifications.Fakes`. This would be a single package that depends on `Castle.Core` and would be a home-grown fake library. We would still allow extensions points like `IFakeEngine` so that consumers could implement their own flavor of Fakes for Moq or FakeItEasy etc. I'm happy to archive the code we have for these implementations or move them to an 'example' project or something, but to cease maintaining them.

Users that, for whatever reason, insist on using the current `Machine.Fakes` can continue to do so under version `1.0` of MSpec.
Hi,
Is there a road map with some timescales for MSpec?
We want to improve the DevOps (VSTS / VS) adapter (e.g. support tags, make the test into a hierarchy, record total times etc )
We don't want to write this if the code base is "moving" soon.
Do you have any idea one when 2.0 will be due. we would love to help beta test it.
Thanks

@robertcoltheart / @ivanz 
xUnit is proposing targeting `netstandard2.0` as a baseline for the 3.0 release. I propose that we do the same for our 2.0 release.

The reason for this is many of the versions we currently support are either already EOL (end of life), or are tied to versions of Windows that will end support soon. This includes `net452`, an OS component of Windows 7, which is hitting end of support in Jan 2020.

This means that we would target:

- `netcoreapp2.0`
- `net472` (see footnote 1 in first link below)
- `uap10.0.16299`

*Sources:*
- https://github.com/dotnet/standard/blob/master/docs/versions.md#net-standard-versions
- https://support.microsoft.com/en-sg/lifecycle/search/548
- https://www.microsoft.com/en-us/windowsforbusiness/end-of-windows-7-support

Please add a runner for Azure DevOps that we can use.
(we are currently using .net 4.5.2)
It would be nice if it also reported the progress in real time
Hi, 

You can see my problem in the following screenshot. I realize the value that appear in the Resharper Runner for the class Subject doesn't match. I've clean the project, deleted all the cache and no luck. The Console runner report the test correctly, but not Resharper Runner. I really don't know if this problem is in the Resharper Plugin or something that is created outside of it because maybe an implementation detail on Resharper.

But it will be really helpful if this can be addressed 

![image](https://user-images.githubusercontent.com/137626/55809237-8d6eb780-5ab3-11e9-9be6-e651c7c8f2a6.png)

Hi,

First of all, thanks for a great testing framework. We use it extensively at work and are generally very pleased with it.

However, we've run recently into a weird situation where equality tests seem to be failing when they shouldn't. It's easier to explain with an example.

Consider a `Person` class, which implements both `IEquatable<T>` and `IComparable<T>`:

- 2 persons are equal if they have the same ID
- the order between 2 persons is defined by their name (alphabetical). So a sorted list of 4 persons might look like `[ Amy, Ben, Ben, Beth]` (with no preference for which `Ben` comes first).

```c#
    class Person : IEquatable<Person>, IComparable<Person>
    {
        private readonly int _id;
        private readonly string _name;

        public Person(int id, string name)
        {
            _id = id;
            _name = name;
        }

        public bool Equals(Person other)
        {
            return (other != null) && other._id == this._id;
        }

        public override int GetHashCode()
        {
            return _id;
        }

        public int CompareTo(Person other)
        {
            return this._name.CompareTo(other._name);
        }
    }
```

And the following test:

```c#
    class Test
    {
        static Person person1, person2;

        Establish context = () =>
        {
            person1 = new Person(1, "John Doe");
            person2 = new Person(2, "John Doe");  // Same name, different person
        };

        It should_have_the_same_rank = () => person1.CompareTo(person2).ShouldEqual(0);         // assertion passes
        It should_not_be_the_same_person = () => person1.ShouldEqual(person2);                  // assertion passes (but should fail?)
        It should_be_2_different_persons_v1 = () => person1.Equals(person2).ShouldBeFalse();    // assertion passes 
        It should_be_2_different_persons_v2 = () => person1.ShouldNotEqual(person2);            // assertion fails (but should pass?)
    }
```

For some reason, 2 of these assertions fail. We had a quick look at why that might be and it seems that MSpec uses a number of Comparer strategies, as defined [here](https://github.com/machine/machine.specifications.should/blob/master/src/Machine.Specifications.Should/ComparerStrategies/ComparerFactory.cs#L9).

I could be wrong, but do these mean that MSpec gives `IComparable<T>` preference over `IEquatable<T>`? If so, that seems to me like a misunderstanding. I always thought IComparable was about ordering things and IEquatable was about defining what makes 2 things equal. So I think I would expect:

- the `EquatableComparer` to come first in the list - and to be the only comparer used for cases which do implement `IEquatable<T>`. If I have explicitly defined the meaning of equality for a type, I'd expect this to be the only definition of equality.
- `IComparable<T>` to not feature in the list, since it is about comparison, not about equality. To me that's a different use case (at least that 's how I read [the docs](https://docs.microsoft.com/en-us/dotnet/api/system.icomparable-1?view=netframework-4.7.2): `IComparable` is about ordering and sorting, `IEquatable` is about equality). For instance, in the example above, 2 persons can have the same name, and therefore the same order, but that doesn't mean they are the same person.

Am I missing something? Is there an underlying reason for using `IComparable<T>` when determining equality? What is the recommended way to test equality for objects which implement `IComparable<T>`?

We also found [this old issue](https://github.com/machine/machine.specifications/issues/200), which is somewhat related.

Thanks for your help! :)

 
Would it be possible to
get the Establish and because  delegate names in to the reports ?

Establish my_complex_pre_conditions_should_show_as_given_part_in_report
Because excersizing_my_behavior_should_show_as_when_part_in_report

Hi,
I am trying to to send the XML output to SonarQube, but get the error
```
 Missing root element <results> 
```
When I look into the report file, it looks a bit strange - lots of 'Enter' and 'Exit' tags, it seems like a debug output of the parser. So my question is: is this intentional? If yes, what is it good for?
And/or could we get something similar to the NUnit output, to send it to a report generator? 