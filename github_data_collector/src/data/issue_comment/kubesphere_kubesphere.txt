Reproduces steps:
1、create storage class more than 10
2、create a pvc，check storage class
excpeted result ：the storage class is  the default sc
actual result：the storage is empty


![image](https://user-images.githubusercontent.com/57089340/72674074-85fe2500-3aad-11ea-8d10-653f7a9fd14d.png)

**问题描述**
以下是我的两种安装openpitrix的方式
1. 最小化安装ks，然后修改configmap，将openpitrix设置为true
2. 安装ks时候直接将openpitrix设置为true。
第一种方式会导致一些安装不顺畅，比如openpitrix-system空间下的一些job卡在等待mysql和etcd的initContainer上。

**安装环境的硬件配置**
aws云主机，ubuntu18
在k8s上部署ks

**错误信息或截图**

**Installer版本**
2.1

**General remarks**

The dashboard does not show CPU, ram and disk usage. 

**Describe the bug(描述下问题)**

![20200117201622](https://user-images.githubusercontent.com/9475863/72611963-67d9dd00-3966-11ea-9b5e-a7e6999dc636.jpg)

After running `k logs -f node-exporter-gw6qr -n kubesphere-monitoring-system kube-rbac-proxy`, I get the following:

```
2020/01/17 11:57:02 http: proxy error: context canceled
2020/01/17 11:58:02 http: proxy error: context canceled
2020/01/17 11:59:02 http: proxy error: context canceled
2020/01/17 12:00:02 http: proxy error: context canceled
2020/01/17 12:01:02 http: proxy error: context canceled
2020/01/17 12:02:02 http: proxy error: context canceled
2020/01/17 12:03:02 http: proxy error: context canceled
2020/01/17 12:04:02 http: proxy error: context canceled
2020/01/17 12:05:02 http: proxy error: context canceled
2020/01/17 12:06:02 http: proxy error: context canceled
2020/01/17 12:07:02 http: proxy error: context canceled
2020/01/17 12:08:02 http: proxy error: context canceled
2020/01/17 12:09:02 http: proxy error: context canceled
2020/01/17 12:10:02 http: proxy error: context canceled
2020/01/17 12:11:02 http: proxy error: context canceled
2020/01/17 12:12:02 http: proxy error: context canceled
2020/01/17 12:13:02 http: proxy error: context canceled
2020/01/17 12:14:02 http: proxy error: context canceled
2020/01/17 12:15:02 http: proxy error: context canceled
2020/01/17 12:16:02 http: proxy error: context canceled
2020/01/17 12:17:02 http: proxy error: context canceled
2020/01/17 12:18:02 http: proxy error: context canceled
```
It seems that node exporters are unable to send data to the collector. 

**Versions used(KubeSphere/Kubernetes的版本)**

KubeSphere: v2.1.0
Kubernetes: 
Client: &version.Version{SemVer:"v2.16.1", GitCommit:"bbdfe5e7803a12bbdf97e94cd847859890cf4050", GitTreeState:"clean"}
Server: &version.Version{SemVer:"v2.16.1", GitCommit:"bbdfe5e7803a12bbdf97e94cd847859890cf4050", GitTreeState:"clean"}

**Environment(环境的硬件配置)**

1 master:  8cpu/8g
3 slave nodes: 8cpu/16g

Istio was installed prior to the installation of Kubesphere.


**Additional Debug Info**

`k exec node-exporter-gw6qr -n kubesphere-monitoring-system -it netstat -lpna`

```
/ $ netstat -lpna
netstat: showing only processes with your user ID
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      -
tcp        0      0 127.0.0.1:38406         0.0.0.0:*               LISTEN      -
tcp        0      0 0.0.0.0:55623           0.0.0.0:*               LISTEN      -
tcp        0      0 127.0.0.1:10248         0.0.0.0:*               LISTEN      -
tcp        0      0 127.0.0.1:10249         0.0.0.0:*               LISTEN      -
tcp        0      0 0.0.0.0:34059           0.0.0.0:*               LISTEN      -
tcp        0      0 127.0.0.1:9099          0.0.0.0:*               LISTEN      -
tcp        0      0 192.168.1.26:9100       0.0.0.0:*               LISTEN      26479/kube-rbac-pro
tcp        0      0 127.0.0.1:9100          0.0.0.0:*               LISTEN      26412/node_exporter
tcp        0      0 0.0.0.0:36844           0.0.0.0:*               LISTEN      -
tcp        0      0 0.0.0.0:111             0.0.0.0:*               LISTEN      -
tcp        0      0 192.168.1.26:7472       0.0.0.0:*               LISTEN      -
tcp        0      0 0.0.0.0:179             0.0.0.0:*               LISTEN      -
tcp        0      0 192.168.1.26:41818      192.168.122.135:44135   TIME_WAIT   -
tcp        0      0 127.0.0.1:9100          127.0.0.1:40752         CLOSE_WAIT  26412/node_exporter
tcp        0      0 192.168.1.26:36598      192.168.122.140:9090    TIME_WAIT   -
tcp        0      0 192.168.1.26:36650      192.168.122.140:9090    TIME_WAIT   -
tcp        0      0 192.168.1.26:57448      192.168.122.136:8080    TIME_WAIT   -
tcp        0      0 192.168.1.26:41782      192.168.122.141:9090    TIME_WAIT   -
tcp        0      0 192.168.1.26:36792      192.168.122.140:9090    TIME_WAIT   -
tcp        0      0 127.0.0.1:34762         127.0.0.1:9099          TIME_WAIT   -
tcp        0      0 192.168.1.26:46766      192.168.1.25:6443       ESTABLISHED -
tcp        0      0 192.168.1.26:41756      192.168.122.141:9090    TIME_WAIT   -
tcp        0    110 127.0.0.1:38406         127.0.0.1:51364         ESTABLISHED -
tcp        0      0 192.168.1.26:36770      192.168.122.140:9090    TIME_WAIT   -
tcp        0      0 192.168.1.26:58716      10.96.0.1:443           ESTABLISHED -
tcp        0      0 127.0.0.1:34906         127.0.0.1:9099          TIME_WAIT   -
tcp        0      0 192.168.1.26:46778      192.168.1.25:6443       ESTABLISHED -
tcp        0      0 192.168.1.26:56460      10.96.0.1:443           ESTABLISHED 26479/kube-rbac-pro
tcp        0      0 127.0.0.1:9100          127.0.0.1:41042         CLOSE_WAIT  26412/node_exporter
tcp        0      0 192.168.1.26:36622      192.168.122.140:9090    TIME_WAIT   -
tcp        0      0 127.0.0.1:34862         127.0.0.1:9099          TIME_WAIT   -
tcp        0      0 192.168.1.26:36678      192.168.122.140:9090    TIME_WAIT   -
tcp        0      0 192.168.1.26:179        192.168.1.27:42814      ESTABLISHED -
tcp        0      0 192.168.1.26:7472       192.168.122.131:41582   ESTABLISHED -
tcp        0      0 192.168.1.26:22         192.168.1.10:56500      ESTABLISHED -
tcp        0      0 192.168.1.26:965        192.168.1.200:2049      ESTABLISHED -
tcp        0      0 127.0.0.1:41042         127.0.0.1:9100          FIN_WAIT2   -
tcp        0      0 127.0.0.1:34810         127.0.0.1:9099          TIME_WAIT   -
tcp        0      0 127.0.0.1:34958         127.0.0.1:9099          TIME_WAIT   -
tcp        0      0 192.168.1.26:36556      192.168.122.140:9090    TIME_WAIT   -
tcp        0      0 127.0.0.1:51364         127.0.0.1:38406         ESTABLISHED -
tcp        0      0 127.0.0.1:34850         127.0.0.1:9099          TIME_WAIT   -
tcp        0      0 192.168.1.26:58718      10.96.0.1:443           ESTABLISHED -
tcp        0      0 192.168.1.26:41770      192.168.122.141:9090    TIME_WAIT   -
tcp        0      0 192.168.1.26:179        192.168.1.28:58960      ESTABLISHED -
tcp        0      0 192.168.1.26:41566      192.168.122.141:9090    TIME_WAIT   -
tcp        0      0 127.0.0.1:34712         127.0.0.1:9099          TIME_WAIT   -
tcp        0      0 192.168.1.26:41748      192.168.122.141:9090    TIME_WAIT   -
tcp        0      0 127.0.0.1:34752         127.0.0.1:9099          TIME_WAIT   -
tcp        0      0 127.0.0.1:9100          127.0.0.1:40464         CLOSE_WAIT  26412/node_exporter
tcp        0      0 192.168.1.26:41730      192.168.122.141:9090    TIME_WAIT   -
tcp        0      0 192.168.1.26:41814      192.168.122.141:9090    TIME_WAIT   -
tcp        0      0 127.0.0.1:34896         127.0.0.1:9099          TIME_WAIT   -
tcp        0      0 192.168.1.26:57170      192.168.122.131:9090    TIME_WAIT   -
tcp        0      0 192.168.1.26:46752      192.168.1.25:6443       ESTABLISHED -
tcp        0      0 127.0.0.1:34988         127.0.0.1:9099          TIME_WAIT   -
tcp        0      0 192.168.1.26:60030      10.96.0.1:443           ESTABLISHED -
tcp        0      0 192.168.1.26:41670      192.168.122.135:44135   TIME_WAIT   -
tcp        0      0 192.168.1.26:47047      192.168.1.25:179        ESTABLISHED -
tcp        0      0 192.168.1.26:48592      192.168.122.130:15014   TIME_WAIT   -
tcp        0      0 192.168.1.26:36576      192.168.122.140:9090    TIME_WAIT   -
tcp        0      0 127.0.0.1:34944         127.0.0.1:9099          TIME_WAIT   -
tcp        0      0 127.0.0.1:34796         127.0.0.1:9099          TIME_WAIT   -
tcp        0      0 :::43382                :::*                    LISTEN      -
tcp        0      0 :::22                   :::*                    LISTEN      -
tcp        0      0 :::30873                :::*                    LISTEN      -
tcp        0      0 :::31164                :::*                    LISTEN      -
tcp        0      0 :::32030                :::*                    LISTEN      -
tcp        0      0 :::30880                :::*                    LISTEN      -
tcp        0      0 :::36257                :::*                    LISTEN      -
tcp        0      0 :::10250                :::*                    LISTEN      -
tcp        0      0 :::30765                :::*                    LISTEN      -
tcp        0      0 :::31438                :::*                    LISTEN      -
tcp        0      0 :::37775                :::*                    LISTEN      -
tcp        0      0 :::111                  :::*                    LISTEN      -
tcp        0      0 :::10256                :::*                    LISTEN      -
tcp        0      0 :::31569                :::*                    LISTEN      -
tcp        0      0 :::31282                :::*                    LISTEN      -
tcp        0      0 :::32691                :::*                    LISTEN      -
tcp        0      0 :::30420                :::*                    LISTEN      -
tcp        0      0 ::ffff:192.168.1.26:10250 ::ffff:192.168.1.25:53114 ESTABLISHED -
tcp        0      0 ::ffff:192.168.1.26:10250 ::ffff:192.168.1.25:53174 ESTABLISHED -
tcp        0      0 ::ffff:192.168.1.26:10250 ::ffff:192.168.1.25:53018 ESTABLISHED -
tcp        0      0 ::ffff:192.168.1.26:10250 ::ffff:192.168.122.140:58854 ESTABLISHED -
tcp        0      0 ::ffff:192.168.1.26:10250 ::ffff:192.168.122.140:58940 ESTABLISHED -
tcp        0      0 ::ffff:192.168.1.26:10250 ::ffff:192.168.1.25:39024 ESTABLISHED -
udp        0      0 0.0.0.0:675             0.0.0.0:*                           -
udp        0      0 127.0.0.1:777           0.0.0.0:*                           -
udp        0      0 0.0.0.0:46360           0.0.0.0:*                           -
udp        0      0 0.0.0.0:59743           0.0.0.0:*                           -
udp        0      0 0.0.0.0:111             0.0.0.0:*                           -
udp        0      0 :::675                  :::*                                -
udp        0      0 :::42603                :::*                                -
udp        0      0 :::111                  :::*                                -
udp        0      0 :::57538                :::*                                -
raw        0      0 fe80::ecee:eeff:feee:eeee%lo:58 ::%22052:*              58          -
raw        0      0 fe80::ecee:eeff:feee:eeee%ens160:58 ::%22052:*              58          -
raw        0      0 fe80::ecee:eeff:feee:eeee%docker0:58 ::%22052:*              58          -
raw        0      0 fe80::ecee:eeff:feee:eeee%tunl0:58 ::%22052:*              58          -
raw        0      0 fe80::ecee:eeff:feee:eeee%5:58 ::%22052:*              58          -
raw        0      0 fe80::ecee:eeff:feee:eeee%6:58 ::%22052:*              58          -
raw        0      0 fe80::ecee:eeff:feee:eeee%7:58 ::%22052:*              58          -
raw        0      0 fe80::ecee:eeff:feee:eeee%8:58 ::%22052:*              58          -
raw        0      0 fe80::ecee:eeff:feee:eeee%9:58 ::%22052:*              58          -
raw        0      0 fe80::ecee:eeff:feee:eeee%10:58 ::%22052:*              58          -
raw        0      0 fe80::ecee:eeff:feee:eeee%11:58 ::%22052:*              58          -
raw        0      0 fe80::ecee:eeff:feee:eeee%12:58 ::%22052:*              58          -
raw        0      0 fe80::20c:29ff:fe77:8cee%13:58 ::%22052:*              58          -
```
[备注]: <> (请补全下面信息，帮助我们更快地定位问题。提交问题前预览下issue，看下是否有格式错误)

**问题描述**
fatal: [ks-allinone]: FAILED! => {
    "changed": false,
    "cmd": "dnf install -y python2-dnf",
    "rc": 1
}
STDOUT:
上次元数据过期检查：0:05:51 前，执行于 2020年01月17日 星期五 15时21分08秒。
未找到匹配的参数： python2-dnf
STDERR:
错误：没有任何匹配: python2-dnf
MSG:
错误：没有任何匹配: python2-dnf
**安装环境的硬件配置**
虚拟机环境centos8.1  ,刚装的干净虚拟机.没有安装任何软件,跟任何操作,直接安装报错.
安装的是all-in-one.
$ curl -L https://kubesphere.io/download/stable/v2.1.0 > installer.tar.gz \
&& tar -zxf installer.tar.gz && cd kubesphere-all-v2.1.0/scripts
 ./install.sh

[备注]: <> (请说明节点的运行环境，是否是物理机，云主机，VMware虚拟机)

**错误信息或截图**
![图片](https://user-images.githubusercontent.com/4714200/72592748-3cdb9300-393e-11ea-9955-85e79c46f27e.png)

**Installer版本**

[备注]: <> (请补全下面信息，帮助我们更快地定位问题。提交问题前预览下issue，看下是否有格式错误)

**问题描述**
按示例的应用是没问题，然后我自己从应用商店部署了个nginx，想用这个nginx模拟流量治理但不能操作，点击灰度发布的时候无法选择 组件，什么样的应用才能识别为支持灰度的组件
我目前是想部署一个v1版本的nginx，然后灰度到v2，其实镜像不变，我只是通过修改html判断
**安装环境的硬件配置**

[备注]: <> (请说明节点的运行环境，是否是物理机，云主机，VMware虚拟机)

**错误信息或截图**
![image](https://user-images.githubusercontent.com/34834139/72581764-6931e800-391b-11ea-8bc6-4dc50a3a6025.png)
![image](https://user-images.githubusercontent.com/34834139/72581876-cded4280-391b-11ea-97a9-45475fa3bbb2.png)



**Installer版本**
KubeSphere 版本 : v2.1.0
Signed-off-by: hongming <talonwan@yunify.com>

/kind cleanup

**What this PR does / why we need it**:

code refactor


**General remarks**

> Please delete this section including header before submitting
> 也可以使用中文
>
> This form is to report bugs. For general usage questions refer to our Slack channel
>        [KubeSphere-users](https://join.slack.com/t/kubesphere/shared_invite/enQtNTE3MDIxNzUxNzQ0LTdkNTc3OTdmNzdiODViZjViNTU5ZDY3M2I2MzY4MTI4OGZlOTJmMDg5ZTFiMDAwYzNlZDY5NjA0NzZlNDU5NmY)

**Describe the bug(描述下问题)**

- [ ] 节点容器组的容量固定为 110。当容量到达时，新建的 pods 会一直 pending， 期望能通过 UI 来修改。
- [ ] 在 installer 里预置一些告警规则来监控 kubesphere 的运行，可以在异常时及时通知管理员。


reproduce steps:
1、create a scm which type is svn
2、edit the scm ,change type from svn to single svn
Exceptd result: change effects, and start to scan repo
Actual result: change no effects

![image](https://user-images.githubusercontent.com/57089340/72501942-2ae0ee00-3873-11ea-87b7-a81ca8813c32.png)

reproduce steps:
1、create a scm，code repo select svn
2、type select svn
3、click OK button
4、after a few minutes, check Branch
Exceptd result: the commit is not empty
Actual result: the commit is  empty
![image](https://user-images.githubusercontent.com/57089340/72501481-e012a680-3871-11ea-945c-cc933805203b.png)


reproduce steps:
1、Create a scm，git code repo select ellaye/devops-branch-test
2、Discover Branches select All branches
3、Pull Stategy select [Two pipelines are created when PR is discovered......]
4、Click create button
Exceptd result: there is two PullRequest
Actual result: there is one PullRequest


![image](https://user-images.githubusercontent.com/57089340/72494829-092a3b80-3860-11ea-82d7-bf22e1ed8a76.png)
![image](https://user-images.githubusercontent.com/57089340/72494897-468ec900-3860-11ea-9517-7cabb9bcc349.png)
