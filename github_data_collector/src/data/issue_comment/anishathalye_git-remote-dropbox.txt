I believe that today, if you don't want to trust the cloud with your source code, you really can't use github, gitlab, vso, bitbucket, or even dropbox, because if you don't want your source code outside your premises, then it limits your options to zero.

But, what if you were to perform client side encryption such that it was transparent to the git user.   Ideally this would be done to a file-system that git talks to locally, and what gets pushed the cloud doesn't even look like a git repository.   This, as opposed to, using an existing git repository and then performing client-side encryption to the individual files.

It seems to me that git-remote-dropbox is in a unique scenario.  Since it's already providing its own protocol, it can (and does) store the actual files in any way that they like.

So if it encrypted the data, with the private key being local, you can truly store stuff on the cloud that cannot be looked at by anyone other than someone who has a private key.

Dropbox is pretty good for hosting large files. It would be really cool if we could support something kind of like [git-lfs](https://git-lfs.github.com/) in git-remote-dropbox and provide a really nice seamless user experience.

We currently store all objects as loose objects on the remote. This means that there's no delta compression.

I haven't thought of a storage format and purely client-side protocol that allows for packing / delta compression similar to git's on-disk format. It would be super cool if it is possible to do this, because that'll mean that repositories will consume a lot less space on Dropbox.

We don't do any garbage collection on the remote, so any dangling objects will remain on the remote.

I haven't figured out a way to do this that works on the client side and is safe (with regards to concurrent operations).

As a first step, it might be nice to write a separate script that performs GC on the remote that's only meant to be used when there are no concurrent operations. This should be pretty easy to write (essentially `git ls-remote`, then `git rev-list --objects` for all the SHAs, and then delete what's not listed).

After that, it might be nice to investigate if it's possible to design the remote helper so that it can perform GC.

Currently, we don't support the `--dry-run` option. It's worth investigating to see if it's possible without too much work.

Currently, we don't support shallow cloning. It's worth investigating to see if it's possible to do this without too much work.

Currently, setting up git-remote-dropbox involves creating a Dropbox API app and then generating an OAuth token for it.

This takes about a minute to do, and it's a one-time-thing, but it would be good if it could be streamlined to take even less time. It would be great if the helper could prompt the user to go through some kind of OAuth setup flow.

Doing this in such a way that we don't have to hard-code API app secrets would be great. (I'm not sure if it's possible though...)

We're already pretty good about figuring out which objects the remote is missing (see the design document), but we don't actually verify that every object we're uploading is actually missing from the server.

Doing an extra round trip to the server to check if the file is already present isn't a great idea. But unnecessarily uploading a large file isn't a great idea either.

If we could write files in a way such that we abort writing if there's a conflict, that would be great. Because git objects are content-addressed, if there's a conflict, that means the object already exists on the remote.

It may be the case that the "add" write mode behaves the way we want, failing fast when a file already exists.
