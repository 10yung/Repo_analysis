I'm wondering how installing Longhorn from Rancher Apps will be affected in a Project or Namespace with set Resource Quotas (since it requires deployments to have resource limits set) 

Will Longhorn handle this correctly?
Longhorn ends up spiking the CPU very high when the backend becomes unavailable, when it tries to `ls` the volume (up to dozens and dozens of times)

Creating a cascading pile up of problems when nothing else can use the CPU anymore

The best idea we can think of is how TCP handles retries, when it backs off double the duration, before retrying
**Steps to reproduce:**

- Create a pod using block volume using:

```
cat << EOF | kubectl apply -f -

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: csi-block-volume-test-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  volumeMode: Block
  storageClassName: longhorn
---
apiVersion: v1
kind: Pod
metadata:
  name: csi-block-volume-test
spec:
  containers:
  - image: busybox
    imagePullPolicy: IfNotPresent
    name: sleep
    args:
    - "/bin/sh"
    - "-c"
    - while true;do date;sleep 5; done
    volumeMounts: []
    volumeDevices:
    - name: longhorn-blk
      devicePath: "/dev/longhorn/longhorn-test-blk"
  volumes:
  - name: longhorn-blk
    persistentVolumeClaim:
      claimName: csi-block-volume-test-pvc

EOF
```

 - Check pod events using: `kubectl describe pod csi-block-volume-test`

```
Warning  FailedMapVolume         16s (x8 over 81s)   kubelet, meldafrawi-longhorn-01  MapVolume.EvalHostSymlinks failed for volume "pvc-aeaef528-364c-11ea-8746-ae27fc55bb46" : lstat /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-aeaef528-364c-11ea-8746-ae27fc55bb46: no such file or directory
```

- csi-attacher logs:

```
I0114 22:40:29.256270       1 connection.go:180] GRPC call: /csi.v1.Controller/ControllerPublishVolume
I0114 22:40:29.256284       1 connection.go:181] GRPC request: {"node_id":"meldafrawi-linode-longhorn-03","volume_capability":{"AccessType":{"Block":{}},"access_mode":{"mode":1}},"volume_context":{"fromBackup":"","numberOfReplicas":"3","staleReplicaTimeout":"2880","storage.kubernetes.io/csiProvisionerIdentity":"1579040250628-8081-driver.longhorn.io"},"volume_id":"pvc-1256dca5-5045-4157-bfb4-c8e397bc9319"}
I0114 22:40:31.332699       1 leaderelection.go:258] successfully renewed lease longhorn-system/external-attacher-leader-driver-longhorn-io
I0114 22:40:33.378224       1 connection.go:183] GRPC response: {}
I0114 22:40:33.378748       1 connection.go:184] GRPC error: <nil>
```
After adding a new path to a node, the page shows a blank page.

To show the UI again, you have to refresh the page

![image](https://user-images.githubusercontent.com/28725423/72369274-b91b6f80-36cd-11ea-82be-d88b6c263ddd.png)

I'll attach the browser logs next time it happens
![2020-01-13-190431](https://user-images.githubusercontent.com/11083305/72306623-beb78c00-36b2-11ea-8822-9d985268b774.png)
this seems to happen when i try to delete a pv or pvc after tearing down a deployment.

without reboot, those tgtd processes never go away.

rancherOS
The total number of snapshot/backup schedule's retain number shouldn't exceed 50, otherwise it can cause issues like #1010.
Clicking a replica name should take you to the Volume it is a part of

![image](https://user-images.githubusercontent.com/28725423/72266141-c6602d80-35eb-11ea-9477-0b9a36a54e20.png)

Volume errors in UI could help troubleshoot what caused it to fault
Does longhorn support simultaneous reading and writing of pods deployed on different nodes? Like cephfs
Hi, one of my volumes is stuck in the deleting state for days. The volume is released but it is now in a failed state for the last remaining node. Also it is assigned to an instance manager which does not exist anymore. 
![deleting](https://user-images.githubusercontent.com/854965/72166395-6ee27780-33c9-11ea-8bcc-540428c047bc.png)

Smashing the delete button does not help ;) I also rebooted the assigned node, no changes. Dropdown options are: Backups, Delete, Create PV/PVC