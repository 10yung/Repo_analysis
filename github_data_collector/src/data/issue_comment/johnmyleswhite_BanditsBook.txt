
Hello John,

First, thank you for writing one of the most enjoyable books in the domain!

One thing that puzzled me was since most of the code was in python, why did we not have the charts and visualizations in python as well. Since I did not know R, I spend quite some time into recreating these with matplotlib while I was working thought the book. (example here: https://github.com/niazangels/bandits/blob/master/01-epsilon-greedy.ipynb)

I know there hasn't been any commits to this repo fo a while, but I was wondering if you would be interested in adding these to your repo. If so what would be a good way to structure them (my plots  are in Jupyter notebooks, but I can split them into files).

I could raise a pull request in case you're interested, but feel free to close this issue if this isn't something you'd like to pursue.
We are currently working on a R package ("[contextual](https://github.com/Nth-iteration-labs/contextual)") that aims to facilitate the implementation and simulation of both context-free and contextual Multi-Armed Bandit policies in R.

As "Bandit Algorithms for Website Optimization" offers a comprehensive entry-level introduction to context-free bandits policy evaluation, we decided to [replicate the book's simulations](https://nth-iteration-labs.github.io/contextual/articles/website_optimization.html). 

In doing so, we found that the book's source code in this repository deterministically chooses the first arm (in other words, the arm with the lowest index) when rewards between arms are tied:
```
def ind_max(x):
  m = max(x)
  return x.index(m)
```

As can be seen in our replication vignette, this introduces a bias that adds up over time, changing simulations' results and plots. To illustrate, left our replication of Figure 4-2 without breaking ties randomly, right when correctly breaking ties randomly:

![compare](https://user-images.githubusercontent.com/95014/53694992-2bd17400-3db6-11e9-8c91-e6e4d91e60a1.png)

A patch along the following lines would resolve this issue by breaking ties randomly:
```
def ind_max(x):
  max_value = max(x)
  max_keys = [k for k, v in enumerate(x) if v == max_value]
  return random.choice(max_keys)
```
(I presume that the now closed but unresolved #10 also alluded to this particular issue)


If the proposed changes are okay, I plan to work on the rest of the Julia code base to bring it up to speed with 0.6.2+
I tried to implement thompson sampling.
please add this if possible.

now only R is shown beside this repository name, if we click to show all your repos. if possible change the description to show that other language code exists in this repo. maybe like "code in julia, R, python, Ruby for my book on Multi-Armed Bandit Algorithms"

The right arm can't be arm number 5 since arm numbers run from 0 to 4. Also after suffling the values for the means the maximum value is not any longer at the end of the list, but in the second place (seed(1) is to blame for this).

Rewrite some example code and use more idiomatic python style.

PS. How to run the tests for these examples?
