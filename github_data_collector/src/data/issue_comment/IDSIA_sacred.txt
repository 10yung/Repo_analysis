We'd like to return the trained model from the @ex.main wrapped function and use it for other eval purposes later on.

```
@ex.main
def train_and_eval(_config):
    model = train_model()
    return model
run = ex.run()
model = run.result
```
This method isn't ideal because the whole object gets stored in the mongodb and is wasteful. We don't want to store it/log it, we just want to return it. Is there a supported way of doing this? If not, how do you recommend we modify the library to add support for this?

![image](https://user-images.githubusercontent.com/5924975/72582747-cbee9980-38b1-11ea-9b0d-9f9b83b8c210.png)

_only log what you need._
When the experiment name ends with one or several dots ("."), sacred fails to parse the commands (it removes as many letters from the command name as there are dots in the end of the experiment name).

For instance, the following code runs fine:
```python
from sacred import Experiment

ex = Experiment('name_of_exp')

@ex.automain
def my_main():
    print('Hello world!')
```

But if I change the experiment name:
```python
from sacred import Experiment

ex = Experiment('name_of_exp.')

@ex.automain
def my_main():
    print('Hello world!')
```
It returns:
```
Usage:
  test_sacred.py [(with UPDATE...)] [options]
  test_sacred.py help [COMMAND]
  test_sacred.py (-h | --help)
  test_sacred.py COMMAND [(with UPDATE...)] [options]
Error: Command "my_main" not found. Available commands are: int_config, y_main, ve_config, int_dependencies, int_named_configs
```
(note the 'y_main' appearing instead of 'my_main').
And if I add two dots:
```python
from sacred import Experiment

ex = Experiment('name_of_exp..')

@ex.automain
def my_main():
    print('Hello world!')
```
It returns:
```
Usage:
  test_sacred.py [(with UPDATE...)] [options]
  test_sacred.py help [COMMAND]
  test_sacred.py (-h | --help)
  test_sacred.py COMMAND [(with UPDATE...)] [options]
Error: Command "my_main" not found. Available commands are: int_config, _main, ve_config, int_dependencies, int_named_configs
```
(note the '_main' appearing instead of 'my_main').

And so on.

I got this bug when updating sacred from 0.7.4 to 0.8.1.
Hello sacred authors!

For reasons that I explain below, I am trying to find a way to instantiate an `Experiment` in a function that I define in some library and then call this function from a main script and run the experiment. The library file where the `Experiment`-instantiating function is defined is take as the `mainfile` of the experiment and the script where I run the experiment is not part of the sources.

I understand that this is not how `sacred` is intended to be used, but I was wondering if there was a way around this behavior. One way I see around this would be to allow the user to specify the `mainfile` of the experiment, although I do not know if this might have unwanted consequences.

To be more explicit, here's a minimal example:

``` #library.py
import sacred

def create_exp(name="myexp"):
    return sacred.Experiment(name)
```

``` #running_script.py
from library import create_exp

ex = create_exp()

@ex.automain
def main():
    print("hello world")
```

For which the `mainfile` is `library.py` and `running_script.py` is not included in the source list. 

-----

In case you wonder why I would do this, here's more context

- I have my own `ExperimentManager` class that is in charge of organizing, instantiating and running the different components of experiments. It collects the configuration of a run and metrics and up to now was logging them using `tensorboard/hparams`

- I found that I could quite easily switch to `sacred` by creating a script where I create a `sacred.Experiment` and an `ExperimentManager`, log the parameter dictionary  that I pass to my `ExperimentManager.prepare_run` with `Experiment.add_config` and then define an `@Experiment.main`-decorated function that calls my `ExperimentManager.do_run`. This is already great but it somewhat duplicates operations.

- Ideally, for user-friendliness, I would like to move on to a more streamlined version where the `sacred.Experiment` and my `ExperimentManager` are blended in some way. I gather from issue #193 that for the moment inheriting from `sacred.Experiment` is not the way so I tried to have `ExperimentManger.__init__ instantiate a `sacred.Experiment` as its attribute, call `add_config` when it sets an experiment up and decorate its `do_run` method as the @ex.main. In this way, all sacred operations are hidden under the hood and the user does not have to care about it. Most of this works, except for the source discovery. 

I understand that this is a little hacky and strays from the script-based approach that is default for sacred, but if you can think of a workable way to ensure import-based source discovery, I would really appreciate it.


Function parameters are not fit between 
**Class Ingredient** and **dependenies.source_discovery_strategies dict**

In Ingredient,
```
def __init__(...):
...
        (
            self.mainfile,
            self.sources,
            self.dependencies,
        ) = gather_sources_and_dependencies(
            _caller_globals, save_git_info, self.base_dir
        )
...
```
is written, passing 3 arguments.


However in **gather_sources_and_dependencies** you can see
```
source_discovery_strategies = {
    "none": lambda globs, path: set(),
    "imported": get_sources_from_imported_modules,
    "sys": get_sources_from_sys_modules,
    "dir": get_sources_from_local_dir,
}
```

So when I set 'none' on SETTING['DISCOVER_SOURCES'],
error occurs.

Basically, 'none' returns set(), not dependent on any arguments.
So it could be resolved as passing non-usable unlimited arguments like below:

```
source_discovery_strategies = {
    "none": *_, **__: set(),
    "imported": get_sources_from_imported_modules,
    "sys": get_sources_from_sys_modules,
    "dir": get_sources_from_local_dir,
}
```

We encountered the following error when capturing stdout.
OS: ubuntu 18.04
Python: 3.7.4
sacred: 0.8.0
```
Traceback (most recent calls WITHOUT Sacred internals):
  File "/home/xxxx/programs/anaconda3/lib/python3.7/contextlib.py", line 119, in __exit__
    next(self.gen)
  File "/home/xxxx/programs/anaconda3/lib/python3.7/subprocess.py", line 990, in wait
    return self._wait(timeout=timeout)
  File "/home/xxxx/programs/anaconda3/lib/python3.7/subprocess.py", line 1616, in _wait
    raise TimeoutExpired(self.args, timeout)
subprocess.TimeoutExpired: Command '['tee', '-a', '/tmp/tmpdhu34kps']' timed out after 1 seconds
```
Hello,  
I am trying to log about 18K data points in 40~ different metrics (close to a million data points).  
I am doing this all at once in a for loop, and when I view the results in omniboard: 1) it takes a very long time to load, and 2) not all the points eventually get plotted.  
If I want to plot many scalars at once (ie. at the end of the experiment and not during) is there a better way than a huge for loop?  

Many thanks,  
Elad
Sacred stores a bunch of store in the database backend. Is there a way to retrieve those objects for later use in a different Python program?
Exploring whether using pyfilesystem could unify the the filesttorage
and the S3Observer.
Hi,
when adding a yaml-file using syntax to execute python funcitons as config to an experiment object, the following error is thrown: 

`
yaml.constructor.ConstructorError: while constructing a Python instance
expected a class, but found <class 'builtin_function_or_method'>
  in "/home/azmkuu/Desktop/tracking_wo_bnw/output/fpn/res101/mot_2017_train/voc_init_iccv19/config.yaml", line 47, column 16
`.

[This is due to the latest security improvements of the pyyaml library](https://github.com/yaml/pyyaml/wiki/PyYAML-yaml.load(input)-Deprecation)  and [your source code](https://github.com/IDSIA/sacred/blob/master/sacred/config/config_files.py) using the `FullLoader`-class:


if opt.has_yaml:

    def load_yaml(filename):
        return opt.yaml.load(filename, Loader=opt.yaml.FullLoader)

    yaml_handler = Handler(load_yaml, opt.yaml.dump, "")

    for extension in yaml_extensions:
        HANDLER_BY_EXT[extension] = yaml_handler


Would it be possible to let the user specify the type of loader they want to use?  I was able to fix this by using the pyyaml's `UnsafeLoader`-class. 
Happy to make a pull-request in case you agree to this fix.
Output does not appear to be captured when running in a local JupyterLab on OSX. Are there any known workarounds?

The result is the "Captured Out" data tab of an experiment is a blank box. Other output we care about appears to be capturing (such as metrics).

When running from the commandline, output is captured correctly.