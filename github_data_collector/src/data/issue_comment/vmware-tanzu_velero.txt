- Add documentation in how to setup autocompletion for Velero CLI

  fixes: https://github.com/vmware-tanzu/velero/issues/1505 

Instructions tested on:
- [x] Linux Ubuntu 18.04 - bash
- [x] Linux Ubuntu 18.04 - zsh
- [x] macOS Catalina - bash
- [x] macOS Catalina - zsh
Signed-off-by: Steve Kriss <krisss@vmware.com>

closes #276

Old tech-debt issue that can finally be cleaned up. I also added a comment about why we (unfortunately) still need the `third_party` copy of the shortcut expander (xref #499).

Basic testing looks good (confirmed the relevant shortcuts work, plus no performance regression) but would like to do a little more before merging.
How do I use Velero to backup and restore rancher catalog apps? I backed up a namespace and restored it fine, but the app did not show up on the apps page.
This initial design only covers the reorganization/creation of commands.

Addressing changes that'll be required in the system is still WIP/to be added.

Signed-off-by: Carlisia <carlisia@vmware.com>
Signed-off-by: Steve Kriss <krisss@vmware.com>

closes #2121 

cc @dymurray

Basic validation on this looks good, but I'd like to do some more testing before merging so leaving in draft status.
**What steps did you take and what happened:**

We're using velero 1.2.0 to back up our EKS cluster and some EFS PVs. The velero service account is using AWS's eks.amazonaws.com/role-arn annotation to attach a role to it. This works fine for backing up data from velero, however restic does not seem to support pulling the AWS token properly with that mechanism, and it is using anonymous authentication, and failing to store data in S3.

**What did you expect to happen:**

Restic should be able to back up using the same credentials velero uses

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

Error message from backup logs:
```
time="2020-01-14T22:06:29Z" level=error msg="Error backing up item" backup=velero/infra-test-efs-backup-for-realz3 error="restic repository is not ready: error running command=restic init --repo=s3:s3-us-east-1.amazonaws.com/velero-platform-v2-backup/restic/kube-system --password-file=/tmp/velero-restic-credentials-kube-system355348533 --cache-dir=/scratch/.cache/restic, stdout=, stderr=Fatal: create key in repository at s3:s3-us-east-1.amazonaws.com/velero-platform-v2-backup/restic/kube-system failed: client.PutObject: Access Denied\n\n: exit status 1" error.file="/go/src/github.com/vmware-tanzu/velero/pkg/restic/repository_ensurer.go:144" error.function="github.com/vmware-tanzu/velero/pkg/restic.(*repositoryEnsurer).EnsureRepo" group=v1 logSource="pkg/backup/resource_backupper.go:288" name=test-pod2 namespace= resource=pods
```

Restic pod logs:
```
time="2020-01-13T21:26:09Z" level=info msg="Setting log-level to INFO"
time="2020-01-13T21:26:09Z" level=info msg="Starting Velero restic server v1.2.0 (5d008491bbf681658d3e372da1a9d3a21ca4c03c)" logSource="pkg/cmd/cli/restic/server.go:62"
time="2020-01-13T21:26:10Z" level=info msg="Starting controllers" logSource="pkg/cmd/cli/restic/server.go:156"
time="2020-01-13T21:26:10Z" level=info msg="Starting controller" controller=pod-volume-backup logSource="pkg/controller/generic_controller.go:76"
time="2020-01-13T21:26:10Z" level=info msg="Waiting for caches to sync" controller=pod-volume-backup logSource="pkg/controller/generic_controller.go:79"
time="2020-01-13T21:26:10Z" level=info msg="Controllers started successfully" logSource="pkg/cmd/cli/restic/server.go:199"
time="2020-01-13T21:26:10Z" level=info msg="Starting controller" controller=pod-volume-restore logSource="pkg/controller/generic_controller.go:76"
time="2020-01-13T21:26:10Z" level=info msg="Waiting for caches to sync" controller=pod-volume-restore logSource="pkg/controller/generic_controller.go:79"
time="2020-01-13T21:26:10Z" level=info msg="Caches are synced" controller=pod-volume-backup logSource="pkg/controller/generic_controller.go:83"
time="2020-01-13T21:26:10Z" level=info msg="Caches are synced" controller=pod-volume-restore logSource="pkg/controller/generic_controller.go:83"
W0114 10:25:17.438939       1 reflector.go:302] github.com/vmware-tanzu/velero/pkg/cmd/cli/restic/server.go:197: watch of *v1.Secret ended with: too old resource version: 11493244 (11502245)
W0114 17:48:06.516931       1 reflector.go:302] github.com/vmware-tanzu/velero/pkg/cmd/cli/restic/server.go:197: watch of *v1.Secret ended with: too old resource version: 11609586 (11656444)
```
AWS Error message:
```
{
  "recipientAccountId": "REDACTED",
  "userIdentity": {
    "type": "AWSAccount",
    "principalId": "",
    "accountId": "ANONYMOUS_PRINCIPAL"
  },
  "responseElements": null,
  "errorMessage": "Access Denied",
  "requestID": "REDACTED",
  "managementEvent": false,
  "eventID": "REDACTED",
  "userAgent": "[Minio (linux; amd64) minio-go/v6.0.14]",
  "eventName": "PutObject",
  "resources": [
    {
      "type": "AWS::S3::Object",
      "ARN": "REDACTED/restic/kube-system/keys/165975608458061c33f1fd9d74c71589a39c1c603bd680ed027a26a7a42b3aa7"
    },
    {
      "type": "AWS::S3::Bucket",
      "ARN": "REDACTED",
      "accountId": "REDACTED"
    }
  ],
  "readOnly": false,
  "eventVersion": "1.06",
  "eventTime": "2020-01-14T22:05:04Z",
  "vpcEndpointId": "REDACTED",
  "requestParameters": {
    "key": "restic/kube-system/keys/165975608458061c33f1fd9d74c71589a39c1c603bd680ed027a26a7a42b3aa7",
    "bucketName": "REDACTED",
    "Host": "REDACTED"
  },
  "awsRegion": "us-east-1",
  "eventSource": "s3.amazonaws.com",
  "sharedEventID": "REDACTED",
  "additionalEventData": {
    "CipherSuite": "ECDHE-RSA-AES128-GCM-SHA256",
    "bytesTransferredOut": 243,
    "bytesTransferredIn": 0,
    "x-amz-id-2": "REDACTED"
  },
  "sourceIPAddress": "REDACTED",
  "errorCode": "AccessDenied",
  "eventType": "AwsApiCall"
}
```

**Anything else you would like to add:**
This was an issue for us last fall with velero. The solution was to update the AWS SDK to the latest version, and things magically worked afterwards. Not sure what might be needed since restic is using minio though. Looks like the latest version of restic uses minio 6.0.43 so maybe updating to that is enough?

For more info on the service-account IAM role stuff:
https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html
https://docs.aws.amazon.com/eks/latest/userguide/specify-service-account-role.html

**Environment:**

- Velero version (use `velero version`):  1.2.0
- Velero features (use `velero client config get features`): <NOT SET>
- Kubernetes version (use `kubectl version`): 1.14.9
- Kubernetes installer & version: EKS
- Cloud provider or hardware configuration: AWS
- OS (e.g. from `/etc/os-release`): amazon-linux

Partial fix for #2159 

Using this patch moves the restore issues beyond version mismatches, however if CRDs in the backup have `Spec.PreserveUnknownFields = True` in the backup, whether they were v1beta1 or v1, they will see these errors if only this change is applied.

```
Errors:
  Velero:     <none>
  Cluster:  error restoring customresourcedefinitions.apiextensions.k8s.io/deletebackuprequests.velero.io: CustomResourceDefinition.apiextensions.k8s.io "deletebackuprequests.velero.io" is invalid: spec.preserveUnknownFields: Invalid value: true: cannot set to true, set x-preserve-unknown-fields to true in spec.versions[*].schema instead
            error restoring customresourcedefinitions.apiextensions.k8s.io/downloadrequests.velero.io: CustomResourceDefinition.apiextensions.k8s.io "downloadrequests.velero.io" is invalid: spec.preserveUnknownFields: Invalid value: true: cannot set to true, set x-preserve-unknown-fields to true in spec.versions[*].schema instead
            error restoring customresourcedefinitions.apiextensions.k8s.io/podvolumebackups.velero.io: CustomResourceDefinition.apiextensions.k8s.io "podvolumebackups.velero.io" is invalid: spec.preserveUnknownFields: Invalid value: true: cannot set to true, set x-preserve-unknown-fields to true in spec.versions[*].schema instead
            error restoring customresourcedefinitions.apiextensions.k8s.io/podvolumerestores.velero.io: CustomResourceDefinition.apiextensions.k8s.io "podvolumerestores.velero.io" is invalid: spec.preserveUnknownFields: Invalid value: true: cannot set to true, set x-preserve-unknown-fields to true in spec.versions[*].schema instead
            error restoring customresourcedefinitions.apiextensions.k8s.io/resticrepositories.velero.io: CustomResourceDefinition.apiextensions.k8s.io "resticrepositories.velero.io" is invalid: spec.preserveUnknownFields: Invalid value: true: cannot set to true, set x-preserve-unknown-fields to true in spec.versions[*].schema instead
            error restoring customresourcedefinitions.apiextensions.k8s.io/restores.velero.io: CustomResourceDefinition.apiextensions.k8s.io "restores.velero.io" is invalid: spec.preserveUnknownFields: Invalid value: true: cannot set to true, set x-preserve-unknown-fields to true in spec.versions[*].schema instead
            error restoring customresourcedefinitions.apiextensions.k8s.io/schedules.velero.io: CustomResourceDefinition.apiextensions.k8s.io "schedules.velero.io" is invalid: spec.preserveUnknownFields: Invalid value: true: cannot set to true, set x-preserve-unknown-fields to true in spec.versions[*].schema instead
            error restoring customresourcedefinitions.apiextensions.k8s.io/serverstatusrequests.velero.io: CustomResourceDefinition.apiextensions.k8s.io "serverstatusrequests.velero.io" is invalid: spec.preserveUnknownFields: Invalid value: true: cannot set to true, set x-preserve-unknown-fields to true in spec.versions[*].schema instead
```

Signed-off-by: Nolan Brubaker <brubakern@vmware.com>
Signed-off-by: Steve Kriss <krisss@vmware.com>

closes #1777 

Open to moving this code around per https://github.com/vmware-tanzu/velero/issues/1777#issuecomment-574361892, but this seems like the simplest approach to me and is consistent with how existing gauge metrics are reconstructed.

Did some basic testing and confirmed the metrics re-populate after a server restart.
# Auto Deployment (minio + velero)

This script is created to automatically install velero on Kubernetes.  
First, it deploys minio as deployment and service on Kubernetes.  
Then, it deploys velero server by executing velero CLI (install).  

## Tested environment
ubuntu 18.04.3 LTS  
Kubernetes v1.15.3  
Docker 19.03.5  


## Usage
### Install:
`./velero.sh i`

### Uninstall:
`./velero.sh u`
**What steps did you take and what happened:**

I deleted multiple backups.

`velero backup delete  tools-b-gitlab-backup-pr-20200110012032 tools-b-gitlab-backup-pr-20200112012003 tools-b-gitlab-backup-pr-20200113012004`

One was deleted successfuly and the two others are stucks :
```
tools-b-gitlab-backup-pr-20200112012003       Deleting                     2020-01-12 02:20:03 +0100 CET   4d        default            <none>
tools-b-gitlab-backup-pr-20200110012032       Deleting                     2020-01-10 02:20:32 +0100 CET   2d        default            <none>
```
` velero backup describe  tools-b-gitlab-backup-pr-20200110012032`

```
Deletion Attempts (1 failed):
  2020-01-14 10:19:15 +0100 CET: Processed
  Errors:
    restic repository is not ready: error running command=restic snapshots --repo=s3:http://oca-miniolb.oca.local/velero/tools-b/restic/ok106-gitlab-pr --password-file=/tmp/velero-restic-credentials-ok106-gitlab-pr375672370 --cache-dir=/scratch/.cache/restic --last, stdout=, stderr=Fatal: unable to create lock in backend: repository is already locked exclusively by PID 1115 on velero-79845659c-bwb25 by  (UID 0, GID 0)
lock was created at 2020-01-14 09:16:35 (40.790581918s ago)
storage ID 8526648a
: exit status 1
    restic repository is not ready: error running command=restic snapshots --repo=s3:http://oca-miniolb.oca.local/velero/tools-b/restic/ok106-gitlab-pr --password-file=/tmp/velero-restic-credentials-ok106-gitlab-pr375672370 --cache-dir=/scratch/.cache/restic --last, stdout=, stderr=Fatal: unable to create lock in backend: repository is already locked exclusively by PID 1115 on velero-79845659c-bwb25 by  (UID 0, GID 0)
lock was created at 2020-01-14 09:16:35 (40.790581918s ago)
storage ID 8526648a
: exit status 1
```

Theses locks happens very often. Most of the time I need to wait for the lock to be deleted ( around 30mins each time ).




