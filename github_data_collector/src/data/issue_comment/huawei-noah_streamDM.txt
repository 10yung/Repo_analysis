### Bug report #100 - part of the solution
#### 1. remove duplicates of the output coreset
#### 2. fixed the break statement

 still needs some fixes in StreamKM

## Summary of the changes
***BucketManager's getCoreset*** returned a lot of duplicate Examples in the resulted coreset. Almost over 50% of the examples was duplicates. With these changes this percentage is around 10%  ***but not zero.***

in ***splitCoresetTreeLeaf*** sometimes the one child had all the points and the other was empty. Now it rechooses a center and splits the points again.

in ***splitCoresetTree*** changed the way of choosing the which node to split. Similar to[ moa's treeCoreset](https://github.com/Waikato/moa/blob/master/moa/src/main/java/moa/clusterers/streamkm/TreeCoreset.java)
in ***BucketManager*** the break statement in ***getCoreset*** does not work properly. Now there is a flag that stops the process when a full bucket is found. 



## Tests
_need to print the resulted coreset in StreamKM be sure getCoreset is invoked at least once_

## Bug Report TreeCoreset-StreamKM-BucketManager
### Expected behavior
  A TreeCoreset contains the methods to extract an m-sized array of Examples from an n-sized stream of  Examples where n>m. 
### Observed behavior

The extracted coreset contains many duplicate Examples with weight = 0.0 

I used a custom file as input since some of the readers have issues. I attached the file below. 
### Command line
./spark.sh "ClusteringTrainEvaluate -c (StreamKM -s 100 -w 1000) -s (SocketTextStreamReader)" 1> result.txt 2> log_streamKM.log

### To reproduce the issue several changes needs to be done. 
1. In bucketManager method getCoreset  needs to change, because break obviously does not work so I used an "**isFound**" flag instead (any other suggestions instead of a flag are welcomed)
       ` def getCoreset: Array[Example] = {`
              ` if(buckets(L-1).isFull) {`
 `buckets(L-1).points.toArray `
   ` }else {`
      `var i = 0`
      `var coreset = Array[Example]()`
   `   for(i <- 0 until L) {`
      `  if(buckets(i).isFull) {`
        `  coreset = buckets(i).points.toArray`
       **`   break`**
     `   }`
     ` }`
    `  val start = i+1`
    `  for(j <- start until L) {`
      `  val examples = buckets(j).points.toArray ++ coreset`
      `  val tree = new TreeCoreset`
      `  coreset = tree.retrieveCoreset(tree.buildCoresetTree(examples, maxsize),`
                  `  new Array[Example](0))`
    `  }`
  `    coreset`
   ` }`
 ` }`
`}`



2. You have to print the output of the coreset in StreamKM using Example.toString and Example.weight to see that some Examples are changing their weight values to 0.0 even if 1.0 is the default value. (Thats probably happening in line 187 of TreeCoreset.scala where the leaf node has no elements so e.n is zero)

3. I did a pull request in StreamKM but the issues are still there. Since method getCoreset is called every time a new Example input comes  (a really slow process.) I checked[ moa's version in java ](https://github.com/Waikato/moa/blob/master/moa/src/main/java/moa/clusterers/streamkm/BucketManager.java) (line 128) and there is a comment about  calling getCoreset after the streaming process is finished. So in spark I'am guessing we should call it when numInstances are equals to rdd.count divided with the number of workers or the repartition of the input.  

4. [in TreeCoreset.scala line 86](https://github.com/huawei-noah/streamDM/blob/master/src/main/scala/org/apache/spark/streamdm/clusterers/clusters/TreeCoreset.scala) funCost has sometimes zero value so when costOfPoint is divided by funcost sum is NaN 
#### Any thoughts on how to solve the problem with the duplicates? Thanks in advance. 


txt file with 1000 randomly chosen points 
[random2.txt](https://github.com/huawei-noah/streamDM/files/2287920/random2.txt)

### Infrastructure details

    Java Version: 1.8.0_144
    Scala Version: 2.11.6
    Spark version: 2.2.0
    OS version: ubuntu 16.4
    Spark Standalone cluster mode

ONCE based on spark streaming

## Summary of the changes
_我们希望将ONCE算法加入到streamdm中，该算法可以从动态的信号流中，统计出带有时间约束序列片段出现的次数，准确率达到了100%，具体算法描述可以参考我们与华为进行的合作项目中ONCE算法的相关论文和专利描述。_

我们在streamDM-master/src/main/scala/org/apache/spark/streamdm中添加了ONCEStreaming文件夹，其中有两个Scala文件用来描述算法。一个文件保存测试数据。
Sender.scala 这个文件的作用是生成测试数据，数据是以数据对的形式出现的，我们在测试数据中使用（0到9随机数）来模拟信号的生成，每一个信号和当时出现的时间组成一个数据对，我们将每50个数据对组成一个list，封装到RDD中通过socket向接收端发送，发送间隔是1秒。（当前我们使用的是数字作为信号，后期可以进行修改）
接收并处理的文件是我们上传的另一个文件，ONCEStreaming.scala，当运行Sender.scala 之后，启动spark集群后运行ONCEStreaming.scala,我们当前程序中进行挖掘的是序列（8，8），时间约束是十秒，spark每5秒对传输来的数据进行处理，写到了程序中，当信号的格式确定之后我们也同样会作出修改，作为参数进行输入，程序的输出就是随着信号流的产生而统计的待计数信号片段出现的频率。
## Tests
测试结果在test_result.txt中
# Enhancement 
## Description

我们提出了ONCE算法，该算法主要处理序列挖掘的问题，当信号依次到达时，该算法可以有效的从动态到达的数据中，计算出带有时间约束的信号序列片段出现的频率，并将该算法应用到Spark Streaming上。我们希望将该算法结合到华为的streamdm上。

## Resources
我们的具体方案描述在论文ONCE: Counting the Frequency of Time-constrained Serial Episodes in a Streaming Sequence 。

**Command line**
Sender.scala文件产生测试数据，
ONCEStreaming.scala 文件使用ONCE算法对数据进行分析。
首先运行Sender.scala产生测试数据拥塞等待socket的链接，启动spark集群，之后运行ONCEStreaming.scala ，socket链接成功后，开始对Sender产生并发送来的信号序列片段进行计数统计。
**Data source**
_这里测试数据是我们自己随机生成的，使用0到9表示10个不同信号，信号与其到达的时间组成数据对，将每50个数据对做成一个队列封装到RDD中作为测试数据，例如（2,50）表示的是，信号2的到达时间是50，我们将每50个数字对作为一组，装入RDD通过socket发送给接收端，作为测试数据使用。生成数据的代码如下：

`
**Infrastructure details**
* **Java Version:“1.8.0_152-ea”** 
* **Scala Version:2.11.8** 
* **Spark version:spark-2.1.0-bin-hadoop2.7* 
* **OS version:macOS 10.13.2** 
* **local mode** 
