When using sd.nn.batchNorm(...), at the line SDVariable g = v.gradient(); in SameDiff.java, the gamma or beta's gradient g is null, Is it normal?
#### Issue Description

I want to be able to build an application which uses DL4J and is able to run on CPUs supporting AVX2 and on CPUs which do not support it. I am adding both binaries to the classpath / both dependencies to the POM.

I expect that DL4J is able to automatically determine if the CPU supports AVX2 or not. If AVX2 is supported, I expect it to load the AVX2 binaries, otherwise I expect it to load the generic binaries.

The observed behavior is that DL4J during loading locates both, the AVX2 and generic binaries and then tries to load the AVX2 binaries despite the CPU not supporting AVX2. This leads to a VM crash because an illegal opcode is found.

`org.bytedeco.javacpp.Loader` is able to deal is problems related to unsatisfied links, but not with checking if a binary is adequate for a particular CPU platform.

DL4J is able to display a message when the generic CPU binaries are used on an AVX2-capable system, so I would expect automatically using the AVX2 binaries on should also be possible.

#### Version Information

Please indicate relevant versions, including, if relevant:

* Deeplearning4j version: 1.0.0-beta6
* Platform information (OS, etc): Linux (Debian/Ubuntu)

#### Additional Information

<details>
<summary>Crash log</summary>
<pre>
Loading .../.javacpp/cache/nd4j-native-1.0.0-beta6-linux-x86_64-avx2.jar/org/nd4j/nativeblas/linux-x86_64-avx2/libnd4jcpu.so
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGILL (0x4) at pc=0x00007f7e71e50ae9, pid=2216, tid=0x00007f7eb26b9700
#
# JRE version: OpenJDK Runtime Environment (8.0_232-b09) (build 1.8.0_232-8u232-b09-0ubuntu1~18.04.1-b09)
# Java VM: OpenJDK 64-Bit Server VM (25.232-b09 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libnd4jcpu.so+0x52c0ae9]  nd4j::OpTracker::getInstance()+0x29
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# .../hs_err_pid2216.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
</pre>
</details>

<details>
<summary>Except from .../hs_err_pid2216.log</summary>
<pre>
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGILL (0x4) at pc=0x00007f7e71e50ae9, pid=2216, tid=0x00007f7eb26b9700
#
# JRE version: OpenJDK Runtime Environment (8.0_232-b09) (build 1.8.0_232-8u232-b09-0ubuntu1~18.04.1-b09)
# Java VM: OpenJDK 64-Bit Server VM (25.232-b09 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libnd4jcpu.so+0x52c0ae9]  nd4j::OpTracker::getInstance()+0x29
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
</pre>
</details>

#### Issue Description

Please describe our issue, along with:
- expected behavior \
  Sequential model created, trained and saved in Keras 2.3.1 + TF 2.0.0 under Python 3.7 would load into a Dl4j model
- encountered behavior \
  Exception thrown on import into Dl4j

Model was created, trained, and saved from a DL rig running Ubuntu 18.0.4 LTS with the software versions listed above, and has been reloaded and evaluated in the same environment with no issues.  

Transferring to a Windows 10 system with no CUDA GPU, attempted to load into a Java 8 application leveraging Dl4j for a runtime deployment - no bueno.  Was originally using Dl4j 1.0.0-beta5, noticed some Keras bugfixes in the changelog for beta6 so upgraded to beta6, cleaned, rebuilt and still seeing the same errors.

#### Version Information

Please indicate relevant versions, including, if relevant:

* Deeplearning4j version \
  1.0.0-beta5 and beta6
* Platform information (OS, etc) \
  Windows 10, Java 8
* CUDA version, if used \
  N/A, using CPU backend
* NVIDIA driver version, if in use \
  N/A, using CPU backend

#### Additional Information

Code snippets and stack trace for exception captured here:
https://gist.github.com/arpieb/f5b70f0f30ed9552e06fcc053bf7a548

#### Contributing

If you'd like to help us fix the issue by contributing some code, but would
like guidance or help in doing so, please mention it!

When I import model.h5, I have the two errors.

```java.lang.NoClassDefFoundError: org/deeplearning4j/nn/conf/BackpropType
        at org.deeplearning4j.nn.conf.MultiLayerConfiguration$Builder.<init>(MultiLayerConfiguration.java:475)
        at org.deeplearning4j.nn.conf.NeuralNetConfiguration$ListBuilder.<init>(NeuralNetConfiguration.java:153)
        at org.deeplearning4j.nn.conf.NeuralNetConfiguration$ListBuilder.<init>(NeuralNetConfiguration.java:159)
        at org.deeplearning4j.nn.conf.NeuralNetConfiguration$Builder.list(NeuralNetConfiguration.java:538)
        at org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerConfiguration(KerasSequentialModel.java:183)
        at org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:245)
        at org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:235)
        at org.deeplearning4j.nn.modelimport.keras.KerasModelImport.importKerasSequentialModelAndWeights(KerasModelImport.java:224)
        at org.deeplearning4j.nn.modelimport.keras.KerasModelImport.importKerasSequentialModelAndWeights(KerasModelImport.java:113)
        at com.banma.dataops.timeSeriesDataPrediction.zkWatcherSingleSet$.modelLoadHDFS(zkWatcherSingleSet.scala:97)
        at com.banma.dataops.timeSeriesDataPrediction.zkWatcherSingleSet$.com$banma$dataops$timeSeriesDataPrediction$zkWatcherSingleSet$$modelGet(zkWatcherSingleSet.scala:74)
        at com.banma.dataops.timeSeriesDataPrediction.zkWatcherSingleSet$$anon$1.nodeChanged(zkWatcherSingleSet.scala:56)
        at org.apache.curator.framework.recipes.cache.NodeCache$4.apply(NodeCache.java:326)
        at org.apache.curator.framework.recipes.cache.NodeCache$4.apply(NodeCache.java:320)
        at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:100)
        at org.apache.curator.shaded.com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:30)
        at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:92)
        at org.apache.curator.framework.recipes.cache.NodeCache.setNewData(NodeCache.java:318)
        at org.apache.curator.framework.recipes.cache.NodeCache.processBackgroundResult(NodeCache.java:285)
        at org.apache.curator.framework.recipes.cache.NodeCache.access$300(NodeCache.java:57)
        at org.apache.curator.framework.recipes.cache.NodeCache$3.processResult(NodeCache.java:123)
        at org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:883)
        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:653)
        at org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152)
        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:272)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:588)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:500)
Caused by: java.lang.ClassNotFoundException: org.deeplearning4j.nn.conf.BackpropType
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at org.apache.flink.util.ChildFirstClassLoader.loadClass(ChildFirstClassLoader.java:69)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
```
		
		
```
java.lang.NoClassDefFoundError: org/deeplearning4j/nn/conf/NeuralNetConfiguration$Builder
        at org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerConfiguration(KerasSequentialModel.java:177)
        at org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:245)
        at org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:235)
        at org.deeplearning4j.nn.modelimport.keras.KerasModelImport.importKerasSequentialModelAndWeights(KerasModelImport.java:224)
        at org.deeplearning4j.nn.modelimport.keras.KerasModelImport.importKerasSequentialModelAndWeights(KerasModelImport.java:113)
        at com.banma.dataops.timeSeriesDataPrediction.zkWatcherSingleSet$.modelLoadHDFS(zkWatcherSingleSet.scala:97)
        at com.banma.dataops.timeSeriesDataPrediction.zkWatcherSingleSet$.com$banma$dataops$timeSeriesDataPrediction$zkWatcherSingleSet$$modelGet(zkWatcherSingleSet.scala:74)
        at com.banma.dataops.timeSeriesDataPrediction.zkWatcherSingleSet$$anon$1.nodeChanged(zkWatcherSingleSet.scala:56)
        at org.apache.curator.framework.recipes.cache.NodeCache$4.apply(NodeCache.java:326)
        at org.apache.curator.framework.recipes.cache.NodeCache$4.apply(NodeCache.java:320)
        at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:100)
        at org.apache.curator.shaded.com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:30)
        at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:92)
        at org.apache.curator.framework.recipes.cache.NodeCache.setNewData(NodeCache.java:318)
        at org.apache.curator.framework.recipes.cache.NodeCache.processBackgroundResult(NodeCache.java:285)
        at org.apache.curator.framework.recipes.cache.NodeCache.access$300(NodeCache.java:57)
        at org.apache.curator.framework.recipes.cache.NodeCache$3.processResult(NodeCache.java:123)
        at org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:883)
        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:653)
        at org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152)
        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:272)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:588)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:500)
Caused by: java.lang.ClassNotFoundException: org.deeplearning4j.nn.conf.NeuralNetConfiguration$Builder
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at org.apache.flink.util.ChildFirstClassLoader.loadClass(ChildFirstClassLoader.java:69)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
```

but I am add the dependency
```
        <dependency>
            <groupId>org.deeplearning4j</groupId>
            <artifactId>deeplearning4j-nn</artifactId>
            <version>${dl4j.version}</version>
        </dependency>
```

# pom.xml

```
.....................................................
    <!--nd4j-->
    <dependencyManagement>
        <dependencies>
            <dependency>
                <groupId>org.nd4j</groupId>
                <artifactId>nd4j-native-platform</artifactId>
                <version>${nd4j.version}</version>
            </dependency>
            <dependency>
                <groupId>org.nd4j</groupId>
                <artifactId>nd4j-cuda-9.2-platform</artifactId>
                <version>${nd4j.version}</version>
            </dependency>
            <dependency>
                <groupId>org.nd4j</groupId>
                <artifactId>nd4j-cuda-10.0-platform</artifactId>
                <version>${nd4j.version}</version>
            </dependency>
            <dependency>
                <groupId>org.nd4j</groupId>
                <artifactId>nd4j-cuda-10.1-platform</artifactId>
                <version>${nd4j.version}</version>
            </dependency>
        </dependencies>
    </dependencyManagement>
.....................................................
      <dependency>
            <groupId>org.nd4j</groupId>
            <artifactId>${nd4j.backend}</artifactId>
            <version>${nd4j.version}</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.nd4j/nd4j-api -->
        <dependency>
            <groupId>org.nd4j</groupId>
            <artifactId>nd4j-api</artifactId>
            <version>${nd4j.version}</version>
        </dependency>
        <dependency>
            <groupId>org.nd4j</groupId>
            <artifactId>jackson</artifactId>
            <version>${nd4j.version}</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.nd4j/nd4j-common -->
        <dependency>
            <groupId>org.nd4j</groupId>
            <artifactId>nd4j-common</artifactId>
            <version>${nd4j.version}</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.deeplearning4j/deeplearning4j-nn -->
        <dependency>
            <groupId>org.deeplearning4j</groupId>
            <artifactId>deeplearning4j-nn</artifactId>
            <version>${dl4j.version}</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.bytedeco/openblas -->
        <dependency>
            <groupId>org.bytedeco</groupId>
            <artifactId>openblas</artifactId>
            <version>0.3.7-1.5.2</version>
        </dependency>

        <!-- Core DL4J functionality -->
        <dependency>
            <groupId>org.deeplearning4j</groupId>
            <artifactId>deeplearning4j-nlp</artifactId>
            <version>${dl4j.version}</version>
        </dependency>

        <dependency>
            <groupId>org.deeplearning4j</groupId>
            <artifactId>deeplearning4j-zoo</artifactId>
            <version>${dl4j.version}</version>
        </dependency>
        <!-- ParallelWrapper & ParallelInference live here -->
        <dependency>
            <groupId>org.deeplearning4j</groupId>
            <artifactId>deeplearning4j-parallel-wrapper</artifactId>
            <version>${dl4j.version}</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.deeplearning4j/deeplearning4j-core -->
        <dependency>
            <groupId>org.deeplearning4j</groupId>
            <artifactId>deeplearning4j-core</artifactId>
            <version>${dl4j.version}</version>
        </dependency>

        <!-- https://mvnrepository.com/artifact/org.bytedeco/hdf5-platform -->
        <dependency>
            <groupId>org.bytedeco</groupId>
            <artifactId>hdf5-platform</artifactId>
            <version>1.10.5-1.5.2</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.bytedeco.javacpp-presets/openblas -->
        <dependency>
            <groupId>org.bytedeco.javacpp-presets</groupId>
            <artifactId>openblas</artifactId>
            <version>0.3.5-1.4.4</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.bytedeco.javacpp-presets/openblas-platform -->
        <dependency>
            <groupId>org.bytedeco.javacpp-presets</groupId>
            <artifactId>openblas-platform</artifactId>
            <version>0.3.5-1.4.4</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.bytedeco/javacpp -->
        <dependency>
            <groupId>org.bytedeco</groupId>
            <artifactId>javacpp</artifactId>
            <version>1.5.2</version>
        </dependency>
.....................................................
```
#### Issue Description

We are observing a drop in our model performance after upgrading from Beta-2 to beta5 with the exact same dataset.

#### Version Information

1.0.0-beta5 

#### Issue Description

Please describe our issue, along with:
- expected behavior
- encountered behavior
INDArray[CpuBackend] has significant performance differences with double[] on similar operations, such as:
INDArray[512].sum1,000,000 times cost 14697ms
double[512] sum1,000,000 times cost 36ms

ClusterUtils.computeSquareDistancesFromNearestCluster call with 1 cluster and 10000 points 100 times cost 15812ms
euclideanDistance between10000 double[] and one double[] cost 601ms

KMeansClustering classifies 50000 points into 50 clusters costs more than two days.
Apache Commons Math KMeansPlusPlusClusterer classifies 50000 points into 50 clusters takes about 3 minutes.

All of above test case test on same dataset many times with warm code.
I have tested the same dataset on a GPU Server(NVIDIA P100 cloud server), with CUDA9 backend, KMeansClustering costs dozens hours to classifies 50000 points into 50 clusters.

#### Version Information

Please indicate relevant versions, including, if relevant:

* Deeplearning4j version: 1.0.0-beta6
* Platform information (OS, etc) Windows 10(64bit), JDK8
* CUDA version, if used: None
* NVIDIA driver version, if in use: None

#### Additional Information

Where applicable, please also provide:

* Full log or exception stack trace (ideally in a Gist: gist.github.com)
* pom.xml file or similar (also in a Gist)

#### Contributing

If you'd like to help us fix the issue by contributing some code, but would
like guidance or help in doing so, please mention it!

I think the cause of the problem is in
 org.nd4j.linalg.cpu.nativecpu.ops.NativeOpExecutioner#exec(org.nd4j.linalg.api.ops.ReduceOp)
It is so abstract and too complicate than just a for loop.

I'd like to find a best clustering algorithm on JVM for millions dataset, and I wrote a MiniBatchKMeans by kotlin(refer to sklearn MiniBatchKMeans) base on Apache Commons Math.
MiniBatchKMeans is real fast, 1823ms on classfies 50000 points(512 dimensions) into 50 clusters, and the result has very small difference with KMeansPlusPlusClusterer on convex dataset, but big difference on discrete data.
I'd like to rewrite it into a popular JVM project, but the INDArray performance is really a big problem.
#### Issue Description

I’m using BarnesHutTsne, but the result does not look like as expected. I tried MNist data and expected a typical clustering similar to https://lvdmaaten.github.io/tsne/examples/mnist_tsne.jpg.

The output of BarnesHutTsne (and Tsne) looks like a donut with randomly arranged points that is really untypical.

#### Version Information

I'm using Deeplearning4j version 1.0.0-beta4.

Add some fluent syntax construction to implement linear solve (like numpy.linalg.solve) in scala.
Analogue from Breeze: a \ b.  But this operator is used in nd4s.

Methods:
as inputs int8_t*, unsigned char*, void* calculate buffer length
UTF8 <-> UTF16
UTF8 <-> UTF32 
UTF16 <-> UTF32

Hi folks,

Windows Defender antivirus shows a trojan in one of the transitive dependencies from deeplearning4j-core/examples package/project and to be more specific in opencv for MacOS.

By default when you compile the project all OS dependencies are downloaded from the central Maven repository, so everyone has this trojan in their machines.

In case this is not a false alarm of Windows Defender, you should consider notifying the users of this framework and address this security issue.

Also, maybe a security check step before publication to mvn repository should take place.

Details :
- dl4j version used 
```
<dependency>
            <groupId>org.deeplearning4j</groupId>
            <artifactId>deeplearning4j-core</artifactId>
            <version>1.0.0-beta4</version>
  </dependency>
```
- from where it comes the trojan 
`org\bytedeco\opencv\4.0.1-1.5\opencv-4.0.1-1.5-macosx-x86_64.jar`
- trojan name - **Trojan:MacOS/Lightaidra**
- Log from Windows Defender 
file:C:\Users\user\.m2\repository\org\bytedeco\opencv\4.0.1-1.5\opencv-4.0.1-1.5-macosx-x86_64.jar->org/bytedeco/opencv/macosx-x86_64/libopencv_dnn.4.0.dylib

Based on Microsoft this trojan is used for DDoS attacks and to establish IRC communication with "villain users".

I had to remove my personal Java projects from this framework.

**Note** I did not check any newer version, but there is a high chance of this being present even on newer versions.