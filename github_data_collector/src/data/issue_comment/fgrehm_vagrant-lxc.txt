My project requires multiple container environments, and each `lxc` needs to be able to ping with each other. I use settings similar to the following
```
config.vm.define :"lxc1" do |node_conf|
    node_conf.vm.network "private_network", ip: "192.168.45.2",lxc__bridge_name: "project"
    ...
end

config.vm.define :"lex2" do |node_conf|
    node_conf.vm.network "private_network", ip: "192.168.42.3",lxc__bridge_name: "project"
    ...
end
```
Host can ping each container,
```
root@lxc:~/lxc/vagrantlxcTest# ping 192.168.42.3
PING 192.168.42.3 (192.168.42.3) 56(84) bytes of data.
64 bytes from 192.168.42.3: icmp_seq=1 ttl=64 time=0.059 ms
64 bytes from 192.168.42.3: icmp_seq=2 ttl=64 time=0.037 ms
``` 
but I can't ping from one container to another, 
```
vagrant@lxc1:~$ ping 192.168.42.3
PING 192.168.42.3 (192.168.42.3) 56(84) bytes of data.
```
Do I need to add other settings to do that?
> I did not do any settings for the `project` bridge, which is directly used.

> I use default `lxcbr0` and match the appropriate IP allows the containers to ping each other

I would very appreciate it if you guys can give me some advice or solutions.
Fixes #485. As per https://apidock.com/ruby/IO/open/class `File::RDWR` seems to lack `File::TRUNC`. Hence the call from `prune_customizations` did nothing since it just overwrite the start of the file (which is presumably already untouched), but left the `VAGRANT-BEGIN/VAGRANT-END` block that followed completely untouched.

By adding `File::TRUNC`, `prune_customizations` will cause those tailing blocks to be removed as intended.
Everything seems to work great except for this one troublesome bug that shows up at the bottom of the image `config` file:
```
$ cat ~/.local/share/lxc/stretch_default_1571630615861_44975/config
# Template used to create this container: /usr/share/rubygems-integration/all/gems/vagrant-lxc-1.4.3/scripts/lxc-template
# Parameters passed to the template: --tarball /home/boltronics/.vagrant.d/boxes/sitepoint-VAGRANTSLASH-debian-stretch-amd64/1.0.2/lxc/rootfs.tar.gz --config /home/boltronics/.vagrant.d/boxes/sitepoint-VAGRANTSLASH-debian-stretch-amd64/1.0.2/lxc/lxc-config
# Template script checksum (SHA-1): eae122a2d6cd572c26668257efa7963c2258186e
# For additional config options, please look at lxc.container.conf(5)

# Uncomment the following line to support nesting containers:
#lxc.include = /usr/share/lxc/config/nesting.conf
# (Be aware this has security implications)


##############################################
# Container specific configuration (automatically set)
lxc.include = /etc/lxc/default.conf
lxc.idmap = u 0 100000 65536
lxc.idmap = g 0 100000 65536
lxc.mount.auto = proc:mixed sys:ro cgroup:mixed
lxc.rootfs.path = dir:/home/boltronics/.local/share/lxc/stretch_default_1571630615861_44975/rootfs
lxc.uts.name = stretch_default_1571630615861_44975

##############################################
# Network configuration (automatically set)

##############################################
# vagrant-lxc base box specific configuration
# Default mount entries
lxc.mount.auto = cgroup:mixed proc:mixed sys:mixed
lxc.mount.entry = /sys/fs/fuse/connections sys/fs/fuse/connections none bind,optional 0 0

# Default console settings
lxc.tty.max = 4
lxc.pty.max = 1024

# Default capabilities
lxc.cap.drop = sys_module mac_admin mac_override sys_time sys_rawio

# When using LXC with apparmor, the container will be confined by default.
# If you wish for it to instead run unconfined, copy the following line
# (uncommented) to the container's configuration file.
#lxc.apparmor.profile = unconfined

# To support container nesting on an Ubuntu host while retaining most of
# apparmor's added security, use the following two lines instead.
#lxc.apparmor.profile = lxc-container-default-with-nesting
#lxc.hook.mount = /usr/share/lxc/hooks/mountcgroups

# If you wish to allow mounting block filesystems, then use the following
# line instead, and make sure to grant access to the block device and/or loop
# devices below in lxc.cgroup.devices.allow.
#lxc.apparmor.profile = lxc-container-default-with-mounting

##############################################
# vagrant-lxc container specific configuration
# VAGRANT-BEGIN
lxc.uts.name=stretch_default_1571630615861_44975
lxc.mount.entry=/sys/fs/pstore sys/fs/pstore none bind,optional 0 0
lxc.mount.entry=tmpfs tmp tmpfs nodev,nosuid,size=2G 0 0
lxc.mount.entry=/home/boltronics/Development/github.com/sitepoint/vmconfig/images/stretch vagrant none bind,create=dir 0 0
# VAGRANT-END
# VAGRANT-BEGIN
lxc.uts.name=stretch_default_1571630615861_44975
lxc.mount.entry=/sys/fs/pstore sys/fs/pstore none bind,optional 0 0
lxc.mount.entry=tmpfs tmp tmpfs nodev,nosuid,size=2G 0 0
lxc.mount.entry=/home/boltronics/Development/github.com/sitepoint/vmconfig/images/stretch vagrant none bind,create=dir 0 0
# VAGRANT-END
# VAGRANT-BEGIN
lxc.uts.name=stretch_default_1571630615861_44975
lxc.mount.entry=/sys/fs/pstore sys/fs/pstore none bind,optional 0 0
lxc.mount.entry=tmpfs tmp tmpfs nodev,nosuid,size=2G 0 0
lxc.mount.entry=/home/boltronics/Development/github.com/sitepoint/vmconfig/images/stretch vagrant none bind,create=dir 0 0
# VAGRANT-END
```
Every time the guest is stopped and started, it adds a new VAGRANT-BEGIN/VAGRANT-END block entry. For a minimal config this doesn't matter too much, but some options do not like to be duplicated and cause the machine to fail to boot the second time (without manually editing the config file to remove duplicate blocks).

This is on Debian 10 (buster), amd64, using the Debian-provided vagrant and vagrant-lxc packages (however the vagrant-lxc package is currently up to date with the latest stable release - 1.4.3).

The container was created just last week using https://github.com/sitepoint/vagrant-lxc-base-boxes 

```
$ apt-cache policy vagrant vagrant-lxc | grep -B 1 -A 1 Installed
vagrant:
  Installed: 2.2.3+dfsg-1
  Candidate: 2.2.3+dfsg-1
--
vagrant-lxc:
  Installed: 1.4.3-1
  Candidate: 1.4.3-1
$ lxc-checkconfig 
Kernel configuration not found at /proc/config.gz; searching...
Kernel configuration found at /boot/config-4.19.0-6-amd64
--- Namespaces ---
Namespaces: enabled
Utsname namespace: enabled
Ipc namespace: enabled
Pid namespace: enabled
User namespace: enabled
Network namespace: enabled

--- Control groups ---
Cgroups: enabled

Cgroup v1 mount points: 
/sys/fs/cgroup/systemd
/sys/fs/cgroup/cpuset
/sys/fs/cgroup/pids
/sys/fs/cgroup/devices
/sys/fs/cgroup/cpu,cpuacct
/sys/fs/cgroup/perf_event
/sys/fs/cgroup/net_cls,net_prio
/sys/fs/cgroup/rdma
/sys/fs/cgroup/blkio
/sys/fs/cgroup/freezer
/sys/fs/cgroup/memory

Cgroup v2 mount points: 
/sys/fs/cgroup/unified

Cgroup v1 clone_children flag: enabled
Cgroup device: enabled
Cgroup sched: enabled
Cgroup cpu account: enabled
Cgroup memory controller: enabled
Cgroup cpuset: enabled

--- Misc ---
Veth pair device: enabled, loaded
Macvlan: enabled, not loaded
Vlan: enabled, loaded
Bridges: enabled, loaded
Advanced netfilter: enabled, loaded
CONFIG_NF_NAT_IPV4: enabled, loaded
CONFIG_NF_NAT_IPV6: enabled, not loaded
CONFIG_IP_NF_TARGET_MASQUERADE: enabled, not loaded
CONFIG_IP6_NF_TARGET_MASQUERADE: enabled, not loaded
CONFIG_NETFILTER_XT_TARGET_CHECKSUM: enabled, loaded
CONFIG_NETFILTER_XT_MATCH_COMMENT: enabled, not loaded
FUSE (for use with lxcfs): enabled, loaded

--- Checkpoint/Restore ---
checkpoint restore: enabled
CONFIG_FHANDLE: enabled
CONFIG_EVENTFD: enabled
CONFIG_EPOLL: enabled
CONFIG_UNIX_DIAG: enabled
CONFIG_INET_DIAG: enabled
CONFIG_PACKET_DIAG: enabled
CONFIG_NETLINK_DIAG: enabled
File capabilities: 

Note : Before booting a new kernel, you can check its configuration
usage : CONFIG=/path/to/config /usr/bin/lxc-checkconfig
$ cat ~/.config/lxc/default.conf 
lxc.include = /etc/lxc/default.conf

lxc.idmap = u 0 100000 65536
lxc.idmap = g 0 100000 65536
lxc.mount.auto = proc:mixed sys:ro cgroup:mixed
$ cat /etc/lxc/default.conf
#lxc.apparmor.profile = generated
lxc.apparmor.allow_nesting = 1
lxc.net.0.type = veth
lxc.net.0.flags = up
lxc.net.0.link = lxcbr0
$ cat Vagrantfile
VAGRANTFILE_API_VERSION = "2"

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
  config.vm.box = "sitepoint/debian-stretch-amd64"
  config.vm.box_version = "1.0.2"
  config.vm.box_download_checksum_type = "sha256"
  config.vm.box_download_checksum = "c11b28024050ac94457e8b5e746365635a485e08d6c856cf7f2115363807b02d"

  config.vm.provider :lxc do |lxc|
    lxc.privileged = false
  end
end
```

Note that `lxc.uts.name` is already set to the correct value in the `# Container specific configuration (automatically set)` block, but seems to be set again unconditionally. While probably not great, this does not seem related to the cause.
I'm on Arch Linux and I'm trying to set up a Vagrant box with a private network. This, however, fails when I try to `vagrant up`. I've done some digging, and the `pipework` script popped up. Most notably, [this check](https://github.com/fgrehm/vagrant-lxc/blob/d4e5122c6c846ea77bfbccee0b825b226dfd02e4/scripts/pipework#L157) prevents `vagrant up` from setting up the private network. On systems with LXC >= 3.1.0, this check yields `2`.

As per https://github.com/lxc/lxc/issues/2782, I understood that there has been a split in information storage since LXC 3.1.0. I could resolve my own issues by writing this patch:

```diff
--- pipework	2019-10-10 14:08:16.545642357 +0200
+++ pipework.patched	2019-10-10 14:08:22.309010624 +0200
@@ -154,7 +154,7 @@
 }
 
 # Try to find a cgroup matching exactly the provided name.
-N=$(find "$CGROUPMNT" -name "$GUESTNAME" | wc -l)
+N=$(find "$CGROUPMNT/lxc.payload" -name "$GUESTNAME" | wc -l)
 case "$N" in
   0)
     # If we didn't find anything, try to lookup the container with Docker.
@@ -237,7 +237,7 @@
 if [ "$DOCKERPID" ]; then
   NSPID=$DOCKERPID
 else
-  NSPID=$(head -n 1 "$(find "$CGROUPMNT" -name "$GUESTNAME" | head -n 1)/tasks")
+  NSPID=$(head -n 1 "$(find "$CGROUPMNT/lxc.payload" -name "$GUESTNAME" | head -n 1)/tasks")
   [ "$NSPID" ] || {
     # it is an alternative way to get the pid
     NSPID=$(lxc-info -n  "$GUESTNAME" | grep PID | grep -Eo '[0-9]+')

```

I intentionally did not send in a PR because I'm certain this patch will break the `vagrant-lxc` plugin for systems using LXC < 3.1.0.

Debug info
---

```
 ○ uname -a
Linux macbook 5.2.18.a-2-hardened #1 SMP PREEMPT Wed Oct 2 08:51:52 CEST 2019 x86_64 GNU/Linux
```

```
 ○ pacman -Qi lxc
Name            : lxc
Version         : 1:3.2.1-1
...
```

```
 ○ cat Vagrantfile
Vagrant.configure("2") do |config|
  config.vm.box = "vagrant-lxc-centos-7-amd64"
  config.vm.synced_folder ".", "/vagrant", disabled: true
  config.vm.provider :lxc do |lxc|
    lxc.privileged = true
    # Same effect as 'customize ["modifyvm", :id, "--memory", "1024"]' for VirtualBox
    lxc.customize 'cgroup.memory.limit_in_bytes', '2048M'
    lxc.customize 'cgroup.devices.allow', 'a'
    lxc.customize 'mount.auto', 'proc:rw sys:rw'
    lxc.customize 'cap.drop', nil
    lxc.customize 'apparmor.profile', 'unconfined'
    lxc.backingstore_option '--fssize', '20G'
  end

  N = 1

  (1..N).each do |i|
    config.vm.define "swarm-node#{i}" do |node|
      node.vm.hostname = "swarm-node#{i}"
      node.vm.network "private_network", ip: "192.168.33.1#{i}", lxc__bridge_name: 'vlxcbr1'
      if i == N
        node.vm.provision "ansible" do |ansible|
          ansible.playbook = "addkey.yml"
          ansible.limit = "all"
          ansible.extra_vars = {
            myhosts: "all"
          }
          ansible.groups = {
            "swarm" => ["swarm-node[1:#{N}]"],
          }
        end
      end
    end
  end
end
```

```
 ○ vagrant up
Bringing machine 'swarm-node1' up with 'lxc' provider...
==> swarm-node1: Importing base box 'vagrant-lxc-centos-7-amd64'...
==> swarm-node1: Starting container...
==> swarm-node1: Waiting for machine to boot. This may take a few minutes...
    swarm-node1: SSH address: 10.0.3.15:22
    swarm-node1: SSH username: vagrant
    swarm-node1: SSH auth method: private key
    swarm-node1:
    swarm-node1: Vagrant insecure key detected. Vagrant will automatically replace
    swarm-node1: this with a newly generated keypair for better security.
    swarm-node1:
    swarm-node1: Inserting generated public key within guest...
    swarm-node1: Removing insecure key from the guest if it's present...
    swarm-node1: Key inserted! Disconnecting and reconnecting using new SSH key...
==> swarm-node1: Machine booted and ready!
==> swarm-node1: Setting up private networks...
There was an error executing ["sudo", "/usr/bin/env", "/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/scripts/pipework", "vlxcbr1", "centos-7_swarm-node1_1570708762000_4617", "192.168.33.11/24"]

For more information on the failure, enable detailed logging by setting
the environment variable VAGRANT_LOG to DEBUG.
```

```
 ○ VAGRANT_LOG=DEBUG vagrant up
 INFO global: Vagrant version: 2.2.5
 INFO global: Ruby version: 2.6.5
 INFO global: RubyGems version: 3.0.6
 INFO global: VAGRANT_LOG="DEBUG"
 INFO global: VAGRANT_INSTALLER_ENV="1"
 INFO global: VAGRANT_INSTALLER_VERSION="2"
 INFO global: VAGRANT_DETECTED_OS="archlinux"
 INFO global: VAGRANT_EXECUTABLE="/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/bin/vagrant"
 INFO global: VAGRANT_INSTALLER_EMBEDDED_DIR="/opt/vagrant/embedded"
 WARN global: resolv replacement has not been enabled!
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/fedora/plugin.rb
 INFO manager: Registered plugin: Fedora guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/atomic/plugin.rb
 INFO manager: Registered plugin: Atomic Host guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/freebsd/plugin.rb
 INFO manager: Registered plugin: FreeBSD guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/tinycore/plugin.rb
 INFO manager: Registered plugin: TinyCore Linux guest.
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/smartos/plugin.rb
 INFO manager: Registered plugin: SmartOS guest.
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/gentoo/plugin.rb
 INFO manager: Registered plugin: Gentoo guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/dragonflybsd/plugin.rb
 INFO manager: Registered plugin: DragonFly BSD guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/coreos/plugin.rb
 INFO manager: Registered plugin: CoreOS guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/darwin/plugin.rb
 INFO manager: Registered plugin: Darwin guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/trisquel/plugin.rb
 INFO manager: Registered plugin: Trisquel guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/pld/plugin.rb
 INFO manager: Registered plugin: PLD Linux guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/nixos/plugin.rb
 INFO manager: Registered plugin: NixOS guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/amazon/plugin.rb
 INFO manager: Registered plugin: Amazon Linux guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/linux/plugin.rb
 INFO manager: Registered plugin: Linux guest.
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/slackware/plugin.rb
 INFO manager: Registered plugin: Slackware guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/debian/plugin.rb
 INFO manager: Registered plugin: Debian guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/bsd/plugin.rb
 INFO manager: Registered plugin: BSD-based guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/alt/plugin.rb
 INFO manager: Registered plugin: ALT Platform guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/suse/plugin.rb
 INFO manager: Registered plugin: SUSE guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/solaris11/plugin.rb
 INFO manager: Registered plugin: Solaris 11 guest.
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/openbsd/plugin.rb
 INFO manager: Registered plugin: OpenBSD guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/redhat/plugin.rb
 INFO manager: Registered plugin: Red Hat Enterprise Linux guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/elementary/plugin.rb
 INFO manager: Registered plugin: Elementary guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/esxi/plugin.rb
 INFO manager: Registered plugin: ESXi guest.
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/mint/plugin.rb
 INFO manager: Registered plugin: Mint guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/netbsd/plugin.rb
 INFO manager: Registered plugin: NetBSD guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/photon/plugin.rb
 INFO manager: Registered plugin: VMware Photon guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/windows/plugin.rb
 INFO manager: Registered plugin: Windows guest.
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/arch/plugin.rb
 INFO manager: Registered plugin: Arch guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/kali/plugin.rb
 INFO manager: Registered plugin: Kali guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/ubuntu/plugin.rb
 INFO manager: Registered plugin: Ubuntu guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/solaris/plugin.rb
 INFO manager: Registered plugin: Solaris guest.
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/haiku/plugin.rb
 INFO manager: Registered plugin: Haiku guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/omnios/plugin.rb
 INFO manager: Registered plugin: OmniOS guest.
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/guests/funtoo/plugin.rb
 INFO manager: Registered plugin: Funtoo guest
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/winrm/plugin.rb
 INFO manager: Registered plugin: winrm command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/provision/plugin.rb
 INFO manager: Registered plugin: provision command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/upload/plugin.rb
 INFO manager: Registered plugin: upload command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/box/plugin.rb
 INFO manager: Registered plugin: box command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/global-status/plugin.rb
 INFO manager: Registered plugin: global-status command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/port/plugin.rb
 INFO manager: Registered plugin: port command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/destroy/plugin.rb
 INFO manager: Registered plugin: destroy command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/up/plugin.rb
 INFO manager: Registered plugin: up command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/cap/plugin.rb
 INFO manager: Registered plugin: cap command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/powershell/plugin.rb
 INFO manager: Registered plugin: powershell command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/resume/plugin.rb
 INFO manager: Registered plugin: resume command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/status/plugin.rb
 INFO manager: Registered plugin: status command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/halt/plugin.rb
 INFO manager: Registered plugin: halt command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/cloud/plugin.rb
 INFO manager: Registered plugin: vagrant-cloud
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/winrm_config/plugin.rb
 INFO manager: Registered plugin: winrm-config command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/reload/plugin.rb
 INFO manager: Registered plugin: reload command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/snapshot/plugin.rb
 INFO manager: Registered plugin: snapshot command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/version/plugin.rb
 INFO manager: Registered plugin: version command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/init/plugin.rb
 INFO manager: Registered plugin: init command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/plugin/plugin.rb
 INFO manager: Registered plugin: plugin command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/provider/plugin.rb
 INFO manager: Registered plugin: provider command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/login/plugin.rb
 INFO manager: Registered plugin: vagrant-login
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/ssh_config/plugin.rb
 INFO manager: Registered plugin: ssh-config command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/list-commands/plugin.rb
 INFO manager: Registered plugin: list-commands command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/suspend/plugin.rb
 INFO manager: Registered plugin: suspend command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/ssh/plugin.rb
 INFO manager: Registered plugin: ssh command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/rdp/plugin.rb
 INFO manager: Registered plugin: rdp command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/validate/plugin.rb
 INFO manager: Registered plugin: validate command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/help/plugin.rb
 INFO manager: Registered plugin: help command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/push/plugin.rb
 INFO manager: Registered plugin: push command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/commands/package/plugin.rb
 INFO manager: Registered plugin: package command
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/hosts/freebsd/plugin.rb
 INFO manager: Registered plugin: FreeBSD host
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/hosts/gentoo/plugin.rb
 INFO manager: Registered plugin: Gentoo host
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/hosts/darwin/plugin.rb
 INFO manager: Registered plugin: Mac OS X host
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/hosts/linux/plugin.rb
 INFO manager: Registered plugin: Linux host
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/hosts/slackware/plugin.rb
 INFO manager: Registered plugin: Slackware host
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/hosts/bsd/plugin.rb
 INFO manager: Registered plugin: BSD host
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/hosts/alt/plugin.rb
 INFO manager: Registered plugin: ALT Platform host
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/hosts/suse/plugin.rb
 INFO manager: Registered plugin: SUSE host
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/hosts/redhat/plugin.rb
 INFO manager: Registered plugin: Red Hat Enterprise Linux host
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/hosts/null/plugin.rb
 INFO manager: Registered plugin: null host
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/hosts/void/plugin.rb
 INFO manager: Registered plugin: Void host
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/hosts/windows/plugin.rb
 INFO manager: Registered plugin: Windows host
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/hosts/arch/plugin.rb
 INFO manager: Registered plugin: Arch host
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/kernel_v1/plugin.rb
 INFO manager: Registered plugin: kernel
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/pushes/heroku/plugin.rb
 INFO manager: Registered plugin: heroku
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/pushes/noop/plugin.rb
 INFO manager: Registered plugin: noop
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/pushes/local-exec/plugin.rb
 INFO manager: Registered plugin: local-exec
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/pushes/ftp/plugin.rb
 INFO manager: Registered plugin: ftp
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/pushes/atlas/plugin.rb
 INFO manager: Registered plugin: atlas
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/synced_folders/smb/plugin.rb
 INFO manager: Registered plugin: SMB synced folders
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/synced_folders/rsync/plugin.rb
 INFO manager: Registered plugin: RSync synced folders
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/synced_folders/nfs/plugin.rb
 INFO manager: Registered plugin: NFS synced folders
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/kernel_v2/plugin.rb
 INFO manager: Registered plugin: kernel
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/provisioners/file/plugin.rb
 INFO manager: Registered plugin: file
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/provisioners/cfengine/plugin.rb
 INFO manager: Registered plugin: CFEngine Provisioner
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/provisioners/puppet/plugin.rb
 INFO manager: Registered plugin: puppet
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/provisioners/chef/plugin.rb
 INFO manager: Registered plugin: chef
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/provisioners/salt/plugin.rb
 INFO manager: Registered plugin: salt
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/provisioners/shell/plugin.rb
 INFO manager: Registered plugin: shell
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/provisioners/ansible/plugin.rb
 INFO manager: Registered plugin: ansible
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/provisioners/docker/plugin.rb
 INFO manager: Registered plugin: docker
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/providers/virtualbox/plugin.rb
 INFO manager: Registered plugin: VirtualBox provider
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/providers/hyperv/plugin.rb
 INFO manager: Registered plugin: Hyper-V provider
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/providers/docker/plugin.rb
 INFO manager: Registered plugin: docker-provider
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/communicators/winssh/plugin.rb
 INFO manager: Registered plugin: windows ssh communicator
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/communicators/winrm/plugin.rb
 INFO manager: Registered plugin: winrm communicator
DEBUG global: Loading core plugin: /opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/communicators/ssh/plugin.rb
 INFO manager: Registered plugin: ssh communicator
 INFO vagrant: `vagrant` invoked: ["up"]
DEBUG vagrant: Creating Vagrant environment
 INFO environment: Environment initialized (#<Vagrant::Environment:0x00000c6a345831a8>)
 INFO environment:   - cwd: /home/niek/vagrant/lxc/centos-7
 INFO environment: Home path: /home/niek/.vagrant.d
DEBUG environment: Effective local data path: /home/niek/vagrant/lxc/centos-7/.vagrant
 INFO environment: Local data path: /home/niek/vagrant/lxc/centos-7/.vagrant
DEBUG environment: Creating: /home/niek/vagrant/lxc/centos-7/.vagrant
DEBUG manager: Enabling localized plugins
 INFO manager: Plugins:
DEBUG bundler: Current generated plugin dependency list: []
DEBUG bundler: Generating new builtin set instance.
DEBUG bundler: Generating new plugin set instance. Skip gems - []
DEBUG bundler: Activating solution set: []
 INFO manager: Loading plugins...
DEBUG manager: Enabling globalized plugins
 INFO manager: Plugins:
 INFO manager:   - vagrant-lxc = [installed: 1.4.3 constraint: > 0]
DEBUG bundler: Current generated plugin dependency list: [<Gem::Dependency type=:runtime name="vagrant-lxc" requirements="= 1.4.3">]
DEBUG bundler: Generating new builtin set instance.
DEBUG bundler: Generating new plugin set instance. Skip gems - []
DEBUG bundler: Activating solution set: ["vagrant-lxc-1.4.3"]
DEBUG bundler: Activating gem vagrant-lxc-1.4.3
 INFO manager: Loading plugins...
 INFO manager: Loading plugin `vagrant-lxc` with default require: `vagrant-lxc`
 INFO manager: Registered plugin: vagrant-lxc
DEBUG manager: Successfully loaded plugin `vagrant-lxc`.
 INFO loader: Set :root = ["#<Pathname:/home/niek/vagrant/lxc/centos-7/Vagrantfile>"]
DEBUG loader: Populating proc cache for #<Pathname:/home/niek/vagrant/lxc/centos-7/Vagrantfile>
DEBUG loader: Load procs for pathname: /home/niek/vagrant/lxc/centos-7/Vagrantfile
 INFO loader: Loading configuration in order: [:home, :root]
DEBUG loader: Loading from: root (evaluating)
DEBUG loader: Configuration loaded successfully, finalizing and returning
DEBUG push: finalizing
 INFO base: VBoxManage path: VBoxManage
 INFO subprocess: Starting process: ["/usr/bin/VBoxManage", "--version"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: 6.0.12r132055
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
DEBUG meta: Finding driver for VirtualBox version: 6.0.12
 INFO meta: Using VirtualBox driver: VagrantPlugins::ProviderVirtualBox::Driver::Version_6_0
 INFO base: VBoxManage path: VBoxManage
DEBUG environment: Initial usable provider list: [[6, :virtualbox], [7, :lxc]]
DEBUG environment: Priority sorted usable provider list: [:lxc, :virtualbox]
DEBUG environment: Preferred provider list: []
DEBUG environment: Using provider `lxc` detected in configuration and usable.
 INFO base: VBoxManage path: VBoxManage
DEBUG meta: Finding driver for VirtualBox version: 6.0.12
 INFO meta: Using VirtualBox driver: VagrantPlugins::ProviderVirtualBox::Driver::Version_6_0
 INFO base: VBoxManage path: VBoxManage
DEBUG environment: Initial usable provider list: [[6, :virtualbox], [7, :lxc]]
DEBUG environment: Priority sorted usable provider list: [:lxc, :virtualbox]
DEBUG environment: Preferred provider list: []
DEBUG environment: Using provider `lxc` detected in configuration and usable.
 INFO loader: Set "6825142240800_machine_swarm-node1" = ["[\"2\", #<Proc:0x00000c6a348ab060@/home/niek/vagrant/lxc/centos-7/Vagrantfile:19>]"]
DEBUG loader: Populating proc cache for ["2", #<Proc:0x00000c6a348ab060@/home/niek/vagrant/lxc/centos-7/Vagrantfile:19>]
 INFO loader: Loading configuration in order: [:home, :root, "6825142240800_machine_swarm-node1"]
DEBUG loader: Loading from: root (cache)
DEBUG loader: Loading from: 6825142240800_machine_swarm-node1 (evaluating)
DEBUG provisioner: Provisioner defined: 
DEBUG loader: Configuration loaded successfully, finalizing and returning
DEBUG push: finalizing
 INFO box_collection: Box found: vagrant-lxc-centos-7-amd64 (lxc)
 INFO environment: Running hook: environment_plugins_loaded
 INFO runner: Preparing hooks for middleware sequence...
 INFO runner: 1 hooks defined.
 INFO runner: Running action: environment_plugins_loaded #<Vagrant::Action::Builder:0x00000c6a351a2510>
 INFO environment: Running hook: environment_load
 INFO runner: Preparing hooks for middleware sequence...
 INFO runner: 1 hooks defined.
 INFO runner: Running action: environment_load #<Vagrant::Action::Builder:0x00000c6a3522d160>
DEBUG checkpoint_client: starting plugin check
 INFO cli: CLI: [] "up" []
DEBUG checkpoint_client: plugin check complete
DEBUG cli: Invoking command class: VagrantPlugins::CommandUp::Command []
DEBUG checkpoint_client: no information received from checkpoint
DEBUG command: 'Up' each target VM...
DEBUG command: Getting target VMs for command. Arguments:
DEBUG command:  -- names: ["swarm-node1"]
DEBUG command:  -- options: {:provider=>nil}
DEBUG command: Finding machine that match name: swarm-node1
 INFO loader: Set "6825142240800_machine_swarm-node1" = ["[\"2\", #<Proc:0x00000c6a348ab060@/home/niek/vagrant/lxc/centos-7/Vagrantfile:19>]"]
 INFO loader: Loading configuration in order: [:home, :root, "6825142240800_machine_swarm-node1"]
DEBUG loader: Loading from: root (cache)
DEBUG loader: Loading from: 6825142240800_machine_swarm-node1 (cache)
DEBUG loader: Configuration loaded successfully, finalizing and returning
DEBUG push: finalizing
 WARN vagrantfile: Failed to locate vagrant-lxc-centos-7-amd64 with version 
 WARN vagrantfile: Performing lookup with inital values vagrant-lxc-centos-7-amd64 with version 
 INFO base: VBoxManage path: VBoxManage
DEBUG meta: Finding driver for VirtualBox version: 6.0.12
 INFO meta: Using VirtualBox driver: VagrantPlugins::ProviderVirtualBox::Driver::Version_6_0
 INFO base: VBoxManage path: VBoxManage
DEBUG environment: Initial usable provider list: [[6, :virtualbox], [7, :lxc]]
DEBUG environment: Priority sorted usable provider list: [:lxc, :virtualbox]
DEBUG environment: Preferred provider list: []
DEBUG environment: Using provider `lxc` detected in configuration and usable.
 INFO environment: Getting machine: swarm-node1 (lxc)
 INFO environment: Uncached load of machine.
 INFO loader: Set "6825142240800_machine_swarm-node1" = ["[\"2\", #<Proc:0x00000c6a348ab060@/home/niek/vagrant/lxc/centos-7/Vagrantfile:19>]"]
 INFO loader: Loading configuration in order: [:home, :root, "6825142240800_machine_swarm-node1"]
DEBUG loader: Loading from: root (cache)
DEBUG loader: Loading from: 6825142240800_machine_swarm-node1 (cache)
DEBUG loader: Configuration loaded successfully, finalizing and returning
DEBUG push: finalizing
 INFO box_collection: Box found: vagrant-lxc-centos-7-amd64 (lxc)
 INFO machine: Initializing machine: swarm-node1
 INFO machine:   - Provider: Vagrant::LXC::Provider
 INFO machine:   - Box: #<Vagrant::Box:0x00000c6a350e3598>
 INFO machine:   - Data dir: /home/niek/vagrant/lxc/centos-7/.vagrant/machines/swarm-node1/lxc
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "which", "lxc-create"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: /usr/bin/lxc-create
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
DEBUG lxc: Instantiating the container for: nil
 INFO machine: New machine ID: nil
DEBUG lxc: Instantiating the container for: nil
 INFO interface: Machine: metadata ["provider", :lxc, {:target=>:"swarm-node1"}]
 INFO command: With machine: swarm-node1 (#<Vagrant::LXC::Provider:0x00000c6a352a8c20 @logger=#<Log4r::Logger:0x00000c6a352a8bd0 @fullname="vagrant::provider::lxc", @outputters=[], @additive=true, @name="lxc", @path="vagrant::provider", @parent=#<VagrantLogger:0x00000c6a33f4da90 @fullname="vagrant", @outputters=[#<Log4r::StderrOutputter:0x00000c6a339950f8 @mon_mutex=#<Thread::Mutex:0x00000c6a33994e00>, @mon_mutex_owner_object_id=6825135876220, @mon_owner=nil, @mon_count=0, @name="stderr", @level=0, @formatter=#<Vagrant::Util::LoggingFormatter:0x00000c6a34321dd8 @formatter=#<Log4r::BasicFormatter:0x00000c6a33a7e370 @depth=7>>, @out=#<IO:<STDERR>>>], @additive=true, @name="vagrant", @path="", @parent=#<Log4r::RootLogger:0x00000c6a33f4d978 @level=0, @outputters=[]>, @level=1, @trace=false>, @level=1, @trace=false>, @machine=#<Vagrant::Machine: swarm-node1 (Vagrant::LXC::Provider)>, @driver=#<Vagrant::LXC::Driver:0x00000c6a3556a828 @container_name=nil, @sudo_wrapper=#<Vagrant::LXC::SudoWrapper:0x00000c6a3556a7b0 @wrapper_path=nil, @privileged=true, @logger=#<Log4r::Logger:0x00000c6a3556a6c0 @fullname="vagrant::lxc::sudo_wrapper", @outputters=[], @additive=true, @name="sudo_wrapper", @path="vagrant::lxc", @parent=#<VagrantLogger:0x00000c6a33f4da90 @fullname="vagrant", @outputters=[#<Log4r::StderrOutputter:0x00000c6a339950f8 @mon_mutex=#<Thread::Mutex:0x00000c6a33994e00>, @mon_mutex_owner_object_id=6825135876220, @mon_owner=nil, @mon_count=0, @name="stderr", @level=0, @formatter=#<Vagrant::Util::LoggingFormatter:0x00000c6a34321dd8 @formatter=#<Log4r::BasicFormatter:0x00000c6a33a7e370 @depth=7>>, @out=#<IO:<STDERR>>>], @additive=true, @name="vagrant", @path="", @parent=#<Log4r::RootLogger:0x00000c6a33f4d978 @level=0, @outputters=[]>, @level=1, @trace=false>, @level=1, @trace=false>>, @cli=#<Vagrant::LXC::Driver::CLI:0x00000c6a355990b0 @sudo_wrapper=#<Vagrant::LXC::SudoWrapper:0x00000c6a3556a7b0 @wrapper_path=nil, @privileged=true, @logger=#<Log4r::Logger:0x00000c6a3556a6c0 @fullname="vagrant::lxc::sudo_wrapper", @outputters=[], @additive=true, @name="sudo_wrapper", @path="vagrant::lxc", @parent=#<VagrantLogger:0x00000c6a33f4da90 @fullname="vagrant", @outputters=[#<Log4r::StderrOutputter:0x00000c6a339950f8 @mon_mutex=#<Thread::Mutex:0x00000c6a33994e00>, @mon_mutex_owner_object_id=6825135876220, @mon_owner=nil, @mon_count=0, @name="stderr", @level=0, @formatter=#<Vagrant::Util::LoggingFormatter:0x00000c6a34321dd8 @formatter=#<Log4r::BasicFormatter:0x00000c6a33a7e370 @depth=7>>, @out=#<IO:<STDERR>>>], @additive=true, @name="vagrant", @path="", @parent=#<Log4r::RootLogger:0x00000c6a33f4d978 @level=0, @outputters=[]>, @level=1, @trace=false>, @level=1, @trace=false>>, @name=nil, @logger=#<Log4r::Logger:0x00000c6a35599060 @fullname="vagrant::provider::lxc::container::cli", @outputters=[], @additive=true, @name="cli", @path="vagrant::provider::lxc::container", @parent=#<Log4r::Logger:0x00000c6a352a8bd0 @fullname="vagrant::provider::lxc", @outputters=[], @additive=true, @name="lxc", @path="vagrant::provider", @parent=#<VagrantLogger:0x00000c6a33f4da90 @fullname="vagrant", @outputters=[#<Log4r::StderrOutputter:0x00000c6a339950f8 @mon_mutex=#<Thread::Mutex:0x00000c6a33994e00>, @mon_mutex_owner_object_id=6825135876220, @mon_owner=nil, @mon_count=0, @name="stderr", @level=0, @formatter=#<Vagrant::Util::LoggingFormatter:0x00000c6a34321dd8 @formatter=#<Log4r::BasicFormatter:0x00000c6a33a7e370 @depth=7>>, @out=#<IO:<STDERR>>>], @additive=true, @name="vagrant", @path="", @parent=#<Log4r::RootLogger:0x00000c6a33f4d978 @level=0, @outputters=[]>, @level=1, @trace=false>, @level=1, @trace=false>, @level=1, @trace=false>>, @logger=#<Log4r::Logger:0x00000c6a355db8c0 @fullname="vagrant::provider::lxc::driver", @outputters=[], @additive=true, @name="driver", @path="vagrant::provider::lxc", @parent=#<Log4r::Logger:0x00000c6a352a8bd0 @fullname="vagrant::provider::lxc", @outputters=[], @additive=true, @name="lxc", @path="vagrant::provider", @parent=#<VagrantLogger:0x00000c6a33f4da90 @fullname="vagrant", @outputters=[#<Log4r::StderrOutputter:0x00000c6a339950f8 @mon_mutex=#<Thread::Mutex:0x00000c6a33994e00>, @mon_mutex_owner_object_id=6825135876220, @mon_owner=nil, @mon_count=0, @name="stderr", @level=0, @formatter=#<Vagrant::Util::LoggingFormatter:0x00000c6a34321dd8 @formatter=#<Log4r::BasicFormatter:0x00000c6a33a7e370 @depth=7>>, @out=#<IO:<STDERR>>>], @additive=true, @name="vagrant", @path="", @parent=#<Log4r::RootLogger:0x00000c6a33f4d978 @level=0, @outputters=[]>, @level=1, @trace=false>, @level=1, @trace=false>, @level=1, @trace=false>, @customizations=[]>, @cap_logger=#<Log4r::Logger:0x00000c6a3552ae08 @fullname="vagrant::capability_host::vagrant::lxc::provider", @outputters=[], @additive=true, @name="provider", @path="vagrant::capability_host::vagrant::lxc", @parent=#<VagrantLogger:0x00000c6a33f4da90 @fullname="vagrant", @outputters=[#<Log4r::StderrOutputter:0x00000c6a339950f8 @mon_mutex=#<Thread::Mutex:0x00000c6a33994e00>, @mon_mutex_owner_object_id=6825135876220, @mon_owner=nil, @mon_count=0, @name="stderr", @level=0, @formatter=#<Vagrant::Util::LoggingFormatter:0x00000c6a34321dd8 @formatter=#<Log4r::BasicFormatter:0x00000c6a33a7e370 @depth=7>>, @out=#<IO:<STDERR>>>], @additive=true, @name="vagrant", @path="", @parent=#<Log4r::RootLogger:0x00000c6a33f4d978 @level=0, @outputters=[]>, @level=1, @trace=false>, @level=1, @trace=false>, @cap_host_chain=[[:lxc, #<#<Class:0x00000c6a3552b308>:0x00000c6a3555d768>]], @cap_args=[#<Vagrant::Machine: swarm-node1 (Vagrant::LXC::Provider)>], @cap_caps={:virtualbox=>#<Vagrant::Registry:0x00000c6a3552b1c8 @items={:forwarded_ports=>#<Proc:0x00000c6a34729d90@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/providers/virtualbox/plugin.rb:27>, :nic_mac_addresses=>#<Proc:0x00000c6a34729d68@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/providers/virtualbox/plugin.rb:32>, :public_address=>#<Proc:0x00000c6a34729d40@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/providers/virtualbox/plugin.rb:37>, :snapshot_list=>#<Proc:0x00000c6a34729d18@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/providers/virtualbox/plugin.rb:42>}, @results_cache={}>, :hyperv=>#<Vagrant::Registry:0x00000c6a3552b128 @items={:public_address=>#<Proc:0x00000c6a345a6798@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/providers/hyperv/plugin.rb:25>, :snapshot_list=>#<Proc:0x00000c6a345a66f8@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/providers/hyperv/plugin.rb:30>}, @results_cache={}>, :docker=>#<Vagrant::Registry:0x00000c6a3552b088 @items={:public_address=>#<Proc:0x00000c6a33ba6f40@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/providers/docker/plugin.rb:60>, :proxy_machine=>#<Proc:0x00000c6a33ba6e50@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/providers/docker/plugin.rb:65>}, @results_cache={}>, :lxc=>#<Vagrant::Registry:0x00000c6a3552afe8 @items={:public_address=>#<Proc:0x00000c6a3458d450@/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/lib/vagrant-lxc/plugin.rb:35>}, @results_cache={}>}>)
DEBUG command: Getting target VMs for command. Arguments:
DEBUG command:  -- names: ["swarm-node1"]
DEBUG command:  -- options: {:provider=>nil}
DEBUG command: Finding machine that match name: swarm-node1
 INFO loader: Set "6825142240800_machine_swarm-node1" = ["[\"2\", #<Proc:0x00000c6a348ab060@/home/niek/vagrant/lxc/centos-7/Vagrantfile:19>]"]
 INFO loader: Loading configuration in order: [:home, :root, "6825142240800_machine_swarm-node1"]
DEBUG loader: Loading from: root (cache)
DEBUG loader: Loading from: 6825142240800_machine_swarm-node1 (cache)
DEBUG loader: Configuration loaded successfully, finalizing and returning
DEBUG push: finalizing
 WARN vagrantfile: Failed to locate vagrant-lxc-centos-7-amd64 with version 
 WARN vagrantfile: Performing lookup with inital values vagrant-lxc-centos-7-amd64 with version 
 INFO base: VBoxManage path: VBoxManage
DEBUG meta: Finding driver for VirtualBox version: 6.0.12
 INFO meta: Using VirtualBox driver: VagrantPlugins::ProviderVirtualBox::Driver::Version_6_0
 INFO base: VBoxManage path: VBoxManage
DEBUG environment: Initial usable provider list: [[6, :virtualbox], [7, :lxc]]
DEBUG environment: Priority sorted usable provider list: [:lxc, :virtualbox]
DEBUG environment: Preferred provider list: []
DEBUG environment: Using provider `lxc` detected in configuration and usable.
 INFO environment: Getting machine: swarm-node1 (lxc)
 INFO environment: Returning cached machine: swarm-node1 (lxc)
 INFO command: With machine: swarm-node1 (#<Vagrant::LXC::Provider:0x00000c6a352a8c20 @logger=#<Log4r::Logger:0x00000c6a352a8bd0 @fullname="vagrant::provider::lxc", @outputters=[], @additive=true, @name="lxc", @path="vagrant::provider", @parent=#<VagrantLogger:0x00000c6a33f4da90 @fullname="vagrant", @outputters=[#<Log4r::StderrOutputter:0x00000c6a339950f8 @mon_mutex=#<Thread::Mutex:0x00000c6a33994e00>, @mon_mutex_owner_object_id=6825135876220, @mon_owner=nil, @mon_count=0, @name="stderr", @level=0, @formatter=#<Vagrant::Util::LoggingFormatter:0x00000c6a34321dd8 @formatter=#<Log4r::BasicFormatter:0x00000c6a33a7e370 @depth=7>>, @out=#<IO:<STDERR>>>], @additive=true, @name="vagrant", @path="", @parent=#<Log4r::RootLogger:0x00000c6a33f4d978 @level=0, @outputters=[]>, @level=1, @trace=false>, @level=1, @trace=false>, @machine=#<Vagrant::Machine: swarm-node1 (Vagrant::LXC::Provider)>, @driver=#<Vagrant::LXC::Driver:0x00000c6a3556a828 @container_name=nil, @sudo_wrapper=#<Vagrant::LXC::SudoWrapper:0x00000c6a3556a7b0 @wrapper_path=nil, @privileged=true, @logger=#<Log4r::Logger:0x00000c6a3556a6c0 @fullname="vagrant::lxc::sudo_wrapper", @outputters=[], @additive=true, @name="sudo_wrapper", @path="vagrant::lxc", @parent=#<VagrantLogger:0x00000c6a33f4da90 @fullname="vagrant", @outputters=[#<Log4r::StderrOutputter:0x00000c6a339950f8 @mon_mutex=#<Thread::Mutex:0x00000c6a33994e00>, @mon_mutex_owner_object_id=6825135876220, @mon_owner=nil, @mon_count=0, @name="stderr", @level=0, @formatter=#<Vagrant::Util::LoggingFormatter:0x00000c6a34321dd8 @formatter=#<Log4r::BasicFormatter:0x00000c6a33a7e370 @depth=7>>, @out=#<IO:<STDERR>>>], @additive=true, @name="vagrant", @path="", @parent=#<Log4r::RootLogger:0x00000c6a33f4d978 @level=0, @outputters=[]>, @level=1, @trace=false>, @level=1, @trace=false>>, @cli=#<Vagrant::LXC::Driver::CLI:0x00000c6a355990b0 @sudo_wrapper=#<Vagrant::LXC::SudoWrapper:0x00000c6a3556a7b0 @wrapper_path=nil, @privileged=true, @logger=#<Log4r::Logger:0x00000c6a3556a6c0 @fullname="vagrant::lxc::sudo_wrapper", @outputters=[], @additive=true, @name="sudo_wrapper", @path="vagrant::lxc", @parent=#<VagrantLogger:0x00000c6a33f4da90 @fullname="vagrant", @outputters=[#<Log4r::StderrOutputter:0x00000c6a339950f8 @mon_mutex=#<Thread::Mutex:0x00000c6a33994e00>, @mon_mutex_owner_object_id=6825135876220, @mon_owner=nil, @mon_count=0, @name="stderr", @level=0, @formatter=#<Vagrant::Util::LoggingFormatter:0x00000c6a34321dd8 @formatter=#<Log4r::BasicFormatter:0x00000c6a33a7e370 @depth=7>>, @out=#<IO:<STDERR>>>], @additive=true, @name="vagrant", @path="", @parent=#<Log4r::RootLogger:0x00000c6a33f4d978 @level=0, @outputters=[]>, @level=1, @trace=false>, @level=1, @trace=false>>, @name=nil, @logger=#<Log4r::Logger:0x00000c6a35599060 @fullname="vagrant::provider::lxc::container::cli", @outputters=[], @additive=true, @name="cli", @path="vagrant::provider::lxc::container", @parent=#<Log4r::Logger:0x00000c6a352a8bd0 @fullname="vagrant::provider::lxc", @outputters=[], @additive=true, @name="lxc", @path="vagrant::provider", @parent=#<VagrantLogger:0x00000c6a33f4da90 @fullname="vagrant", @outputters=[#<Log4r::StderrOutputter:0x00000c6a339950f8 @mon_mutex=#<Thread::Mutex:0x00000c6a33994e00>, @mon_mutex_owner_object_id=6825135876220, @mon_owner=nil, @mon_count=0, @name="stderr", @level=0, @formatter=#<Vagrant::Util::LoggingFormatter:0x00000c6a34321dd8 @formatter=#<Log4r::BasicFormatter:0x00000c6a33a7e370 @depth=7>>, @out=#<IO:<STDERR>>>], @additive=true, @name="vagrant", @path="", @parent=#<Log4r::RootLogger:0x00000c6a33f4d978 @level=0, @outputters=[]>, @level=1, @trace=false>, @level=1, @trace=false>, @level=1, @trace=false>>, @logger=#<Log4r::Logger:0x00000c6a355db8c0 @fullname="vagrant::provider::lxc::driver", @outputters=[], @additive=true, @name="driver", @path="vagrant::provider::lxc", @parent=#<Log4r::Logger:0x00000c6a352a8bd0 @fullname="vagrant::provider::lxc", @outputters=[], @additive=true, @name="lxc", @path="vagrant::provider", @parent=#<VagrantLogger:0x00000c6a33f4da90 @fullname="vagrant", @outputters=[#<Log4r::StderrOutputter:0x00000c6a339950f8 @mon_mutex=#<Thread::Mutex:0x00000c6a33994e00>, @mon_mutex_owner_object_id=6825135876220, @mon_owner=nil, @mon_count=0, @name="stderr", @level=0, @formatter=#<Vagrant::Util::LoggingFormatter:0x00000c6a34321dd8 @formatter=#<Log4r::BasicFormatter:0x00000c6a33a7e370 @depth=7>>, @out=#<IO:<STDERR>>>], @additive=true, @name="vagrant", @path="", @parent=#<Log4r::RootLogger:0x00000c6a33f4d978 @level=0, @outputters=[]>, @level=1, @trace=false>, @level=1, @trace=false>, @level=1, @trace=false>, @customizations=[]>, @cap_logger=#<Log4r::Logger:0x00000c6a3552ae08 @fullname="vagrant::capability_host::vagrant::lxc::provider", @outputters=[], @additive=true, @name="provider", @path="vagrant::capability_host::vagrant::lxc", @parent=#<VagrantLogger:0x00000c6a33f4da90 @fullname="vagrant", @outputters=[#<Log4r::StderrOutputter:0x00000c6a339950f8 @mon_mutex=#<Thread::Mutex:0x00000c6a33994e00>, @mon_mutex_owner_object_id=6825135876220, @mon_owner=nil, @mon_count=0, @name="stderr", @level=0, @formatter=#<Vagrant::Util::LoggingFormatter:0x00000c6a34321dd8 @formatter=#<Log4r::BasicFormatter:0x00000c6a33a7e370 @depth=7>>, @out=#<IO:<STDERR>>>], @additive=true, @name="vagrant", @path="", @parent=#<Log4r::RootLogger:0x00000c6a33f4d978 @level=0, @outputters=[]>, @level=1, @trace=false>, @level=1, @trace=false>, @cap_host_chain=[[:lxc, #<#<Class:0x00000c6a3552b308>:0x00000c6a3555d768>]], @cap_args=[#<Vagrant::Machine: swarm-node1 (Vagrant::LXC::Provider)>], @cap_caps={:virtualbox=>#<Vagrant::Registry:0x00000c6a3552b1c8 @items={:forwarded_ports=>#<Proc:0x00000c6a34729d90@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/providers/virtualbox/plugin.rb:27>, :nic_mac_addresses=>#<Proc:0x00000c6a34729d68@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/providers/virtualbox/plugin.rb:32>, :public_address=>#<Proc:0x00000c6a34729d40@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/providers/virtualbox/plugin.rb:37>, :snapshot_list=>#<Proc:0x00000c6a34729d18@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/providers/virtualbox/plugin.rb:42>}, @results_cache={}>, :hyperv=>#<Vagrant::Registry:0x00000c6a3552b128 @items={:public_address=>#<Proc:0x00000c6a345a6798@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/providers/hyperv/plugin.rb:25>, :snapshot_list=>#<Proc:0x00000c6a345a66f8@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/providers/hyperv/plugin.rb:30>}, @results_cache={}>, :docker=>#<Vagrant::Registry:0x00000c6a3552b088 @items={:public_address=>#<Proc:0x00000c6a33ba6f40@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/providers/docker/plugin.rb:60>, :proxy_machine=>#<Proc:0x00000c6a33ba6e50@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/providers/docker/plugin.rb:65>}, @results_cache={}>, :lxc=>#<Vagrant::Registry:0x00000c6a3552afe8 @items={:public_address=>#<Proc:0x00000c6a3458d450@/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/lib/vagrant-lxc/plugin.rb:35>}, @results_cache={}>}>)
 INFO interface: info: Bringing machine 'swarm-node1' up with 'lxc' provider...
Bringing machine 'swarm-node1' up with 'lxc' provider...
 INFO batch_action: Enabling parallelization by default.
 INFO batch_action: Disabling parallelization because only executing one action
 INFO batch_action: Batch action will parallelize: false
 INFO batch_action: Starting action: #<Vagrant::Machine:0x00000c6a351cb528> up {:destroy_on_error=>true, :install_provider=>false, :parallel=>true, :provision_ignore_sentinel=>false, :provision_types=>nil}
 INFO machine: Calling action: up on provider LXC (new VM)
DEBUG environment: Attempting to acquire process-lock: machine-action-4e302da4b51fd90dd53d7f86d656be9a
DEBUG environment: Attempting to acquire process-lock: dotlock
 INFO environment: Acquired process lock: dotlock
 INFO environment: Released process lock: dotlock
 INFO environment: Acquired process lock: machine-action-4e302da4b51fd90dd53d7f86d656be9a
 INFO interface: Machine: action ["up", "start", {:target=>:"swarm-node1"}]
 INFO host: Autodetecting host type for [#<Vagrant::Environment: /home/niek/vagrant/lxc/centos-7>]
DEBUG host: Trying: freebsd
DEBUG host: Trying: gentoo
DEBUG host: Trying: darwin
DEBUG host: Trying: slackware
DEBUG host: Trying: alt
DEBUG host: Trying: suse
DEBUG host: Trying: redhat
DEBUG host: Trying: void
DEBUG host: Trying: arch
 INFO host: Detected: arch!
 INFO runner: Preparing hooks for middleware sequence...
 INFO runner: 2 hooks defined.
 INFO runner: Running action: machine_action_up #<Vagrant::Action::Builder:0x00000c6a34a5ba18>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35133e08>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::ConfigValidate:0x00000c6a35133de0>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35133db8>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35133d40>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::Call:0x00000c6a35133d18>
 INFO runner: Preparing hooks for middleware sequence...
 INFO runner: 2 hooks defined.
 INFO runner: Running action: machine_action_up #<Vagrant::Action::Builder:0x00000c6a3523b0f8>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35242fb0>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::IsState:0x00000c6a35242f88>
DEBUG is_state: Checking if machine state is 'not_created'
DEBUG is_state: -- Machine state: not_created
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a352829d0>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a352829d0>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::IsState:0x00000c6a35242f88>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35242fb0>
 INFO runner: Preparing hooks for middleware sequence...
 INFO runner: 2 hooks defined.
 INFO runner: Running action: machine_action_up #<Vagrant::Action::Warden:0x00000c6a352d2480>
 INFO warden: Calling IN action: #<Proc:0x00000c6a3549a628@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:120 (lambda)>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a352d23b8>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::HandleBox:0x00000c6a352d2390>
 INFO handle_box: Machine already has box. HandleBox will not run.
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35316518>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a353164a0>
 INFO warden: Calling IN action: #<Vagrant::LXC::Action::HandleBoxMetadata:0x00000c6a35316428>
 INFO interface: info: Importing base box 'vagrant-lxc-centos-7-amd64'...
 INFO interface: info: ==> swarm-node1: Importing base box 'vagrant-lxc-centos-7-amd64'...
==> swarm-node1: Importing base box 'vagrant-lxc-centos-7-amd64'...
 INFO handle_box_metadata: Validating box contents
 INFO handle_box_metadata: Setting box options on environment
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a354560e0>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35456068>
 INFO warden: Calling IN action: #<Vagrant::LXC::Action::Create:0x00000c6a35456040>
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-create", "--version"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: 3.2.1
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-update-config", "-c", "/home/niek/.vagrant.d/boxes/vagrant-lxc-centos-7-amd64/0/lxc/lxc-config"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
DEBUG driver: Creating container...
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-create", "-B", "best", "--template", "/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/scripts/lxc-template", "--name", "centos-7_swarm-node1_1570708822359_18199", "--fssize", "20G", "--", "--tarball", "/home/niek/.vagrant.d/boxes/vagrant-lxc-centos-7-amd64/0/lxc/rootfs.tar.gz", "--config", "/home/niek/.vagrant.d/boxes/vagrant-lxc-centos-7-amd64/0/lxc/lxc-config"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stderr: lxc-create: centos-7_swarm-node1_1570708822359_18199: storage/btrfs.c: btrfs_create: 961 Inappropriate ioctl for device - Failed to create btrfs subvolume "/var/lib/lxc/centos-7_swarm-node1_1570708822359_18199/rootfs"
DEBUG subprocess: stderr: lxc-create: centos-7_swarm-node1_1570708822359_18199: storage/zfs.c: zfs_create: 761 Failed to create zfs dataset "zfs:lxc/centos-7_swarm-node1_1570708822359_18199": lxc-create: centos-7_swarm-node1_1570708822359_18199:
DEBUG subprocess: stderr: lxc-create: centos-7_swarm-node1_1570708822359_18199: storage/lvm.c: do_lvm_create: 204 Failed to create logical volume "centos-7_swarm-node1_1570708822359_18199":   Volume group "lxc" not foun
lxc-create: centos-7_swarm-node1_1570708822359_18199: storage/lvm.c: lvm_create: 676 Error creating new logical volume "lvm:/dev/lxc/centos-7_swarm-node1_1570708822359_18199" of size "21474836480 bytes"
DEBUG subprocess: stdout: Unpacking the rootfs
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 31995
DEBUG subprocess: Exit status: 0
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-config", "lxc.lxcpath"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: /var/lib/lxc
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-update-config", "-c", "/var/lib/lxc/centos-7_swarm-node1_1570708822359_18199/config"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO machine: New machine ID: "centos-7_swarm-node1_1570708822359_18199"
DEBUG lxc: Instantiating the container for: "centos-7_swarm-node1_1570708822359_18199"
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-ls"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: centos-7_swarm-node1_1570708822359_18199
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35456018>
 INFO warden: Calling IN action: #<Proc:0x00000c6a35455fa0@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:120 (lambda)>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35133cc8>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35133c50>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::ConfigValidate:0x00000c6a35133c28>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35133c00>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35133b88>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BoxCheckOutdated:0x00000c6a35133b60>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35177900>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35177888>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::Call:0x00000c6a35177860>
 INFO runner: Preparing hooks for middleware sequence...
 INFO runner: 2 hooks defined.
 INFO runner: Running action: machine_action_up #<Vagrant::Action::Builder:0x00000c6a3432b568>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a343443b0>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::IsState:0x00000c6a343442e8>
DEBUG is_state: Checking if machine state is 'running'
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-info", "--name", "centos-7_swarm-node1_1570708822359_18199"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: Name:           centos-7_swarm-node1_1570708822359_18199
State:          STOPPED
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
DEBUG is_state: -- Machine state: stopped
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a34572a10>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a34572a10>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::IsState:0x00000c6a343442e8>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a343443b0>
 INFO runner: Preparing hooks for middleware sequence...
 INFO runner: 2 hooks defined.
 INFO runner: Running action: machine_action_up #<Vagrant::Action::Warden:0x00000c6a34825bb8>
 INFO warden: Calling IN action: #<Proc:0x00000c6a351880e8@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:120 (lambda)>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a34825aa0>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::Provision:0x00000c6a34825a78>
 INFO provision: Checking provisioner sentinel file...
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a348c6c48>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a348c6bd0>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::EnvSet:0x00000c6a348c6ba8>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a348c6ab8>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a348c6a40>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::HandleForwardedPortCollisions:0x00000c6a348c6928>
DEBUG environment: Attempting to acquire process-lock: fpcollision
DEBUG environment: Attempting to acquire process-lock: dotlock
 INFO environment: Acquired process lock: dotlock
 INFO environment: Released process lock: dotlock
 INFO environment: Acquired process lock: fpcollision
 INFO handle_port_collisions: Detecting any forwarded port collisions...
DEBUG handle_port_collisions: Extra in use: {}
DEBUG handle_port_collisions: Remap: {}
DEBUG handle_port_collisions: Repair: true
 INFO environment: Released process lock: fpcollision
DEBUG environment: Attempting to acquire process-lock: dotlock
 INFO environment: Acquired process lock: dotlock
 INFO environment: Released process lock: dotlock
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a349338c0>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a349337f8>
 INFO warden: Calling IN action: #<Vagrant::LXC::Action::PrepareNFSValidIds:0x00000c6a34933780>
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-ls"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: centos-7_swarm-node1_1570708822359_18199
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 31999
DEBUG subprocess: Exit status: 0
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a34998400>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a34998338>
 INFO warden: Calling IN action: #<VagrantPlugins::SyncedFolderNFS::ActionCleanup:0x00000c6a34998310>
DEBUG host: Searching for cap: nfs_prune
DEBUG host: Checking in: arch
DEBUG host: Checking in: linux
DEBUG host: Found cap: nfs_prune in linux
 INFO nfs: NFS pruning. Valid IDs: ["centos-7_swarm-node1_1570708822359_18199"]
DEBUG host: Searching for cap: nfs_prune
DEBUG host: Checking in: arch
DEBUG host: Checking in: linux
DEBUG host: Found cap: nfs_prune in linux
 INFO host: Execute capability: nfs_prune [#<Vagrant::Environment: /home/niek/vagrant/lxc/centos-7>, #<Vagrant::UI::Prefixed:0x00000c6a35205868 @logger=#<Log4r::Logger:0x00000c6a35205818 @fullname="vagrant::ui::interface", @outputters=[], @additive=true, @name="interface", @path="vagrant::ui", @parent=#<VagrantLogger:0x00000c6a33f4da90 @fullname="vagrant", @outputters=[#<Log4r::StderrOutputter:0x00000c6a339950f8 @mon_mutex=#<Thread::Mutex:0x00000c6a33994e00>, @mon_mutex_owner_object_id=6825135876220, @mon_owner=nil, @mon_count=0, @name="stderr", @level=0, @formatter=#<Vagrant::Util::LoggingFormatter:0x00000c6a34321dd8 @formatter=#<Log4r::BasicFormatter:0x00000c6a33a7e370 @depth=7>>, @out=#<IO:<STDERR>>>], @additive=true, @name="vagrant", @path="", @parent=#<Log4r::RootLogger:0x00000c6a33f4d978 @level=0, @outputters=[]>, @level=1, @trace=false>, @level=1, @trace=false>, @opts={}, @stdin=#<IO:<STDIN>>, @stdout=#<IO:<STDOUT>>, @stderr=#<IO:<STDERR>>, @prefix=:"swarm-node1", @ui=#<Vagrant::UI::Basic:0x00000c6a34582e60 @logger=#<Log4r::Logger:0x00000c6a34582de8 @fullname="vagrant::ui::interface", @outputters=[], @additive=true, @name="interface", @path="vagrant::ui", @parent=#<VagrantLogger:0x00000c6a33f4da90 @fullname="vagrant", @outputters=[#<Log4r::StderrOutputter:0x00000c6a339950f8 @mon_mutex=#<Thread::Mutex:0x00000c6a33994e00>, @mon_mutex_owner_object_id=6825135876220, @mon_owner=nil, @mon_count=0, @name="stderr", @level=0, @formatter=#<Vagrant::Util::LoggingFormatter:0x00000c6a34321dd8 @formatter=#<Log4r::BasicFormatter:0x00000c6a33a7e370 @depth=7>>, @out=#<IO:<STDERR>>>], @additive=true, @name="vagrant", @path="", @parent=#<Log4r::RootLogger:0x00000c6a33f4d978 @level=0, @outputters=[]>, @level=1, @trace=false>, @level=1, @trace=false>, @opts={:color=>:default}, @stdin=#<IO:<STDIN>>, @stdout=#<IO:<STDOUT>>, @stderr=#<IO:<STDERR>>, @lock=#<Thread::Mutex:0x00000c6a34597158>>>, ["centos-7_swarm-node1_1570708822359_18199"]] (arch)
 INFO linux: Pruning invalid NFS entries...
DEBUG linux: Known valid NFS export IDs: ["centos-7_swarm-node1_1570708822359_18199"]
DEBUG linux: Composite valid NFS export IDs with user: ["1000 centos-7_swarm-node1_1570708822359_18199"]
DEBUG linux: NFS export IDs to be removed: []
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a34a15b80>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a34a15a68>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::SyncedFolderCleanup:0x00000c6a34a15a18>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35076218>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a350761a0>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::SyncedFolders:0x00000c6a35076178>
 INFO synced_folders: SyncedFolders loading from cache: false
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a350b1c50>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a350b1bd8>
 INFO warden: Calling IN action: #<Vagrant::LXC::Action::PrepareNFSSettings:0x00000c6a350b1bb0>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a350ffcc0>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a350ffc48>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::SetHostname:0x00000c6a350ffbf8>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a350ffbd0>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a350ffb30>
 INFO warden: Calling IN action: #<Vagrant::LXC::Action::WarnNetworks:0x00000c6a350ffb08>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a350ffae0>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a350ffa68>
 INFO warden: Calling IN action: #<Vagrant::LXC::Action::ForwardPorts:0x00000c6a350ffa40>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35139e48>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35139dd0>
 INFO warden: Calling IN action: #<Vagrant::LXC::Action::PrivateNetworks:0x00000c6a35139da8>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35139d80>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35139d08>
 INFO warden: Calling IN action: #<Vagrant::LXC::Action::Boot:0x00000c6a35139ce0>
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-create", "--version"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: 3.2.1
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO interface: info: Starting container...
 INFO interface: info: ==> swarm-node1: Starting container...
==> swarm-node1: Starting container...
 INFO driver: Starting container...
DEBUG driver: Prunning vagrant-lxc customizations
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-config", "lxc.lxcpath"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: /var/lib/lxc
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "cat", "/var/lib/lxc/centos-7_swarm-node1_1570708822359_18199/config"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: # Template used to create this container: /home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/scripts/lxc-template
# Parameters passed to the template: --tarball /home/niek/.vagrant.d/boxes/vagrant-lxc-centos-7-amd64/0/lxc/rootfs.tar.gz --config /home/niek/.vagrant.d/boxes/vagrant-lxc-centos-7-amd64/0/lxc/lxc-config
# Template script checksum (SHA-1): eae122a2d6cd572c26668257efa7963c2258186e
# For additional config options, please look at lxc.container.conf(5)

# Uncomment the following line to support nesting containers:
#lxc.include = /usr/share/lxc/config/nesting.conf
# (Be aware this has security implications)


##############################################
# Container specific configuration (automatically set)
lxc.net.0.type = veth
lxc.net.0.link = lxcbr0
lxc.net.0.flags = up
lxc.net.0.ipv4.address = 0.0.0.0/24
lxc.rootfs.path = dir:/var/lib/lxc/centos-7_swarm-node1_1570708822359_18199/rootfs
lxc.uts.name = centos-7_swarm-node1_1570708822359_18199

##############################################
# Network configuration (automatically set)

##############################################
# vagrant-lxc base box specific configuration
# Taken from the oracle.common.conf.in
# Console settings

lxc.tty.dir = lxc
lxc.tty.max = 4
lxc.pty.max = 1024

# Mount entries
lxc.mount.auto = proc:mixed sys:ro

# Ensure hostname is changed on clone
lxc.hook.clone = /usr/share/lxc/hooks/clonehostname

# Capabilities
# Uncomment these if you don't run anything that needs the capability, and
# would like the container to run with less privilege.
#
# Dropping sys_admin disables container root from doing a lot of things
# that could be bad like re-mounting lxc fstab entries rw for example,
# but also disables some useful things like being able to nfs mount, and
# things that are already namespaced with ns_capable() kernel checks, like
# hostname(1).
# lxc.cap.drop = sys_admin
# lxc.cap.drop = net_raw          # breaks dhcp/ping
# lxc.cap.drop = setgid           # breaks login (initgroups/setgroups)
# lxc.cap.drop = dac_read_search  # breaks login (pam unix_chkpwd)
# lxc.cap.drop = setuid           # breaks sshd,nfs statd
# lxc.cap.drop = audit_control    # breaks sshd (set_loginuid failed)
# lxc.cap.drop = audit_write
#
lxc.cap.drop = mac_admin mac_override
lxc.cap.drop = sys_module sys_nice sys_pacct
lxc.cap.drop = sys_rawio sys_time

# Control Group devices: all denied except those whitelisted
lxc.cgroup.devices.deny = a
# Allow any mknod (but not reading/writing the node)
lxc.cgroup.devices.allow = c *:* m
lxc.cgroup.devices.allow = b *:* m
lxc.cgroup.devices.allow = c 1:3 rwm	# /dev/null
lxc.cgroup.devices.allow = c 1:5 rwm	# /dev/zero
lxc.cgroup.devices.allow = c 1:7 rwm	# /dev/full
lxc.cgroup.devices.allow = c 5:0 rwm	# /dev/tty
lxc.cgroup.devices.allow = c 1:8 rwm	# /dev/random
lxc.cgroup.devices.allow = c 1:9 rwm	# /dev/urandom
lxc.cgroup.devices.allow = c 136:* rwm	# /dev/tty[1-4] ptys and lxc console
lxc.cgroup.devices.allow = c 5:2 rwm	# /dev/ptmx pty master

# Needed by default docker config
lxc.cgroup.devices.allow = c 5:1 rwm # /dev/console
lxc.cgroup.devices.allow = c 4:0 rwm # /dev/tty0
lxc.cgroup.devices.allow = c 4:1 rwm # /dev/tty1
lxc.cgroup.devices.allow = c 10:200 rwm # /dev/net/tun

# Blacklist some syscalls which are not safe in privileged
# containers
lxc.seccomp.profile = /usr/share/lxc/config/common.seccomp

##############################################
# vagrant-lxc container specific configuration
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "cp", "-f", "/tmp/lxc-config20191010-906791-w2beic", "/var/lib/lxc/centos-7_swarm-node1_1570708822359_18199/config"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "chown", "root:root", "/var/lib/lxc/centos-7_swarm-node1_1570708822359_18199/config"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "cat", "/var/lib/lxc/centos-7_swarm-node1_1570708822359_18199/config"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: # Template used to create this container: /home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/scripts/lxc-template
# Parameters passed to the template: --tarball /home/niek/.vagrant.d/boxes/vagrant-lxc-centos-7-amd64/0/lxc/rootfs.tar.gz --config /home/niek/.vagrant.d/boxes/vagrant-lxc-centos-7-amd64/0/lxc/lxc-config
# Template script checksum (SHA-1): eae122a2d6cd572c26668257efa7963c2258186e
# For additional config options, please look at lxc.container.conf(5)

# Uncomment the following line to support nesting containers:
#lxc.include = /usr/share/lxc/config/nesting.conf
# (Be aware this has security implications)


##############################################
# Container specific configuration (automatically set)
lxc.net.0.type = veth
lxc.net.0.link = lxcbr0
lxc.net.0.flags = up
lxc.net.0.ipv4.address = 0.0.0.0/24
lxc.rootfs.path = dir:/var/lib/lxc/centos-7_swarm-node1_1570708822359_18199/rootfs
lxc.uts.name = centos-7_swarm-node1_1570708822359_18199

##############################################
# Network configuration (automatically set)

##############################################
# vagrant-lxc base box specific configuration
# Taken from the oracle.common.conf.in
# Console settings

lxc.tty.dir = lxc
lxc.tty.max = 4
lxc.pty.max = 1024

# Mount entries
lxc.mount.auto = proc:mixed sys:ro

# Ensure hostname is changed on clone
lxc.hook.clone = /usr/share/lxc/hooks/clonehostname

# Capabilities
# Uncomment these if you don't run anything that needs the capability, and
# would like the container to run with less privilege.
#
# Dropping sys_admin disables container root from doing a lot of things
# that could be bad like re-mounting lxc fstab entries rw for example,
# but also disables some useful things like being able to nfs mount, and
# things that are already namespaced with ns_capable() kernel checks, like
# hostname(1).
# lxc.cap.drop = sys_admin
# lxc.cap.drop = net_raw          # breaks dhcp/ping
# lxc.cap.drop = setgid           # breaks login (initgroups/setgroups)
# lxc.cap.drop = dac_read_search  # breaks login (pam unix_chkpwd)
# lxc.cap.drop = setuid           # breaks sshd,nfs statd
# lxc.cap.drop = audit_control    # breaks sshd (set_loginuid failed)
# lxc.cap.drop = audit_write
#
lxc.cap.drop = mac_admin mac_override
lxc.cap.drop = sys_module sys_nice sys_pacct
lxc.cap.drop = sys_rawio sys_time

# Control Group devices: all denied except those whitelisted
lxc.cgroup.devices.deny = a
# Allow any mknod (but not reading/writing the node)
lxc.cgroup.devices.allow = c *:* m
lxc.cgroup.devices.allow = b *:* m
lxc.cgroup.devices.allow = c 1:3 rwm	# /dev/null
lxc.cgroup.devices.allow = c 1:5 rwm	# /dev/zero
lxc.cgroup.devices.allow = c 1:7 rwm	# /dev/full
lxc.cgroup.devices.allow = c 5:0 rwm	# /dev/tty
lxc.cgroup.devices.allow = c 1:8 rwm	# /dev/random
lxc.cgroup.devices.allow = c 1:9 rwm	# /dev/urandom
lxc.cgroup.devices.allow = c 136:* rwm	# /dev/tty[1-4] ptys and lxc console
lxc.cgroup.devices.allow = c 5:2 rwm	# /dev/ptmx pty master

# Needed by default docker config
lxc.cgroup.devices.allow = c 5:1 rwm # /dev/console
lxc.cgroup.devices.allow = c 4:0 rwm # /dev/tty0
lxc.cgroup.devices.allow = c 4:1 rwm # /dev/tty1
lxc.cgroup.devices.allow = c 10:200 rwm # /dev/net/tun

# Blacklist some syscalls which are not safe in privileged
# containers
lxc.seccomp.profile = /usr/share/lxc/config/common.seccomp

##############################################
# vagrant-lxc container specific configuration
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "cp", "-f", "/tmp/lxc-config20191010-906791-1s98h19", "/var/lib/lxc/centos-7_swarm-node1_1570708822359_18199/config"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "chown", "root:root", "/var/lib/lxc/centos-7_swarm-node1_1570708822359_18199/config"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-start", "-d", "--name", "centos-7_swarm-node1_1570708822359_18199"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35139cb8>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35139c40>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::WaitForCommunicator:0x00000c6a35139c18>
 INFO interface: output: Waiting for machine to boot. This may take a few minutes...
 INFO interface: output: ==> swarm-node1: Waiting for machine to boot. This may take a few minutes...
==> swarm-node1: Waiting for machine to boot. This may take a few minutes...
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-info", "--name", "centos-7_swarm-node1_1570708822359_18199"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: Name:           centos-7_swarm-node1_1570708822359_18199
State:          RUNNING
PID:            907753
DEBUG subprocess: stdout: CPU use:        0.05 seconds
BlkIO use:      0 bytes
Memory use:     14.64 MiB
DEBUG subprocess: stdout: KMem use:       3.60 MiB
Link:           vethHLD9IG
 TX bytes:      90 bytes
 RX bytes:      286 bytes
 Total bytes:   376 bytes
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-info", "--name", "centos-7_swarm-node1_1570708822359_18199"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: Name:           centos-7_swarm-node1_1570708822359_18199
DEBUG subprocess: stdout: State:          RUNNING
DEBUG subprocess: stdout: PID:            907753
DEBUG subprocess: stdout: CPU use:        0.23 seconds
DEBUG subprocess: stdout: BlkIO use:      0 bytes
DEBUG subprocess: stdout: Memory use:     20.07 MiB
DEBUG subprocess: stdout: KMem use:       7.32 MiB
DEBUG subprocess: stdout: Link:           vethHLD9IG
DEBUG subprocess: stdout:  TX bytes:      176 bytes
DEBUG subprocess: stdout:  RX bytes:      286 bytes
DEBUG subprocess: stdout:  Total bytes:   462 bytes
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO machine: Calling action: ssh_ip on provider LXC (centos-7_swarm-node1_1570708822359_18199)
 INFO interface: Machine: action ["ssh_ip", "start", {:target=>:"swarm-node1"}]
 INFO runner: Preparing hooks for middleware sequence...
 INFO runner: 1 hooks defined.
 INFO runner: Running action: machine_action_ssh_ip #<Vagrant::Action::Builder:0x00000c6a354e6438>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a354f1d88>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::Call:0x00000c6a354f1d60>
 INFO runner: Preparing hooks for middleware sequence...
 INFO runner: 1 hooks defined.
 INFO runner: Running action: machine_action_ssh_ip #<Vagrant::Action::Builder:0x00000c6a35579bc0>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35586af0>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::ConfigValidate:0x00000c6a35586ac8>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35586aa0>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35586aa0>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::ConfigValidate:0x00000c6a35586ac8>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35586af0>
 INFO runner: Preparing hooks for middleware sequence...
 INFO runner: 1 hooks defined.
 INFO runner: Running action: machine_action_ssh_ip #<Vagrant::Action::Warden:0x00000c6a355c01d8>
 INFO warden: Calling IN action: #<Proc:0x00000c6a356487b8@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:120 (lambda)>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a355c0110>
 INFO warden: Calling IN action: #<Vagrant::LXC::Action::FetchIpWithLxcInfo:0x00000c6a355c00e8>
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-info", "--name", "centos-7_swarm-node1_1570708822359_18199", "-iH"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO retryable: Retryable exception raised: #<Vagrant::LXC::Errors::ExecuteError: There was an error executing lxc-info

For more information on the failure, enable detailed logging by setting
the environment variable VAGRANT_LOG to DEBUG.>
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-info", "--name", "centos-7_swarm-node1_1570708822359_18199"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: Name:           centos-7_swarm-node1_1570708822359_18199
State:          RUNNING
DEBUG subprocess: stdout: PID:            907753
DEBUG subprocess: stdout: CPU use:        1.43 seconds
DEBUG subprocess: stdout: BlkIO use:      8.00 KiB
DEBUG subprocess: stdout: Memory use:     32.60 MiB
DEBUG subprocess: stdout: KMem use:       12.75 MiB
DEBUG subprocess: stdout: Link:           vethHLD9IG
DEBUG subprocess: stdout:  TX bytes:      266 bytes
 RX bytes:      572 bytes
 Total bytes:   838 bytes
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-info", "--name", "centos-7_swarm-node1_1570708822359_18199"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: Name:           centos-7_swarm-node1_1570708822359_18199
State:          RUNNING
DEBUG subprocess: stdout: PID:            907753
DEBUG subprocess: stdout: CPU use:        1.43 seconds
DEBUG subprocess: stdout: BlkIO use:      8.00 KiB
DEBUG subprocess: stdout: Memory use:     32.41 MiB
DEBUG subprocess: stdout: KMem use:       12.70 MiB
DEBUG subprocess: stdout: Link:           vethHLD9IG
 TX bytes:      858 bytes
DEBUG subprocess: stdout:  RX bytes:      906 bytes
 Total bytes:   1.72 KiB
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-info", "--name", "centos-7_swarm-node1_1570708822359_18199"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: Name:           centos-7_swarm-node1_1570708822359_18199
State:          RUNNING
DEBUG subprocess: stdout: PID:            907753
DEBUG subprocess: stdout: CPU use:        1.43 seconds
DEBUG subprocess: stdout: BlkIO use:      8.00 KiB
DEBUG subprocess: stdout: Memory use:     32.40 MiB
DEBUG subprocess: stdout: KMem use:       12.69 MiB
DEBUG subprocess: stdout: Link:           vethHLD9IG
DEBUG subprocess: stdout:  TX bytes:      858 bytes
 RX bytes:      948 bytes
 Total bytes:   1.76 KiB
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-info", "--name", "centos-7_swarm-node1_1570708822359_18199", "-iH"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO retryable: Retryable exception raised: #<Vagrant::LXC::Errors::ExecuteError: There was an error executing lxc-info

For more information on the failure, enable detailed logging by setting
the environment variable VAGRANT_LOG to DEBUG.>
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-info", "--name", "centos-7_swarm-node1_1570708822359_18199"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: Name:           centos-7_swarm-node1_1570708822359_18199
DEBUG subprocess: stdout: State:          RUNNING
DEBUG subprocess: stdout: PID:            907753
DEBUG subprocess: stdout: CPU use:        1.51 seconds
DEBUG subprocess: stdout: BlkIO use:      8.00 KiB
DEBUG subprocess: stdout: Memory use:     25.07 MiB
DEBUG subprocess: stdout: KMem use:       7.43 MiB
DEBUG subprocess: stdout: Link:           vethHLD9IG
DEBUG subprocess: stdout:  TX bytes:      1.21 KiB
 RX bytes:      1.88 KiB
 Total bytes:   3.10 KiB
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-info", "--name", "centos-7_swarm-node1_1570708822359_18199"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: Name:           centos-7_swarm-node1_1570708822359_18199
State:          RUNNING
DEBUG subprocess: stdout: PID:            907753
DEBUG subprocess: stdout: CPU use:        1.51 seconds
DEBUG subprocess: stdout: BlkIO use:      8.00 KiB
DEBUG subprocess: stdout: Memory use:     25.07 MiB
DEBUG subprocess: stdout: KMem use:       7.43 ○ vagrant up
Bringing machine 'swarm-node1' up with 'lxc' provider...
==> swarm-node1: Importing base box 'vagrant-lxc-centos-7-amd64'...
==> swarm-node1: Starting container...
==> swarm-node1: Waiting for machine to boot. This may take a few minutes...
    swarm-node1: SSH address: 10.0.3.15:22
    swarm-node1: SSH username: vagrant
    swarm-node1: SSH auth method: privat` MiB
DEBUG subprocess: stdout: Link:           vethHLD9IG
 TX bytes:      1.32 KiB
DEBUG subprocess: stdout:  RX bytes:      2.30 KiB
 Total bytes:   3.62 KiB
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-info", "--name", "centos-7_swarm-node1_1570708822359_18199", "-iH"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: 10.0.3.251
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a3560fcd8>
 INFO warden: Calling IN action: #<Proc:0x00000c6a3560fc10@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:120 (lambda)>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a354f1d10>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a354f1d10>
 INFO warden: Calling OUT action: #<Proc:0x00000c6a3560fc10@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:120 (lambda)>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a3560fcd8>
 INFO warden: Calling OUT action: #<Vagrant::LXC::Action::FetchIpWithLxcInfo:0x00000c6a355c00e8>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a355c0110>
 INFO warden: Calling OUT action: #<Proc:0x00000c6a356487b8@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:120 (lambda)>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::Call:0x00000c6a354f1d60>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a354f1d88>
 INFO interface: Machine: action ["ssh_ip", "end", {:target=>:"swarm-node1"}]
DEBUG ssh: Checking key permissions: /home/niek/.vagrant.d/insecure_private_key
 INFO interface: detail: SSH address: 10.0.3.251:22
 INFO interface: detail:     swarm-node1: SSH address: 10.0.3.251:22
    swarm-node1: SSH address: 10.0.3.251:22
 INFO interface: detail: SSH username: vagrant
 INFO interface: detail:     swarm-node1: SSH username: vagrant
    swarm-node1: SSH username: vagrant
 INFO interface: detail: SSH auth method: private key
 INFO interface: detail:     swarm-node1: SSH auth method: private key
    swarm-node1: SSH auth method: private key
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-info", "--name", "centos-7_swarm-node1_1570708822359_18199"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-info", "--name", "centos-7_swarm-node1_1570708822359_18199"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: Name:           centos-7_swarm-node1_1570708822359_18199
DEBUG subprocess: stdout: State:          RUNNING
DEBUG subprocess: stdout: PID:            907753
DEBUG subprocess: stdout: IP:             10.0.3.251
DEBUG subprocess: stdout: CPU use:        1.61 seconds
DEBUG subprocess: stdout: BlkIO use:      16.00 KiB
DEBUG subprocess: stdout: Memory use:     23.86 MiB
DEBUG subprocess: stdout: Name:           centos-7_swarm-node1_1570708822359_18199
DEBUG subprocess: stdout: State:          RUNNING
DEBUG subprocess: stdout: PID:            907753
DEBUG subprocess: stdout: KMem use:       7.01 MiB
DEBUG subprocess: stdout: Link:           vethHLD9IG
 TX bytes:      1.32 KiB
DEBUG subprocess: stdout:  RX bytes:      2.79 KiB
 Total bytes:   4.11 KiB
DEBUG subprocess: stdout: IP:             10.0.3.251
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO machine: Calling action: ssh_ip on provider LXC (centos-7_swarm-node1_1570708822359_18199)
 INFO interface: Machine: action ["ssh_ip", "start", {:target=>:"swarm-node1"}]
 INFO runner: Preparing hooks for middleware sequence...
 INFO runner: 1 hooks defined.
 INFO runner: Running action: machine_action_ssh_ip #<Vagrant::Action::Builder:0x00000c6a35274178>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35293fa0>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::Call:0x00000c6a35293f78>
DEBUG subprocess: stdout: CPU use:        1.61 seconds
BlkIO use:      16.00 KiB
Memory use:     22.60 MiB
KMem use:       6.95 MiB
Link:           vethHLD9IG
 TX bytes:      1.32 KiB
 RX bytes:      2.79 KiB
 Total bytes:   4.11 KiB
 INFO runner: Preparing hooks for middleware sequence...
 INFO runner: 1 hooks defined.
 INFO runner: Running action: machine_action_ssh_ip #<Vagrant::Action::Builder:0x00000c6a35443e40>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35440c90>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::ConfigValidate:0x00000c6a35440c68>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35440c40>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35440c40>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::ConfigValidate:0x00000c6a35440c68>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35440c90>
 INFO runner: Preparing hooks for middleware sequence...
 INFO runner: 1 hooks defined.
 INFO runner: Running action: machine_action_ssh_ip #<Vagrant::Action::Warden:0x00000c6a3548a0e8>
 INFO warden: Calling IN action: #<Proc:0x00000c6a3551b868@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:120 (lambda)>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35489ff8>
 INFO warden: Calling IN action: #<Vagrant::LXC::Action::FetchIpWithLxcInfo:0x00000c6a35489fd0>
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-info", "--name", "centos-7_swarm-node1_1570708822359_18199", "-iH"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: 10.0.3.251
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a354cfeb8>
 INFO warden: Calling IN action: #<Proc:0x00000c6a354cfdf0@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:120 (lambda)>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35293f28>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35293f28>
 INFO warden: Calling OUT action: #<Proc:0x00000c6a354cfdf0@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:120 (lambda)>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a354cfeb8>
 INFO warden: Calling OUT action: #<Vagrant::LXC::Action::FetchIpWithLxcInfo:0x00000c6a35489fd0>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35489ff8>
 INFO warden: Calling OUT action: #<Proc:0x00000c6a3551b868@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:120 (lambda)>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::Call:0x00000c6a35293f78>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35293fa0>
 INFO interface: Machine: action ["ssh_ip", "end", {:target=>:"swarm-node1"}]
DEBUG ssh: Checking key permissions: /home/niek/.vagrant.d/insecure_private_key
 INFO ssh: Attempting SSH connection...
 INFO ssh: Attempting to connect to SSH...
 INFO ssh:   - Host: 10.0.3.251
 INFO ssh:   - Port: 22
 INFO ssh:   - Username: vagrant
 INFO ssh:   - Password? false
 INFO ssh:   - Key Path: ["/home/niek/.vagrant.d/insecure_private_key"]
DEBUG ssh:   - connect_opts: {:auth_methods=>["none", "hostbased", "publickey"], :config=>false, :forward_agent=>false, :send_env=>false, :keys_only=>true, :verify_host_key=>:never, :password=>nil, :port=>22, :timeout=>15, :user_known_hosts_file=>[], :verbose=>:debug, :encryption=>["aes256-ctr", "aes192-ctr", "aes128-ctr", "aes256-cbc", "aes192-cbc", "aes128-cbc", "rijndael-cbc@lysator.liu.se", "blowfish-ctr", "blowfish-cbc", "cast128-ctr", "cast128-cbc", "3des-ctr", "3des-cbc", "idea-cbc", "arcfour256", "arcfour128", "arcfour", "none"], :logger=>#<Logger:0x00000c6a355d9070 @level=0, @progname=nil, @default_formatter=#<Logger::Formatter:0x00000c6a355d9020 @datetime_format=nil>, @formatter=nil, @logdev=#<Logger::LogDevice:0x00000c6a355d8fd0 @shift_period_suffix=nil, @shift_size=nil, @shift_age=nil, @filename=nil, @dev=#<StringIO:0x00000c6a355d90c0>, @mon_mutex=#<Thread::Mutex:0x00000c6a355d8f58>, @mon_mutex_owner_object_id=6825150695400, @mon_owner=nil, @mon_count=0>>, :keys=>["/home/niek/.vagrant.d/insecure_private_key"], :remote_user=>"vagrant"}
DEBUG ssh: == Net-SSH connection debug-level log START ==
DEBUG ssh: D, [2019-10-10T14:00:34.511721 #906791] DEBUG -- net.ssh.transport.session[6351aaf576c]: establishing connection to 10.0.3.251:22

DEBUG ssh: == Net-SSH connection debug-level log END ==
 INFO ssh: SSH not ready: #<Vagrant::Errors::SSHConnectionRefused: SSH connection was refused! This usually happens if the VM failed to
boot properly. Some steps to try to fix this: First, try reloading your
VM with `vagrant reload`, since a simple restart sometimes fixes things.
If that doesn't work, destroy your VM and recreate it with a `vagrant destroy`
followed by a `vagrant up`. If that doesn't work, contact a Vagrant
maintainer (support channels listed on the website) for more assistance.>
 INFO interface: detail: Warning: Connection refused. Retrying...
 INFO interface: detail:     swarm-node1: Warning: Connection refused. Retrying...
    swarm-node1: Warning: Connection refused. Retrying...
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-info", "--name", "centos-7_swarm-node1_1570708822359_18199"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: Name:           centos-7_swarm-node1_1570708822359_18199
State:          RUNNING
PID:            907753
DEBUG subprocess: stdout: IP:             10.0.3.251
DEBUG subprocess: stdout: CPU use:        1.66 seconds
DEBUG subprocess: stdout: BlkIO use:      16.00 KiB
DEBUG subprocess: stdout: Memory use:     22.67 MiB
DEBUG subprocess: stdout: KMem use:       6.68 MiB
DEBUG subprocess: stdout: Link:           vethHLD9IG
DEBUG subprocess: stdout:  TX bytes:      1.42 KiB
 RX bytes:      2.90 KiB
 Total bytes:   4.32 KiB
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO machine: Calling action: ssh_ip on provider LXC (centos-7_swarm-node1_1570708822359_18199)
 INFO interface: Machine: action ["ssh_ip", "start", {:target=>:"swarm-node1"}]
 INFO runner: Preparing hooks for middleware sequence...
 INFO runner: 1 hooks defined.
 INFO runner: Running action: machine_action_ssh_ip #<Vagrant::Action::Builder:0x00000c6a35666420>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35672e50>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::Call:0x00000c6a35672e28>
 INFO runner: Preparing hooks for middleware sequence...
 INFO runner: 1 hooks defined.
 INFO runner: Running action: machine_action_ssh_ip #<Vagrant::Action::Builder:0x00000c6a356f6bd8>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35707988>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::ConfigValidate:0x00000c6a35707960>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35707938>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35707938>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::ConfigValidate:0x00000c6a35707960>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35707988>
 INFO runner: Preparing hooks for middleware sequence...
 INFO runner: 1 hooks defined.
 INFO runner: Running action: machine_action_ssh_ip #<Vagrant::Action::Warden:0x00000c6a35750c78>
 INFO warden: Calling IN action: #<Proc:0x00000c6a34134e58@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:120 (lambda)>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35750b88>
 INFO warden: Calling IN action: #<Vagrant::LXC::Action::FetchIpWithLxcInfo:0x00000c6a35750b60>
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-info", "--name", "centos-7_swarm-node1_1570708822359_18199", "-iH"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: 10.0.3.251
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a33e0b308>
 INFO warden: Calling IN action: #<Proc:0x00000c6a33e0ab10@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:120 (lambda)>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35672dd8>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35672dd8>
 INFO warden: Calling OUT action: #<Proc:0x00000c6a33e0ab10@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:120 (lambda)>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a33e0b308>
 INFO warden: Calling OUT action: #<Vagrant::LXC::Action::FetchIpWithLxcInfo:0x00000c6a35750b60>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35750b88>
 INFO warden: Calling OUT action: #<Proc:0x00000c6a34134e58@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:120 (lambda)>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::Call:0x00000c6a35672e28>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35672e50>
 INFO interface: Machine: action ["ssh_ip", "end", {:target=>:"swarm-node1"}]
DEBUG ssh: Checking key permissions: /home/niek/.vagrant.d/insecure_private_key
 INFO ssh: Attempting SSH connection...
 INFO ssh: Attempting to connect to SSH...
 INFO ssh:   - Host: 10.0.3.251
 INFO ssh:   - Port: 22
 INFO ssh:   - Username: vagrant
 INFO ssh:   - Password? false
 INFO ssh:   - Key Path: ["/home/niek/.vagrant.d/insecure_private_key"]
DEBUG ssh:   - connect_opts: {:auth_methods=>["none", "hostbased", "publickey"], :config=>false, :forward_agent=>false, :send_env=>false, :keys_only=>true, :verify_host_key=>:never, :password=>nil, :port=>22, :timeout=>15, :user_known_hosts_file=>[], :verbose=>:debug, :encryption=>["aes256-ctr", "aes192-ctr", "aes128-ctr", "aes256-cbc", "aes192-cbc", "aes128-cbc", "rijndael-cbc@lysator.liu.se", "blowfish-ctr", "blowfish-cbc", "cast128-ctr", "cast128-cbc", "3des-ctr", "3des-cbc", "idea-cbc", "arcfour256", "arcfour128", "arcfour", "none"], :logger=>#<Logger:0x00000c6a345a0b40 @level=0, @progname=nil, @default_formatter=#<Logger::Formatter:0x00000c6a345a0ac8 @datetime_format=nil>, @formatter=nil, @logdev=#<Logger::LogDevice:0x00000c6a345a0a28 @shift_period_suffix=nil, @shift_size=nil, @shift_age=nil, @filename=nil, @dev=#<StringIO:0x00000c6a345a0be0>, @mon_mutex=#<Thread::Mutex:0x00000c6a345a0988>, @mon_mutex_owner_object_id=6825142191380, @mon_owner=nil, @mon_count=0>>, :keys=>["/home/niek/.vagrant.d/insecure_private_key"], :remote_user=>"vagrant"}
DEBUG ssh: == Net-SSH connection debug-level log START ==
DEBUG ssh: D, [2019-10-10T14:00:34.574054 #906791] DEBUG -- net.ssh.transport.session[6351a2d5168]: establishing connection to 10.0.3.251:22
D, [2019-10-10T14:00:34.574291 #906791] DEBUG -- net.ssh.transport.session[6351a2d5168]: connection established
I, [2019-10-10T14:00:34.574345 #906791]  INFO -- net.ssh.transport.server_version[6351a2d7b84]: negotiating protocol version
D, [2019-10-10T14:00:34.574360 #906791] DEBUG -- net.ssh.transport.server_version[6351a2d7b84]: local is `SSH-2.0-Ruby/Net::SSH_5.1.0 x86_64-linux'
D, [2019-10-10T14:00:34.579892 #906791] DEBUG -- net.ssh.transport.server_version[6351a2d7b84]: remote is `SSH-2.0-OpenSSH_7.4'
I, [2019-10-10T14:00:34.580076 #906791]  INFO -- net.ssh.transport.algorithms[6351a2d9d94]: sending KEXINIT
D, [2019-10-10T14:00:34.580199 #906791] DEBUG -- socket[6351a2d4aec]: queueing packet nr 0 type 20 len 1324
D, [2019-10-10T14:00:34.580254 #906791] DEBUG -- socket[6351a2d4aec]: sent 1328 bytes
D, [2019-10-10T14:00:34.581229 #906791] DEBUG -- socket[6351a2d4aec]: read 1280 bytes
D, [2019-10-10T14:00:34.581316 #906791] DEBUG -- socket[6351a2d4aec]: received packet nr 0 type 20 len 1276
I, [2019-10-10T14:00:34.581348 #906791]  INFO -- net.ssh.transport.algorithms[6351a2d9d94]: got KEXINIT from server
I, [2019-10-10T14:00:34.581439 #906791]  INFO -- net.ssh.transport.algorithms[6351a2d9d94]: negotiating algorithms
D, [2019-10-10T14:00:34.581510 #906791] DEBUG -- net.ssh.transport.algorithms[6351a2d9d94]: negotiated:
* kex: ecdh-sha2-nistp521
* host_key: ssh-ed25519
* encryption_server: aes256-ctr
* encryption_client: aes256-ctr
* hmac_client: hmac-sha2-512
* hmac_server: hmac-sha2-512
* compression_client: none
* compression_server: none
* language_client: 
* language_server: 
D, [2019-10-10T14:00:34.581528 #906791] DEBUG -- net.ssh.transport.algorithms[6351a2d9d94]: exchanging keys
D, [2019-10-10T14:00:34.581942 #906791] DEBUG -- socket[6351a2d4aec]: queueing packet nr 1 type 30 len 148
D, [2019-10-10T14:00:34.581993 #906791] DEBUG -- socket[6351a2d4aec]: sent 152 bytes
D, [2019-10-10T14:00:34.586429 #906791] DEBUG -- socket[6351a2d4aec]: read 312 bytes
D, [2019-10-10T14:00:34.586478 #906791] DEBUG -- socket[6351a2d4aec]: received packet nr 1 type 31 len 292
D, [2019-10-10T14:00:34.587124 #906791] DEBUG -- socket[6351a2d4aec]: queueing packet nr 2 type 21 len 20
D, [2019-10-10T14:00:34.587169 #906791] DEBUG -- socket[6351a2d4aec]: sent 24 bytes
D, [2019-10-10T14:00:34.587207 #906791] DEBUG -- socket[6351a2d4aec]: received packet nr 2 type 21 len 12
D, [2019-10-10T14:00:34.587393 #906791] DEBUG -- net.ssh.authentication.session[6351a271118]: beginning authentication of `vagrant'
D, [2019-10-10T14:00:34.587450 #906791] DEBUG -- socket[6351a2d4aec]: queueing packet nr 3 type 5 len 28
D, [2019-10-10T14:00:34.587489 #906791] DEBUG -- socket[6351a2d4aec]: sent 96 bytes
D, [2019-10-10T14:00:34.587536 #906791] DEBUG -- socket[6351a2d4aec]: read 96 bytes
D, [2019-10-10T14:00:34.587577 #906791] DEBUG -- socket[6351a2d4aec]: received packet nr 3 type 6 len 28
D, [2019-10-10T14:00:34.587627 #906791] DEBUG -- net.ssh.authentication.session[6351a271118]: trying none
D, [2019-10-10T14:00:34.587697 #906791] DEBUG -- socket[6351a2d4aec]: queueing packet nr 4 type 50 len 44
D, [2019-10-10T14:00:34.587734 #906791] DEBUG -- socket[6351a2d4aec]: sent 112 bytes
D, [2019-10-10T14:00:34.588822 #906791] DEBUG -- socket[6351a2d4aec]: read 128 bytes
D, [2019-10-10T14:00:34.588892 #906791] DEBUG -- socket[6351a2d4aec]: received packet nr 4 type 51 len 60
D, [2019-10-10T14:00:34.588929 #906791] DEBUG -- net.ssh.authentication.session[6351a271118]: allowed methods: publickey,gssapi-keyex,gssapi-with-mic,password
D, [2019-10-10T14:00:34.588958 #906791] DEBUG -- net.ssh.authentication.methods.none[6351a277b08]: none failed
D, [2019-10-10T14:00:34.588988 #906791] DEBUG -- net.ssh.authentication.session[6351a271118]: trying publickey
D, [2019-10-10T14:00:34.589165 #906791] DEBUG -- net.ssh.authentication.agent[63519ef71b8]: connecting to ssh-agent
D, [2019-10-10T14:00:34.589245 #906791] DEBUG -- net.ssh.authentication.agent[63519ef71b8]: sending agent request 1 len 44
D, [2019-10-10T14:00:34.589536 #906791] DEBUG -- net.ssh.authentication.agent[63519ef71b8]: received agent packet 5 len 1
D, [2019-10-10T14:00:34.589567 #906791] DEBUG -- net.ssh.authentication.agent[63519ef71b8]: sending agent request 11 len 0
D, [2019-10-10T14:00:34.589755 #906791] DEBUG -- net.ssh.authentication.agent[63519ef71b8]: received agent packet 12 len 611
D, [2019-10-10T14:00:34.589944 #906791] DEBUG -- net.ssh.authentication.methods.publickey[6351a28201c]: trying publickey (dd:3b:b8:2e:85:04:06:e9:ab:ff:a8:0a:c0:04:6e:d6)
D, [2019-10-10T14:00:34.590029 #906791] DEBUG -- socket[6351a2d4aec]: queueing packet nr 5 type 50 len 348
D, [2019-10-10T14:00:34.590095 #906791] DEBUG -- socket[6351a2d4aec]: sent 416 bytes
D, [2019-10-10T14:00:34.590572 #906791] DEBUG -- socket[6351a2d4aec]: read 368 bytes
D, [2019-10-10T14:00:34.590638 #906791] DEBUG -- socket[6351a2d4aec]: received packet nr 5 type 60 len 300
D, [2019-10-10T14:00:34.591799 #906791] DEBUG -- socket[6351a2d4aec]: queueing packet nr 6 type 50 len 620
D, [2019-10-10T14:00:34.591847 #906791] DEBUG -- socket[6351a2d4aec]: sent 688 bytes
D, [2019-10-10T14:00:34.592698 #906791] DEBUG -- socket[6351a2d4aec]: read 80 bytes
D, [2019-10-10T14:00:34.592794 #906791] DEBUG -- socket[6351a2d4aec]: received packet nr 6 type 52 len 12
D, [2019-10-10T14:00:34.592847 #906791] DEBUG -- net.ssh.authentication.methods.publickey[6351a28201c]: publickey succeeded (dd:3b:b8:2e:85:04:06:e9:ab:ff:a8:0a:c0:04:6e:d6)

DEBUG ssh: == Net-SSH connection debug-level log END ==
DEBUG ssh: Checking whether SSH is ready...
DEBUG ssh: Re-using SSH connection.
 INFO ssh: SSH is ready!
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute:  (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 0
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-info", "--name", "centos-7_swarm-node1_1570708822359_18199"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: Name:           centos-7_swarm-node1_1570708822359_18199
DEBUG subprocess: stdout: State:          RUNNING
PID:            907753
DEBUG subprocess: stdout: IP:             10.0.3.251
DEBUG subprocess: stdout: CPU use:        1.80 seconds
DEBUG subprocess: stdout: BlkIO use:      104.00 KiB
DEBUG subprocess: stdout: Memory use:     26.03 MiB
DEBUG subprocess: stdout: KMem use:       7.95 MiB
DEBUG subprocess: stdout: Link:           vethHLD9IG
DEBUG subprocess: stdout:  TX bytes:      10.76 KiB
 RX bytes:      11.22 KiB
 Total bytes:   21.99 KiB
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO machine: Calling action: ssh_ip on provider LXC (centos-7_swarm-node1_1570708822359_18199)
 INFO interface: Machine: action ["ssh_ip", "start", {:target=>:"swarm-node1"}]
 INFO runner: Preparing hooks for middleware sequence...
 INFO runner: 1 hooks defined.
 INFO runner: Running action: machine_action_ssh_ip #<Vagrant::Action::Builder:0x00000c6a349c03b0>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a349ef200>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::Call:0x00000c6a349ef1d8>
 INFO runner: Preparing hooks for middleware sequence...
 INFO runner: 1 hooks defined.
 INFO runner: Running action: machine_action_ssh_ip #<Vagrant::Action::Builder:0x00000c6a350bfb20>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a350cfa70>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::ConfigValidate:0x00000c6a350cfa48>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a350cf9f8>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a350cf9f8>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::ConfigValidate:0x00000c6a350cfa48>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a350cfa70>
 INFO runner: Preparing hooks for middleware sequence...
 INFO runner: 1 hooks defined.
 INFO runner: Running action: machine_action_ssh_ip #<Vagrant::Action::Warden:0x00000c6a351330c0>
 INFO warden: Calling IN action: #<Proc:0x00000c6a351e8718@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:120 (lambda)>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35132f08>
 INFO warden: Calling IN action: #<Vagrant::LXC::Action::FetchIpWithLxcInfo:0x00000c6a35132ee0>
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-info", "--name", "centos-7_swarm-node1_1570708822359_18199", "-iH"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: 10.0.3.251
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35189a38>
 INFO warden: Calling IN action: #<Proc:0x00000c6a35189998@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:120 (lambda)>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a349ef188>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a349ef188>
 INFO warden: Calling OUT action: #<Proc:0x00000c6a35189998@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:120 (lambda)>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35189a38>
 INFO warden: Calling OUT action: #<Vagrant::LXC::Action::FetchIpWithLxcInfo:0x00000c6a35132ee0>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35132f08>
 INFO warden: Calling OUT action: #<Proc:0x00000c6a351e8718@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:120 (lambda)>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::Call:0x00000c6a349ef1d8>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a349ef200>
 INFO interface: Machine: action ["ssh_ip", "end", {:target=>:"swarm-node1"}]
DEBUG ssh: Checking key permissions: /home/niek/.vagrant.d/insecure_private_key
 INFO interface: detail: 
Vagrant insecure key detected. Vagrant will automatically replace
this with a newly generated keypair for better security.
 INFO interface: detail:     swarm-node1: 
    swarm-node1: Vagrant insecure key detected. Vagrant will automatically replace
    swarm-node1: this with a newly generated keypair for better security.
    swarm-node1: 
    swarm-node1: Vagrant insecure key detected. Vagrant will automatically replace
    swarm-node1: this with a newly generated keypair for better security.
DEBUG ssh: Checking whether SSH is ready...
DEBUG ssh: Re-using SSH connection.
 INFO ssh: SSH is ready!
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute:  (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 0
 INFO guest: Autodetecting host type for [#<Vagrant::Machine: swarm-node1 (Vagrant::LXC::Provider)>]
DEBUG guest: Trying: atomic
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: grep 'ostree=' /proc/cmdline (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 1
DEBUG guest: Trying: trisquel
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: [ -x /usr/bin/lsb_release ] && /usr/bin/lsb_release -i 2>/dev/null | grep Trisquel (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 1
DEBUG guest: Trying: elementary
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: if test -r /etc/os-release; then
source /etc/os-release && test 'xelementary' = "x$ID" && exit
fi
if test -x /usr/bin/lsb_release; then
/usr/bin/lsb_release -i 2>/dev/null | grep -qi 'elementary' && exit
fi
if test -r /etc/issue; then
cat /etc/issue | grep -qi 'elementary' && exit
fi
exit 1
 (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 1
DEBUG guest: Trying: mint
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: if test -r /etc/os-release; then
source /etc/os-release && test 'xLinux Mint' = "x$ID" && exit
fi
if test -x /usr/bin/lsb_release; then
/usr/bin/lsb_release -i 2>/dev/null | grep -qi 'Linux Mint' && exit
fi
if test -r /etc/issue; then
cat /etc/issue | grep -qi 'Linux Mint' && exit
fi
exit 1
 (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 1
DEBUG guest: Trying: fedora
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: grep 'Fedora release' /etc/redhat-release (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 1
DEBUG guest: Trying: dragonflybsd
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: uname -s | grep -i 'DragonFly' (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 1
DEBUG guest: Trying: pld
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: cat /etc/pld-release (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: stderr: cat: /etc/pld-release: No such file or directory

DEBUG ssh: Exit status: 1
DEBUG guest: Trying: amazon
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: grep 'Amazon Linux' /etc/os-release (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 1
DEBUG guest: Trying: alt
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: cat /etc/altlinux-release (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: stderr: cat: /etc/altlinux-release: No such file or directory

DEBUG ssh: Exit status: 1
DEBUG guest: Trying: kali
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: if test -r /etc/os-release; then
source /etc/os-release && test 'xkali' = "x$ID" && exit
fi
if test -x /usr/bin/lsb_release; then
/usr/bin/lsb_release -i 2>/dev/null | grep -qi 'kali' && exit
fi
if test -r /etc/issue; then
cat /etc/issue | grep -qi 'kali' && exit
fi
exit 1
 (sudo=false)
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-info", "--name", "centos-7_swarm-node1_1570708822359_18199"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 1
DEBUG guest: Trying: ubuntu
DEBUG subprocess: stdout: Name:           centos-7_swarm-node1_1570708822359_18199
State:          RUNNING
DEBUG subprocess: stdout: PID:            907753
DEBUG subprocess: stdout: IP:             10.0.3.251
DEBUG subprocess: stdout: CPU use:        2.26 seconds
DEBUG subprocess: stdout: BlkIO use:      104.00 KiB
DEBUG subprocess: stdout: Memory use:     26.90 MiB
DEBUG subprocess: stdout: KMem use:       8.02 MiB
DEBUG subprocess: stdout: Link:           vethHLD9IG
 TX bytes:      49.75 KiB
 RX bytes:      42.62 KiB
 Total bytes:   92.38 KiB
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: if test -r /etc/os-release; then
source /etc/os-release && test 'xubuntu' = "x$ID" && exit
fi
if test -x /usr/bin/lsb_release; then
/usr/bin/lsb_release -i 2>/dev/null | grep -qi 'ubuntu' && exit
fi
if test -r /etc/issue; then
cat /etc/issue | grep -qi 'ubuntu' && exit
fi
exit 1
 (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 1
DEBUG guest: Trying: funtoo
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: grep Funtoo /etc/gentoo-release (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: stderr: grep: /etc/gentoo-release: No such file or directory

DEBUG ssh: Exit status: 2
DEBUG guest: Trying: freebsd
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: uname -s | grep 'FreeBSD' (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 1
DEBUG guest: Trying: tinycore
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: if test -r /etc/os-release; then
source /etc/os-release && test 'xCore Linux' = "x$ID" && exit
fi
if test -x /usr/bin/lsb_release; then
/usr/bin/lsb_release -i 2>/dev/null | grep -qi 'Core Linux' && exit
fi
if test -r /etc/issue; then
cat /etc/issue | grep -qi 'Core Linux' && exit
fi
exit 1
 (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 1
DEBUG guest: Trying: smartos
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: cat /etc/release | grep -i SmartOS (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: stderr: cat: /etc/release: No such file or directory

DEBUG ssh: Exit status: 1
DEBUG guest: Trying: gentoo
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: grep Gentoo /etc/gentoo-release (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: stderr: grep: /etc/gentoo-release: No such file or directory

DEBUG ssh: Exit status: 2
DEBUG guest: Trying: coreos
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: (cat /etc/os-release | grep ID=coreos) || (cat /etc/os-release | grep -E 'ID_LIKE=.*coreos.*') (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 1
DEBUG guest: Trying: darwin
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: uname -s | grep 'Darwin' (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 1
DEBUG guest: Trying: nixos
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: test -f /run/current-system/nixos-version (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 1
DEBUG guest: Trying: slackware
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: cat /etc/slackware-version (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: stderr: cat: /etc/slackware-version: No such file or directory

DEBUG ssh: Exit status: 1
DEBUG guest: Trying: debian
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: if test -r /etc/os-release; then
source /etc/os-release && test 'xdebian' = "x$ID" && exit
fi
if test -x /usr/bin/lsb_release; then
/usr/bin/lsb_release -i 2>/dev/null | grep -qi 'debian' && exit
fi
if test -r /etc/issue; then
cat /etc/issue | grep -qi 'debian' && exit
fi
exit 1
 (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 1
DEBUG guest: Trying: suse
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: test -f /etc/SuSE-release || grep -q SUSE /etc/os-release (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 1
DEBUG guest: Trying: solaris11
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: grep 'Solaris 11' /etc/release (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: stderr: grep: /etc/release: No such file or directory

DEBUG ssh: Exit status: 2
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: uname -sr | grep 'SunOS 5.11' (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 1
DEBUG guest: Trying: openbsd
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: uname -s | grep 'OpenBSD' (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 1
DEBUG guest: Trying: redhat
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: cat /etc/redhat-release (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 0
 INFO guest: Detected: redhat!
DEBUG guest: Searching for cap: insert_public_key
DEBUG guest: Checking in: redhat
DEBUG guest: Checking in: linux
DEBUG guest: Found cap: insert_public_key in linux
DEBUG ssh: Checking whether SSH is ready...
DEBUG ssh: Re-using SSH connection.
 INFO ssh: SSH is ready!
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute:  (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 0
DEBUG guest: Searching for cap: remove_public_key
DEBUG guest: Checking in: redhat
DEBUG guest: Checking in: linux
DEBUG guest: Found cap: remove_public_key in linux
 INFO ssh: Inserting key to avoid password: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC33dQjvmhemomln76+hpjUnsZD/a2umvSey0S3JjmSgjec/mHPGwnACfGePAPg9aRoW00WBsU1G9wOb1H5I/wWwTJN1cFw8eraGq4e2qRfL+2myYQP8j/JdC3x8eSrZQAOZT7VD0o8uGcc2iA+Fcl3i1S+zEbM05Kt60R5Oqstdl+oQjNeuVMcYaJ8nveBri6IVkQhZqXLRCylpcm9ByalVSqxd2+KEqe2pffOZ/46bZTQbAv/c+OOiOqEL1N6+pcr86+rIqelecQvXQbA46ZgrhKgxZTUmoqd+TO+hQ1jdgOB9XZwaEqUuAtDSZx3d7n850WJFJL1+kpHlq2g52hj vagrant
 INFO interface: detail: 
Inserting generated public key within guest...
 INFO interface: detail:     swarm-node1: 
    swarm-node1: Inserting generated public key within guest...
    swarm-node1: 
    swarm-node1: Inserting generated public key within guest...
DEBUG ssh: Checking whether SSH is ready...
DEBUG ssh: Re-using SSH connection.
 INFO ssh: SSH is ready!
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-info", "--name", "centos-7_swarm-node1_1570708822359_18199"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: Name:           centos-7_swarm-node1_1570708822359_18199
DEBUG subprocess: stdout: State:          RUNNING
PID:            907753
DEBUG subprocess: stdout: IP:             10.0.3.251
DEBUG subprocess: stdout: CPU use:        2.95 seconds
BlkIO use:      104.00 KiB
DEBUG subprocess: stdout: Memory use:     26.23 MiB
DEBUG subprocess: stdout: KMem use:       8.28 MiB
Link:           vethHLD9IG
 TX bytes:      110.54 KiB
DEBUG subprocess: stdout:  RX bytes:      89.97 KiB
 Total bytes:   200.51 KiB
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute:  (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 0
DEBUG guest: Searching for cap: insert_public_key
DEBUG guest: Checking in: redhat
DEBUG guest: Checking in: linux
DEBUG guest: Found cap: insert_public_key in linux
 INFO guest: Execute capability: insert_public_key [#<Vagrant::Machine: swarm-node1 (Vagrant::LXC::Provider)>, "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC33dQjvmhemomln76+hpjUnsZD/a2umvSey0S3JjmSgjec/mHPGwnACfGePAPg9aRoW00WBsU1G9wOb1H5I/wWwTJN1cFw8eraGq4e2qRfL+2myYQP8j/JdC3x8eSrZQAOZT7VD0o8uGcc2iA+Fcl3i1S+zEbM05Kt60R5Oqstdl+oQjNeuVMcYaJ8nveBri6IVkQhZqXLRCylpcm9ByalVSqxd2+KEqe2pffOZ/46bZTQbAv/c+OOiOqEL1N6+pcr86+rIqelecQvXQbA46ZgrhKgxZTUmoqd+TO+hQ1jdgOB9XZwaEqUuAtDSZx3d7n850WJFJL1+kpHlq2g52hj vagrant"] (redhat)
DEBUG ssh: Uploading: /tmp/vagrant-linux-insert-public-key20191010-906791-10u1a1w to /tmp/vagrant-insert-pubkey-1570708836
DEBUG ssh: Re-using SSH connection.
DEBUG ssh: Ensuring remote directory exists for destination upload
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: mkdir -p "/tmp" (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 0
DEBUG ssh: Uploading file /tmp/vagrant-linux-insert-public-key20191010-906791-10u1a1w to remote /tmp/vagrant-insert-pubkey-1570708836
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: mkdir -p ~/.ssh
chmod 0700 ~/.ssh
cat '/tmp/vagrant-insert-pubkey-1570708836' >> ~/.ssh/authorized_keys && chmod 0600 ~/.ssh/authorized_keys
result=$?
rm -f '/tmp/vagrant-insert-pubkey-1570708836'
exit $result
 (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 0
DEBUG host: Searching for cap: set_ssh_key_permissions
DEBUG host: Checking in: arch
DEBUG host: Checking in: linux
DEBUG host: Found cap: set_ssh_key_permissions in linux
DEBUG host: Searching for cap: set_ssh_key_permissions
DEBUG host: Checking in: arch
DEBUG host: Checking in: linux
DEBUG host: Found cap: set_ssh_key_permissions in linux
 INFO host: Execute capability: set_ssh_key_permissions [#<Vagrant::Environment: /home/niek/vagrant/lxc/centos-7>, #<Pathname:/home/niek/vagrant/lxc/centos-7/.vagrant/machines/swarm-node1/lxc/private_key>] (arch)
 INFO interface: detail: Removing insecure key from the guest if it's present...
 INFO interface: detail:     swarm-node1: Removing insecure key from the guest if it's present...
    swarm-node1: Removing insecure key from the guest if it's present...
DEBUG ssh: Checking whether SSH is ready...
DEBUG ssh: Re-using SSH connection.
 INFO ssh: SSH is ready!
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute:  (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 0
DEBUG guest: Searching for cap: remove_public_key
DEBUG guest: Checking in: redhat
DEBUG guest: Checking in: linux
DEBUG guest: Found cap: remove_public_key in linux
 INFO guest: Execute capability: remove_public_key [#<Vagrant::Machine: swarm-node1 (Vagrant::LXC::Provider)>, "ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA6NF8iallvQVp22WDkTkyrtvp9eWW6A8YVr+kz4TjGYe7gHzIw+niNltGEFHzD8+v1I2YJ6oXevct1YeS0o9HZyN1Q9qgCgzUFtdOKLv6IedplqoPkcmF0aYet2PkEDo3MlTBckFXPITAMzF8dJSIFo9D8HfdOV0IAdx4O7PtixWKn5y2hMNG0zQPyUecp4pzC6kivAIhyfHilFR61RGL+GPXQ2MWZWFYbAGjyiYJnAmCP3NOTd0jMZEnDkbUvxhMmBYSdETk1rRgm+R4LOzFUGaHqHDLKLX+FIPKcF96hrucXzcWyLbIbEgE98OHlnVYCzRdK8jlqm8tehUc9c9WhQ== vagrant insecure public key"] (redhat)
DEBUG ssh: Uploading: /tmp/vagrant-linux-remove-public-key20191010-906791-1251d2s to /tmp/vagrant-remove-pubkey-1570708836
DEBUG ssh: Re-using SSH connection.
DEBUG ssh: Ensuring remote directory exists for destination upload
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: mkdir -p "/tmp" (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 0
DEBUG ssh: Uploading file /tmp/vagrant-linux-remove-public-key20191010-906791-1251d2s to remote /tmp/vagrant-remove-pubkey-1570708836
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: if test -f ~/.ssh/authorized_keys; then
              grep -v -x -f '/tmp/vagrant-remove-pubkey-1570708836' ~/.ssh/authorized_keys > ~/.ssh/authorized_keys.tmp
              mv ~/.ssh/authorized_keys.tmp ~/.ssh/authorized_keys && chmod 0600 ~/.ssh/authorized_keys
              result=$?
            fi
            rm -f '/tmp/vagrant-remove-pubkey-1570708836'
            exit $result
 (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 0
 INFO interface: detail: Key inserted! Disconnecting and reconnecting using new SSH key...
 INFO interface: detail:     swarm-node1: Key inserted! Disconnecting and reconnecting using new SSH key...
    swarm-node1: Key inserted! Disconnecting and reconnecting using new SSH key...
DEBUG ssh: Checking whether SSH is ready...
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-info", "--name", "centos-7_swarm-node1_1570708822359_18199"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: Name:           centos-7_swarm-node1_1570708822359_18199
State:          RUNNING
PID:            907753
DEBUG subprocess: stdout: IP:             10.0.3.251
DEBUG subprocess: stdout: CPU use:        3.25 seconds
DEBUG subprocess: stdout: BlkIO use:      112.00 KiB
DEBUG subprocess: stdout: Memory use:     23.77 MiB
DEBUG subprocess: stdout: KMem use:       7.23 MiB
DEBUG subprocess: stdout: Link:           vethHLD9IG
DEBUG subprocess: stdout:  TX bytes:      138.70 KiB
 RX bytes:      113.34 KiB
 Total bytes:   252.04 KiB
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO machine: Calling action: ssh_ip on provider LXC (centos-7_swarm-node1_1570708822359_18199)
 INFO interface: Machine: action ["ssh_ip", "start", {:target=>:"swarm-node1"}]
 INFO runner: Preparing hooks for middleware sequence...
 INFO runner: 1 hooks defined.
 INFO runner: Running action: machine_action_ssh_ip #<Vagrant::Action::Builder:0x00000c6a35154ef0>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35177090>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::Call:0x00000c6a35176ff0>
 INFO runner: Preparing hooks for middleware sequence...
 INFO runner: 1 hooks defined.
 INFO runner: Running action: machine_action_ssh_ip #<Vagrant::Action::Builder:0x00000c6a35221f18>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a3522c918>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::ConfigValidate:0x00000c6a3522c8a0>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a3522c878>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a3522c878>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::ConfigValidate:0x00000c6a3522c8a0>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a3522c918>
 INFO runner: Preparing hooks for middleware sequence...
 INFO runner: 1 hooks defined.
 INFO runner: Running action: machine_action_ssh_ip #<Vagrant::Action::Warden:0x00000c6a3529fee0>
 INFO warden: Calling IN action: #<Proc:0x00000c6a354576e8@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:120 (lambda)>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a3529fdf0>
 INFO warden: Calling IN action: #<Vagrant::LXC::Action::FetchIpWithLxcInfo:0x00000c6a3529fdc8>
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "lxc-info", "--name", "centos-7_swarm-node1_1570708822359_18199", "-iH"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stdout: 10.0.3.251
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a352ff020>
 INFO warden: Calling IN action: #<Proc:0x00000c6a352fefa8@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:120 (lambda)>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35176fa0>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35176fa0>
 INFO warden: Calling OUT action: #<Proc:0x00000c6a352fefa8@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:120 (lambda)>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a352ff020>
 INFO warden: Calling OUT action: #<Vagrant::LXC::Action::FetchIpWithLxcInfo:0x00000c6a3529fdc8>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a3529fdf0>
 INFO warden: Calling OUT action: #<Proc:0x00000c6a354576e8@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:120 (lambda)>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::Call:0x00000c6a35176ff0>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35177090>
 INFO interface: Machine: action ["ssh_ip", "end", {:target=>:"swarm-node1"}]
DEBUG ssh: Checking key permissions: /home/niek/vagrant/lxc/centos-7/.vagrant/machines/swarm-node1/lxc/private_key
 INFO ssh: Attempting SSH connection...
 INFO ssh: Attempting to connect to SSH...
 INFO ssh:   - Host: 10.0.3.251
 INFO ssh:   - Port: 22
 INFO ssh:   - Username: vagrant
 INFO ssh:   - Password? false
 INFO ssh:   - Key Path: ["/home/niek/vagrant/lxc/centos-7/.vagrant/machines/swarm-node1/lxc/private_key"]
DEBUG ssh:   - connect_opts: {:auth_methods=>["none", "hostbased", "publickey"], :config=>false, :forward_agent=>false, :send_env=>false, :keys_only=>true, :verify_host_key=>:never, :password=>nil, :port=>22, :timeout=>15, :user_known_hosts_file=>[], :verbose=>:debug, :encryption=>["aes256-ctr", "aes192-ctr", "aes128-ctr", "aes256-cbc", "aes192-cbc", "aes128-cbc", "rijndael-cbc@lysator.liu.se", "blowfish-ctr", "blowfish-cbc", "cast128-ctr", "cast128-cbc", "3des-ctr", "3des-cbc", "idea-cbc", "arcfour256", "arcfour128", "arcfour", "none"], :logger=>#<Logger:0x00000c6a3551b4d0 @level=0, @progname=nil, @default_formatter=#<Logger::Formatter:0x00000c6a3551b480 @datetime_format=nil>, @formatter=nil, @logdev=#<Logger::LogDevice:0x00000c6a3551b430 @shift_period_suffix=nil, @shift_size=nil, @shift_age=nil, @filename=nil, @dev=#<StringIO:0x00000c6a3551b520>, @mon_mutex=#<Thread::Mutex:0x00000c6a3551b3e0>, @mon_mutex_owner_object_id=6825150306840, @mon_owner=nil, @mon_count=0>>, :keys=>["/home/niek/vagrant/lxc/centos-7/.vagrant/machines/swarm-node1/lxc/private_key"], :remote_user=>"vagrant"}
DEBUG ssh: == Net-SSH connection debug-level log START ==
DEBUG ssh: D, [2019-10-10T14:00:36.995754 #906791] DEBUG -- net.ssh.transport.session[6351aa8c7e4]: establishing connection to 10.0.3.251:22
D, [2019-10-10T14:00:36.996018 #906791] DEBUG -- net.ssh.transport.session[6351aa8c7e4]: connection established
I, [2019-10-10T14:00:36.996086 #906791]  INFO -- net.ssh.transport.server_version[6351aa8c140]: negotiating protocol version
D, [2019-10-10T14:00:36.996104 #906791] DEBUG -- net.ssh.transport.server_version[6351aa8c140]: local is `SSH-2.0-Ruby/Net::SSH_5.1.0 x86_64-linux'
D, [2019-10-10T14:00:37.003232 #906791] DEBUG -- net.ssh.transport.server_version[6351aa8c140]: remote is `SSH-2.0-OpenSSH_7.4'
I, [2019-10-10T14:00:37.003410 #906791]  INFO -- net.ssh.transport.algorithms[6351aa95754]: sending KEXINIT
D, [2019-10-10T14:00:37.003559 #906791] DEBUG -- socket[6351aa8c5b4]: queueing packet nr 0 type 20 len 1324
D, [2019-10-10T14:00:37.003666 #906791] DEBUG -- socket[6351aa8c5b4]: sent 1328 bytes
D, [2019-10-10T14:00:37.005167 #906791] DEBUG -- socket[6351aa8c5b4]: read 1280 bytes
D, [2019-10-10T14:00:37.005255 #906791] DEBUG -- socket[6351aa8c5b4]: received packet nr 0 type 20 len 1276
I, [2019-10-10T14:00:37.005281 #906791]  INFO -- net.ssh.transport.algorithms[6351aa95754]: got KEXINIT from server
I, [2019-10-10T14:00:37.005361 #906791]  INFO -- net.ssh.transport.algorithms[6351aa95754]: negotiating algorithms
D, [2019-10-10T14:00:37.005443 #906791] DEBUG -- net.ssh.transport.algorithms[6351aa95754]: negotiated:
* kex: ecdh-sha2-nistp521
* host_key: ssh-ed25519
* encryption_server: aes256-ctr
* encryption_client: aes256-ctr
* hmac_client: hmac-sha2-512
* hmac_server: hmac-sha2-512
* compression_client: none
* compression_server: none
* language_client: 
* language_server: 
D, [2019-10-10T14:00:37.005464 #906791] DEBUG -- net.ssh.transport.algorithms[6351aa95754]: exchanging keys
D, [2019-10-10T14:00:37.005838 #906791] DEBUG -- socket[6351aa8c5b4]: queueing packet nr 1 type 30 len 148
D, [2019-10-10T14:00:37.005893 #906791] DEBUG -- socket[6351aa8c5b4]: sent 152 bytes
D, [2019-10-10T14:00:37.010886 #906791] DEBUG -- socket[6351aa8c5b4]: read 312 bytes
D, [2019-10-10T14:00:37.010962 #906791] DEBUG -- socket[6351aa8c5b4]: received packet nr 1 type 31 len 292
D, [2019-10-10T14:00:37.011574 #906791] DEBUG -- socket[6351aa8c5b4]: queueing packet nr 2 type 21 len 20
D, [2019-10-10T14:00:37.011621 #906791] DEBUG -- socket[6351aa8c5b4]: sent 24 bytes
D, [2019-10-10T14:00:37.011651 #906791] DEBUG -- socket[6351aa8c5b4]: received packet nr 2 type 21 len 12
D, [2019-10-10T14:00:37.011792 #906791] DEBUG -- net.ssh.authentication.session[6351aaaf62c]: beginning authentication of `vagrant'
D, [2019-10-10T14:00:37.011845 #906791] DEBUG -- socket[6351aa8c5b4]: queueing packet nr 3 type 5 len 28
D, [2019-10-10T14:00:37.011879 #906791] DEBUG -- socket[6351aa8c5b4]: sent 96 bytes
D, [2019-10-10T14:00:37.011906 #906791] DEBUG -- socket[6351aa8c5b4]: read 96 bytes
D, [2019-10-10T14:00:37.011941 #906791] DEBUG -- socket[6351aa8c5b4]: received packet nr 3 type 6 len 28
D, [2019-10-10T14:00:37.011980 #906791] DEBUG -- net.ssh.authentication.session[6351aaaf62c]: trying none
D, [2019-10-10T14:00:37.012038 #906791] DEBUG -- socket[6351aa8c5b4]: queueing packet nr 4 type 50 len 44
D, [2019-10-10T14:00:37.012070 #906791] DEBUG -- socket[6351aa8c5b4]: sent 112 bytes
D, [2019-10-10T14:00:37.013019 #906791] DEBUG -- socket[6351aa8c5b4]: read 128 bytes
D, [2019-10-10T14:00:37.013096 #906791] DEBUG -- socket[6351aa8c5b4]: received packet nr 4 type 51 len 60
D, [2019-10-10T14:00:37.013130 #906791] DEBUG -- net.ssh.authentication.session[6351aaaf62c]: allowed methods: publickey,gssapi-keyex,gssapi-with-mic,password
D, [2019-10-10T14:00:37.013156 #906791] DEBUG -- net.ssh.authentication.methods.none[6351aaae3d0]: none failed
D, [2019-10-10T14:00:37.013182 #906791] DEBUG -- net.ssh.authentication.session[6351aaaf62c]: trying publickey
D, [2019-10-10T14:00:37.013339 #906791] DEBUG -- net.ssh.authentication.agent[6351aab4ca8]: connecting to ssh-agent
D, [2019-10-10T14:00:37.013411 #906791] DEBUG -- net.ssh.authentication.agent[6351aab4ca8]: sending agent request 1 len 44
D, [2019-10-10T14:00:37.013622 #906791] DEBUG -- net.ssh.authentication.agent[6351aab4ca8]: received agent packet 5 len 1
D, [2019-10-10T14:00:37.013649 #906791] DEBUG -- net.ssh.authentication.agent[6351aab4ca8]: sending agent request 11 len 0
D, [2019-10-10T14:00:37.013777 #906791] DEBUG -- net.ssh.authentication.agent[6351aab4ca8]: received agent packet 12 len 611
D, [2019-10-10T14:00:37.013944 #906791] DEBUG -- net.ssh.authentication.methods.publickey[6351aab4f64]: trying publickey (55:84:29:c2:80:c8:96:cd:09:71:d9:08:2b:42:e9:60)
D, [2019-10-10T14:00:37.014019 #906791] DEBUG -- socket[6351aa8c5b4]: queueing packet nr 5 type 50 len 348
D, [2019-10-10T14:00:37.014074 #906791] DEBUG -- socket[6351aa8c5b4]: sent 416 bytes
D, [2019-10-10T14:00:37.014722 #906791] DEBUG -- socket[6351aa8c5b4]: read 368 bytes
D, [2019-10-10T14:00:37.014783 #906791] DEBUG -- socket[6351aa8c5b4]: received packet nr 5 type 60 len 300
D, [2019-10-10T14:00:37.015920 #906791] DEBUG -- socket[6351aa8c5b4]: queueing packet nr 6 type 50 len 620
D, [2019-10-10T14:00:37.015965 #906791] DEBUG -- socket[6351aa8c5b4]: sent 688 bytes
D, [2019-10-10T14:00:37.016735 #906791] DEBUG -- socket[6351aa8c5b4]: read 80 bytes
D, [2019-10-10T14:00:37.016806 #906791] DEBUG -- socket[6351aa8c5b4]: received packet nr 6 type 52 len 12
D, [2019-10-10T14:00:37.016837 #906791] DEBUG -- net.ssh.authentication.methods.publickey[6351aab4f64]: publickey succeeded (55:84:29:c2:80:c8:96:cd:09:71:d9:08:2b:42:e9:60)

DEBUG ssh: == Net-SSH connection debug-level log END ==
 INFO ssh: SSH is ready!
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute:  (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 0
 INFO interface: output: Machine booted and ready!
 INFO interface: output: ==> swarm-node1: Machine booted and ready!
==> swarm-node1: Machine booted and ready!
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35139bc8>
 INFO warden: Calling IN action: #<Proc:0x00000c6a35139b50@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:120 (lambda)>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35177810>
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35177798>
 INFO warden: Calling IN action: #<VagrantPlugins::CommandUp::StoreBoxMetadata:0x00000c6a35177770>
DEBUG storeboxmetadata: Writing box metadata file to /home/niek/vagrant/lxc/centos-7/.vagrant/machines/swarm-node1/lxc/box_meta
 INFO warden: Calling IN action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a351aae68>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a351aae68>
 INFO warden: Calling OUT action: #<VagrantPlugins::CommandUp::StoreBoxMetadata:0x00000c6a35177770>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35177798>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35177810>
 INFO warden: Calling OUT action: #<Proc:0x00000c6a35139b50@/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:120 (lambda)>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35139bc8>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::WaitForCommunicator:0x00000c6a35139c18>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35139c40>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35139cb8>
 INFO warden: Calling OUT action: #<Vagrant::LXC::Action::Boot:0x00000c6a35139ce0>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::BeforeTriggerAction:0x00000c6a35139d08>
 INFO warden: Calling OUT action: #<Vagrant::Action::Builtin::AfterTriggerAction:0x00000c6a35139d80>
 INFO interface: output: Setting up private networks...
 INFO interface: output: ==> swarm-node1: Setting up private networks...
==> swarm-node1: Setting up private networks...
 INFO driver: Configuring network interface for centos-7_swarm-node1_1570708822359_18199 using 192.168.33.11 and bridge vlxcbr1
 INFO driver: Checking whether bridge vlxcbr1 exists
 INFO driver: Creating the bridge vlxcbr1
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "brctl", "addbr", "vlxcbr1"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO driver: Checking whether the bridge vlxcbr1 has an IP
 INFO driver: Adding 192.168.33.254 to the bridge vlxcbr1
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "ip", "addr", "add", "192.168.33.254/24", "dev", "vlxcbr1"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "ip", "link", "set", "vlxcbr1", "up"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/bin/env", "/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/scripts/pipework", "vlxcbr1", "centos-7_swarm-node1_1570708822359_18199", "192.168.33.11/24"]
 INFO subprocess: Command not in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stderr: Found more than one container matching centos-7_swarm-node1_1570708822359_18199.
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 1
ERROR warden: Error occurred: There was an error executing ["sudo", "/usr/bin/env", "/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/scripts/pipework", "vlxcbr1", "centos-7_swarm-node1_1570708822359_18199", "192.168.33.11/24"]

For more information on the failure, enable detailed logging by setting
the environment variable VAGRANT_LOG to DEBUG.
 INFO warden: Beginning recovery process...
 INFO warden: Calling recover: #<Vagrant::Action::Builtin::HandleForwardedPortCollisions:0x00000c6a348c6928>
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
ERROR warden: Error occurred: There was an error executing ["sudo", "/usr/bin/env", "/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/scripts/pipework", "vlxcbr1", "centos-7_swarm-node1_1570708822359_18199", "192.168.33.11/24"]

For more information on the failure, enable detailed logging by setting
the environment variable VAGRANT_LOG to DEBUG.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
ERROR warden: Error occurred: There was an error executing ["sudo", "/usr/bin/env", "/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/scripts/pipework", "vlxcbr1", "centos-7_swarm-node1_1570708822359_18199", "192.168.33.11/24"]

For more information on the failure, enable detailed logging by setting
the environment variable VAGRANT_LOG to DEBUG.
 INFO warden: Beginning recovery process...
 INFO warden: Calling recover: #<Vagrant::Action::Builtin::Call:0x00000c6a35177860>
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Calling recover: #<Vagrant::Action::Builtin::Call:0x00000c6a35133d18>
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
ERROR warden: Error occurred: There was an error executing ["sudo", "/usr/bin/env", "/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/scripts/pipework", "vlxcbr1", "centos-7_swarm-node1_1570708822359_18199", "192.168.33.11/24"]

For more information on the failure, enable detailed logging by setting
the environment variable VAGRANT_LOG to DEBUG.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
ERROR warden: Error occurred: There was an error executing ["sudo", "/usr/bin/env", "/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/scripts/pipework", "vlxcbr1", "centos-7_swarm-node1_1570708822359_18199", "192.168.33.11/24"]

For more information on the failure, enable detailed logging by setting
the environment variable VAGRANT_LOG to DEBUG.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO environment: Released process lock: machine-action-4e302da4b51fd90dd53d7f86d656be9a
 INFO environment: Running hook: environment_unload
 INFO runner: Preparing hooks for middleware sequence...
 INFO runner: 1 hooks defined.
 INFO runner: Running action: environment_unload #<Vagrant::Action::Builder:0x00000c6a34271870>
ERROR vagrant: Vagrant experienced an error! Details:
ERROR vagrant: #<Vagrant::LXC::Errors::ExecuteError: There was an error executing ["sudo", "/usr/bin/env", "/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/scripts/pipework", "vlxcbr1", "centos-7_swarm-node1_1570708822359_18199", "192.168.33.11/24"]

For more information on the failure, enable detailed logging by setting
the environment variable VAGRANT_LOG to DEBUG.>
ERROR vagrant: There was an error executing ["sudo", "/usr/bin/env", "/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/scripts/pipework", "vlxcbr1", "centos-7_swarm-node1_1570708822359_18199", "192.168.33.11/24"]

For more information on the failure, enable detailed logging by setting
the environment variable VAGRANT_LOG to DEBUG.
ERROR vagrant: /home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/lib/vagrant-lxc/sudo_wrapper.rb:73:in `block in execute'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/util/retryable.rb:17:in `retryable'
/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/lib/vagrant-lxc/sudo_wrapper.rb:63:in `execute'
/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/lib/vagrant-lxc/sudo_wrapper.rb:36:in `run'
/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/lib/vagrant-lxc/driver.rb:180:in `configure_private_network'
/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/lib/vagrant-lxc/action/private_networks.rb:34:in `block in configure_private_networks'
/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/lib/vagrant-lxc/action/private_networks.rb:25:in `each'
/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/lib/vagrant-lxc/action/private_networks.rb:25:in `find'
/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/lib/vagrant-lxc/action/private_networks.rb:25:in `configure_private_networks'
/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/lib/vagrant-lxc/action/private_networks.rb:14:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/lib/vagrant-lxc/action/forward_ports.rb:31:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/lib/vagrant-lxc/action/warn_networks.rb:14:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/set_hostname.rb:16:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/lib/vagrant-lxc/action/prepare_nfs_settings.rb:15:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/synced_folders.rb:87:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/synced_folder_cleanup.rb:28:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/plugins/synced_folders/nfs/action_cleanup.rb:25:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/lib/vagrant-lxc/action/prepare_nfs_valid_ids.rb:14:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/handle_forwarded_port_collisions.rb:49:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/env_set.rb:19:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/provision.rb:80:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:121:in `block in finalize_action'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builder.rb:116:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/runner.rb:102:in `block in run'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/util/busy.rb:19:in `busy'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/runner.rb:102:in `run'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/call.rb:53:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/box_check_outdated.rb:36:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/config_validate.rb:25:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:121:in `block in finalize_action'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/lib/vagrant-lxc/action/create.rb:47:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/lib/vagrant-lxc/action/handle_box_metadata.rb:43:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/handle_box.rb:56:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:121:in `block in finalize_action'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builder.rb:116:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/runner.rb:102:in `block in run'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/util/busy.rb:19:in `busy'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/runner.rb:102:in `run'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/call.rb:53:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/config_validate.rb:25:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/builder.rb:116:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/runner.rb:102:in `block in run'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/util/busy.rb:19:in `busy'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/action/runner.rb:102:in `run'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/machine.rb:238:in `action_raw'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/machine.rb:209:in `block in action'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/environment.rb:613:in `lock'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/machine.rb:195:in `call'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/machine.rb:195:in `action'
/opt/vagrant/embedded/gems/gems/vagrant-2.2.5/lib/vagrant/batch_action.rb:86:in `block (2 levels) in run'
 INFO interface: error: There was an error executing ["sudo", "/usr/bin/env", "/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/scripts/pipework", "vlxcbr1", "centos-7_swarm-node1_1570708822359_18199", "192.168.33.11/24"]

For more information on the failure, enable detailed logging by setting
the environment variable VAGRANT_LOG to DEBUG.
There was an error executing ["sudo", "/usr/bin/env", "/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/scripts/pipework", "vlxcbr1", "centos-7_swarm-node1_1570708822359_18199", "192.168.33.11/24"]

For more information on the failure, enable detailed logging by setting
the environment variable VAGRANT_LOG to DEBUG.
 INFO interface: Machine: error-exit ["Vagrant::LXC::Errors::ExecuteError", "There was an error executing [\"sudo\", \"/usr/bin/env\", \"/home/niek/.vagrant.d/gems/2.6.5/gems/vagrant-lxc-1.4.3/scripts/pipework\", \"vlxcbr1\", \"centos-7_swarm-node1_1570708822359_18199\", \"192.168.33.11/24\"]\n\nFor more information on the failure, enable detailed logging by setting\nthe environment variable VAGRANT_LOG to DEBUG."]
```

Fix catchall regex to avoid privilege escalation.

Related to issue : https://github.com/fgrehm/vagrant-lxc/issues/482
Hello,

I use vagrant-lxc for a while and we found a privilege escalation in the lxc-wrapper which may cause a risk for the host system.

If you want to restrict access to root user for a user by using "sudo /path/to/lxc-wrapper" there is a privilege escalation which result in a root shell.

Before releasing the privilege escalation i would like to help fix the issue.

Does someone can help me ? PM me.
Given following `Vagrantfile`:
```
Vagrant.configure("2") do |config|
  config.vm.box = "debian/stretch64"
  config.vm.hostname = "kitchen"

  config.vm.define "kitchen" do |node|
    node.vm.provider :lxc do |lxc|
      lxc.container_name = :machine
      lxc.backingstore = 'best'
      lxc.fetch_ip_tries = 100
    end
  end
end
```
I'm unable to `vagrant up`

The result:
```
Bringing machine 'kitchen' up with 'lxc' provider...
==> kitchen: Importing base box 'debian/stretch64'...
==> kitchen: Checking if box 'debian/stretch64' version '9.1.0' is up to date...
==> kitchen: Setting up mount entries for shared folders...
    kitchen: /vagrant => /home/stanislaw/projects/scout/kitchen
==> kitchen: Starting container...
==> kitchen: Waiting for machine to boot. This may take a few minutes...
    kitchen: SSH address: 192.168.8.42:22
    kitchen: SSH username: vagrant
    kitchen: SSH auth method: private key
    kitchen: 
    kitchen: Vagrant insecure key detected. Vagrant will automatically replace
    kitchen: this with a newly generated keypair for better security.
    kitchen: 
    kitchen: Inserting generated public key within guest...
    kitchen: Removing insecure key from the guest if it's present...
    kitchen: Key inserted! Disconnecting and reconnecting using new SSH key...
==> kitchen: Machine booted and ready!
==> kitchen: Setting hostname...
The following SSH command responded with a non-zero exit status.
Vagrant assumes that this means the command failed!

systemctl stop ifup@eth0.service;systemctl start ifup@eth0.service

Stdout from the command:



Stderr from the command:

A dependency job for ifup@eth0.service failed. See 'journalctl -xe' for details.
```
Going into the container I can find some relevant information in journal:
```
Apr 12 22:32:52 stretch dbus[60]: [system] Activating via systemd: service name='org.freedesktop.hostname1' unit='dbus-org.freedesktop.hostname1.service'
Apr 12 22:32:52 stretch systemd[1]: systemd-udevd.service: Cannot add dependency job, ignoring: Unit systemd-udevd.service is masked.
Apr 12 22:32:52 stretch systemd[1]: systemd-udevd.service: Cannot add dependency job, ignoring: Unit systemd-udevd.service is masked.
Apr 12 22:32:52 stretch systemd[1]: Failed to set devices.allow on /system.slice/systemd-hostnamed.service: Operation not permitted
Apr 12 22:32:52 stretch systemd[1]: Failed to set devices.allow on /system.slice/systemd-hostnamed.service: Operation not permitted
Apr 12 22:32:52 stretch systemd[1]: Starting Hostname Service...
-- Subject: Unit systemd-hostnamed.service has begun start-up
-- Defined-By: systemd
-- Support: https://www.debian.org/support
-- 
-- Unit systemd-hostnamed.service has begun starting up.
Apr 12 22:32:52 stretch systemd[1]: run-user-1000.mount: Failed to reset devices.list: Operation not permitted
Apr 12 22:32:53 stretch dbus[60]: [system] Successfully activated service 'org.freedesktop.hostname1'
Apr 12 22:32:53 stretch systemd[1]: Started Hostname Service.
-- Subject: Unit systemd-hostnamed.service has finished start-up
-- Defined-By: systemd
-- Support: https://www.debian.org/support
-- 
-- Unit systemd-hostnamed.service has finished starting up.
-- 
-- The start-up result is done.

...
Apr 12 22:32:54 kitchen systemd[1]: Starting of sys-subsystem-net-devices-eth0.device not supported.
-- Subject: Unit sys-subsystem-net-devices-eth0.device has failed
-- Defined-By: systemd
-- Support: https://www.debian.org/support
-- 
-- Unit sys-subsystem-net-devices-eth0.device has failed.
-- 
-- The result is unsupported.
Apr 12 22:32:54 kitchen systemd[1]: Dependency failed for ifup for eth0.
-- Subject: Unit ifup@eth0.service has failed
-- Defined-By: systemd
-- Support: https://www.debian.org/support
-- 
-- Unit ifup@eth0.service has failed.
-- 
-- The result is dependency.
Apr 12 22:32:54 kitchen systemd[1]: ifup@eth0.service: Job ifup@eth0.service/start failed with result 'dependency'.
Apr 12 22:32:54 kitchen sudo[754]: pam_unix(sudo:session): session closed for user root
Apr 12 22:32:54 kitchen sshd[636]: pam_unix(sshd:session): session closed for user vagrant
Apr 12 22:32:54 kitchen systemd-logind[58]: Removed session 3.
-- Subject: Session 3 has been terminated
-- Defined-By: systemd
-- Support: https://www.debian.org/support
-- Documentation: http://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
```

I think the: `Apr 12 22:32:54 kitchen systemd[1]: Starting of sys-subsystem-net-devices-eth0.device not supported.` is the root cause.
I remember vagrant-lxc working ok on Debian 9.8

The `vagrant up --debug` log (cut to the 'setting hostname' part):

```
==> kitchen: Setting hostname...
DEBUG ssh: Checking whether SSH is ready...
DEBUG ssh: Re-using SSH connection.
 INFO ssh: SSH is ready!
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute:  (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 0
DEBUG guest: Searching for cap: change_host_name
DEBUG guest: Checking in: debian
DEBUG guest: Found cap: change_host_name in debian
 INFO guest: Execute capability: change_host_name [#<Vagrant::Machine: kitchen (Vagrant::LXC::Provider)>, "kitchen"] (debian)
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: hostname -f | grep '^kitchen$' (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 1
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: # Set the hostname
echo 'kitchen' > /etc/hostname

# Prepend ourselves to /etc/hosts
grep -w 'kitchen' /etc/hosts || {
  if grep -w '^127\.0\.1\.1' /etc/hosts ; then
    sed -i'' 's/^127\.0\.1\.1\s.*$/127.0.1.1\tkitchen\tkitchen/' /etc/hosts
  else
    sed -i'' '1i 127.0.1.1\tkitchen\tkitchen' /etc/hosts
  fi
}

# Update mailname
echo 'kitchen' > /etc/mailname

 (sudo=true)
DEBUG ssh: stderr: mesg: ttyname failed: Inappropriate ioctl for device

DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 0
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: command -v hostnamectl && hostnamectl (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 0
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: hostnamectl set-hostname 'kitchen' (sudo=true)
DEBUG ssh: stderr: mesg: ttyname failed: Inappropriate ioctl for device

DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 0
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: ps -o comm= 1 | grep systemd (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 0
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: systemctl -q is-active systemd-networkd.service (sudo=true)
DEBUG ssh: stderr: mesg: ttyname failed: Inappropriate ioctl for device

DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 3
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: systemctl -q is-active NetworkManager.service (sudo=true)
DEBUG ssh: stderr: mesg: ttyname failed: Inappropriate ioctl for device

DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 3
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: /sbin/ip -o -0 addr | grep -v LOOPBACK | awk '{print $2}' | sed 's/://' (sudo=true)
DEBUG ssh: stderr: mesg: ttyname failed: Inappropriate ioctl for device

DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 0
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: cat /proc/net/dev | grep -E '^[a-z0-9 ]+:' | awk '{print $1}' | sed 's/://' (sudo=true)
DEBUG ssh: stderr: mesg: ttyname failed: Inappropriate ioctl for device

DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 0
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: /sbin/ip -o -0 addr | grep LOOPBACK | awk '{print $2}' | sed 's/://' (sudo=true)
DEBUG ssh: stderr: mesg: ttyname failed: Inappropriate ioctl for device

DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 0
DEBUG network_interfaces: loopback interfaces: ["lo"]
DEBUG network_interfaces: Unsorted list: ["eth0@if12", "eth0"]
DEBUG network_interfaces: Sorted list: ["eth0@if12", "eth0"]
DEBUG network_interfaces: Ethernet preferred sorted list: ["eth0", "eth0@if12"]
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: ps -o comm= 1 | grep systemd (sudo=false)
DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: Exit status: 0
DEBUG changehostname: Attempting to restart networking with systemctl
DEBUG changehostname: Restarting interface eth0 on guest kitchen
DEBUG ssh: Re-using SSH connection.
 INFO ssh: Execute: systemctl stop ifup@eth0.service;systemctl start ifup@eth0.service (sudo=true)
DEBUG ssh: stderr: mesg: ttyname failed: Inappropriate ioctl for device

DEBUG ssh: stderr: 41e57d38-b4f7-4e46-9c38-13873d338b86-vagrant-ssh
DEBUG ssh: stderr: A dependency job for ifup@eth0.service failed. See 'journalctl -xe' for details.

DEBUG ssh: Exit status: 1
ERROR warden: Error occurred: The following SSH command responded with a non-zero exit status.
Vagrant assumes that this means the command failed!

systemctl stop ifup@eth0.service;systemctl start ifup@eth0.service

Stdout from the command:



Stderr from the command:

A dependency job for ifup@eth0.service failed. See 'journalctl -xe' for details.

 INFO warden: Beginning recovery process...
 INFO warden: Calling recover: #<Vagrant::Action::Builtin::HandleForwardedPortCollisions:0x0000000002856130>
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
ERROR warden: Error occurred: The following SSH command responded with a non-zero exit status.
Vagrant assumes that this means the command failed!

systemctl stop ifup@eth0.service;systemctl start ifup@eth0.service

Stdout from the command:



Stderr from the command:

A dependency job for ifup@eth0.service failed. See 'journalctl -xe' for details.

 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
ERROR warden: Error occurred: The following SSH command responded with a non-zero exit status.
Vagrant assumes that this means the command failed!

systemctl stop ifup@eth0.service;systemctl start ifup@eth0.service

Stdout from the command:



Stderr from the command:

A dependency job for ifup@eth0.service failed. See 'journalctl -xe' for details.

 INFO warden: Beginning recovery process...
 INFO warden: Calling recover: #<Vagrant::Action::Builtin::Call:0x00000000029383a0>
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Calling recover: #<Vagrant::Action::Builtin::Call:0x0000000002a15ef8>
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
ERROR warden: Error occurred: The following SSH command responded with a non-zero exit status.
Vagrant assumes that this means the command failed!

systemctl stop ifup@eth0.service;systemctl start ifup@eth0.service

Stdout from the command:



Stderr from the command:

A dependency job for ifup@eth0.service failed. See 'journalctl -xe' for details.

 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
ERROR warden: Error occurred: The following SSH command responded with a non-zero exit status.
Vagrant assumes that this means the command failed!

systemctl stop ifup@eth0.service;systemctl start ifup@eth0.service

Stdout from the command:



Stderr from the command:

A dependency job for ifup@eth0.service failed. See 'journalctl -xe' for details.

 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO warden: Beginning recovery process...
 INFO warden: Recovery complete.
 INFO environment: Released process lock: machine-action-88329c3515316201c8ca7fd9c2a595f8
 INFO environment: Running hook: environment_unload
 INFO runner: Preparing hooks for middleware sequence...
 INFO runner: 2 hooks defined.
 INFO runner: Running action: environment_unload #<Vagrant::Action::Builder:0x00007f6cfc3f3318>
ERROR vagrant: Vagrant experienced an error! Details:
ERROR vagrant: #<Vagrant::Errors::VagrantError: The following SSH command responded with a non-zero exit status.
Vagrant assumes that this means the command failed!

systemctl stop ifup@eth0.service;systemctl start ifup@eth0.service

Stdout from the command:



Stderr from the command:

A dependency job for ifup@eth0.service failed. See 'journalctl -xe' for details.
>
ERROR vagrant: The following SSH command responded with a non-zero exit status.
Vagrant assumes that this means the command failed!

systemctl stop ifup@eth0.service;systemctl start ifup@eth0.service

Stdout from the command:



Stderr from the command:

A dependency job for ifup@eth0.service failed. See 'journalctl -xe' for details.

ERROR vagrant: /opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/plugins/communicators/ssh/communicator.rb:265:in `execute'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/plugins/communicators/ssh/communicator.rb:275:in `sudo'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/plugins/guests/debian/cap/change_host_name.rb:99:in `block in restart_each_interface'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/plugins/guests/debian/cap/change_host_name.rb:92:in `each'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/plugins/guests/debian/cap/change_host_name.rb:92:in `restart_each_interface'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/plugins/guests/debian/cap/change_host_name.rb:65:in `change_host_name'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/capability_host.rb:111:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/capability_host.rb:111:in `capability'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/guest.rb:43:in `capability'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/set_hostname.rb:21:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/home/stanislaw/.vagrant.d/gems/2.4.4/gems/vagrant-lxc-1.4.3/lib/vagrant-lxc/action/prepare_nfs_settings.rb:15:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/synced_folders.rb:87:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/synced_folder_cleanup.rb:28:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/plugins/synced_folders/nfs/action_cleanup.rb:25:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/home/stanislaw/.vagrant.d/gems/2.4.4/gems/vagrant-lxc-1.4.3/lib/vagrant-lxc/action/prepare_nfs_valid_ids.rb:14:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/handle_forwarded_port_collisions.rb:49:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/env_set.rb:19:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/provision.rb:80:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:121:in `block in finalize_action'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builder.rb:116:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/runner.rb:102:in `block in run'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/util/busy.rb:19:in `busy'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/runner.rb:102:in `run'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/call.rb:53:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/box_check_outdated.rb:84:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/config_validate.rb:25:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:121:in `block in finalize_action'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/home/stanislaw/.vagrant.d/gems/2.4.4/gems/vagrant-lxc-1.4.3/lib/vagrant-lxc/action/create.rb:47:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/home/stanislaw/.vagrant.d/gems/2.4.4/gems/vagrant-lxc-1.4.3/lib/vagrant-lxc/action/handle_box_metadata.rb:43:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/handle_box.rb:56:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:121:in `block in finalize_action'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builder.rb:116:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/runner.rb:102:in `block in run'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/util/busy.rb:19:in `busy'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/runner.rb:102:in `run'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/call.rb:53:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/after_trigger.rb:26:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/config_validate.rb:25:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builtin/before_trigger.rb:23:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/warden.rb:50:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/builder.rb:116:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/runner.rb:102:in `block in run'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/util/busy.rb:19:in `busy'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/action/runner.rb:102:in `run'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/machine.rb:238:in `action_raw'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/machine.rb:209:in `block in action'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/environment.rb:615:in `lock'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/machine.rb:195:in `call'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/machine.rb:195:in `action'
/opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/lib/vagrant/batch_action.rb:82:in `block (2 levels) in run'
 INFO interface: error: The following SSH command responded with a non-zero exit status.
Vagrant assumes that this means the command failed!

systemctl stop ifup@eth0.service;systemctl start ifup@eth0.service

Stdout from the command:



Stderr from the command:

A dependency job for ifup@eth0.service failed. See 'journalctl -xe' for details.

The following SSH command responded with a non-zero exit status.
Vagrant assumes that this means the command failed!

systemctl stop ifup@eth0.service;systemctl start ifup@eth0.service

Stdout from the command:



Stderr from the command:

A dependency job for ifup@eth0.service failed. See 'journalctl -xe' for details.

 INFO interface: Machine: error-exit ["Vagrant::Errors::VagrantError", "The following SSH command responded with a non-zero exit status.\nVagrant assumes that this means the command failed!\n\nsystemctl stop ifup@eth0.service;systemctl start ifup@eth0.service\n\nStdout from the command:\n\n\n\nStderr from the command:\n\nA dependency job for ifup@eth0.service failed. See 'journalctl -xe' for details.\n"]

```

How do I get this working?

EDIT: After some investigation I found that in the Vagrant itself, the hostname change (`change_host_name.rb`) logic has changed to something that apparently doesn't work under LXC

EDIT2: vagrant 2.1.5 works, vagrant 2.2.4 does not.
On Opensuse Tumbleweed, I could not set a static ip with a CentOS7 container. Here are the relevant info (I think):

```
$ lsb_release -a
LSB Version:    core-2.0-noarch:core-3.2-noarch:core-4.0-noarch:core-2.0-x86_64:core-3.2-x86_64:core-4.0-x86_64:desktop-4.0-amd64:desktop-4.0-noarch:graphics-2.0-amd64:graphics-2.0-noarch:graphics-3.2-amd64:graphics-3.2-noarch:graphics-4.0-amd64:graphics-4.0-noarch
Distributor ID: openSUSE
Description:    openSUSE Tumbleweed
Release:        20190305
Codename:       n/a

$ sudo lxc-info --version
2.0.9

$ vagrant --version
Vagrant 2.2.2

$ vagrant plugin list
vagrant-cachier (1.2.1, global)
vagrant-libvirt (0.0.45, global)
vagrant-lxc (1.4.3, global)
```

Using the following Vagrantfile and starting the container with debug:

```
Vagrant.configure("2") do |config|
  config.vm.define "test1", primary: true do |test1|
    test1.vm.hostname = "test1"
    test1.vm.provider :lxc do |lxc|
      lxc.customize 'network.ipv4', '10.0.3.5/24'
    end
  end
end
```

```
$ REDIR_LOG=1 LXC_START_LOG_FILE=/tmp/lxc-start.log VAGRANT_LOG=debug vagrant up --provider=lxc test1
 INFO global: Vagrant version: 2.2.2
 INFO global: Ruby version: 2.6.1
 INFO global: RubyGems version: 3.0.1
 INFO global: VAGRANT_LOG="debug
```

Started container gets the DHCP ip address, not the one I give:

```
$ vagrant ssh                                                                                        
Last login: Wed Mar 13 11:31:57 2019 from gateway
[vagrant@test1 ~]$ ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
130: eth0@if131: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether aa:c7:39:ce:31:e2 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.0.3.155/24 brd 10.0.3.255 scope global dynamic eth0
       valid_lft 3525sec preferred_lft 3525sec
    inet6 fe80::a8c7:39ff:fece:31e2/64 scope link 
       valid_lft forever preferred_lft forever
```

Full debug output is attached.

[vagrant-output.txt](https://github.com/fgrehm/vagrant-lxc/files/2961244/vagrant-output.txt)

Instead if I start the container with the following config:

```
Vagrant.configure("2") do |config|
  config.vm.define "test1", primary: true do |test1|
    test1.vm.hostname = "test1"
    test1.vm.provider :lxc do |lxc|
      lxc.customize "network.type", "veth"
      lxc.customize "network.link", "lxcbr0"
      lxc.customize 'network.ipv4', '10.0.3.5/24'
    end
  end
end
```

A new interface is created inside the container with the correct ip address set:

```
$ vagrant ssh
Last login: Wed Mar 13 11:35:52 2019 from gateway
[vagrant@test1 ~]$ ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
132: eth0@if133: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether ae:ac:18:28:62:cf brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.0.3.208/24 brd 10.0.3.255 scope global dynamic eth0
       valid_lft 3544sec preferred_lft 3544sec
    inet6 fe80::acac:18ff:fe28:62cf/64 scope link 
       valid_lft forever preferred_lft forever
134: eth1@if135: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000
    link/ether 8a:7d:f2:cd:a3:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.0.3.5/24 brd 10.0.3.255 scope global eth1
       valid_lft forever preferred_lft forever
```

How can I get the static ip working on primary interface?
On Opensuse Tumbleweed, I could not get private networking to work with a CentOS7 container. Here are the relevant info (I think):

```
$ lsb_release -a
LSB Version:    core-2.0-noarch:core-3.2-noarch:core-4.0-noarch:core-2.0-x86_64:core-3.2-x86_64:core-4.0-x86_64:desktop-4.0-amd64:desktop-4.0-noarch:graphics-2.0-amd64:graphics-2.0-noarch:graphics-3.2-amd64:graphics-3.2-noarch:graphics-4.0-amd64:graphics-4.0-noarch
Distributor ID: openSUSE
Description:    openSUSE Tumbleweed
Release:        20190305
Codename:       n/a

$ sudo lxc-info --version
2.0.9

$ vagrant --version
Vagrant 2.2.2

$ vagrant plugin list
vagrant-cachier (1.2.1, global)
vagrant-libvirt (0.0.45, global)
vagrant-lxc (1.4.3, global)
```

Using the following Vagrantfile and starting the container with debug:

```
Vagrant.configure("2") do |config|
  config.vm.define "test1", primary: true do |test1|
    test1.vm.hostname = "test1"
    test1.vm.network "private_network", ip: "192.168.2.100", lxc__bridge_name: 'vlxcbr1'
  end
end
```

```
$ REDIR_LOG=1 LXC_START_LOG_FILE=/tmp/lxc-start.log VAGRANT_LOG=debug vagrant up --provider=lxc test1
 INFO global: Vagrant version: 2.2.2
 INFO global: Ruby version: 2.6.1
 INFO global: RubyGems version: 3.0.1
 INFO global: VAGRANT_LOG="debug"
...
 INFO interface: output: Setting up private networks...
 INFO interface: output: ==> test1: Setting up private networks...
==> test1: Setting up private networks...
 INFO driver: Configuring network interface for oyas_test1_1552473237752_9965 using 192.168.2.100 and bridge vlxcbr1
 INFO driver: Checking whether bridge vlxcbr1 exists
 INFO driver: Creating the bridge vlxcbr1
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/local/bin/vagrant-lxc-wrapper", "brctl", "addbr", "vlxcbr1"]
 INFO subprocess: Vagrant not running in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO driver: Checking whether the bridge vlxcbr1 has an IP
 INFO driver: Adding 192.168.2.254 to the bridge vlxcbr1
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/local/bin/vagrant-lxc-wrapper", "ip", "addr", "add", "192.168.2.254/24", "dev", "vlxcbr1"]
 INFO subprocess: Vagrant not running in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/local/bin/vagrant-lxc-wrapper", "ip", "link", "set", "vlxcbr1", "up"]
 INFO subprocess: Vagrant not running in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 32000
DEBUG subprocess: Exit status: 0
 INFO subprocess: Starting process: ["/usr/bin/sudo", "/usr/local/bin/vagrant-lxc-wrapper", "/home/ebayer/.vagrant.d/gems/2.6.1/gems/vagrant-lxc-1.4.3/scripts/pipework", "vlxcbr1", "oyas_test1_1552473237752_9965", "192.168.2.100/24"]
 INFO subprocess: Vagrant not running in installer, restoring original environment...
DEBUG subprocess: Selecting on IO
DEBUG subprocess: stderr: /home/ebayer/.vagrant.d/gems/2.6.1/gems/vagrant-lxc-1.4.3/scripts/pipework: line 393: Address:: command not found
DEBUG subprocess: Waiting for process to exit. Remaining to timeout: 31999
DEBUG subprocess: Exit status: 0
 INFO warden: Calling OUT action: #<Vagrant::LXC::Action::PrivateNetworks:0x0000000002638e98>
```

I can see that new bridge is created and got the default ip address, but state is down:

```
$ ip a
7: lxcbr0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 00:16:3e:00:00:00 brd ff:ff:ff:ff:ff:ff
    inet 10.0.3.1/24 scope global lxcbr0
       valid_lft forever preferred_lft forever
    inet6 fe80::216:3eff:fe00:0/64 scope link 
       valid_lft forever preferred_lft forever
120: vlxcbr1: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000
    link/ether 52:58:14:0e:64:cb brd ff:ff:ff:ff:ff:ff
    inet 192.168.2.254/24 scope global vlxcbr1
       valid_lft forever preferred_lft forever
    inet6 fe80::5022:71ff:fe98:a96e/64 scope link 
       valid_lft forever preferred_lft forever
122: veth1pl11149@if121: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue master vlxcbr1 state LOWERLAYERDOWN group default qlen 1000
    link/ether 52:58:14:0e:64:cb brd ff:ff:ff:ff:ff:ff link-netnsid 0

$ sudo brctl show
vlxcbr1         8000.5258140e64cb       no              veth1pl11149
```

And interface inside the container is created but without an ip address:

```
$ vagrant ssh
[vagrant@test1 ~]$ ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
118: eth0@if119: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 0e:ff:a2:fc:3f:d2 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.0.3.75/24 brd 10.0.3.255 scope global dynamic eth0
       valid_lft 3544sec preferred_lft 3544sec
    inet6 fe80::cff:a2ff:fefc:3fd2/64 scope link 
       valid_lft forever preferred_lft forever
121: eth1@if122: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000
    link/ether ba:53:fd:82:03:16 brd ff:ff:ff:ff:ff:ff link-netnsid 0
```

If I ssh into container and set the interface to up and give an ip address, it works:

```
$ vagrant ssh
[vagrant@test1 ~]$ sudo ip l set up dev eth1
[vagrant@test1 ~]$ sudo ip addr add 192.168.2.100/24 dev eth1
[vagrant@test1 ~]$ ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
118: eth0@if119: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 0e:ff:a2:fc:3f:d2 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.0.3.75/24 brd 10.0.3.255 scope global dynamic eth0
       valid_lft 2800sec preferred_lft 2800sec
    inet6 fe80::cff:a2ff:fefc:3fd2/64 scope link 
       valid_lft forever preferred_lft forever
121: eth1@if122: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether ba:53:fd:82:03:16 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 192.168.2.100/24 scope global eth1
       valid_lft forever preferred_lft forever
    inet6 fe80::b853:fdff:fe82:316/64 scope link 
       valid_lft forever preferred_lft forever
```

And bridge and new interface status are fixed on the host after that:

```
$ ip a
120: vlxcbr1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 52:58:14:0e:64:cb brd ff:ff:ff:ff:ff:ff
    inet 192.168.2.254/24 scope global vlxcbr1
       valid_lft forever preferred_lft forever
    inet6 fe80::5022:71ff:fe98:a96e/64 scope link 
       valid_lft forever preferred_lft forever
122: veth1pl11149@if121: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master vlxcbr1 state UP group default qlen 1000
    link/ether 52:58:14:0e:64:cb brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet6 fe80::5058:14ff:fe0e:64cb/64 scope link 
       valid_lft forever preferred_lft forever
```

How can I further debug this? Is there some pre-start tasks that I should do on the host?

My lxc and lxc-net configs are like this:

```
$ sudo grep -i bridge /etc/default/lxc
USE_LXC_BRIDGE="true"  # overridden in lxc-net

$ sudo cat /etc/lxc/default.conf
#lxc.network.type = empty
lxc.network.type = veth
lxc.network.link = lxcbr0
lxc.network.flags = up
```
It appears that `packer` supports exporting Vagrant boxes for LXC, but it isn't clear what the inputs are, and how they're used? Can I build a box using QEMU, and then export it using the `vagrant` post-processor, with `lxc` as the type? That is how I handle generating `docker` boxes. Is there a guide somewhere? 

For reference, this page lists LXC as a valid `vagrant` type for the post processor:

https://www.packer.io/docs/post-processors/vagrant.html

And these rather old, non conclusive issues discuss support:

https://github.com/fgrehm/vagrant-lxc/issues/219
https://github.com/hashicorp/packer/issues/790

But I couldn't find anything recent, or definitive. Nor could I find a guide on how to use to generate base boxes.