
Hello,

Is it possible to enable streaming mode that will process changes to the output automatically whenever there are changes available in kafka, without running spark submit in interval mode.

Thank you
spark 2.3.4 in spark standalone cluster running on EC2s (no k8s), java8, metorikku v0.43, amazonlinux1

2 job logs attached (from separate spark submit runs, both same command as below), you can see first run wrote a None partition! while second run wrote proper date partition. Run number 3+ (onwards) also worked well like run 2.

`/home/ec2-user/spark_home/bin/spark-submit --master spark://domain:6066 --conf spark.driver.extraJavaOptions='-Dinput_wildcard=20200107 -Ddatestr=20200107 -Dhourstr= -Dmonthstr=202001' --conf spark.sql.parquet.writeLegacyFormat=true --conf spark.executor.extraJavaOptions='-Dinput_wildcard=20200107 -Ddatestr=20200107 -Dhourstr= -Dmonthstr=202001' --executor-cores 2 --executor-memory 8g --driver-memory 2g --name tesla_catalyst_rawcsv_tesla_uk__tesla_uk_raw_INGEST_20200107 --class com.yotpo.metorikku.Metorikku --verbose --deploy-mode cluster s3a://buck/metorikku.jar --config /home/ec2-user/jsonconfigs/si/job/tesla_catalyst/metorikku/input/rawcsvtesla_uk_tesla_uk_raw_input.yaml`





I implemented a ElasticsearchInstrumentation which will send metrics to Elasticsearch. However, ElasticsearchClient is always closed before sending metrics. The cause is from following code,
line 24 at Job.scala :
```
  val instrumentationClient = instrumentationFactory.create()
  sparkContext.addSparkListener(new SparkListener() {
    override def onJobEnd(taskEnd: SparkListenerJobEnd): Unit = {
      instrumentationClient.close()
    }
  })
```
A spark application usually  creates several jobs (each operation action will create a job). And instrumentationClient will be closed multiple times when these jobs finished. 
I think the instrumentationClient can be closed only when the application ends.


Hi, Spark dataframe does only support append or overwrite data. I implement a  simple JdbcUpsertWriter to upsert data in relation database. It will check whether the  dataframe row exists in database. If the row exists,generates an UPDATE statement and if row does not exists, creates an INSERT statement.
output dir: x/y
partition by colTicker

new source data for colTicker can appear everyday, sometimes new values sometimes existing values.

example:
day1 had 2 files under below folder structure:
colTicker=AMZN
colTicker=MSFT (lets say 150 rows in the file)

day2 had 1 file:

colTicker=GOOG

day3 had 3 files:
colTicker=MSFT (lets say 120 rows in the file)
colTicker=IBM
colTicker=LYFT

i want to replace data (so not storing 2 days versions/duplicates) for same colTicker but keep data for all distinct colTicker partitions

so my expected output data after each day:

day1 output data under outputdir:
colTicker=AMZN
colTicker=MSFT (150)

day2 output data under outputdir:
colTicker=AMZN (existing data should be untouched, was loaded in day1 only, never removed)
colTicker=MSFT (150)
colTicker=GOOG

day3 output data under outputdir:
colTicker=AMZN (existing data should be untouched, was loaded in day1 only, never removed)
colTicker=MSFT (120, previous 150 should be overwritten)
colTicker=GOOG (existing data should be untouched, was loaded in day1 only, never removed)
colTicker=IBM
colTicker=LYFT

issue is:
 if i try savemode=append then after day3 MSFT shows 270 rows
 if i try savemode=overwrite then after day3 AMZN/GOOG data is gone

