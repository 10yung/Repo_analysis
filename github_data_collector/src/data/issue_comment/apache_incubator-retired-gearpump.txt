Be sure to do all of the following to help us incorporate your contribution
quickly and easily:

 - [ ] Make sure the commit message is formatted like:
   `[GEARPUMP-<Jira issue #>] Meaningful description of pull request` 
 - [ ] Make sure tests pass via `sbt clean test`.
 - [ ] Make sure old documentation affected by the pull request has been updated and new documentation added for new functionality. 


Be sure to do all of the following to help us incorporate your contribution
quickly and easily:

 - [ ] Make sure the commit message is formatted like:
   `[GEARPUMP-<Jira issue #>] Meaningful description of pull request` 
 - [x] Make sure tests pass via `sbt clean test`.
 - [ ] Make sure old documentation affected by the pull request has been updated and new documentation added for new functionality. 


R: @huafengw 

This is work in process and even doesn't compile (breaking `RemoteMaterializerImpl`). The motivation is to make user application even simpler from

```scala
val context = ClientContext(akkaConfig)
val app = StreamApp(name, context)
...
val runningApplication = context.submit(app)
context.close()
```
to 

```scala
val app = StreamApp(name, akkaConfig) // creates ClientContext 
val runningApplication: RunningApplication = app.run() // invokes context.submit(app)
```

and address the following issues I find

1. One subtlety in the current way is `StreamApp` is implicitly converted to `StreamApplication` for `context.submit`. It can be broken if a user forgets to import the conversion (and user doesn't know where it is).
2.  I find it difficult to explain the usage of `class StreamApp(name: String, system: ActorSystem, userConfig: UserConfig, private val graph: Graph[Op, OpEdge])`. We'd better make the constructor private if it's not intended for users. 
3. Both `RunningApplication` and `ClientContext` have a `askAppMaster` method to query application status. How should their roles be divided ?
4. `RunningApplication` has no ScalaDoc.

One downside about the new way is we can't close `ClientContext` and its underlying `ActorSystem`. Is it a big problem ?

Still implementing upstream, downstream access but the flow is now working
Cassandra database integration
- [X] CassandraSource
- [X] CassandraSink
- [X] CassandraStore

Reuses some Spark-Cassandra connector files and follows how that works. The intent is to allow the connector to be reused when version for other processing systems is available. The Source looks up token ranges in the desired table, splits to independent sets of partitions and assigns those to available number of source tasks, allowing very good parallelism. All fetches of data except the first one are asynchronous. The Sink can be trivially parallelised by the user where different writes are assigned to different tasks.

The Source scans a current table snapshot and does not currently honour updates (so not a continuous stream). The source is not time replayable. There are options how to handle both these, but must be properly thought through. The test coverage is poor at the moment. but this first attempt will allow iteration and continuous improvement of the code and adding features.

[GEARPUMP-172](https://issues.apache.org/jira/browse/GEARPUMP-172)

The time interval rotation will rolling out a new file when time interval get the limit

[GEARPUMP-152](https://issues.apache.org/jira/browse/GEARPUMP-152)

Storm 1.0.0 has been released with a lot of changes. We need to upgrade our support for Storm to 1.0.x

[Redis](redis.io) is a hight performance in memory storage , and have widely used in a lot of project . 

It's should support redis as DataSource and DataSink .

