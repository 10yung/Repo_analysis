Mish is a novel activation function proposed in this [paper](https://arxiv.org/abs/1908.08681).
It has shown promising results so far and has been adopted in several packages including:

|     |     |  |
| :---: | :---: |  :---: |
|[TensorFlow-Addons](https://github.com/tensorflow/addons/tree/master/tensorflow_addons/activations)|[SpaCy (Tok2Vec Layer)](https://github.com/explosion/spaCy)|[Thinc - SpaCy's official NLP based ML library](https://github.com/explosion/thinc/releases/tag/v7.3.0)|
|[Eclipse's deeplearning4j](https://github.com/eclipse/deeplearning4j/issues/8417)|[Hasktorch](https://github.com/hasktorch/hasktorch/blob/e5d4fb1b11663de3e032941f78b9d25c5da06f6d/hasktorch/src/Torch/Typed/Native.hs#L314)|[Echo AI](https://github.com/digantamisra98/Echo)|
|[CNTKX - Extension of Microsoft's CNTK](https://github.com/delzac/cntkx)|[FastAI-Dev](https://github.com/fastai/fastai_dev/blob/0f613ba3205990c83de9dba0c8798a9eec5452ce/dev/local/layers.py#L441)|[Darknet](https://github.com/AlexeyAB/darknet/commit/bf8ea4183dc265ac17f7c9d939dc815269f0a213)|
|[Yolov3](https://github.com/ultralytics/yolov3/commit/444a9f7099d4ff1aef12783704e3df9a8c3aa4b3)|[BeeDNN - Library in C++](https://github.com/edeforas/BeeDNN)|[Gen-EfficientNet-PyTorch](https://github.com/rwightman/gen-efficientnet-pytorch)|
|[dnet](https://github.com/umangjpatel/dnet)|[ruby-dnn](https://github.com/unagiootoro/ruby-dnn#implemented)|[blackcat-tensors](https://github.com/josephjaspers/blackcat_tensors/commit/87fa2f427d044a120816f2bc83176bdb455ceccd)|
|[DL4S](https://palle-k.github.io/DL4S/)|[HuggingFace Transformers](https://github.com/huggingface/transformers/pull/1753)|[PAGI](https://github.com/ProjectAGI/pagi)|
|[OpenCV](https://github.com/opencv/opencv/pull/15808)|[Odin-AI](https://github.com/imito/odin-ai)|[Mini DNN](https://github.com/yixuan/MiniDNN/commit/a405bd082183e685222472fa5851fd5024b1a7e6)|
|[Efficient Segmentation Networks](https://github.com/xiaoyufenfei/Efficient-Segmentation-Networks)|[TF Semantic Segmentation](https://pypi.org/project/tf-semantic-segmentation/0.1.0/)|[Dynastes](https://github.com/veqtor/dynastes)|
|[DLib](https://github.com/davisking/dlib/commit/edff12d2e17f65a2ded6ea71c6ba402055cea896)|[Copernicus](https://github.com/dmbernaal/copernicus)|[AllenNLP](https://github.com/allenai/allennlp/pull/3466#event-2951976800)|
|[PyWick](https://github.com/achaiah/pywick)|||

All benchmarks, analysis and links to official package implementations can be found in this [repository](https://github.com/digantamisra98/Mish) 

Mish also was recently used for a submission on the [Stanford DAWN Cifar-10 Training Time Benchmark](https://dawn.cs.stanford.edu/benchmark/CIFAR10/train.html) where it obtained 94% accuracy in just 10.7 seconds which is the current best score on 4 GPU and second fastest overall. Additionally, Mish has shown to improve convergence rate by requiring less epochs. Reference - 

![0 (2)](https://user-images.githubusercontent.com/34192716/72232207-9c0b7180-35e5-11ea-920f-295a7c873216.jpg)

Mish also has shown consistent improved ImageNet scores and is more robust. Reference - 

![0](https://user-images.githubusercontent.com/34192716/72232233-b9d8d680-35e5-11ea-97bb-fdb715a6f276.jpg)

Additional ImageNet benchmarks along with Network architectures and weights are avilable on my [repository](https://github.com/digantamisra98/Mish).

Summary of Vision related results: 

![Capture](https://user-images.githubusercontent.com/34192716/72232240-c78e5c00-35e5-11ea-9d1e-dd69f1ef33a7.PNG)

It would be nice to have Mish as an option within the activation function group.

This is the comparison of Mish with other conventional activation functions in a SEResNet-50 for CIFAR-10: 
![se50_1](https://user-images.githubusercontent.com/34192716/69002745-0de37980-091b-11ea-87da-ac8d17c79e07.png)

"php": "^7.3"
"laravel/framework": "^6.1"
"php-ai/php-ml": "^0.8.0"

Use case:
Trying to create some estimations / predictions based on existing database (grouped by year / month / day).

```
$samples = [[2019, 12, 1], [2019, 12, 2], [2019, 12, 3]];
$targets = [1, 2, 3];

$regression = new LeastSquares();
$regression->train($samples, $targets);

$regression->predict([2019, 12, 4]);
```
Expected result is "4" but throws Matrix exception instead.

I debugged it and seems that when a column is the same it does not count it.

If I append a value with different year will work fine (for example [2018, 11, 1]) but I will not always have older data to train it.

I got it done using an workaround.

```
$date = Carbon::parse('2019-12-1');
$date->diffInDays(Carbon::parse('2019-12-20'));
```

In this way I reduce the samples to days differences but I hope it could be done in a cleaner way.
After using the example code from php-ml-examples, the code stops to work right after function train() was called.
`use Phpml\SupportVectorMachine\Kernel;
    use Phpml\Classification\SVC;

    $samples = [[1, 3], [1, 4], [2, 4], [3, 1], [4, 1], [4, 2]];
    $labels = ['a', 'a', 'a', 'b', 'b', 'b'];
    $svcClassifier = new SVC(
        Kernel::LINEAR, // $kernel
        1.0,            // $cost
        3,              // $degree
        null,           // $gamma
        0.0,            // $coef0
        0.001,          // $tolerance
        100,            // $cacheSize
        true,           // $shrinking
        true            // $probabilityEstimates, set to true
    );
    
    $svcClassifier->train($samples, $labels);`

After this part of code, nothing works and webpage is sent to client only with stuff, written before this part of code. No error is displayed.
How can I use SVC without this error?

How can I predict using a label, instead of using samples
My code:

```
<?php

include_once './vendor/autoload.php';
include_once 'Portuguese.php';


//source: https://www.softnix.co.th/2018/08/19/naive-bays-text-classification-with-php/


use Phpml\Classification\NaiveBayes;
use Phpml\FeatureExtraction\StopWords\Portuguese;
use Phpml\FeatureExtraction\TfIdfTransformer;
use Phpml\FeatureExtraction\TokenCountVectorizer;
use Phpml\Tokenization\WordTokenizer;
use Phpml\ModelManager;

$arr_text = [
    "London bridge is falling down",
    "japan samurai Universal Studio spider man",
    "china beijing",
    "thai Chiangmai",
    "Universal Studio Hollywood",
    //"2020 Olympic games"
];
$arr_text2 = [
    "2020 Olympic games"
];
$arr_label = [
    "London",
    "Japan",
    "China",
    "Thailand",
    "USA",
    //"Japan"
];
$arr_label2 = [
    "Japan"
];

$tokenize = new WordTokenizer();
$vectorizer = new TokenCountVectorizer($tokenize, new Portuguese());

$vectorizer->fit($arr_text);
$vocabulary = $vectorizer->getVocabulary();
//var_dump($vocabulary);
$arr_transform = $arr_text;
$vectorizer->transform($arr_transform);

$transformer = new TfIdfTransformer($arr_transform);
$transformer->transform($arr_transform);


$vectorizer2 = new TokenCountVectorizer($tokenize, new Portuguese());

$vectorizer2->fit($arr_text);
$vocabulary2 = $vectorizer2->getVocabulary();
//var_dump($vocabulary);
$arr_transform2 = $arr_text2;
$vectorizer2->transform($arr_transform2);

$transformer2 = new TfIdfTransformer($arr_transform2);
$transformer2->transform($arr_transform2);



$classifier = new NaiveBayes();
$classifier->train($arr_transform, $arr_label);
//$classifier->train($arr_transform2, $arr_label2);

$arr_testset = [
    'Hello Chiangmai I am Siam',
    'I want to go Universal Studio',
    'I want to go Universal Studio because I want to watch spider man',
    'Sonic in 2020'
];

$vectorizer->transform($arr_testset);
$transformer->transform($arr_testset);
$result = $classifier->predict($arr_testset);
var_dump($result);


$modelManager = new ModelManager();
$modelManager->saveToFile($classifier, 'bbc.phpml');
```


Stack errors:


Call Stack
--

1 | 0.0005 | 419024 | {main}(  ) | ...\teste.php:0
2 | 0.0028 | 467136 | Phpml\FeatureExtraction\TfIdfTransformer->__construct(  ) | ...\teste.php:61
3 | 0.0028 | 467136 | Phpml\FeatureExtraction\TfIdfTransformer->fit(  ) | ...\TfIdfTransformer.php:19


Seems a method getRandomPoint() uses values to randomize points, but need to be used indexes of an array of values to get values by random chosen index:
$point[$n] = random_int($min[$n], $max[$n]);
need to be replaced with something like this:
$point[$n] = $data_array[$n][random_int($min_val_index[$n], $max_val_index[$n])];
because a random_int() now limits usage only integer data values.
Is their a way to get a percentage of how certain the classifier is based on the input and the labels?

My dataset consist of 2 target class, both have 500 data (1000 data total). While using StratifiedRandomSplit to split it into random train-test samples, i found that there is two same data in the test samples. i already check the original data and the data is just one. 