Background: we are using a spring/netflix based microservice architecture running on kubernetes. Most of the services are only being used internally and therefore there are no kubernetes-Services. The applications register themself to a Eureka-Server (https://github.com/Netflix/eureka/wiki/Eureka-at-a-glance)

To register a service, spring uses either the IP Address (of the Pod) or the hostname. Its possible to overwrite the hostname and 'fake' another IP address. But if I use telepresence with 'swap-deployment' it looks that there is no chance to get the PODs IP Address. 

https://www.telepresence.io/reference/limitations shows a way to inject the PODs IP from the Downward API

It would be totally great, if you could add the PODs IP as an environment variable like e.g. 'TELEPRESENCE_POD_IP' for 'swapped-deployments'.

Thanks and cheers,
Hendrik
### What were you trying to do?

Turn on telepresence using: telepresence --namespace xendit-integration

### What did you expect to happen?

(please tell us)

### What happened instead?

(please tell us - the traceback is automatically included, see below.
use https://gist.github.com to pass along full telepresence.log)

### Automatically included information

Command line: `['/usr/local/bin/telepresence', '--namespace', 'xendit-integration']`
Version: `0.103`
Python version: `3.7.6 (default, Dec 30 2019, 19:38:28) 
[Clang 11.0.0 (clang-1100.0.33.16)]`
kubectl version: `Client Version: v1.13.11-dispatcher // Server Version: v1.12.10-gke.17`
oc version: `(error: [Errno 2] No such file or directory: 'oc': 'oc')`
OS: `Darwin ip-192-168-10-249.ap-southeast-1.compute.internal 18.7.0 Darwin Kernel Version 18.7.0: Tue Aug 20 16:57:14 PDT 2019; root:xnu-4903.271.2~2/RELEASE_X86_64 x86_64`

```
Traceback (most recent call last):
  File "/usr/local/bin/telepresence/telepresence/cli.py", line 135, in crash_reporting
    yield
  File "/usr/local/bin/telepresence/telepresence/main.py", line 82, in main
    runner, remote_info, env, socks_port, ssh, mount_dir, pod_info
  File "/usr/local/bin/telepresence/telepresence/outbound/setup.py", line 112, in launch
    runner_, remote_info, command, args.also_proxy, env, ssh
  File "/usr/local/bin/telepresence/telepresence/outbound/local.py", line 126, in launch_vpn
    connect_sshuttle(runner, remote_info, also_proxy, ssh)
  File "/usr/local/bin/telepresence/telepresence/outbound/vpn.py", line 310, in connect_sshuttle
    raise RuntimeError("vpn-tcp tunnel did not connect")
RuntimeError: vpn-tcp tunnel did not connect

```

Logs:

```
sult for b'edgeapi.slack.com' is ['54.214.87.230']
  55.5 TEL | [60] timed out after 5.01 secs.
  55.5 TEL | [61] Capturing: python3 -c 'import socket; socket.gethostbyname("hellotelepresence-16.a.sanity.check.telepresence.io")'
  55.6  26 | c : DNS request from ('172.25.249.121', 62447) to None: 69 bytes
  55.7  26 | c : DNS request from ('172.25.249.121', 49999) to None: 35 bytes
  55.7  14 | 2020-01-16T08:59:09+0000 [stdout#info] Sanity check: b'hellotelepresence-16.a.sanity.check.telepresence.io'
  55.9  14 | 2020-01-16T08:59:09+0000 [stdout#info] A query: b'edgeapi.slack.com'
  55.9  14 | 2020-01-16T08:59:09+0000 [stdout#info] Result for b'edgeapi.slack.com' is ['54.214.87.230']
  56.5 TEL | [61] timed out after 1.01 secs.
  58.3  26 | c : DNS request from ('172.25.249.121', 62455) to None: 33 bytes
  58.5  14 | 2020-01-16T08:59:11+0000 [stdout#info] A query: b'www.gstatic.com'
  58.5  14 | 2020-01-16T08:59:11+0000 [stdout#info] Result for b'www.gstatic.com' is ['173.194.202.94']

```

It would be handy to tweak the ServerAliveInterval and ServerCountMax arguments that telepresence sends to ssh, as well as maybe pass/override other arguments.
## What were you trying to do?
I was trying to swap one of my containers into our dev cluster. This has been working for some time before without any problems, but all of a sudden it has stopped working and I am not able to get it working again. Already tried to upgrade and downgrade kubectl, trying to swap different images/pods has not helped either. Other collegues with the same telepresence version don't have the same problem.

## What did you expect to happen?
I expected my local docker image to be deployed into our dev cluster.

## What happened instead?
Telepresence apologized for containing a bug and sent me to this form.

## Current versions in use
telepresence = 0.103
docker = 19.03.3, build a872fc2f86
kubectl client = 1.15.0 (Also tried 1.17.0 and 1.14.0)
kubectl server = 1.15.6

There is no minikube in use.

## Related issues (with minikube)
https://github.com/telepresenceio/telepresence/issues/873
https://github.com/telepresenceio/telepresence/issues/928

## Output
Looks like there's a bug in our code. Sorry about that!

Traceback (most recent call last):
  File "/usr/bin/telepresence/telepresence/cli.py", line 135, in crash_reporting
    yield
  File "/usr/bin/telepresence/telepresence/main.py", line 82, in main
    runner, remote_info, env, socks_port, ssh, mount_dir, pod_info
  File "/usr/bin/telepresence/telepresence/outbound/setup.py", line 169, in launch
    pod_info,
  File "/usr/bin/telepresence/telepresence/outbound/container.py", line 141, in run_docker_command
    raise RuntimeError("SSH to the network container failed to start.")
RuntimeError: SSH to the network container failed to start.


Here are the last few lines of the logfile (see /home/onex/onex/workspace/auth/telepresence.log for the complete logs):

  53.7  48 |   29.8 TEL | [119] exit 255 in 0.00 secs.
  53.7  48 | Traceback (most recent call last):
  53.7  48 |   File "/usr/bin/entrypoint.py", line 130, in <module>
  53.7  48 |     main()
  53.7  48 |   File "/usr/bin/entrypoint.py", line 66, in main
  53.7  48 |     proxy(loads(sys.argv[2]))
  53.7  48 |   File "/usr/bin/entrypoint.py", line 86, in proxy
  53.7  48 |     "SSH from local container to the cluster failed to start."
  53.7  48 | RuntimeError: SSH from local container to the cluster failed to start.
  53.7  48 | [INFO  tini (1)] Main child exited normally (with status '1')
  53.8 TEL | [56] exit 255 in 3.82 secs.
  53.9 TEL | [48] Network container: exit 1

## Complete telepresence.log
[Uploading telepresence.log…]()

### What were you trying to do?

try it out

### What did you expect to happen?

a shell open

### What happened instead?

(please tell us - the traceback is automatically included, see below.
use https://gist.github.com to pass along full telepresence.log)

### Automatically included information

Command line: `['/usr/local/bin/telepresence', '--run-shell']`
Version: `0.103`
Python version: `3.7.6 (default, Dec 30 2019, 19:38:28) 
[Clang 11.0.0 (clang-1100.0.33.16)]`
kubectl version: `(error: Command '['kubectl', 'version', '--short']' returned non-zero exit status 1.)`
oc version: `(error: [Errno 2] No such file or directory: 'oc': 'oc')`
OS: `Darwin ip-192-168-250-38.ec2.internal 18.7.0 Darwin Kernel Version 18.7.0: Sun Dec  1 18:59:03 PST 2019; root:xnu-4903.278.19~1/RELEASE_X86_64 x86_64`

```
Traceback (most recent call last):
  File "/usr/local/bin/telepresence/telepresence/cli.py", line 135, in crash_reporting
    yield
  File "/usr/local/bin/telepresence/telepresence/main.py", line 82, in main
    runner, remote_info, env, socks_port, ssh, mount_dir, pod_info
  File "/usr/local/bin/telepresence/telepresence/outbound/setup.py", line 112, in launch
    runner_, remote_info, command, args.also_proxy, env, ssh
  File "/usr/local/bin/telepresence/telepresence/outbound/local.py", line 126, in launch_vpn
    connect_sshuttle(runner, remote_info, also_proxy, ssh)
  File "/usr/local/bin/telepresence/telepresence/outbound/vpn.py", line 310, in connect_sshuttle
    raise RuntimeError("vpn-tcp tunnel did not connect")
RuntimeError: vpn-tcp tunnel did not connect

```

Logs:

```
000 [stdout#info] Result for b'hellotelepresence-12.compute-1.amazonaws.com' is ['127.0.0.1']
  52.8  31 | c : DNS request from ('192.168.250.38', 49541) to None: 62 bytes
  53.0  18 | 2020-01-14T22:31:09+0000 [stdout#info] Result for b'hellotelepresence-12.compute-1.amazonaws.com' is ['127.0.0.1']
  53.9  31 | c : DNS request from ('192.168.250.38', 55594) to None: 41 bytes
  54.1  18 | 2020-01-14T22:31:10+0000 [stdout#info] A query: b'gspe35-ssl.ls.apple.com'
  54.1  18 | 2020-01-14T22:31:10+0000 [stdout#info] Result for b'gspe35-ssl.ls.apple.com' is ['23.218.120.198']
  54.8 TEL | [57] timed out after 5.01 secs.
  54.8 TEL | [58] Capturing: python3 -c 'import socket; socket.gethostbyname("hellotelepresence-12.a.sanity.check.telepresence.io")'
  54.9  31 | c : DNS request from ('192.168.250.38', 49978) to None: 69 bytes
  55.0  18 | 2020-01-14T22:31:11+0000 [stdout#info] Sanity check: b'hellotelepresence-12.a.sanity.check.telepresence.io'
  55.8 TEL | [58] timed out after 1.01 secs.

```

Added the ability to set a node selector for a new deployment

Fixes #1096 
At least in VPN mode, k8s-proxy logs EVERY query executed from telepresence host machine. 
I see this as a **privacy breach**, since any cluster or logging system user can see which websites were accessed by a telepresence user while they were connected to telepresence.

Example of log messages:
```
2020-01-13T10:44:45+0000 [stdout#info] A query: b'github.com'
2020-01-13T10:44:45+0000 [stdout#info] Result for b'github.com' is ['140.82.118.3']
```

Ideally, such logs messages would be disabled by default, but it would be alright for us to have it as a CLI option (since we're using wrapper shellscript anyway).

### What were you trying to do?

Connect to AKS cluster with telepresence.

### What did you expect to happen?

I was expecting to get shell with access to aks services

### What happened instead?
  80.4  53 | 2020-01-09T03:46:52+0000 [Poll#error] Failed to contact Telepresence client:
  80.4  53 | 2020-01-09T03:46:52+0000 [Poll#error] An error occurred while connecting: 99: Address not available.
  80.4  53 | 2020-01-09T03:46:52+0000 [Poll#warn] Perhaps it's time to exit?

### Automatically included information

Command line: `['/usr/local/bin/telepresence', '--run-shell']`
Version: `0.103`
Python version: `3.7.6 (default, Dec 30 2019, 19:38:28) 
[Clang 11.0.0 (clang-1100.0.33.16)]`
kubectl version: `Client Version: v1.17.0 // Server Version: v1.14.8`
oc version: `(error: [Errno 2] No such file or directory: 'oc': 'oc')`
OS: `Darwin Przemyslaws-MacBook-Pro-15.local 18.7.0 Darwin Kernel Version 18.7.0: Sun Dec  1 18:59:03 PST 2019; root:xnu-4903.278.19~1/RELEASE_X86_64 x86_64`

```
Traceback (most recent call last):
  File "/usr/local/bin/telepresence/telepresence/cli.py", line 135, in crash_reporting
    yield
  File "/usr/local/bin/telepresence/telepresence/main.py", line 68, in main
    socks_port, ssh = do_connect(runner, remote_info)
  File "/usr/local/bin/telepresence/telepresence/connect/connect.py", line 119, in do_connect
    args.from_pod
  File "/usr/local/bin/telepresence/telepresence/connect/connect.py", line 70, in connect
    raise RuntimeError("SSH to the cluster failed to start. See logfile.")
RuntimeError: SSH to the cluster failed to start. See logfile.

```

Logs:

```
 65 |                  node.kubernetes.io/unreachable:NoExecute for 300s
  78.4  65 | Events:
  78.4  65 |   Type    Reason     Age   From                               Message
  78.4  65 |   ----    ------     ----  ----                               -------
  78.4  65 |   Normal  Scheduled  50s   default-scheduler                  Successfully assigned default/telepresence-1578541208-743932-82176-6cfc4c6f4-92qfl to aks-agentpool-21324540-2
  78.4  65 |   Normal  Pulling    48s   kubelet, aks-agentpool-21324540-2  Pulling image "datawire/telepresence-k8s:0.103"
  78.4  65 |   Normal  Pulled     42s   kubelet, aks-agentpool-21324540-2  Successfully pulled image "datawire/telepresence-k8s:0.103"
  78.4  65 |   Normal  Created    33s   kubelet, aks-agentpool-21324540-2  Created container telepresence-1578541208-743932-82176
  78.4  65 |   Normal  Started    33s   kubelet, aks-agentpool-21324540-2  Started container telepresence-1578541208-743932-82176
  78.5 TEL | [65] ran in 0.28 secs.

```

This is a weird/unconventional bug, not from telepresence but there might be a workaround :)

It happens on iTerm2 + zsh + macOS for me, haven't tested in other places.

### Summary
Cannot reinsert sudo password after telepresence crashes/exits, in the same terminal session.

### Overview
1. When telepresence is active, it requires you to insert the sudo password.
2. After the connection is lost, telepresence exits and goes back to the shell t erminal.
3. The most common action is to run `telepresence` again
4. Password is asked again if Sudo Password `timestamp_timeout` is expired
5. Cannot insert new password

### How to reproduce
1. `sudo visudo`, edit/add `Defaults     timestamp_timeout = 0` to set the sudo password timeout to `0`
2. Run `telepresence`, insert sudo password
3. Kill the telepresence pod to force a `Connection closed by remote host`
4. Run `telepresence` and *try* to insert sudo password again — my result is that I can never press Return (see image)

![Screen Shot 2020-01-07 at 10 40 20](https://user-images.githubusercontent.com/284283/71885256-442ebf80-313a-11ea-9085-78a46ac51c09.png)

### What were you trying to do?

(please tell us)

### What did you expect to happen?

(please tell us)

### What happened instead?

(please tell us - the traceback is automatically included, see below.
use https://gist.github.com to pass along full telepresence.log)

### Automatically included information

Command line: `['/usr/local/bin/telepresence', '--namespace', 'kube-federation-system', '--swap-deployment', 'kubefed-controller-manager']`
Version: `0.103`
Python version: `3.7.5 (default, Nov  1 2019, 02:16:23) 
[Clang 11.0.0 (clang-1100.0.33.8)]`
kubectl version: `Client Version: v1.16.2 // Server Version: v1.13.5`
oc version: `(error: [Errno 2] No such file or directory: 'oc': 'oc')`
OS: `Darwin localhost 19.2.0 Darwin Kernel Version 19.2.0: Sat Nov  9 03:47:04 PST 2019; root:xnu-6153.61.1~20/RELEASE_X86_64 x86_64`

```
Traceback (most recent call last):
  File "/usr/local/bin/telepresence/telepresence/cli.py", line 135, in crash_reporting
    yield
  File "/usr/local/bin/telepresence/telepresence/main.py", line 68, in main
    socks_port, ssh = do_connect(runner, remote_info)
  File "/usr/local/bin/telepresence/telepresence/connect/connect.py", line 119, in do_connect
    args.from_pod
  File "/usr/local/bin/telepresence/telepresence/connect/connect.py", line 70, in connect
    raise RuntimeError("SSH to the cluster failed to start. See logfile.")
RuntimeError: SSH to the cluster failed to start. See logfile.

```

Logs:

```
 | QoS Class:       Burstable
 140.7 188 | Node-Selectors:  <none>
 140.7 188 | Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
 140.7 188 |                  node.kubernetes.io/unreachable:NoExecute for 300s
 140.7 188 | Events:
 140.7 188 |   Type    Reason     Age    From                 Message
 140.7 188 |   ----    ------     ----   ----                 -------
 140.7 188 |   Normal  Scheduled  2m16s  default-scheduler    Successfully assigned kube-federation-system/kubefed-controlle-b4ecfbbc5a9548258e4ff691f48b9e1f-7567f79n7wmf to i-rybg9xwe
 140.7 188 |   Normal  Pulling    2m15s  kubelet, i-rybg9xwe  pulling image "datawire/telepresence-k8s:0.103"
 140.7 188 |   Normal  Pulled     32s    kubelet, i-rybg9xwe  Successfully pulled image "datawire/telepresence-k8s:0.103"
 140.7 188 |   Normal  Created    32s    kubelet, i-rybg9xwe  Created container
 140.7 188 |   Normal  Started    31s    kubelet, i-rybg9xwe  Started container
 140.8 TEL | [188] ran in 0.59 secs.

```
