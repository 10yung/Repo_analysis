Reopend from #1501

    NoamLR:
        lr = optimizer.lr * model_size ** -0.5
             * min(step ** -0.5, step * warmup_step ** -1.5)
    WarmupLR:
        lr = optimizer.lr * warmup_step ** 0.5
             * min(step ** -0.5, step * warmup_step ** -1.5)

WarmupLR is almost same as NoamLR except for the scale factor for the base lr.
In this new scheduler, the maximum learning rate is scheduled to equal to the optimizer's learning rate as it. I think this is more intuitive.

With this new scheduler, I changed NoamLR to Deprecated.
I modified the model a little to make it fit my task and ran it with the WSJ recipe. In the training stage it worked well but when doing recognition, it gave error as follows. 

```
2020-01-19 15:57:39,849 (splitjson:40) INFO: /gs/hs0/tga-tslab/bowenz/espnet/tools/venv/bin/python3 /gs/hs0/tga-tslab/bowenz/espnet/egs/wsj/asr1/../../../utils/splitjson.py --parts 32 dump/test_dev93/deltafalse/data.json
2020-01-19 15:57:39,849 (splitjson:40) INFO: /gs/hs0/tga-tslab/bowenz/espnet/tools/venv/bin/python3 /gs/hs0/tga-tslab/bowenz/espnet/egs/wsj/asr1/../../../utils/splitjson.py --parts 32 dump/test_eval92/deltafalse/data.json
2020-01-19 15:57:39,881 (splitjson:52) INFO: number of utterances = 503
2020-01-19 15:57:39,882 (splitjson:52) INFO: number of utterances = 333
bash: line 1: 47824 Killed                  ( asr_recog.py --config conf/decode.yaml --ngpu 0 --backend pytorch --recog-json dump/test_dev93/deltafalse/split32utt/data.25.json --result-label exp/train_si284_pytorch_train_no_preprocess/decode_test_dev93_decode_lm_word65000/data.25.json --model exp/train_si284_pytorch_train_no_preprocess/results/model.last10.avg.best --word-rnnlm exp/train_rnnlm_pytorch_lm_word65000/rnnlm.model.best ) 2>> exp/train_si284_pytorch_train_no_preprocess/decode_test_dev93_decode_lm_word65000/log/decode.25.log >> exp/train_si284_pytorch_train_no_preprocess/decode_test_dev93_decode_lm_word65000/log/decode.25.log
bash: line 1: 47826 Killed                  ( asr_recog.py --config conf/decode.yaml --ngpu 0 --backend pytorch --recog-json dump/test_dev93/deltafalse/split32utt/data.5.json --result-label exp/train_si284_pytorch_train_no_preprocess/decode_test_dev93_decode_lm_word65000/data.5.json --model exp/train_si284_pytorch_train_no_preprocess/results/model.last10.avg.best --word-rnnlm exp/train_rnnlm_pytorch_lm_word65000/rnnlm.model.best ) 2>> exp/train_si284_pytorch_train_no_preprocess/decode_test_dev93_decode_lm_word65000/log/decode.5.log >> exp/train_si284_pytorch_train_no_preprocess/decode_test_dev93_decode_lm_word65000/log/decode.5.log
bash: line 1: 48096 Killed                  ( asr_recog.py --config conf/decode.yaml --ngpu 0 --backend pytorch --recog-json dump/test_eval92/deltafalse/split32utt/data.3.json --result-label exp/train_si284_pytorch_train_no_preprocess/decode_test_eval92_decode_lm_word65000/data.3.json --model exp/train_si284_pytorch_train_no_preprocess/results/model.last10.avg.best --word-rnnlm exp/train_rnnlm_pytorch_lm_word65000/rnnlm.model.best ) 2>> exp/train_si284_pytorch_train_no_preprocess/decode_test_eval92_decode_lm_word65000/log/decode.3.log >> exp/train_si284_pytorch_train_no_preprocess/decode_test_eval92_decode_lm_word65000/log/decode.3.log
```

The error led me to the log files decode.\*.log, but when I opened them, no error or traceback was shown in the log.
Also, most of the decode.\*.logs have already shown the predictions and recognition results, the results seem favorable enough, but still, I cannot avoid such 'line1: Killed' error. 
I've run the recognition part several times, it always gave three line1: killed errors, but the decode.\*.log files that were pointed to differed each time.
It would be very helpful if you can indicate what is probably going wrong as I don't really have any clue about it, thanks a lot.
In the directory:   egs/ljspeech/tts1/run.sh  line 41. teacher_model_path=""

I fist run the this scirpt for with train_config=conf/tuning/train_pytorch_transformer.v2.yaml

Now I want to train a fastspeech model, but I am not sure which path is the teacher_model_path, does anybody can help me?


**teacher_model_path=""**
teacher_decode_config=conf/decode_for_knowledge_dist.yaml
do_filtering=false     # whether to do filtering using focus rate
focus_rate_thres=0.65  # for phn taco2 around 0.65, phn transformer around 0.9
                       # if you want to do filtering please carefully check this threshold


thanks for any help

I just started to modify JSUT recipe to make it ASRTTS
Ref:
https://github.com/espnet/espnet/pull/818/files
creatorscan:asrtts3-v.0.7.0

After Stage 1, I found the above ASRTTS recipe uses X-Vector Pretained Model (English)
```
    # Check pretrained model existence
    if [ ! -e ${nnet_dir} ];then
        echo "X-vector model does not exist. Download pre-trained model."
        wget http://kaldi-asr.org/models/8/0008_sitw_v2_1a.tar.gz
        tar xvf 0008_sitw_v2_1a.tar.gz
        mv 0008_sitw_v2_1a/exp/xvector_nnet_1a exp
        rm -rf 0008_sitw_v2_1a.tar.gz 0008_sitw_v2_1a
    fi
    # Extract x-vector
    for name in ${train_set} ${dev_set} ${eval_set}; do
        sid/nnet3/xvector/extract_xvectors.sh --cmd "$train_cmd --mem 4G" --nj ${nj} \
            ${nnet_dir} data/${name}_mfcc \
            ${nnet_dir}/xvectors_${name}
    done
    # Update json
    for name in ${train_set} ${dev_set} ${eval_set}; do
        local/update_json.sh ${dumpdir}/${name}/data.json ${nnet_dir}/xvectors_${name}/xvector.scp
    done
```
I wonder if there is any pretained, X-Vector Model, if not, I guess I have to create by myself to apply ASRTTS to JSUT (assuming X-Vector Pretained model is language dependent)

Thank you!


I am getting this error. Any solution please?

> sudo ./run.sh --docker_gpu 7 --docker_egs chime4/asr1 --docker_folders /export/corpora4/CHiME4/CHiME3 --dlayers 1 --ngpu 1
[sudo] password for shafkat: 
Building docker image...
Now running docker build --build-arg FROM_TAG=gpu-cuda10.0-cudnn7-u18 --build-arg THIS_USER=shafkat --build-arg THIS_UID=0 -f prebuilt/Dockerfile -t espnet/espnet:gpu-cuda10.0-cudnn7-u18-user-shafkat .
Sending build context to Docker daemon  56.32kB
Step 1/8 : ARG FROM_TAG
Step 2/8 : FROM espnet/espnet:${FROM_TAG}
 ---> 937b583f3b76
Step 3/8 : LABEL maintainer "Nelson Yalta <nyalta21@gmail.com>"
 ---> Using cache
 ---> 5ed7c573bb6d
Step 4/8 : ARG THIS_USER
 ---> Using cache
 ---> 4069c79f1b2e
Step 5/8 : ARG THIS_UID
 ---> Using cache
 ---> ce5ad4c6bb5e
Step 6/8 : RUN if [ ! -z "${THIS_UID}"  ]; then     useradd -m -r -u ${THIS_UID} -g root ${THIS_USER};     fi
 ---> Running in c48c8624f984
useradd: UID 0 is not unique
The command '/bin/sh -c if [ ! -z "${THIS_UID}"  ]; then     useradd -m -r -u ${THIS_UID} -g root ${THIS_USER};     fi' returned a non-zero code: 4

Hi, there!

I'm currently decoding my CTC model (mtlalpha 1.0) with pure CTC mode (ctc_weight 1.0), and find that in current implementation, ctc-prefix-search will also compute \<blank\> token's next-label-score, which to my understanding shall be excluded. Details below: 

With ctc_weight=1.0, **ctc_beam** becomes the size of whole vocabulary (including \<blank\> token)

https://github.com/espnet/espnet/blob/28ce90a17148afcb36f4e593966911b9c3a6230b/espnet/nets/pytorch_backend/e2e_asr_transformer.py#L291-L296

Then \<blank\> token-id will be included in **local_best_ids**, and prefix-score computed. To my understanding, \<blank\> should be excluded here.

https://github.com/espnet/espnet/blob/28ce90a17148afcb36f4e593966911b9c3a6230b/espnet/nets/pytorch_backend/e2e_asr_transformer.py#L328-L335


This is what I get after I run  **make check_install** command:

> [ 92%] Building CXX object CMakeFiles/test_time.dir/tests/random.cpp.o
[100%] Linking CXX executable test_time
make[3]: Leaving directory '/data/ahnaf/espnet_asr/espnet/tools/warp-transducer/build'
[100%] Built target test_time
make[2]: Leaving directory '/data/ahnaf/espnet_asr/espnet/tools/warp-transducer/build'
make[1]: Leaving directory '/data/ahnaf/espnet_asr/espnet/tools/warp-transducer/build'
. venv/bin/activate; cd warp-transducer/pytorch_binding && python setup.py install
running install
running bdist_egg
running egg_info
creating warprnnt_pytorch.egg-info
writing warprnnt_pytorch.egg-info/PKG-INFO
writing dependency_links to warprnnt_pytorch.egg-info/dependency_links.txt
writing top-level names to warprnnt_pytorch.egg-info/top_level.txt
writing manifest file 'warprnnt_pytorch.egg-info/SOURCES.txt'
reading manifest file 'warprnnt_pytorch.egg-info/SOURCES.txt'
writing manifest file 'warprnnt_pytorch.egg-info/SOURCES.txt'
installing library code to build/bdist.linux-x86_64/egg
running install_lib
running build_py
creating build
creating build/lib.linux-x86_64-3.6
creating build/lib.linux-x86_64-3.6/warprnnt_pytorch
copying warprnnt_pytorch/__init__.py -> build/lib.linux-x86_64-3.6/warprnnt_pytorch
running build_ext
building 'warprnnt_pytorch.warp_rnnt' extension
creating build/temp.linux-x86_64-3.6
creating build/temp.linux-x86_64-3.6/src
gcc -pthread -B /data/ahnaf/espnet_asr/espnet/tools/venv/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/data/ahnaf/espnet_asr/espnet/tools/warp-transducer/include -I/data/ahnaf/espnet_asr/espnet/tools/venv/lib/python3.6/site-packages/torch/lib/include -I/data/ahnaf/espnet_asr/espnet/tools/venv/lib/python3.6/site-packages/torch/lib/include/torch/csrc/api/include -I/data/ahnaf/espnet_asr/espnet/tools/venv/lib/python3.6/site-packages/torch/lib/include/TH -I/data/ahnaf/espnet_asr/espnet/tools/venv/lib/python3.6/site-packages/torch/lib/include/THC -I/data/ahnaf/espnet_asr/espnet/tools/venv/include/python3.6m -c src/binding.cpp -o build/temp.linux-x86_64-3.6/src/binding.o -std=c++11 -fPIC -DWARPRNNT_ENABLE_GPU -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=warp_rnnt -D_GLIBCXX_USE_CXX11_ABI=0
cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
In file included from /data/ahnaf/espnet_asr/espnet/tools/venv/lib/python3.6/site-packages/torch/lib/include/THC/THC.h:4:0,
                 from src/binding.cpp:8:
/data/ahnaf/espnet_asr/espnet/tools/venv/lib/python3.6/site-packages/torch/lib/include/THC/THCGeneral.h:17:10: fatal error: cublas_v2.h: No such file or directory
 #include "cublas_v2.h"
          ^~~~~~~~~~~~~
compilation terminated.
error: command 'gcc' failed with exit status 1
Makefile:98: recipe for target 'warp-transducer.done' failed
make: *** [warp-transducer.done] Error 1
(espnet) shafkat@server-master:/data/ahnaf/espnet_asr/espnet/tools$

Things I have done to install:
1. Installed the kaldi package using conda - **conda install -c conda-forge kaldi**
2. Cloned the espnet repo from github
3. Used this command: **make -j 10 PYTHON_VERSION=3.6 CUDA_VERSION=10.0**. Got kaldi.done error though.
4. Used this command after then: **make check_install** .  And got the above-mentioned error.

Am I missing something to do?
Anyone test this.

This PR is too complex to explain, so I'd like to show the examples only:

e.g.  2nodes and 2 processes per node, i.e. world_size==4, without any "--launcher"

```
# --multiprocessing_distributed is true by default
(host1) % python -m espnet2.bin.asr_train \
    --ngpu 2 \
    --dist_rank 0  \
    --dist_world_size 2  \
    --dist_master_addr host1  \
    --dist_master_port 20000
(host2) % python -m espnet2.bin.asr_train \
    --ngpu 2 \
    --dist_rank 1  \
    --dist_world_size 2  \
    --dist_master_addr host1  \
    --dist_master_port 20000
```

e.g.  2nodes and 2 processes per node, i.e. world_size==4,  with "--launcher"

```
# with slurm
% srun -c2 -N2 --ntasks-per-node 1 --gres gpu:2 python -m espnet2.bin.asr_train \
    --ngpu 2 \
    --dist_init_method "file://$(pwd)/exp/asr_train/.dist_init_$(openssl rand -base64 12)" \
    --dist_launcher slurm

# with mpi
# This doesn't mean backend="mpi". 
% mpirun -np 2 python -m espnet2.bin.asr_train \
    --ngpu 2 \
    --dist_init_method "file://$(pwd)/exp/asr_train/.dist_init_$(openssl rand -base64 12)" \
    --dist_launcher mpi
```

(If your file system doesn't support fcntl, then use `--dist_master_addr` and  `--dist_master_port`)

e.g.  Single node and multi-GPUs

```
# "multiprocessing-distributed" mode (default).
# This mode is more efficient than Conventional DataParallel
% python -m espnet2.bin.asr_train --ngpu 2

# Conventional DataParallel multi-GPUs
% python -m espnet2.bin.asr_train --multiprocessing_distributed false --ngpu 2
```

I added `drop_last` arguments for Batch Sampler. 
  - ~~For training, drop_last is true. In Distributed training, mini-batch is divided by worldsize and each worker must have 1 or more batch-size. To avoid 0-batchsize, drop_last=true for training~~.
  - For training, drop_last is false.
 - For validation, drop_last is false. ~~Validation mode perform only at RANK==0 worker.~~
 - For inference, drop_last is false.
 - By default,  drop_last is false.
the PR is related to #1490.
it also takes some findings from #1497 for running.

- [ ] ami
- [ ] babel
- [ ] commonvoice

The asr is still in training - waiting for the result