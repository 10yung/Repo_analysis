**Important notices**
Before you add a new report, we ask you kindly to acknowledge the following:

[yes] I have read the contributing guide lines at https://github.com/opnsense/core/blob/master/CONTRIBUTING.md

[yes] I have searched the existing issues and I'm convinced that mine is new.

**Is your feature request related to a problem? Please describe.**
I have a pf states inconsistence from time to time on a PBX behind an OPNSense firewall.
My PBX loses conectivity with a trunk provider and I can't figure out why.
At first this happend when my ISP changed the public IP, but it hasn't changed for a long while and the problem persists.

**Describe the solution you'd like**
I'd like to automate the steps I found to be the work around to this issue.
Actually I go to Firewall > Diagnostics > State Dump and filter the states related to the VOIP trunk provider IP address.
The state table shows two states, and then, after the filter is applied I press the kill button and when the PBX registers again it generates new states and the problem is gone.
I would like a new option in the Command combo box of the Edit Job menu at System > Settings > Cron.
The new command would be named 'Kill states' and the filter would be set in the Parameter box below the Command combo.
The parameter could accept IPs or Aliases.
I could execute this command every hour and in the worst case the VOIP service would fail for that time.

**Describe alternatives you've considered**
I have tested pfctl command with the -k command using the IP address of the trunk provider and it worked.
The alternative is to add the cron job manually from a commnad shell but it will not survive system updates.

**Additional context**

![imagen](https://user-images.githubusercontent.com/20299175/72550443-80b49680-3871-11ea-843b-f0d49f57a75b.png)

I have the following error message in my logs 

```config[51091]: Model OPNsense\Diagnostics\Netflow can't be saved, skip ( Phalcon\Validation\Exception: [OPNsense\Diagnostics\Netflow:capture.interfaces] WAN interfaces missing in listening interfaces: wan in /usr/local/opnsense/mvc/app/models/OPNsense/Base/BaseModel.php:540 Stack trace: #0 /usr/local/opnsense/mvc/app/models/OPNsense/Base/BaseModel.php(650): OPNsense\Base\BaseModel->serializeToConfig() #1 /usr/local/opnsense/mvc/script/run_migrations.php(57): OPNsense\Base\BaseModel->runMigrations() #2 {main} )```

I have had this happen for at least the last few releases but cannot remember when I started to get this

```***GOT REQUEST TO UPGRADE: all***
Updating OPNsense repository catalogue...
OPNsense repository is up to date.
All repositories are up to date.
Updating OPNsense repository catalogue...
OPNsense repository is up to date.
All repositories are up to date.
Checking for upgrades (1 candidates): . done
Processing candidates (1 candidates): . done
The following 1 package(s) will be affected (of 0 checked):

Installed packages to be UPGRADED:
	opnsense: 19.7.9 -> 19.7.9_1

Number of packages to be upgraded: 1

4 MiB to be downloaded.
[1/1] Fetching opnsense-19.7.9_1.txz: .......... done
Checking integrity... done (0 conflicting)
[1/1] Upgrading opnsense from 19.7.9 to 19.7.9_1...
[1/1] Extracting opnsense-19.7.9_1: .......... done
Stopping configd...done
Resetting root shell
Updating /etc/shells
Unhooking from /etc/rc
Unhooking from /etc/rc.shutdown
Updating /etc/shells
Registering root shell
Hooking into /etc/rc
Hooking into /etc/rc.shutdown
Starting configd.
Keep version OPNsense\Monit\Monit (1.0.8)
Keep version OPNsense\Firewall\Alias (1.0.0)
Keep version OPNsense\OpenVPN\Export (0.0.1)
Keep version OPNsense\CaptivePortal\CaptivePortal (1.0.0)
Keep version OPNsense\Cron\Cron (1.0.1)
Keep version OPNsense\Backup\NextcloudSettings (1.0.0)
Keep version OPNsense\TrafficShaper\TrafficShaper (1.0.3)
Keep version OPNsense\IDS\IDS (1.0.3)
Keep version OPNsense\Proxy\Proxy (1.0.3)
*** OPNsense\Diagnostics\Netflow Migration failed, check log for details
Keep version OPNsense\Routes\Route (1.0.0)
Keep version OPNsense\AcmeClient\AcmeClient (1.6.0)
Keep version OPNsense\Dnscryptproxy\Forward (0.1.0)
Keep version OPNsense\Dnscryptproxy\General (0.1.0)
Keep version OPNsense\Dnscryptproxy\Whitelist (0.1.0)
Keep version OPNsense\Dnscryptproxy\Server (1.0.0)
Keep version OPNsense\Dnscryptproxy\Dnsbl (1.0.0)
Keep version OPNsense\Dnscryptproxy\Cloak (0.1.0)
Keep version OPNsense\Syslog\Syslog (1.0.0)
Keep version OPNsense\Wireguard\General (0.0.1)
Keep version OPNsense\Wireguard\Server (0.0.2)
Keep version OPNsense\Wireguard\Client (0.0.4)
Keep version OPNsense\IPsec\IPsec (0.0.0)
Writing firmware setting...done.
Writing trust files...done.
Configuring login behaviour...done.
Configuring system logging...done.
=====
Message from opnsense-19.7.9_1:

--
Roar!
Checking integrity... done (0 conflicting)
Nothing to do.
The following package files will be deleted:
	/var/cache/pkg/opnsense-19.7.9_1-d0c48ea6ed.txz
	/var/cache/pkg/opnsense-19.7.9_1.txz
The cleanup will free 4 MiB
Deleting files: .. done
All done
Starting web GUI...done.
Generating RRD graphs...done.
***DONE***
```

Not sure why this started happening as I have not touched the netflow stuff at all

![image](https://user-images.githubusercontent.com/3628354/72384536-8e550980-3715-11ea-90c3-63d213679f34.png)




[x] I have read the contributing guide lines at https://github.com/opnsense/core/blob/master/CONTRIBUTING.md

[x] I have searched the existing issues and I'm convinced that mine is new.

**Is your feature request related to a problem? Please describe.**
When working with ip aliases, VIPs, etc. there may exist several addresses within the same subnet. This will cause warning when using 'squid -k check'

squid.conf
```
acl localnet src 10.8.0.0/24 # Possible internal network (interfaces v4)
acl localnet src 2002:db8:b00:9080::/64 # Possible internal network (interfaces v6)
acl localnet src 10.8.0.4/32 # Possible internal network (aliases)
acl localnet src 2002:db8:b00:9080::4/64 # Possible internal network (aliases)
acl localnet src 192.168.2.0/24 # Possible internal network (interfaces v4)
acl localnet src 192.168.2.136/32 # Possible internal network (aliases)
acl localnet src 192.168.2.137/32 # Possible internal network (aliases)
acl localnet src 192.168.2.138/32 # Possible internal network (aliases)
acl localnet src 192.168.2.132/32 # Possible internal network (aliases)

```
squid -k check
```
2020/01/14 08:09:25| WARNING: (A) '10.8.0.4' is a subnetwork of (B) '10.8.0.0/24'
2020/01/14 08:09:25| WARNING: because of this '10.8.0.4' is ignored to keep splay tree searching predictable
2020/01/14 08:09:25| WARNING: You should probably remove '10.8.0.4' from the ACL named 'localnet'
2020/01/14 08:09:25| aclIpParseIpData: WARNING: Netmask masks away part of the specified IP in '2002:db8:b00:1020::4/64'
2020/01/14 08:09:25| WARNING: (B) '2002:db8:b00:1020::/64' is a subnetwork of (A) '2002:db8:b00:1020::/64'
2020/01/14 08:09:25| WARNING: because of this '2001:7c0:b00:1020::/64' is ignored to keep splay tree searching predictable
2020/01/14 08:09:25| WARNING: You should probably remove '2001:7c0:b00:9080::/64' from the ACL named 'localnet'
2020/01/14 08:09:25| WARNING: (A) '192.168.2.136' is a subnetwork of (B) '192.168.2.0/24'
2020/01/14 08:09:25| WARNING: because of this '192.168.2.136' is ignored to keep splay tree searching predictable
2020/01/14 08:09:25| WARNING: You should probably remove '192.168.2.136' from the ACL named 'localnet'
2020/01/14 08:09:25| WARNING: (A) '192.168.2.136' is a subnetwork of (B) '192.168.2.0/24'
2020/01/14 08:09:25| WARNING: because of this '192.168.2.136' is ignored to keep splay tree searching predictable
2020/01/14 08:09:25| WARNING: You should probably remove '192.168.2.136' from the ACL named 'localnet'
2020/01/14 08:09:25| WARNING: (A) '192.168.2.137' is a subnetwork of (B) '192.168.2.0/24'
2020/01/14 08:09:25| WARNING: because of this '192.168.2.137' is ignored to keep splay tree searching predictable
2020/01/14 08:09:25| WARNING: You should probably remove '192.168.2.137' from the ACL named 'localnet'
2020/01/14 08:09:25| WARNING: (A) '192.168.2.138' is a subnetwork of (B) '192.168.2.0/24'
2020/01/14 08:09:25| WARNING: because of this '192.168.2.138' is ignored to keep splay tree searching predictable
2020/01/14 08:09:25| WARNING: You should probably remove '192.168.2.138' from the ACL named 'localnet'
2020/01/14 08:09:25| WARNING: (A) '192.168.2.132' is a subnetwork of (B) '192.168.2.0/24'
2020/01/14 08:09:25| WARNING: because of this '192.168.2.132' is ignored to keep splay tree searching predictable
2020/01/14 08:09:25| WARNING: You should probably remove '192.168.2.132' from the ACL named 'localnet'
```

**Describe the solution you'd like**
Run the src ips through aggregate (https://www.freebsd.org/cgi/man.cgi?query=aggregate).
Unforunatelly I am not familar with Jinja and how/whether you can filter via external commands. One must build one list from core/src/opnsense/service/templates/OPNsense/Proxy/squid.conf  lines 120 (interface addr) and 129 (VIPs), pipe it through aggregate and then build the acls.

I think aggregate will just handle ipv4.

**Describe alternatives you've considered**
If template engine cannot call external filters (I fear so, at least without additional python code), you could use php when save button is pressed to aggregate the IPs and write result as config entry to config.xml for localnets which can be used directly by template engine.
**Describe the bug**
The MAC address database used to get e.g. the hardware vendor from a mac address in the DHCP -> Leases page is pretty outdated.
According to [this forum post](https://forum.opnsense.org/index.php?topic=10380.0#msg47557), OPNsense uses the netaddr Python lib, which hasn't been updated since January 2017 and has no self-update mechanism built in. From the issues in their Github project, this has been mentioned multiple times, but it seems to be rather dead.

**To Reproduce**
1. Go to Services -> DHCPv4 -> Leases (status_dhcp_leases.php)
2. Have any recent device on your network
3. See vendor names only for older hardware.

**Expected behavior**
Show vendor names for all MAC addresses.

**Additional context**
Maybe using the mac-vendor-lookup Python library is a valid replacement:
https://pypi.org/project/mac-vendor-lookup/

Add ipv6 addresses to listening interfaces. Also add ipv6 transparent mode listeners:
- Either VIPs if using CARP
- Or interface address if not using CARP

All you need to do is adding a forwarding rule to local interface ip6 address for transparent mode, e.g.

int: lan:
src: lan
dst: any
proto: inet6
dstport: 80
target: lan-ipv6 address

ToDo: Add 'transparentv6_proxy' template in
https://github.com/opnsense/core/blob/master/src/www/firewall_nat_edit.php
**Describe the bug**
Hi, the failover based on latency are buggy, when latency is high the gateway are removed from group gateways many times, and when the latency is normal again, the gateway returns to group only one time, and the connections don't go through it and I need apply firewall rules again.

**Expected behavior**
If latency is already high, and the gateway was removed from group gateways, it not should removed again and again

**Relevant log files**
Jan 8 01:31:08	monit[39059]: 'gateway_alert' status succeeded (0) -- no output
Jan 8 01:29:08	monit[39059]: 'gateway_alert' status failed (1) -- MONITOR: FLINGW has high latency, removing from routing group ESSSFLNGW
Jan 8 01:27:08	monit[39059]: 'gateway_alert' status failed (1) -- MONITOR: FLINGW has high latency, removing from routing group ESSSFLNGW
Jan 8 01:25:08	monit[39059]: 'gateway_alert' status failed (1) -- MONITOR: FLINGW has high latency, removing from routing group ESSSFLNGW
Jan 8 01:23:07	monit[39059]: 'gateway_alert' status failed (1) -- MONITOR: FLINGW has high latency, removing from routing group ESSSFLNGW
Jan 8 01:21:07	monit[39059]: 'gateway_alert' status failed (1) -- MONITOR: FLINGW has high latency, removing from routing group ESSSFLNGW
Jan 8 01:19:07	monit[39059]: 'gateway_alert' status failed (1) -- MONITOR: FLINGW has high latency, removing from routing group ESSSFLNGW
Jan 8 01:17:07	monit[39059]: 'gateway_alert' status failed (1) -- MONITOR: FLINGW has high latency, removing from routing group ESSSFLNGW
Jan 8 01:15:07	monit[39059]: 'gateway_alert' status failed (1) -- MONITOR: FLINGW has high latency, removing from routing group ESSSFLNGW
Jan 8 01:13:07	monit[39059]: 'gateway_alert' status failed (1) -- MONITOR: FLINGW has high latency, removing from routing group ESSSFLNGW
Jan 8 01:11:07	monit[39059]: 'gateway_alert' status failed (1) -- MONITOR: FLINGW has high latency, removing from routing group ESSSFLNGW
Jan 8 01:09:06	monit[39059]: 'gateway_alert' status failed (1) -- MONITOR: FLINGW has high latency, removing from routing group ESSSFLNGW
Jan 8 01:07:06	monit[39059]: 'gateway_alert' status failed (1) -- MONITOR: FLINGW has high latency, removing from routing group ESSSFLNGW

**Environment**
OPNsense 19.781 (amd64, OpenSSL).

**Important notices**
Before you add a new report, we ask you kindly to acknowledge the following:

[x] I have read the contributing guide lines at https://github.com/opnsense/core/blob/master/CONTRIBUTING.md

[x] I have searched the existing issues and I'm convinced that mine is new.

**Is your feature request related to a problem? Please describe.**
no

**Describe the solution you'd like**
add some VPN Tunnel states (Tunnel, Phase 1 /2) into the API for monitoring purposes:

- API should contain the status of the VPN Tunnel as shown in the OpnSense Dashboard (see screenshot)
- API should contain the status of the Phase2 Entries as shown in VPN -> IPSec -> Status Overview

**Describe alternatives you've considered**
parsing the vpn status of the dashboard - will run into issues upon updates of opnsense - not a valuable solution

**Additional context**
![image](https://user-images.githubusercontent.com/3057403/71886693-efd90f00-313c-11ea-9c46-df9acec14a86.png)
At present, visibility of end-user identity is lost within OPNsense when accessing the Administration front-end through reverse proxy. Proposed here is native lighttpd support for capturing these data, which might serve as a baseline for expanded configuration scenarios moving forward. Shown here is the result: https://i.imgur.com/AHwlyTC.png

I have also tested the direct, non-proxied access of OPNsense via the configured lighttpd port (rather than nginx port), which results in the expected logging of end-user IP.

mod_extforward exists in the current version of lighttpd distributed with OPNsense, so no additional components are required beyond this revision to lighty-webConfigurator.conf. 

Of note:
1. I'm not certain that the inclusion of mod_extforward at line 221 is the expected behavior of core developers. Please confirm that this does not belong at line 238, noting that this module **must** be loaded after both mod_openssl and mod_accesslog to function properly.
2. The configuration proposed here supports proxied connections from localhost, which will not meet the needs of all deployments. Although the mod does support _extforward.forwarder = ( "all" => "trust")_ this is clearly a bad path-forward, so consideration may be given to adding a "Support Proxied Connections" option at System: Settings: Administration, wherein the user can select a checkbox and add a list of proxy IPs to allow. The intention of this functionality should be further clarified via help text to avoid user perception of this setting effectively _enabling_ a reverse proxy vs. only supporting connections from them.
3. mod_extforward also supports connections from HAProxy but enabling this configuration is beyond the scope of this pull request. I've acknowledged this observation in the contents of the code.
**Important notices**
Before you add a new report, we ask you kindly to acknowledge the following:

[x] I have read the contributing guide lines at https://github.com/opnsense/core/blob/master/CONTRIBUTING.md

[x] I have searched the existing issues and I'm convinced that mine is new.

**Bug decscription**
Walking through the Wizard, configuring a static IPv4 WAN, still 2 gateways get configured:
1. The IPv4 "WANGW" corresponding to your static WAN configuration. It will show as offline in System: Gateways: Single. 
2. The IPv6 "WAN_DHCP6" from the default DHCP setting for IPv6 of the interface.  It will be online in System: Gateways: Single, and cannot be removed there (of course, comes from interface configuration). Note: Getting state "online" happens even when no IPv6 configuration / DHCP is present.

**To Reproduce**
Reset to defaults.
Wizard: Configure static IPv4 WAN, have no DHCP on WAN side. 
Test ping to some WAN address, fails with "ping: sendto: No route to host"

**Expected behavior**
1. Disable IPv6 DHCP
2. Allow IPv6 setting (e.g. IPv6 Configuration Type: None) in Wizard

**Workaround**
1. After running through the wizard, go to Interfaces: WAN, and set "IPv6 Configuration Type: None".
2. Edit the IPv4 GW, changing nothing, save. The IPv4  GW will now change to state online.
3. ping works now. 

**Additional context**
related to #3874 

**Environment**
19.7.8

**Important notices**
Before you add a new report, we ask you kindly to acknowledge the following:

[x] I have read the contributing guide lines at https://github.com/opnsense/core/blob/master/CONTRIBUTING.md

[x] I have searched the existing issues and I'm convinced that mine is new.

**Use case**
Be able to reconfigure the system right after a configuration reset and reboot, without need to manually power on (e.g. firewall being remote, having separate connection to the first LAN port getting configured to default 192.168.1.1/24)

**Current behaviour**
"Shut down after changes are complete".

**Proposed solution**
Remove the "yes" button, instead show 2 buttons 
1. Load defaults and poweroff
2. Load defaults and reboot

As an extra, add a confirmation dialog before performing the config reset.
