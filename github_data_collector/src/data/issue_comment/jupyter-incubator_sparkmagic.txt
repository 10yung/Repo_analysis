Creates a PR template to make expectations clear when contributing 


------
### Description
<!-- FILL IN -->

### Checklist
- [ ] Wrote description of my changes above
- [ ] Added or modified unit tests to reflect my changes
- [ ] Manually tested with a notebook
- [ ] If adding a feature, there is an example notebook and/or documentation in the `README`

As addressed in issue #614 this PR proposes the following changes
- Using http clients maps on the `SparkController` class to reuse clients since it is not needed a new client per cell
- Using `requests.Session` instead of plain `requests` to ensure we store the generated Kerberos ticket cookie instead of negotiating the ticket in every request
- Add a configuration option to override the default `HTTPKerberosAuth` constructor
Due to the following line `self._auth = HTTPKerberosAuth(mutual_authentication=REQUIRED)` in this class https://github.com/jupyter-incubator/sparkmagic/blob/master/sparkmagic/sparkmagic/livyclientlib/reliablehttpclient.py#L16, the Kerberos auth constructor is not able to be configured which causes the sparkmagic to always sends 2 requests to Livy, the first sending 401 and the second 200 since the cookie was issued, but since we already have the cookie due to the first 401 why we are getting a new one everytime?

An ideal approach would be, to be able to pass custom args to the kerberos auth constructor  through the `config.json` and to change the `reliablehttpclient.py` to use the requests session (https://requests.readthedocs.io/en/master/user/advanced/#session-objects) instead of plain `requests`

Here is the kerberos auth constructor for reference to a possible solution
```python
class HTTPKerberosAuth(AuthBase):
    """Attaches HTTP GSSAPI/Kerberos Authentication to the given Request
    object."""
    def __init__(
            self, mutual_authentication=REQUIRED,
            service="HTTP", delegate=False, force_preemptive=False,
            principal=None, hostname_override=None,
            sanitize_mutual_error_response=True, send_cbt=True):
```

I am submitting this issue to see if anyone is already working on it, if not my plan is to submit a PR for this problem

I was getting the 400 error  Missing Required Header for CSRF protection, when trying to add an endpoint. I added the X-Requested-By Header in livyreliablehttpclient, headers variable, and in reliablehttpclient too for good measure. 

I am able to successfully add the endpoint now, but when I try to create session is throwing the same 400 CSRF error. I imagine its something I'm doing wrong, but not sure where to go from here.


```
~/.local/lib/python3.7/site-packages/sparkmagic/livyclientlib/reliablehttpclient.py in post(self, relative_url, accepted_status_codes, data)
     48         return self._send_request(relative_url, accepted_status_codes, requests.get)
     49 
---> 50     def post(self, relative_url, accepted_status_codes, data):
     51         """Sends a post request. Returns a response."""
     52         return self._send_request(relative_url, accepted_status_codes, requests.post, data)

~/.local/lib/python3.7/site-packages/sparkmagic/livyclientlib/reliablehttpclient.py in _send_request(self, relative_url, accepted_status_codes, function, data)
     55         """Sends a delete request. Returns a response."""
     56         return self._send_request(relative_url, accepted_status_codes, requests.delete)
---> 57 
     58     def _send_request(self, relative_url, accepted_status_codes, function, data=None):
     59         print(self.compose_url(relative_url))

~/.local/lib/python3.7/site-packages/sparkmagic/livyclientlib/reliablehttpclient.py in _send_request_helper(self, url, accepted_status_codes, function, data, retry_count)
     94                 if error:
     95                     raise HttpClientException(u"Error sending http request and maximum retry encountered.")
---> 96                 else:
     97                     raise HttpClientException(u"Invalid status code '{}' from {} with error payload: {}"
     98                                               .format(status, url, text))

HttpClientException: Invalid status code '400' from http://td2vspk0.travp.net:8999/sessions with error payload: <html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=ISO-8859-1"/>
<title>Error 400 </title>
</head>
<body>
<h2>HTTP ERROR: 400</h2>
<p>Problem accessing /sessions. Reason:
<pre>    Missing Required Header for CSRF protection.</pre></p>
<hr /><i><small>Powered by Jetty://</small></i>
</body>
</html>
```
**Describe the bug**
In AWS EMR cluster I am using sparkmagic to run a spark job on the cluster that I want to display as a bqplot widget in jupyterhub that is running in Docker container.  Instead of displaying as a widget it shows the definition of the widget in code.  I have verified that node.js is installed on both the container and the cluster and that the environment variables are set correctly, But I haven't been able to get the results to display properly

**To Reproduce**
Create an EMR-5.28.0 cluster with Hadoop, Spark, JupyterHub, Hive, Hue, Zookeeper.  In your bootstrap install bqplot==0.11.6, numpy==1.16.4, ipywidgets==7.5.0, jupyter-contrib-nbextensions==0.5.1, jupyter-nbextensions-configurator==0.4.1, widgetsnbextension==3.5.0, nodejs-v12.14.0

Then run
jupyter nbextension enable --system --py widgetsnbextension
jupyter nbextension enable --system --py bqplot

In a step or in the docker image on the cluster install the same packages as above and run the enables.

Create a pyspark3 notebook in jupyterhub and run this code:
``` python
from bqplot import pyplot as plt
import numpy as np

plt.figure(1, title='line chart')
np.random.seed(120)
n = 200
x = np.linspace(0.0, 10.0, n)
y = np.cumsum(np.random.randn(n))
plt.plot(x, y)
plt.show()
```
Instead of a graph like the one in the Screenshots section you will get a definition of your spark job and output like:

``` python
VBox(children=(Figure(fig_margin={'top': 60, 'bottom': 60, 'left': 60, 'right': 60}, 
layout=Layout(min_width='125px'), scale_x=LinearScale(allow_padding=False, max=1.0, min=0.0), 
scale_y=LinearScale(allow_padding=False, max=1.0, min=0.0), title='line chart'), 
Toolbar(figure=Figure(fig_margin={'top': 60, 'bottom': 60, 'left': 60, 'right': 60}, 
layout=Layout(min_width='125px'), scale_x=LinearScale(allow_padding=False, max=1.0, min=0.0), 
scale_y=LinearScale(allow_padding=False, max=1.0, min=0.0), title='line chart'))))
```

**Expected behavior**
A graph looking like the one in the attached Screenshot

**Screenshots**
![bqplot chart](https://user-images.githubusercontent.com/37423418/71587941-5169ff80-2ae5-11ea-9d69-9969647236fb.png)


**Versions:**
 - SparkMagic 0.12.7
 - Livy (if you know it) 0.6.0
 - Spark 2.4.4
- Python 3.6.8

**Additional context**
This is an extremely dumbed down version of what I am trying to do but this demonstrates my issue we are also wanting to use qgrid widgets as well.

If a column has null value in every row/record, %%sql will not drop that entire column.

To reproduce, create a table where a column has only null values, e.g.
%%sql
insert into table
values   (1, null),
         (2, null),
         (3, null)

I have attached screenshots using results from %%sql and spark.sql()

[Screen Shot 2019-12-26 at 2.50.52 pm.pdf](https://github.com/jupyter-incubator/sparkmagic/files/4001303/Screen.Shot.2019-12-26.at.2.50.52.pm.pdf)


**Versions:**
 - SparkMagic 0.12.0
 - Livy 0.6.0
 - Kernel: Spark

**Additional context**
I believe the problem comes from the fact that since JSON doesn't pick up null values, when the data got converted into dict and then converted into dataframe, it couldn't have known that there was a missing column:

https://github.com/jupyter-incubator/sparkmagic/blob/master/sparkmagic/sparkmagic/utils/utils.py#L52
https://github.com/jupyter-incubator/sparkmagic/blob/master/sparkmagic/sparkmagic/livyclientlib/sqlquery.py#L58

We need a way to pick up the schema before populating all the data.



Right now documentation is a brief README and a bunch of examples in Jupyter notebooks that also double as a manual test suite. We should really have real documentation.

As per https://www.divio.com/blog/documentation/, this would need:

* Tutorial: "Your first SparkMagic query"
* Howtos: various real-world use cases
* Reference: Config options, APIs
* Explanation: How it works
@devstein @juhoautio @suhsteve (assuming you're not already getting notified of new issues?)

Here's my current checklist for accepting PRs, should probably add it to docs somewhere:

* Has unit tests for change.
* If it's adding feature, has examples in the example notebooks.
* Documented in README where relevant.
* Manual testing on notebook.

See also the https://github.com/jupyter-incubator/sparkmagic/milestone/7 milestone in general.
1. Python 2 is no longer being maintained as of Jan 1, 2020.
2. The first PySpark release in 2020 will drop Python 2.

Users on Python 2 can continue to use old versions of Sparkmagic, but we should be strongly discouraging Python 2 usage.

Important: add metadata to `setup.py` to say "this only supports Python 3 or later", and then `pip` will automatically pick the right version of SparkMagic. Specifically `python_requires=">=3.5"` or something along those lines.
We should run flake8, and perhaps pylint, on the code as part of CI.