<!--
Thanks for using pyspider!

如果你需要使用中文提问，请将问题提交到 https://segmentfault.com/t/pyspider
-->

* pyspider version:
* Operating system:
* Start up command:

### Expected behavior

<!-- What do you think should happen? -->

### Actual behavior

<!-- What actually happens? -->

### How to reproduce

<!-- 

The best chance of getting help is providing enough information that can be reproduce the issue you have.

If it's related to API or extraction behavior, please paste the script of your project.
If it's related to scheduling of whole project, please paste the screenshot of queue status on the top in dashboard.

-->

但是在web控制台页面上端看不到有scheduler、fetcher、processor被阻塞。
所以这算是bug吗？


* pyspider version:0.3.10
* Operating system:Win10 64Bit
* Start up command:pyspider all

Dear All:
When I crawl a website, the error comes

`
[E 191205 16:21:21 base_handler:203] netloc '|file|中英双字.rmvb|' contains invalid characters under NFKC normalization
    Traceback (most recent call last):
      File "d:\programdata\anaconda3\envs\py36\lib\site-packages\pyspider\libs\base_handler.py", line 196, in run_task
        result = self._run_task(task, response)
      File "d:\programdata\anaconda3\envs\py36\lib\site-packages\pyspider\libs\base_handler.py", line 176, in _run_task
        return self._run_func(function, response, task)
      File "d:\programdata\anaconda3\envs\py36\lib\site-packages\pyspider\libs\base_handler.py", line 155, in _run_func
        ret = function(*arguments[:len(args) - 1])
      File "<renren_film>", line 41, in detail_page
      File "d:\programdata\anaconda3\envs\py36\lib\site-packages\pyspider\libs\response.py", line 146, in doc
        doc.make_links_absolute(utils.text(self.url))
      File "d:\programdata\anaconda3\envs\py36\lib\site-packages\pyquery\pyquery.py", line 1514, in make_links_absolute
        self('a').each(repl('href'))
      File "d:\programdata\anaconda3\envs\py36\lib\site-packages\pyquery\pyquery.py", line 713, in each
        if callback(func, i, element) is False:
      File "d:\programdata\anaconda3\envs\py36\lib\site-packages\pyquery\pyquery.py", line 131, in callback
        return func(*args[:func_code(func).co_argcount])
      File "d:\programdata\anaconda3\envs\py36\lib\site-packages\pyquery\pyquery.py", line 1511, in rep
        urljoin(base_url, attr_value.strip()))
      File "d:\programdata\anaconda3\envs\py36\lib\urllib\parse.py", line 512, in urljoin
        urlparse(url, bscheme, allow_fragments)
      File "d:\programdata\anaconda3\envs\py36\lib\urllib\parse.py", line 368, in urlparse
        splitresult = urlsplit(url, scheme, allow_fragments)
      File "d:\programdata\anaconda3\envs\py36\lib\urllib\parse.py", line 465, in urlsplit
        _checknetloc(netloc)
      File "d:\programdata\anaconda3\envs\py36\lib\urllib\parse.py", line 410, in _checknetloc
`
I have no idea what is happened


I'm trying to use the puppeteer fetcher with this script from the examples:


```
from pyspider.libs.base_handler import *


class Handler(BaseHandler):
    def on_start(self):
        self.crawl('http://www.twitch.tv/directory/game/Dota%202',
                   fetch_type='chrome', callback=self.index_page)

    def index_page(self, response):
        return {
            "url": response.url,
            "channels": [{
                "title": x('.title').text(),
                "viewers": x('.info').contents()[2],
                "name": x('.info a').text(),
            } for x in response.doc('.stream.item').items()]
        }
```

The result is this:
`{'channels': [], 'url': 'https://www.twitch.tv/directory/game/Dota%202'}`

The puppeteer fetcher is supposed to be running since I see this when I start start pyspider:
`puppeteer fetcher running on port 22222`

When I modify the content of the js_script and rerun the script, pyspider it doesn't do anything. It doesn't even give an error if I insert faulty code.

I've already found a related issue:
[https://github.com/binux/pyspider/issues/902](https://github.com/binux/pyspider/issues/902)

but it didn't help.

<!--
Thanks for using pyspider!

如果你需要使用中文提问，请将问题提交到 https://segmentfault.com/t/pyspider
-->

* pyspider version: Latest commit ad3ae13
* Operating system: Arch Linux
* Start up command: ./pyspider

### Expected behavior

<!-- What do you think should happen? -->
Get results.

### Actual behavior

<!-- What actually happens? -->
No results.

### How to reproduce

1. Use latest development version of pyspider.
2. Use above script
3. Start pyspider & run the script.

<!-- 

The best chance of getting help is providing enough information that can be reproduce the issue you have.

If it's related to API or extraction behavior, please paste the script of your project.
If it's related to scheduling of whole project, please paste the screenshot of queue status on the top in dashboard.

-->

I'm trying to replicate the deployment demo setup from here:

[http://docs.pyspider.org/en/latest/Deployment-demo.pyspider.org/](http://docs.pyspider.org/en/latest/Deployment-demo.pyspider.org/)
but I'm getting these errors at the nginx volumes lines:

```
Starting pyspider_nginx_1 ... error

ERROR: for pyspider_nginx_1  Cannot start service nginx: b'OCI runtime create failed: container_linux.go:346: starting container process caused "process_linux.go:449: container init caused \\"rootfs_linux.go:58: mounting \\\\\\"/home/binux/nfs/profile/nginx/nginx.conf\\\\\\" to rootfs \\\\\\"/var/lib/docker/overlay2/866fba6db330f1e3ce2ad44dc2fbcee3bf3c37762681f92f5ec67faedca3dc29/merged\\\\\\" at \\\\\\"/var/lib/docker/overlay2/866fba6db330f1e3ce2ad44dc2fbcee3bf3c37762681f92f5ec67faedca3dc29/merged/etc/nginx/nginx.conf\\\\\\" caused \\\\\\"not a directory\\\\\\"\\"": unknown: Are you trying to mount a directory onto a file (or vice-versa)? Check if the specified host path exists and is the expected type'

ERROR: for nginx  Cannot start service nginx: b'OCI runtime create failed: container_linux.go:346: starting container process caused "process_linux.go:449: container init caused \\"rootfs_linux.go:58: mounting \\\\\\"/home/binux/nfs/profile/nginx/nginx.conf\\\\\\" to rootfs \\\\\\"/var/lib/docker/overlay2/866fba6db330f1e3ce2ad44dc2fbcee3bf3c37762681f92f5ec67faedca3dc29/merged\\\\\\" at \\\\\\"/var/lib/docker/overlay2/866fba6db330f1e3ce2ad44dc2fbcee3bf3c37762681f92f5ec67faedca3dc29/merged/etc/nginx/nginx.conf\\\\\\" caused \\\\\\"not a directory\\\\\\"\\"": unknown: Are you trying to mount a directory onto a file (or vice-versa)? Check if the specified host path exists and is the expected type'
ERROR: Encountered errors while bringing up the project.
```
I've tried several things but I still don't know what I'm doing wrong. How can I set the volumes up?

Hi there,
I just wonder if this project is still alive.

Since I found that the latest release was back at April 2018 and there were some issues related to new version of Python feature like 'async', seems like there is no further maintenance work by the team.


<!--
Thanks for using pyspider!

如果你需要使用中文提问，请将问题提交到 https://segmentfault.com/t/pyspider
-->

* pyspider version: 0.3.10
* Operating system: Ubuntu 18.04.2 LTS
* Start up command: pyspider all

举个例子：
```
def on_start(self):
	...
	val = 890984766742986795
	self.crawl(some_url, callback=self.topic_list_page, save={'val': val})
	
def topic_list_page(self, response):
	val = response.save['val']
	print(val)  # 输出：890984766742986800
```

<!--
Thanks for using pyspider!

如果你需要使用中文提问，请将问题提交到 https://segmentfault.com/t/pyspider
-->

* pyspider version: 0.3.10
* Operating system: Ubuntu 18.04
* Start up command: pyspider all  / pyspider

### Expected behavior

<!-- What do you think should happen? -->
No Error

### Actual behavior

<!-- What actually happens? -->
I'm getting this error when I start pyspider:
`WebDav interface not enabled: ImportError('cannot import name safe_re_encode',)`
### How to reproduce
Start pyspider with:
`$ pyspider all`
OR
`$ pyspider`

I had this issue before:
[https://github.com/binux/pyspider/issues/831](https://github.com/binux/pyspider/issues/831)
[https://github.com/binux/pyspider/issues/889](https://github.com/binux/pyspider/issues/889)
which was resolved as suggested in the previous link with:
`$ pip install wsgidav==2.4.1`

<!-- 

The best chance of getting help is providing enough information that can be reproduce the issue you have.

If it's related to API or extraction behavior, please paste the script of your project.
If it's related to scheduling of whole project, please paste the screenshot of queue status on the top in dashboard.

-->

Traceback (most recent call last):
  File "/usr/local/bin/pyspider", line 11, in <module>
    load_entry_point('pyspider==0.3.10', 'console_scripts', 'pyspider')()
  File "/usr/local/lib/python2.7/site-packages/pyspider/run.py", line 754, in main
    cli()
  File "/usr/local/lib/python2.7/site-packages/click-3.3-py2.7.egg/click/core.py", line 610, in __call__
    return self.main(*args, **kwargs)
  File "/usr/local/lib/python2.7/site-packages/click-3.3-py2.7.egg/click/core.py", line 589, in main
    with self.make_context(prog_name, args, **extra) as ctx:
  File "/usr/local/lib/python2.7/site-packages/click-3.3-py2.7.egg/click/core.py", line 506, in make_context
    self.parse_args(ctx, args)
  File "/usr/local/lib/python2.7/site-packages/click-3.3-py2.7.egg/click/core.py", line 898, in parse_args
    return Command.parse_args(self, ctx, args)
  File "/usr/local/lib/python2.7/site-packages/click-3.3-py2.7.egg/click/core.py", line 767, in parse_args
    value, args = param.handle_parse_result(ctx, opts, args)
  File "/usr/local/lib/python2.7/site-packages/click-3.3-py2.7.egg/click/core.py", line 1252, in handle_parse_result
    self.callback, ctx, self, value)
  File "/usr/local/lib/python2.7/site-packages/click-3.3-py2.7.egg/click/core.py", line 53, in invoke_param_callback
    return callback(ctx, param, value)
  File "/usr/local/lib/python2.7/site-packages/pyspider/run.py", line 35, in read_config
    config = underline_dict(json.load(value))
  File "/usr/local/lib/python2.7/json/__init__.py", line 290, in load
    **kw)
  File "/usr/local/lib/python2.7/json/__init__.py", line 338, in loads
    return _default_decoder.decode(s)
  File "/usr/local/lib/python2.7/json/decoder.py", line 366, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/local/lib/python2.7/json/decoder.py", line 382, in raw_decode
    obj, end = self.scan_once(s, idx)
ValueError: Expecting , delimiter: line 5 column 5 (char 227)
