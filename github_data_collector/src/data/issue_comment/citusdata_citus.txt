DESCRIPTION: avoid hanging on connections dropped during `ROLLBACK SAVEPOINT`

While trying to create a test case for #2179 I came across a hang

The cause is that `ROLLBACK SAVEPOINT` leaves the connection alone until `adaptive_executor.c` gets the connection again from `placement_connection.c` & proceeds to poll it for readable/writeable
This is to track improvements and features that can be implemented upon https://github.com/citusdata/citus/pull/3376. I will clean-up and edit this issue, but for now, just pasting @marcocitus 's ideas from https://github.com/citusdata/citus/pull/3376#pullrequestreview-344457808 so we don't lose track of them:


Some ideas for future improvements:
- Replace read_intermediate_result function scan with a custom scan that returns tuples as they are parsed. That saves a lot of data copying, especially in large INSERT..SELECTs.
- Clean up intermediate results at the end of execution to avoid keeping and potentially flushing a lot of data on disk.
- Support GROUP BY distribution_column_of_target_table queries in the final INSERT..SELECT step

Some ideas for exciting new use cases for append-mostly data that look more feasible now:
- Do all your pre-processing in Citus. Simply shove your JSON objects into a distributed table with a bigserial as its distribution column and then transform it in parallel using INSERT..SELECT.
- Create a new distributed table type which has no distribution column and exactly one shard on each table (could use append) and supports metadata syncing. Writes are always done to the local shard. Allows you to write locally on any node, providing infinite scale and extremely high write-availability, and INSERT..SELECT is used to put data into more structured form.
- Create append-partitioned tables where each shard is a foreign table. Use INSERT..SELECT to put the data into a more structured form.
- Use intermediate result infrastructure for re-partition joins, allows combining subqueries and CTEs with re-partition joins.
- Change the distribution column (type) of your table.
- Keep N copies of your data with different distribution columns to do different types of joins.

Updates multi_extension to show values rather than result of equality comparison in order to see what those different values are
DESCRIPTION: avoid marking reference table shards unhealthy in the presence of savepoints

Alternative to #3386

TODO: add test

Fixes #2179
When we sync metadata we always do so as superuser because we need to modify tables like pg_dist_partition directly.

However, we also create tables as superuser, which means that we need to open a new session that cannot see uncommitted `CREATE TYPE`. That means we cannot support a transaction that first creates a type and then a distributed table that depends on that type. We currently work around this by creating the types in a non-transactional way, which creates other issues.

To stop this issue from proliferating we need to create distributed tables as a the current user when inside a transaction block, while changing the metadata as superuser. This is non-trivial since the OID of the uncommitted table and distribution column type need to be written to the metadata table.
I can reproduce this on v9.1.2 (and master as well)


```SQL
 psql -c "CREATE TABLE users_table (user_id int, time timestamp, value_1 int, value_2 int, value_3 float, value_4 bigint);" -p 5432 postgres
 psql -c "SELECT create_distributed_table('users_table', 'user_id');" -p 5432 postgres

WITH a AS
  (SELECT *
   FROM users_Table)
SELECT count(*),
       user_id
FROM a
GROUP BY user_id
HAVING (max(value_1) >
          (SELECT max(value_2)
           FROM a));


ERROR:  result "4_2" does not exist
CONTEXT:  while executing command on localhost:9700
```
In INSERT..SELECT the planner allows subqueries in RETURNING to pass through and fail on the worker.
```
postgres=# INSERT INTO test (y, x) SELECT * FROM test ON CONFLICT DO NOTHING RETURNING (SELECT max(x) FROM test LIMIT 1);
ERROR:  relation "public.test" does not exist
CONTEXT:  while executing command on localhost:9701
```
When I use master_copy_shard_placement() to recover a broken shard, the new shard is created without the default privileges defined in the schema.
In my particular case I am also working with a partitioned table.
As updates to the parent table are coming in during the master_copy_shard_placement() operation, the new shard immediately fails with a permission error.
I can work around the issue by locking the parent table, doing the copy operation, and manually setting the correct permissions.  I am running Citusdata 8.1 and postgresql 11.1.
We currently use `worker_hash_partition_table`, `worker_fetch_partition_file`, and `worker_merge_files_into_table` to perform re-partition jobs. However, these functions are not aware of distributed transactions, may leave files behind, and have various inefficiencies.

We would like to use `worker_partition_query_result`, `fetch_intermediate_results`, and `read_intermediate_result` as used in #3376. This is dependent on removing task-tracker, since intermediate result functions require being in a distributed transactions.

Once we made this conversion we can use CTEs and recursively planned subqueries within re-partition joins.
Marco's test case runs into #3360 with this PR. But that's addressed in #3382 

TODO: write a test case

The "connection pointer is NULL" warning is the result of `FinishRemoteTransactionCommit` calling `HandleResultTransactionResultError`. Maybe `connectionSetChanged = true` isn't enough to prevent the connection from being reused *(spoiler: we need to check pgConn nullness elsewhere in code)*

Fixes #2179 