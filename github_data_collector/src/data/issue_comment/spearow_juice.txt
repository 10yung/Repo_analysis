# Versions
Using Rust Stable (stable-x86_64-pc-windows-gnu). 
CUDA is v10.2
CUDNN is v7.6.5

# Tests
Testing this with
```cargo run --release --verbose mackey-glass lstm-dense```
RCudnn to support it is at;
https://github.com/lissahyacinth/rust-cudnn

# Status
This is currently unfinished, with remaining tasks

- [x] Resolve Heap Error
- [ ] Add match arms Ok/Error for RNN_Forward
- [ ] Add Compute Input Gradient
- [ ] Add Compute Parameters Gradient
- [ ] Verify against Mackey-Glass or MNIST dataset
- [ ] PR Rust-Cudnn and use Crates version for testing

~~# Memory Error~~
~~error: process didn't exit successfully: `target\release\juice-examples.exe mackey-glass lstm-dense` (exit code: 0xc0000374, STATUS_HEAP_CORRUPTION)~~

~~This memory error is due to a part of RNN_FORWARD failing, likely to do with the initialisation of the weights. Leaving this on draft PR while I resolve it.~~
Because cargo does not supports workspaces inside workspace, but I do want to add some more example networks (besides MNIST)
Hello, 

I am wondering if juice supports LSTM RNN. If not I could try to contribute and make that layer available.  

"Juice is a open Machine Learning Framework for hackers to build classical, deep or hybrid machine learning applications."

What should be available for classical and hybrid applications? I imagine linear/logistic regression, maybe SVM?
I'm curious what the current list of items needed to catch native up to the current feature set is, across all the repositories. (My ulterior motive here is not having CUDA-compatible hardware right now.)

This would help provide a reference implementation on vanilla machines that can be used to check against the OpenCL or CUDA implementations in case hardware or framework bugs show up there.
Bigger work items:

* [ ] Recursive Neural Networks / Long Term Short Term Memory
* [ ] OpenCL Backend [juice/coaster-nn]
* [ ] Autodiff [juice/coaster-nn]
* [ ] Accuracy enum instead of type usage [juice/greenglas/coaster]

---

Low hanging fruits:

* [ ] Honour biases [juice]
* [ ] Gradient calculation for ND-convolution native backend [coaster-nn]
* [ ] Regression examples [juice-examples]
 * [ ] Add preprocessing filters to greenglas

These should be represented by individual ticktes

---

The full presentation of November 30th, 2017:
[bernhard_schuster__rust_machine_learning_with_juice.pdf](https://github.com/spearow/juice/files/1596207/bernhard_schuster__rust_machine_learning_with_juice.pdf)
  
It looks like there's no mean-squared layer type just log-likelihood, so for [regression](https://github.com/spearow/juice-examples/issues/16) i think one is needed at the output.
Currently there is only the `accuracy` implemented for the confusion_matrix implemented, but `precision/recall` might be a good tool to display the state too. 
The crate issues quite a few warnings. Some of the unneeded identifiers could be intentional for later implementation, but at the moment it creates a lot of compiler output to sift through. Removing the warnings would also ensure the codebase is readable for humans and compilers the same way.
 concatenation is a essential step in many architectures today, and juice is missing out on those