I am currently working on accelerator support via SYCL. I will attempt to rebuild the memory pool to allocate everything put in a pool to device as well (using pinned memory in GPUs) via SYCL. The code is written in a generic enough way that I can make use of the existing class structure to achieve this. 

SYCL, in theory, will allow single-source compilation through the following build tools:
[hipSYCL](https://github.com/illuhad/hipSYCL): AMD/NVIDIA GPUs
[Intel's SYCL](https://github.com/intel/llvm): SPIR-V devices, incl. Intel FPGAs
[Xilinx's fork of Intel's SYCL](https://github.com/triSYCL/sycl): Xilinx FPGAs
![Taken from hipSYCL: https://github.com/illuhad/hipSYCL](https://github.com/illuhad/hipSYCL/raw/master/doc/img/sycl-targets.png)
All of the above support CPU compilation through OpenMP. In theory, one should be able to recover single-threaded execution with zero or little overhead. I have yet to try this.

I will have to think about how to give some control over the number of threads spawned in all of the cases above, as this is very device specific. The SYCL runtime in theory does this for you, but I suspect this will be very suboptimal.

Any function that operates at the level of raw pointers will be rewritten in SYCL. Mainly, this will involve the entirety of `evaluator.cpp` and some associated helper functions. NTT and INTT have already been done and keyswitch is halfway there. I will allocate memory prior to compute for scratchpad buffers for temporary polynomial results. In any case, this should be handled by the memory system somehow. I should expose an easy interface to the device buffer versions of a raw pointer by exposing `.get_device()` for every `Pointer` object that intialises with a pool.

The progress I have made so far is accelerating the `key_switch_inplace` function, a key bottleneck in the compute, but only managed to reduce relinearisation by 60% mainly due to memory allocation and transfer overheads. This will hopefully be fixed with a proper memory system. You can track my progress here: https://github.com/jon-chuang/SEAL. 

My vision is to continue developing independently and merge any new changes, for approximately 6 months. Hopefully, by the end of those 6 months, the Microsoft team might be happy enough with performance and code quality to merge my changes.

I also believe that key generation times need to be reduced, and this can either be accelerated on a user's local GPU, including iGPU, or simply with a single CPU hardware thread for each key. 

I am using hipSYCL at the moment to compile through HIP to PTX, which has a new runtime slated soon that is meant to introduce a lot of important features, like launching kernels asynchronously through a single queue (equivalent of CUDA stream) or performing synchronisation both at the HtD/DtH level and also at the global to shared memory level, solely on a given view of a buffer rather than the entire buffer. I will update my code as these features become available, and in the meantime my code will probably be quite suboptimal. However, my aim is to set up the infrastructure first, and optimise later.

I also have to start writing tests for my code, as I have been relying on the existing tests, which have served me decently, but I probably need more, especially to test data transfers. I should probably also add conditional topic-associated debugging.

Here are my thoughts on SYCL from another post:
> With regards to SYCL, I don't think it's mature enough that for the next few months, you can match an expertly written CUDA kernel. For one thing, there are several features not supported yet. However, I think that in the long term, it's a good investment. I would give it a year to match CUDA/HIP and have full support for FPGAs. Furthermore, you should be able to compile to Intel iGPUs as well, for the key generation phase which have several (I)NTTs.

Is it conceivable that there exists a HE scheme in which a computation can be decrypted by a server only after a predesignated series of operations have been performed? 

Meaning to say, that somehow, the operations themselves become infused with a public key, such that their precise alignment leads to their being able to be decrypted under a public key as a plaintext? CPA security must be maintained for every operation prior to a final operation.

Under the current paradigm, we are interested in CPA security, but what if we want to relax it to a differentially private security, that also has an element of verifiability, in the form of verifying the server has performed the correct operations? One reveals precisely the information one would like to reveal.

For instance, this would be of use to training a deep learning model entirely on the cloud based on data from multiple parties without the communication overhead associated with multiparty computation, as the gradients can be averaged, masked with a noise and then decrypted directly in the server. These gradients can be combined with gradients from other data sources and update a central learner. 
I can't invoke it despite `#include "seal/util/common.h"` where it is defined.

Hello!
Is it possible to perform operations on one specific element of an encrypted Batch Encoder Array?, as currently (as I know of) you can only perform operations on every single element.
As a continuation of the question about 128-bit words, I want to get some feedback on the possibility of increasing N beyond what SEAL supports for now.

For instance, in the context of CKKS at least, N=2^17 or N=2^18 could be accesible. At 16 bytes per 128-bit integer, N=2^18 with n=2^17 vector slots would yield a plaintext vectors of size 2MB. This may be of use for instance if processing a very large dataset or a very large image. From what I understand, with the default choice of coeff mod at least, the largest N for 128-bit security right now is N=2^16. I understand there is non-linear scaling in the complexity of the operations performed (for instance, the keyswitch key sizes grow as something like O(N^3)), so what I propose may not be suitable.

Say we have k=`key_modulus.size()`=4\*15=60, resulting in a q < 2^3200, (we seem to be doing approximately scaling of `coeff_modulus` size with n), so we have 60 levels, much larger than the 8 or 15 used now with N=2^14 or 2^16. From the formula given in the HEAX paper, with n=2^17 slots, 59\*60=3540 such vectors, and 16 bytes each, we get a key size of approximately 7.4GB. This is in contrast to the 14\*15\*2^15 \*16 B~100MB keyswitch key sizes for n=2^15. This could possibly be mitigated by either pipelining over the keyswitch outer loop from an intermediate fast memory say Intel Optane or by spreading the computation across multiple devices that have a high-speed interconnect between them. Can someone who worked with the HEAX team comment on how much of a bottleneck reading from the DDR memory to the chip was for n=2^14? I am also unsure how many kskeys need to be generated for a given N or k. For every 2x increase in the max circuit depth, we get an 8x increase in the key sizes.

Something exciting about this is that with k=60, one could possibly evaluate neural networks of depth 60 with some significant memory overhead but relatively insignificant computational overhead, at the cost of needing to batch together data of 4x as large into a single SIMD vector. For instance, Darknet-53, the latest YOLOv3 model, could probably fit for this. 

For N=2^19, we get keyswitch key sizes of 60GB, which can still fit into a large set of DRAMs, and we can evaluate circuits of something like 120 levels, which is about as high as a deep resnet would go. Furthermore, evaluating something like an LSTM may also become feasible, for instance for private speech recognition (or something more sensitive).

If we consider the NTT to be the key bottleneck for performance, this scales as n * log_2 n, so we get 17/15 ~= 1.13x computational overhead per slot. This basically nothing compared to bootstrapping, which I understand is several order of magnitude greater than keyswitch (can someone give an estimate? From what I understand it's about 1000x the clock cycles?). Keyswitch overhead goes up in k^2, so we get 59\*60/14\*15 ~= 16 times overhead. As the rest of the operations are pointwise, we get an approximate total overhead for keyswitch strictly less than 18x (for the inner loop NTT), which is significantly less than bootstrapping.

I am not sure how accurate the calculations I have performed are. Are there any references to understand the security level and noise growth? I suppose these can be gleaned from the original papers on CKKS/RNS-CKKS?

Returning back to EVA, I am wondering if it can help optimise bootstrapping as well if it is ensured (or determines using microbenchmarking) that plaintexts and outputs lie within a certain range.

I am actually quite interested in pursuing the evaluation of depth ~60 circuits as a research project working with the SEAL, HEAX and CHET/EVA teams; it is primarily a systems challenge. We could probably start with deep CNNs. Then we can move on to LSTMs with sigmoid gating polynomial approximation done with input range restrictions (using analysis performed by EVA) and doing a rescaling analysis, and similarly for Transformers and softmax. I am currently figuring out how to integrate GPU support for SEAL. With multi-GPU support using something like a combination of RDMA and MPI/GPUDirect (for multinode), and with the right hardware setup with very large DRAMs, this seems within reach.
EVA (https://arxiv.org/pdf/1912.11951.pdf) makes optimisations about `CoeffModulus` and the fixed point scale (which takes part in the plaintext encoding/decoding phase through a multiplicative factor). It also adjusts the rescale factor. 

I was wondering whether in certain situations this can reduce the arithmetic and also memory requirements of SEAL if a tool like EVA determines that a circuit can live with merely 32-bit integers, while still, for instance, maintaining 128-bit security. Furthermore, I am wondering about the possibility of supporting more levels by means of 128-bit integers (e.g. `__int128` using Clang/GCC). As bootstrapping is still very slow for now, providing the option to use more levels despite the overhead may still be equitable. 

I am wondering about the purpose of the memory pool abstraction and why it was necessary to write a custom memory management system for SEAL/HE. It seems it may have been more important before the implementation of the RNS representation? 

I also don't quite understand the semantics of `allocate`. What kind of pointer is `Pointer` exactly? 
To expand the reach of SEAL to new platforms it would be worthwhile implementing bindings for MacOS and iOS similar to the ones for dotnet.

I propose creating an Objective-C++ wrapper with ergonomic Swift bindings.

I have been working on this for sometime and am curious if it is something that you would consider merging upstream.

Happy to put out some preliminary pull requests for your review.

Do you have a style guide you adhere to? I’m hoping to use automatic formatting in Visual Studio Code on a Mac. For instance, a `.clang-format` file with format `BasedOnStyle: Microsoft` (or whatever the current style is) would work nicely. This would make it easier for external PRs to adhere to the given style.