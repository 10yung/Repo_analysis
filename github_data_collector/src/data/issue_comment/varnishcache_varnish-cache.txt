A panic which I do not understand straight away because I see no way how a `task->priv == NULL` could end up on the `idle_queue`.

Other things to note:

* `    temp = \221\023R,` fixed in a06095da13188d919a403fcab8c7bff8a9e61997

```
Panic at: Fri, 17 Jan 2020 16:44:28 GMT
Assert error in pool_getidleworker(), cache/cache_wrk.c line 203:
  Condition(((wrk)) != 0) not true.
version = varnish-trunk revision 4df4d2a497727a671d2f2b64d3ccd03fc4cbc3d6, vrt api = 10.0
ident = -jsolaris,-sumem,-sdefault,-hcritbit,ports
now = 54559567.752497 (mono), 1579279466.613156 (real)
Backtrace:
  45d4ab: /opt/local/sbin/varnishd'pan_ic+0x16b [0x45d4ab]
  4c74f7: /opt/local/sbin/varnishd'VAS_Fail+0x47 [0x4c74f7]
  47d115: /opt/local/sbin/varnishd'pool_getidleworker+0x135 [0x47d115]
  47dc74: /opt/local/sbin/varnishd'Pool_Task+0x74 [0x47dc74]
  44cbad: /opt/local/sbin/varnishd'VBF_Fetch+0x24d [0x44cbad]
  462848: /opt/local/sbin/varnishd'CNT_Request+0x1478 [0x462848]
  4878e4: /opt/local/sbin/varnishd'http1_req+0x5c4 [0x4878e4]
  47d43c: /opt/local/sbin/varnishd'WRK_Thread+0x24c [0x47d43c]
  47d8a8: /opt/local/sbin/varnishd'pool_thread+0x38 [0x47d8a8]
  fffffc7fef24e1ea: /lib/amd64/libc.so.1'_thrp_setup+0x8a [0xfffffc7fef24e1ea]
thread = (cache-worker)
thr.req = 4bb7800 {
  vxid = 10098887, transport = HTTP/1 {
    state = HTTP1::Proc
  }
  step = R_STP_MISS,
  req_body = R_BODY_NONE,
  restarts = 0, esi_level = 0,
  sp = 3773920 {
    fd = 83, vxid = 10098886,
    t_open = 1579279466.581912,
    t_idle = 1579279466.581912,
    ws = 3773960 {
      id = \"ses\",
      {s, f, r, e} = {37739a0, +192, 0, +344},
    },
    transport = HTTP/1 {
      state = HTTP1::Proc
    }
    client = X.X.X.X 10916 /tmp/varnish.proxy.sock,
  },
  worker = fffffc7fe87fdcc0 {
    ws = fffffc7fe87fdd48 {
      id = \"wrk\",
      {s, f, r, e} = {fffffc7fe87fd470, +0, 0, +2040},
    },
    VCL::method = MISS,
    VCL::return = fetch,
    VCL::methods = {RECV, HASH, MISS},
  },
  ws = 4bb7948 {
    id = \"req\",
    {s, f, r, e} = {4bb98a0, +312, 0, +57144},
  },
  http_conn = 4bb9820 {
    fd = 83 (@3773944),
    doclose = NULL,
    ws = 4bb7948 {
      [Already dumped, see above]
    },
    {rxbuf_b, rxbuf_e} = {4bb98a0, 4bb9957},
    {pipeline_b, pipeline_e} = {0, 0},
    content_length = -1,
    body_status = none,
    first_byte_timeout = 0.000000,
    between_bytes_timeout = 0.000000,
  },
  http[req] = 4bb79e8 {
    ws = 4bb7948 {
      [Already dumped, see above]
    },
    hdrs {
      \"GET\",
      \"/media/*REDACTED*.jpg\",
      \"HTTP/1.1\",
      \"Host: *REDACTED*\",
      \"User-Agent: *REDACTED*\",
      \"X-Forwarded-For: *REDACTED*\",
      \"X-Forwarded-Proto: https\",
      \"X-Real-Ip: *REDACTED*\",
      \"X-Varnish-Static: true\",
    },
  },
  vdc = 4bb97f8 {
    nxt = 0,
    retval = 0,
  },
  vcl = {
    name = \"boot\",
    busy = 45,
    discard = 0,
    state = auto,
    temp = \221\023R,
    conf = {
      syntax = \"41\",
      srcname = {
        \"/opt/local/etc/varnish/default.vcl\",
        \"Builtin\",
        \"/opt/local/etc/varnish/common.vcl\",
        \"/opt/local/etc/varnish/prefresh.vcl\",
      },
    },
  },
  objcore[REQ] = 4727bd0 {
    refcnt = 2,
    flags = {busy},
    exp_flags = {},
    boc = 3b368d0 {
      refcnt = 2,
      state = invalid,
      vary = 0,
      stevedore_priv = 0,
    },
    exp = {0.000000, 0.000000, 0.000000, 0.000000},
    objhead = 2d15040,
    stevedore = 0,
  },
  flags = {
  },
  privs = 4bb79d8 {
  },
  top = 4bb9880 {
    req = 4bb7800 {
      [Already dumped, see above]
    },
    privs = 4bb9898 {
    },
  },
},
thr.busyobj = 46c82e0 {
  end = 46d82c0,
  retries = 0,
  req = 4bb7800 {
    [Already dumped, see above]
  },
  sp = 3773920 {
    [Already dumped, see above]
  },
  vfc = 46ca1d8 {
    failed = 0,
    req = 0,
    resp = 0,
    wrk = 0,
    oc = 0,
    obj_flags = 0x0,
  },
  ws = 46c8320 {
    id = \"bo\",
    {s, f, r, e} = {46ca220, +0, 0, +57496},
  },
  ws_bo = 0,
  objcore[fetch] = 4727bd0 {
    [Already dumped, see above]
  },
  flags = {do_stream},
  director_req = 5d5330 {
    cli_name = boot.default,
    admin_health = healthy, changed = 1579018017.866319,
    type = backend {
      ipv4 = 127.0.0.1,
      port = 8080,
      hosthdr = 127.0.0.1,
      n_conn = 9,
    },
  },
  vcl = {
    name = \"boot\",
    busy = 45,
    discard = 0,
    state = auto,
    temp = \221\023R,
    conf = {
      syntax = \"41\",
      srcname = {
        \"/opt/local/etc/varnish/default.vcl\",
        \"Builtin\",
        \"/opt/local/etc/varnish/common.vcl\",
        \"/opt/local/etc/varnish/prefresh.vcl\",
      },
    },
  },
},
vmods = {
  blob = {6eaf70, Varnish trunk 4df4d2a497727a671d2f2b64d3ccd03fc4cbc3d6, 0.0},
  std = {6eba10, Varnish trunk 4df4d2a497727a671d2f2b64d3ccd03fc4cbc3d6, 0.0},
},
```

The purpose of this ticket is to reach agreement on how to fix a race during the worker process shutdown:

309e807d3ff8810017451cb16c667d99125415aa exposes a race caused by objects being freed after `STV_close()` has run (see the bottom of this note for an explanation why it is only exposed by the `umem` stevedore).
The current `umem` code asserts that no stevedore operations happen after the close, yet they exist:
* the expiry thread continued to run, this was fixed in 34b687e636d454eb03261552628b273c0ef58e45
* but we continue to see panics from calls into the stevedore presumably from ordinary worker threads handling passes, like this one from vtest c00010.vtc:

```
**** dT   1.673
***  v1   debug|Info: Child (49224) ended
***  v1   debug|Error: Child (49224) Panic at: Tue, 14 Jan 2020 13:48:38 GMT
***  v1   debug|Assert error in smu_free(), ../../../bin/varnishd/storage/storage_umem.c line 226:
***  v1   debug|  Condition((sc->smu_cache) != 0) not true.
***  v1   debug|version = varnish-trunk revision 34b687e636d454eb03261552628b273c0ef58e45, vrt api = 10.0
***  v1   debug|ident = -jsolaris,-sdefault,-sdefault,-hcritbit,ports
***  v1   debug|now = 43947354.882951 (mono), 1579009717.344206 (real)
***  v1   debug|
***  v1   debug|Info: 
***  v1   debug|Child (49224) said Child dies
```

While I do not have a stack trace yet pinpointing the root cause, my working hypothesis is that the above is caused by `ObjSlim()` called from `HSH_Cancel()` called from the worker thread for hfm/hfp/private objects.

This ticket is to ask how we should solve this race.

two ideas:

* make our shutdown graceful in that we wait for all worker threads to terminate

  this option would close the `STV_close()` race with the assumed definition that no stevedore operations should happen after a close.

  In my opinion this would actually fix our worker shutdown, which currently terminates all threads forcibly no matter what state they are in.

  To implement this option, we could promote the existing pool destruction code which, right now, is only active when the `drop_pools` debug bit is set.

* define stevedores to be required to support frees after close

  this is a trivial change in `umem` (see 4df4d2a497727a671d2f2b64d3ccd03fc4cbc3d6) should fix the panics we are seeing in vtest. But in my mind, this option might be the wrong choice from a design perspective and it would probably lead to follow up issues should we ever want to support pluggeable stevedores (with a lifetime shorter than the worker process).

Why we only observe this race in vtest with `umem`:

* The race only affects stevedores implementing the `.close` callback which, in varnish-cache core, only `deprecated_persistent` and `umem` use.
* Of these two, `umem` is the only one we actually test extensively from vtest
* `umem` drops a cache in the `.close` callback, which is being used during frees.

Using http/1 and `beresp.do_stream = true`, we can take as long as we want to send data,
but after switching to http/2, the send_timeout is triggered and connection closes.

## Expected Behavior
Either connection should close using http/1 to honor send_timeout configuration, or it should not close in http/2.

## Current Behavior
Connection is closed after send_timeout using http/2, but not http/1, both with `beresp.do_stream = true`, `beresp.do_gzip = false` and `beresp.ttl = 0s;`

## Possible Solution
Improve docs to explain http/2 behaviour
Ability to configure send_timeout in VCL will solve problem (#2983)

## Steps to Reproduce
1. have a backend which send data regularly, taking a high time, for exemple, we sent a dot every second for 700seconds (to reach the default 600s timeout)
2. use http/2 to call the backend. we used haproxy in tcp mode for ssl termination
3. wait until the timeout, connection get closed.
4. using http/1, connection is not closed

## Context
We're sending a file to client, generating it on the fly.  Right now we increased timeout to 4 hours so it's working, but we might be keeping idle connections for naught.
I must stress out that connection is not closed using http/1, so we're sending enough data to keep connection alive

## Your Environment
* Version used: 6.3.1
* Operating System and version: docker image
* Source of binary packages used (if any) "official" docker image 6.3.1
Running behind a haproxy 7.5 in proxy mode

(The 15th of march is a Sunday, so we'll probably shift it a few days.)

Name, Number, Rank & things to do before then ?

The `.timestamp` field is a string in the `varnishstat -j` output, whereas all the others are objects that all follow the same structure.
This makes writing parsers a pain because `timestamp` always needs to be special-cased for little to no extra value.

I'd like to see it gone for the next major, and if people are ok with that, there's the question of the replacement:
1. none: sit on this ticket and once we know that the next release will be `7.0`, just retire `.timestamp`.
2. offer a `.time` attribute that follows the common structure

I'm personally in favor of option 1, once the data has been de-serialized, adding your own timestamp is super easy. Also, `varnishstat -1` doesn't have a `timestamp` field.
Following up on the whole "import essential vmods in-tree"  and specifically https://github.com/varnishcache/varnish-cache/pull/3055#issuecomment-529339906, here's `vmod_cookie`
The bar is not very high, it needs to either be an empty string or a
list of absolute directories separated by colon characters. But that
validation is pretty strict.

---

When we discussed #3172 during last bugwash the question of validating path parameters came up. In this patch series I first trimmed some fat, then implemented a absolute path validation, and plugged everything into parameter handling and libvcc.

A nice side effect to this change is that we only ever build VCL and VMOD paths once per change, including once for the initial value. So between the single allocation for the data structure and no longer rebuilding both paths whenever we load a VCL it's a bit of churn that goes away, admittedly not in a critical part of the code base.

Another interesting result of this change is that we now have a single call site for `VFIL_setpath()`, and that could still be the case if #3181 reached consensus.

I deliberately left "path escape" out of the scope. By that I mean using `"../"` directory components to attempt reading files outside of the designated path. There was no clear "we should do that" feeling from the bugwash, contrary to the absolute paths question.

There might be other means to escape the path, like creating symbolic links, I don't know whether we could really do something about that. And next thing you know someone's gonna yell that we broke their `../` use case.
After updating hitch on v-c.o, github webhooks failed via https.

It would be generous to describe the resulting diagnostics as obscure.

In github's end all you see is "We couldn’t deliver this payload: No error"

In varnishlog I get:

     295244 Begin          c sess 0 PROXY
     295244 SessOpen       c 10.0.0.202 54523 a1 10.0.0.100 444 1578303689.314196 34
     295244 Proxy          c 2 192.30.252.97 26095 10.0.0.202 443
     295244 SessClose      c OVERLOAD 0.049
     295244 End            c 

I realize that the length of some of the tlv's may be under an attackers control, but I would still expect some hint that maybe I have a misconfiguration and something pointing in the direction of the session workspace ?
I am launching the command from systemd Ubuntu server 18.04, without -w parameter, and with -D and with -P

##  Expected Behavior
Launching with no problem

## Current Behavior
There is an execution error: ~missing -w parameter

## Possible Solution
Get working without -w

## Steps to Reproduce (for bugs)
1. Change on systemd->varnishncsa deleting the default -w .log

## Context
I want to send all the output to syslog

## Your Environment
* Version used: 5.2.1
* Operating System and version: Ubuntu server 18.04
* Source of binary packages used (if any) default

Generalize cci to also handle `centos:8`, scrub `rst2pdf` references while at it