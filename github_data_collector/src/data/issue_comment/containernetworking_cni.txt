`json` identifier cannot handle json with comments well. Change the identifier to `jsonc`.

Ref: github/linguist#4153

![json](https://user-images.githubusercontent.com/6169722/72587778-3a267100-3931-11ea-8c38-62f66dd5bc43.PNG)

A follow-up fix PR of #736 , IMO, we should use black-box testing for our utility functions.

Signed-off-by: Bruce Ma <brucema19901024@gmail.com>
Examples in the documentation suggest that `//` can be used for comments. E.g.  the [CNI spec](https://github.com/containernetworking/cni/blob/master/SPEC.md#example-configurations)
contains an example like

```
{
  "cniVersion": "0.4.0",
  "name": "dbnet",
  "type": "bridge",
  // type (plugin) specific
  "bridge": "cni0",
  ...
```

Unfortunately, using such a configuration with `podman` results into

```
Error: error parsing configuration list: invalid character '/' looking for beginning of value
```

Because configuration files seem to be JSON (which does not support comments at all), it should be clarified how to write comments in configuration files.  If not supported (and CNI spec is wrong), such an extension (`//`) should be explicitly required because comments are essential in configuration files.

If a plugin exits with a non-0 exit status and prints stuff to stderr *but not stdout*, then `pkg/invoke/raw_exec.go:pluginErr()` will categorize this as "netplugin failed with no error message". This seems to be in part because it forwards the plugin's stderr to its parent's stderr, which is nice and all, but it means that the stderr might end up getting logged somewhere less useful (eg, in cri-o's logs rather than kubelet's). Plus, the "failed with no error message" part is just wrong.

(Seen in two unrelated contexts with different plugins in the same day, lol)

@dcbw @squeed 
Hello everybody. I asked about my problem in coredns/coredns [(url)](https://github.com/coredns/coredns/issues/3411) and I taked answer it is cni problem. 

How to resolve that issue ?
"cniVersion": "0.3.1"
```
-----------------------------------------------------------------------
kube-system coredns-5644d7b6d9-47k52 0/1 Running 0 3h29m
kube-system coredns-5644d7b6d9-8vmzv 0/1 Running 0 3h29m
-----------------------------------------------------------------------
===========================
-----------------------------------------------------------------------
kubectl describe pod/coredns-5644d7b6d9-47k52 -n kube-system
-----------------------------------------------------------------------
Name: coredns-5644d7b6d9-47k52
Namespace: kube-system
Priority: 2000000000
Priority Class Name: system-cluster-critical
Node: pr40/178.88.161.72
Start Time: Wed, 30 Oct 2019 18:37:46 +0600
Labels: k8s-app=kube-dns
pod-template-hash=5644d7b6d9
Annotations:
Status: Running
IP: 172.17.1.2
IPs:
IP: 172.17.1.2
Controlled By: ReplicaSet/coredns-5644d7b6d9
Containers:
coredns:
Container ID: docker://1f8513460e1b11a5311e5f73d54a5cef35d489784fc36486292134a92dbe34d8
Image: k8s.gcr.io/coredns:1.6.2
Image ID: docker-pullable://k8s.gcr.io/coredns@sha256:12eb885b8685b1b13a04ecf5c23bc809c2e57917252fd7b0be9e9c00644e8ee5
Ports: 53/UDP, 53/TCP, 9153/TCP
Host Ports: 0/UDP, 0/TCP, 0/TCP
Args:
-conf
/etc/coredns/Corefile
State: Running
Started: Wed, 30 Oct 2019 18:37:47 +0600
Ready: False
Restart Count: 0
Limits:
memory: 170Mi
Requests:
cpu: 100m
memory: 70Mi
Liveness: http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
Readiness: http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
Environment:
Mounts:
/etc/coredns from config-volume (ro)
/var/run/secrets/kubernetes.io/serviceaccount from coredns-token-g2xt6 (ro)
Conditions:
Type Status
Initialized True
Ready False
ContainersReady False
PodScheduled True
Volumes:
config-volume:
Type: ConfigMap (a volume populated by a ConfigMap)
Name: coredns
Optional: false
coredns-token-g2xt6:
Type: Secret (a volume populated by a Secret)
SecretName: coredns-token-g2xt6
Optional: false
QoS Class: Burstable
Node-Selectors: beta.kubernetes.io/os=linux
Tolerations: CriticalAddonsOnly
node-role.kubernetes.io/master:NoSchedule
node.kubernetes.io/not-ready:NoExecute for 300s
node.kubernetes.io/unreachable:NoExecute for 300s
Events:
Type Reason Age From Message
-----------------------------------------------------------------------
Warning Unhealthy (x1231 over 3h24m) kubelet, pr40 Readiness probe failed: HTTP probe failed with statuscode: 503
-----------------------------------------------------------------------


===========================

-----------------------------------------------------------------------
cat /etc/cni/net.d/10-flannel.conflist
-----------------------------------------------------------------------

{
  "name": "cbr0",
  "cniVersion": "0.3.1",
  "plugins": [
    {
      "type": "flannel",
      "delegate": {
        "hairpinMode": true,
        "isDefaultGateway": true
      }
    },
    {
      "type": "portmap",
      "capabilities": {
        "portMappings": true
      }
    }
  ]
}
```
This is a "get all the things" function, which a runtime could use on container teardown to avoid having to itself cache the uniqueness tuple when tearing down all networks for the container.

@containernetworking/cni-maintainers @squeed @mars1024 
Hi, I'm Yevgeny Pats Founder of [Fuzzit](https://fuzzit.dev) - Continuous fuzzing as a service platform.

We have a free plan for OSS and I would be happy to contribute a PR if that's interesting.
The PR will include the following
- [go-fuzz](https://github.com/dvyukov/go-fuzz) fuzzers (This is generic step not-connected to fuzzit)
- Continuous Fuzzing of master branch which will generate new corpus and look for new crashes
- Regression on every PR that will run the fuzzers through all the generated corpus and fixed crashes from previous step. This will prevent new or old bugs from crippling into master.

You can see our basic example [fuzzitdev/example-go](https://github.com/fuzzitdev/example-go) and you can see an example of "in the wild" integration [prometheus/prometheus](https://github.com/prometheus/prometheus), [coredns/coredns](https://github.com/coredns/coredns), [google/syzkaller](https://github.com/google/syzkaller).

Let me know if this is something worth working on.

Cheers,
Yevgeny
Provide a experimental way to fix #687 

We let sub process called by invoke.Exec has the process group id same as its parent process id, so when the parent process is killed, all the sub processes will be killed in tree recursively.

Fox example, if we use libcni in CRI to call plugin **A**, and plugin **A** call plugin **B** and **C**, and plugin **B** call another plugin **D**, then the pid/pgid maybe like,

process|pid|pgid
-|-|-
|CRI|100|120|
|A|103|100|
|B|105|103|
|C|106|103|
|D|108|105|

If CRI (pid==100) was killed, processes(A pid==103) with group id == 100 will be killed, and processes(B pid==105 C pid==106) with group id == 103 will be killed, and process D will be killed in the same way recursively.
For the same reason , if CRI try to kill process A, B&C&D will be killed recursively.

Signed-off-by: Bruce Ma <brucema19901024@gmail.com>

I have been working on a netfilter plugin assuming that mesos would support the plugin chaining standard of 2017. Turns out that they do not! Better make sure others are notified about this, because I do not think they will change their documentation on this, as I have requested of them.


https://github.com/containernetworking/cni/pull/693

As discussed in our maintainers meeting, we believe that libcni should handle plugin cleanup when the timeout passed by the runtime triggers. libcni should send plugins a TERM signal, wait a reasonable amount of time (a couple seconds), and then KILL plugins.

One complication is that the plugins must be part of the same process group so that (on linux) we can signal the process group and deliver the same signal to all plugins spawned from the main plugin. This could be tricky as Go doesn't easily allow changes between fork & exect.

Some references that might help:
https://stackoverflow.com/questions/34730941/ensure-executables-called-in-go-process-get-killed-when-process-is-killed

We could also use syscall.SysProcAttr on Linux like so:
```
cmd := exec.Command("./long-process")

cmd.SysProcAttr = &syscall.SysProcAttr{
    Setpgid: true,
    Pgid: <some unique number we come up with?>,
}

cmd.Run()
```

And then we can signal the `Pgid`. Code for Windows is in the above stackoverflow link to do essentially the same thing.