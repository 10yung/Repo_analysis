**Describe the bug**
In yarn mode only the hive table stored as textfile can be accessed. Both parquet, orc files cannot.
In local mode everything is working ok.

**error stack**
```
 java.io.InvalidClassException: org.apache.spark.sql.execution.datasources.FilePartition; local class incompatible: stream classdesc serialVersionUID = 3545283692550565578, local class serialVersionUID = 7485156529130905057
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:616)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1630)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1521)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1781)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:371)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)
	at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2788)
	at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2788)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.collect(Dataset.scala:2788)
	at yaooqinn.kyuubi.operation.statement.ExecuteStatementInClientMode.execute(ExecuteStatementInClientMode.scala:176)
	at yaooqinn.kyuubi.operation.statement.ExecuteStatementOperation$$anon$1$$anon$2.run(ExecuteStatementOperation.scala:74)
	at yaooqinn.kyuubi.operation.statement.ExecuteStatementOperation$$anon$1$$anon$2.run(ExecuteStatementOperation.scala:70)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
	at yaooqinn.kyuubi.operation.statement.ExecuteStatementOperation$$anon$1.run(ExecuteStatementOperation.scala:70)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.InvalidClassException: org.apache.spark.sql.execution.datasources.FilePartition; local class incompatible: stream classdesc serialVersionUID = 3545283692550565578, local class serialVersionUID = 7485156529130905057
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:616)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1630)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1521)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1781)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:371)
```


**Additional context**
spark: 2.4.4
kyuubi: 0.8.0
hadoop: hdp 2.6.5
kerberos: on

hi! 
kyuubi can run spark sql sript now ? i try to run spark sql script and Passing parameters to script  but not runing. is issues ?
double click table or select without where conditions will query all the data，can only show first 200 rows？
**Describe the bug**
when i set the keytab and principal in the spark-defaults-conf.xml and start the kyuubi, the error happened.So i discard the  kerberos auth and delete the keytab and pricipal in the spark-defaults-conf.xml and start the kyuubi server, the message in log display when i make the zk.ha true, kyuubi will use kerberos to auth and the error is there is no spark.yarn.keytab.As a  result,how do i connect to zeekeeper without kerberos? My zookeeper is no auth.

**To Reproduce**
Steps to reproduce the behavior:
1. Configurations
    frist i export environment:
    export HADOOP_CONF_DIR=/etc/hadoop/conf
    export SPARK_HOME=/opt/spark/spark-2.1.3-bin-hadoop2.6
    second i editor spark-deaults.conf file to add these:
    spark.yarn.principal ***/***@HADOOP.COM
    spark.yarn.keytab ****.keytab
2. Environments
    spark:spark-2.3-bin-hadoop2.7
    linux:centos7
    java:1.8 
    kyuubi:master
    hadoop: CDH-5.13.3
    zookeeper:3.4.5+cdh5.13.3
3. Operations
    /opt/spark/kyuubi-0.7.0-bin-spark-2.1.3/bin/start-kyuubi.sh 
   --master yarn 
   --conf spark.kyuubi.ha.enabled=true 
   --conf spark.kyuubi.ha.zk.quorum=134.64.14.231,134.64.14.232,134.64.14.233 
   --conf spark.kyuubi.ha.zk.client.port=2181 
   --conf spark.kyuubi.authentication=KERBEROS 
   --conf spark.driver.memory=20g 
   --conf spark.hadoop.fs.hdfs.impl.disable.cache=true 
   --conf spark.executor.heartbeatInterval=30s
4. See error
    19/07/25 14:56:02 INFO state.ConnectionStateManager: State change: CONNECTED
    19/07/25 14:56:02 ERROR client.ZooKeeperSaslClient: An error: 
    (java.security.PrivilegedActionException: javax.security.sasl.SaslException: GSS initiate failed 
    [Caused by GSSException: No valid credentials provided (Mechanism level: Server not found in 
    Kerberos database (7) - UNKNOWN_SERVER)]) occurred when evaluating Zookeeper Quorum 
    Member's received SASL token. This may be caused by Java's being unable to resolve the 
    Zookeeper Quorum Member's hostname correctly. You may want to try to adding '- 
    Dsun.net.spi.nameservice.provider.1=dns,sun' to your client's JVMFLAGS environment. 
    Zookeeper Client will go to AUTH_FAILED state.
    19/07/25 14:56:02 ERROR zookeeper.ClientCnxn: SASL authentication with Zookeeper Quorum 
    member failed: javax.security.sasl.SaslException: An error: 
    (java.security.PrivilegedActionException: javax.security.sasl.SaslException: GSS initiate failed 
    [Caused by GSSException: No valid credentials provided (Mechanism level: Server not found in 
    Kerberos database (7) - UNKNOWN_SERVER)]) occurred when evaluating Zookeeper Quorum 
    Member's received SASL token. This may be caused by Java's being unable to resolve the 
    Zookeeper Quorum Member's hostname correctly. You may want to try to adding '- 
    Dsun.net.spi.nameservice.provider.1=dns,sun' to your client's JVMFLAGS environment. 
    Zookeeper Client will go to AUTH_FAILED state.
    19/07/25 14:56:02 ERROR curator.ConnectionState: Authentication failed
    Error: Starting Kyuubi by client yaooqinn.kyuubi.service.ServiceException: Unable to create 
    Kyuubi namespace /kyuubiserver on ZooKeeper
    Run with --help for usage help or --verbose for debug output
    19/07/25 14:56:02 INFO server.KyuubiServer: Shutting down KyuubiServer
    19/07/25 14:56:02 INFO util.ShutdownHookManager: Shutdown hook called
    **Expected behavior**
    A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Additional context**
Add any other context about the problem here.

Has the project completed the support for kudu now? I haven't found the code to support kudu. I only saw the document introduction of integrated kudu. Can you introduce the implementation logic of integrated kudu?

This error occurs when you try to create multiple spark contexts.

situation like bellow:
19/04/19 10:13:35 ERROR KyuubiOperation:
Error executing query as bi_pdz,
select * from bi_tmp.tmp_product_list_all_new_2 limit 10
Current operation state RUNNING,
java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_3_piece0 of broadcast_3
        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
        at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206)
        at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)
        at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)
        at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)
        at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
        at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:139)
        at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:195)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
        at scala.Option.getOrElse(Option.scala:121)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
        at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
        at scala.Option.getOrElse(Option.scala:121)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
        at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
        at scala.Option.getOrElse(Option.scala:121)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
        at org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:84)
        at org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:84)
        at scala.collection.parallel.AugmentedIterableIterator$class.map2combiner(RemainsIterator.scala:115)
        at scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:62)
        at scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1054)
19/04/19 10:13:35 ERROR Utils: Exception encountered
org.apache.spark.SparkException: Failed to get broadcast_3_piece0 of broadcast_3
        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:178)
        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150)
        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:150)
        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:222)
        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
        at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206)
        at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)
        at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)
        at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)
        at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
        at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:139)
        at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:195)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
        at scala.Option.getOrElse(Option.scala:121)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
        at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
        at scala.Option.getOrElse(Option.scala:121)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
        at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
        at scala.Option.getOrElse(Option.scala:121)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
        at org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:84)
        at org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:84)
        at scala.collection.parallel.AugmentedIterableIterator$class.map2combiner(RemainsIterator.scala:115)
        at scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:62)
        at scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1054)
        at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)
        at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)
        at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)
        at scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)
        at scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1051)
        at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)
        at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)
        at scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runSubtask(ForkJoinPool.java:1357)
        at scala.concurrent.forkjoin.ForkJoinPool.tryHelpStealer(ForkJoinPool.java:2253)
        at scala.concurrent.forkjoin.ForkJoinPool.awaitJoin(ForkJoinPool.java:2377)
        at scala.concurrent.forkjoin.ForkJoinTask.doJoin(ForkJoinTask.java:341)
        at scala.concurrent.forkjoin.ForkJoinTask.join(ForkJoinTask.java:673)
        at scala.collection.parallel.ForkJoinTasks$WrappedTask$class.sync(Tasks.scala:378)
        at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:443)
        at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.internal(Tasks.scala:173)
        at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:443)
        at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:149)

**Is your feature request related to a problem? Please describe.**
When submit a kyuubiAppMaster, it would only upload the jars under SPARK_JARS_DIR to kyuubiStagingDir. However, there are some jarsDir under SPARK_JARS_DIR would be used by some spark plugins, such as ranger, atlas.

**Describe the solution you'd like**
For the libDir under SPARK_JARS_DIR, when copied to jarsStream, we create zipEntry with libDirName prefix and keep its origin directory structure.

**Is your feature request related to a problem? Please describe.**
For the kyuubiAppMaster, it should have two modes: specific mode for a user and  normal mode for all user.
For the normal mode, it receive all requests from all users and it would be alive until be killed.
Therefor, its session management has no difference with local kyuubiServer.

For the specific mode, it only receive the requests from a specific user, and it should unregister itself when it has idled for a define timeout.

**Describe the solution you'd like**
There should be a parameter present the mode of kyuubiAppMaster.
```
spark.kyuubi.yarn.appmaster.specific.mode    
```
For the specific mode,  we would implement SpecifiCSparkSessionCacheMgr extends sparkSessionCacheMgr which only has one active sparkContext and the sparkSession cleaner thread should stop the kyuubiAppMaster when sparkContext idle for more than defined duration.
 


**Is your feature request related to a problem? Please describe.**
Now, kyuubi  only has kyuubiSession with local mode, which is weak scaleable.
Therefor, we implement kyuubiClusterSession, which will submit a kyuubiAppMaster and communicate with this appMaster to deliver statements and get computation results.

**Describe the solution you'd like**
The parameter below present the kyuubi session mode.
```
spark.kyuubi.session.mode   client/cluster
```
When the kyuubiSession mode is cluster,  the implement of kyuubiSession is KyuubiClusterSession.
The kyuubiClusterSession will check whether there is already exist an kyuubiAppMaster belong to current user, if not, a new kyuubiAppMaster would be create.
Then, the kyuubiClusterSession would communicate with this appMaster and deliver its statement and get computation result.


