Excuse me, I want to ask how to implement the pooling layer or  other functions defined by ourselves? @foolwood 
I meet the same problem with the same environment. After I change the default setting to nonrecursive, the problem still exists. Although the hint change, the problem seems unchanged.
Error Message below:
Error using vl_argparse (line 108)
Expected either a param-value pair or a structure.

Error in run_DCFNet>DCFNet_initialize (line 54)
state = vl_argparse(state, param, 'nonrecursive');


Error in run_DCFNet (line 27)
[state, ~] = DCFNet_initialize(im{1}, init_rect, param);


Error in demo_DCFNet (line 15)
res = run_DCFNet(subS,0,0,param);

It seems like the param must be a structure. If not, the function will be in loop. But I do not allow it to loop, the function have to stop because of the lack of structured parameter. Can you help me?
Thanks a lot.

_Originally posted by @tzjtatata in https://github.com/foolwood/DCFNet/issues/9#issuecomment-374826229_
In your tracking process, the output feature size is 125 *125*32 before the DCF layer.
However, in your training code, the networkType is set to 12, and the output size after two convolutional laysers is smaller than the input image size by 4. (feature_za=input_size-[4,4]) 
I wonder why  the saved netork (.mat file) after training can generate different feature size?
Look for your early reply! And thank you very much.
What is the function of LRN layer? I notice that some siamese trackers use BatchNorm layer.
Thanks for your work and code.

When i use the VID2015 as training data ,it will generate 464873 pairs traning data, even the batchsize is set to be 32, it still have more than 10000 calculations in one epoch. How to solve this problem when you are training the network? 

Thanks~
Dear Qiang Wang:
can you explain how  imgcrop_multiscale.m work?
I cannot grasp your thought though DCFNet indeed achieve a better results.
all best;
heng.
Thanks for good work as usual~
Take type-7 and type-12 network for example.  I find that the **padding of the conv layers is all 0 when training**(input size: 125 output size: 121). But **when tracking, the padding is set to 1** (input size: 125 output size: 125) while using the same conv parameters. Can you explain why you do like this? It is a theoretical setting(better in theory) or just a experimental result (better performance in experiments)?
why is it very slow to load  when I want to run demo ?
In one of the issues #5, you mentioned that the resolution map is an important factor why DCFNet has better performance than CFNet. Could you elaborate more information about this? How much AUC would loss if the resolution of the feature map is smaller, such as 33/63 as mentioned?
Hi, 

I noticed that while training, the CNN output (before DCF) is not mult with cos window (or hann) in contradiction to tracking pipeline that mult the CNN output.

Why?