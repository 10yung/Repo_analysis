Hi all,

I am using the version 2.0.0-M2 and I realized that updateOne can update just one attribute in the collection in the Sink.

Here I have two questions:
1. Why updateOne does not support a list of attributes to be updated? 
2. I am missing is the replaceOne method that is available in the scala driver. Are you planning to implement it?

Thanks for your help!
## Purpose

Set virtual-host-style access as the default for AWS S3 (from being path-style before).

**This change may break existing usages as it applies much harder restrictions on the bucket names.**

## References

https://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/
#1679
#2053 
#2086
#1993 
#1308

## Changes

* Mark the `path-style-access` setting as deprecated with references to the blog post
* Issue warnings if `path-style-access` is enabled

## Background Context

Even though virtual-host-style access is much more restrictive on bucket names, it is the designated way forward for S3.

Elasticsearch changes its REST API between major versions.
Alpakka could make use of [Testcontainers Elasticsearch](https://www.testcontainers.org/modules/elasticsearch/) to run against different ES versions in our tests.

## References

#945
#1232
#2025 
<!--
### Are you looking for help?

If you have a [Lightbend Subscription](https://www.lightbend.com/lightbend-platform-subscription), please reach out via the [Lightbend Portal](https://portal.lightbend.com/).

This is an issue tracker used to manage and track the development of this particular module.

Please report issues regarding other projects in their respective issue trackers, e.g.:
 - Akka:          https://github.com/akka/akka/issues 
 - Akka HTTP:     https://github.com/akka/akka-http/issues 
 - Alpakka:       https://github.com/akka/alpakka/issues 
 - Alpakka Kafka: https://github.com/akka/alpakka-kafka/issues 

Please ask questions or discuss ideas in the [discuss.akka.io forum](https://discuss.akka.io/).

## Please add the following sections to your bug report
-->

### Versions used 

<!-- add any other relevant versions here, please -->

Akka version: 

2.6.0

### Expected Behavior

We should be able to start an MQTT Server using alpakka mqtt streaming when running last Alpakka artifact in a project along with Akka 2.6.1. 


### Actual Behavior

Get this runtime exception:
```
java.lang.NoClassDefFoundError: akka/actor/typed/scaladsl/adapter/package$UntypedActorRefOps$ at akka.stream.alpakka.mqtt.streaming.scaladsl.ActorMqttServerSession.<init>(MqttSession.scala:455)
```
The problem is that implicit class UntypedActorRefOps has been renamed to ClassicActorRefOps from Akka 2.5.27 to Akka 2.6.x so when the artifact is built compiling with Akka 2.5.27 it fails to run with Akka 2.6.x

### Relevant logs

### Reproducible Test Case

Build Alpakka with Akka 2.5.27 and then use the artifact in a project with Akka 2.6.1. Try to start a MQTT broker with akka.stream.alpakka.mqtt.streaming.scaladsl.ActorMqttServerSession.
We are using MongoDB v4.2 on IBM Cloud over the Reactive MongoDB Driver v1.11. We are currently experiencing the following exception randomly, usually after some period of the app inactivity:
```
com.mongodb.MongoSocketWriteException: Exception sending message
    at com.mongodb.internal.connection.InternalStreamConnection.translateWriteException(InternalStreamConnection.java:541)
    at com.mongodb.internal.connection.InternalStreamConnection.access$1100(InternalStreamConnection.java:74)
    at com.mongodb.internal.connection.InternalStreamConnection$3.failed(InternalStreamConnection.java:470)
    at com.mongodb.internal.connection.AsynchronousChannelStream$1.failed(AsynchronousChannelStream.java:97)
    at com.mongodb.internal.connection.AsynchronousChannelStream$2.failed(AsynchronousChannelStream.java:173)
    at com.mongodb.internal.connection.AsynchronousChannelStream$AsyncWritableByteChannelAdapter$WriteCompletionHandler.failed(AsynchronousChannelStream.java:198)
    at com.mongodb.internal.connection.tlschannel.async.AsynchronousTlsChannel$10$1.run(AsynchronousTlsChannel.java:269)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Connection reset by peer
    at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
    at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
    at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
    at sun.nio.ch.IOUtil.write(IOUtil.java:65)
    at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
    at com.mongodb.internal.connection.tlschannel.impl.TlsChannelImpl.writeToChannel(TlsChannelImpl.java:479)
    at com.mongodb.internal.connection.tlschannel.impl.TlsChannelImpl.writeToChannel(TlsChannelImpl.java:464)
    at com.mongodb.internal.connection.tlschannel.impl.TlsChannelImpl.wrapAndWrite(TlsChannelImpl.java:403)
    at com.mongodb.internal.connection.tlschannel.impl.TlsChannelImpl.write(TlsChannelImpl.java:391)
    at com.mongodb.internal.connection.tlschannel.ClientTlsChannel.write(ClientTlsChannel.java:181)
    at com.mongodb.internal.connection.tlschannel.async.AsynchronousTlsChannelGroup.writeHandlingTasks(AsynchronousTlsChannelGroup.java:553)
    at com.mongodb.internal.connection.tlschannel.async.AsynchronousTlsChannelGroup.doWrite(AsynchronousTlsChannelGroup.java:501)
    at com.mongodb.internal.connection.tlschannel.async.AsynchronousTlsChannelGroup.access$400(AsynchronousTlsChannelGroup.java:67)
    at com.mongodb.internal.connection.tlschannel.async.AsynchronousTlsChannelGroup$6.run(AsynchronousTlsChannelGroup.java:459)
    ... 3 more
```

From what I've read and from what the JavaDoc gives for the driver - keepAlive parameter is supposed to be set to true by default. But to be on the safe side - we are setting it explicitly:

```
MongoClientSettings settings = MongoClientSettings.builder()
    .applyConnectionString(new ConnectionString("xxx")))
    .applyToSocketSettings(builder -> builder.keepAlive(true))
    .build();
MongoClient client = MongoClients.create(settings);
```

But that does not seem to solve the issue. It occurs randomly and rarely. Any help is much appreciated
```
[error] Test akka.stream.alpakka.ftp.FtpsWithProxyStageTest.remove failed: java.util.concurrent.TimeoutException: null, took 3.006 sec
[error]     at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1784)
[error]     at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
[error]     at scala.concurrent.java8.FuturesConvertersImpl$CF.super$get(FutureConvertersImpl.scala:91)
[error]     at scala.concurrent.java8.FuturesConvertersImpl$CF.$anonfun$get$2(FutureConvertersImpl.scala:91)
[error]     at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:62)
[error]     at scala.concurrent.package$.blocking(package.scala:124)
[error]     at scala.concurrent.java8.FuturesConvertersImpl$CF.get(FutureConvertersImpl.scala:91)
[error]     at akka.stream.alpakka.ftp.CommonFtpStageTest.remove(CommonFtpStageTest.java:110)
[error]     at akka.stream.alpakka.ftp.FtpsWithProxyStageTest.remove(FtpsWithProxyStageTest.java:44)
[error]     ...
```

https://travis-ci.com/akka/alpakka/jobs/275777720#L1271

Similar to #1954 
## Purpose

The purpose of this PR is to implement a retry logic for Elasticsearch flow which guarantees message ordering making it compatible with `ElasticsearchFlow.createWithContext`.

## References

References #2031

## Changes
* Backport of `RetryFlowCoordinator` from Akka 2.6
* Changed `ElasticsearchSimpleFlowStage` input/output ports types
* Changed `ElasticsearchFlow.simpleStageFlow` to wrap flow with `RetryFlowCoordinator`
* Added test mirroring the current `ElasticsearchFlowStage` retry testing logic

## Background Context
This PR adds support for retry logic to `ElasticsearchSimpleFlowStage` using a backported implementation of `RetryFlowCoordinator` from Akka 2.6.

The idea is to use `RetryFlowCoordinator` to check for failures in the bulk's `WriteResult`.
In case any failure is found, the failed messages are resubmitted to the `ElasticsearchSimpleFlowStage` with the successful results as passthrough.

Furthermore the single `WriteMessage` passtrhough value is augmented with the index of the message inside the bulk so that at the end they can be reordered to the original ordering.


Alpakka `2.0.0-M2`

We sporadically see this error in our logs, the auth token handling is not handling error responses correctly. In this case a retry should be done like in #2057 and #1931

https://github.com/akka/alpakka/blob/f420069bad2105e86f0c4b69e5221cde7fbeb7fe/google-cloud-storage/src/main/scala/akka/stream/alpakka/googlecloud/storage/impl/GoogleTokenApi.scala#L53-L54

Also apply to the `google-cloud-pub-sub` module

https://github.com/akka/alpakka/blob/f420069bad2105e86f0c4b69e5221cde7fbeb7fe/google-cloud-pub-sub/src/main/scala/akka/stream/alpakka/googlecloud/pubsub/impl/GoogleTokenApi.scala#L54-L55

Also the trace for this error does not show it is related to alpakka?

```
play.api.UnexpectedException: Unexpected exception[DeserializationException: Object is missing required member 'access_token']
    at play.api.http.HttpErrorHandlerExceptions$.throwableToUsefulException(HttpErrorHandler.scala:328)
    at play.api.http.DefaultHttpErrorHandler.onServerError(HttpErrorHandler.scala:251)
    at play.core.server.AkkaHttpServer$$anonfun$2.applyOrElse(AkkaHttpServer.scala:421)
    at play.core.server.AkkaHttpServer$$anonfun$2.applyOrElse(AkkaHttpServer.scala:417)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:453)
    at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
    at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:94)
    at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92)
    at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:47)
    at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:47)
    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
    at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
    at java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:177)

spray.json.DeserializationException: Object is missing required member 'access_token'
    at spray.json.package$.deserializationError(package.scala:23)
    at spray.json.ProductFormats.fromField(ProductFormats.scala:61)
    at spray.json.ProductFormats.fromField$(ProductFormats.scala:51)
    at spray.json.DefaultJsonProtocol$.fromField(DefaultJsonProtocol.scala:30)
    at spray.json.ProductFormatsInstances$$anon$3.read(ProductFormatsInstances.scala:81)
    at spray.json.ProductFormatsInstances$$anon$3.read(ProductFormatsInstances.scala:71)
    at akka.http.scaladsl.marshallers.sprayjson.SprayJsonSupport.$anonfun$sprayJsonUnmarshaller$1(SprayJsonSupport.scala:31)
    at akka.http.scaladsl.util.FastFuture$.$anonfun$map$1(FastFuture.scala:23)
    at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41)
    at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$3(FastFuture.scala:51)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:447)
    at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
    at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:94)
    at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92)
    at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:47)
    at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:47)
    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
    at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
    at java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:177)

java.util.NoSuchElementException: key not found: access_token
    at scala.collection.MapOps.default(Map.scala:246)
    at scala.collection.MapOps.default$(Map.scala:245)
    at scala.collection.AbstractMap.default(Map.scala:376)
    at scala.collection.MapOps.apply(Map.scala:148)
    at scala.collection.MapOps.apply$(Map.scala:147)
    at scala.collection.AbstractMap.apply(Map.scala:376)
    at spray.json.ProductFormats.fromField(ProductFormats.scala:58)
    at spray.json.ProductFormats.fromField$(ProductFormats.scala:51)
    at spray.json.DefaultJsonProtocol$.fromField(DefaultJsonProtocol.scala:30)
    at spray.json.ProductFormatsInstances$$anon$3.read(ProductFormatsInstances.scala:81)
    at spray.json.ProductFormatsInstances$$anon$3.read(ProductFormatsInstances.scala:71)
    at akka.http.scaladsl.marshallers.sprayjson.SprayJsonSupport.$anonfun$sprayJsonUnmarshaller$1(SprayJsonSupport.scala:31)
    at akka.http.scaladsl.util.FastFuture$.$anonfun$map$1(FastFuture.scala:23)
    at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41)
    at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$3(FastFuture.scala:51)
    at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:447)
    at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
    at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:94)
    at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92)
    at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:47)
    at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:47)
    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
    at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
    at java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:177)
```
<!--
# Release Train Issue Template for Alpakka

(Liberally copied and adopted from Scala itself https://github.com/scala/scala-dev/blob/b11cd2e4a4431de7867db6b39362bea8fa6650e7/notes/releases/template.md)

For every Alpakka release, make a copy of this file named after the release, and expand the variables.
Ideally replacing variables could become a script you can run on your local machine.

Variables to be expanded in this template:
- 2.0.0=???

Key links:
  - akka/alpakka milestone: https://github.com/akka/alpakka/milestone/?
-->
### ~ 1 week before the release

- [ ] Check that any new `deprecated` annotations use the correct version name
- [ ] Check that open PRs and issues assigned to the milestone are reasonable
- [ ] Decide on planned release date
- [ ] Notify depending projects (Akka persistence plugins) about the upcoming release
- [ ] Create a new milestone for the [next version](https://github.com/akka/alpakka/milestones)
- [ ] Check [closed issues without a milestone](https://github.com/akka/alpakka/issues?utf8=%E2%9C%93&q=is%3Aissue%20is%3Aclosed%20no%3Amilestone) and either assign them the 'upcoming' release milestone or `invalid/not release-bound`

### 1 day before the release

- [ ] Make sure all important / big PRs have been merged by now
- [ ] Communicate that a new version is about to be released in [Gitter Akka Dev Channel](https://gitter.im/akka/dev), so that no new Pull Requests are merged

### Preparing release notes in the documentation / announcement

- [ ] For non-patch releases: rename the 'alpakka-x.x-stable' and 'alpakka-supported-x.x-stable' reporting projects in [WhiteSource](https://saas.whitesourcesoftware.com/Wss/WSS.html#!project;id=517292) accordingly (unfortunately this requires permissions that cannot be shared outside of Lightbend)
- [ ] Check readiness levels in `/project/project-info.conf`, and put in the release date for any new modules
- [ ] Check supported projects listing in `whitesourceSupported`
- [ ] Add a release notes entry in `docs/src/main/paradox/release-notes/` listing contributors generated by [`sbt-authors`](https://github.com/2m/authors) (eg. `sbt authors v0.22 HEAD`)
- [ ] For non-patch releases: Create a news item draft PR on [akka.github.com](https://github.com/akka/akka.github.com), using the milestone
- [ ] Move all [unclosed issues](https://github.com/akka/alpakka/issues?q=is%3Aopen+is%3Aissue+milestone%3A2.0.0) for this milestone to the next milestone
- [ ] Release notes PR has been merged

### Cutting the release

- [ ] Wait until [master build finished](https://travis-ci.com/akka/alpakka/builds/) after merging the release notes
- [ ] Create a [new release](https://github.com/akka/alpakka/releases/new) with the next tag version `v2.0.0`, title and release description linking to announcement, release notes and milestone
- [ ] Check that Travis CI release build has executed successfully (Travis will start a [CI build](https://travis-ci.com/akka/alpakka/builds) for the new tag and publish artifacts to Bintray and documentation to Gustav)
- [ ] Go to [Bintray](https://bintray.com/akka/maven/alpakka) and select the just released version
- [ ] Go to the Maven Central tab, check the *Close and release repository when done* checkbox and sync with Sonatype (using your Sonatype TOKEN key and password)
- [ ] Close the [2.0.0 milestone](https://github.com/akka/alpakka/milestones?direction=asc&sort=due_date)

### Check availability

- [ ] Check [API](https://doc.akka.io/api/alpakka/2.0.0/) documentation
- [ ] Check [reference](https://doc.akka.io/docs/alpakka/2.0.0/) documentation
- [ ] Check the release on [Maven central](http://central.maven.org/maven2/com/lightbend/akka/akka-stream-alpakka-xml_2.12/2.0.0/)

### When everything is on maven central
  - [ ] Log into `gustav.akka.io` as `akkarepo` 
    - [ ] update the `current` links on `repo.akka.io` to point to the latest version with
         ```
         ln -nsf 2.0.0 www/docs/alpakka/current
         ln -nsf 2.0.0 www/api/alpakka/current
         ln -nsf 2.0.0 www/docs/alpakka/1.1
         ln -nsf 2.0.0 www/api/alpakka/1.1
         ```
    - [ ] check changes and commit the new version to the local git repository
         ```
         cd ~/www
         git add docs/alpakka/1.1 docs/alpakka/current docs/alpakka/2.0.0
         git add api/alpakka/1.1 api/alpakka/current api/alpakka/2.0.0
         git commit -m "Alpakka 2.0.0"
         ```

### Announcements

- [ ] For non-patch releases: Merge draft news item for [akka.io](https://github.com/akka/akka.github.com)
- [ ] Send a release notification to [Lightbend discuss](https://discuss.akka.io)
- [ ] Tweet using the akkateam account (or ask someone to) about the new release
- [ ] Announce on [Gitter akka/akka](https://gitter.im/akka/akka)
- [ ] Announce internally

### Afterwards

- [ ] If Couchbase has relevant changes, create/update PR in [Akka Persistence Couchbase](https://github.com/akka/akka-persistence-couchbase/) to upgrade to 2.0.0
- [ ] Update version for [Lightbend Supported Modules](https://developer.lightbend.com/docs/lightbend-platform/introduction/getting-help/build-dependencies.html#_alpakka) in [private project](https://github.com/lightbend/lightbend-platform-docs/blob/master/docs/modules/getting-help/examples/build.sbt)
- Close this issue
```
[info] CSV Formatting
[info] - should format simple value *** FAILED *** (1 second, 595 milliseconds)
[info]   A timeout occurred waiting for a future to complete. Queried 2 times, sleeping 60 milliseconds between each query. (CsvFormattingSpec.scala:57)
[info]   org.scalatest.concurrent.Futures$FutureConcept$$anon$1:
[info]   at org.scalatest.concurrent.Futures$FutureConcept.tryTryAgain$1(Futures.scala:538)
[info]   at org.scalatest.concurrent.Futures$FutureConcept.futureValueImpl(Futures.scala:550)
[info]   at org.scalatest.concurrent.Futures$FutureConcept.futureValueImpl$(Futures.scala:479)
[info]   at org.scalatest.concurrent.ScalaFutures$$anon$1.futureValueImpl(ScalaFutures.scala:275)
[info]   at org.scalatest.concurrent.Futures$FutureConcept.futureValue(Futures.scala:476)
[info]   at org.scalatest.concurrent.Futures$FutureConcept.futureValue$(Futures.scala:475)
[info]   at org.scalatest.concurrent.ScalaFutures$$anon$1.futureValue(ScalaFutures.scala:275)
[info]   at docs.scaladsl.CsvFormattingSpec.$anonfun$new$3(CsvFormattingSpec.scala:57)
[info]   at akka.stream.testkit.scaladsl.StreamTestKit$.assertAllStagesStopped(StreamTestKit.scala:32)
[info]   at docs.scaladsl.CsvFormattingSpec.$anonfun$new$2(CsvFormattingSpec.scala:43)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.WordSpecLike$$anon$3.apply(WordSpecLike.scala:1075)
[info]   at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
[info]   at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
[info]   at org.scalatest.WordSpec.withFixture(WordSpec.scala:1881)
[info]   at org.scalatest.WordSpecLike.invokeWithFixture$1(WordSpecLike.scala:1073)
[info]   at org.scalatest.WordSpecLike.$anonfun$runTest$1(WordSpecLike.scala:1085)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:286)
[info]   at org.scalatest.WordSpecLike.runTest(WordSpecLike.scala:1085)
[info]   at org.scalatest.WordSpecLike.runTest$(WordSpecLike.scala:1067)
[info]   at docs.scaladsl.CsvSpec.org$scalatest$BeforeAndAfterEach$$super$runTest(CsvSpec.scala:13)
[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:221)
[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:214)
[info]   at docs.scaladsl.CsvSpec.runTest(CsvSpec.scala:13)
[info]   at org.scalatest.WordSpecLike.$anonfun$runTests$1(WordSpecLike.scala:1144)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:393)
[info]   at scala.collection.immutable.List.foreach(List.scala:392)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:381)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:370)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:407)
[info]   at scala.collection.immutable.List.foreach(List.scala:392)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:381)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:376)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:458)
[info]   at org.scalatest.WordSpecLike.runTests(WordSpecLike.scala:1144)
[info]   at org.scalatest.WordSpecLike.runTests$(WordSpecLike.scala:1143)
[info]   at org.scalatest.WordSpec.runTests(WordSpec.scala:1881)
[info]   at org.scalatest.Suite.run(Suite.scala:1124)
[info]   at org.scalatest.Suite.run$(Suite.scala:1106)
[info]   at org.scalatest.WordSpec.org$scalatest$WordSpecLike$$super$run(WordSpec.scala:1881)
[info]   at org.scalatest.WordSpecLike.$anonfun$run$1(WordSpecLike.scala:1189)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:518)
[info]   at org.scalatest.WordSpecLike.run(WordSpecLike.scala:1189)
[info]   at org.scalatest.WordSpecLike.run$(WordSpecLike.scala:1187)
[info]   at docs.scaladsl.CsvSpec.org$scalatest$BeforeAndAfterAll$$super$run(CsvSpec.scala:13)
[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info]   at docs.scaladsl.CsvSpec.run(CsvSpec.scala:13)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:317)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:510)
[info]   at sbt.TestRunner.runTest$1(TestFramework.scala:113)
[info]   at sbt.TestRunner.run(TestFramework.scala:124)
[info]   at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282)
[info]   at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246)
[info]   at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282)
[info]   at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282)
[info]   at sbt.TestFunction.apply(TestFramework.scala:294)
[info]   at sbt.Tests$.$anonfun$toTask$1(Tests.scala:309)
[info]   at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46)
[info]   at sbt.std.Transform$$anon$4.work(System.scala:67)
[info]   at sbt.Execute.$anonfun$submit$2(Execute.scala:269)
[info]   at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16)
[info]   at sbt.Execute.work(Execute.scala:278)
[info]   at sbt.Execute.$anonfun$submit$1(Execute.scala:269)
[info]   at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178)
[info]   at sbt.CompletionService$$anon$2.call(CompletionService.scala:37)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[info]   at java.lang.Thread.run(Thread.java:748)
```

https://travis-ci.com/akka/alpakka/jobs/272322348#L532