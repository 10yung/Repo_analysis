https://hub.docker.com/r/apache/zeppelin
https://hub.docker.com/r/apache/zeppelin
Hi,
Any reason why zeppelin is taking only 1 core and not ALL cores in my machine? Any way to change that? 
Hi,
I'm having issues with this command:
```
val spark = (SparkSession
          .builder()
          .appName("interfacing spark sql to hive metastore without configuration file")
          .config("hive.metastore.uris", "thrift://hive-metastorerver-201227-ro-001:903,thrift://hive-metasteserver-201727-ro-002:9083,thrift://hive-metastoreser-201727-ro-003:9083")
          .enableHiveSupport()
          .getOrCreate())
```
I get error:
`java.lang.IllegalArgumentException: Unable to instantiate SparkSession with Hive support because Hive classes are not found.`
I've added . the jars via the interpreter and restarted it, but still getting the issue. Any idea how to solve this? 
I added `org.apache.spark:spark-hive_2.11:2.1.0` under "jdbc"
tried to run spark interpreter command sc.version results in an exception:


:: problems summary ::
:::: WARNINGS
		module not found: org.elasticsearch#elasticsearch-spark-20_2.11;5.0.0-alpha5

	==== local-m2-cache: tried

	  file:/root/.m2/repository/org/elasticsearch/elasticsearch-spark-20_2.11/5.0.0-alpha5/elasticsearch-spark-20_2.11-5.0.0-alpha5.pom

	  -- artifact org.elasticsearch#elasticsearch-spark-20_2.11;5.0.0-alpha5!elasticsearch-spark-20_2.11.jar:

	  file:/root/.m2/repository/org/elasticsearch/elasticsearch-spark-20_2.11/5.0.0-alpha5/elasticsearch-spark-20_2.11-5.0.0-alpha5.jar

	==== local-ivy-cache: tried

	  /root/.ivy2/local/org.elasticsearch/elasticsearch-spark-20_2.11/5.0.0-alpha5/ivys/ivy.xml

	  -- artifact org.elasticsearch#elasticsearch-spark-20_2.11;5.0.0-alpha5!elasticsearch-spark-20_2.11.jar:

	  /root/.ivy2/local/org.elasticsearch/elasticsearch-spark-20_2.11/5.0.0-alpha5/jars/elasticsearch-spark-20_2.11.jar

	==== central: tried

	  https://repo1.maven.org/maven2/org/elasticsearch/elasticsearch-spark-20_2.11/5.0.0-alpha5/elasticsearch-spark-20_2.11-5.0.0-alpha5.pom

	  -- artifact org.elasticsearch#elasticsearch-spark-20_2.11;5.0.0-alpha5!elasticsearch-spark-20_2.11.jar:

	  https://repo1.maven.org/maven2/org/elasticsearch/elasticsearch-spark-20_2.11/5.0.0-alpha5/elasticsearch-spark-20_2.11-5.0.0-alpha5.jar

	==== spark-packages: tried

	  http://dl.bintray.com/spark-packages/maven/org/elasticsearch/elasticsearch-spark-20_2.11/5.0.0-alpha5/elasticsearch-spark-20_2.11-5.0.0-alpha5.pom

	  -- artifact org.elasticsearch#elasticsearch-spark-20_2.11;5.0.0-alpha5!elasticsearch-spark-20_2.11.jar:

	  http://dl.bintray.com/spark-packages/maven/org/elasticsearch/elasticsearch-spark-20_2.11/5.0.0-alpha5/elasticsearch-spark-20_2.11-5.0.0-alpha5.jar

		::::::::::::::::::::::::::::::::::::::::::::::

		::          UNRESOLVED DEPENDENCIES         ::

		::::::::::::::::::::::::::::::::::::::::::::::

		:: org.elasticsearch#elasticsearch-spark-20_2.11;5.0.0-alpha5: not found

		::::::::::::::::::::::::::::::::::::::::::::::



:::: ERRORS
	Server access error at url https://repo1.maven.org/maven2/org/elasticsearch/elasticsearch-spark-20_2.11/5.0.0-alpha5/elasticsearch-spark-20_2.11-5.0.0-alpha5.pom (javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target)

	Server access error at url https://repo1.maven.org/maven2/org/elasticsearch/elasticsearch-spark-20_2.11/5.0.0-alpha5/elasticsearch-spark-20_2.11-5.0.0-alpha5.jar (javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target)


:: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS
Exception in thread "main" java.lang.RuntimeException: [unresolved dependency: org.elasticsearch#elasticsearch-spark-20_2.11;5.0.0-alpha5: not found]
	at org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:1177)
	at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:298)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:153)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterManagedProcess.start(RemoteInterpreterManagedProcess.java:143)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.reference(RemoteInterpreterProcess.java:73)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:265)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getFormType(RemoteInterpreter.java:430)
	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.getFormType(LazyOpenInterpreter.java:111)
	at org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:387)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:175)
	at org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:329)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

Inside the docker container i issued a curl command against the maven repo library with following result:
curl: (60) SSL certificate problem: self signed certificate in certificate chain
More details here: http://curl.haxx.se/docs/sslcerts.html

Any ideas how to resolve?


How to install file interpreter in Zeppelin docker?
As described,  I viewed the page http://zeppelin.apache.org/docs/0.7.2/manual/interpreterinstallation.html

And I exec the command:
./bin/install-interpreter.sh --name file


`root@020cfd6241f9:/usr/zeppelin# ./bin/install-interpreter.sh --name file
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/zeppelin/lib/interpreter/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/zeppelin/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Install file(org.apache.zeppelin:zeppelin-file:0.7.2) to /usr/zeppelin/interpreter/file ... 
Interpreter file installed under /usr/zeppelin/interpreter/file.

1. Restart Zeppelin
2. Create interpreter setting in 'Interpreter' menu on Zeppelin GUI
3. Then you can bind the interpreter on your note
`


I do not know how to Restart Zeppelin to make the file interpreter to effect?
Thank you for your awesome containerisation of zeppelin - easiest way to run for sure.

So I'd like to tweak your Dockerfile to do some stuff slightly different, but when I clone and try to build the container locally I get the following.  Have you seen this before? I'm not behind a proxy so this should work.  Is this a docker networking issue? Or the Zeppelin build is indeed dodgy?

```
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-war-plugin:2.5:war (default-war) on project zeppelin-web: Execution default-war of goal org.apache.maven.plugins:maven-war-plugin:2.5:war failed: Plugin org.apache.maven.plugins:maven-war-plugin:2.5 or one of its dependencies could not be resolved: Failed to collect dependencies at org.apache.maven.plugins:maven-war-plugin:jar:2.5 -> org.codehaus.plexus:plexus-io:jar:2.1.3: Failed to read artifact descriptor for org.codehaus.plexus:plexus-io:jar:2.1.3: Could not transfer artifact org.codehaus.plexus:plexus-io:pom:2.1.3 from/to central (https://repo.maven.apache.org/maven2): repo.maven.apache.org:443 failed to respond -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :zeppelin-web
```
Hi

Does anyone here has an experience with adding hive-site.xml file to this docker?
I tried to add it to Zeppelin conf / spark conf / external folder, and nothing happened, it just ignore it although I added `export ZEPPELIN_INTP_CLASSPATH_OVERRIDES` to zeppelin-env.sh file.

I also added the relevant mariadb jar for that and still nothing

Thanks for the help
D.
Currently the version installed by this docker image is python 3.4. 

Is there any plan to upgrade to 3.5+? If not, do you know what this would entail on the end of someone consuming this docker image?