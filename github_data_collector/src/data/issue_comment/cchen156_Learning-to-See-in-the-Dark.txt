Hello. Which version of numpy and scipy did you use?
Update dataset download link

PTAL
Hey, why do you not set the 'no_auto_bright' field in the rawpy postprocessing to false. 
Hi. How much time does it usually take to train on the SONY model using the specified configuration. Thanks.
I don't have a iPhone 6s handy, but I heard some people are getting good results with iPhone 6s. If I try Sony model with raw images taken with iPhone with two or three cameras, (iPhone 8 or iPhone 11), will the result be roughly the same as with iPhone 6s? Thanks!
It looks like the google links in the Readme have been updated. Therefore, download_dataset.py needs to be updated to work.

For Sony, I've used:
download_file_from_google_drive('1G6VruemZtpOyHjOC5N8Ww3ftVXOydSXx', 'Sony.zip')
os.system('unzip Sony.zip -d dataset')
    conv10 = slim.conv2d(conv9, 12, [1, 1], rate=1, activation_fn=None, scope='g_conv10')
    out = tf.depth_to_space(conv10, 2)

As stated above, in the last part of the network you set conv10's num_outputs is 12 and then do depth_to_space(conv10, 2) to make the channel of out to 3. 
Now, I want to know why you didn't set conv10's num_outputs 3 directly. 
After downloading the model, I tried to test own sony photos.
When I run the test code, i got no failure, it looks like worked ok but no photos will be saved in results. I also adapt reading black levels. Do someone know why this happen? I attached a screenshot how I put the test data in the folder.
thanks!

![Bildschirmfoto 2019-08-30 um 13 36 52](https://user-images.githubusercontent.com/20739395/64017971-41d4ad00-cb2b-11e9-8a33-23a615ac5c47.png)


I download the dataset (Fuji) and read the image using the code provided, I got the input images with size 1344x2010x9, and the output images with size 4032x6032x3. Here, the width of the output is NOT 3 times the width of input (2010 vs 6032), is there anything wrong? or do I have to cut 2 pixels from the output image? If so, how to cut? from left or right or both?

Thanks.
Why not set the ratio to 1 and let the network learn a ratio when training the model?

I noticed that the author set the ratio to "gt exposure time / short exposure time" when training the model, and we need to set a ratio externally when testing the model.

However, after trying different ratios, I found that the result images had different qualities besides different exposure, and there exists a perfect ratio, which we don't know before testing.

I assume that the ratio we choose affects the network performance, since the ratio is applied to the image before it is put into the network. So why not let the network learn a ratio?