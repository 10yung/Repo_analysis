Add the ability to do structured json logging via a flag. This will allow the logs in kubernetes to easily be parsed into existing logging infrastructure. This would help with developers to see if their SealedSecret was generated properly. Seeing clear logs is highly beneficial. Currently some logs are almost json, but not quite.
Testing out kubeseal.


Deployed the helmchart 1.6.1. Was able to get the cert just fine. 

```
kubectl create secret generic mysql --dry-run --from-literal=ROOT_PASSWORD=my-root-password --namespace wdev -o json | kubeseal -o yaml > .\secret.yaml
```

```
2020/01/12 23:49:23 Error updating wdev/mysql, giving up: no key could decrypt secret (ROOT_PASSWORD)
2020/01/12 23:49:23 Event(v1.ObjectReference{Kind:"SealedSecret", Namespace:"wdev", Name:"mysql", UID:"6ff7d9d6-fb69-40a8-87a3-00582e748f34", APIVersion:"bitnami.com/v1alpha1", ResourceVersion:"17107584", FieldPath:""}): type: 'Warning' reason: 'ErrUnsealFailed' Failed to unseal: no key could decrypt secret (ROOT_PASSWORD)
E0112 23:49:23.056086       1 controller.go:194] no key could decrypt secret (ROOT_PASSWORD)
```

After deployment, the controller shows it could not decrypt the secret. Following the concept of scopes, https://github.com/bitnami-labs/sealed-secrets#scopes I deleted the empty secret and the kubeseal secret, made the namespace and switched context to that namespace. Then, I deployed the kubeseal secret again. I got the same error in the logs, but this time with the secret populated with the correct data.

It looks like everything went ok. The logs seem to show otherwise so I am a bit confused here. 
`kubeseal --recovery-unseal --recovery-private-key master.key -o yaml < some_secret.yaml` fails with error message `error: invalid configuration: no configuration has been provided` if you don't have a `~/.kube/config` file.

I'd assume the fix would be similar to that of https://github.com/bitnami-labs/sealed-secrets/issues/101

I actually managed to circumvent the problem by just adding the `--cert` option to the command, like this:
`kubeseal --recovery-unseal --recovery-private-key master.key -o yaml --cert secrets.pem < some_secret.yaml`, which apparently triggers the fix from the issue linked above so that the command runs successfully also without the `~/.kube/config` file.

If you're wondering why I need/want to run it like this, without the `~/.kube/config`, the answer is that I've created a CLI tool that wraps this command (and the command for sealing) for ease of use and I want to have tests that verify it's able to seal and unseal the secrets correctly. The tests are of course using a dummy keypair for this purpose and the tests run in an evironment that doesn't have access to any kubernetes cluster.

Experienced with `kubeseal version: v0.9.6`
Improve documents about `sealedsecrets.bitnami.com/sealed-secrets-key=active` 
mentioned in https://github.com/bitnami-labs/sealed-secrets/issues/258#issuecomment-531281239

Create document in `docs/` with some examples and link to `FAQ` section in `README`.
I will soon open an PR about this.
I would prefer a way to automatically delete older keys after a set period of time(configurable).

Let's say I rotate my keys every x days.  After x days, mark the older key as active but superseded and then after y days(configurable) remove the superseded keys that are more than y days old.

Another option is to mark them inactive (instead of deleting) and let the admin manage the deletion.
Hello,

I'm looking for a solution like `kubeseal` to `encrypt/decrypt` on the fly our kubernetes secrets to allow us to put all of them into a git repository _(helm chart)_.

So I followed the recent Improvements made on the tool like the ability to deploy `kubeseal` into a single namespace and to configure it to **not watch** secrets in all namespace _(flag: --all-namespaces=false)_.

**Now my question is:** 

We are sharing a kubernetes cluster _(we are restricted in namespaces that we owned, no admin access on the cluster)_ and we have applications in different namespaces. Can we have a kubeseal instance deployed on a dedicated namespace and tell him to manage `encryption/decryption` of secrets located in different namespaces _(not all namespaces but a list of allowed namespaces)_ or Do I need to deploy a kubeseal controller on each namespace and switch between all of them when I want to encrypt a secret _(because I think not the same encryption key pair is used)_?

Thanks you for your reply 
<raw dump from a discussion in the k8s channel>

The user opens the sealed secrets UI
and sees a large text box with a skeleton of a k8s Secret resource
and instructions to paste over it an existing unencrypted secret or an already encrypted secret.
Then:

a) if the input is an already encrypted secret skip to step (c)
b) if the input is an plain old k8s unencrypted secret, the UI will encrypt it in-browser via client side version of kubeseal
c) now the large text field contains a valid encrypted sealed secret. the UI now offers a way to further refine it, by adding more items (key/value pairs) or updating existing items. Updating an existing item will cause the UI client to encrypt that value (akin to what `--raw` does).

The UI won't ever decrypt the value, nor in other ways have a privileged channel with the controller other that possibly being a bunch of HTML/JS/WASM files served from it.
Proposal:

Check the presence of an annotation sealedsecrets.bitnami.com/managed: "true" ; if true then treat the secret as owned by sealed-secret controller even if there is no ownersReference.

Main use cases:

1. A user wants to install something via kustomize (e.g. argocd) and cannot remove a secret from the base nor re-kind the secret (because it's against kustomize philosophy) 
2. A user wants to be resilient against cascade deletes of their unsealed secrets due to the disappearance of the CRD; the user (somehow, to be defined elsewhere) removes the ownerReferences to avoid the automatic cascade, but then updates of the sealed secrets no longer work.
3. A user has a bunch of legacy secrets and wants to replace them with sealed secrets


When trying to seal a secret with the kubeseal client like in the follwing, kubeseal hangs:

`kubeseal < mysecret.yml -o yaml`

When I set a timeout with the `--request-timeout` option, I get a more detailed message:
```
E1114 14:56:18.638781    8199 round_trippers.go:174] CancelRequest not implemented by *oidc.roundTripper
E1114 14:56:18.639062    8199 request.go:858] Unexpected error when reading response body: net/http: request canceled (Client.Timeout exceeded while reading body)
error: cannot fetch certificate: Unexpected error when reading response body. Please retry. Original error: net/http: request canceled (Client.Timeout exceeded while reading body)
```

using it via:
`kubeseal < mysecret.yml -o yaml --cert certfile.cert` works.

What am I doing wrong?

Some details about my setup:
* using release version 0.9.5 (client & controller)
* accessing the cert.pem, via port-forwarding the sealed-secret-controller service and downloading it from /v1/cert.pem works
* RBAC is enabled (but the used user has full permissions on the resources in the namespace)

Thanks for your help
Hi, I have big trouble to understand documentation for 0.9.x.
It seems that current wording is perfect for security person but not for average user.
I hope that my questions will help simplify README or create some sort of FAQ / TL;DR doc to clear confusion for other users allowing quick start.

In my case I just wanted a tool that just encrypts secrets and then it can be safely committed to repo.
It uses some keys to encrypt it, it's understandable, so I made a backup of it to safe location to allow disaster recovery.
I started using this tool from 0.8.x and it's doing it's job great. I like it.

I've seen 0.9.x and wanted to upgrade but some parts of it changes automatically, I'm still not sure what it means to me. README is very complicated to understand (maybe it's just me?)

Normally as a cluster operator you want to know what is going on, what can go wrong and what we should do regularly in simple terms.
Especially if you need to recreate whole cluster in case of some disaster, so you basically DON'T want automatic changes because some steps to recreate cluster can stop working and it can go very wrong.

So after migration to 0.9.x do I need to...
- change my backed-up key for encryption ?
- change secrets in repo ?
- ignore some changes in encrypted secrets in tools that diffs and auto apply yamls with secrets from repo ? (argo CD)
- do all of the above steps are different after waiting default time for automatic rotation ?
- do all of the above will change in future updates ?

And bonus question...
How to modify controller yaml to keep old, no automatic behavior just in case to prevent fuckups?
There is info in README (but not in changelog with migration info) about arg for controller but there is a risk that I edit yaml wrong, my setup explodes and do some changes that I don't want. Can you provide small remark about how to apply it in yaml?