This pull request restores a field that was inadvertently removed in the last regeneration cycle. 
Remember, an issue is not the place to ask questions. You can use [Stack Overflow](http://stackoverflow.com/questions/tagged/ibm-watson-cognitive) for that, or you may want to start a discussion on the [dW Answers](https://developer.ibm.com/answers/questions/ask/?topics=watson).

Before you open an issue, please check if a similar issue already exists or has been closed before.

### When reporting a bug, please be sure to include the following:  
- [ ] Start the title with the service name in brackets:  `[speech-to-text] websockets...`
- [ ] Steps to reproduce
- [ ] Expected behavior
- [ ] Actual behavior
- [ ] Node version
- [ ] SDK version: 5.2.1


### When you open an issue for a feature request, please add as much detail as possible:
- [ ] A descriptive title starting with the service name
- [ ] A description of the problem you're trying to solve
- [ ] A suggested solution if possible

I upgraded from v4 to v5 and found that the 'include_count' param is removed, which is stated in the release notes. I think this is really useful as Watson Assistant has a 2000 limit for intents inside a workspace. If I cannot get the current Intent count in the workspace, I do not know how many intents I can add. For example, when I need to add 200 intents and the workspace has 1900 intents inside, the adding requests will fail.

Really need the count to determine if I could add Intents to a workspace. Is it possible to add it back? In the official docs: [https://cloud.ibm.com/apidocs/assistant/assistant-v1#list-intents](https://cloud.ibm.com/apidocs/assistant/assistant-v1#list-intents), the include_count param is still there.

In the curl results, the 'pagination' contains 'total' and 'matched' when I set param 'include_count' to 'true', but the Node.js client doesn't have this option.
https://cloud.ibm.com/apidocs/speech-to-text/speech-to-text supports mpeg, and I have .mp4 files. I thought recognize API would support mp4, but doesn't seem so? Do I miss anything? 
I can feed in either .mp4 or .aac.
```
const Speech = { 

    recognize: function(path){
        window.btoa = require('Base64').btoa;
        console.log(path);

        var body = new FormData();
        let metadata = {
          part_content_type: 'audio/mpeg;'
        };
        body.append('metadata', JSON.stringify(metadata));
        body.append('upload', {
            uri: path,
            name: path,
            type: `audio/mpeg`,
        });
        return new Promise((resolve, reject) => {
            fetch(recognizeAPI, {
                method: 'POST',
                headers: {
                    Accept: 'application/json',
                    'Authorization': 'Basic '+btoa(credential),
                },
                body,
            })
            .then(response => response.json())
            .then(responseJson =>{
                resolve (responseJson);
            })
            .catch(e => {console.log(e);})
        });
    },
```
`Remember,` an issue is not the place to ask questions. You can use [Stack Overflow](http://stackoverflow.com/questions/tagged/ibm-watson) for that, or you may want to start a discussion on the [dW Answers](https://developer.ibm.com/answers/questions/ask/?topics=watson).

 Before you open an issue, please check if a similar issue already exists or has been closed before.

**Check service status**

1.  For service issues or 5xx errors, first, go to the [IBM Cloud status page](https://cloud.ibm.com/status?component=compare-comply%2Cdiscovery%2Cconversation%2Cwatson-vision-combined%2Cnatural-language-understanding%2Cnatural-language-classifier%2Clanguage-translator%2Cpersonality-insights%2Cspeech-to-text%2Ctext-to-speech%2Ctone-analyzer&selected=status) and check the status of the service.
1.  If the service status is OK, continue with a bug report.

---

**Overview**
_What is the issue? Be concise and clear._
```const NaturalLanguageUnderstandingV1 = require("ibm-watson/natural-language-understanding/v1");
const { IamAuthenticator } = require("ibm-watson/auth");

const naturalLanguageUnderstanding = new NaturalLanguageUnderstandingV1({
  version: "2019-07-12",
  authenticator: new IamAuthenticator({
    apikey: String.raw`XXXXXXXXXX`
  }),
  url: String.raw`XXXXXXXXX`
});

const analyzeParams = {
  url: "www.ibm.com",
  features: {
    categories: {
      limit: 3
    }
  }
};

function analyze() {
  naturalLanguageUnderstanding
    .analyze(analyzeParams)
    .then(analysisResults => {
      return JSON.stringify(analysisResults, null, 2);
    })
    .catch(err => {
      console.log("error:", err);
    });
}

module.exports = {
  anaylze: analyze
};
```

**And this error is coming**

`error: { Error: connect ECONNREFUSED 127.0.0.1:80
    at RequestWrapper.formatError (/Users/muneeburrehman/RunningCodes/web/5thSemchatApp/node_modules/ibm-cloud-sdk-core/lib/request-wrapper.js:208:21)
    at /Users/muneeburrehman/RunningCodes/web/5thSemchatApp/node_modules/ibm-cloud-sdk-core/lib/request-wrapper.js:196:25
    at process._tickCallback (internal/process/next_tick.js:68:7)
  message: 'connect ECONNREFUSED 127.0.0.1:80',
  statusText: 'ECONNREFUSED',
  body:
   'Response not received - no connection was made to the service.' }`
**Expected behavior**
_What did you expect to happen?_

**Actual behavior**
_What actually happened?_

**How to reproduce**
_Help us to reproduce what you experienced. Include your code snippets (without credentials)_

**Screenshots**
_If applicable, add screenshots to help explain your problem._

**SDK Version**
_Please provide the SDK version here._

**Additional information:**
- OS: _for example, iOS_
- Which version of `Node` are you using?

**Additional context**
_Add any other details about the problem._

Hello guys,
I tryed to use v2 integration but return the error:
message: 'Invalid Session',
How can i have this session id?
![image](https://user-images.githubusercontent.com/11997891/72101879-7855dc00-3304-11ea-8da6-4f7ee2708d17.png)

I am trying to use V2 and I get 404 error - resource not found. Any reason why?
```javascript
const AssistantV2 = require('ibm-watson/assistant/v2');
const IamAuthenticator = require('ibm-watson/auth').IamAuthenticator;
const uuidv4 = require('uuid/v4');

const assistant = new AssistantV2({
    authenticator: new IamAuthenticator({
        apikey: '--api-key-here--'
    }),
    // url: 'https://gateway.watsonplatform.net/assistant/api/',
    // url: 'https://api.us-south.assistant.watson.cloud.ibm.com',
        url: 'https://api.us-south.assistant.watson.cloud.ibm.com/instances/uuid-here',
        version: '2019-02-28'
});

const ASSISTANT_ID = 'uuid-here';

assistant.createSession({
        assistantId: ASSISTANT_ID
    })
    .then(res => {
        console.log(JSON.stringify(res, null, 2));
    })
    .catch(err => {
        console.log(err);
    });
```
We need to update examples like https://github.com/watson-developer-cloud/node-sdk/blob/master/examples/speech_to_text.v1.js to use the new authenticators.
Looking at previous resolved issues i found a closed one (#30) that i don't understand if it has been resolved with added feature or not.

I attemped to build a custom library including only a subset of services for use client-side on browsers.
I made a new directory in the root folder and included that index.js

```
'use strict';

exports.ToneAnalyzerV3 = require('ibm-watson/tone-analyzer/v3');
exports.NaturalLanguageUnderstandingV1 = require('ibm-watson/natural-language-understanding/v1');
exports.SpeechToTextV1 = require('ibm-watson/speech-to-text/v1');
exports.TextToSpeechV1 = require('ibm-watson/text-to-speech/v1');
exports.Authenticator = require('ibm-watson/auth');
```
That is my package dependencies from the package.json after installing browserify and tinyify.

```
{
  "name": "watson-developer-cloud-custom-bundle",
  "version": "1.0.0",
  "description": "Use the Watson JS SDK in browsers via browserify",
  "main": "index.js",
  "scripts": {
    "start": "node index.js"
  },
  "author": "",
  "license": "Apache-2.0",
  "dependencies": {
    "browserify": "^16.5.0",
    "ibm-watson": "latest",
    "tinyify": "^2.5.2"
  },
  "devDependencies": {},
  "engines": {
    "node": ">=6"
  }
}
```
I generate the new bundle like this:
```
browserify -p tinyify index.js --standalone Watson > watson-sdk.min.js
```
I can see the included prototypes of the bundle when i load it in browser.
![image](https://user-images.githubusercontent.com/29121228/71071902-22b76500-2186-11ea-8f3e-79dd61b148cc.png)

While i have been trying to instantiate a class i have come accross some examples using different techniques like the following:
```
const speechToText = new SpeechToTextV1({
  authenticator: new IamAuthenticator({ apikey: '<apikey>' }),
  url: 'https://stream.watsonplatform.net/speech-to-text/api/'
});

var speechToText = new SpeechToTextV1({
  username: 'INSERT YOUR USERNAME FOR THE SERVICE HERE',
  password: 'INSERT YOUR PASSWORD FOR THE SERVICE HERE',
  url: 'https://stream.watsonplatform.net/speech-to-text/api/'
});
```

Since i already have an API made in PHP that produces the accessToken from the credentials of my services, and because i don't want to expose the service credentials either in the client-side code or embeded into the library sdk, how can i instantiate a service using the prefetched accessToken i have? Reading on #30 about the token expiration, i have solved this by fetching a fresh token from my API if a request from the client validates as unauthenticated, or also if the endpoint is down since i have a failover support embeded with two services credentials just in case, so i handle the tokens properly through my API and client side code that fetches fresh ones, i just need to hide the credentials from eyesight and use the produced accessToken, if possible. Is there an example somwhere if this is supported?

Thats a sample of a token object I am currently using.
![image](https://user-images.githubusercontent.com/29121228/71072342-f7814580-2186-11ea-969d-098e9a5fdb67.png)

And further to my question, since i can see that axios is being used for the services POST or GET requests if not a websocket connection is used (STT/TTS), are the promises resolved inside the library synchronously or do i have to call them with an 'async' function and having to 'await' until i get back a response?

I am also eager to know if the STT accepts a mediaStream passed in as a paramter (so that i don't have to rewrite microphone access code since i have that before i start the service), and if a TSS accepts an audio object passed in as a parameter (so that i can reuse one audio object, because that technique seems to work better for mobile support).

Lastly my intention is to use a future supported sdk that i can get a subset of services into a bundle in order to use it from various browsers. I am currently using the speech javascript sdk that offers a few out of the box features like handling microphone access etc, but i really want to further the exploration of the services and be able to use custom models, train them, and have something used that will be maintained in the future in order to get new updates and reproduce my custom bundle.



**Overview**
Visual Recognition doesn't recognize IAM_APIKEY and URL from credentials.env provided by the service.
When I put the credentials.env provided by the visual recognition service to the working directory, the api returns 'Error: Missing required parameters: apikey'.

**Expected behavior**
The apikey is loaded from credentials.env.


**Actual behavior**
the api returns 'Error: Missing required parameters: apikey'.

**How to reproduce**
1. Download credentials.env to the working directory of #2 code  from Visual Recognition service.
2. Run following code with node.js

```
var VisualRecognitionV3 = require('ibm-watson/visual-recognition/v3'); 
var visualRecognition = new VisualRecognitionV3({ 
  version: '2018-05-01'
});
```
**Screenshots**
_If applicable, add screenshots to help explain your problem._

**SDK Version**
v5.2.0

**Additional information:**
The SDK only reads the variables started  from WATSON_VISION_COMBINED_.
However the values of credentials.env are started from VISUAL_RECOGNITION_.
SDK v4 reads both values. 

With the V1 API it is possible to create a Watson Assistant workspace: https://cloud.ibm.com/apidocs/assistant/assistant-v1?code=node#create-workspace

Previously, one of the parameters was **dialog_nodes**. This matched the name in the exported skills / workspaces (JSON files). Thus, it was straighforward to parse such a file and then pass it as parameter(s) to the **create** function. With the new camelCasing of parameters, this roundtrip is broken.