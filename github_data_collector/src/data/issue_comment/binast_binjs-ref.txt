
Context 0.1 specifies that we can only use 20 bits for a key. We're currently using an algorithm that always returns optimal-for-space keys and this algorithm may fail by exhausting the 20 bits while less optimal-for-space algorithms could succeed.

We should devise a backup strategy that will work in cases in which we currently fail.
This is a first step towards decompressing Huffman content.
In practice with context 0.1 we have some files which are *almost* monomorphic. But as soon as they have a single field with a different value then we have to use one bit per field for all of the fields.

If we used entropy codes like ANS we could get more efficient encodings than Huffman in these cases.
The "context 0.1" file format has one model for a given field.

Say you have a file where someone's pasted two source files together and the first half of the file uses "var" and the second half uses "let" and "const." In context 0.1 you'll end up with a model with a mixture of var, let and const and need 1-2 bits for each symbol to describe variable kinds.

If instead the file format could have multiple models and switch between them, then for the first half of the file it could have a model specifying 0-bits for "var" and then a switch and a model with 1 bit for "let" or "const", and that would be good.

There are probably fancier ideas where you switch sets of related models at once.

Brotli does something like this, if you look at RFC7932 and the Block-Switch stuff.
So sprach @dominiccooney 

> The format only shares codes between `StaticMemberAssignmentTarget.property` and `StaticMemberExpression.property`; more sharing or imputing is probably beneficial.
Quote from @dominiccooney 

> The input AST structure has a number of redundancies which the compressor does not exploit, but could exploit, for smaller file sizes. For example there is a lot of overlap between parameter lists and declared names.

**Note** Pretty sure we have tested this specific example already with a previous version of the encoding. Need to dig up the results.
Quoted from @dominiccooney 

> The presence of an `EmptyCodeTable` could suppress encoding the length for a given array, which must be zero at that point.

If I understand correctly, the idea is to use `EmptyCodeTable` for 0-length arrays, instead of a `UnitCodeTable` followed by a 32-bit length.
Quoted from @dominiccooney 

> The format only supports seeking within the decompressed stream. With better native compression, the Brotli "wrapper" would not be necessary and the format could seek instead within the compressed stream. I believe the primary problem with sizes today is the verbose encoding of code tables.

(I don't understand the last sentence)
In Context 0.1, the root is hardcoded as being `Script`. Is there anything that prevents us from using `Program` as root?