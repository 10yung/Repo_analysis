Hi, when utilising the faiss lib for KNN search, you choose `faiss.IndexFlatIP`, which has a relatively low performance compared with other indexes via faiss. Could you please share if you select `IndexFlatIP` based on _some reasons_, or it's worth encouraging to try the more time-effective index algorithms, e.g., `HNSWx`? Thanks!
第一：不管是运行那个命令都存在如下的错误：ValueError: could not convert string to float: 'encoding="utf-8"?>'
第二：仔细查看发现在脚本：MUSE-master/src/evaluation/wordsim.py", line 40, in get_word_pairs
中函数get_word_pairs(path, lower=True):中读取文件“EN_WS-353-ALL.txt”时第一行为“<?xml version="1.0" encoding="UTF-8"?>”为何在函数中float(line[2])转换为float呢？为啥把encoding="UTF-8"?>转换为浮点数呢？
Hi,

What would be the best approach to align multiple (more than 2) monolingual word embeddings into a single vector space? I come from [fastText_multilingual](https://github.com/babylonhealth/fastText_multilingual) which is unfortunately outdated. With this repo you could get 70+ words embeddings aligned into a single vector space.

I also read this article from Facebook talking about how they merged lot of different word embeddings: https://engineering.fb.com/ml-applications/under-the-hood-multilingual-embeddings/

I would like to be able to do the same, not necessary for that much of languages, but at least a dozen. Any tip?

Thanks!
as per June 25, 2019 (https://fasttext.cc/blog/2019/06/25/blog-post.html) the python module is named fastext. Since I don't want to change the fastText function, I just created an import alias
wiki.en-es.es.vec 


HELLO, it seems that do not have ‘’wiki.en-es.es.vec “ file
In the Multilingual word Embeddings section, the Arabic file is not correct. It is only ~417MB and there is no \n at the end of the file like other languages.
FYI, there are some issues downloading data with get_evaluation.sh. 

First, it's pretty easy to be rate-limited from https://dl.fbaipublicfiles.com/arrival. Furthermore, I get 'Access Denied' when trying to download the wordsim data (not rate limit)
Second, http://alt.qcri.org/semeval2017/task2/data/uploads... is down so you can't get the semeval2017 data that way.
Third, the two ways of getting data mentioned on the README aren't equivalent -- for instance, `wget https://dl.fbaipublicfiles.com/arrival/dictionaries.tar.gz` give you different dictionaries (well, more) than running get_evaluation.sh.

Not currently causing me problems because I've now figured it out, but did trip me up for a little while so thought you should know.
Hi, I have a question when I try to do unsupervised mapping between the source and target embeddings which are all **subwords**(have BPE procedures). Since the SRC and TGT embeddings are **subwords** and all the dictionaries are **words**, how can I get the evaluation dictionary that are **subwords**? And the question also exists when I try to do unsupervised mapping between subword embeddings. 
Hello, I want to mapping some languages in a single vector space just you have described in the README. But the code seems to adapt only two languages. If I want to mapping more languages (>=3), what should I do?
![image](https://user-images.githubusercontent.com/31796893/66988377-0307b000-f0f5-11e9-9ff0-e9c3772be6fc.png)

Hi!

Python module for fastText is now called 'fasttext', 
see https://fasttext.cc/blog/2019/06/25/blog-post.html 

The load_fasttext_model function should be updated.
(https://github.com/facebookresearch/MUSE/blob/master/src/utils.py#L71)