* Users can now stream data to Solr from any Spark supported datasource
* In order to preserve exactly once semantics, users should set `id` (primary field) field before streaming the data out to Solr
CODE:
```
val collection="PM_signals"
val zkhost="zk:2181" // Replace this with your cluster zkhost
val opts = Map("collection" -> collection, "zkhost" -> zkhost, "sample_seed" -> "5150")
val df = spark.read.format("solr").options(opts).load
df.show
```
ERROR:

```
org.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException: Error from server at http://172.18.0.10:8983/solr/PM_signals: sort param field can't be found: random_5150
  at org.apache.solr.client.solrj.impl.HttpSolrClient.executeMethod(HttpSolrClient.java:656)
  at org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:262)
  at org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:245)
  at org.apache.solr.client.solrj.impl.LBSolrClient.doRequest(LBSolrClient.java:368)
  at org.apache.solr.client.solrj.impl.LBSolrClient.request(LBSolrClient.java:296)
  at org.apache.solr.client.solrj.impl.BaseCloudSolrClient.sendRequest(BaseCloudSolrClient.java:1128)
  at org.apache.solr.client.solrj.impl.BaseCloudSolrClient.requestWithRetryOnStaleState(BaseCloudSolrClient.java:897)
  at org.apache.solr.client.solrj.impl.BaseCloudSolrClient.request(BaseCloudSolrClient.java:829)
  at org.apache.solr.client.solrj.SolrRequest.process(SolrRequest.java:211)
  at org.apache.solr.client.solrj.SolrClient.query(SolrClient.java:1019)
  at com.lucidworks.spark.util.SolrQuerySupport$.getNumDocsFromSolr(SolrQuerySupport.scala:560)
  at com.lucidworks.spark.rdd.SolrRDD.calculateSplitsPerShard(SolrRDD.scala:92)
  at com.lucidworks.spark.rdd.SelectSolrRDD$$anonfun$1.apply$mcI$sp(SelectSolrRDD.scala:100)```
Can I send a JSON string to Solr ? Without have to deserialize to DataFrame.

Thank you
@kiranchitturi 
I see the #118 . Why not like this
```
val options = Map(
  "collection" -> "{solr_collection_name}",
  "zkhost" -> "{zk_connect_string}",
  "httpBasicAuthUser" -> "{httpBasicAuthUser}",
  "httpBasicAuthPassword" -> "{httpBasicAuthPassword}",
)
val df = spark.read.format("solr")
  .options(options)
  .load
```
This seems to be more friendly to multi-permission support. 
It can be controlled by spark options.




This patch introduces a new configuration : `add_new_files`  . This makes non-madatory the creation of new fields from spark, and let solr dynamicaly create the fields. This have been discussed #246  
Changed the httpclient 3.1 class imports to httpclient 4.x class imports. This
would affect anyone trying to exclude spark-solr's hadoop dependencies and include
newer hadoop dependencies for other purposes. This didn't seem to break anything -- the 
tests passed, the project compiled, and our spark jobs worked! -- but we don't utilize any of 
the hadoop/hdfs features of the spark-solr connector. However, this PR should be harmless.
Reference issue here: lucidworks/spark-solr#272
During our work to upgrade a project to a newer version of Hadoop (3.2), we discovered that the spark-solr connector imports a couple of HttpClient classes (`NoHttpResponseException` and `ConnectTimeoutException`) from the HttpClient 3.1 package (included by hadoop 2.7) instead of the version 4.x equivalent classes.

This was discovered during a Spark job test run where we index our data into Solr -- one of the hosts of our SolrCloud went down, and a `NoHttpResponseException` was supposed to be thrown. However, the class couldn't be found, because we had excluded all the hadoop dependencies at runtime, which meant HttpClient 3.1 was not added as a dependency.

Referencing HttpClient 3.1 classes would prevent the spark-solr connector from working with Hadoop version 2.8 or later, but changing the references now shouldn't affect its integration with hadoop 2.7.
csvDF.write.format("solr").options(options).mode(org.apache.spark.sql.SaveMode.Overwrite).save
java.util.NoSuchElementException: None.get
  at scala.None$.get(Option.scala:347)
  at scala.None$.get(Option.scala:345)
  at com.lucidworks.spark.SolrRelation.insert(SolrRelation.scala:634)
  at solr.DefaultSource.createRelation(DefaultSource.scala:27)
  at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)
  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)
  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)
  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)
  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)
  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)
  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)
  ... 49 elided

Hi Kiran,
i am testing Hdp3 migration, in that when i try to push the data from hive dataframe  and job  is getting hang , running more  time and no response even solr connected with zk successfully
here is my code
from pyspark.sql import SparkSession
from pyspark_llap.sql.session import HiveWarehouseSession
spark=SparkSession.builder.appName("exportProceedingDataToSolr").enableHiveSupport().getOrCreate()
hive = HiveWarehouseSession.session(spark).build()
DF = hive.executeQuery("select * from table")
DF.write.format("solr").option("zkhost",zk_host).option("collection",solr_proceedings_collection).option("batch_size", "10000").option("commit_within", "5000").mode("append").save()
print('-- Cases export to Solr successfully completed--')
FYI:
i am doubting on new HDP 3 since spark  and hive  catalog is separated and earlier we can use both and  could  able to push  the data from the data frame  whether hive or spark.
please guide  me how to proceed further, I appreciate your reply in this regards.

Thanks.
Raj
Hi team.  I’m just wondering if there are any missing dependencies on the classpath for this project as I'm running into a class not found exception when connecting to a secure solr cloud instance (basic auth, ssl).  Everything is working as expected on a non-secure solr cloud instance.

The process looks pretty straightforward according to the doco so I’m wondering if I’m missing anything obvious or if I need to bring any extra classes to the classpath when using this project?  

Any advice would be greatly appreciated.

Thanks,

Dwane  

Environments tried
7.6 and 8.1.1 solr cloud
SSL, Basic Auth Plugin, Rules Based Authorisation Plugin enabled
Spark v 2.4.3
Spark-Solr build spark-solr-3.7.0-20190619.153847-16-shaded.jar

[ClassNotFoundSparkSolr.txt](https://github.com/lucidworks/spark-solr/files/3394996/ClassNotFoundSparkSolr.txt)

```
./spark-2.4.3-bin-hado./spark-2.4.3-bin-hadoop2.7/bin/spark-shell --master local[*] --jars spark-solr-3.7.0-20190619.153847-16-shaded.jar --conf 'spark.driver.extraJavaOptions=-Dbasicauth=solr:SolrRocks'

val options = Map(
        "collection" -> "My_Collection",
        "zkhost" -> "zkn1:2181,zkn2:2181,zkn3:2181/solr/SPARKTEST"
      )

val df = spark.read.format("solr").options(options).load 

