My pytorch version is 1.4.0+cpu and tensorflow version is 2.0.0-dev20191002.
/torch/serialization.py,line 289,in_check_seekable
‚ÄòNoneType‚Äô object has no attribute 'seek'
You can only torch.load from a file that is seekable.Please pre_load the data into a buffer like io.BytesIO and try to load from it instead.
But how should I do to sovle the quetions?
Another question.
OSError:Unable to load weights from pytorch checkpoint file.If you tried to load a PyTorch model from a TF 2.0 checkpoint,please set from_tf=true.
## ‚ùì Questions & Help

<!-- A clear and concise description of the question. -->
 Hello ! I'm wondering what is the structure of the model saved after fine-tuning. For example, after the sequence classification fine-tuning , how to show the layers information of newly-formed model ? Is new model's sentence vector different from the original one which is extracted from pre-trained model ? 
## ‚ùì Questions & Help

<!-- A clear and concise description of the question. -->
How to start a server and client to get feature vectorsÔºåor Which part of the code should I study in https://github.com/huggingface/transformers.git.



I'm trying to run TFBertForTokenClassification with tensorflow_datasets.load('glue/sst2'):

`import tensorflow as tf
import tensorflow_datasets
from transformers import *

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = TFBertForTokenClassification.from_pretrained('bert-base-uncased')
data = tensorflow_datasets.load('glue/sst2')

train_dataset = glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, task='sst-2')
valid_dataset = glue_convert_examples_to_features(data['validation'], tokenizer, max_length=128, task='sst-2')
train_dataset = train_dataset.shuffle(100).batch(32).repeat(2)
valid_dataset = valid_dataset.batch(64)

optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

history = model.fit(train_dataset, epochs=2, steps_per_epoch=115,
                    validation_data=valid_dataset, validation_steps=7)
`


while model.fit I get this error:

Train for 115 steps, validate for 7 steps
Epoch 1/2
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_token_classification_1/bert/pooler/dense/kernel:0', 'tf_bert_for_token_classification_1/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_token_classification_1/bert/pooler/dense/kernel:0', 'tf_bert_for_token_classification_1/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_token_classification_1/bert/pooler/dense/kernel:0', 'tf_bert_for_token_classification_1/bert/pooler/dense/bias:0'] when minimizing the loss.
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_token_classification_1/bert/pooler/dense/kernel:0', 'tf_bert_for_token_classification_1/bert/pooler/dense/bias:0'] when minimizing the loss.
  1/115 [..............................] - ETA: 25:00
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-15-f52b3b390355> in <module>()
      1 history = model.fit(train_dataset, epochs=2, steps_per_epoch=115,
----> 2                     validation_data=valid_dataset, validation_steps=7)

11 frames
/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  assertion failed: [Condition x == y did not hold element-wise:] [x (loss/output_1_loss/SparseSoftmaxCrossEntropyWithLogits/Shape_1:0) = ] [32 1] [y (loss/output_1_loss/SparseSoftmaxCrossEntropyWithLogits/strided_slice:0) = ] [32 128]
	 [[node loss/output_1_loss/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert (defined at <ipython-input-15-f52b3b390355>:2) ]]
	 [[Reshape_824/_584]]
  (1) Invalid argument:  assertion failed: [Condition x == y did not hold element-wise:] [x (loss/output_1_loss/SparseSoftmaxCrossEntropyWithLogits/Shape_1:0) = ] [32 1] [y (loss/output_1_loss/SparseSoftmaxCrossEntropyWithLogits/strided_slice:0) = ] [32 128]
	 [[node loss/output_1_loss/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert (defined at <ipython-input-15-f52b3b390355>:2) ]]
0 successful operations.
0 derived errors ignored. [Op:__inference_distributed_function_477134]

Function call stack:
distributed_function -> distributed_function

## üêõ Bug

<!-- Important information -->

Model I am using: Bert

Language I am using the model on (English)

The problem arise when using:
* [x] the official example scripts: (give details):
I have a venv running with TF2.0 and transformers, and I am running mrpc dataset with BERT. Here's the code: 

train_dataset = glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, task='mrpc')
valid_dataset = glue_convert_examples_to_features(data['validation'], tokenizer, max_length=128, task='mrpc')
train_dataset = train_dataset.shuffle(100).batch(32).repeat(2)
valid_dataset = valid_dataset.batch(64)


The tasks I am working on is:
* [ ] an official GLUE/SQUaD task: (give the name)


## To Reproduce

Steps to reproduce the behavior:

I am using the official code on hugging face for BERT and  sequence calssification and it is not working... 


tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
model = TFBertForSequenceClassification.from_pretrained('bert-base-cased', force_download=True)
data = tensorflow_datasets.load('glue/mrpc')


<!-- If you have a code sample, error messages, stack traces, please provide it here as well. -->
Error message: 

File "hugf-bert.py", line 20, in <module>
    train_dataset = glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, task='mrpc')
  File "/Users/ns5kn/Documents/insight/transformers/src/transformers/data/processors/glue.py", line 84, in glue_convert_examples_to_features
    logger.info("Writing example %d/%d" % (ex_index, len(examples)))
TypeError: object of type '_OptionsDataset' has no len()

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

* OS: mac 10.14
* Python version: 3.6
* Tensorflow version: 2.0
* PyTorch Transformers version (or branch):
* Using GPU ?
* Distributed or parallel setup ?
* Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->

Just an option to save the model to other than the working directory.

Default functionality hasn't changed.
## üêõ Bug

<!-- Important information -->

Model I am using (Bert, XLNet....): GPT2

Language I am using the model on (English, Chinese....): English

The problem arise when using:
tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')

The tasks I am working on is:

* [ ] my own task or dataset:

## To Reproduce

Steps to reproduce the behavior:

1.tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')
2.
3.

<!-- If you have a code sample, error messages, stack traces, please provide it here as well. --><ipython-input-97-b0a0cde738fe> in <module>
----> 1 tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')

/media/bahram/New Volume/Projects/Python_LM/transformers/tokenization_utils.py in from_pretrained(cls, *inputs, **kwargs)
    307 
    308         """
--> 309         return cls._from_pretrained(*inputs, **kwargs)
    310 
    311     @classmethod

/media/bahram/New Volume/Projects/Python_LM/transformers/tokenization_utils.py in _from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)
    459         # Instantiate tokenizer.
    460         try:
--> 461             tokenizer = cls(*init_inputs, **init_kwargs)
    462         except OSError:
    463             raise OSError(

/media/bahram/New Volume/Projects/Python_LM/transformers/tokenization_gpt2.py in __init__(self, vocab_file, merges_file, unk_token, bos_token, eos_token, pad_to_max_length, add_prefix_space, max_length, stride, truncation_strategy, **kwargs)

AttributeError: 'Tokenizer' object has no attribute 'with_pre_tokenizer'

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->
AttributeError: 'Tokenizer' object has no attribute 'with_pre_tokenizer'
## Environment

* OS: Ubuntu 18
* Python version: 3.7
* PyTorch version:
* PyTorch Transformers version (or branch):
* Using GPU yes
* Distributed or parallel setup No
* Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->

## ‚ùì Questions & Help

<!-- A clear and concise description of the question. -->
I put the wiki.train.raw and the wiki.test.raw in /dataset,
then run the command:
python run_lm_finetuning.py     --output_dir=output     --model_type=roberta     --model_name_or_path=roberta-base     --do_train     --train_data_file=/dataset/wiki.train.raw     --do_eval     --eval_data_file=/dataset/wiki.test.raw     --mlm

errors:
Traceback (most recent call last):
  File "run_lm_finetuning.py", line 717, in <module>
    main()
  File "run_lm_finetuning.py", line 662, in main
    train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)
  File "run_lm_finetuning.py", line 127, in load_and_cache_examples
    block_size=args.block_size,
  File "run_lm_finetuning.py", line 86, in __init__
    assert os.path.isfile(file_path)
AssertionError 

then I find the code in run_lm_finetuning.py, 
I don't know change file_path in def __init__:
def __init__(self, tokenizer, args, file_path="train", block_size=512):
I hope you can help me out.

This is an explanation and a proposed fix for #2559 
The code set `pad_token_label_id = 0`, and increase the total number of labels `num_labels = len(labels) + 1`, but made no change to the label list. Thus the first label in label list has the same index as pad_token_label_id.

Following instructions in README take GermEval 2014 as an example, for one sentence in test dataset the token `Aachen` is labeled as `B-LOC` (`B-LOC` is the first label in label list), yet because of the collision with pad_token_label_id, both pad tokens and `Aachen` are encoded as 0:
![image](https://user-images.githubusercontent.com/1331543/72657322-bb90fa00-3957-11ea-980d-33ac12e54545.png)
And the test_predictions.txt is also off by one:
```
1951 I-PERpart
bis I-PERpart
1953 I-PERpart
wurde I-PERpart
...
```

The fix adds a placeholder label `[PAD]` at position 0 when loading the datasets and all labels positions are shifted by 1. The resulting encoding for the same sample sentence:
![image](https://user-images.githubusercontent.com/1331543/72657627-5e974300-395b-11ea-905e-4a3028f4b5c4.png)

And the test_predictions.txt thus has correct index:
```
1951 O
bis O
1953 O
wurde O
...
```
