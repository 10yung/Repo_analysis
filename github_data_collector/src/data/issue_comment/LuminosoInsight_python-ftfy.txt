`ftfy.fix_text('&#65534;', fix_entities=True)` returns an empty string. I would have expected U+FFFE, which is otherwise left intact.

I'm not actually sure what the correct unescaping behaviour is here according to the standards, but `'\ufffe'.encode('ascii', 'xmlcharrefreplace').decode('ascii')` generates it (which is how I ran into the issue - we use that method to generate strings with entities in our unit tests, and Hypothesis happened to generate U+FFFE).

edit: I wrote U+FFFD by mistake when I meant to write U+FFFE. Fixed now.
```
>>> import ftfy
>>> s = "Ä×èÈÄÄî▒è¤ô_üiâAâjâüâpâXüj_10òb.png"
>>> ftfy.ftfy(s)
'Ä×èÈÄÄî▒è¤ô_üiâAâjâüâpâXüj_10òb.png'
>>> s.encode('cp850').decode('shift-jis')
'時間試験観点（アニメパス）_10秒.png'
```

Possible?   Thanks
I have problems with mojibake of the letter ö in König. 
`U+00F6  ö       [Ll] LATIN SMALL LETTER O WITH DIAERESIS`

These are some examples that might be mojibaked, not sure about the first 2, just added them in case they are solvable. The first 2 look the same in the browser but different in unicode.
K�nig or K���nig
`U+FFFD  �       [So] REPLACEMENT CHARACTER`

Knig (This is mojibake-solved by ftfy.fix_text() as a `U+0022  "       [Po] QUOTATION MARK`)
`U+0094  \x94    [Cc] <unknown>`

KÃ¶nig
`U+00C3  Ã       [Lu] LATIN CAPITAL LETTER A WITH TILDE`
`U+00B6  ¶       [Po] PILCROW SIGN`

K√∂nig
`U+221A  √       [Sm] SQUARE ROOT`
`U+2202  ∂       [Sm] PARTIAL DIFFERENTIAL`

Are any of these solvable?
I'm looking into using ftfy to help upstream data sources clean up their data. a common format is CSV files. These data sources often manage to mix up encoding in different columns in the same row (maybe they're from different databases or web forms).

ftfy will be even more useful for me if it were possible to fix mixed encodings on the same line, or alternatively to add an argument to control what is considered a segment.

Obviously I could just make some code to use ftfy functions in CSV fields myself to get the desired results, but ftfy is a well documented easy to use tool so I'd really like offload the analysis to the bad data providers themselves.

If it's agreed this could be a feature, I might be able to spend some time on it.

It would be great if ftfy could fix cases like this:

``` python
>>> s = u'¼Ò¸®¿¤ - »ç¶ûÇÏ´Â ÀÚ¿©'
>>> print s.encode('latin1').decode('euc_kr')
소리엘 - 사랑하는 자여
```

but it doesn't:

``` python
>>> print ftfy.fix_text_segment(s)
1⁄4Ò ̧®¿¤ - »ç¶ûÇÏ ́Â ÀÚ¿©
```

Source: http://media.yohan.net/7.html

There is apparently a fair amount of Spanish text out there that contains a mix-up between Windows-1252 and MacRoman before being encoded in UTF-8.

Because Latin-1 for Windows-1252 is the only single-byte mixup we detect, we assume that's what happened, and get text that looks like: "PrevŽn diputados inaugurar periodo de sesiones con c—digo penal".

This is not a false positive, because the encoding is in fact incorrect (it's actually got the UTF-8 encoding of the wrong characters in it), and ftfy is trying to fix it. It's in fact using the same fix that any web browser would use. However, the resulting text makes no sense, because it's not the correct fix.

This mixup is apparently common enough that it would be worth fixing as another special case.
