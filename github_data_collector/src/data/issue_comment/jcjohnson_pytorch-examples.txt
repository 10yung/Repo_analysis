已经解决了win10下的训练自己的数据问题，加Q群857449786 注明pytorch examples 共同研究
https://github.com/jcjohnson/pytorch-examples/blob/73a662bbe9fce257ec8c3cdbb44a8112a42d39f3/autograd/tf_two_layer_net.py#L50
Super Minor Typo error 
"in TensorFlow the the act of updating the value of the weights is part of" 
should be 
"in TensorFlow the act of updating the value of the weights is part of"

And Thank You @jcjohnson for this super helpful write-up!!
Hey @jcjohnson, first of all thank you for these, eternally thankful...
The issue-
4th paragraph Pytorch:Autograd -
"for example we usually don't want to backpropagate through the weight update steps when training a neural network"

This should be done when the network is being evaluated too right? At that time we don't want extra memory to be used(to keep track) if we aren't going to update the weights
`# dtype = torch.device("cuda:0") # Uncomment this to run on GPU`
should be 
`# device = torch.device("cuda:0") # Uncomment this to run on GPU`
In the first example:

> Backprop to compute gradients of w1 and w2 with respect to loss

should be

> Backprop to compute gradient of loss with respect to w1 and w2

Thank you for the nice write-up!
https://github.com/jcjohnson/pytorch-examples/blob/0f1b88a38e1fd761dd4ee22398d0efec7a3c7faf/autograd/two_layer_net_custom_function.py#L30

self.saved_tensors in custom backward function doesn't work hence cannot compute backward using the subclass method