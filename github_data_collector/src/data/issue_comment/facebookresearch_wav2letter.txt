Add docker image build automation with github actions
terminate called after throwing an instance of 'std::invalid_argument'
  what():  mismatched # of elements in moddims
*** Aborted at 1579265403 (unix time) try "date -d @1579265403" if you are using GNU date ***
PC: @     0x7f45cbe95428 gsignal
*** SIGABRT (@0x1ec6000086c0) received by PID 34496 (TID 0x7f462281d7c0) from PID 34496; stack trace: ***
    @     0x7f46216f6390 (unknown)
    @     0x7f45cbe95428 gsignal
    @     0x7f45cbe9702a abort
    @     0x7f45cc9fa84d __gnu_cxx::__verbose_terminate_handler()
    @     0x7f45cc9f86b6 (unknown)
    @     0x7f45cc9f8701 std::terminate()
    @     0x7f45cc9f8919 __cxa_throw
    @           0x6ad552 fl::moddims()
    @           0x6e6f66 fl::View::forward()
    @           0x6dedff fl::UnaryModule::forward()
    @           0x6ceb42 fl::Sequential::forward()
    @           0x492fdf _ZZ4mainENKUlSt10shared_ptrIN2fl6ModuleEES_IN3w2l17SequenceCriterionEES_INS3_10W2lDatasetEES_INS0_19FirstOrderOptimizerEES9_ddbiE3_clES2_S5_S7_S9_S9_ddbi.constprop.12666
    @           0x41bf80 main
    @     0x7f45cbe80830 __libc_start_main
    @           0x48de89 _start
    @                0x0 (unknown)
Makefile:2: recipe for target 'train' failed
make: *** [train] Aborted (core dumped)

How to do inference/decode on audio/speech containing custom names of people or cities, etc?
Is this already supported by wav2letter? Do I need to retrain the models with audio containing the names?
Hi there, do you have any tutorial about training without lexicon? since in some language are very difficult to build one
So I have 8 GTX 1080 GPUs , and trying to use them all for training ConvGLU. The first, I tring using only one GPU (using mpirun -n 1) the runtime for one epoch is  `runtime: 01:31:27`  but when I run with 6 GPUs runtime perepoch is about `runtime: 01:13:52` while the nvidia-smi command says I using 100% of 6 GPU Memories.
Am I doing it wrong? why there is no big difference from running with one GPU or more? Is there any better way to accelerate the training time?
Hi, 

I've tried running docker (cpu-latest), but I'm getting error. I followed https://github.com/facebookresearch/wav2letter/wiki/Building-Running-with-Docker


**Here are the steps:**

1. Getting docker image
`docker run --rm -itd --ipc=host --name w2l wav2letter/wav2letter:cpu-latest`

2. Accessing container
`docker exec -it w2l bash`

3. Running Tests
`cd /root/wav2letter/build && make test`

**Here is the error that I'm getting:**

```
90% tests passed, 2 tests failed out of 21

Total Test time (real) =  15.82 sec

The following tests FAILED:
          4 - Seq2SeqTest (Failed)
         20 - W2lModuleTest (Failed)
Errors while running CTest
Makefile:72: recipe for target 'test' failed
make: *** [test] Error 8
```

Also, I tried following inference example per wiki:

For this, I followed: https://github.com/facebookresearch/wav2letter/wiki/Inference-Run-Examples

1. Example ASR
`make simple_streaming_asr_example`

**Here is the error that I'm getting:**

```
make: *** No rule to make target 'simple_streaming_asr_example'.  Stop.
```

I'm on windows. 

When I try to build the docker images locally - I'm running into issues with the following:

```
[  6%] Building CXX object tests/CMakeFiles/AllReduceTest.dir/__/flashlight/autograd/Functions.cpp.o
c++: internal compiler error: Killed (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-5/README.Bugs> for instructions.
make[2]: *** [tests/CMakeFiles/ContribModuleTest.dir/contrib/modules/ContribModuleTest.cpp.o] Error 4
tests/CMakeFiles/ContribModuleTest.dir/build.make:62: recipe for target 'tests/CMakeFiles/ContribModuleTest.dir/contrib/modules/ContribModuleTest.cpp.o' failed
make[1]: *** [tests/CMakeFiles/ContribModuleTest.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
CMakeFiles/Makefile2:1122: recipe for target 'tests/CMakeFiles/ContribModuleTest.dir/all' failed
```


i try to install wav2letter blinding python. but i got an error. how can i solve it ? thanks .
![2020-01-14 18-19-53屏幕截图](https://user-images.githubusercontent.com/19542945/72335960-ce5ac300-36fa-11ea-9b88-5077bac971fb.png)
 
I am trying to build wav2letter. but i found it is very hard to build dependency of wav2letter.  do you have easy  and detailed tutorials ? thanks 
I use seq2seq decoder for Spanish model, but the output transcriptions are not completed. The decoder is not able to recognize the words at the end of some sentences.

I do not know if it is related to:
- An unbalanced dataset in terms of sentence duration.
- The silence model. Some audio files containing only one word between long pauses does not obtain any output.

The same architecture works well using LibriSpeech for English models. I read the same issue but it had not been solved.

Thanks in advance.
I believe it would be to wav2letter's advantage to have a simple sample that can show it at work. Something like the proverbial hello world program. 

Because wav2letter is so complex and because ASR in general is complex, having a baseline that people can refer to when trying to build a system that is using gigabytes of data, several thousands transcription and audio files would save much time and many issues. I believe deepsearch has such a one-audio file that can be used although I have not tried it.

A minimal system that shows how to create a minimum n-gram (kenlm fails if your transcription sample  does not have too many lines, although there might be a flag to override the behavior) would avoid many issues created to inquire for the helloworld-like example.