Hi guys,
I'm trying to build a DeepLearning network which contains a subtraction operation between two layers, and I can't find a way to do that while MATLAB has just an addition operation (additionLayer)?
for more clarification, the equation below discribes the step that I should do:
C = X - AvgPooling(X)
 any suggestions guys?

the rbm gets initialized by a weight matrix of 2 X 25 and bias matrices also get initialized similarly. Could u let me know where they get initialized so that I can change the dimensions for my dataset?

"mlcnn.m" has a method named: "updateParams".
Unfortunately in this version, it does not update the output layer's weights, so the whole network is like a car with a working engine but without a mechanism to transfer engine's power to the tires.
I have a suggestion to make it work, to do so I have changed the body of "updateParams" method to:

function self = updateParams(self)
%net = updateParams()
%--------------------------------------------------------------------------
%Update network parameters based on states of netowrk gradient, perform
%regularization such as weight decay and weight rescaling
%--------------------------------------------------------------------------
    wPenalty = 0;
    for lL = 2:self.nLayers % Changed from: for lL = 2:self.nLayers-1 by Masih Azad
        switch self.layers{lL}.type
        % CURRENTLY, ONLY UPDATE FILTERS AND FM BIASES
        % PERHAPS, IN THE FUTURE, WE'LL BE FANCY, AND DO FANCY UPDATES
        case {'conv'} % Changed from: case {'conv','output'} by Masih Azad
            lRate = self.layers{lL}.lRate;
            for jM = 1:self.layers{lL}.nFM
                % UPDATE FEATURE BIASES
                self.layers{lL}.b(jM) = self.layers{lL}.b(jM) - ...
                                    lRate*self.layers{lL}.db(jM);

```
            % UPDATE FILTERS
            for iM = 1:self.layers{lL-1}.nFM
                if self.wPenalty > 0 % L2 REGULARIZATION
                    wPenalty = self.layers{lL}.filter(:,:,iM,jM)*self.wPenalty;
                elseif self.wPenalty < 0 % L1-REGULARIZATION (SUB GRADIENTS)
                    wPenalty = sign(self.layers{lL}.filter(:,:,iM,jM))*abs(self.wPenalty);
                end
                self.layers{lL}.filter(:,:,iM,jM) = ...
                self.layers{lL}.filter(:,:,iM,jM) - ...
                lRate*(self.layers{lL}.dFilter(:,:,iM,jM)+wPenalty);
            end
        end
        %% Added by Masih Azad
        case {'output'}
            lRate = self.layers{lL}.lRate;
            self.layers{lL}.b = self.layers{lL}.b - ...
                                lRate*self.layers{lL}.db;

            if self.wPenalty > 0 % L2 REGULARIZATION
                wPenalty = self.layers{lL}.W*self.wPenalty;
            elseif self.wPenalty < 0 % L1-REGULARIZATION (SUB GRADIENTS)
                wPenalty = sign(self.layers{lL}.W)*abs(self.wPenalty);
            end                
            self.layers{lL}.W = self.layers{lL}.W - lRate*(self.layers{lL}.dW+wPenalty);
        %%
    end
end
```

end

Why the values of property "mlcnn.netOut" does not match with values of "testLablels". After executiong demoMLCNN.m. 
Even elements of mlcnn.netOut are between 0 and 1 but not exactly 0 and 1.

Hi I was curious if dbn can call crbm instead of rbm to build a convolutional dbn as in 2009 ICML paper of Lee. 
