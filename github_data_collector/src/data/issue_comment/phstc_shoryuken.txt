Similar to #591, the `AutoExtendVisibility` middleware is unaware of the update of visibility timeout. Here is an simple example to illustrate:

```ruby
class NoopWorker < ApplicationWorker
  shoryuken_options queue: Settings.bg.queues.low,
                    auto_visibility_timeout: true

  def perform(msg_handler, _params)
    # The queue lelve visibility timeout is set to 30 seconds by default
    msg_handler.visibility_timeout = 2.minutes

    sleep(3.minutes)

    msg_handler.delete
  end
end
```

```
2020-01-16T05:33:27Z TID-ovh301lqc INFO: Starting
2020-01-16T05:33:27Z TID-ovh4qp1x4 NoopWorker/dev_markhuang_low/001f825b-8d80-4614-bebe-3a0d2ffdea2f INFO: started at 2020-01-15 21:33:27 -0800
2020-01-16T05:33:52Z TID-ovh4qp2bk WARN: Extending message dev_markhuang_low/001f825b-8d80-4614-bebe-3a0d2ffdea2f visibility timeout by 30s
2020-01-16T05:34:17Z TID-ovh4rqmww WARN: Extending message dev_markhuang_low/001f825b-8d80-4614-bebe-3a0d2ffdea2f visibility timeout by 30s
2020-01-16T05:34:43Z TID-ovh4qp2bk WARN: Extending message dev_markhuang_low/001f825b-8d80-4614-bebe-3a0d2ffdea2f visibility timeout by 30s
2020-01-16T05:35:08Z TID-ovh4qp2bk WARN: Extending message dev_markhuang_low/001f825b-8d80-4614-bebe-3a0d2ffdea2f visibility timeout by 30s
2020-01-16T05:35:33Z TID-ovh4qp2bk WARN: Extending message dev_markhuang_low/001f825b-8d80-4614-bebe-3a0d2ffdea2f visibility timeout by 30s
2020-01-16T05:35:58Z TID-ovh4rqmww WARN: Extending message dev_markhuang_low/001f825b-8d80-4614-bebe-3a0d2ffdea2f visibility timeout by 30s
2020-01-16T05:36:23Z TID-ovh4rqmww WARN: Extending message dev_markhuang_low/001f825b-8d80-4614-bebe-3a0d2ffdea2f visibility timeout by 30s
2020-01-16T05:36:28Z TID-ovh4qp1x4 NoopWorker/dev_markhuang_low/001f825b-8d80-4614-bebe-3a0d2ffdea2f WARN: exceeded the queue visibility timeout by 150443.14 ms
2020-01-16T05:36:28Z TID-ovh4qp1x4 NoopWorker/dev_markhuang_low/001f825b-8d80-4614-bebe-3a0d2ffdea2f INFO: completed in: 180443.14 ms
```

Ideally, the auto extension of message visibility should be triggered ~120 seconds after manually bumping the visibility timeout. However, as shown in the log, it's unnecessarily triggered multiple times according to the default queue visibility timeout regardless the fact that it's already been changed for this message.

Wonder if you think this is identified as "unexpected" behavior? Also, unfortunately, I don't have an idea on top of my head that could fix this, but still wanna bring it up here and maybe discuss about it!
This PR tries to address #591, it

- Adds the ability to query `visibility_timeout` from message level
- Changes the logic inside `Timing` middleware to print "exceeds visibility timeout" warning according to message level `visibility_timeout`
Happy new year!

According to [AWS documenation][doc], it's possible to set the `VisibilityTimeout` for a specific message regardless of the `VisibilityTimeout` value in the queue level. This is useful since in our application, messages in one queue can be processed by different shoyuken workers, which takes up very different amount of time. We want to set the visibility timout to a bigger value for those messages that takes longer time, so that it won't get processed multiple times.

Fortunately, we were able to use the `Message#visibility_timeout=` setter to change the message's visibility timeout (from 60 seconds to 2 hours in this case). However, shoryuken would log warnings like this

```
started at 2020-01-04 08:01:09 +0000
...
exceeded the queue visibility timeout by 705623.076762 ms
completed in: 765623.076762 ms
```

...which is misleading.

I think this is caused by shoryuken always uses the queue level visibility timeout, and not respecting the visibility timeout in the message level

https://github.com/phstc/shoryuken/blob/af6ba758767aa7e541cae0ad3e4b146ef8eb7792/lib/shoryuken/middleware/server/timing.rb#L16

I'm happy to open a PR if this is a legit issue or otherwise I might be missing something here?

Thanks!

[doc]: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ChangeMessageVisibility.html
We just moved a large workload over to a pool of Shoryuken workers from Resque and have been noticing a memory leak (the same workload did not exhibit the same memory growth behavior in Resque).

This is our first use of Shoryuken, so we don't have another version to compare it to.

From Gemfile:
```
  gem 'aws-sdk-s3', '~> 1.8'
  gem 'aws-sdk-sqs', '~> 1.8'
  gem 'shoryuken', '~> 3.0'
```

From Gemfile.lock: 
```
    aws-sdk-core (3.48.3)
      aws-eventstream (~> 1.0, >= 1.0.2)
      aws-partitions (~> 1.0)
      aws-sigv4 (~> 1.1)
      jmespath (~> 1.0)
    aws-sdk-s3 (1.22.0)
      aws-sdk-core (~> 3, >= 3.26.0)
      aws-sdk-kms (~> 1)
      aws-sigv4 (~> 1.0)
    aws-sdk-sqs (1.9.0)
      aws-sdk-core (~> 3, >= 3.26.0)
      aws-sigv4 (~> 1.0)
    shoryuken (3.3.1)
      aws-sdk-core (>= 2)
      concurrent-ruby
      thor
```
config:
```
aws:
  receive_message:
    attribute_names:
    - ApproximateReceiveCount
    - SentTimestamp
concurrency: 1
```
![Screen Shot 2019-04-02 at 11 15 27 AM](https://user-images.githubusercontent.com/13109210/55426218-15474580-5539-11e9-8db2-e2c65eda8e85.png)

The above chart shows a per container memory usage over time (overlaid is an instance with the min, max, and the mean memory usage). All of the containers are cycled, and then you can see that they steadily grow over time. There are 10 processes in one container. Each process grows to use about 2 GB of memory before the container OOMs. I spent some time looking at heap dumps and it shows that there is a significant growth of the following items over time: 

`/usr/local/bundle/gems/aws-sdk-core-3.34.0/lib/seahorse/util.rb:9:STRING`
`/usr/local/bundle/gems/aws-sdk-core-3.34.0/lib/seahorse/client/handler_list_entry.rb:71:HASH`

Updating the `aws-sdk-sqs` version to `1.13` did not fix any issues, those two items still show up as culprits for leakage.

Any suggestions or further questions would be greatly appreciated, thanks!
Would it be possible to only log a single event for exceptions?

[Currently](https://github.com/phstc/shoryuken/blob/master/lib/shoryuken/processor.rb#L25-L26) there is one log event being created for the exception message and one log event for the exception backtrace.

This can sometimes make it challenging to debug in a high volume/noisy application as additional tagging is needed to identify the two events as related or they need to be manually identified.

ex: instead of something like: 
```
logger.error { "Processor failed: #{ex.message}" }
logger.error { ex.backtrace.join("\n") } unless ex.backtrace.nil?
```
simplify to:
```
logger.error { "Processor Failed: #{ex.message}. Backtrace: #{ex.backtrace.join("\n")}" }
``` 

When running Workers using `InlineExecutor`, `perform_async` does not run the server middleware.

Given:

```ruby
require 'spec_helper'
require 'shoryuken'

RSpec.describe 'middleware for InlineExecutor' do
  before do
    # Enable inline execution
    Shoryuken.worker_executor = Shoryuken::Worker::InlineExecutor

    # Add middleware
    Shoryuken.configure_server do |config|
      config.server_middleware do |chain|
        chain.add middleware_class
      end
    end
  end

  let(:middleware_class) do
    stub_const('TestMiddleware', Class.new do
      def call(worker_instance, queue, sqs_msg, body)
        yield
      end
    end)
  end

  context 'when a Shoryuken::Worker class' do
    subject(:worker_class) do
      stub_const('TestWorker', Class.new do
        include Shoryuken::Worker

        shoryuken_options queue: 'default'

        def perform(sqs_msg, body); end
      end)
    end

    describe '#perform_async' do
      subject(:perform_async) { worker_class.perform_async(body) }
      let(:body) { 'test' }

      before do
        expect_any_instance_of(worker_class).to receive(:perform)
          .and_call_original

        expect_any_instance_of(middleware_class).to receive(:call)
          .and_call_original
      end

      it do
        expect { perform_async }.to_not raise_error
      end
    end
  end
end
```

I expect the test to pass, but it fails with:

```
Failures:

1) middleware for InlineExecutor when a Shoryuken::Worker class #perform_async should not raise Exception
     Failure/Error: DEFAULT_FAILURE_NOTIFIER = lambda { |failure, _opts| raise failure }
       Exactly one instance should have received the following message(s) but didn't: call
```

Removing the  `expect_any_instance_of(middleware_class).to receive(:call)` expectation allows the test to pass.

Sidekiq supports this by [running the middleware stack](https://github.com/mperham/sidekiq/blob/master/lib/sidekiq/testing.rb#L295).
Much appreciated!

![sqs_management_console](https://user-images.githubusercontent.com/2084598/39527950-6602fa7c-4de8-11e8-8aac-770f106ee3fd.png)
