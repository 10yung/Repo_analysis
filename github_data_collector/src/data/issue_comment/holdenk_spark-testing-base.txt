@holdenk taking up https://github.com/holdenk/spark-testing-base/pull/228 again.
The full outer join does not care about ordering or partitioning and works on distributed, larger datasets.
I also included an optional skip of the schema equality. One common use case was to load input and expected data from a csv and compare the transformed input to expected output. when loading a csv Spark automatically makes all fields nullable regardless of any schema. the output of the tested functionality could have non-nullable fields (e.g. groupby().count() results in a non-nullable `count` column)
For unit tests it's often just interesting whether or not the data matches, not if spark inferred the schema correctly
I've noticed that some extra folders like "spark-uuid" in `System.getProperty("java.io.tmpdir")` are created. But, some of them are not deleted. Problem is somewhere here [https://github.com/holdenk/spark-testing-base/blob/79eef40cdab48ee7aca8902754e3c456f569eea6/core/src/main/1.3/scala/com/holdenkarau/spark/testing/Utils.scala#L109](), it returns true, file is not deleted. 

I read a bit about java.io.File#delete - seems that file cannot be deleted by two general reasons: permissions and busying by some process. So, since some of them are deleted - looks like permissions is not the reason. It's really annoying to collect megabytes in temp directory. 

Spark version 2.2.1, scala version 2.11

Any thoughts here?  
Build is failing due to:
`::::::::::::::::::::::::::::::::::::::::::::::
[warn] 	::          UNRESOLVED DEPENDENCIES         ::
[warn] 	::::::::::::::::::::::::::::::::::::::::::::::
[warn] 	:: org.apache.spark#spark-streaming-kafka-0-8_2.12;2.4.0: not found
[warn] 	::::::::::::::::::::::::::::::::::::::::::::::
[warn]
[warn] 	Note: Unresolved dependencies path:
[warn] 		org.apache.spark:spark-streaming-kafka-0-8_2.12:2.4.0 (/Users/a.morales/spark-testing-base/build.sbt#L232-243)
[warn] 		  +- com.holdenkarau:root_2.12:2.4.0_0.13.0
[error] sbt.librarymanagement.ResolveException: unresolved dependency: org.apache.spark#spark-streaming-kafka-0-8_2.12;2.4.0: not found
[error] 	at sbt.internal.librarymanagement.IvyActions$.resolveAndRetrieve(IvyActions.scala:332)
[error] 	at sbt.internal.librarymanagement.IvyActions$.$anonfun$updateEither$1(IvyActions.scala:208)
[error] 	at sbt.internal.librarymanagement.IvySbt$Module.$anonfun$withModule$1(Ivy.scala:239)
[error] 	at sbt.internal.librarymanagement.IvySbt.$anonfun$withIvy$1(Ivy.scala:204)
[error] 	at sbt.internal.librarymanagement.IvySbt.sbt$internal$librarymanagement$IvySbt$$action$1(Ivy.scala:70)
[error] 	at sbt.internal.librarymanagement.IvySbt$$anon$3.call(Ivy.scala:77)
[error] 	at xsbt.boot.Locks$GlobalLock.withChannel$1(Locks.scala:95)
[error] 	at xsbt.boot.Locks$GlobalLock.xsbt$boot$Locks$GlobalLock$$withChannelRetries$1(Locks.scala:80)
[error] 	at xsbt.boot.Locks$GlobalLock$$anonfun$withFileLock$1.apply(Locks.scala:99)
[error] 	at xsbt.boot.Using$.withResource(Using.scala:10)
[error] 	at xsbt.boot.Using$.apply(Using.scala:9)
[error] 	at xsbt.boot.Locks$GlobalLock.ignoringDeadlockAvoided(Locks.scala:60)
[error] 	at xsbt.boot.Locks$GlobalLock.withLock(Locks.scala:50)
[error] 	at xsbt.boot.Locks$.apply0(Locks.scala:31)
[error] 	at xsbt.boot.Locks$.apply(Locks.scala:28)
[error] 	at sbt.internal.librarymanagement.IvySbt.withDefaultLogger(Ivy.scala:77)
[error] 	at sbt.internal.librarymanagement.IvySbt.withIvy(Ivy.scala:199)
[error] 	at sbt.internal.librarymanagement.IvySbt.withIvy(Ivy.scala:196)
[error] 	at sbt.internal.librarymanagement.IvySbt$Module.withModule(Ivy.scala:238)
[error] 	at sbt.internal.librarymanagement.IvyActions$.updateEither(IvyActions.scala:193)
[error] 	at sbt.librarymanagement.ivy.IvyDependencyResolution.update(IvyDependencyResolution.scala:20)
[error] 	at sbt.librarymanagement.DependencyResolution.update(DependencyResolution.scala:56)
[error] 	at sbt.internal.LibraryManagement$.resolve$1(LibraryManagement.scala:45)
[error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$12(LibraryManagement.scala:93)
[error] 	at sbt.util.Tracked$.$anonfun$lastOutput$1(Tracked.scala:68)
[error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$19(LibraryManagement.scala:106)
[error] 	at scala.util.control.Exception$Catch.apply(Exception.scala:224)
[error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$11(LibraryManagement.scala:106)
[error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$11$adapted(LibraryManagement.scala:89)
[error] 	at sbt.util.Tracked$.$anonfun$inputChanged$1(Tracked.scala:149)
[error] 	at sbt.internal.LibraryManagement$.cachedUpdate(LibraryManagement.scala:120)
[error] 	at sbt.Classpaths$.$anonfun$updateTask$5(Defaults.scala:2561)
[error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:44)
[error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:40)
[error] 	at sbt.std.Transform$$anon$4.work(System.scala:67)
[error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:269)
[error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16)
[error] 	at sbt.Execute.work(Execute.scala:278)
[error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:269)
[error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178)
[error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:37)
[error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[error] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[error] 	at java.lang.Thread.run(Thread.java:748)
[error] (update) sbt.librarymanagement.ResolveException: unresolved dependency: org.apache.spark#spark-streaming-kafka-0-8_2.12;2.4.0: not found`

I noticed that [`DataFrameGenerator` always returns a non-null value](https://github.com/holdenk/spark-testing-base/blob/936c34b6d5530eb664e7a9f447ed640542398d7e/core/src/main/1.3/scala/com/holdenkarau/spark/testing/DataframeGenerator.scala#L111-L138), even for columns which are nullable.
Is this the desired behavior?
If not, I would write a PR to change this ðŸ˜ƒ 
upon clicking it, I see this:

2019-09-03 19:56:06,589 WARN  [ServletHandler] [] Error for /api/v1/applications
java.lang.NoClassDefFoundError: org/glassfish/jersey/internal/inject/AbstractBinder

and nothing shows up in the tab view...

Spark 2.4.0
Hello everyone, 

I'm working on a project in Azure Cloud and HDInsight cluster. In this project, I'm working with (very) specific storages.
Because of this, I would like to, while my driver and executor are on my local computer, I want them to write on this storages by default. 

I have already set up my own SparkSession and SparkContext trait with this configurations and it work. 

Now, I would like to integrate your library for being able to easily test my dataframes. 

I have seen [this issue](https://github.com/holdenk/spark-testing-base/issues/234) for work arounds about the HiveSupport. It worked. Now, when I'm integrating Azure configurations, I'm getting NPE on Hive Database Creation. 

I would like to know if you have any idea about a solution. 

Now, enough talking, here is my code : 

#### Trait for setup my Azure configuration
##### Codes : 
```scala
trait AzureTU {

  /**
    * Azure storage account for storing test blob storage
    */
  protected val storageAccountName: String = "storageAccountName"

  /**
    * Blob storage name
    */
  protected val storageBlobName: String = "blobStorageName"

  /**
    * Complete URI for test blob storage
    */
  protected val blobStorageURI: String = s"wasbs://$storageBlobName@$storageAccountName.blob.core.windows.net"

  /**
    * Storage account key for creating blob storage
    */
  protected val storageAccountKey: String = "myAccountKey"

  /**
    * Azure Data Lake Store name
    */
  protected val adlStoragename: String = "myDataLakeName"

  /**
    * Azure Data Lake Store full adress
    */
  protected val adlStorageURI: String = s"adl://$adlStoragename.azuredatalakestore.net"

  /**
    * Returning map for hadoop azure settings
    *
    */
  protected def azureHadoopSettings: Map[String, String] = {
    println("Returning Hadoop Azure")
    Map(s"fs.azure.account.key.$storageAccountName.blob.core.windows.net" -> storageAccountKey,
      "fs.azure" -> "org.apache.hadoop.fs.azure.NativeAzureFileSystem",
      "fs.wasbs.impl" -> "org.apache.hadoop.fs.azure.NativeAzureFileSystem",
      "fs.defaultFS" -> blobStorageURI,
      "fs.adl.oauth2.access.token.provider.type" -> "ClientCredential",
      "fs.adl.oauth2.refresh.url" -> s"https://login.microsoftonline.com/mySubcriptionId/oauth2/token",
      "fs.adl.oauth2.client.id" -> "myClientId",
      "fs.adl.oauth2.credential" -> "myCredential",
      "dfs.adls.oauth2.access.token.provider" -> "org.apache.hadoop.fs.adls.oauth2.ConfCredentialBasedAccessTokenProvider",
      "fs.adl.impl" -> "org.apache.hadoop.fs.adl.AdlFileSystem",
      "fs.AbstractFileSystem.adl.impl" -> "org.apache.hadoop.fs.adl.Adl")
  }
}
```

##### Sources 

- Blob storage : 
  - [Databricks](https://docs.databricks.com/spark/latest/data-sources/azure/azure-storage.html#access-azure-blob-storage-using-the-dataframe-api)
- Datalake : 
  - [Databricks](https://docs.databricks.com/spark/latest/data-sources/azure/azure-datalake.html#access-directly-with-spark-apis-using-a-service-principal-and-oauth-2-0)

#### My personnal trait for my SparkSession

##### Codes : 
```scala
import com.sncf.ssg.cdcbibigdata.trvs.logs.CdCLogs
import com.sncf.ssg.cdcbibigdata.trvs.tests.props.AzureTU
import org.apache.spark.sql.SparkSession
import org.apache.spark.{SparkConf, SparkContext}

/**
  * This trait is here for instantiate all UnitTests classes.
  *
  */
trait SparkLocalConf extends AzureTU with CdCLogs {

  /**
    * Configuration list for local configuration
    *
    * @see [[https://stackoverflow.com/questions/38487667/overwrite-specific-partitions-in-spark-dataframe-write-method spark.sql.sources.partitionOverwriteMode]]
    */
  val confs: Map[String, String] = Map(
    "spark.sql.sources.partitionOverwriteMode" ->"dynamic", // this needs to be used to reproduce the hive dynamic partitions system for more details
    "spark.sql.warehouse.dir" -> s"$blobStorageURI/tmp/spark-warehouse"
  ) ++ azureHadoopSettings

  def conf: SparkConf = new SparkConf().setAll(confs)

  /**
    * SparkSession object to reuse in every classes.
    * Some configurations are set up here.
    */
  implicit lazy val spark: SparkSession = SparkSession
      .builder()
      .appName(appName)
      .master(master)
      .config(conf)
      .enableHiveSupport()
      .getOrCreate()

  /**
    * SparkContext object to reuse in every classes
    */
  implicit lazy val sc: SparkContext = this.spark.sparkContext

  /**
    * Name of the application for the SparkSession Manager
    */
  lazy protected val appName: String = getClass.getName
  /**
    * Master for the SparkSession. Default to yarn.
    * This can be local or local[*] for testing purposes only
    */
  protected val master: String = "local[*]"
}
```

##### Tests (for execution, if needed)

```scala
import org.apache.logging.log4j.scala.Logging
import org.scalatest.{BeforeAndAfterEach, FlatSpec, Matchers}

class SparkLocalConfTest extends FlatSpec with BeforeAndAfterEach with Logging with Matchers with SparkLocalConf {

  behavior of "SparkLocalConf"

  import spark.implicits._

  private[this] val testDb: String = getClass.getName.split("\\.") .last

  override def beforeEach(): Unit = {
    spark.sql(s"drop database if exists $testDb cascade")
    spark.sql(s"create database if not exists $testDb")
  }

  it should "be able to use HiveSupport in SparkSession" in {
    noException should be thrownBy spark.sql(s"create table $testDb.test as select 1 as a")
    spark.sqlContext.tableNames(testDb).contains("test") shouldBe true
  }

  it should "create database on blob storage" in {
    val expected = s"theUriOfMyBlobStorage/tmp/spark-warehouse/${testDb.toLowerCase}.db"
    val actual = spark.sql(s"describe database $testDb") // get info from database
      .where($"database_description_item" === "Location") // keep only line with location information
      .select($"database_description_value") // select value from location
      .collect.mkString.replaceAll("\\[|\\]", "")
    actual shouldBe expected
  }
}
```

#### The trait resulting of [the issue linked below](https://github.com/holdenk/spark-testing-base/issues/234)
##### Codes 

```scala
import java.io.File

import com.holdenkarau.spark.testing.DataFrameSuiteBase
import com.sncf.ssg.cdcbibigdata.trvs.tests.props.AzureTU
import org.apache.spark.SparkConf
import org.scalatest.{BeforeAndAfterAll, Suite}
import org.apache.spark.sql.internal.StaticSQLConf.CATALOG_IMPLEMENTATION

private[spark] trait DataFrameSuiteBaseWorkAround extends DataFrameSuiteBase with AzureTU with BeforeAndAfterAll{ self: Suite =>

  override def afterAll() {
    super.afterAll()
    new File("metastore_db/db.lck").delete()
    new File("metastore_db/dbex.lck").delete()
  }

  /**
    * Configuration list for local configuration
    *
    * @see [[https://stackoverflow.com/questions/38487667/overwrite-specific-partitions-in-spark-dataframe-write-method spark.sql.sources.partitionOverwriteMode]]
    */
  val confs: Map[String, String] = Map(
    "spark.sql.sources.partitionOverwriteMode" ->"dynamic", // this needs to be used to reproduce the hive dynamic partitions system for more details
    "spark.sql.warehouse.dir" -> s"$blobStorageURI/tmp/spark-warehouse",
    CATALOG_IMPLEMENTATION.key -> "hive") ++ azureHadoopSettings // If I don't put Azure Settings, this seems to work. 

  override def conf: SparkConf = super.conf.setAll(confs)
}
```

#### Tests (for execution, if needed)

```scala
import com.sncf.ssg.cdcbibigdata.trvs.tests.props.AzureTU
import org.scalatest._

/**
  * Ignore this tests, just here to tests no production code
  */
class DataFrameSuiteBaseWorkAroundTest extends FlatSpec with DataFrameSuiteBaseWorkAround with BeforeAndAfterAll with BeforeAndAfterEach with Matchers with AzureTU {

  behavior of "SparkLocalConf"

  import spark.implicits._

  private[this] val testDb: String = getClass.getName.split("\\.").last

  override def beforeEach(): Unit = {
    spark.sql(s"drop database if exists $testDb cascade")
    spark.sql(s"create database if not exists $testDb") // The NPE seems to be here
  }

  it should "be able to use HiveSupport in SparkSession" in { // should work because of workarounds
    noException should be thrownBy spark.sql(s"create table $testDb.test as select 1 as a")
    spark.sqlContext.tableNames(testDb).contains("test") shouldBe true
  }

  it should "create database on blob storage" in {
    val expected = s"$blobStorageURI/tmp/spark-warehouse/${testDb.toLowerCase}.db"
    val actual = spark.sql(s"describe database $testDb") // get info from database
      .where($"database_description_item" === "Location") // keep only line with location information
      .select($"database_description_value") // select value from location
      .collect.mkString.replaceAll("\\[|\\]", "")
    actual shouldBe expected
  }
}
```

I really hope you could help me on this point since I wasn't able to find any clue on the solution  :(

Of course, if you ever need some precisions, please let me know !
Is there an option to turn Spark UI back on ? To do a big more heavy debugging.
1. Added hadoop 3.1.1 dependency
And look to see what things we were doing to support previous versions and clean them up.