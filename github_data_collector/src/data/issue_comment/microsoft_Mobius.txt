Bumps `spark.version` from 2.3.1 to 2.4.4.

Updates `spark-core_2.11` from 2.3.1 to 2.4.4

Updates `spark-streaming_2.11` from 2.3.1 to 2.4.4

Updates `spark-streaming-kafka-0-8_2.11` from 2.3.1 to 2.4.4

Updates `spark-sql_2.11` from 2.3.1 to 2.4.4

Updates `spark-hive_2.11` from 2.3.1 to 2.4.4

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot ignore this [patch|minor|major] version` will close this PR and stop Dependabot creating any more for this minor/major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/microsoft/Mobius/network/alerts).

</details>

I have a simple java producer which is pushing message to kafka. I can read message using simple java consumer but not using spark streaming. Using spark streaming all messages are coming as null.

I have attached the code snippet.
I am passing the custom deserializer class in kafka properties. 
Please confirm if we need to pass it in kafkautils.createdirect stream method as well? Somehow the method is throwing invalid argument types if I try to pass as per javdoc documents.

Please help as I am new to Spark.

![img_20190122_200511](https://user-images.githubusercontent.com/46351937/51542667-c8345c80-1e81-11e9-8f24-76ba7e402a46.jpg)


### Please help me. What's wrong?
I cant call Reduce in "Pi Example"

I did everything as a guide - https://www.ics.uci.edu/~shantas/Install_Spark_on_Windows10.pdf
----------------------------
##List of my components
- Windows10
- JDK:8u60 
- Spark: spark-2.0.2-bin-hadoop2.6
- Win Utils: Uzip all in %SPARK_HOME%/bin |
- Mobius: v2.0.200 

##All variables been regitred
- HADOOP_HOME - C:\spark-2.0.2-bin-hadoop2.6
- JAVA_HOME - C:\Java
- SCALA_HOME - C:\Program Files (x86)\scala
- SPARK_HOME - C:\spark-2.0.2-bin-hadoop2.6
- SPARKCLR_HOME - C:\Mobius\runtime
- TMP - C:\tmp (i change defoult path like this https://github.com/Microsoft/Mobius/blob/6ed8316625118e273576360f1112526eeb49b1c4/notes/troubleshooting-mobius.md)
- PATHs been registrated too
## Run in VisualStudio
1.  I change Java running settings for heap size in sparkclr-submit.cmd file
(
```
...
:debugmode
"%JAVA_HOME%\bin\java" -Xms512m -Xmx1024m -cp "%LAUNCH_CLASSPATH%" org.apache.spark.deploy.csharp.CSharpRunner debug
goto :eof
...
```
)
 2. call `sparkclr-submit debugg` in command line 
 3. Load VisualStudio Exaples and Run "Pi " project with params
```
<add key="CSharpBackendPortNumber" value="5567"/>
<add key="CSharpWorkerPath" value="C:/Mobius/runtime/bin/CSharpWorker.exe"/>
```
and SparkContext: `var _conf = new SparkConf().Set("spark.local.dir", "C:\\tmp\\SparkCLRTemp");`

**programm hase down** on Reduce part of function CalculatePiUsingAnonymousMethod
```
private static void CalculatePiUsingAnonymousMethod(int n, RDD<int> rdd)
        {
            var _preCount = rdd
                            .Map(i =>
                            {
                                var random = new Random();
                                var x = random.NextDouble() * 2 - 1;
                                var y = random.NextDouble() * 2 - 1;

                                return (x * x + y * y) < 1 ? 1 : 0;
                            });
            var _count = _preCount.Reduce((a,b)=> {
                return a + b;
            });
        }
```
And show this
```
[2018-11-23 15:18:20,447] [1] [INFO ] [Microsoft.Spark.CSharp.Configuration.ConfigurationService] - ConfigurationService runMode is DEBUG
[2018-11-23 15:18:20,456] [1] [INFO ] [Microsoft.Spark.CSharp.Configuration.ConfigurationService+SparkCLRDebugConfiguration] - CSharpBackend port number read from app config 5567. Using it to connect to CSharpBackend
[2018-11-23 15:18:20,456] [1] [INFO ] [Microsoft.Spark.CSharp.Proxy.Ipc.SparkCLRIpcProxy] - CSharpBackend port number to be used in JvMBridge is 5567
[2018-11-23 15:18:20,545] [1] [INFO ] [Microsoft.Spark.CSharp.Core.SparkConf] - spark.master not set. Assuming debug mode.
[2018-11-23 15:18:20,548] [1] [INFO ] [Microsoft.Spark.CSharp.Core.SparkConf] - Spark master set to local
[2018-11-23 15:18:20,550] [1] [INFO ] [Microsoft.Spark.CSharp.Core.SparkConf] - spark.app.name not set. Assuming debug mode
[2018-11-23 15:18:20,551] [1] [INFO ] [Microsoft.Spark.CSharp.Core.SparkConf] - Spark app name set to debug app
[2018-11-23 15:18:20,552] [1] [INFO ] [Microsoft.Spark.CSharp.Core.SparkConf] - Spark configuration key-value set to spark.local.dir=C:\tmp\SparkCLRTemp
[2018-11-23 15:18:22,063] [1] [INFO ] [Microsoft.Spark.CSharp.Core.SparkContext] - Parallelizing 300001 items to form RDD in the cluster with 3 partitions
[2018-11-23 15:18:22,790] [1] [INFO ] [Microsoft.Spark.CSharp.Core.RDD`1[[System.Int32, mscorlib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089]]] - Executing Map operation on RDD (preservesPartitioning=False)
[2018-11-23 15:20:14,247] [1] [INFO ] [Microsoft.Spark.CSharp.Core.RDD`1[[System.Int32, mscorlib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089]]] - Executing Reduce operation on RDD
[2018-11-23 15:20:14,258] [1] [INFO ] [Microsoft.Spark.CSharp.Configuration.ConfigurationService+SparkCLRDebugConfiguration] - Worker path read from setting CSharpWorkerPath in app config
[2018-11-23 15:20:15,011] [1] [ERROR] [Microsoft.Spark.CSharp.Interop.Ipc.JvmBridge] - JVM method execution failed: Static method collectAndServe failed for class org.apache.spark.api.python.PythonRDD when called with 1 parameters ([Index=1, Type=JvmObjectReference, Value=14], )
[2018-11-23 15:20:15,011] [1] [ERROR] [Microsoft.Spark.CSharp.Interop.Ipc.JvmBridge] - org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.net.SocketException: Connection reset by peer: socket write error
        at java.net.SocketOutputStream.socketWrite0(Native Method)
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109)
        at java.net.SocketOutputStream.write(SocketOutputStream.java:153)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
        at java.io.DataOutputStream.write(DataOutputStream.java:107)
        at java.io.FilterOutputStream.write(FilterOutputStream.java:97)
        at org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:492)
        at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:504)
        at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:504)
        at scala.collection.Iterator$class.foreach(Iterator.scala:893)
        at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
        at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
        at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953)
        at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
        at scala.Option.foreach(Option.scala:257)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913)
        at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)
        at org.apache.spark.rdd.RDD.collect(RDD.scala:911)
        at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)
        at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.spark.api.csharp.CSharpBackendHandler.handleMethodCall(CSharpBackendHandler.scala:156)
        at org.apache.spark.api.csharp.CSharpBackendHandler.handleBackendRequest(CSharpBackendHandler.scala:106)
        at org.apache.spark.api.csharp.CSharpBackendHandler.channelRead0(CSharpBackendHandler.scala:32)
        at org.apache.spark.api.csharp.CSharpBackendHandler.channelRead0(CSharpBackendHandler.scala:28)
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Connection reset by peer: socket write error
        at java.net.SocketOutputStream.socketWrite0(Native Method)
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109)
        at java.net.SocketOutputStream.write(SocketOutputStream.java:153)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
        at java.io.DataOutputStream.write(DataOutputStream.java:107)
        at java.io.FilterOutputStream.write(FilterOutputStream.java:97)
        at org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:492)
        at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:504)
        at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:504)
        at scala.collection.Iterator$class.foreach(Iterator.scala:893)
        at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
        at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
        at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953)
        at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)

[2018-11-23 15:20:15,021] [1] [ERROR] [Microsoft.Spark.CSharp.Interop.Ipc.JvmBridge] - JVM method execution failed: Static method collectAndServe failed for class org.apache.spark.api.python.PythonRDD when called with 1 parameters ([Index=1, Type=JvmObjectReference, Value=14], )
[2018-11-23 15:20:15,022] [1] [ERROR] [Microsoft.Spark.CSharp.Interop.Ipc.JvmBridge] -
*******************************************************************************************************************************
   в Microsoft.Spark.CSharp.Interop.Ipc.JvmBridge.CallJavaMethod(Boolean isStatic, Object classNameOrJvmObjectReference, String methodName, Object[] parameters)
*******************************************************************************************************************************
```








I am trying to run WORDCOUNT example and it works if u run the CLI such as


**sparkclr-submit.cmd  --exe SparkClrWordCount.exe 
C:\Users\Downloads\Mobius-master\examples\Batch\WordCount\bin\Debug file:///C:/Users/eyadmin/Desktop/TEST/words.txt**   

- it works!
-----------------------------------------------------------

HOWEVER, if i run using the visual studio such as, 

step 1 : sparkclr-submit.cmd  **debug**

step 2 :  running the WORDCOUNT example using visual studio,  I get the following error, 

_{"Value cannot be null.\r\nParameter name: path1"}

StackTrace = "   at System.IO.Path.Combine(String path1, String path2, String path3)\r\n   at Microsoft.Spark.CSharp.Configuration.ConfigurationService.SparkCLRDebugConfiguration.GetSparkCLRArtifactsPath(String sparkCLRSubFolderName, String fileName)\r\n   at Microsoft...._
Downloaded the master yesterday and tried to run build.cmd in the VS2015 command prompt running JDK 8.0.600.27 64bit.  After having to manually download winutils and tartools (both failed in the script with file not found) from the urls in the script, got the following error.  

> 
> ←[31m- Kafka offset checkpoint and replay *** FAILED ***←[0m
> ←[31m  java.io.IOException: Cannot run program "C:\Mobius\MobiusMaster\build\to
> ls\winutils\bin\winutils.exe": CreateProcess error=193, %1 is not a valid Win32
> application←[0m
> ←[31m  at java.lang.ProcessBuilder.start(Unknown Source)←[0m
> ←[31m  at org.apache.hadoop.util.Shell.runCommand(Shell.java:404)←[0m
> ←[31m  at org.apache.hadoop.util.Shell.run(Shell.java:379)←[0m
> ←[31m  at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:
> 89)←[0m
> ←[31m  at org.apache.hadoop.util.Shell.execCommand(Shell.java:678)←[0m
> ←[31m  at org.apache.hadoop.util.Shell.execCommand(Shell.java:661)←[0m
> ←[31m  at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSys
> em.java:639)←[0m
> ←[31m  at org.apache.hadoop.fs.FilterFileSystem.setPermission(FilterFileSystem.
> ava:468)←[0m
> ←[31m  at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.jav
> :456)←[0m
> ←[31m  at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.jav
> :424)←[0m
> ←[31m  ...←[0m
> ←[31m  Cause: java.io.IOException: CreateProcess error=193, %1 is not a valid W
> n32 application←[0m
> ←[31m  at java.lang.ProcessImpl.create(Native Method)←[0m
> ←[31m  at java.lang.ProcessImpl.<init>(Unknown Source)←[0m
> ←[31m  at java.lang.ProcessImpl.start(Unknown Source)←[0m
> ←[31m  at java.lang.ProcessBuilder.start(Unknown Source)←[0m
> ←[31m  at org.apache.hadoop.util.Shell.runCommand(Shell.java:404)←[0m
> ←[31m  at org.apache.hadoop.util.Shell.run(Shell.java:379)←[0m
> ←[31m  at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:
> 89)←[0m
> ←[31m  at org.apache.hadoop.util.Shell.execCommand(Shell.java:678)←[0m
> ←[31m  at org.apache.hadoop.util.Shell.execCommand(Shell.java:661)←[0m
> ←[31m  at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSys
> em.java:639)←[0m
> ←[31m  ...←[0m
> ←[36mRun completed in 5 seconds, 986 milliseconds.←[0m
> ←[36mTotal number of tests run: 38←[0m
> ←[36mSuites: completed 11, aborted 0←[0m
> ←[36mTests: succeeded 37, failed 1, canceled 0, ignored 0, pending 0←[0m
> ←[31m*** 1 TEST FAILED ***←[0m
> 18/06/27 09:59:02 INFO ShutdownHookManager: Shutdown hook called
> 18/06/27 09:59:02 INFO ShutdownHookManager: Deleting directory C:\Users\LUM5220
> \AppData\Local\Temp\spark-d67b92dd-99de-4370-bbd5-f75320c2b424
> [INFO] ------------------------------------------------------------------------
> [INFO] BUILD FAILURE
> [INFO] ------------------------------------------------------------------------
> [INFO] Total time: 53.707 s
> [INFO] Finished at: 2018-06-27T09:59:02-06:00
> [INFO] Final Memory: 52M/665M
> [INFO] ------------------------------------------------------------------------
> [ERROR] Failed to execute goal org.scalatest:scalatest-maven-plugin:1.0:test (t
> st) on project spark-clr_2.11: There are test failures -> [Help 1]
> [ERROR]
> [ERROR] To see the full stack trace of the errors, re-run Maven with the -e swi
> ch.
> [ERROR] Re-run Maven using the -X switch to enable full debug logging.
> [ERROR]
> [ERROR] For more information about the errors and possible solutions, please re
> d the following articles:
> [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureEx
> eption
>         1 file(s) copied.
> Build Mobius Scala components failed, stop building.
> 
> C:\Mobius\MobiusMaster\build>
The documentation file appears to have been generated with no space between the hashes and the header text. This is causing the headers to not display correctly, and is difficult to read. See below for an example of with and without the space:

##<center><H1><font color="darkorchid4">Mobius API Documentation</font></H1></center>
###<font color="#68228B">Microsoft.Spark.CSharp.Core.Accumulator</font>
####Summary
  
            
            A shared variable that can be accumulated, i.e., has a commutative and associative "add"
            operation. Worker tasks on a Spark cluster can add values to an Accumulator with ...

<br />
<br />

## <center><H1><font color="darkorchid4">Mobius API Documentation</font></H1></center>
### <font color="#68228B">Microsoft.Spark.CSharp.Core.Accumulator</font>
#### Summary
  
            
            A shared variable that can be accumulated, i.e., has a commutative and associative "add"
            operation. Worker tasks on a Spark cluster can add values to an Accumulator with ...

I'm trying to use newAPIHadoopFile to implement  custom InputFormat and RecordReader classes  instead of using the default ones provided by Spark. 
I can do the same in Java but am unable to find the correct way of achieving this using Mobius. 

For reference, these are the classes and interfaces I want to override
```
java.lang.Object
org.apache.hadoop.mapreduce.InputFormat<K,V>
org.apache.hadoop.mapreduce.lib.input.FileInputFormat<K,V>
```
and `Interface RecordReader<K,V>`

[https://hadoop.apache.org/docs/r2.4.1/api/org/apache/hadoop/mapreduce/RecordReader.html]()

[https://hadoop.apache.org/docs/stable/api/index.htmlorg/apache/hadoop/mapreduce/lib/input/FileInputFormat.html](url)

In Mobius, trying to use a custom InputFormat class throws a 'ClassNotFound' exception. I'm guessing it only excepts classes provided by Hadoop.

Does Mobius support custom partitioning ?

I get a serialization error any time I'm passing data at runtime.  This happens with any calls using a lambda expression with data originating outside of the lambda expression.  Using broadcast variables also gives the same error.

Gives serialization error:
`string x="/path";
var results = rdd.Map(input =>
{
Console.WriteLine(x);
});`

but, no serialization error here:
`var results = rdd.Map(input =>
{
Console.WriteLine("/path");
});`

This is running 2.0.2 Spark, Linux Mono 5.10.1.20, built with msbuild.  I've also tested Mono4.8.1 with xbuild and get the same error.

actual error:

ERROR System.Runtime.Serialization.SerializationException: Type '(MyClassName+<>c__DisplayClass4_0' in Assembly 'MyAssembly, Version=1.0.0.0, Culture=neutral, PublicKeyToken=null' is not marked as serializable.
  at System.Runtime.Serialization.FormatterServices.InternalGetSerializableMembers (System.RuntimeType type) [0x00045] in <8fbafb724c144c9dad69bccfec38ae40>:0
  at System.Runtime.Serialization.FormatterServices+<>c__DisplayClass9_0.<GetSerializableMembers>b__0 (System.Runtime.Serialization.MemberHolder _) [0x00000] in <8fbafb724c144c9dad69bccfec38ae40>:0
  at System.Collections.Concurrent.ConcurrentDictionary`2[TKey,TValue].GetOrAdd (TKey key, System.Func`2[T,TResult] valueFactory) [0x00034] in <8fbafb724c144c9dad69bccfec38ae40>:0
  at System.Runtime.Serialization.FormatterServices.GetSerializableMembers (System.Type type, System.Runtime.Serialization.StreamingContext context) [0x0005e] in <8fbafb724c144c9dad69bccfec38ae40>:0



