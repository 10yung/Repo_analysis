## Issue Addressed

None

## Proposed Changes

Fixed all clippy warnings I could find.

## Additional Info

Allowed 5 warnings. ([1](beacon_node/rest_api/src/lib.rs), [2](beacon_node/rest_api/src/router.rs), [3](eth2/utils/lighthouse_metrics/src/lib.rs), [4](eth2/utils/ssz/tests/tests.rs), [5](eth2/utils/ssz_types/src/bitfield.rs))
I think I caught a bug in [reduced_tree.rs](https://github.com/sigp/lighthouse/pull/813/files/3109d64b4136b43a5ec91e14aaff6d0e9acead8d#diff-bbbd4735bcad3e54a979ed09d44f816e), where the error would not actually get returned. Possible bug avoided!
## Description

The forwards block iterator can be given an end state as a reference point. This should be done for non-finalized iterations. 

We should pass the state and the block_hash to Store::forwards_block_roots_iterator as the end_state and end_block_root params. At the moment the function on the BeaconChain hardcodes the current head (which should be true for finalized iters). 

We should loop through state.block_roots to see if the block is a recent ancester. If so, use current head as the state. 

If not, we can search for the block and state which is resource heavy or error and renegotiate requests.
## Description

This updates the network crates to support shard subnets and the naive attestation aggregation strategy. This adds a variety of features including gossipsub message validation. 

This upgrade addresses the following issues:
- #754 
- #720 
- #691 
- #580 
- #532 
- #511 
- #810 

## Description

It would be great if there was a rest API method to detect whether a node is synchronized.

For example, the `prysm` client has a `/eth/v1alpha1/node/syncing` (https://api.prylabs.net/#/Node/GetSyncStatus) which returns a bool.

This is important when orchestrating multiple beacon nodes, and knowing when they are ready to serve traffic.

Potentially related issues: #714 #720 
## Issue Addressed

Closes #797 

## Proposed Changes

- Removes outdated .yaml file.
- Add proposer slashing endpoint and associated test.
- Add attester slashing endpoint and associated test.
- Add documentation to the lighthouse book.
- Fix some clippy warnings
- Sort the beacon api mdbook to make it easier to find endpoints.

Sproul's SPRP Tips
## Description

Since we made the hot database sparse, disk reads of states have **increased** from around 100MB/min to 800MB/min. This is due to the loading of epoch boundary states when processing attestations, which bypasses the checkpoint cache.

## Steps to resolve

I have a work-in-progress implementation on my [`epoch-boundary-cache`](https://github.com/michaelsproul/lighthouse/tree/epoch-boundary-cache) branch that uses the existing checkpoint cache, but has the disadvantage of increasing the size of the checkpoint cache from 4 to 70 (it needs to be >64 in order to have a good chance of the two most recent boundary states being in the cache). It successfully reduces reads to about 50MB/min, but the memory usage has proved problematic for my node, even with 4GB of memory (70 states should only be 700MB max, so this could be a manifestation of #800).

In order to keep the cache small (or at least have that as an option), I think we could use an [LRU cache](https://crates.io/crates/lru-cache) and set its minimum size to something slightly larger than 4 (10 perhaps?). During the refactor I think it would also be beneficial to split the cache into separate block and state caches, as when we load an epoch boundary state we don't have a full checkpoint, but we may want to add it to the cache if it wasn't already there.

## Issue Addressed

NA

## Proposed Changes

Removes `ReducedTree` and replaces it with `ProtoArray`.

## TODO:

- Remove Age's sync-bug-finder: https://github.com/sigp/lighthouse/pull/804/commits/268dd064bd9fe04723b2ba0715e9c8d334ed1c0a

## Description

In `per_block_processing` we store a list of intermediate states when fast-fowarding a state through skip slots:

https://github.com/sigp/lighthouse/blob/03443c3e5714ae5478173de4a87f92a1c9577549/beacon_node/beacon_chain/src/beacon_chain.rs#L1364-L1378

Given that each state is on the order of MB (they're ~5mb when SSZ encoded, not sure what they consumer when in RAM) there's an attack here to force us to skip forward more blocks than we can keep in RAM.

## Suggestions

1. Enforce a max-skip slots value. This was found to be annoying in testnets, so perhaps we allow it to be disabled at run-time.
2. Store the blocks in the DB as we go, and unwind if we encounter an error.

I'd probably lean towards a combination of the both. (2) still has a DoS vector that we can fill up a filesystem, but we can fit _many_ more states in the DB than in RAM because there's more memory and there is compression\efficiency when storing in the DB that is not present in RAM.

## Description

Presently there is no way for a user to submit slashings to the beacon node. We can fix this by providing some API endpoints to insert them into the op pool.

## Steps to resolve

Add two HTTP endpoints to the [`rest_api`](https://github.com/sigp/lighthouse/tree/master/beacon_node/rest_api):

- POST `/beacon/proposer_slashing`: accepts a `ProposerSlashing` and feeds it to [`op_
pool::insert_proposer_slashing`](https://github.com/sigp/lighthouse/blob/03443c3e5714ae5478173de4a87f92a1c9577549/eth2/operation_pool/src/lib.rs#L142-L156).
- POST `/beacon/attester_slashing`: same as above, but `s/proposer/attester`.

@pscott I think this is a good one for you :)
