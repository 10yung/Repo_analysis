I want to know all the (X, Y) values of the edge for one object to estimate its size.
Any solution? Thanks.
@s9xie 
[Python] Working on a video file/Stream, This model is eating ram, for a large video(10 mins), by the time I reach the EOV, it has already used 12GB ram and dows not release even after releasing the file.
Even tried it on the tutorial by pyImageSearch and on a live youtube stream

Any help is appreciated.
Thanks
Hi, is there any code for testing RGBD data? For example the NYUD dataset as written in the paper? Thank you!

Tried to compile it on Mojave. I have installed HDF5 from Homebrew and source. Both produced the same following error message.


[ 77%] Linking CXX shared library ../../lib/libcaffe.dylib
Undefined symbols for architecture x86_64:
  "_H5LTfind_dataset", referenced from:
      caffe::SGDSolver<float>::RestoreSolverStateFromHDF5(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in solver.cpp.o
      caffe::SGDSolver<double>::RestoreSolverStateFromHDF5(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in solver.cpp.o
      void caffe::hdf5_load_nd_dataset_helper<float>(long long, char const*, int, int, caffe::Blob<float>*) in hdf5.cpp.o
      void caffe::hdf5_load_nd_dataset_helper<double>(long long, char const*, int, int, caffe::Blob<double>*) in hdf5.cpp.o
  "_H5LTget_dataset_info", referenced from:
      void caffe::hdf5_load_nd_dataset_helper<float>(long long, char const*, int, int, caffe::Blob<float>*) in hdf5.cpp.o
      void caffe::hdf5_load_nd_dataset_helper<double>(long long, char const*, int, int, caffe::Blob<double>*) in hdf5.cpp.o
      caffe::hdf5_load_string(long long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in hdf5.cpp.o
  "_H5LTget_dataset_ndims", referenced from:
      void caffe::hdf5_load_nd_dataset_helper<float>(long long, char const*, int, int, caffe::Blob<float>*) in hdf5.cpp.o
      void caffe::hdf5_load_nd_dataset_helper<double>(long long, char const*, int, int, caffe::Blob<double>*) in hdf5.cpp.o
  "_H5LTmake_dataset_double", referenced from:
      void caffe::hdf5_save_nd_dataset<double>(long long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, caffe::Blob<double> const&, bool) in hdf5.cpp.o
  "_H5LTmake_dataset_float", referenced from:
      void caffe::hdf5_save_nd_dataset<float>(long long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, caffe::Blob<float> const&, bool) in hdf5.cpp.o
  "_H5LTmake_dataset_int", referenced from:
      caffe::hdf5_save_int(long long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) in hdf5.cpp.o
  "_H5LTmake_dataset_string", referenced from:
      caffe::hdf5_save_string(long long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in hdf5.cpp.o
  "_H5LTread_dataset_double", referenced from:
      void caffe::hdf5_load_nd_dataset<double>(long long, char const*, int, int, caffe::Blob<double>*) in hdf5.cpp.o
  "_H5LTread_dataset_float", referenced from:
      void caffe::hdf5_load_nd_dataset<float>(long long, char const*, int, int, caffe::Blob<float>*) in hdf5.cpp.o
  "_H5LTread_dataset_int", referenced from:
      caffe::hdf5_load_int(long long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in hdf5.cpp.o
  "_H5LTread_dataset_string", referenced from:
      caffe::hdf5_load_string(long long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in hdf5.cpp.o
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make[2]: *** [lib/libcaffe.dylib] Error 1
make[1]: *** [src/caffe/CMakeFiles/caffe.dir/all] Error 2
make: *** [all] Error 2
Do you have a version of tenosrflow?
Fix rendered markdown lists and add new lines for markdown conformity.
If I run "PostprocessHED.m" ( https://github.com/phillipi/pix2pix/blob/master/scripts/edges/PostprocessHED.m ) in MATLAB I get an error because I need to define these parameters:

%%% parameters
% hed_mat_dir: the hed mat file directory (the output of 'batch_hed.py')
% edge_dir: the output HED edges directory
% image_width: resize the edge map to [image_width, image_width] 
% threshold: threshold for image binarization (default 25.0/255.0)
% small_edge: remove small edges (default 5)

Usually in python I would do something like this:
python script.py --hed_mat_dir C:\folder\hed --edge_dir C:\folder\edge --image_width 256,256 --threeshold 25.0/255.0 --small_edge 0
But I have no idea how to do the same with MATLAB, can you show me an example, please?
Sorry for the noobish question.
I'm following the "Batch Processing" procedure here ( https://github.com/s9xie/hed#batch-processing ) and got to the point where I run "batch_hed.py" ( https://github.com/phillipi/pix2pix/blob/master/scripts/edges/batch_hed.py ) and all my pictures are converted into .MAT files (MATLAB) files, the final step requires to run "PostprocessHED.m" ( https://github.com/phillipi/pix2pix/blob/master/scripts/edges/PostprocessHED.m ) to convert them into normal pictures, but instructions says that I need MATLAB to do it: "get the cpp file edgesNmsMex.cpp from https://raw.githubusercontent.com/pdollar/edges/master/private/edgesNmsMex.cpp and compile it in Matlab: mex edgesNmsMex.cpp You also need to download and install Piotr's Computer Vision Matlab Toolbox: https://pdollar.github.io/toolbox/"
Is there any method to do it for free without MATLAB? That's a proprietary paid software. I noticed that the author of this Google Coolaboratory is using Octave as a free alternative to MATLAB: https://colab.research.google.com/drive/1SI57hLgHCX6mbZ2jNveNvq1Pqx07Jag7 , so I guess a 100% free approach is definitely possible, however, that Coolaboratory works only online, I'd like a local/offline solution for Windows 10 like I'm doing now if possible.
Any help is much appreciated, thank you very much for your kind attention!
The pre-trained model link is 404'ing:

http://vcl.ucsd.edu/hed/hed_pretrained_bsds.caffemodel

Additionally, the directory listing of http://vcl.ucsd.edu/ appears to be empty.

Can the repo owners update the links? Or can anyone watching this repo provide me with the pre-trained model and prototxt files?

Thank you!