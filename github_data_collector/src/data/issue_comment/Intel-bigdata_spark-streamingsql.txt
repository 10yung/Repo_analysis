2018-12-27 23:02:09,023 WARN [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager:70 - Lost task 396.0 in stage 6003.0 (TID 4033967, bdhi2115): java.lang.AssertionError: assertion failed: Failed to get records for spark-executor-groupFxyoNDww_122320 s1mme_event_topic 521 12444811684 after polling for 1500
	at scala.Predef$.assert(Predef.scala:179)
	at org.apache.spark.streaming.kafka010.CachedKafkaConsumer.get(CachedKafkaConsumer.scala:73)
	at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:227)
	at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:193)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:389)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.DataFrame$$anonfun$toJSON$1$$anon$1.hasNext(DataFrame.scala:1691)
	at org.apache.spark.sql.DataFrame$$anonfun$toJSON$1$$anon$1.next(DataFrame.scala:1697)
	at org.apache.spark.sql.DataFrame$$anonfun$toJSON$1$$anon$1.next(DataFrame.scala:1690)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at org.apache.spark.sql.DataFrame$$anonfun$toJSON$1$$anon$1.foreach(DataFrame.scala:1690)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:176)
	at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:45)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at org.apache.spark.sql.DataFrame$$anonfun$toJSON$1$$anon$1.to(DataFrame.scala:1690)
	at scala.collection.TraversableOnce$class.toList(TraversableOnce.scala:257)
	at org.apache.spark.sql.DataFrame$$anonfun$toJSON$1$$anon$1.toList(DataFrame.scala:1690)
	at com.asiainfo.ocdp.stream.label.LabelManager$$anonfun$execLabels$1.apply(LabelManager.scala:48)
	at com.asiainfo.ocdp.stream.label.LabelManager$$anonfun$execLabels$1.apply(LabelManager.scala:37)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$22.apply(RDD.scala:717)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$22.apply(RDD.scala:717)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:275)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:277)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:277)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:277)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

always show this error, but i can't find the reason.   Anyone meet this ?
```
**here is code** 

val ssc = new StreamingContext("local[10]", "test", Duration(10000))
val sc = ssc.sparkContext

val streamSqlContext = new StreamSQLContext(ssc, new SQLContext(sc))
streamSqlContext.command(
  """
    |CREATE TEMPORARY TABLE kafka1 (
    |  word string,
    |  name string
    |)
    |USING org.apache.spark.sql.streaming.sources.KafkaSource
    |OPTIONS(
    |  zkQuorum "xxxx",
    |  groupId  "test1",
    |  topics   "test1:1",
    |  messageToRow "org.apache.spark.sql.streaming.examples.MessageDelimiter")
  """.stripMargin)

streamSqlContext.command(
  """
    |CREATE TEMPORARY TABLE kafka2 (
    |  word string,
    |  name string
    |)
    |USING org.apache.spark.sql.streaming.sources.KafkaSource
    |OPTIONS(
    |  zkQuorum "xxxxx",
    |  groupId  "test2",
    |  topics   "test2:1",
    |  messageToRow "org.apache.spark.sql.streaming.examples.MessageDelimiter")
  """.stripMargin)
val dataFrameDStream = streamSqlContext.sql(
  """
    |SELECT t1.word,t1.name,t2.name
    |FROM kafka1 AS t1
    |JOIN kafka2 AS t2
    |ON t1.word=t2.word
  """.stripMargin)
    dataFrameDStream.explain(true)
    dataFrameDStream.foreachRDD { r => r.foreach(row => print(row)) }

ssc.start()
ssc.awaitTerminationOrTimeout(60 * 1000000)
ssc.stop()
```

topic:test1   input a row data:
ee,gg

topic:test2   input a row data:
ee,gg

**and print the plan:**

== Parsed Logical Plan ==
'Project [unresolvedalias('t1.word),unresolvedalias('t1.name),unresolvedalias('t2.name)]
 'Join Inner, Some(('t1.word = 't2.word))
  'UnresolvedRelation [kafka1], Some(t1)
  'UnresolvedRelation [kafka2], Some(t2)

== Analyzed Logical Plan ==
word: string, name: string, name: string
Project [word#0,name#1,name#3]
 Join Inner, Some((word#0 = word#2))
  Subquery t1
   Subquery kafka1
    Relation[word#0,name#1] KafkaRelation(10.200.20.242:2181,10.200.20.243:2181,10.200.20.244:2181/Apps/bip,web_pageaccess,Map(web_pageaccess -> 1),None,org.apache.spark.sql.streaming.examples.MessageDelimiter@6e674747,StructType(StructField(word,StringType,true), StructField(name,StringType,true)),org.apache.spark.sql.SQLContext@5ea16042)
  Subquery t2
   Subquery kafka2
    Relation[word#2,name#3] KafkaRelation(10.200.20.242:2181,10.200.20.243:2181,10.200.20.244:2181/Apps/bip,dol_client,Map(dol_client -> 1),None,org.apache.spark.sql.streaming.examples.MessageDelimiter@1ec11eca,StructType(StructField(word,StringType,true), StructField(name,StringType,true)),org.apache.spark.sql.SQLContext@5ea16042)

== Optimized Logical Plan ==
Project [word#0,name#1,name#3]
 Join Inner, Some((word#0 = word#2))
  Relation[word#0,name#1] KafkaRelation(10.200.20.242:2181,10.200.20.243:2181,10.200.20.244:2181/Apps/bip,web_pageaccess,Map(web_pageaccess -> 1),None,org.apache.spark.sql.streaming.examples.MessageDelimiter@6e674747,StructType(StructField(word,StringType,true), StructField(name,StringType,true)),org.apache.spark.sql.SQLContext@5ea16042)
  Relation[word#2,name#3] KafkaRelation(10.200.20.242:2181,10.200.20.243:2181,10.200.20.244:2181/Apps/bip,dol_client,Map(dol_client -> 1),None,org.apache.spark.sql.streaming.examples.MessageDelimiter@1ec11eca,StructType(StructField(word,StringType,true), StructField(name,StringType,true)),org.apache.spark.sql.SQLContext@5ea16042)

== Physical Plan ==
TungstenProject [word#0,name#1,name#3]
 SortMergeJoin [word#0], [word#2]
  TungstenSort [word#0 ASC], false, 0
   TungstenExchange hashpartitioning(word#0)
    ConvertToUnsafe
     PhysicalDStream [word#0,name#1], org.apache.spark.streaming.dstream.MappedDStream@a8040d0
  TungstenSort [word#2 ASC], false, 0
   TungstenExchange hashpartitioning(word#2)
    ConvertToUnsafe
     PhysicalDStream [word#2,name#3], org.apache.spark.streaming.dstream.MappedDStream@6a1f1d12

Code Generation: true

**print the result** 

(null,ee,gg)

**and  throws exception:** 

java.lang.ArrayIndexOutOfBoundsException: 1
    at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.genericGet(rows.scala:226)
    at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow$class.getAs(rows.scala:34)
    at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow$class.isNullAt(rows.scala:35)
    at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.isNullAt(rows.scala:220)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.insertAll(BypassMergeSortShuffleWriter.java:119)
    at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:73)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
    at org.apache.spark.scheduler.Task.run(Task.scala:88)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:722)
16/06/02 16:34:32 ERROR Executor: Exception in task 0.0 in stage 4.0 (TID 4)


I made the jar of project and trying to run my own defined streaming sql program(using hive context instead of sqlContext) Im getting following exception when I registered the table.

Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/spark/sql/catalyst/plans/logical/CreateTableAsSelect
        at org.apache.spark.sql.streaming.SchemaDStream.<init>(SchemaDStream.scala:64)
        at org.apache.spark.sql.streaming.SchemaDStream.<init>(SchemaDStream.scala:48)
        at org.apache.spark.sql.streaming.StreamSQLContext.createSchemaDStream(StreamSQLContext.scala:64)
        at com.yash.testcep.OutliersFinder$.main(OutliersFinder.scala:40)
        at com.yash.testcep.OutliersFinder.main(OutliersFinder.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:674)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.catalyst.plans.logical.CreateTableAsSelect
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)

add support to apache spark 1.6.1

add support to apache spark 1.6.1

Hello,

I created a jar for this project and tried to execute the class "UdafEnabledQuery". I keep getting the following error:
Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/spark/sql/execution/SparkPlan$

How do we resolve this?

Thank you.
Sai


https://github.com/Intel-bigdata/spark-streamingsql/issues/17

WARN TaskSetManager: Lost task 0.0 in stage 11.0 (TID 871, xa36502.eng.platformlab.ibm.com): java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.spark.sql.types.UTF8String

