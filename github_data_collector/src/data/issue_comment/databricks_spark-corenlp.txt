```
import org.apache.spark.sql.functions._
import com.databricks.spark.corenlp.functions._

val input = Seq(
  (1, "Stanford University is located in California. It is a great university"),
  (2, "")
).toDF("id", "text")

input.withColumn("sentiment", sentiment($"text")).show()
```
**Error: org.apache.spark.SparkException: Failed to execute user defined function(anonfun$sentiment$1: (string) => int)**

Any plans to catch this issue properly? 



With [Stanford NLP up and running in Python](https://stanfordnlp.github.io/stanfordnlp/) is there an intention of developing a Python/PySpark wrapper as well? And in the meantime, what would be the best way, say to Lemmatize using Stanford NLP using PySpark? In a UDF?  
Hi,

For those who are interested I am maintaining a [french version of this library.](https://framagit.org/interchu/spark-corenlp-french)

Could we have a spark wrapper for this function:
https://github.com/stanfordnlp/CoreNLP/blob/5fdbfb209069276e95e1765093df9855d2cf2c38/src/edu/stanford/nlp/tagger/maxent/TTags.java#L288

to get the set of all possible POS tags, not that tags of a particular sentence?

In order to support other languages like Chinese, we should load different property based on the language. In this commit, I use Java property to support load Corenlp property file dynamiclly. 
Hi all, 
Would you tell me how to use this lib in java?
I can import it, but i do not know to use.
"import com.databricks.spark.corenlp.functions;"
The maven project pom.xml:
    <dependency>
      <groupId>databricks</groupId>
      <artifactId>spark-corenlp</artifactId>
      <version>0.2.0-s_2.11</version>
    </dependency>

In the following, there is my program.
 SparkSession spark = SparkSession.builder().getOrCreate();
 JavaSparkContext sc = new JavaSparkContext(spark.sparkContext());  
 StructField[] structFields = new StructField[]{
              new StructField("intColumn", DataTypes.IntegerType, true, Metadata.empty()),
              new StructField("stringColumn", DataTypes.StringType, true, Metadata.empty())
      };

StructType structType = new StructType(structFields);

List<Row> rows = new ArrayList<>();
rows.add(RowFactory.create(1, "<xml>Stanford University is located in California. " +
   	        "It is a great university.</xml>"));
      
Dataset<Row> df = spark.createDataFrame(rows, structType);
      
System.out.println("Test Count = " + df.count());
      
df.show();

Rick


I have been using the Stanford CoreNLP wrapper for Apache Spark to do NEP analysis and found it works well. However, i want to extend the simple example to where I can map the analysis back to an original dataframe id. See below, I have added two more row to the simple example.

val input = Seq(
  (1, "<xml>Apple is located in California. It is a great company.</xml>"),
  (2, "<xml>Google is located in California. It is a great company.</xml>"),
  (3, "<xml>Netflix is located in California. It is a great company.</xml>")
).toDF("id", "text")

input.show()

input: org.apache.spark.sql.DataFrame = [id: int, text: string]
+---+--------------------+
| id|                text|
+---+--------------------+
|  1|<xml>Apple is loc...|
|  2|<xml>Google is lo...|
|  3|<xml>Netflix is l...|
+---+--------------------+
I can then run this dataframe through the Spark CoreNLP wrapper to do both sentiment and NEP analysis.

val output = input
  .select(cleanxml('text).as('doc))
  .select(explode(ssplit('doc)).as('sen))
  .select('sen, tokenize('sen).as('words), ner('sen).as('nerTags), sentiment('sen).as('sentiment))
However, in the output below i have lost the connection back to the original dataframe row ids.

+--------------------+--------------------+--------------------+---------+
|                 sen|               words|             nerTags|sentiment|
+--------------------+--------------------+--------------------+---------+
|Apple is located ...|[Apple, is, locat...|[ORGANIZATION, O,...|        2|
|It is a great com...|[It, is, a, great...|  [O, O, O, O, O, O]|        4|
|Google is located...|[Google, is, loca...|[ORGANIZATION, O,...|        3|
|It is a great com...|[It, is, a, great...|  [O, O, O, O, O, O]|        4|
|Netflix is locate...|[Netflix, is, loc...|[ORGANIZATION, O,...|        3|
|It is a great com...|[It, is, a, great...|  [O, O, O, O, O, O]|        4|
+--------------------+--------------------+--------------------+---------+
Ideally, I want something like the following:

+--+---------------------+--------------------+--------------------+---------+
|id|                  sen|               words|             nerTags|sentiment|
+--+---------------------+--------------------+--------------------+---------+
| 1| Apple is located ...|[Apple, is, locat...|[ORGANIZATION, O,...|        2|
| 1| It is a great com...|[It, is, a, great...|  [O, O, O, O, O, O]|        4|
| 2| Google is located...|[Google, is, loca...|[ORGANIZATION, O,...|        3|
| 2| It is a great com...|[It, is, a, great...|  [O, O, O, O, O, O]|        4|
| 3| Netflix is locate...|[Netflix, is, loc...|[ORGANIZATION, O,...|        3|
| 3| It is a great com...|[It, is, a, great...|  [O, O, O, O, O, O]|        4|
+--+---------------------+--------------------+--------------------+---------+
I have tried to create a UDF but am unable to make it work.
Hi there, a Scala newbie question. I'm trying to use this package. However,

- How should it be added to `build.sbt`? `"com.databricks" % "spark-corpnlp" % "0.3.0-SNAPSHOT"` does not work ("not found" error).
- The Readme mentioned that "CoreNLP jars must be added to dependencies". Could you please kindly paste the link to the jars in question? Are these the jars for this project? The jars for the Stanford project? Are they to be added in build.sbt, or have the files manually pasted into a specific directory?

Anyone else who has got this working, please help by providing more detailed setup instructions. Thx!!
I have this code to run corenlp with spanish language. I use the databricks api in scala:

var props: Properties = new Properties()
props.setProperty("annotators", "tokenize, ssplit, pos, lemma, ner")
props.setProperty("tokenize.language", "es")
props.setProperty("tokenize.verbose", "true")
props.setProperty("pos.model", "edu/stanford/nlp/models/pos-tagger/spanish/spanish-distsim.tagger")
props.setProperty("ner.model", "edu/stanford/nlp/models/ner/spanish.ancora.distsim.s512.crf.ser.gz")
props.setProperty("parse.model", "edu/stanford/nlp/models/lexparser/spanishPCFG.ser.gz")
val sentimentPipeline = new StanfordCoreNLP(props)
val output = df
.select(explode(ssplit('_c3)).as('sen))
.select('sen, tokenize('sen).as('words) , ner('sen).as('nerTags) )
output.show(truncate = false)

My POM.xml file look like this:

<dependency>
  <groupId>edu.stanford.nlp</groupId>
  <artifactId>stanford-corenlp</artifactId>
  <version>3.8.0</version>
</dependency>
<dependency>
  <groupId>edu.stanford.nlp</groupId>
  <artifactId>stanford-corenlp</artifactId>
  <version>3.8.0</version>
  <classifier>models-spanish</classifier>
</dependency>
<dependency>
  <groupId>databricks</groupId>
  <artifactId>spark-corenlp</artifactId>
  <version>0.2.0-s_2.10</version>
</dependency>
i get this error:
Caused by: java.io.IOException: Unable to open "edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger" as class path, filename or URL

I saw in my log this before the error:

17/07/30 19:03:04 INFO AnnotatorPool: Replacing old annotator "tokenize" with signature [tokenize.language:es;tokenize.verbose:true;] with new annotator with signature [ssplit.isOneSentence:true;tokenize.language:en;tokenize.class:PTBTokenizer;]
17/07/30 19:03:04 INFO AnnotatorPool: Replacing old annotator "ssplit" with signature [tokenize.language:es;tokenize.verbose:true;] with new annotator with signature [ssplit.isOneSentence:true;tokenize.language:en;tokenize.class:PTBTokenizer;]

I think this is the reason of my error because the language has been replaced "automatically"Â¿?
thanks