local counter = 0
local inspect = require 'inspect'

request = function()
   wrk.headers["x-auth-token"] = counter
   counter = counter + 1
   return wrk.format(nil, path)
end


function done(summary, latency, requests)
    print("======start of summary=============")
	print(inspect(summary))
	-- QPS
	print("QPS", summary.requests / summary.duration * 1000 * 1000)
	print("mean",latency.mean)
	print("percentile(50)",latency:percentile(50))
	print("percentile(75)",latency:percentile(75))
	print("percentile(90)",latency:percentile(90))
	print("percentile(99)",latency:percentile(99))
	print("percentile(99.9)",latency:percentile(99.9))
	print("percentile(99.99)",latency:percentile(99.99))
	print("percentile(99.999)",latency:percentile(99.999))
	print("percentile(100.000)",latency:percentile(100.000))
	print("======end of summary=============")
end
Hello

This pull request fixes the build script of this project.
Specifically, it adds missing Make dependencies so that the targets of the project are re-generated correctly whenever there are updates to any of the dependent source files.

In this way, the project is incrementally built and we no longer sacrifice time in clean builds (i.e., builds after a make clean).

Note that this fix follows the best practices for tracking dependencies automatically (through gcc -MD)

For more details, see here.
https://www.gnu.org/software/make/manual/html_node/Automatic-Prerequisites.html
```Building LuaJIT...
HOSTCC    host/minilua.o
HOSTLINK  host/minilua
DYNASM    host/buildvm_arch.h
HOSTCC    host/buildvm.o
HOSTCC    host/buildvm_asm.o
HOSTCC    host/buildvm_peobj.o
HOSTCC    host/buildvm_lib.o
HOSTCC    host/buildvm_fold.o
HOSTLINK  host/buildvm
BUILDVM   lj_vm.s
ASM       lj_vm.o
CC        lj_gc.o
BUILDVM   lj_ffdef.h
CC        lj_err.o
CC        lj_char.o
BUILDVM   lj_bcdef.h
CC        lj_bc.o
CC        lj_obj.o
CC        lj_str.o
CC        lj_tab.o
CC        lj_func.o
CC        lj_udata.o
CC        lj_meta.o
CC        lj_debug.o
CC        lj_state.o
CC        lj_dispatch.o
CC        lj_vmevent.o
CC        lj_vmmath.o
CC        lj_strscan.o
CC        lj_api.o
CC        lj_lex.o
CC        lj_parse.o
CC        lj_bcread.o
CC        lj_bcwrite.o
CC        lj_load.o
CC        lj_ir.o
CC        lj_opt_mem.o
BUILDVM   lj_folddef.h
make[1]: *** [lj_folddef.h] Segmentation fault: 11
make[1]: *** Deleting file `lj_folddef.h'
make: *** [deps/luajit/src/libluajit.a] Error 2```

I am on macOS 10.15.1 (19B88)

Is there any solution for this? Thank you.
Hello,

Are there plans to place some tags around versions for wrk2? Currently if you build master, you get a version number of 4.0.0. Should this be tagged? Could future changes also get versions tagged?
If OpenSSL is not in standard location, we can use it by setting OPENSSL_HOME environment variable
Does wrk2 support gRPC?
- Port wrk 4.1.0 changes
- cherry-pick important updates from wrk master branch post-4.1.0 release
One of the valid "nits" with wrk2 is that it can "over-report" latencies by up to 1msec because the rate-limiting model uses the call:
`    aeCreateTimeEvent(thread->loop, msec_to_wait, delay_request, c, NULL);`
to wait before sending a request if "its time has not yet come". Because of the 1msec resolution of the ae async framework's aeTimeEvent and aeCreateTimeEvent, this can end up "oversleeping" by up to a millisecond, which ends up looking like a server problem when it is actually a load generator problem. 

And the approach of "forgiving up to 1msec" is not a good one, as such an approach would miss real issues. IMO it is better to report pessimistic (could be somewhat worse than reality) latency numbers than ones that are better than reality.

But modern *nix variants can deal with clocks at a much finer resolution than 1msec (with e.g. nanosleep(), and [timerfd](http://man7.org/linux/man-pages/man2/timerfd_create.2.html)), and the events should really be using a much finer resolution (e.g. 10-20 usec would not be unreasonable).

The really cool code in ae.c and friends appear to have originated from redis, and have not been touched in "forever". I'd like to work to improve the basic aeTimeEvent in that framework to include microsecond resolution information, along with a configurable quantum for actual time event resolution chunking.

The approach I'd take would probably keep the current external APIs (e.g. aeCreateTimeEvent which takes a 1-msec-unit time parameter) and all the current fields in e.g. aeTimeEvent (including when_sec and when_ms), but add an additional when_usec field for the optional microseconds-within-the-millisecond amount (defaults to 0) that some APIs may supply. We would then add additional APIs for those who want finer resolution (e.g. aeCreateTimeEventUsec(), aeWaitUsec(), aeGetTimeUsec()). We would change the underlying implementations that currently populate and use struct timevals (like aeProcessEvents(), aeApiPoll()), which already supports microsecond resolution, to correctly populate and usec-resolution information, and will use a timerfd to support sub-millisecond-resolution timing in epoll_wait() rather than rely on the timeout parameter.

The benefit of all this will be much more timely wakeups for delayed requests and a less pessimistic reporting of sub-millisecond response time levels, and better per-thread handling when >1000/sec/thread requests rates are actually possible.
wrk2 was created in Nov. 2014 as an example of correcting coordinated omission in a load generator. It was basically a quick fork of wrk at the time,  with minimal changes needed to achieve the purpose, created by @giletene and @mikeb01 as a result of a quick conversation at QCon SF.

The project turned out to be way more popular than I thought, or than originally intended. Wrk seems like a very solid base, but people looking for constant-rate capabilities and proper (not susceptible to coordinated omission) latency measurement seem to have picked up wrk2.

But since we had not put any real work into maintaining or enhancing wrk2 over the years, I'm sure wrk has added quite a bit in the 5 years since that we should simply "catch up on".

One simple way to do this is to follow Vizzini's directive and "go back to the beginning". Since applying the changes to wrk 5 years ago was "fairly simple" and since we had not strayed very far from the original work done in 2014, we can just pick up the latest/greatest wrk2, and apply the same logical changes to it that we did back then, to get an "up to date" wrk2 with all the wrk goodies. It took me and @mikeb01 only a few days to do it the first time, and applying it again "should" be even shorter... ;-)

I would prefer that we do this before starting to add any new features from PRs that have accumulated over the years, and any new features we want to add that are wrk2-specific (e.g. I'd really like to add a .hlog output support, which has long been part of hdrhistogram_c). Once we apply those PRs and additions, catching up with wrk will involve much more work...

Does anyone out there want to volunteer to do this "catch up to latest wrk" work? 

This PR adds support for specifying, and for benchmarking,
multiple HTTP(S) endpoints in a single wrk2 run.

Our main motivation of running a benchmark over multiple endpoints
is to allow benchmarking of e.g. a whole web application instead
of the pages and/or restful resources that make up said
application individually.

Most of the heavy lifting is done in a LUA script, `multiple-endpoints.lua`
The script allows for specifying an arbitrary number of HTTP(S) endpoints
to include in the benchmark. Endpoints will be connected to in a random, evenly
distributed fashion. After a run finished, the overall latency will be reported
 (i.e. there's currently no break-down of latency per endpoint).

Furthermore, this PR introduces a change in wrk.c that will force a thread
to reconnect (i.e. close socket / open socket using current value of
`wrk.thread.addr`) each time `wrk.thread.addr` is set from a LUA script.

Lastly, the PR includes a patch by @janmejay to handle remote connection
close. @dongsupark identified this issue during our testing.

**Known Limitations** Please note that currently, benchmarking multiple endpoints requires threads == connections, as we close & reconnect as soon as a thread assigns `wrk.thread.addr`, which impedes ongoing async requests. There are a number of ways to remove this limitation; and we are actively investigating. However, we'd like to start getting early feedback on our direction, hence moved to create this PR with a known limitation.