|Wazuh version|Component|Install type|Install method|Platform|
|---|---|---|---|---|
| 3.10.2 | Centralized Configuration | Manager/Agent | Packages | Windows |

Hello,

I'm attempting to get the centralized configuration feature to work in our environment-- I'm able to get the agent.conf file properly pushed to the clients from the wazuh manager, but it appears that the agent.conf file isn't properly overwriting the ossec.conf file and applying the new settings. The changed I'm trying to remotely apply to all the agents is simply to add the whodata="yes" modifier to search locations for syscheck.

Despite the agent.conf file properly being pushed and synced to the clients, it doesn't appear that whodata is being sent via alerts to the manager. I've verified that the GPO on all devices is properly auditing SACL data, and in a test was able to produce the whodata after manually editing the ossec.conf file. But for some reason the agent.conf file with the whodata setting isn't taking precedence or overwriting the ossec.conf file.

[ossec.txt](https://github.com/wazuh/wazuh/files/4078644/ossec.txt)

[agent.txt](https://github.com/wazuh/wazuh/files/4078646/agent.txt)

Thanks for all your help!

|Wazuh version|Component|Install type|Install method|Platform|
|---|---|---|---|---|
| 3.11.1-1 | Vulnerability detector | Manager | Packages | RPM/Debian |

Hello team

  We noticed a non-expected behavior using a custom `msu.json` for our Vulnerability detector. After creating a new `msu.json` including the 2020 vulnerabilities, the Wazuh manager is not able to alert about the new vulnerabilities referring to new CVEs included in the new `msu.json`.

### Context
**Wazuh manager** 
Version: 3.11.1.
OS: Debian. 
Vulnerability configuration: 
```
 <vulnerability-detector>
    <enabled>yes</enabled>
    <interval>30s</interval>
    <ignore_time>1s</ignore_time>
    <run_on_start>yes</run_on_start>

    <provider name="canonical">
      <enabled>no</enabled>
      <os>precise</os>
      <os>trusty</os>
      <os>xenial</os>
      <os>bionic</os>
      <update_interval>1h</update_interval>
    </provider>

    <provider name="debian">
      <enabled>no</enabled>
      <os>wheezy</os>
      <os>stretch</os>
      <os>jessie</os>
      <os>buster</os>
      <update_interval>1h</update_interval>
    </provider>

    <provider name="redhat">
      <enabled>no</enabled>
      <update_from_year>2010</update_from_year>
      <update_interval>1h</update_interval>
    </provider>

    <provider name="nvd">
      <enabled>yes</enabled>
      <update_from_year>2010</update_from_year>
      <update_interval>1h</update_interval>
    </provider>

  </vulnerability-detector>
```
**Wazuh agent**
Version: 3.11.1.
OS: Windows Server 2016 with CryptoAPI by default. 
Configuration: all disabled except `syscollector` module as follow:
```
  <wodle name="syscollector">
    <disabled>no</disabled>
    <interval>1m</interval>
    <scan_on_start>yes</scan_on_start>
    <hardware>yes</hardware>
    <os>yes</os>
    <network>yes</network>
    <packages>yes</packages>
    <ports all="no">yes</ports>
    <processes>yes</processes>
  </wodle>
```

Since `feed.wazuh.com` is not ready yet, having the `msu.json` updated can be accomplished only with Wazuh upgrades (when available) or manual `msu.json` building. 

In this case, we wanted to detect the vulnerability `CVE-2020-0601`, which is not included in the default Wazuh v3.11.1 `msu.json`. For that, I built the attached [msu.json.zip](https://github.com/wazuh/wazuh/files/4077607/msu.zip). This file was built from 2010 vulnerabilities to current (2020 17th Jan). 

### Goal
Replacing the default `msu.json` Wazuh manager file by the attached one, we want to be able to alert when a Wazuh agent not updated has the vulnerability  `CVE-2020-0601`. 

### Steps
We stopped the Wazuh manager. We replaced the default `msu.json.gz` (`/var/ossec/queue/vulnerabilities/dictionaries/msu.json.gz`) by our custom attached file. 
After replacement, we restarted the manager with `wazuh_modules.debug=2` in `internal_options.conf`. We saw the following message: 

```
check_hotfix(): DEBUG: (5578): We have not found a hotfix that solves CVE-2020-0601 for agent 2 in the Microsoft feed, so it is not possible to know it is vulnerable.
```
This message is weird, because if we check the CVE database: 
```
root@808357e461f5:/var/ossec/queue/vulnerabilities# sqlite3 cve.db "select * from MSU where CVEID=\"CVE-2020-0601\"" | grep 2016
CVE-2020-0601|Windows Server 2016  (Server Core installation)|4534271|Windows CryptoAPI Spoofing Vulnerability|https://support.microsoft.com/help/4534271|||1
CVE-2020-0601|Windows Server 2016|4534271|Windows CryptoAPI Spoofing Vulnerability|https://support.microsoft.com/help/4534271|||1
```
CVE included in NVD_CVE database: 
```
root@808357e461f5:/var/ossec/queue/vulnerabilities# sqlite3 cve.db 
SQLite version 3.24.0 2018-06-04 19:24:41
Enter ".help" for usage hints.
sqlite>  select CVE_ID from NVD_CVE where CVE_ID = 'CVE-2020-0601';
CVE-2020-0601
```

In the msu.json:

```
          "CVE-2020-0601": [
                  ........
                  ........
               {
                    "patch": "4534271",
                    "product": "Windows Server 2016",
                    "restart_required": "",
                    "subtype": "",
                    "title": "Windows CryptoAPI Spoofing Vulnerability",
                    "url": "https://support.microsoft.com/help/4534271"
               },
               {
                    "patch": "4534271",
                    "product": "Windows Server 2016  (Server Core installation)",
                    "restart_required": "",
                    "subtype": "",
                    "title": "Windows CryptoAPI Spoofing Vulnerability",
                    "url": "https://support.microsoft.com/help/4534271"
               }
                  ........
                  ........
```
We tried removing the `cve.db` and the agent database. 
The Microsoft Catalog has the corresponding update available: 
![image](https://user-images.githubusercontent.com/3830293/72632225-26d9cc80-3956-11ea-859a-325b9b7259d4.png)
https://www.catalog.update.microsoft.com/Search.aspx?q=4534293
So, if the `cve.db` tables have the information, we don't understand why the alert is not produced. 

Best regards, 
Alberto R
|Related issue|
|---|
|#3640|

## Description

These are the unit tests for the new functions implemented in the PR #4256.
|Wazuh version|Component|Install type|Install method|Platform|
|---|---|---|---|---|
| 3.1x | API |  |  |  |

Description

We are working on a table to show the output of this request: `GET /syscheck/:agent_id`

When we tried to filter with `q` parameter these requests fails:
 
**Filter by mtime or date** (any operator):
```json
GET /syscheck/003
{
  "q":"mtime=2006-05-07 09:28:01"
}

{
  "error": 622,
  "message": "Param not valid. Review queries documentation: https://documentation.wazuh.com/current/user-manual/api/queries.html.  Field: q",
  "data": false
}

```

**File** When include slash:
```json
GET /syscheck/003
{
  "q":"file!=/usr/bin/c89-gcc"
}

{
  "error": 622,
  "message": "Param not valid. Review queries documentation: https://documentation.wazuh.com/current/user-manual/api/queries.html.  Field: q",
  "data": false
}
```

If possible we would need more parameters in the API for filter the requests by:

* `mtime`: a date or date range
* `date`: a date or date range
* `uname`: a user or list of users
* `uid`: an ID of user or list of IDs
* `gname`: a Group name or list of groups
* `gid`: an ID of group or list of IDs

On the other hand, we need some outputs with the list of unique values for these fields:

* `uname`
* `uid`
* `gname`
* `gid`
* `file`

Thanks and regards,
Jose



|Related issue|
|---|
|#4463|

<!--
This template reflects sections that must be included in new Pull requests.
Contributions from the community are really appreciated. If this is the case, please add the
"contribution" to properly track the Pull Request.

Please fill the table above. Feel free to extend it at your convenience.
-->

## Description

<!--
Add a clear description of how the problem has been solved.
-->

The fix is the one explained in #4463

## Tests

After applying the fix, the same test using `pympler` have being done in both master and worker nodes, with similar results:
```
  types |   # objects |   total size
======= | =========== | ============
```

From time to time, other objects (timer.Handler) appeared but after some seconds they were deleted, so it shows the memory leak is gone.

Test stages:
- [x] Same information for tag and subtags shown in `cluster.log` for both master and worker
- [x] No errors in `cluster.log` for both master and worker
<!--
Depending on the affected components by this PR, the following checks should be selected and marked.
-->

## Description

`wazuh-clusterd` may increase memory usage over time due to inappropriate setting of loggers and filters. 

The problem comes from a continuous creation of loggers and filters (`wazuh.ClusterFilter`) when processing local_requests in the `Handler`class in the following block of code:
https://github.com/wazuh/wazuh/blob/5a6f81a0c7629ca8a8ae3e780fe1a69bf506b694/framework/wazuh/cluster/common.py#L157-L162

Due to the following code line in the `AbstractServerHandler` class, the previous lines are going to continously create new logger childs (`logger.getChild(tag)`) up until the maximum `random.randint(0, 1000))`.
https://github.com/wazuh/wazuh/blob/5a6f81a0c7629ca8a8ae3e780fe1a69bf506b694/framework/wazuh/cluster/server.py#L32

Also, due to `self.logger.addFilter(self.logger_filter)`, every new local request using a `Handler` will create a `ClusterFilter` object and add it to the logger, potentially accumulating 2^20 `ClusterFilter` objects in the memory as we can see in the following code:
https://github.com/wazuh/wazuh/blob/5a6f81a0c7629ca8a8ae3e780fe1a69bf506b694/framework/wazuh/cluster/local_server.py#L27-L31

## Fix
We have setup two context variables in order to set the tag and subtag for the logger filter based on the context.
```
# Context vars
context_tag: ContextVar[str] = ContextVar('tag', default='')
context_subtag: ContextVar[str] = ContextVar('subtag', default='')
```
This way, we no longer get a new `logger` neither use `addfilter` when we are in the `Handler` class constructor, but rather use the main one and setup the appropriate context for the filter to use. Such as:
```
        # logging.Logger object used to write logs
        self.logger = logging.getLogger('wazuh')
        # logging tag
        self.tag = tag
        # modify filter tags with context vars
        cluster.context_tag.set(self.tag)
        cluster.context_subtag.set("Main")
```
With the new filter in ClusterFilter class:
```
    def filter(self, record):
        record.tag = context_tag.get() if context_tag.get() != '' else self.tag
        record.subtag = context_subtag.get() if context_subtag.get() != '' else self.subtag
        return True
```

## How to reproduce the bug
SImilarly to the research done in #4444 by @crd1985, we have used `pympler` library in order to get the incremental # of objects between sync operations in the cluster, while sending a local request petition (cluster socket in `/var/ossec/queue/cluster/c-internal.sock`) every 2 seconds (sync happens in a 10 seconds cycle). Leading to:
```
                                types |   # objects |   total size
===================================== | =========== | ============
                                 dict |          15 |      2.46 KB
                                 list |          10 |    984     B
                                  str |           5 |    389     B
  wazuh.cluster.cluster.ClusterFilter |           5 |    280     B
                       logging.Logger |           5 |    280     B
```
Also, we have further stressed the cluster with thousands of petitions and checked the summary of objects that are retained in the memory after we stop:
```
                                        types |   # objects |   total size
============================================= | =========== | ============
          wazuh.cluster.cluster.ClusterFilter |      106055 |      5.66 MB
                               logging.Logger |        1020 |     55.78 KB
```

## Description

This PR removes function `del_plist` from Rootcheck which is duplicate. Instead of using it, the function `w_del_plist` will be used.

This PR must be tested on Windows and Linux(agent or manager, not relevant). It must be ensured that Rootcheck scan works correctly.

## Tests

<!-- Minimum checks required -->
- Compilation without warnings in every supported platform
  - [x] Linux
  - [x] Windows
  - [ ] MAC OS X
- [x] Source installation

<!-- Checks for huge PRs that affect the product more generally -->
- [x] Rootcheck scan works

|Related issue|
|---|
|https://github.com/wazuh/wazuh-kibana-app/issues/2004|

## Description
This PR adds two new digests to the Filebeat module to generate the geolocation information of Guarddutty alerts.

Result:
![image](https://user-images.githubusercontent.com/3064506/72523979-3aefd200-3861-11ea-9396-4186c3cb1f1c.png)

## Tests

<!--
Depending on the affected components by this PR, the following checks should be selected and marked.
-->

<!-- Minimum checks required -->
- Compilation without warnings in every supported platform
  - [ ] Linux
  - [ ] Windows
  - [ ] MAC OS X
- [ ] Source installation
- [ ] Package installation
- [ ] Source upgrade
- [ ] Package upgrade
- [ ] Review logs syntax and correct language
- [ ] QA templates contemplate the added capabilities

<!-- Depending on the affected OS -->
- Memory tests for Linux
  - [ ] Scan-build report
  - [ ] Coverity
  - [ ] Valgrind (memcheck and descriptor leaks check)
  - [ ] Dr. Memory
  - [ ] AddressSanitizer
- Memory tests for Windows
  - [ ] Scan-build report
  - [ ] Coverity
  - [ ] Dr. Memory
- Memory tests for macOS
  - [ ] Scan-build report
  - [ ] Leaks
  - [ ] AddressSanitizer

<!-- Checks for huge PRs that affect the product more generally -->
- [ ] Retrocompatibility with older Wazuh versions
- [ ] Working on cluster environments
- [ ] Configuration on demand reports new parameters
- [x] The data flow works as expected (agent-manager-api-app)
- [ ] Added unit tests (for new features)
- [ ] Stress test for affected components
## Description
Currently, there are two main ways to upgrade an agent:
- `/var/ossec/bin/agent_upgrade`
- API endpoint PUT`/agents/:agent_id/upgrade_custom` y PUT`/agents/:agent_id/upgrade`

The API runs as `ossec` user by default, so a problem arises if the `agent_upgrade` utility is executed as root and then an API endpoint:
```
WazuhAPI 2020-01-14 09:37:44 WazAPI: 10.55.65.16 PUT /agents/:agent_id/upgrade
WazuhAPI 2020-01-14 09:37:44 WazAPI: CMD - Command: /var/ossec/framework/python/bin/python3 args:/u00/wazuh/api/models/wazuh-api.py stdin:{"function":"PUT/agents/:agent_id/upgrade","arguments":{"agent_id":"128","wait_for_complete":false}}
WazuhAPI 2020-01-14 09:37:45 WazAPI: CMD - Exit code: 0
WazuhAPI 2020-01-14 09:37:45 WazAPI: CMD - STDOUT:
---
{"message": "[Errno 13] Permission denied: '/var/ossec/var/upgrade/wazuh_agent_v3.11.1_windows.wpk'", "error": 1000}
---
WazuhAPI 2020-01-14 09:37:45 WazAPI: CMD - STDOUT: 117 bytes
WazuhAPI 2020-01-14 09:37:45 WazAPI: [Errno 13] Permission denied: '/var/ossec/var/upgrade/wazuh_agent_v3.11.1_windows.wpk'
WazuhAPI 2020-01-14 09:37:45 WazAPI: [10.55.65.16] PUT /agents/128/upgrade - 200 - error: '1000'.

[root@host wazuh]# ll /var/ossec/var/upgrade/
total 2936
-rw-r-----. 1 root root 3004061 Jan 10 08:12 wazuh_agent_v3.11.1_windows.wpk
```

## Fix
The wpk file is created here:
https://github.com/wazuh/wazuh/blob/fa2272a6a964be5d1ecc5abc23e0410f5eb3ca56/framework/wazuh/agent.py#L1961-L1964
After that, some code must be added in order to set file permissions in the following way: `-rw-rw---- ossec:ossec`
|Wazuh version|Component|Install type|Install method|Platform|
|---|---|---|---|---|
|3.11.1-1 | Remoted/Analysisd/db | Manager| Any | Any |

Hello team,

While testing 3.11 which allows registering up to 100000 agents, I found out that as soon as you "register" more than 65528 agents remoted crashes, and that due to the number of file descriptor that can open by default (it seems to be like one file descriptor for each agent registered) :

```
 # Maximum number of file descriptor that Remoted can open [1024..1048576]
remoted.rlimit_nofile=65536
```

![Crashremoted](https://user-images.githubusercontent.com/23264276/72376914-1388f000-370f-11ea-8e47-4377e822ae02.gif)

It is so probable that the same thing will happen to` analysisd` and other daemons as they all have default values in the internal options.

Increasing the `remoted.rlimit_nofile` solved the issue for `remoted`.

We made a PR here: https://github.com/wazuh/wazuh/pull/4332

However, we think that this should not be normal behavior and should be fixed at the design level.

Regards,

Miguel Casares