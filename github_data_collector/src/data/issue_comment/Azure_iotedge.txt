We are adding a test for EdgeAgent direct method "ping" method to Connectivity tests.

We won't have a ReceiverSource for DirectMethod in that case. So this PR makes ReceiverSource and ReceiverTestResults Optional. 

It also changes the implementation of the DMReportGeneration to accomodate when these Optional values are not filled in.
I have and IoT Edge module which listens for Module Twin changes, Connection Status changes, and Direct Module Method calls. When any of those callbacks happen, the event is printed to the module log with a timestamp.

In the web portal, at the device level, there is an option to enable/disable the device's connection to the IoT Hub.
![image](https://user-images.githubusercontent.com/11834450/72638343-dbf59000-3928-11ea-8d35-995dce8ce42e.png)

With the edge device in the enabled state, I can use the web portal to send Module Twin changes and Direct method calls, both of which are seen my my running module on that device. When I use the web portal and change my Edge device's "Enable connection to IoT Hub" setting from Enabled to Disabled and click the Save button, that change does not get propagated to the actual device for about 5 minutes. This means that I can still use the web portal to send further module twin changes and direct method calls even though the device is listed as "disabled". Is this typical? After approximately 5 minutes, the further attempts to send Module Twin changes or Direct method calls are no longer seen by my module. Is there any way to make that faster?

Furthermore, if I then use the web portal to change my Edge device's "Enable connection to IoT Hub" setting from Disabled to Enabled and click the Save button, that change does not get propagated to the actual device until the next time the module gets the next 

``` 
ConnectionStatusCb(status=IOTHUB_CLIENT_CONNECTION_UNAUTHENTICATED, reason=IOTHUB_CLIENT_CONNECTION_EXPIRED_SAS_TOKEN)
```

which seems to be every 48 minutes! Until then, all further attempts to update the Module Twin 
 or send a Direct Module Method via the web portal are not seen by the module. That's a very long time to wait for what what the web portal shows as an "enabled" device. Is this expected? Is there any way to make that faster? After the next

```
ConnectionStatusCb(status=IOTHUB_CLIENT_CONNECTION_AUTHENTICATED, reason=IOTHUB_CLIENT_CONNECTION_OK)
```
a single partial Module Twin change finally comes through, containing the delta between last online state and the state after re-connection (in other words, if the Module Twin had a field A that went from value 1 to value 2 and then back to value 1, then field A is not communicated in the partial update.

Related to the questions above, when a device is marked as Enabled on the portal, but the actual device hasn't yet reactivated (which, again, could be up to 48 minutes), there doesn't appear to be anything in either the Device Twin or the Module Twin that is visible in the portal to indicate that the pending changes have not yet been synchronized. Is there some way to see that from the cloud side?

Add developer docs for new features in IoT Edge:

* Module processing priority
* Edge Hub database store size limiting
* System modules in-memory database store backup and restore
Add network controller report and generator; will enable in another PR.
## Expected Behavior
With container create option,
"HostConfig": {
…
"Init": true
…
}

Should create container with option "--init=true" which "Run an init inside the container that forwards signals and reaps processes"

## Current Behavior
IoT Hub will report back container create option,
"HostConfig": {
…
"Init": true
…
}
but container will not be created with that parameter.  This is reflected by no "tini" process in the container. And, docker inspect not showing "Init": true.


## Steps to Reproduce
Provide a detailed set of steps to reproduce the bug.
1. Create Iot Hub Edge Module
2. Set desired container create options to 
"HostConfig": {
…
"Init": true
…
}
3. Wait until it reports back
"HostConfig": {
…
"Init": true
…
}
4. Run docker inspect on the container.  Notice it does not have "Init": true.

### Device Information
* Host OS [Ubuntu 18.04]: 
* Architecture [amd64]: 
* Container OS [Linux containers]: 

### Runtime Versions
* iotedged [1.0.8 (208b2204fd30e856d00b280112422130c104b9f0)]: 
* Edge Agent [1.0]: 
* Edge Hub [1.0]: 
* Docker/Moby [19.03.5]: 


Currently, NetworkController (NC) does not retry on any of its calls. 

This is a problem when NC starts up before TestResultCoordinator (TRC).
When NC starts up, it immediately sends two TestOperationResult's to TRC, one that says Disabling and another that says Disabled.

These will fail if the TRC is not up yet, and we will not know the NCStatus for the beginning of the run. 

By adding retries, NC will be able to successfully report to TRC once TRC is up.
Works in portal but fails in console app.

```C#
        static void Main(string[] args)
        {
            ModuleClient modClient = ModuleClient.CreateFromConnectionString(_moduleConnStr, TransportType.Mqtt);

            Task.Run(async () => await InvokeMethod(modClient));

            Console.ReadKey();
        }

        static async Task<MethodResponse> InvokeMethod(ModuleClient moduleClient)
        {
            try
            {
                await moduleClient.OpenAsync();


                // Create the request
                MethodRequest request = new MethodRequest(_method, Encoding.UTF8.GetBytes("{ \"Message\": \"Test\" }"));

                // Execute request
                var resp = await moduleClient.InvokeMethodAsync(_deviceId, _moduleId, request);

                // Write response to log for debugging
                Console.WriteLine($"Got result {resp.ResultAsJson} with status code {resp.Status}");

                return resp;
            }
            catch (Exception ex)
            {
                Console.WriteLine(ex.Message);
                Console.WriteLine(ex.StackTrace);
                return await Task.FromResult(new MethodResponse(500));
            }
        }
```
Hi,

As in the subject I have my IOTEdge set up on Windows Server. In portal device runtime configuration I have "mcr.microsoft.com/azureiotedge-hub:1.0.7.1".
I am using edge as transparent gateway. Messages from downstream devices are going through but when I launch "iotedge check" in Powershell I get the following error:

Configuration checks
--------------------
√ config.yaml is well-formed - OK
√ config.yaml has well-formed connection string - OK
× container engine is installed and functional - Error
    Could not communicate with container engine at npipe://./pipe/iotedge_moby_engine.
    Please check your moby-engine installation and ensure the service is running.
    You might need to run this command as Administrator.

2 check(s) succeeded.
1 check(s) raised errors. Re-run with --verbose for more details.

Should I be worried? What may be the cause of this issue?
