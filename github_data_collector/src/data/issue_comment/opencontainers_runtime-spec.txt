It's been a couple of years and I have no idea what I am doing.
Some of this could be automated, but for now I've included some of the
shell that gets close to the process.

Signed-off-by: Vincent Batts <vbatts@hashbangbash.com>

![image](https://user-images.githubusercontent.com/67049/72085624-9d7e2680-32d3-11ea-83b9-859da2422fac.png)

Fixes #1010 

VOTE: https://groups.google.com/a/opencontainers.org/d/msg/dev/lYFhr-kmSAQ/9QmAI6yVDAAJ

PRs since v1.0.1:
e09c7c4 Merge pull request #1008 from RenaudWasTaken/hooks
020fda7 Merge pull request #1024 from giuseppe/add-personality-schema
bab266e Merge pull request #898 from 0x0916/fix-weight-type
2d3f265 Merge pull request #903 from wking/state-version-source
903de9f Merge pull request #916 from wking/unknown-properties-one-must
68f17f5 Merge pull request #1012 from justincormack/personality
8b180f3 Merge pull request #985 from masters-of-cats/master
cd13d2d Merge pull request #941 from cyphar/config-umask-option
19e92ca Merge pull request #1019 from pjbgf/add-act-log
52e2591 Merge pull request #1018 from giuseppe/seccomp-flags
4f2ab15 Merge pull request #1017 from giuseppe/fix-crun-repository
c9a5f61 Merge pull request #1016 from estesp/update-meetings
a950415 Merge pull request #1013 from odinuge/hugetlb-pattern-fix
7a49e34 Merge pull request #1011 from odinuge/hugetlb-pattern
8ffda14 Merge pull request #1009 from KentaTada/Fix-sample-ociVersion
5b71a03 Merge pull request #1007 from KentaTada/Fix-Namespaces-to-use-LinuxNamespaceType
a1b50f6 Merge pull request #1006 from lifubang/pidnamespace
ad53dcd Merge pull request #999 from jterry75/layerfolder_order
197975d Merge pull request #1001 from vsoch/update/CoC-link
29686db Merge pull request #998 from jhowardmsft/jjh/commandline
1722abf Merge pull request #992 from xiaochenshen/rdt-mba-software-controller
31e0d16 Merge pull request #994 from q384566678/vm-parameters-fix
038448f Merge pull request #993 from HaraldNordgren/go_versions
5806c35 Merge pull request #981 from kinvolk/alban/source-mount
5684b8a Merge pull request #932 from xiaochenshen/rdt-mba
cc07cb9 Merge pull request #988 from linericyang/master
eba862d Merge pull request #989 from dineshgovindasamy/windowsnamespace
d810dbc Merge pull request #976 from cwilhit/master
34fd552 Merge pull request #977 from wking/2018-07-04-meeting-bump
7c4c8f6 Merge pull request #969 from jodh-intel/add-kata-containers-to-implementations
695b816 Merge pull request #965 from wking/console-man-page
16d4c82 Merge pull request #970 from opencontainers/add-gvisor
c102f09 Merge pull request #968 from wking/bump-go
a1998ec Merge pull request #953 from kinvolk/alban/hook-paths
b4a682c Merge pull request #952 from ntrrg/patch-1
3234dc8 Merge pull request #959 from kinvolk/alban/957-device-owners
c09d7f4 Merge pull request #951 from wking/json-from-rfc-8259
4ad8e74 Merge pull request #943 from wking/2018-jan-meeting
d362ed3 Merge pull request #949 from sameo/vm-section
6be516e Merge pull request #956 from kinvolk/alban/uid-mapping
fa4b36a Merge pull request #942 from paravmellanox/master
7a8c2c8 Merge pull request #947 from q384566678/fix-makefile-test
520c634 Merge pull request #945 from wking/json-schema-minimal-ids
b2d941e Merge pull request #940 from q384566678/defaultAction-fix
75c847a Merge pull request #933 from q384566678/hook-link
a5b8598 Merge pull request #936 from wking/root-dedent
Signed-off-by: zhouhao <zhouhao@cn.fujitsu.com>

Carrying over a rebased #931
In working with the runtime-spec and implementing a system that uses OCI [hooks](https://github.com/opencontainers/runtime-spec/blob/v1.0.1/config.md#posix-platform-hooks) in the runtime spec, a number of performance issues came to bear that required the development of a system / model for conditionally filtering which hooks are set/present in a runtime spec. One such system is specified in the [libpod hooks spec](https://github.com/containers/libpod/tree/v0.6.2/pkg/hooks)

Additionally, there exists a desire by the kubernetes SIG-Node team, and associated device teams to have a common means for administering / enabling conditional runtime spec hooks. This desire comes from a need to use hooks for certain types of devices in a common way for CRI integrations. Eg. it would be good for have a common device hook injection pattern for CRIO, containerd, ...

The model proposed, as defined by the CRIO team in building the above libpod hooks package, involves having oci hooks specs configured for being conditionally injected when pre-specified condition(s) occur.

This proposed (and already in production) extension to the runtime-spec "provides a way for users to configure the intended hooks for Open Container Initiative containers so they will only be executed for containers that need their functionality, and then only for the stages where they're needed." Stages meaning prestart, poststop, etc. see [hooks](https://github.com/opencontainers/runtime-spec/blob/v1.0.1/config.md#posix-platform-hooks)

```
type When struct {
	Always        *bool             `json:"always,omitempty"`
	Annotations   map[string]string `json:"annotations,omitempty"`
	Commands      []string          `json:"commands,omitempty"`
	HasBindMounts *bool             `json:"hasBindMounts,omitempty"`
}
```

As shown the conditions may be based on pod/container annotations that are passed down through the CRI that act as a flag to enable/disable predefined hook specs.

It is suggested that libpod's conditional hooks package be moved into this repo in some form, at least for use as common tooling for conditionally enabling oci hooks. Without having a common means for enabling oci-hooks the effort to users and vendors of devices needing said hooks would be prohibitive.
https://github.com/opencontainers/runtime-spec/blob/master/config-linux.md#cgroups-path

The systemd cgroup path convention implemented in runc/crun should be added to the spec.

This convention is becoming important for cgroup v2, because rootless containers depends on systemd for cgroup delegation in most environments.
There are cases where it would be necessary to skip the `setgroups(2)` syscall so that the original additional groups can be maintained.

It can be used, for example, by rootless containers to keep access to a storage directory that is accessible only by a secondary group.

runc already skips the `setgroups` in some cases: either if the user had euid != 0 or if `/proc/self/setgroups` is set to `deny`.  I'd like to add a third condition where the `setgroups` is skipped also if explicitly requested.

Do we need a new field under [`process/user`](https://github.com/opencontainers/runtime-spec/blob/master/config.md#user), e.g. `keepOriginalGroups`?  Would be enough to reuse `additionalGids` to have some special value (e.g. `-1` to keep current groups)?

weight, leafWeight, and weightDevice are removed in kernel 5.0

https://github.com/torvalds/linux/commit/f382fb0bcef4c37dc049e9f6963e3baf204d815c
Hi there,

Do you think it would be possible to tag a new release according to semver? Some go module based dependencies have to rewrite their go.mod to get the latest features in. :innocent:  Thanks in advance! :)
This is a feature request within the scope of #1002. I decided to open a separate issue so we can have a discussion on one specific controller and I found the memory controller to be a very interesting one for this discussion.

The point of this discussion is to figure out how to support the new features that are coming in cgroupv2 (including a closer look at the motivations to adopt these features) and what to do about the controls that are going away and being removed in cgroupv2 (together with a look at why these are going away and what was wrong with them.)

So let's start looking at specific parts of the standard.

## Memory Limits

The memory controller in cgroupv2 exposes [these controls](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/Documentation/admin-guide/cgroup-v2.rst?h=v5.0#n1001), of which the following are available to control memory limits:

 * `memory.low`: **Best-effort memory protection.** This is similar to cgroupv1's `memory.soft_limit_in_bytes`, but better. It's reasonable to map OCI's `memory.reservation` to this control, since it's close in meaning.
 * `memory.high`: **Memory usage throttle limit.** Once memory usage goes above this limit, the container will start getting memory pressure (even if the host itself is not under memory pressure.) In practical terms, when processes try to allocate memory, the kernel will first go into reclaim and try to free memory (from this container) before giving them memory. But note that **going over the high limit never invokes the OOM killer**, which means under extreme conditions the limit may be breached, but will not cause OOMs. This limit has no counterpart in cgroupv1.
 * `memory.max`: **Memory usage hard limit.** Hard limit, once this one is reached, if reclaim can't shrink memory usage of this cgroup, then a container OOM will happen here. OCI's `memory.limit` typically wouold map to this control, since it's the one with the closest semantics (including OOMing if necessary.)
 * `memory.min`: **Hard memory protection.** This is a fairly new control and it sets a limit under which pages from the container will not be reclaimed, even if the host itself is under memory pressure. This is useful for guaranteed reservations, for some workloads that should not be affected (or have reduced impact) in an oversubscribed host.

### High Limit

The current high limit in cgroupv1 (and current OCI spec) is undesirable in that it allows OOMs to happen. As a consequence, some container managers avoid using it. For example, Kubernetes in many cases will not not set a high limit, instead it will monitor memory usage and decide to evict containers to prevent host OOMs.

That's not really great, since container managers would like to control memory pressure on containers and tell the kernel to try to shrink them, just the side effect of OOMing it is undesirable in most cases (the container manager still would prefer to make that kind of decision.)

The new control `memory.high` is a much more useful than cgroupv1's `memory.limit_in_bytes` (which is actually still available as `memory.max` in cgroupv2.) There would be a great advantage to exposing it as part of the OCI.

### Reservation (Low Limit)

While cgroupv2 offers a new tunable `memory.min` (for a hard guarantee), it seems `memory.low` is pretty close to the behavior that container managers want to control.

So it seems that cgroupv2 (through `memory.low`) and cgroupv1 (through `memory.soft_limit_in_bytes`) are offering the same control here. But that's really not the case, since cgroupv1's `memory.soft_limit_in_bytes` has a number of issues that have been fixed in cgroupv2. The biggest issue with `memory.soft_limit_in_bytes` is that it's not really hierarchical (doesn't "borrow" from a reservation of the parent cgroup), which makes subtree delegation not really viable (since a manager for a subtree could request an arbitrarily high soft limit if they wanted to.) `memory.low` fixes that. See [issues with memory cgroupv1](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/Documentation/admin-guide/cgroup-v2.rst?h=v5.0#n2268) for more details and more of these issues.

## OOMs

The memory controller in cgroupv2 also exposes [this control](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/Documentation/admin-guide/cgroup-v2.rst?h=v5.0#n1001) for controlling container OOMs:

 * `memory.oom.group`: **Determines whether the cgroup should be treated as an indivisible workload by the OOM killer.** If set, all tasks belonging to the cgroup are killed together or not at all. This can be used to avoid partial kills to guarantee workload integrity.

This is a great new control, that's unavailable in cgroupv1. When OOMs happen, killing a whole container (eviction) is often what the container manager wants, so such a setting from the cgroup subsystem is quite useful.

### Disabling OOMs

cgroupv1 has `memory.oom_control`, which allows disabling OOMs for a container and is no longer present in cgroupv2. This will be addressed in the next section, about removed controls.

## Removed Controls

These three controls, which are encoded in the OCI, are no longer available in cgroupv2:
 * `memory.kmem.limit_in_bytes` and `memory.kmem.tcp.limit_in_bytes`: **Hard limit for kernel and tcp buffer memory, respectively.** These can be used to set a separate limit for kernel memory usage only.
 * `memory.swappiness`: **Swappiness parameter of vmscan.** Similar to the global `vm.swappiness` sysctl, but per container.
 * `memory.oom_control`: **Disable OOMs.** This control has some other uses in notifications, but writing it is used to disable OOMs for a specific cgroup.

I asked @htejun about these and here is some rationale on why they were removed (and ought to have been removed.) Below are some comments on each of those.

### Kernel Memory Limits (`kmem` and `kmem.tcp`)

Accounting and limiting kernel memory separately has a number of issues, mostly arising from different memory usages siloed into separate buckets.

Tejun: "In cgroup2, all significant memory usages are accounted and controlled together by default. There’s no distinction between user, kernel or network memories. **Memories are memories and they’re handled the same way.**"

I guess another way to look at this is considering the history of memory controller in cgroupv1. First only user memory accounting was available, because it was the easiest to measure and was quite useful. The kernel limits required deeper changes and were introduced later. For quite a few kernel versions, kernel memory accounting was buggy and/or problematic (so it was even possible to not enable accounting, by not setting any limit, even an arbitrary one.) That's all behind us at this point, so we should just look at a single memory limit per container and not worry about whether that's used in userspace or kernel.

The two OCI configurations `memory.kernel` and `memory.kernelTCP` set those limits. I would propose that a reasonable way to handle these on a cgroupv2 system is to **simply ignore them**. Since the other counters will include kernel memory, limits for that memory usage will be implicitly set, so using the new controls only makes sense here.

### Swappiness

Tejun: "It’s not very clear what swappiness encodes. A lot of it is compared to file-backed IOs, how [un]favorable IOs for anonymous memory are considering their inherently higher randomness. As such, it’s more a function of the underlying hardware than workloads. Also, the implementation wasn’t quite right either – iirc, the behavior would differ depending on who’s reclaiming."

I guess the only setting that makes sense here is *disabling swap altogether* and that setting can be better achieved by setting `memory.swap.max`.

The current OCI specification includes a `memory.swappiness` setting for this control. For backwards compatibility, we can look into possibly translating (or more specifically, reconcyling it with `memory.swap`) on the corner cases that try to disable swap for a specific container using this setting.

### Disabling OOMs

Tejun: "In cgroup1, userspace could block kernel oom actions. This puts the victims in a completely new blocked state and can and often did lead to deadlock conditions as it was extending the dependency chain for the forward progress guarantee out to userspace."

Tejun: "In cgroup2, userspace can’t block kernel from making forward progress. Instead, kernel provides metrics to measure resource pressures (PSI) allowing userspace agent to detect and remediate memory contention issues way before kernel OOM condition is reached. Instead of blocking cgroup which is running out of memory completely, the kernel slows them down which is reflected in PSI so that userspace can handle the situation. This way, userspace has way earlier warnings and kernel isn’t blocked on guaranteeing forward progress."

So, clearly, the corner cases of this setting are pretty problematic...

There is a separate way to control OOM behavior in the kernel, [oom_score_adj](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/Documentation/filesystems/proc.txt?h=v5.0#n1498) which can be set to -1000 to disable OOM killer on a specific process. That has slightly different behavior than the cgroupv1 option (furthermore, it's set by process and not by cgroup), but it could potentially be leveraged for the same purpose.

OCI includes a `memory.disableOOMKiller` setting to disable the OOM killer. Perhaps translating that setting into using `oom_score_adj` instead could be a path towards deprecating this option.

## Final Thoughts

The memory controller is a very interesting case study into migration to cgroupv2, perhaps because of how *useful* the new controls are that exist only in v2. In particular, `memory.high` would be immediately useful to container managers such as Kubernetes.

There is the problem of the settings that were removed (in the sense that these were exposed in the OCI and we need to figure out how to best deprecate or adapt them.) Hopefully this issue makes a good case of *why* these are gone, why their being gone is justified (as they were not well enough defined, not properly hierarchical, or mainly there for historic reasons) and moving forward with deprecating the OCI fields is a good idea.

Let's use this issue for a discussion on how to best adopt cgroupv2 (I believe a decision on how to handle the memory controller will quite possibly easily end up applying to other controllers too.)

I talked to @vbatts about bringing this up on a OCI call, planing to do so on the upcoming meeting on Wednesday, March 13th.

Thanks!
Filipe

Obsoletes #883
Obsoletes #995

Signed-off-by: Vincent Batts <vbatts@hashbangbash.com>