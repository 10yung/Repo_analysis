**Describe the bug**
I am trying to execute a Taskflow using dynamic tasking multiple times, but the number of dynamic tasks created varies between executions. Essentially I am traversing a tree starting from the root, each node is visited as a single task and creates sub-tasks for all children using `tf::Subflow`. The tree is traversed multiple times but it changes after each traversal. My code does what it should, i.e. the traversal works, however upon using `tf::ExecutorObserver` to inspect the execution, I get broken data. It shows that child nodes are sometimes processed before parent nodes, which should be impossible because the parent node creates the task for the child node during its execution. Also, the names of the tasks are sometimes corrupted. 

I created a small test project illustrating the problem here: https://github.com/Mortano/taskflow_test 

Is running the same `tf::Taskflow` object with a subflow graph that changes between executions not supported? 

**To Reproduce**
Run the example project here: https://github.com/Mortano/taskflow_test 

**Desktop (please complete the following information):**
 - OS: Ubuntu 18.04, compiling with gcc 7.4.0, using v2.2.0 of cpp-taskflow

This eliminates quite some warnings for MSVC++.
**Describe the bug**
Testcase ParallelForOnIndex fails on Solaris:

```
[doctest] doctest version is "2.3.1"
[doctest] run with "--help" for options
===============================================================================
../unittest/taskflow.cpp:581:
TEST CASE:  ParallelForOnIndex
  PositiveFloatingStep

../unittest/taskflow.cpp:679: FATAL ERROR: REQUIRE( n == counter ) is NOT correct!
  values: REQUIRE( 3 == 2 )

===============================================================================
[doctest] test cases:      1 |      0 passed |      1 failed |     14 skipped
[doctest] assertions:  12424 |  12423 passed |      1 failed |
[doctest] Status: FAILURE!
```

**To Reproduce**
Steps to reproduce the behavior:
1. Compile.
2. Test.
3. See error

**Desktop (please complete the following information):**
 - OS: SunOS solaris-asus 5.11 omnios-r151032-702376803e i86pc i386 i86pc
 - Compiler: gcc (OmniOS 151032/8.3.0-il-0) 8.3.0
It is no longer possible to run a taskflow with zero workers, i.e. on the thread that calls
`executor.run(taskflow)`

I have a workload where I need to run some of my taskflows on a specific thread (the main thread). Prior to this commit https://github.com/cpp-taskflow/cpp-taskflow/commit/58d96bb3c9e3a0fad9174c03b1dbcbca567c7fd3 it was possible by passing the taskflow to an executor with zero workers, but now it throws an exception.
`if(_workers.size() == 0) {
    TF_THROW(Error::EXECUTOR, "no workers to execute the graph");
  }`

Is there another way to run a taskflow on a specific thread that I'm not aware of?
All implementations TBB, OpenMP and CppTaskflow exhibits very strange performance profile on the Black & Scholes benchmark.

## Problem 1

10000 rounds take about 30s for TBB and OpenMP and 1min30 for CPP-Taskflow.
I've triple-checked if maybe I was not using release mode or if there was something else and I'm at a loss.

This is the VTune Amplifier summary of all implementations, note that all implementation are spending most of their time waiting at a barrier or busy spinning.

### TBB

![image](https://user-images.githubusercontent.com/22738317/70354766-bfb5fc00-1870-11ea-9386-174eb9a4bccd.png)

### OpenMP

![image](https://user-images.githubusercontent.com/22738317/70354839-ed02aa00-1870-11ea-8b15-a3e669228878.png)

### CppTaskflow

![image](https://user-images.githubusercontent.com/22738317/70354885-0efc2c80-1871-11ea-8511-c9b1b09be698.png)

### System

i9-9980XE 18 physical cores, 36 logical. Overclocked at 4.1GHz (all-cores turbo)

## Problem 2

While TBB uses 99.8% of my CPU on this benchmark, Cpp-Taskflow only uses about 30% of each core.

## Alternative implementation

I've done an alternative implementation in the Nim language + OpenMP in [my own runtime repo](https://github.com/mratsim/weave/blob/master/benchmarks/black_scholes/openmp_black_scholes.nim).
Sequential execution is 3.5s-4s and parallel execution around 300-350ms and the output are the same.

So the current benchmark are about 100x slower than expected.

I was thinking that maybe there was some false sharing but it seems like the arrays are padded by 4x times the cache line size:

https://github.com/cpp-taskflow/cpp-taskflow/blob/5d1b0e3dd84a64343184ead413aed17335995eb2/benchmark/black_scholes/common.hpp#L264-L276
**Please describe your feature request.**
Is there any way to support asynchronous task?
Use case could be a workflow with several tasks which issue requests over network, and each task is asynchronously executed.


**Please describe your feature request.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

continous running every 1 minute or 30 second ?

**Please describe your feature request.**
I am unsure how to unite taskflow graphs into a main taskflow graph easily when calling functions that themselves have their own taskflow graphs.

My issue likely comes from a shallow understanding of the nature of task-based parallelism and lambda functions. I am writing a program with a taskflow graph. Within that program, I would like to call functions from a separate header which themselves have taskflow graphs within their scopes. What is the easiest way to include taskflow graphs from within functions into a main taskflow graph?  To maintain compatibility, I would like tasks within called functions to work only with variables in the function scope.

Here is an example with main() calling foo() and bar():
[function_scope.zip](https://github.com/cpp-taskflow/cpp-taskflow/files/3849423/function_scope.zip)

**Please describe your feature request.**
It would be nice if cpp-taskflow could provide a pkg-config file to make discovery and consumption of this project easier in non-cmake build systems such as autotools, make, waf, meson, build2 et.c.


Reading the research paper I noticed https://github.com/fastflow/fastflow wasn't mentioned. Since both approaches are similar, it would be interesting to see a comparison. 
