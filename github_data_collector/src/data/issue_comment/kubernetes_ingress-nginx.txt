<!--

Welcome to ingress-nginx!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

-->

<!--

IMPORTANT!!!

Please complete the next sections or the issue will be closed.
This questions are the first thing we need to know to understand the context.

-->

**NGINX Ingress controller version**: 0.23.0、0.26.0、0.27.0 (I only tested these 3 versions)
**Environment**:
ingress-nginx cmd line:
```
/nginx-ingress-controller \
  --default-backend-service=kube-system/default-http-backend \
  --ingress-class=kube-system.ingress-3 \
  --configmap=kube-system/proxy-nginx-config \
  --tcp-services-configmap=kube-system/proxy-nginx-tcp \
  --udp-services-configmap=kube-system/proxy-nginx-udp \
  --health-check-path=/healthz \
  --healthz-port=450 \
  --status-port=451 \
  --annotations-prefix=ingress.kubernetes.io \
  --enable-ssl-passthrough \
  --v=3
```

ingress-nginx configmap:
```
apiVersion: v1
data:
  enable-sticky-sessions: "true"
  enable-vts-status: "true"
  force-ssl-redirect: "false"
  proxy-body-size: 5G
  proxy-buffer-size: 4k
  proxy-buffers-number: "64"
  proxy-read-timeout: "300"
  server-tokens: "false"
  skip-access-log-urls: /nginx_status/format/json
  ssl-redirect: "false"
kind: ConfigMap
metadata:
    loadbalance.caicloud.io/created-by: kube-system.ingress-3
    loadbalance.caicloud.io/proxy: nginx
  name: proxy-nginx-config
  namespace: kube-system
```

ingress example：
```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    ingress.kubernetes.io/affinity: "true"
    ingress.kubernetes.io/enable-cors: "true"
    ingress.kubernetes.io/force-ssl-redirect: "true"
    ingress.kubernetes.io/limit-connections: "0"
    ingress.kubernetes.io/limit-rps: "0"
    ingress.kubernetes.io/load-balance: round_robin
    ingress.kubernetes.io/ssl-passthrough: "false"
    kubernetes.io/ingress.class: kube-system.ingress-3
  labels:
    loadbalance.caicloud.io/created-by: kube-system.ingress-3
  name: i1
  namespace: pp
spec:
  rules:
  - host: test.netingress.com
    http:
      paths:
      - backend:
          serviceName: ingressapp
          servicePort: 80
        path: /
  tls:
  - hosts:
    - test.netingress.com
    secretName: aaa.netingress.v1
```


**What happened**:
For https rule, nginx gets a wrong src ip or "real ip": 127.0.0.1.
nginx log:
```
127.0.0.1 - [127.0.0.1] - - [17/Jan/2020:08:19:39 +0000] "GET / HTTP/2.0" 200 468 "-" "curl/7.54.0" 30 0.001 [pp-hhlaa-80] 192.168.70.249:80 468 0.001 200 58c22d6e9031297535bb0316a3e7e7e3
```
<!-- (please include exact error messages if you can) -->

It works fine for http rule.

**What you expected to happen**:
nginx recognizes correct src ip for https rule.

<!-- What do you think went wrong? -->

**How to reproduce it**:
<!---

As minimally and precisely as possible. Keep in mind we do not have access to your cluster or application.
Help up us (if possible) reproducing the issue using minikube or kind.

## Install minikube/kind

- Minikube https://minikube.sigs.k8s.io/docs/start/
- Kind https://kind.sigs.k8s.io/docs/user/quick-start/

## Install the ingress controller

kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/service-nodeport.yaml

## Install an application that will act as default backend (is just an echo app)

kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/docs/examples/http-svc.yaml

## Create an ingress (please add any additional annotation required)

echo "
  apiVersion: networking.k8s.io/v1beta1
  kind: Ingress
  metadata:
    name: foo-bar
  spec:
    rules:
    - host: foo.bar
      http:
        paths:
        - backend:
            serviceName: http-svc
            servicePort: 80
          path: /
" | kubectl apply -f -

## make a request

POD_NAME=$(k get pods -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx -o NAME)
kubectl exec -it -n ingress-nginx $POD_NAME -- curl -H 'Host: foo.bar' localhost

--->

**Anything else we need to know**:

<!-- If this is actually about documentation, add `/kind documentation` below -->

/kind bug

I have to build this image to publish it for internal Microsoft consumption and for some reason it was failing to build due to the following error:

```
#6 [3/8] RUN apk add -U --no-cache   diffutils   libcap
#6 0.535 /bin/sh: 1: apk: not found
#6 ERROR: executor failed running [/bin/sh -c apk add -U --no-cache   diffutils   libcap]: runc did not terminate sucessfully
```

It appears that it was attempting to use a base image that was for the 0.27.0 release and it was refusing to download the new base image even though the sha had been updated. Im not sure if this is an issue with buildx not fetching the base image correctly or something else but figured I would open an issue in case other people run into this same problem.

/kind bug

<!--

Welcome to ingress-nginx!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

-->

<!--

IMPORTANT!!!

Please complete the next sections or the issue will be closed.
This questions are the first thing we need to know to understand the context.

-->

**NGINX Ingress controller version**: 
0.27.0

**Kubernetes version** (use `kubectl version`): 
v1.15.4-gke.22

**Environment**:

- **Cloud provider or hardware configuration**: 
  GKE
<!--
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:
-->

**What happened**:

Changing an `nginx.ingress.kubernetes.io/enable-opentracing` ingress annotation from `"true"` to `"false"` (or vice versa) is not reflected in the tracing headers sent to the app until ingress-nginx is restarted

**What you expected to happen**:

The headers to reflect the annotation without needing to restart ingress-nginx

<!-- What do you think went wrong? -->

**How to reproduce it**:


Enable opentracing and configure a zipkin-collector (this may apply to the other types of collector but we don't have them configured in our cluster) 

apply `nginx.ingress.kubernetes.io/enable-opentracing: "true"` to an ingress

You should see the app receives tracing headers; (e.g. using an echo-server workload)

```
x-b3-traceid
x-b3-spanid
x-b3-parentspanid
x-b3-sampled
x-b3-flags
```

Update the ingress annotation to `"false"` and the app continues receiving the same headers and values (particularly `x-b3-sampled:  1`)

Restart the ingresses

The app now sees `x-b3-sampled: 0` and no longer gets `x-b3-flags` (the other id headers still get sent, this is expected behaviour)

(The same happens again if you now switch it back to `"true"`)
<!---
As minimally and precisely as possible. Keep in mind we do not have access to your cluster or application.
Help up us (if possible) reproducing the issue using minikube or kind.

## Install minikube/kind

- Minikube https://minikube.sigs.k8s.io/docs/start/
- Kind https://kind.sigs.k8s.io/docs/user/quick-start/

## Install the ingress controller

kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/service-nodeport.yaml

## Install an application that will act as default backend (is just an echo app)

kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/docs/examples/http-svc.yaml

## Create an ingress (please add any additional annotation required)

echo "
  apiVersion: networking.k8s.io/v1beta1
  kind: Ingress
  metadata:
    name: foo-bar
  spec:
    rules:
    - host: foo.bar
      http:
        paths:
        - backend:
            serviceName: http-svc
            servicePort: 80
          path: /
" | kubectl apply -f -

## make a request

POD_NAME=$(k get pods -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx -o NAME)
kubectl exec -it -n ingress-nginx $POD_NAME -- curl -H 'Host: foo.bar' localhost

--->

**Anything else we need to know**:

<!-- If this is actually about documentation, add `/kind documentation` below -->

/kind bug

<!--- Provide a general summary of your changes in the Title above --->
<!--- Please don't @-mention people in PR or commit messages (do so in an additional comment). --->

## What this PR does / why we need it:
<!--- Why is this change required? What problem does it solve? -->
<!--- If it fixes an open issue, please link to the issue here. -->

## Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)

## Which issue/s this PR fixes
<!--
(optional, in `fixes #<issue number>` format, will close that issue when PR gets merged):

fixes #
-->

## How Has This Been Tested?
<!--- Please describe in detail how you tested your changes. -->
<!--- Include details of your testing environment, and the tests you ran to -->
<!--- see how your change affects other areas of the code, etc. -->

## Checklist:
<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->
- [ ] My change requires a change to the documentation.
- [ ] I have updated the documentation accordingly.
- [ ] I've read the [CONTRIBUTION](https://github.com/kubernetes/ingress-nginx/blob/master/CONTRIBUTING.md) guide
- [ ] I have added tests to cover my changes.
- [ ] All new and existing tests passed.

- should shutdown after waiting 60 seconds for pending connections to be closed [It]
- should shutdown after waiting 150 seconds for pending connections to be closed [It]
- should delete Ingress updated to catch-all [It]
- uses custom default backend [It]

<!--

Welcome to ingress-nginx!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

-->

<!--

IMPORTANT!!!

Please complete the next sections or the issue will be closed.
This questions are the first thing we need to know to understand the context.

-->

**NGINX Ingress controller version**: 0.27.0

**What happened**:

When the oldest Ingress that specifies a TLS host does not reference a secret, the default certificate is always served, even if another Ingress references a valid secret.

**What you expected to happen**:

If multiple Ingress specify a TLS host, but only one references a valid secret, the certificate from that secret should be used, not the default certificate

**How to reproduce it**:

1. create a tls secret "mysecret" with a valid certificate for "foo.bar.com"
2. create Ingress without secret reference
   
   ```
   echo "apiVersion: extensions/v1beta1
   kind: Ingress
   metadata:
     name: default-cert
   spec:
     rules:
     - host: foo.bar.com
       http:
         paths:
         - path: /foo
           backend:
             serviceName: foobar
             servicePort: 80
     tls:
     - hosts:
       - foo.bar.com" | kubectl apply -f-
   ```

3. create Ingress with secret reference

   ```
   echo "apiVersion: extensions/v1beta1
   kind: Ingress
   metadata:
     name: custom-cert
   spec:
     rules:
     - host: foo.bar.com
       http:
         paths:
         - path: /bar
           backend:
             serviceName: foobar
             servicePort: 80
     tls:
     - hosts:
       - foo.bar.com
       secretName: mysecret" | kubectl apply -f-
   ```

4. check certificate returned

   ```
   openssl s_client -connect $INGRESS_ADDR -servername foo.bar.com
   ```

**Anything else we need to know**:

Applying steps 2 and 3 in reverse order will serve the correct certificate. Restart the controller after removing Ingresses when testing because certificates may linger.

See #2279 and #4563 which asked for this behaviour.

I suspect this was broken by #4816.

/kind bug

I would like to start a discussion about the possibility to extend the current configuration of external Auth providers in order to support Lua plugin based solutions, too.

The current ingress-nginx implementation provides a way to involve such OIDC client implementations that run as external processes and ingress-nginx has to contact them via a HTTP sub-request. Such an external process can be provided for example by [oauth2-proxy](https://github.com/pusher/oauth2_proxy) or by [vouch-proxy](https://github.com/vouch/vouch-proxy), etc. In this situation some parts of the OIDC flow are executed inside ingress-nginx, while other parts are executed by the external solutions, and consequently, there are configuration options in ingress-nginx to control the internal part https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#external-authentication

[With the possibility to use an OIDC Lua plugin ](https://www.elvinefendi.com/2019/11/22/ingress-nginx-openidc-plugin.html )all parts of the client side OIDC flow are processed in ingress-nginx, i.e. there should be a way to configure all the necessary parameters in the ingress-nginx configuration. This issue is a request to discuss and agree on the details of that configuration if the community thinks that it is a valuable extension to ingress-nginx.

If we consider the needs of an OIDC client the requestor’s proposal is that the user should be able to configure the necessary parameters in a ConfigMap and a Secret. The ConfigMap and Secret names should be configured on the corresponding Ingress resources in annotations, similarly to the current practice of the external authentication related “auth-*” annotations.

On this way we would have a plugin agnostic way to provide the configuration to the plugin at runtime. Also it would be more dynamic if e.g. the OIDC client ID and the corresponding client secret could be provided via a K8s Secret resource instead of the usual environment variables. 

/kind feature

When using Helm to install, it's very hard to figure out how to get the ConfigMap working. Proposing a little bit of help.

<!--- Provide a general summary of your changes in the Title above --->
<!--- Please don't @-mention people in PR or commit messages (do so in an additional comment). --->

## What this PR does / why we need it:
<!--- Why is this change required? What problem does it solve? -->
<!--- If it fixes an open issue, please link to the issue here. -->

## Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)

## Which issue/s this PR fixes
<!--
(optional, in `fixes #<issue number>` format, will close that issue when PR gets merged):

fixes #
-->

## How Has This Been Tested?
<!--- Please describe in detail how you tested your changes. -->
<!--- Include details of your testing environment, and the tests you ran to -->
<!--- see how your change affects other areas of the code, etc. -->

## Checklist:
<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->
- [ ] My change requires a change to the documentation.
- [x] I have updated the documentation accordingly.
- [ ] I've read the [CONTRIBUTION](https://github.com/kubernetes/ingress-nginx/blob/master/CONTRIBUTING.md) guide
- [ ] I have added tests to cover my changes.
- [ ] All new and existing tests passed.

<!--

Welcome to ingress-nginx!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

-->

<!--

IMPORTANT!!!

Please complete the next sections or the issue will be closed.
This questions are the first thing we need to know to understand the context.

-->

**NGINX Ingress controller version**: 0.25.0

**Kubernetes version** (use `kubectl version`): v1.12

**What happened**: The Ingress annotations included `auth-tls-pass-certificate-to-upstream: true`, but nginx.conf was rendered without the line `proxy_set_header ssl-client-cert        $ssl_client_escaped_cert;`

**What you expected to happen**: Consider multiple Ingress resources on a single hostname. With other TLS-related annotations (like `auth-tls-verify-client`), adding them to one Ingress will cause cause them to be enabled for all Ingresses on that hostname. However, with `auth-tls-pass-certificate-to-upstream`, the certificate is not passed passed to the upstream unless `auth-tls-pass-certificate-to-upstream: true` is set on all ingresses for that hostname. Since this annotation simply controls a boolean in the location section of the template, I would expect to be able to enable it on a per-ingress basis.

When reading the template, it is clear what the problem is:
https://github.com/kubernetes/ingress-nginx/blob/b30115aba7e11da3da275910e17448b17f9892aa/rootfs/etc/nginx/template/nginx.tmpl#L932
`PassCertToUpstream` is scoped under `server`, not `location`, so the value may actually be populated by a different ingress resource.

<!-- What do you think went wrong? -->

**How to reproduce it**:
```yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: foo
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/auth-tls-secret: default/ca-cert
    nginx.ingress.kubernetes.io/auth-tls-verify-client: optional
spec:
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          serviceName: http-svc1
          servicePort: 80
        path: /foo
---
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: bar
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/auth-tls-pass-certificate-to-upstream: "true"
    nginx.ingress.kubernetes.io/auth-tls-secret: default/ca-cert
    nginx.ingress.kubernetes.io/auth-tls-verify-client: optional
spec:
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          serviceName: http-svc2
          servicePort: 80
        path: /bar
```
<!---

As minimally and precisely as possible. Keep in mind we do not have access to your cluster or application.
Help up us (if possible) reproducing the issue using minikube or kind.

## Install minikube/kind

- Minikube https://minikube.sigs.k8s.io/docs/start/
- Kind https://kind.sigs.k8s.io/docs/user/quick-start/

## Install the ingress controller

kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/service-nodeport.yaml

## Install an application that will act as default backend (is just an echo app)

kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/docs/examples/http-svc.yaml

## Create an ingress (please add any additional annotation required)

echo "
  apiVersion: networking.k8s.io/v1beta1
  kind: Ingress
  metadata:
    name: foo-bar
  spec:
    rules:
    - host: foo.bar
      http:
        paths:
        - backend:
            serviceName: http-svc
            servicePort: 80
          path: /
" | kubectl apply -f -

## make a request

POD_NAME=$(k get pods -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx -o NAME)
kubectl exec -it -n ingress-nginx $POD_NAME -- curl -H 'Host: foo.bar' localhost

--->

**Anything else we need to know**:

I suspect this patch to the e2e tests will reproduce the issue, but I don't have a system to run the e2e tests on at the moment to try it:
```diff
diff --git a/test/e2e/annotations/authtls.go b/test/e2e/annotations/authtls.go
index 350e21a92..abbea4a88 100644
--- a/test/e2e/annotations/authtls.go
+++ b/test/e2e/annotations/authtls.go
@@ -125,11 +125,14 @@ var _ = framework.IngressNginxDescribe("Annotations - AuthTLS", func() {
                Expect(err).ToNot(HaveOccurred())
 
                annotations := map[string]string{
-                       "nginx.ingress.kubernetes.io/auth-tls-secret":                       nameSpace + "/" + host,
-                       "nginx.ingress.kubernetes.io/auth-tls-error-page":                   f.GetURL(framework.HTTP) + errorPath,
-                       "nginx.ingress.kubernetes.io/auth-tls-pass-certificate-to-upstream": "true",
+                       "nginx.ingress.kubernetes.io/auth-tls-secret":     nameSpace + "/" + host,
+                       "nginx.ingress.kubernetes.io/auth-tls-error-page": f.GetURL(framework.HTTP) + errorPath,
                }
 
+               f.EnsureIngress(framework.NewSingleIngressWithTLS(host, "/foo", host, []string{host}, nameSpace, framework.EchoService, 80, annotations))
+
+               annotations["nginx.ingress.kubernetes.io/auth-tls-pass-certificate-to-upstream"] = "true"
+
                f.EnsureIngress(framework.NewSingleIngressWithTLS(host, "/", host, []string{host}, nameSpace, framework.EchoService, 80, annotations))
 
                assertSslClientCertificateConfig(f, host, "on", "1")
```

<!-- If this is actually about documentation, add `/kind documentation` below -->

/kind bug

<!--

Welcome to ingress-nginx!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

-->

<!--

IMPORTANT!!!

Please complete the next sections or the issue will be closed.
This questions are the first thing we need to know to understand the context.

-->

**NGINX Ingress controller version**: unable to verify

**Kubernetes version** (use `kubectl version`): 1.13

**Environment**:

- **Cloud provider or hardware configuration**: On premise, underlying hypervisor is VMware
- **OS** (e.g. from /etc/os-release): Debian
- **Kernel** (e.g. `uname -a`): Debian 4.9.168-1+deb9u2 (2019-05-13) x86_64 GNU/Linux- 
- **Install tools**: N/A
- **Others**: N/A

**What happened**: Nginx istances not using the tls specified in the ingresses but sticking to the default certificate in local /etc/kubernetes/ssl folder

<!-- (please include exact error messages if you can) -->

**What you expected to happen**:
Nginx istances should use certificates specified by ingresses instead of the default ones.

<!-- What do you think went wrong? -->
Local nginx did not receive the configuration specified in the ingress text.

**How to reproduce it**:
Implement an ingress like the following one: 
```

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: "1m"
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "5"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "120"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "120"
    nginx.ingress.kubernetes.io/proxy-next-upstream: "error"
    nginx.ingress.kubernetes.io/proxy-next-upstream-tries: "3"
    nginx.ingress.kubernetes.io/proxy-request-buffering: "on"
    nginx.ingress.kubernetes.io/ssl-redirect: "False"


    nginx.ingress.kubernetes.io/rewrite-target: /$1
    nginx.ingress.kubernetes.io/use-regex: "true"


    nginx.ingress.kubernetes.io/configuration-snippet: |
      if ( $request_method !~ ^(GET|OPTIONS|HEAD)$ ) {
          return 405;
      }
      set $vpn "yes";
      if ($not_vpn_ip) {
          set $vpn "no";
      }

      if ($ingress_name != "") {
        set $service "assets_apps";
        set $subenv "our_apps";
        set $service_id "apps_mycompany-assets_apps";
        set $real_ingress "true";
      }
      more_set_headers "X-companyname-Uuid: $tid";
      more_clear_headers 'x-companyappname-*';
      more_clear_headers 'x-powered-by';
      more_clear_headers 'x-our-orchestrator';
      more_clear_input_headers 'x-ourwebsite-cache-id';
      more_set_headers "Our-Service-Name: $service";
      more_set_headers "Our-Service-Subenv: $subenv";
      more_set_headers "Our-Service-Id: $service_id";
      more_set_headers "Our-Service-Ingress: $ingress_name";
      more_set_headers "strict-transport-security: $strict_header";
      proxy_set_header X-CompanyHeader-UUID $tid;

      set $cors "false";
      set $cors_done "false";

  name: mywebsite-apps
  namespace: apps--ourcompany
spec:
  tls:
  - hosts:
    - mywebsite
    secretName: mywebsite-certs
  rules:
  - host: mywebsite
    http:
      paths:

      - path: /Apps(.*)
        backend:
          serviceName: apps--ourcompany-assets--apps-default-service
          servicePort: 8090

```

Verify if the NGINX Configuration appears as this:

```
server {
		server_name apps.ourwebsite.it ;
		
		listen 80 proxy_protocol;
		
		set $proxy_upstream_name "-";
		set $pass_access_scheme $scheme;
		set $pass_server_port $server_port;
		set $best_http_host $http_host;
		set $pass_port $pass_server_port;
		
		listen 443 proxy_protocol  ssl http2;
		
		# PEM sha: 5c5cb985aced79d49bef4011061c96ef8882fbf4
		ssl_certificate                         /etc/ingress-controller/ssl/default-fake-certificate.pem;
		ssl_certificate_key                     /etc/ingress-controller/ssl/default-fake-certificate.pem;
		
		ssl_certificate_by_lua_block {
			certificate.call()
		}
		
		location / {
			
			set $namespace      "ourstore";
			set $ingress_name   "apps.ourwebsite.it-ourstore";
			set $service_name   "ourstore-assets--ourstore-default-service";
			set $service_port   "8090";
			set $location_path  "/";
			
			rewrite_by_lua_block {
				lua_ingress.rewrite({
					force_ssl_redirect = true,
					use_port_in_redirects = false,
				})
				balancer.rewrite()
				plugins.run()
			}
			
			header_filter_by_lua_block {
				
				plugins.run()
			}
			body_filter_by_lua_block {
				
			}
			
			log_by_lua_block {
				
				balancer.log()
				
				monitor.call()
				
				plugins.run()
			}
			
			if ($scheme = https) {
				more_set_headers                        "Strict-Transport-Security: max-age=15724800; includeSubDomains";
			}
			
			port_in_redirect off;
			
			set $proxy_upstream_name    "ourstore-ourstore-assets--ourstore-default-service-8090";
			set $proxy_host             $proxy_upstream_name;
			
			client_max_body_size                    1m;
			
			proxy_set_header Host                   $best_http_host;
			
			# Pass the extracted client certificate to the backend
			
			# Allow websocket connections
			proxy_set_header                        Upgrade           $http_upgrade;
			
			proxy_set_header                        Connection        $connection_upgrade;
			
			proxy_set_header X-Request-ID           $req_id;
			proxy_set_header X-Real-IP              $the_real_ip;
			
			proxy_set_header X-Forwarded-For        $the_real_ip;
			
			proxy_set_header X-Forwarded-Host       $best_http_host;
			proxy_set_header X-Forwarded-Port       $pass_port;
			proxy_set_header X-Forwarded-Proto      $pass_access_scheme;
			
			proxy_set_header X-Original-URI         $request_uri;
			
			proxy_set_header X-Scheme               $pass_access_scheme;
			
			# Pass the original X-Forwarded-For
			proxy_set_header X-Original-Forwarded-For $http_x_forwarded_for;
			
			# mitigate HTTPoxy Vulnerability
			# https://www.nginx.com/blog/mitigating-the-httpoxy-vulnerability-with-nginx/
			proxy_set_header Proxy                  "";
			
			# Custom headers to proxied server
			
			proxy_connect_timeout                   5s;
			proxy_send_timeout                      120s;
			proxy_read_timeout                      120s;
			
			proxy_buffering                         off;
			proxy_buffer_size                       4k;
			proxy_buffers                           4 4k;
			proxy_request_buffering                 on;
			
			proxy_http_version                      1.1;
			
			proxy_cookie_domain                     off;
			proxy_cookie_path                       off;
			
			# In case of errors try the next upstream server before returning an error
			proxy_next_upstream                     error;
			proxy_next_upstream_tries               3;
			
			if ( $request_method !~ ^(GET|POST|OPTIONS|DELETE|PUT|HEAD)$ ) {
				return 405;
			}
			set $vpn "yes";
			if ($not_vpn_ip) {
				set $vpn "no";
			}
			
			if ($ingress_name != "") {
				set $service "assets_ourstore";
				set $subenv "ourstore";
				set $service_id "ourstore-assets_ourstore";
				set $real_ingress "true";
			}
			more_set_headers "X-Our-UUID: $tid";
			more_clear_headers 'x-ourapp-*';
			more_clear_headers 'x-powered-by';
			more_clear_headers 'x-our-orchestrator';
			more_clear_input_headers 'x-our-cache-id';
			more_set_headers "X-Our-Service-Name: $service";
			more_set_headers "X-Our-Service-Subenv: $subenv";
			more_set_headers "X-Our-Service-Id: $service_id";
			more_set_headers "X-Our-Service-Ingress: $ingress_name";
			more_set_headers "strict-transport-security: $strict_header";
			proxy_set_header X-Our-UUID $tid;
			
			set $cors "false";
			set $cors_done "false";
			
			proxy_pass http://upstream_balancer;
			
			proxy_redirect                          off;
			
		}
		
	}

```
More specifically I don't expect nginx to use the local default SSL certificate as specified in the following line:

`ssl_certificate                         /etc/ingress-controller/ssl/default-fake-certificate.pem;`

