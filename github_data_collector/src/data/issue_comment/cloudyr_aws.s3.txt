As per title, `s3write_using(FUN = write_rds)` doesn't work for large files. I assume this is also true for other large file paths as it seems to be a problem with the `sprintf` function and large integers.

Reproducible example:
(You will need an S3 bucket to run all of it)
```R
library(tidyverse)
library(aws.s3)
library(nycflights13)

# make a big df
df <- map_dfr(1:50, ~flights)
# check dimensions (should be almost 320 MM entries)
dim(df)[1] * dim(df)[2] / 1000000

s3write_using(
  x = df,
  FUN = write_rds, 
  object = 'some/s3/file_path.rds', 
  bucket = 'my_s3_bucket')
```

This produces the following error:
```
Error in sprintf("File size is %d. Consider setting 'multipart = TRUE'.",  : 
  invalid format '%d'; use format %f, %e, %g or %a for numeric objects

> traceback()
4: sprintf("File size is %d. Consider setting 'multipart = TRUE'.", 
       headers[["Content-Length"]])
3: message(sprintf("File size is %d. Consider setting 'multipart = TRUE'.", 
       headers[["Content-Length"]]))
2: put_object(file = tmp, bucket = bucket, object = object)
1: s3write_using(x = df, FUN = write_rds, object = "some/s3/file_path.rds", 
       bucket = "my_s3_bucket")
```

I think this should be fixable by simply switching the parameter in the message function. Thoughts?
Please specify whether your issue is about:

 - [x] a possible bug
 - [ ] a question about package functionality
 - [ ] a suggested code or documentation change, improvement to the code, or feature request

If you are reporting (1) a bug or (2) a question about code, please supply:

 - [a fully reproducible example](http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example) using a publicly available dataset (or provide your data)
 - if an error is occurring, include the output of `traceback()` run immediately after the error occurs
 - the output of `sessionInfo()`

Put your code here:

```R
## load package
library("aws.s3")

## code goes here
# Not Applicable 

## session info for your system
sessionInfo()
```
```Downloading GitHub repo cloudyr/aws.s3@master

✔  checking for file ‘/private/var/folders/y1/_zzs2xzj5sx6x47gysqpxt9c0000gn/T/RtmpsbWUAC/remotes55ff7d8b92c2/cloudyr-aws.s3-c0c5d8d/DESCRIPTION’ ...
─  preparing ‘aws.s3’:
E  checking DESCRIPTION meta-information ...
   Authors@R field gives no person with maintainer role, valid email
   address and non-empty name.

   See section 'The DESCRIPTION file' in the 'Writing R Extensions'
   manual.

Error: Failed to install 'aws.s3' from GitHub:
  System command error, exit status: 1, stdout + stderr:
E> * checking for file ‘/private/var/folders/y1/_zzs2xzj5sx6x47gysqpxt9c0000gn/T/RtmpsbWUAC/remotes55ff7d8b92c2/cloudyr-aws.s3-c0c5d8d/DESCRIPTION’ ... OK
E> * preparing ‘aws.s3’:
E> * checking DESCRIPTION meta-information ... ERROR
E> Authors@R field gives no person with maintainer role, valid email
E> address and non-empty name.
E>
E> See section 'The DESCRIPTION file' in the 'Writing R Extensions'
E> manual.
E>
```

I have been reading in multiple parquet files from S3 using R. Recently, I decided to parallelize this process in order to read in these files quicker using the original function. I have shown both the lapply and mclapply below:  

```R
## load package

library(aws.s3)
library(parallel)
library(data.table)
library(dplyr)
library(arrow)

num_cores <- detectCores() - 2
rerating_data_files <- some_vector_of_s3_file_paths

read_parquet_from_s3 <- function(file){
  tryCatch( expr = { return(setDT(arrow::read_parquet(aws.s3::get_object(file))))} ,
    error = function(e) {
      cat(c('\n','error with reading in file: ',file,'\n')) } ) }

test <- rbindlist(mclapply(rerating_data_files,function(x) read_parquet_from_s3(x),
                 mc.cores = min(num_cores,length(rerating_data_files))),use.names=T)
test$quote_id %>% n_distinct()     ## 2342 

data_temp <- rbindlist(lapply(rerating_data_files,function(x) read_parquet_from_s3(x)),use.names = T)
data_temp$quote_id %>% n_distinct()     ## 2542 

```
This is so odd because running the EXACT SAME snippet of code 5 or 6 times results in the mclapply bringing back a datatable with the correct 2542 distinct quotes. 

I'm curious if anyone else is having this problem with aws.s3 and mclapply. 



I saw `Error in sprintf("File size is %d. Consider setting 'multipart = TRUE'.",  : 
  invalid format '%d'; use format %f, %e, %g or %a for numeric objects`

When I tried to lead a large data frame. The smaller data frame is OK.
Added name as per directions on opening pull request for fixing typo.

Please ensure the following before submitting a PR:

 - [ ] if suggesting code changes or improvements, [open an issue](https://github.com/cloudyr/aws.s3/issues/new) first
 - [ ] for all but trivial changes (e.g., typo fixes), add your name to [DESCRIPTION](https://github.com/cloudyr/aws.s3/blob/master/DESCRIPTION)
 - [ ] for all but trivial changes (e.g., typo fixes), documentation your change in [NEWS.md](https://github.com/cloudyr/aws.s3/blob/master/NEWS.md) with a parenthetical reference to the issue number being addressed
 - [ ] if changing documentation, edit files in `/R` not `/man` and run `devtools::document()` to update documentation
 - [ ] add code or new test files to [`/tests`](https://github.com/cloudyr/aws.s3/tree/master/tests/testthat) for any new functionality or bug fix
 - [ ] make sure `R CMD check` runs without error before submitting the PR


Fixed typo with grammatical mistakes.

Please ensure the following before submitting a PR:

 - [ ] if suggesting code changes or improvements, [open an issue](https://github.com/cloudyr/aws.s3/issues/new) first
 - [x] for all but trivial changes (e.g., typo fixes), add your name to [DESCRIPTION](https://github.com/cloudyr/aws.s3/blob/master/DESCRIPTION)
 - [ ] for all but trivial changes (e.g., typo fixes), documentation your change in [NEWS.md](https://github.com/cloudyr/aws.s3/blob/master/NEWS.md) with a parenthetical reference to the issue number being addressed
 - [ ] if changing documentation, edit files in `/R` not `/man` and run `devtools::document()` to update documentation
 - [ ] add code or new test files to [`/tests`](https://github.com/cloudyr/aws.s3/tree/master/tests/testthat) for any new functionality or bug fix
 - [ ] make sure `R CMD check` runs without error before submitting the PR


Please specify whether your issue is about:

 - [X] a possible bug
 - [ ] a question about package functionality
 - [ ] a suggested code or documentation change, improvement to the code, or feature request

I try to run docker compose with minio and R on top of it. I am able to access minio using python library boto3, but there is an issue with aws.s3

Dockerfile:

```
FROM r-base

RUN apt-get update \
  && apt-get install -y --no-install-recommends \
  libcurl4-openssl-dev \
  libpq-dev \
  libssl-dev \
  libxml2-dev \
  libpq-dev \
  pkg-config \
  libgit2-dev

RUN R -e "install.packages(c('aws.s3', 'httr'))"

RUN apt-get install -y python3
RUN apt install -y python3-pip
RUN pip3 install boto3

COPY . ./
```

I build it with

`
docker build . -t segmentation
`

docker-compose.yml

```
version: '3'
services:
  minio:
    image: minio/minio:latest
    ports:
      - "9000:9000"
    volumes:
      - ./data:/data
    environment:
      - "MINIO_ACCESS_KEY=minio"
      - "MINIO_SECRET_KEY=miniominio"
    command: server /data

  model:
    image: segmentation:latest
    depends_on:
      - minio
```

I run interactively

```
docker-compose run model bash
```

And execute script

```R
library(aws.s3)

httr::with_verbose(bucketlist(verbose=T,  base_url = "http://minio:9000", key = "minio", secret = "miniominio", use_https = FALSE))
```

getting error:

```
Locating credentials
Checking for credentials in user-supplied values
Using user-supplied value for AWS Access Key ID
Using user-supplied value for AWS Secret Access Key
Using default value for AWS Region ('us-east-1')
S3 Request URL: http://http://minio:9000/
Executing request with AWS credentials
Locating credentials
Checking for credentials in user-supplied values
Using user-supplied value for AWS Access Key ID
Using user-supplied value for AWS Secret Access Key
Using user-supplied value for AWS Region ('us-east-1')
Error in curl::curl_fetch_memory(url, handle = handle) :
  Could not resolve host: http
```

When I change endpoint to 127.0.0.1:9000 (which is where minio should be by default)

```R
Sys.setenv("AWS_ACCESS_KEY_ID" = "minio")
Sys.setenv("AWS_SECRET_ACCESS_KEY" = "miniominio")
Sys.setenv("AWS_S3_ENDPOINT" = "127.0.01:9000")

httr::with_verbose(bucketlist(verbose=T, base_url = Sys.getenv("AWS_S3_ENDPOINT"), use_https = FALSE))
```

I get different error:

```
Locating credentials
Checking for credentials in user-supplied values
Checking for credentials in Environment Variables
Using Environment Variable 'AWS_ACCESS_KEY_ID' for AWS Access Key ID
Using Environment Variable 'AWS_SECRET_ACCESS_KEY' for AWS Secret Access Key
Using default value for AWS Region ('us-east-1')
S3 Request URL: http://127.0.01:9000/
Executing request with AWS credentials
Locating credentials
Checking for credentials in user-supplied values
Using user-supplied value for AWS Access Key ID
Using user-supplied value for AWS Secret Access Key
Using user-supplied value for AWS Region ('us-east-1')
Error in curl::curl_fetch_memory(url, handle = handle) :
  Failed to connect to 127.0.01 port 9000: Connection refused
```


Surprisingly it works in python with boto3

```
import boto3

AWS_ACCESS_KEY_ID='minio'
AWS_SECRET_ACCESS_KEY='miniominio'
endpoint = 'http://minio:9000'

session = boto3.Session(
                aws_access_key_id=AWS_ACCESS_KEY_ID,
                aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
           )
s3 = session.resource('s3', endpoint_url=endpoint)

list(s3.buckets.all())
```

 - the output of `sessionInfo()`

`
R version 3.6.1 (2019-07-05)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Debian GNU/Linux bullseye/sid

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.8.0
LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.8.0

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
 [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
 [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] aws.s3_0.3.12

loaded via a namespace (and not attached):
 [1] httr_1.4.1          compiler_3.6.1      R6_2.4.1
 [4] tools_3.6.1         base64enc_0.1-3     curl_4.2
 [7] Rcpp_1.0.3          aws.signature_0.5.2 xml2_1.2.2
[10] digest_0.6.23
`


Please specify whether your issue is about:

 - [ ] a possible bug
 - [ ] a question about package functionality
 - [ x] a suggested code or documentation change, improvement to the code, or feature request

Give that `aws.signature::locate_credentials` takes the `profile` argument` it would be useful to be able to pass that through `s3Http` and therefore the higher `API` functions. Currently you are unable to and therefore relay on setting `environment variables` this seem somewhat cumbersome considering the code already exists to accept it.

Put your code here:

```R
## load package
library("aws.s3")

bucketlist(profile = "NOT_BEFAULT")
bucketlist() # uses default
```


 - [x] if suggesting code changes or improvements, [open an issue](https://github.com/cloudyr/aws.s3/issues/new) first
 - [x] for all but trivial changes (e.g., typo fixes), add your name to [DESCRIPTION](https://github.com/cloudyr/aws.s3/blob/master/DESCRIPTION)
 - [x] for all but trivial changes (e.g., typo fixes), documentation your change in [NEWS.md](https://github.com/cloudyr/aws.s3/blob/master/NEWS.md) with a parenthetical reference to the issue number being addressed
 - [x] if changing documentation, edit files in `/R` not `/man` and run `devtools::document()` to update documentation
 - [x] add code or new test files to [`/tests`](https://github.com/cloudyr/aws.s3/tree/master/tests/testthat) for any new functionality or bug fix
 - [x] make sure `R CMD check` runs without error before submitting the PR


Please specify whether your issue is about:

 - [x] a possible bug
 - [ ] a question about package functionality
 - [ ] a suggested code or documentation change, improvement to the code, or feature request

I'm run `get_bucket` from within a docker container running on an EC2 instance. The roles of the instance allow the operation I need with no credentials. In particular, from the instance's shell I can run something like `aws s3 ls s3://my-bucket/some/prefix/` and get the answer I want. Note that neither `~/.aws/credentials` nor `AWS_*` environment variables are available. From R, I'm trying the following:

```R
## load package
library("aws.s3")

## code goes here
aws.s3::get_bucket("my-bucket", "some/prefix", verbose=TRUE)
```

This returns the following error:

```
Locating credentials
Checking for credentials in user-supplied values
Checking for credentials in Environment Variables
Searching for credentials file(s)
No user-supplied credentials, environment variables, instance metadata, or credentials file found!
Using default value for AWS Region ('us-east-1')
S3 Request URL: https://s3.amazonaws.com/my-bucket/
Executing request without AWS credentials
Parsing AWS API response
Client error: (403) Forbidden
List of 4
 $ Code     : chr "AccessDenied"
 $ Message  : chr "Access Denied"
 $ RequestId: chr "B78DC8F704A90043"
 $ HostId   : chr "u2N9kv0xYE5hEV7W[cut]"
 - attr(*, "headers")=List of 7
  ..$ x-amz-bucket-region: chr "us-east-1"
  ..$ x-amz-request-id   : chr "B78DC8F704A90043"
  ..$ x-amz-id-2         : chr "u2N9kv0xYE5hEV7WpvAJNUehK8a4a[cut]"
  ..$ content-type       : chr "application/xml"
  ..$ transfer-encoding  : chr "chunked"
  ..$ date               : chr "Thu, 14 Nov 2019 08:01:54 GMT"
  ..$ server             : chr "AmazonS3"
  ..- attr(*, "class")= chr [1:2] "insensitive" "list"
 - attr(*, "class")= chr "aws_error"
NULL
Error in parse_aws_s3_response(r, Sig, verbose = verbose) :
  Forbidden (HTTP 403).
```

Is there a way to use `aws.s3` without credentials and ask it to assume it has them?

## session info for your system
```
R version 3.4.2 (2017-09-28)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Debian GNU/Linux 9 (stretch)

Matrix products: default
BLAS: /usr/lib/openblas-base/libblas.so.3
LAPACK: /usr/lib/libopenblasp-r0.2.19.so

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
 [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=C
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
 [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] aws.s3_0.3.20

loaded via a namespace (and not attached):
 [1] httr_1.3.1          compiler_3.4.2      R6_2.2.2
 [4] tools_3.4.2         base64enc_0.1-3     curl_3.0
 [7] Rcpp_0.12.14        aws.signature_0.5.2 xml2_1.1.1
[10] digest_0.6.12
```

