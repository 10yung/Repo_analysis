importing direct from keras raises error 
> 
AttributeError: module 'tensorflow' has no attribute 'get_default_graph'

as recommended by the tf community, we should import keras as tensorflow.keras
## Problem description
Update an existing LDA model with an incremental approach. We create a LDA model for a collection of documents on demand basis. We save the resulting model file on the cloud. When a new LDA request arrives with fresh data, I need a way to incrementally update the model (live training) wit the these data. Typically I would use `lda.update()`. But what happens when the `lda.update()` takes as input a corpus that includes the same documents of the previous model?

Assumed to have `model1` trained on `corpus1` and a new `corpus2`, which is the best approach to do the incremental training of the new `model2` against `corpus2`?

I have seen a `lda.diff` function. So one could train `model2` and then run `mdiff, annotation = model1.diff(model2)`, then check `diff` and `annotation` and decide to promote `model2`. Does it make sense? Which is the best criterion to promote the new model then?

Thank you in advance!

#### Steps/code/corpus to reproduce

```python
from smart_open import open

# load the existing LDA model
current_model = LdaModel.load(open(s3_model_path))

# load the corpus
data = TextCorpus( open(s3_corpus_path) )
corpus_sentences = data.get_texts()
dictionary = Dictionary(corpus_sentences)
corpus = [dictionary.doc2bow(text) for text in corpus_sentences]

# update current model on the corpus
current_model.update(corpus)
```

#### Versions
```
Linux-5.0.0-36-generic-x86_64-with-Ubuntu-18.04-bionic
Python 3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
NumPy 1.17.4
SciPy 1.0.0
gensim 3.8.1
FAST_VERSION 1
```

Hi,

I use the `gensim` wrapper, `LdaMallet()` [[link]](https://radimrehurek.com/gensim/models/wrappers/ldamallet.html), to run `MALLET`.

Gensim library provide a parameter `workers` to assign the `--num-threads` argument in `MALLET`.  
(Ref: [Gensim Code - line274](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/wrappers/ldamallet.py))

But I found the `workers` seems not working, here is the different setting and running time:
```
 `workers=1` -> run time: 7.32 sec   # <--
 `workers=2` -> run time: 2min 25s
 `workers=4` -> run time: 2min 38s
 `workers=16` -> run time: 3min 13s  # <--
```
   
   
No matter I run this on my computer:
```
openjdk version "1.8.0_162"
OpenJDK Runtime Environment (build 1.8.0_162-8u162-b12-0ubuntu0.16.04.2-b12)
OpenJDK 64-Bit Server VM (build 25.162-b12, mixed mode)
```
or on the Colab:
```
openjdk version "11.0.4" 2019-07-16
OpenJDK Runtime Environment (build 11.0.4+11-post-Ubuntu-1ubuntu218.04.3)
OpenJDK 64-Bit Server VM (build 11.0.4+11-post-Ubuntu-1ubuntu218.04.3, mixed mode, sharing)
```
the results are similar, more workers spent more time.
(and I have also tried `mallet-2.0.8` & `mallet-2.0.7`)  
  
  
Dose it means I am not using a proper way to run MALLET LDA in parallel?  
  
Thanks!   

   
---
reference code:

```
# code in gensim (python)
# (i tried with different `workers`)

workers = 16
gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word, 
                                 optimize_interval=1, iterations=6000, workers=workers)
```
```
# the equivalent commands in mallet (key in shell, ignore the I/O setting):

$ bin/mallet train-topics --num-threads 16
```
Im trying to run naive-bayes.py from the following file(https://github.com/rockash/Fake-news-Detection) but i get the error mentioned in the title, i have tried uninstalling gensim and reinstalling it again using both pip and conda but nothing seems to work.i also have mingw installed and have added /bin to path as the warning suggests but it doesnt work either.I'm using windows 10 64 bit
![Screenshot (2)](https://user-images.githubusercontent.com/36754286/72397510-21b62b00-3766-11ea-913b-6a283d85fff3.png)


Our implementation of FastText training error-backpropagation does some fishy things that deviate from the FB reference implementation. 

For example, at..

https://github.com/RaRe-Technologies/gensim/blob/fbc7d0952f1461fb5de3f6423318ae33d87524e3/gensim/models/fasttext_inner.pyx#L338

...we simply short-circuit skip to the next loop when an exponent is out of the desired range. (The same approach appears in Word2Vec and Doc2Vec cython code, as well.)

However, the seemingly-analogous code in Facebook's FastText instead clips the values to 0.0/1.0 in these cases, allowing backprop to proceed. See:

https://github.com/facebookresearch/fastText/blob/26bcbfc6b288396bd189691768b8c29086c0dab7/src/loss.cc#L52

Our deviation from Facebook's code's practice is suspicious on both correctness & consistency grounds. This simple `continue` does however match the behavior we copied long-ago from `word2vec.c`. 

Other perhaps-more superficial changes are that FB's code makes its lookup-tables 512 slots long instead of 1000, but allows exponents to 8 instead of 6:

https://github.com/facebookresearch/fastText/blob/51e6738d734286251b6ad02e4fdbbcfe5b679382/src/loss.cc#L16

Again, our FT implementation seems to have copied our copy-of-word2vec.c choices, instead of the reference FB implementation choices. If anything, it could make more sense to update the word2vec-derived code with these newer choices – as they at least plausibly represent practices improved by experience. 
#### Problem description
Hey everyone,
I encountered an issue when loading a pre-trained facebook FastText models. Loading a 7,24 GB pretrained model blows up to more than 20 GB in RAM on my machine when loading with Gensim. So my computer keeps swapping memory like crazy and never loads the model. It would be awesome if we could lower the memory footprint in Gensim's FastText loading mechanism. Is this a known problem and is anyone aware how to fix it? 

#### Steps/code/corpus to reproduce
1. Download a pre-trained FastText model  (e.g., cc.en.300.bin) from: https://fasttext.cc/docs/en/crawl-vectors.html
2. Try to load the model using `load_facebook_model('cc.en.300.bin')`

#### Versions

Please provide the output of:

```python
Darwin-19.0.0-x86_64-i386-64bit
Python 3.7.6 | packaged by conda-forge | (default, Dec 26 2019, 23:46:52) 
[Clang 9.0.0 (tags/RELEASE_900/final)]
NumPy 1.17.2
SciPy 1.4.1
gensim 3.8.1
```

My pc is 64 cpu with cuda GPU,but when i use 
```
lda_corpus = models.ldamulticore.LdaMulticore(corpus=train_corpus, num_topics=num_topics, id2word=dictionary,
                              alpha=0.01, eta=0.01, minimum_probability=0.0001, 
                              iterations=100,workers=10)
```

using the code output nothing and mechine seems get dead cycle, but there are nothing output.

when i use the below code:
```
lda_corpus = models.LdaModel(train_corpus, num_topics=num_topics, id2word=dictionary,
                        alpha=0.01, eta=0.01, minimum_probability=0.001,
                        update_every = 1, iterations=iterations) 
```

got work.

why not use multicore when in linux(ubuntu)?
any help is thanks very much.
#### Problem description

I'm trying to use lemmatize function to my text but getting StopIteration exception.

#### Steps/code/corpus to reproduce

```
from gensim.utils import lemmatize


s = lemmatize('eight')
print(s)
```

Result:
```
python3 lem.py 
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/pattern/text/__init__.py", line 609, in _read
    raise StopIteration
StopIteration

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "lem.py", line 4, in <module>
    s = lemmatize('eight')
  File "/usr/local/lib/python3.7/site-packages/gensim/utils.py", line 1692, in lemmatize
    parsed = parse(content, lemmata=True, collapse=False)
  File "/usr/local/lib/python3.7/site-packages/pattern/text/en/__init__.py", line 169, in parse
    return parser.parse(s, *args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/pattern/text/__init__.py", line 1172, in parse
    s[i] = self.find_tags(s[i], **kwargs)
  File "/usr/local/lib/python3.7/site-packages/pattern/text/en/__init__.py", line 114, in find_tags
    return _Parser.find_tags(self, tokens, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/pattern/text/__init__.py", line 1113, in find_tags
    lexicon = kwargs.get("lexicon", self.lexicon or {}),
  File "/usr/local/lib/python3.7/site-packages/pattern/text/__init__.py", line 376, in __len__
    return self._lazy("__len__")
  File "/usr/local/lib/python3.7/site-packages/pattern/text/__init__.py", line 368, in _lazy
    self.load()
  File "/usr/local/lib/python3.7/site-packages/pattern/text/__init__.py", line 625, in load
    dict.update(self, (x.split(" ")[:2] for x in _read(self._path) if len(x.split(" ")) > 1))
  File "/usr/local/lib/python3.7/site-packages/pattern/text/__init__.py", line 625, in <genexpr>
    dict.update(self, (x.split(" ")[:2] for x in _read(self._path) if len(x.split(" ")) > 1))
RuntimeError: generator raised StopIteration

```

#### Versions

I'm using MacOS, Python3:

```>>> import platform; print(platform.platform())
Darwin-18.7.0-x86_64-i386-64bit
>>> import sys; print("Python", sys.version)
Python 3.7.4 (default, Sep  7 2019, 18:27:02) 
[Clang 10.0.1 (clang-1001.0.46.4)]
>>> import numpy; print("NumPy", numpy.__version__)
NumPy 1.18.0
>>> import scipy; print("SciPy", scipy.__version__)
SciPy 1.4.1
>>> import gensim; print("gensim", gensim.__version__)
from gensim.models import word2vec;print("FAST_VERSION", word2vec.FAST_VERSION)
gensim 3.8.1
>>> from gensim.models import word2vec;print("FAST_VERSION", word2vec.FAST_VERSION)
FAST_VERSION 0
```

```
pip3 freeze | grep pattern
pattern3==3.0.0
pip3 freeze | grep gensim
gensim==3.8.1
```

PR to test changes to build recipes - adjusting Python & other-library versions. See discussion #2713.

(Replaces #2714)


I recommend the next major gensim release (4.x) also drop official Python 3.5 testing/support. Rationale:

* it's close to full end-of-life (under 10 months to 2020-09-13) – currently receiving only security fixes
* major Python-using cloud services (including Heroku, AWS Lambda, Google Cloud/Colab, etc) already only support Python 3.6+
* some useful features (like the variable annotations needed for dataclasses) only arrive in 3.6
* for most users, moving forward to 3.6.x+ isn't hard; those absolutely stuck on 3.5 can use an earlier gensim version, or try their luck in an unsupported configuration (some modules might still work in Python 3.5)

Meanwhile, Python 3.8 has been out a few months (with support at Heroku & AWS Lambda) and deserves added build/test support. So we could replace the 3.5 builds/test in our CI setup with a new 3.8 target. 
