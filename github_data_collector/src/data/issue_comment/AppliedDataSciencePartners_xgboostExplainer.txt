Hi
   I want to keep somme variable like an ID raw . After Xgboost esxplainer I lost my ID... have you an idea to keep my variable??

Thanks a lot

Paul 
Hello, 
Thanks for creating this useful package. The waterfall plots are quite informative and intuitive.
I found that when I varied the base_score in the buildExplainer function, the predicted values output in the showWaterfall function varied significantly. Concerned about the accuracy of predicted values in the actual xgboost model, the predicted values varied, but not nearly as much. Is this an error? Or am I doing something incorrectly? Should the base_score entered in the buildExplainer always match whatever was entered in the actual xgboost model?

This is what I observed for a single predicted outcome:
base_score = 0.5: pred = .48 in both xgb predict function and explainer waterfall function
base_score = 0.2: pred = .43 in xgb predict function, but explainer waterfall function was .18.*
base_score = 0.85: pred = .53 in xgb predict function, but explainer waterfall function was .83.*
*Note: in all three examples, the xgb model used in the explainer function had a base_score of 0.5. Therefore, it varied from the base_score entered in the explainer in the 2nd and 3rd examples.

Thanks for any suggestions.
I would like to package this for https://conda-forge.org/ to make it simpler to install in our environments. This requires though that releases of `xgboostExplainer` are done (git tags are sufficient).
When running the code exemple from the basic buildExplainer function i get the following error:

Creating the trees of the xgboost model...Error in xgb.model.dt.tree(col_names, model = xgb.model, trees = trees_idx) :  unused argument (trees = trees_idx)
Each call to `showWaterfall` runs `explainPredictions`. For datasets with hundreds or thousands of columns, this is tedious. Runtime can be hours long. To expedite the function call, `showWaterfall` should also accept an `explanations` variable that references prediction breakdowns that have already been made, maybe like this:

```
showWaterfall2 = function(xgb.model, explainer, DMatrix, data.matrix, 
                         idx, type = "binary", threshold = 0.0001, 
                         limits = c(NA, NA), explanations = NULL){
        if(!is.null(explanations)){
                breakdown = explanations[idx,]
        } else{
                breakdown = explainPredictions(xgb.model, 
                                               explainer, 
                                               slice(DMatrix,as.integer(idx)))
        }
etc...
```
Building the explainer relies on creating several new columns with names such as "tree" and "leaf". If those column names are already used in the training dataset, one can run into issues, specifically #23. In my case, I ran into this issue when using the explainer on a Natural Language Processing xgb model. My model was counting the instances of the word "tree" in the document within a column also named "tree".

The error is triggered when trying to row bind the `tree_list_breakdown` and `tree_breakdown` lists within the call to `buildExplainerFromTreeList`. A warning when the dataset already uses column names that include key terms such as "tree", "leaf", and "intercept" would be helpful. Perhaps written into the first few lines of the function.

Similarly, the `explainPredictions` function uses variable names that can cause issues. In my case, I had a column named "x" in my dataset that caused problems with the "x" used as the iterator in the for loop for going through all the trees. 
While I can get the package to work with these kinds of models, the stated predictions don't match when the model is not reg:linear (using "regression" type). Would it be possible to support these types of regression?
I have been trying to figure out what xgboostExplainer does under the hood and came across something strange (to my eyes anyway) when looking at the definition of findLeaves.R.  

![image](https://user-images.githubusercontent.com/26222179/44400988-0a5d1900-a545-11e8-89d5-d71c341fa6b6.png)

I can't work out:
1. why findLeaves() is used in the definition of findLeaves() (see line11)
2. how you can have three parameters of findLeaves() (see line 11) when it only has two by the definition being created. What does the `with = FALSE` part do?

Any help with either of these queries would be really appreciated!
Hi,

Is there a way to enforce constraints in the features contribution calculation, for example to enforce some features to have positive contribution?

Please note I'm already using monotonicity constraints in the xgboost training.
 
Thanks,
Paolo

I just observed that in the example code given on the medium.com blog, you calculated individual weights by simply calling rowSums(pred.breakdown) on the data set containing the feature contributions. 
I think this is not correct because adding negative and positive feature contributions may cancel out effects. 

Shouldn't it be s.th like:

weights = apply(pred.breakdown,1,function(x) sum(abs(x)))
   
I was referring to this blog:
https://medium.com/applied-data-science/new-r-package-the-xgboost-explainer-51dd7d1aa211