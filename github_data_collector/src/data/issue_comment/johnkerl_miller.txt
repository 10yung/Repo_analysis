Hi,
when I run 

```
mlr --csv --from tmp_a.csv put -q 'tee > $comune.".csv", $*'
```

using [this csv](https://gist.github.com/88819d3cef49f4ad00da4457e1361b9b), I have this error: `fopen: Too many open files`.

I'm using `Miller 5.6.1-dev`.

What's wrong on my command?

Thank you
Very similar to #159, I can't match a square bracket, asterisk, etc, regardless how many backslashes I put because `mlr_alloc_double_backslash` is doubling them. Adding exceptions for square brackets, like the one already there for period, allows me to match them as expected, but it seems we'd either need an exception for all the ERE metacharacters or take a different approach to escaping backslashes.

```
$ echo 'a=[' | c/mlr put '$a = gsub($a, "\[", "left_square")'
mlr: could not compile regex "\[" : Invalid regular expression
$ git stash pop
$ git diff
diff --git a/c/lib/mlrutil.c b/c/lib/mlrutil.c
index a532a25e..dfd86e18 100644
--- a/c/lib/mlrutil.c
+++ b/c/lib/mlrutil.c
@@ -502,7 +502,7 @@ char* mlr_alloc_double_backslash(char* input) {
        char* output = mlr_malloc_or_die(input_length + num_backslashes + 1);
        for (p = input, q = output; *p; p++) {
                if (*p == '\\') {
-                       if (p[1] != '.') {
+                       if (p[1] != '.' && p[1] != '[' && p[1] != ']') {
                                *(q++) = *p;
                        }
                        *(q++) = *p;
$ make
$ echo 'a=[' | c/mlr put '$a = gsub($a, "\[", "left_square")'
a=left_square
```
Hi,
if I run

```
echo -e 'a=2,b=3\na=4,b=1' | mlr filter '@max=max(@max,$b);$b==@max'
```

I have `a=2,b=3` and it's right.

If I start from this inputFile

```
score=1
score=1
score=1
score=1
score=1
score=1
score=1
score=7.1
score=1
```

and run

```
mlr filter '@max=max(@max,$score);$score==@max' inputFile
```

I have nearby all items back

```
score=1
score=1
score=1
score=1
score=1
score=1
score=1
score=7.1
```

Why do I not have only `score=7.1`?

Thank you
I found `miller` to be very useful with SRE tasks and often use it in Docker containers.

Sadly, there doesn't seem to be a package for Alpine available, a distribution very popular with Docker because of its small footprint. So for now I'm stuck with the large debian and ubuntu images.

Let's fix this: https://wiki.alpinelinux.org/wiki/Creating_an_Alpine_package
`head` verb strangely influences previous `tee` command (previous as "sooner in pipeline"). It reduces number of lines printed into the output file. The [input.csv](https://github.com/johnkerl/miller/files/3901303/input.csv.txt) is just increasing sequence from 1 to 1000 as shown and attached. 

Using Miller v5.6.2

Steps to reproduce: 

```bash
$ wc -l input.csv # input contains one field "id" as sequence 1 -> 1000
1001 input.csv
$ head input.csv 
id
1
2
3
4
5
6
7
8
9
$ mlr --csv filter "NR<=100" then tee chunked.csv then head -n 5 input.csv
id
1
2
3
4
5
$ wc -l chunked.csv 
7 chunked.csv
$ cat chunked.csv # chunked.csv for some reason contains 6 records..?
id
1
2
3
4
5
6
$ mlr --csv --fs ';' filter "NR<=100" then tee chunked.csv input.csv | head -n 5
id
1
2
3
4
$ wc -l chunked.csv # without head verb all 100 lines are printed
101 chunked.csv
$ head chunked.csv 
id
1
2
3
4
5
6
7
8
9
$ 
```

I'd expect `tee then head` to tee all lines and then head just the few. 
I am not sure, but I think I have run into a bug. I find it in version 5.4.0 and 5.6.2 on mac os x.

my command line is:  mlr --csv put '$a=substr($zip,0,4)' filename
and the contents of filename are:
name,zip
first,21034-1719
second,95003

My output is:
name,zip,a
first,21034-1719,2103
second,95003,(error)

Is it my understanding which is wrong?

I also try to use 5.6.2 pre-build .exe on windows10 and I get this (unrelated) error:
      0 [main] mlr (3972) C:\mlr.exe: *** fatal error - cygheap base mismatch detected - 0x18032D408/0x180317408.
This problem is probably due to using incompatible versions of the cygwin DLL.
Search for cygwin1.dll using the Windows Start->Find/Search facility
and delete all but the most recent version.  The most recent version *should*
reside in x:\cygwin\bin, where 'x' is the drive on which you have
installed the cygwin distribution.  Rebooting is also suggested if you
are unable to find another cygwin DLL.
Often when I am doing an initial reconnaissance of a dataset I want to know how many uniq values there are for each individual field. 
The mlr "uniq" verb allows me to do this one field at a time, but if I have many fields this is tedious. I could write a bash/zsh "for" loop to loop over the fields, but it would be much nicer if there was an extra option for the uniq verb like the "-u" option for count-distinct (with an extra column showing the field name).
Also it would be nice to have an option for specifying all fields without having to type them all out (which can get very messy).
Hi,
I have this CSV https://gist.github.com/aborruso/c4f48934e3aa9bada0f5d19e4963d54d .

If I run

```bash
mlr --c2p put '
  num start = 10;
  for (a = 1; a <= NR; a += 1) {
    if ($rule=="") {
		$rule=start
	} 
	elif ($rule>0) {
		$rule=$rule;
		start+=10;
	}
  }
' input.csv
```

`start` is always equals to `10`,  it seems that `start+=10` never happens.

I would like to have something like below, but above all I would like to know what I'm wrong :)

Thank you

![image](https://user-images.githubusercontent.com/30607/68742914-8ed80380-05f1-11ea-8209-929729b1e83a.png)

I would like `mlr tail` to consume `+` prefixed number to specify line number to start printing from. 

From linux `tail` command man page:
```
-n, --lines=[+]NUM
    output the last NUM lines, instead of the last 10; or use -n +NUM to output starting with line NUM
```
The line number parameter is one-based, inclusive so `tail -n +1` prints the whole file, `tail -n +2` skips the first line. Example: 
```
$ cat test.csv
id,data
1,a
2,b
3,c
4,d
$ mlr tail -n +3 test.csv
id,data
3,c
4,d
```

----

Background for this feature request: I need to split a csv file into chunks of certain number of lines say 1000. Perhaps there is a better way using mlr? Still this feature might make sense in similar cases. 

Best tool for handling csv files I've found. Big thanks. 
Hi,
I'm sure that's a direct and better way to do it, but I do not find it :(

I have in example this CSV with a carriage return in one of its field names, and my goal is to use DSL to replace it with a space.

```
"field 
A",field B
1,2
3,3
6,6
```

To do it you could run

```
mlr -I --csv -N put -S 'if (NR == 1) {for (k in $*) {$[k] = clean_whitespace(gsub($[k], "\n", " "))}}' input.csv
```

- using `-N` (a great new feature) you apply `--implicit-csv-header` and `--headerless-csv-output` and the row with field names is no more the heading, but the first data row;
- then I apply a search and replace only to the first row (using `NR == 1`);
- and at the end, since we have `--headerless-csv-output`, the first row becomes again the heading.

The result will be

```
field A,field B
1,2
3,3
6,6
```

Probably not so useful, but it has saved my day.

Thank you always @johnkerl 