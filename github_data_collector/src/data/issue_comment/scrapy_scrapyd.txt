/usr/local/bin/scrapyd-deploy:23: ScrapyDeprecationWarning: Module `scrapy.utils.http` is deprecated, Please import from `w3lib.http` instead.
Scrapy (1.8.0)

Twisted library requires Python 3.5 which causes the travis build to fail.

ImportError: Twisted on Python 3 requires Python 3.5 or later.

here's an example 
https://travis-ci.org/scrapy/scrapyd/jobs/614893114?utm_medium=notification&utm_source=github_status

Hello,

Could someone pls review this PR about #12?

Thank you
```
(VideoManage) ➜  video_scrapy git:(master) ✗ scrapyd
Unhandled Error
Traceback (most recent call last):
  File "/Users/allran/.virtualenv/VideoManage/lib/python3.7/site-packages/twisted/application/app.py", line 674, in run
    runApp(config)
  File "/Users/allran/.virtualenv/VideoManage/lib/python3.7/site-packages/twisted/scripts/twistd.py", line 25, in runApp
    runner.run()
  File "/Users/allran/.virtualenv/VideoManage/lib/python3.7/site-packages/twisted/application/app.py", line 381, in run
    self.application = self.createOrGetApplication()
  File "/Users/allran/.virtualenv/VideoManage/lib/python3.7/site-packages/twisted/application/app.py", line 453, in createOrGetApplication
    application = getApplication(self.config, passphrase)
--- <exception caught here> ---
  File "/Users/allran/.virtualenv/VideoManage/lib/python3.7/site-packages/twisted/application/app.py", line 464, in getApplication
    application = service.loadApplication(filename, style, passphrase)
  File "/Users/allran/.virtualenv/VideoManage/lib/python3.7/site-packages/twisted/application/service.py", line 416, in loadApplication
    application = sob.loadValueFromFile(filename, 'application')
  File "/Users/allran/.virtualenv/VideoManage/lib/python3.7/site-packages/twisted/persisted/sob.py", line 177, in loadValueFromFile
    eval(codeObj, d, d)
  File "/Users/allran/.virtualenv/VideoManage/lib/python3.7/site-packages/scrapyd/txapp.py", line 2, in <module>
    from scrapyd import get_application
builtins.ImportError: cannot import name 'get_application' from 'scrapyd' (/Users/allran/.virtualenv/VideoManage/lib/python3.7/site-packages/scrapyd/__init__.py)


Failed to load application: cannot import name 'get_application' from 'scrapyd' (/Users/allran/.virtualenv/VideoManage/lib/python3.7/site-packages/scrapyd/__init__.py)
```
```curl http://192.168.1.1:100/cancel.json -d project=Scrapy -d job=da045f 7ed9bf11e9a89d0242ac110004```
```{"node_name": "332e7d9a8e6a", "status": "ok", "prevstate": "running"}```
But...The mission has not been stopped.

scrapyweb v1.1.0
it always happend,when i open the scrapyd service,and after some time ,if i do a request by the api,it raise the exception:requests.exceptions.ReadTimeout. However ,if i restart the service or just press "ctrl+c" ,it works again
I'm adding some customization to this project on my own fork to support MondoDB jobs and queues. However I've encountered an issue where messages get lost if the spawn_process function fails for whatever reason, likely memory or not enough resources.

Could someone give me some pointers as of what would be best practice to handle such errors? As of now I've added a stack to ensure that all messages get addressed. 

```python
    def _wait_for_project(self, slot):
        poller = self.app.getComponent(IPoller)
        if not self.stack:
            poller.next().addCallback(self._spawn_process, slot)
        else:
            message = self.stack.pop()
            self._spawn_process(message, slot)

    def _spawn_process(self, message, slot):
        self.stack.append(message)
        msg = native_stringify_dict(message, keys_only=False)
        project = msg['_project']
        args = [sys.executable, '-m', self.runner, 'crawl']
        args += get_crawl_args(msg)
        e = self.app.getComponent(IEnvironment)
        env = e.get_environment(msg, slot)
        env = native_stringify_dict(env, keys_only=False)
        pp = ScrapyProcessProtocol(slot, project, msg['_spider'], \
            msg['_job'], env)
        pp.deferred.addBoth(self._process_finished, slot)
        reactor.spawnProcess(pp, sys.executable, args=args, env=env)
        self.processes[slot] = pp
        self.stack.pop()
```
This PR use project_priority_map to store (priority, -timestamp) as value,
so as to respect both job priorities across projects and the FIFO principle.

See also issue #187 and PR #344.

`remove_pending_jobs = off`