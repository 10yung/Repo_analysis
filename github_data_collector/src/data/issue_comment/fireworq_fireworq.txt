Great project, I am very interested in using this in my own project but would like to deploy in Kubernetes and am finding a bit difficult to build these images locally (not using docker-compose). Any chance you can create a docker image and push an official image to docker hub? 
Pull request to https://github.com/lukaszx0/queues.io

List at https://queues.io/

In order to improve performance maybe there will be useful to add some benchmarks to readme, some Sidekiq vs Fireworq performance, architecture diagram, etc.
Hello, I was wondering if it is possible to save the job when they finish and the result is successful, now I think they are eliminated, but for example: if I need to keep track of the job that results successful and the result of each job means something useful to verify .

maybe I could have an option that saves the successful jobs for a period of time and then it is eliminated, that would help not to save a lot of data in the database.

pd: Merry Christmas :santa: 

Hi there!

First of all, nice promising project! I'm looking into using it but there are some things I'd like to point out:

1. you seem to rely on MySQL's locking functions (`GET_LOCK()` and `IS_USED_LOCK()`) for coordination;

While that's ok for small single-instance MySQL instances, that won't work on XtraDB clusters. I have a 5-node XtraDB 5.7 cluster that I would like to use with `fireworq` for high-availability and redundancy but because of the nature of distributed transactions and locking, these functions are not permitted (`ERROR 1105 (HY000): Percona-XtraDB-Cluster prohibits use of GET_LOCK with pxc_strict_mode = ENFORCING`) - same with `LOCK TABLE` and friends.

I suggest using an external `etcd` instance for [leader election](https://godoc.org/github.com/coreos/etcd/clientv3/concurrency#Election) - if you don't want to depend on `etcd`, you may implement your own leader election logic using [raft](https://github.com/coreos/etcd/tree/master/raft) - so all `fireworq` nodes would talk directly to each other to determine who's the "leader" in the `fireworq` cluster - if the leader goes down, the other remaining machines would decide between themselves who the new leader is (we can get this for free with `etcd`).

2. relying on `AUTO_INCREMENT` IDs;

While that works great for single-instance MySQL servers, it makes things more complicated when running under a cluster because nodes need to coordinate between them (so IDs don't collide). Since you're already using `uint64s` for IDs, I would suggest using something more deterministic like the current time in [nanoseconds resolution](https://repl.it/repls/TepidBlushingGodwit) or [something else more guaranteed to be unique](https://blog.kowalczyk.info/article/JyRZ/generating-good-unique-ids-in-go.html), so that IDs can be generated by `fireworq` nodes themselves and INSERTed directly into the database without relying on AUTO_INCREMENT, while guaranteeing that they're unique (and time-sorted) and thus having good indexing properties.

The end result would be that job queueing could be done by any `fireworq` instance in the cluster, not just the master, with minimal to no coordination between them.

3. eliminating the primary/backup roles;

It would be possible to scale up `fireworq` to have many nodes capable of working at the same time for high-availability, redundancy and increased performance. For example, imagine that you have a 4-node `fireworq` cluster and you're able to queue jobs to any of them at any given time and have a load balancer in front of them so that downed `fireworq` nodes are automatically worked around.

This could be done by adding another field to a job queue (like `ring_key INT UNSIGNED NOT NULL`) - and whenever pushing a new job to a queue, you just add a random uint32 value there, let's say, from 1 to 1024 (this will be the maximum number of nodes a cluster can have). 

Whenever nodes join or leave the cluster, you rebalance their ring assignments, so each currently-alive node gets assigned a portion of the "ring" of all possible values (from 1 till 1024). For example, if your cluster has 4 nodes, the ring would look like:

```
node1: from 1 to 256 (256 keys)
node2: from 257 to 512 (256 keys)
node3: from 513 to 769 (256 keys)
node4: from 770 to 1024 (256 keys)
```

So, for example, `node3` would poll the database for available work to be done by including in the query something like `AND (ring_key BETWEEN 513 AND 769)`. Say `node3` goes offline because the machine was turned off; the leader (`node4` - determined by the leader election algorithm - either by using an external `etcd` instance or built-in Raft) would detect that and rebalance the ring between the remaining nodes that are alive and announce to all machines this new distribution, so our distribution now would look like:

```
node1: from 1 to 341 (341 keys)
node2: from 342 to 683 (341 keys)
node4: from 684 to 1024 (340 keys)
```

This coordination between the leader node and followers could also be backed by `etcd` or managed manually through `raft` (if you want to avoid adding an external dependency). If the leader itself goes down, Raft/etcd takes care of a new election for a new leader, and once that new leader is elected, it triggers the same rebalance process; this way all nodes keep on working as if nothing happened and can scale in capacity up and down (MySQL/PXC scaling would be the bottleneck).

What do you think? Is that something you guys would be interested in implementing? If not, I could look into adding support for that myself (using `etcd`) since I'll need it, but it might be a while until I'll be able to start working on that.
Nice project but adding a new database backend just for the fireworq is not very nice
What about having different kind of storage backend like MySQL & MongoDB (or others...) ?

