Section 1.5.1 (Downloading and starting the VM) talks about installing Oracle VirtualBox and Vagrant.  
I already have VirtualBox running.  Do I need to install Vagrant on my Mac or in VirtualBox.  


While doing spark-submit for the realtime dashboard application, i get following error üëç 
spark@spark-in-action:~/uc1-docker$ ./run-all.sh
Zookeeper already started
Kafka already started
sia-dashboard already running
Submitting Spark job
Starting Kafka direct stream to broker list: 192.168.10.2:9092
Exception in thread "main" java.lang.NoClassDefFoundError: scala/collection/GenTraversableOnce$class
        at kafka.utils.Pool.<init>(Pool.scala:28)
        at kafka.consumer.FetchRequestAndResponseStatsRegistry$.<init>(FetchRequestAndResponseStats.scala:60)
        at kafka.consumer.FetchRequestAndResponseStatsRegistry$.<clinit>(FetchRequestAndResponseStats.scala)
        at kafka.consumer.SimpleConsumer.<init>(SimpleConsumer.scala:39)
        at org.apache.spark.streaming.kafka.KafkaCluster.connect(KafkaCluster.scala:52)
        at org.apache.spark.streaming.kafka.KafkaCluster$$anonfun$org$apache$spark$streaming$kafka$KafkaCluster$$withBrokers$1.apply(KafkaCluster.scala:345)
        at org.apache.spark.streaming.kafka.KafkaCluster$$anonfun$org$apache$spark$streaming$kafka$KafkaCluster$$withBrokers$1.apply(KafkaCluster.scala:342)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
        at org.apache.spark.streaming.kafka.KafkaCluster.org$apache$spark$streaming$kafka$KafkaCluster$$withBrokers(KafkaCluster.scala:342)
        at org.apache.spark.streaming.kafka.KafkaCluster.getPartitionMetadata(KafkaCluster.scala:125)
        at org.apache.spark.streaming.kafka.KafkaCluster.getPartitions(KafkaCluster.scala:112)
        at org.apache.spark.streaming.kafka.KafkaUtils$.getFromOffsets(KafkaUtils.scala:211)
        at org.apache.spark.streaming.kafka.KafkaUtils$.createDirectStream(KafkaUtils.scala:484)
        at org.sia.loganalyzer.StreamingLogAnalyzer$.main(StreamingLogAnalyzer.scala:76)
        at org.sia.loganalyzer.StreamingLogAnalyzer.main(StreamingLogAnalyzer.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: scala.collection.GenTraversableOnce$class
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        ... 25 more


The task is to count the number of buy and sell orders per second. 

The code example does not take into account the time stamp at all. How is it possible to know that what is reduced is actually within a second. What i understood was that the mini-batch was every 3 seconds. 

Honestly i am quite confused, when we say the number of sell and buy per second, what do we mean exactly ? Do we mean buy and sell that have a time stamp that fall within 1 second of distance, do we mean what we get per second, independently of the time stamp ? 

Can this be at least clarified ?
Hi,
sources of dashboard in:
https://github.com/spark-in-action/first-edition/releases/
* zip
* tar.gz

do not contain sources of dashboard.