
<!--
    WARNING: THE KALDI ISSUE TRACKER IS **ONLY** USED FOR KALDI DEVELOPMENT!

    If you have a question about using Kaldi, please use the kald-help discussion group:

    https://groups.google.com/forum/#!forum/kaldi-help

    Instructions for joining are available at: http://kaldi-asr.org/forums.html
-->

Is there a reason, why miniconda must be installed in the home directory? (Many scripts have the path hard coded)
https://github.com/kaldi-asr/kaldi/blob/cbdbedefcdd47e02a685c1dc2d1b128c30bdf6b2/tools/extras/install_miniconda.sh#L6

I would like to have miniconda installed either somewhere relative to the `kaldi_root` or have an environment variable to set the path to the miniconda installation.

My motivation to have it somewhere else:
 - When miniconda is installed in the home directory, I cannot install miniconda in a shared folder for all users in our department (currently done for kaldi)
 - We make a backup of the home directory, but the miniconda should not be in the backup (miniconda can be large, some packages need much space: e.g. numpy, mkl, pytorch, tensorflow, ...)
the training logs for kaldi pybind with PyTorch and nnet3 training are also contained.

kaldi pybind shares the same network architecture and `feats.scp` with nnet3. 

There are only two differences between kaldi pybind and nnet3:
(1) kaldi pybind uses BatchNorm to replace the first LDA layer
(2) kaldi pybind uses the optimizer from PyTorch.

WER/CER from kaldi nnet3 is better than kaldi pybind but kaldi pybind training with PyTorch is much faster.

The training time in total for 6 epochs is summarized as follows:
   - kaldi pybind with PyTorch: about 45 minutes
   - kaldi nnet3: about 4 hours 37 minutes == 277 minutes

It is possible that kaldi nnet3 can use less number of epochs to converge to a point that
has better CER/WER than kaldi pybind.

A very simple scheduler is used in PyTorch; the results for kaldi pybind may be improved
by using a better learning rate scheduler.

----

So what do we gain from kaldi pybind ?

1. training time. It is much faster
2. free to use various kinds of networks supported by PyTorch or it is very easy
  to write your own `nn.Module`.
3. you can try distributed training supported by Pytorch, e.g., DDP, or use horovod.
4. other fancy stuff limited by your imagination.
When running aspire/s5/local/multi_condition/run_nnet2_common.sh, 

The line "python local/multi_condition/aspire_prep_rir_noise_list.py data/impulses_noises data/impulses_noises/info"

Could not find the file aspire_prep_rir_noise_list.py

Could you include the file into the directory?
that python subprocess open code need [universal_newlines=True] option at python3.

i checked it in my env and it's working well in python 2 and 3.
In quite some shell scripts #!/bin/bash is used, this won't work on distributions that don't have bash in /bin , for example freeBSD. Could you maybe replace them with #!/usr/bin/env bash , which should be more portable.

example are the scripts in tools/extra or in egs/wsj/s5
There is a chance #active tokens is less than min active config, and this will lead to pruning beam and cutoff be inf, which is not disireable.
Fix this bug
Fix #3626

Now it is able to read/write `*.npy` files.

Note that it supports only little endian, single/double precision, 1-D/2-D numpy arrays with C order.
- I'd like a diagnostic script to be created that can keep track of how fast things are training, similar to how Kaldi's existing diagnostic outputs work.  The initial thing is to discover the hierarchical structure of the model in terms of modules or whatever they are called (should be fairly easy), create some data structure that mirrors that and is easy to traverse and has hooks on which we can add diagnostic stuff.  We then need to find a way to print out or graphically display information associated with the individual parameters in a way that somehow reflects the structure.  Ideally it would be nice to have it display the layers in the `natural` order, and we could possibly have some kind of convention for that?  E.g. have the layer names dictate the order in some suitable way?

The most important diagnostic is the relative change in each parameter matrix, which has to do with the norm of the matrix and the norm of the change.  The absolute magnitude of each parameter matrix, as l2 norm, is useful too.

There are lots of diagnostics that Kaldi produces that won't be possible to extract from the model because they are data-dependent, like stats about the activation of various ReLU units... I think this can be left till later, if at all, it would have to be done via special moudles that were  no-ops but which stored stats.
This is an ongoing task but whoever takes it on can close the issue.  
Also the kaldi10 branch should probably be kept up to date; it contains changes that will probably be needed at some point.