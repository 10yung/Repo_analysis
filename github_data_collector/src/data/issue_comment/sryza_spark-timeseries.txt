    Exponential Smoothing Model with damped trend method is very successful for many series, especially for longer forecast horizons. Reference to https://otexts.org/fpp2/holt.html
 
  Hope to have a Exponential Smoothing Model with damped trend method.
Whats wrong with my program???
java.lang.IllegalArgumentException: requirement failed: Lengths must match!: a.length == b.length (0 != 17)
	at breeze.linalg.operators.DenseVectorOps$$anon$187.apply(DenseVectorOps.scala:189)
	at breeze.linalg.operators.DenseVectorOps$$anon$187.apply(DenseVectorOps.scala:187)
	at breeze.linalg.NumericOps$class.$colon$eq(NumericOps.scala:183)
	at breeze.linalg.DenseVector.$colon$eq(DenseVector.scala:51)
	at com.cloudera.sparkts.models.ARIMAModel.forecast(ARIMA.scala:691)
	at mlib.local2.TimeSeriesModel$$anonfun$3.apply(TimeSeriesModel.scala:74)
	at mlib.local2.TimeSeriesModel$$anonfun$3.apply(TimeSeriesModel.scala:71)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$foreach$1$$anonfun$apply$27.apply(RDD.scala:875)
	at org.apache.spark.rdd.RDD$$anonfun$foreach$1$$anonfun$apply$27.apply(RDD.scala:875)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Hi @sryza 
I try to run a similar spark-ts example as Stocks.py As you did there, I would like to make a `time_series_rdd_from_observations(dtIndex, obs, "timestamp", "timestamp", "price")`. Running the code yields in 
`Exception in thread "Thread-3" java.lang.NoClassDefFoundError: org/apache/spark/sql/DataFrame
	at java.lang.Class.getDeclaredMethods0(Native Method)
	at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)
	at java.lang.Class.privateGetPublicMethods(Class.java:2902)
	at java.lang.Class.getMethods(Class.java:1615)
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:284)
	at py4j.commands.ReflectionCommand.getMember(ReflectionCommand.java:140)
	at py4j.commands.ReflectionCommand.execute(ReflectionCommand.java:91)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.DataFrame
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 9 more
ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File "/home/username/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 883, in send_command
    response = connection.send_command(command)
  File "/home/username/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1040, in send_command
    "Error while receiving", e, proto.ERROR_ON_RECEIVE)
Py4JNetworkError: Error while receiving
Traceback (most recent call last):
  File "/home/username/path/to/file/test.py", line 71, in <module>
    time_series_rdd_from_observations(dtIndex, obs, "timestamp", "timestamp", "price")
  File "/home/username/.local/lib/python2.7/site-packages/sparkts/timeseriesrdd.py", line 235, in time_series_rdd_from_observations
    jtsrdd = jvm.com.cloudera.sparkts.api.java.JavaTimeSeriesRDDFactory.javaTimeSeriesRDDFromObservations( \
  File "/home/username/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1363, in __getattr__
py4j.protocol.Py4JError: com.cloudera.sparkts.api.java.JavaTimeSeriesRDDFactory.javaTimeSeriesRDDFromObservations does not exist in the JVM`. 
I run the file via `spark-submit --jars sparkts-0.3.0-jar-with-dependencies.jar test.py`.

Any idea?
when I builded the project(mvn package),I see org.apache.spark.SparkException:Only one SparkContext may be running in this JVM(see SPARK-2243).To ignore the error set spark.driver.allowMultipleContexts=true.I set spark.driver.allowMultipleContexts=true,but there is still error.how can I solve the problem.
I am following https://github.com/sryza/spark-ts-examples/blob/master/jvm/src/main/scala/com/cloudera/tsexamples/SingleSeriesARIMA.scala example using Spark-2.1.0

Can someone help me out to get confidence interval for the forcast
as given by python http://www.statsmodels.org/dev/generated/statsmodels.tsa.arima_model.ARIMAResults.forecast.html#statsmodels.tsa.arima_model.ARIMAResults.forecast

conf_int : array
2d array of the confidence interval for the forecast

Thanks,
Dasharath
BOBYQA in apache-math sometimes into infinite loop，this custom ver just fixed it based on FORTRAN code & MATH-621 。 The infinite loop sample can see HoltWintersModelSuite 。
@sryza I am using cross sectional time series for a problem (similar to ARX models) where I got into problems of size > 1024 and looks like the gram solver is not scaling (assuming OLS uses that)...Are you ok if I move the spark-ts solvers to use BFGS (for smooth opt) / OWLQN (for trend filter) ?
Similar to scikit-learn, it will be useful to have KNN model for time series prediction. 
@sryza We would like to contribute KNN model to the spark-ts package. Opening it for initial review. I will introduce KNNModel and implement the prediction/forecast using it. Also the lag function should use TimeSeries.lag..we optimized it for performance.

Reference: https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/neighbors/regression.py
Hi，
   I have got TimeSeriesRDD in spark, but I don't know how to get the forecast.

 val rdd = sc.textFile(path).map { line =>
   val tokens = line.split(",")
   val series = new DenseVector(tokens.tail.map(_.toDouble))
   (tokens.head, series.asInstanceOf[Vector])
 }

 val start = ZonedDateTime.of(2015, 4, 9, 0, 0, 0, 0, ZoneId.of("Z"))
 val index = uniform(start, 250, new DayFrequency(1))
 val tsRDD = new TimeSeriesRDD(index, rdd);
 val arimaModel = tsRDD.map(tuple => (tuple._1, ARIMA.fitModel(1,0,1,tuple._2)))


Thanks