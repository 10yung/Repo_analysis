I tested the latest Chinese CoreNLP 3.92 version, and found the results are quite horrible. Here are few examples:

我的朋友：always tags "我的" as one NN token.
我的狗吃苹果： ‘我的狗’ tagged as one NN token.
他的狗吃苹果：'狗吃' tagged as one NN token.
高质量就业成时代: '就业' tagged as VV

When I compared them with the results from http://nlp.stanford.edu:8080/parser/index.jsp, surprisingly, the results of those examples are all right. Why is that? Are the models different? Is there a bug in the new 3.92 version model? 
```
from nltk.tag import StanfordNERTagger

stanford_classifier  =  '/home/test/stanford-ner-2018-10-16/classifiers/example.serialized.ncc.ncc.ser.gz'
stanford_ner = '/home/test/stanford-ner-2018-10-16/stanford-ner.jar'
# Build NER tagger object
st = StanfordNERTagger(stanford_classifier, stanford_ner)

# A sample text for NER tagging
text = 'Ram works at Google' 
# Tag the sentence and print output
tagged = st.tag(str(text).split())
print(tagged)
```

Error reported:
/home/test/Desktop/myenv/bin/python3 /home/test/NlpSource-master/sss.py
Traceback (most recent call last):
  File "/home/test/NlpSource-master/sss.py", line 32, in <module>
    tagged = st.tag(str(text).split())
  File "/home/test/Desktop/myenv/lib/python3.7/site-packages/nltk/tag/stanford.py", line 93, in tag
    return sum(self.tag_sents([tokens]), [])
  File "/home/test/Desktop/myenv/lib/python3.7/site-packages/nltk/tag/stanford.py", line 116, in tag_sents
    cmd, classpath=self._stanford_jar, stdout=PIPE, stderr=PIPE
  File "/home/test/Desktop/myenv/lib/python3.7/site-packages/nltk/internals.py", line 146, in java
    raise OSError('Java command failed : ' + str(cmd))
OSError: Java command failed : ['/usr/bin/java', '-mx1000m', '-cp', '/home/test/stanford-ner-2018-10-16/stanford-ner.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-loadClassifier', '/home/test/stanford-ner-2018-10-16/classifiers/example.serialized.ncc.ncc.ser.gz', '-textFile', '/tmp/tmph8k3mnol', '-outputFormat', 'slashTags', '-tokenizerFactory', 'edu.stanford.nlp.process.WhitespaceTokenizer', '-tokenizerOptions', '"tokenizeNLs=false"', '-encoding', 'utf8']
Invoked on Tue Jan 14 13:52:00 IST 2020 with arguments: -loadClassifier /home/test/stanford-ner-2018-10-16/classifiers/example.serialized.ncc.ncc.ser.gz -textFile /tmp/tmph8k3mnol -outputFormat slashTags -tokenizerFactory edu.stanford.nlp.process.WhitespaceTokenizer -tokenizerOptions "tokenizeNLs=false" -encoding utf8
tokenizerOptions="tokenizeNLs=false"
loadClassifier=/home/test/stanford-ner-2018-10-16/classifiers/example.serialized.ncc.ncc.ser.gz
encoding=utf8
outputFormat=slashTags
textFile=/tmp/tmph8k3mnol
tokenizerFactory=edu.stanford.nlp.process.WhitespaceTokenizer
Exception in thread "main" java.lang.RuntimeException: java.lang.ClassCastException: class java.util.Properties cannot be cast to class [Ledu.stanford.nlp.util.Index; (java.util.Properties is in module java.base of loader 'bootstrap'; [Ledu.stanford.nlp.util.Index; is in unnamed module of loader 'app')
	at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1520)
	at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:2993)
Caused by: java.lang.ClassCastException: class java.util.Properties cannot be cast to class [Ledu.stanford.nlp.util.Index; (java.util.Properties is in module java.base of loader 'bootstrap'; [Ledu.stanford.nlp.util.Index; is in unnamed module of loader 'app')
	at edu.stanford.nlp.ie.crf.CRFClassifier.loadClassifier(CRFClassifier.java:2600)
	at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1473)
	at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1505)
	at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1516)
	... 1 more


Process finished with exit code 1
I am testing CoreNLPClient with the sentence "Apple is great" with the following output:

Starting server with command: java -Xmx8G -cp /Users/congminmin/nlp/stnlp/stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties StanfordCoreNLP.properties -preload [tokenize]
{'index': 1, 'word': 'apple', 'originalText': 'apple', 'lemma': 'apple', 'characterOffsetBegin': 0, 'characterOffsetEnd': 5, 'pos': 'NN', 'ner': 'O', 'speaker': 'PER0', 'before': '', 'after': ' '}
{'index': 2, 'word': 'is', 'originalText': 'is', 'lemma': 'be', 'characterOffsetBegin': 6, 'characterOffsetEnd': 8, 'pos': 'VBZ', 'ner': 'O', 'speaker': 'PER0', 'before': ' ', 'after': ' '}
{'index': 3, 'word': 'good', 'originalText': 'good', 'lemma': 'good', 'characterOffsetBegin': 9, 'characterOffsetEnd': 13, 'pos': 'JJ', 'ner': 'O', 'speaker': 'PER0', 'before': ' ', 'after': ''}

What are 'ner', 'speaker' in the output?
I have a token which is recognized as others in some classifiers however the same token is identified as Person or Organization. In such a case the entity need to be identified whenever the first classifier detected as other.

example models used are 
ner.model = edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz,edu/stanford/nlp/models/ner/english.conll.4class.caseless.distsim.crf.ser.gz

Any Rule to be defined to overwrite the condition
**Started server with command:**
java -mx4g -cp "*" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -annotators "tokenize,ssplit,pos,lemma,parse,sentiment" -port 9000 -timeout 30000

**Python Client:**
`
from collections import defaultdict
from stanfordcorenlp import StanfordCoreNLP
import json

class StanfordNLP:
    def __init__(self, host='http://localhost', port=9000):
        self.nlp = StanfordCoreNLP(host, port=port,
                                   timeout=30000)  # , quiet=False, logging_level=logging.DEBUG)
        self.props = {
            'annotators': 'tokenize, ssplit, pos, lemma, ner, parse, depparse, dcoref, relation, truecase',
            'pipelineLanguage': 'en',
            'truecase.overwriteText': 'true',
            'outputFormat': 'json'
        }If I provide the input text :
text = 'rajesh lives in hyderbad'

    def ner(self, sentence):
        return self.nlp.ner(sentence)

    def annotate(self, sentence):
        return json.loads(self.nlp.annotate(sentence, properties=self.props))

    @staticmethod
    def tokens_to_dict(_tokens):
        tokens = defaultdict(dict)
        for token in _tokens:
            tokens[int(token['index'])] = {
                'ner': token['ner']
            }
        return tokens

if __name__ == '__main__':
    sNLP = StanfordNLP()
    **text = 'Rajesh lives in Hyderabad'**
    print ("NER:", `sNLP.ner(text))`

**_Expected Output:
NER: [('Rajesh', 'PERSON'), ('lives', 'O'), ('in', 'O'), ('Hyderabad', 'LOCATION')]_**

**_Actual Output:
NER: [('Rajesh', 'PERSON'), ('lives', 'O'), ('in', 'O'), ('Hyderabad', 'LOCATION')]_**

**If I provide the input text :
text = 'rajesh lives in hyderbad'**
**_Expected Output:
NER: [('rajesh', 'PERSON'), ('lives', 'O'), ('in', 'O'), ('hyderabad', 'LOCATION')]_**

**_Actual Output:
NER: [('rajesh', 'O'), ('lives', 'O'), ('in', 'O'), ('hyderabad', 'O')]_**

**I wish to solve this by using true-case annotation however my attempt didn't worked out**

Hi, I've seen NFLAnnotator.java in `src/edu/stanford/nlp/pipeline/package-info.java` but I can not find this file. Where the NFLAnnotator.java locates ?
This sentence generates a crash when trying to train the English SR Parser:

```
( (S (NP-SBJ (DT the) (NNPS knights) (NNP inn)) (VP (VBD was) (ADJP-PRD (ADJP (JJ small)) (ADJP (RB very) (JJ small)))) (. .)) (FRAG (PRN (S (NP-SBJ (PRP i)) (VP (VBP mean)))) (NP (CD 1) (NN room)) (PP (IN in) (NP (DT every) (NN room))) (. !)) (UCP (S (NP-SBJ (PRP it)) (VP (VBD was) (ADJP-PRD (JJ cozy)) (NP-ADV (DT a) (JJ little)))) (CC and) (NP (DT a) (JJ small) (NN tv)) (. .)) )
```
I am using the `ColumnDataClassifier.java  ` and for each of my test Datum and I get following scores like in the bellow screenshot:

<img width="896" alt="Screenshot 2019-12-07 at 10 04 11" src="https://user-images.githubusercontent.com/37305687/70420758-b0999e80-1a68-11ea-85b3-6f78f9e02a11.png">

**My issue** is: What is the **max value of the score** that can reach each test Datum ?
And it is possible to set this **max-score-value** to something like **10**. The purpose is to establish somme statistics on my **test Datum**.
I am getting IndexOutofBoundsException while running the annotation pipeline. Following is the trace of the error that I am getting:

Adding annotator tokenize
Adding annotator ssplit
Adding annotator pos
Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.8 sec].
Adding annotator lemma
Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].
ner.fine.regexner: Read 580705 unique entries out of 581864 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_caseless.tab, 0 TokensRegex patterns.
ner.fine.regexner: Read 4869 unique entries out of 4869 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_cased.tab, 0 TokensRegex patterns.
ner.fine.regexner: Read 585574 unique entries from 2 files
Adding annotator parse
Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.5 sec].

Processing file /Users/syau/Desktop/Fall2019/830-610-890/XMLParser/Pilot/RST-PDTB-parser/DPLP/data/input.txt ... writing to /Users/syau/Desktop/Fall2019/830-610-890/XMLParser/Pilot/RST-PDTB-parser/DPLP/CoreNLP/input.txt.out
Exception in thread "main" java.lang.IndexOutOfBoundsException: Index 9 out of bounds for length 9
	at java.base/jdk.internal.util.Preconditions.outOfBounds(Preconditions.java:64)
	at java.base/jdk.internal.util.Preconditions.outOfBoundsCheckIndex(Preconditions.java:70)
	at java.base/jdk.internal.util.Preconditions.checkIndex(Preconditions.java:248)
	at java.base/java.util.Objects.checkIndex(Objects.java:372)
	at java.base/java.util.ArrayList.get(ArrayList.java:458)
	at edu.stanford.nlp.pipeline.NERCombinerAnnotator.transferNERAnnotationsToAnnotation(NERCombinerAnnotator.java:403)
	at edu.stanford.nlp.pipeline.NERCombinerAnnotator.annotate(NERCombinerAnnotator.java:495)
	at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:639)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:649)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1228)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1062)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.run(StanfordCoreNLP.java:1328)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1391)
I am trying to train a own ner, but I always got out of memory.
Therefore, I try use batch training. 
The following is my step and cmd.
- java -cp "stanford-ner.jar" -mx4g edu.stanford.nlp.ie.crf.CRFClassifier -prop train/prop.txt

But after my first batch training, I got the domain.gz. How I train second batch based on domain.gz.
Because my input of cmd is stanford-ner.jar.
