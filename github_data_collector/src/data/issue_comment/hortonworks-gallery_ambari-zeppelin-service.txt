hello, i have a custom service in ambari,and when i changed the configuration via ambari ui it gave me success but that value didnt change in real linux machine.can u give me some help?thanks a lot.
Zepplin install failed on Azure HDP 2.5 standard 3 master 3 node cluster. 


x
master1.t53hiwxtr3xunlr5uyfutxkiaa.bx.internal.cloudapp.net
 Tasks
  Copy Open Zeppelin Notebook Install
stderr:   /var/lib/ambari-agent/data/errors-221.txt

Traceback (most recent call last):
  File "/var/lib/ambari-agent/cache/stacks/HDP/2.4/services/ZEPPELIN/package/scripts/master.py", line 235, in <module>
    Master().execute()
  File "/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py", line 219, in execute
    method(env)
  File "/var/lib/ambari-agent/cache/stacks/HDP/2.4/services/ZEPPELIN/package/scripts/master.py", line 52, in install
    Execute('sudo yum install -y epel-release')
  File "/usr/lib/python2.6/site-packages/resource_management/core/base.py", line 154, in __init__
    self.env.run()
  File "/usr/lib/python2.6/site-packages/resource_management/core/environment.py", line 158, in run
    self.run_action(resource, action)
  File "/usr/lib/python2.6/site-packages/resource_management/core/environment.py", line 121, in run_action
    provider_action()
  File "/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py", line 238, in action_run
    tries=self.resource.tries, try_sleep=self.resource.try_sleep)
  File "/usr/lib/python2.6/site-packages/resource_management/core/shell.py", line 70, in inner
    result = function(command, **kwargs)
  File "/usr/lib/python2.6/site-packages/resource_management/core/shell.py", line 92, in checked_call
    tries=tries, try_sleep=try_sleep)
  File "/usr/lib/python2.6/site-packages/resource_management/core/shell.py", line 140, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File "/usr/lib/python2.6/site-packages/resource_management/core/shell.py", line 291, in _call
    raise Fail(err_msg)
resource_management.core.exceptions.Fail: Execution of 'sudo yum install -y epel-release' returned 1. sudo: sorry, you must have a tty to run sudo
stdout:   /var/lib/ambari-agent/data/output-221.txt

2017-04-21 02:55:53,398 - The hadoop conf dir /usr/hdp/current/hadoop-client/conf exists, will call conf-select on it for version 2.4.0.0-169
2017-04-21 02:55:53,398 - Checking if need to create versioned conf dir /etc/hadoop/2.4.0.0-169/0
2017-04-21 02:55:53,398 - call['conf-select create-conf-dir --package hadoop --stack-version 2.4.0.0-169 --conf-version 0'] {'logoutput': False, 'sudo': True, 'quiet': False, 'stderr': -1}
2017-04-21 02:55:53,420 - call returned (1, '/etc/hadoop/2.4.0.0-169/0 exist already', '')
2017-04-21 02:55:53,420 - checked_call['conf-select set-conf-dir --package hadoop --stack-version 2.4.0.0-169 --conf-version 0'] {'logoutput': False, 'sudo': True, 'quiet': False}
2017-04-21 02:55:53,441 - checked_call returned (0, '/usr/hdp/2.4.0.0-169/hadoop/conf -> /etc/hadoop/2.4.0.0-169/0')
2017-04-21 02:55:53,441 - Ensuring that hadoop has the correct symlink structure
2017-04-21 02:55:53,442 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf
2017-04-21 02:55:53,443 - Group['spark'] {}
2017-04-21 02:55:53,444 - Group['zeppelin'] {}
2017-04-21 02:55:53,444 - Group['hadoop'] {}
2017-04-21 02:55:53,444 - Group['users'] {}
2017-04-21 02:55:53,444 - Group['knox'] {}
2017-04-21 02:55:53,445 - User['hive'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': [u'hadoop']}
2017-04-21 02:55:53,445 - User['storm'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': [u'hadoop']}
2017-04-21 02:55:53,446 - User['zookeeper'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': [u'hadoop']}
2017-04-21 02:55:53,446 - User['oozie'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': [u'users']}
2017-04-21 02:55:53,447 - User['atlas'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': [u'hadoop']}
2017-04-21 02:55:53,447 - User['ams'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': [u'hadoop']}
2017-04-21 02:55:53,448 - User['falcon'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': [u'users']}
2017-04-21 02:55:53,448 - User['tez'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': [u'users']}
2017-04-21 02:55:53,449 - User['zeppelin'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': [u'hadoop']}
2017-04-21 02:55:53,450 - User['spark'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': [u'hadoop']}
2017-04-21 02:55:53,450 - User['ambari-qa'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': [u'users']}
2017-04-21 02:55:53,451 - User['flume'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': [u'hadoop']}
2017-04-21 02:55:53,451 - User['kafka'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': [u'hadoop']}
2017-04-21 02:55:53,452 - User['hdfs'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': [u'hadoop']}
2017-04-21 02:55:53,453 - User['sqoop'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': [u'hadoop']}
2017-04-21 02:55:53,453 - User['yarn'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': [u'hadoop']}
2017-04-21 02:55:53,454 - User['mapred'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': [u'hadoop']}
2017-04-21 02:55:53,454 - User['hbase'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': [u'hadoop']}
2017-04-21 02:55:53,455 - User['knox'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': [u'hadoop']}
2017-04-21 02:55:53,455 - User['hcat'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': [u'hadoop']}
2017-04-21 02:55:53,456 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
2017-04-21 02:55:53,457 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa'] {'not_if': '(test $(id -u ambari-qa) -gt 1000) || (false)'}
2017-04-21 02:55:53,462 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa'] due to not_if
2017-04-21 02:55:53,462 - Directory['/tmp/hbase-hbase'] {'owner': 'hbase', 'recursive': True, 'mode': 0775, 'cd_access': 'a'}
2017-04-21 02:55:53,463 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
2017-04-21 02:55:53,463 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase'] {'not_if': '(test $(id -u hbase) -gt 1000) || (false)'}
2017-04-21 02:55:53,467 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase'] due to not_if
2017-04-21 02:55:53,467 - Group['hdfs'] {}
2017-04-21 02:55:53,468 - User['hdfs'] {'fetch_nonlocal_groups': True, 'groups': [u'hadoop', u'hdfs']}
2017-04-21 02:55:53,468 - Directory['/etc/hadoop'] {'mode': 0755}
2017-04-21 02:55:53,481 - File['/usr/hdp/current/hadoop-client/conf/hadoop-env.sh'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop'}
2017-04-21 02:55:53,482 - Directory['/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'] {'owner': 'hdfs', 'group': 'hadoop', 'mode': 0777}
2017-04-21 02:55:53,493 - Repository['HDP-2.4'] {'base_url': 'http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.4.3.0', 'action': ['create'], 'components': [u'HDP', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'HDP', 'mirror_list': None}
2017-04-21 02:55:53,500 - File['/etc/yum.repos.d/HDP.repo'] {'content': '[HDP-2.4]\nname=HDP-2.4\nbaseurl=http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.4.3.0\n\npath=/\nenabled=1\ngpgcheck=0'}
2017-04-21 02:55:53,501 - Repository['HDP-UTILS-1.1.0.20'] {'base_url': 'http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.20/repos/centos7', 'action': ['create'], 'components': [u'HDP-UTILS', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'HDP-UTILS', 'mirror_list': None}
2017-04-21 02:55:53,503 - File['/etc/yum.repos.d/HDP-UTILS.repo'] {'content': '[HDP-UTILS-1.1.0.20]\nname=HDP-UTILS-1.1.0.20\nbaseurl=http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.20/repos/centos7\n\npath=/\nenabled=1\ngpgcheck=0'}
2017-04-21 02:55:53,504 - Package['unzip'] {}
2017-04-21 02:55:53,579 - Skipping installation of existing package unzip
2017-04-21 02:55:53,579 - Package['curl'] {}
2017-04-21 02:55:53,589 - Skipping installation of existing package curl
2017-04-21 02:55:53,589 - Package['hdp-select'] {}
2017-04-21 02:55:53,598 - Skipping installation of existing package hdp-select
2017-04-21 02:55:53,768 - Execute['find /var/lib/ambari-agent/cache/stacks/HDP/2.4/services/ZEPPELIN/package -iname "*.sh" | xargs chmod +x'] {}
2017-04-21 02:55:53,775 - Execute['echo platform.linux_distribution:CentOS Linux+7.2.1511+Core'] {}
2017-04-21 02:55:53,777 - Execute['echo Installing python packages for Centos'] {}
2017-04-21 02:55:53,780 - Execute['sudo yum install -y epel-release'] {}
I keep getting the next message when trying to restart Zeppelin service:

```
Traceback (most recent call last):
  File "/var/lib/ambari-agent/cache/common-services/ZEPPELIN/0.6.0.2.5/package/scripts/master.py", line 312, in <module>
    Master().execute()
  File "/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py", line 280, in execute
    method(env)
  File "/var/lib/ambari-agent/cache/common-services/ZEPPELIN/0.6.0.2.5/package/scripts/master.py", line 185, in start
    + params.zeppelin_log_file, user=params.zeppelin_user)
  File "/usr/lib/python2.6/site-packages/resource_management/core/base.py", line 155, in __init__
    self.env.run()
  File "/usr/lib/python2.6/site-packages/resource_management/core/environment.py", line 160, in run
    self.run_action(resource, action)
  File "/usr/lib/python2.6/site-packages/resource_management/core/environment.py", line 124, in run_action
    provider_action()
  File "/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py", line 273, in action_run
    tries=self.resource.tries, try_sleep=self.resource.try_sleep)
  File "/usr/lib/python2.6/site-packages/resource_management/core/shell.py", line 71, in inner
    result = function(command, **kwargs)
  File "/usr/lib/python2.6/site-packages/resource_management/core/shell.py", line 93, in checked_call
    tries=tries, try_sleep=try_sleep)
  File "/usr/lib/python2.6/site-packages/resource_management/core/shell.py", line 141, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File "/usr/lib/python2.6/site-packages/resource_management/core/shell.py", line 294, in _call
    raise Fail(err_msg)
resource_management.core.exceptions.Fail: Execution of '/usr/hdp/current/zeppelin-server/bin/zeppelin-daemon.sh restart >> /var/log/zeppelin/zeppelin-setup.log' returned 1. mkdir: cannot create directory ‘/var/run/zeppelin’: Permission denied
/usr/hdp/current/zeppelin-server/bin/zeppelin-daemon.sh: line 187: /var/run/zeppelin/zeppelin-zeppelin-XXXXXXXXXX.pid: No such file or directory
cat: /var/run/zeppelin/zeppelin-zeppelin-XXXXXXXXXX.pid: No such file or directory
```

The important part is here: **mkdir: cannot create directory ‘/var/run/zeppelin’: Permission denied**

To solve it:

```
FOLDER=/var/run/zeppelin

sudo mkdir -p $FOLDER
sudo chown -Rf zeppelin:hadoop $FOLDER
```

Will you fix it for the next release? Thank you.

When running a **Hive** paragraph in **Zeppelin** two things happen:
- Processes become **zombie** at system level after displaying the output.
- Paragraphs cannot be **removed**.

Any solution for this? Thank you.

I'm trying to install zeppelin according this link `https://community.hortonworks.com/articles/34424/apache-zeppelin-on-hdp-242.html`, but the deploy process fails and show the following error:

```
Traceback (most recent call last):
  File "/var/lib/ambari-agent/cache/stacks/HDP/2.4/services/ZEPPELIN/package/scripts/master.py", line 236, in <module>
    Master().execute()
  File "/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py", line 219, in execute
    method(env)
  File "/var/lib/ambari-agent/cache/stacks/HDP/2.4/services/ZEPPELIN/package/scripts/master.py", line 65, in install
    recursive_ownership=True                  
  File "/usr/lib/python2.6/site-packages/resource_management/core/base.py", line 113, in __new__
    cls(name.pop(0), env, provider, **kwargs)
  File "/usr/lib/python2.6/site-packages/resource_management/core/base.py", line 146, in __init__
    raise Fail("%s received unsupported argument %s" % (self, key))
resource_management.core.exceptions.Fail: Directory['/var/run/zeppelin-notebook'] received unsupported argument create_parents
```

I've been searching for a solution without success, I'm using Ambari Version 2.2.1.1, HDP 2.4.3.0-227, Python 2.6 and CENTOS 7. I hope someone can help me, thanks.

Install script runs [some commands](https://github.com/hortonworks-gallery/ambari-zeppelin-service/blob/d6f2bd777aa5849d08173dd22b6336585e641299/package/scripts/master.py#L115) as hardcoded `hdfs` user:

``` python
Execute('hadoop fs -mkdir -p /user/' + user, user='hdfs', ignore_failures=True) 
Execute('hadoop fs -chown ' + user + ' /user/' + user, user='hdfs') 
Execute('hadoop fs -chgrp ' + user + ' /user/' + user, user='hdfs') 

Execute('hadoop fs -mkdir -p ' + spark_jar_dir, user='hdfs', ignore_failures=True) 
Execute('hadoop fs -chown ' + user + ' ' + spark_jar_dir, user='hdfs') 
Execute('hadoop fs -chgrp ' + user + ' ' + spark_jar_dir, user='hdfs') 
```

This won't work if the cluster uses a diferent user (in our organization we prefix the username with the cluster name: `<clustername>-hdfs`:

```
Skipping failure of Execute['hadoop fs -mkdir -p /user/zeppelin'] due to ignore_failures. Failure reason: Execution of 'hadoop fs -mkdir -p /user/zeppelin' returned 1. 16/09/05 15:15:47 WARN ipc.Client: Exception encountered while connecting to the server : 
javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
```

File "/var/lib/ambari-agent/cache/stacks/HDP/2.4/services/ZEPPELIN/package/scripts/master.py", line 230, in post_request  

jsonresp = json.loads(response.decode('utf-8'))
UnboundLocalError: local variable 'response' referenced before assignment

Above assignment should only be done if exception does not occur.

2016-06-07 19:54:27,248 - Group['spark'] {}
2016-06-07 19:54:27,249 - Group['zeppelin'] {}
2016-06-07 19:54:27,249 - Adding group Group['zeppelin']
2016-06-07 19:54:27,285 - Group['hadoop'] {}
2016-06-07 19:54:27,285 - Group['users'] {}
2016-06-07 19:54:27,286 - User['storm'] {'gid': 'hadoop', 'groups': [u'hadoop']}
2016-06-07 19:54:27,287 - User['zookeeper'] {'gid': 'hadoop', 'groups': [u'hadoop']}
2016-06-07 19:54:27,288 - User['spark'] {'gid': 'hadoop', 'groups': [u'hadoop']}
2016-06-07 19:54:27,288 - User['oozie'] {'gid': 'hadoop', 'groups': [u'users']}
2016-06-07 19:54:27,289 - User['ambari-qa'] {'gid': 'hadoop', 'groups': [u'users']}
2016-06-07 19:54:27,290 - User['hdfs'] {'gid': 'hadoop', 'groups': [u'hadoop']}
2016-06-07 19:54:27,291 - User['zeppelin'] {'gid': 'hadoop', 'groups': [u'hadoop']}
2016-06-07 19:54:27,292 - Adding user User['zeppelin']
2016-06-07 19:54:27,512 - User['yarn'] {'gid': 'hadoop', 'groups': [u'hadoop']}
2016-06-07 19:54:27,513 - User['mapred'] {'gid': 'hadoop', 'groups': [u'hadoop']}
2016-06-07 19:54:27,513 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
2016-06-07 19:54:27,515 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa'] {'not_if': '(test $(id -u ambari-qa) -gt 1000) || (false)'}
2016-06-07 19:54:27,522 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa'] due to not_if
2016-06-07 19:54:27,522 - Group['hdfs'] {'ignore_failures': False}
2016-06-07 19:54:27,523 - User['hdfs'] {'ignore_failures': False, 'groups': [u'hadoop', u'hdfs']}
2016-06-07 19:54:27,523 - Directory['/etc/hadoop'] {'mode': 0755}
2016-06-07 19:54:27,536 - File['/usr/hdp/current/hadoop-client/conf/hadoop-env.sh'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop'}
2016-06-07 19:54:27,537 - Directory['/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'] {'owner': 'hdfs', 'group': 'hadoop', 'mode': 0777}
2016-06-07 19:54:27,546 - Repository['HDP-2.3'] {'base_url': 'http://public-repo-1.hortonworks.com/HDP/ubuntu14/2.x/updates/2.3.4.7', 'action': ['create'], 'components': [u'HDP', 'main'], 'repo_template': '{{package_type}} {{base_url}} {{components}}', 'repo_file_name': 'HDP', 'mirror_list': None}
2016-06-07 19:54:27,552 - File['/tmp/tmpeQHze_'] {'content': 'deb http://public-repo-1.hortonworks.com/HDP/ubuntu14/2.x/updates/2.3.4.7 HDP main'}
2016-06-07 19:54:27,553 - Writing File['/tmp/tmpeQHze_'] because contents don't match
2016-06-07 19:54:27,561 - File['/tmp/tmphOycwy'] {'content': StaticFile('/etc/apt/sources.list.d/HDP.list')}
2016-06-07 19:54:27,562 - Writing File['/tmp/tmphOycwy'] because contents don't match
2016-06-07 19:54:27,562 - File['/etc/apt/sources.list.d/HDP.list'] {'content': StaticFile('/tmp/tmpeQHze_')}
2016-06-07 19:54:27,563 - Writing File['/etc/apt/sources.list.d/HDP.list'] because contents don't match
2016-06-07 19:54:27,564 - checked_call['apt-get update -qq -o Dir::Etc::sourcelist=sources.list.d/HDP.list -o Dir::Etc::sourceparts=- -o APT::Get::List-Cleanup=0'] {'sudo': True, 'quiet': False}
2016-06-07 19:54:36,343 - checked_call returned (0, '')
2016-06-07 19:54:36,344 - Repository['HDP-UTILS-1.1.0.20'] {'base_url': 'http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.20/repos/ubuntu14', 'action': ['create'], 'components': [u'HDP-UTILS', 'main'], 'repo_template': '{{package_type}} {{base_url}} {{components}}', 'repo_file_name': 'HDP-UTILS', 'mirror_list': None}
2016-06-07 19:54:36,346 - File['/tmp/tmpYbbAZu'] {'content': 'deb http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.20/repos/ubuntu14 HDP-UTILS main'}
2016-06-07 19:54:36,347 - Writing File['/tmp/tmpYbbAZu'] because contents don't match
2016-06-07 19:54:36,347 - File['/tmp/tmpSTZKqf'] {'content': StaticFile('/etc/apt/sources.list.d/HDP-UTILS.list')}
2016-06-07 19:54:36,347 - Writing File['/tmp/tmpSTZKqf'] because contents don't match
2016-06-07 19:54:36,348 - File['/etc/apt/sources.list.d/HDP-UTILS.list'] {'content': StaticFile('/tmp/tmpYbbAZu')}
2016-06-07 19:54:36,348 - Writing File['/etc/apt/sources.list.d/HDP-UTILS.list'] because contents don't match
2016-06-07 19:54:36,349 - checked_call['apt-get update -qq -o Dir::Etc::sourcelist=sources.list.d/HDP-UTILS.list -o Dir::Etc::sourceparts=- -o APT::Get::List-Cleanup=0'] {'sudo': True, 'quiet': False}
2016-06-07 19:54:49,422 - checked_call returned (0, '')
2016-06-07 19:54:49,423 - Package['unzip'] {}
2016-06-07 19:54:49,439 - Skipping installation of existing package unzip
2016-06-07 19:54:49,440 - Package['curl'] {}
2016-06-07 19:54:49,455 - Skipping installation of existing package curl
2016-06-07 19:54:49,455 - Package['hdp-select'] {}
2016-06-07 19:54:49,470 - Skipping installation of existing package hdp-select
2016-06-07 19:54:49,652 - Execute['find /var/lib/ambari-agent/cache/stacks/HDP/2.3/services/ZEPPELIN/package -iname "*.sh" | xargs chmod +x'] {}
2016-06-07 19:54:49,660 - Execute['echo platform.linux_distribution:Ubuntu+14.04+trusty'] {}
2016-06-07 19:54:49,664 - Package['liblapack-dev'] {}
2016-06-07 19:54:49,681 - Installing package liblapack-dev ('/usr/bin/apt-get -q -o Dpkg::Options::=--force-confdef --allow-unauthenticated --assume-yes install liblapack-dev')
2016-06-07 19:55:06,178 - Package['gfortran'] {}
2016-06-07 19:55:06,194 - Installing package gfortran ('/usr/bin/apt-get -q -o Dpkg::Options::=--force-confdef --allow-unauthenticated --assume-yes install gfortran')
2016-06-07 19:56:11,435 - Package['python-dev'] {}
2016-06-07 19:56:11,454 - Installing package python-dev ('/usr/bin/apt-get -q -o Dpkg::Options::=--force-confdef --allow-unauthenticated --assume-yes install python-dev')
2016-06-07 19:57:04,434 - Package['g++'] {}
2016-06-07 19:57:04,452 - Installing package g++ ('/usr/bin/apt-get -q -o Dpkg::Options::=--force-confdef --allow-unauthenticated --assume-yes install g++')
2016-06-07 19:57:58,456 - Package['python-pip'] {}
2016-06-07 19:57:58,474 - Installing package python-pip ('/usr/bin/apt-get -q -o Dpkg::Options::=--force-confdef --allow-unauthenticated --assume-yes install python-pip')
## 2016-06-07 19:58:19,340 - Package['zeppelin'] {}

2016-06-07 19:58:19,358 - Installing package zeppelin ('/usr/bin/apt-get -q -o Dpkg::Options::=--force-confdef --allow-unauthenticated --assume-yes install zeppelin')
2016-06-07 19:58:19,729 - Execution of '['/usr/bin/apt-get', '-q', '-o', 'Dpkg::Options::=--force-confdef', '--allow-unauthenticated', '--assume-yes', 'install', u'zeppelin']' returned 100. Reading package lists...
Building dependency tree...
Reading state information...
E: Unable to locate package zeppelin
2016-06-07 19:58:19,730 - Failed to install package zeppelin. Executing `/usr/bin/apt-get update -qq`
2016-06-07 19:58:51,124 - Retrying to install package zeppelin

Hello, 
is there any ways to deploy the latest version of zeppelin with this plugin? I scan the code and it doesn't seem to be obvious. 

Stumbled upon an issue with leading space while processing config -> public host name. It should be trimmed, to avoid urllib2 open error
