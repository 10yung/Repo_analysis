When I do `df..write.format("tfrecords").option('writeLocality','local').save(path)` or `df..write.format("tfrecords").save(path)` the file is not getting created on the main Driver node. I dont see the folder as well. I am unable to _overwrite_ as that mode is not working. I am unable to delete those files as they are in worker node. Is there a simple API that can delete the files created through this spark job
Are there examples compatible with TF 2.0, similar to https://github.com/tensorflow/ecosystem/tree/master/distribution_strategy?
Aligning with `org.tensorflow.hadoop.io.TFRecordFileOutputFormat`, enable compression in `org.tensorflow.hadoop.io.TFRecordFileOutputFormatV1` as well.
To activate compression in old MapReduce APIs, simply specify options as below:
```bash
-Dmapred.output.compress=true
-Dmapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec
```
This pull request can be considered as supplementary to #61.
```
     case VectorType => {
        val field = row.get(index)
        field match {
          case v: SparseVector => FloatListFeatureEncoder.encode(v.toDense.toArray.map(_.toFloat))
          case v: DenseVector => FloatListFeatureEncoder.encode(v.toArray.map(_.toFloat))
          case _ => throw new RuntimeException(s"Cannot convert $field to vector")
        }
      }

I found this code in your DefaultTfRecordRowEncoder.scala, explicitly converse a SparseVector to a DenseVector.

I have a 1000-dimentional feature vector in my DataFrame which has about 90 non-zero values. So this conversion make the size of tfrecord dataset very much larger than snappy.parquet in Spark.

I'm a little confused about the conversion.
When I ran 
`mvn clean install -Dspark.version=2.3.2`

The build fails because of the following error appears:
`error: scala.reflect.internal.MissingRequirementError: object java.lang.Object in compiler mirror not found.
	at scala.reflect.internal.MissingRequirementError$.signal(MissingRequirementError.scala:17)
	at scala.reflect.internal.MissingRequirementError$.notFound(MissingRequirementError.scala:18)
	at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:53)
	at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:45)
	at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:45)
	at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:66)
	at scala.reflect.internal.Mirrors$RootsBase.getClassByName(Mirrors.scala:102)
	at scala.reflect.internal.Mirrors$RootsBase.getRequiredClass(Mirrors.scala:105)
	at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute(Definitions.scala:257)
	at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass(Definitions.scala:257)
	at scala.reflect.internal.Definitions$DefinitionsClass.init(Definitions.scala:1374)
	at scala.tools.nsc.Global$Run.<init>(Global.scala:1278)
	at scala.tools.nsc.Driver.doCompile(Driver.scala:33)
	at scala.tools.nsc.MainClass.doCompile(Main.scala:23)
	at scala.tools.nsc.Driver.process(Driver.scala:55)
	at scala.tools.nsc.Main.process(Main.scala)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:567)
	at sbt.compiler.RawCompiler.apply(RawCompiler.scala:26)
	at sbt.compiler.AnalyzingCompiler$$anonfun$compileSources$1$$anonfun$apply$2.apply(AnalyzingCompiler.scala:147)
	at sbt.compiler.AnalyzingCompiler$$anonfun$compileSources$1$$anonfun$apply$2.apply(AnalyzingCompiler.scala:142)
	at sbt.IO$.withTemporaryDirectory(IO.scala:285)
	at sbt.compiler.AnalyzingCompiler$$anonfun$compileSources$1.apply(AnalyzingCompiler.scala:142)
	at sbt.compiler.AnalyzingCompiler$$anonfun$compileSources$1.apply(AnalyzingCompiler.scala:139)
	at sbt.IO$.withTemporaryDirectory(IO.scala:285)
	at sbt.compiler.AnalyzingCompiler$.compileSources(AnalyzingCompiler.scala:139)
	at sbt.compiler.IC$.compileInterfaceJar(IncrementalCompiler.scala:33)
	at com.typesafe.zinc.Compiler$.compilerInterface(Compiler.scala:148)
	at com.typesafe.zinc.Compiler$.create(Compiler.scala:53)
	at com.typesafe.zinc.Compiler.create(Compiler.scala)
	at sbt_inc.SbtIncrementalCompiler.<init>(SbtIncrementalCompiler.java:65)
	at scala_maven.ScalaCompilerSupport.incrementalCompile(ScalaCompilerSupport.java:294)
	at scala_maven.ScalaCompilerSupport.compile(ScalaCompilerSupport.java:119)
	at scala_maven.ScalaCompilerSupport.doExecute(ScalaCompilerSupport.java:99)
	at scala_maven.ScalaMojoSupport.execute(ScalaMojoSupport.java:482)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:137)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:210)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:156)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:148)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:117)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:81)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:56)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:305)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:192)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:105)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:956)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:192)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:567)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:282)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:225)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:406)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:347)`
Change spark-tensorflow-connector to be spark 3.0.0 preview
Hi, I'm testing Distribution Strategies Example on my own K8S cluster, there is an unexpected error before the end of the task.

![image](https://user-images.githubusercontent.com/20874664/63822576-5c6c2200-c983-11e9-96a0-5edc4f660e1a.png)

However, when I run keras_model_to_estimator.py locally, it ends normally.

![image](https://user-images.githubusercontent.com/20874664/63822793-224f5000-c984-11e9-802e-654d0184156f.png)

```docker
 HADOOP_VERSION=2.7
APACHE_SPARK_VERSION=2.4.3
PYTHON_VERSION=3
```
codes:
```
spark = None
    try:            
        print('creating spark cluster')
        spark = SparkSession.builder\
            .appName(spark_app_name) \
            .config('spark.kubernetes.namespace', spark_k8s_namespace) \
            .config('spark.jars.packages', 'org.tensorflow:spark-tensorflow-connector_2.11:1.14.0') \
            .config("mapreduce.fileoutputcommitter.algorithm.version", "2") \
            .config('spark.executor.instances', str(spark_n_executors)) \
            .config('spark.sql.windowExec.buffer.spill.threshold', '2097152') \
            .config('spark.sql.windowExec.buffer.in.memory.threshold', '2097152') \
            .enableHiveSupport() \
            .getOrCreate()
                
        # --- logic ---

        df = spark.read.parquet(datalake_location)
       ...
        df.repartition(num_partition)\
            .write\
            .format("tfrecords")\
            .mode("overwrite")\
            .save(dataset_location)
        
    finally:
        # terminate spark context
        if spark:
            print('stopping spark cluster')
            spark.stop()
```
After all jobs has been finished, it will write to `_temporary/` under target directory  first and then write to the target directory sequentially which cost a lot of time as following:

![image](https://user-images.githubusercontent.com/9409333/62619228-d3943480-b948-11e9-8c32-a20a02fd924a.png)

how to read the tfrecord use python

its always wrong,when i read FixedSequnceFeatures
