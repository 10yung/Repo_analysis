Hi,
Thank you all the developers for this amazing project.

I would like to use this project to install VM on a hardware during its early development stage to test applications. And I wish to be able to boot the VM via the network.

Questions:
- Does the project support UEFI SNP(Simple Network Protocol) for booting via the network?
  Browsing through the code, it seems that it does not support the protocol, but I would just like to make sure.
  - If there is support or ongoing implementation, is there any way that I can contribute to it?
  - If there is no support for it but the project would like add support, can I fork and do a pull request?
  - Or might it be that the project does not intend to add support for UEFI SNP?

Depends on #24 and addresses #6 

This PR adds support for booting via the [Xen HVM direct boot ABI](https://xenbits.xen.org/docs/4.12-testing/misc/pvh.html).

This uses a 32-bit unpaged entry point, so we just point it at `ram32_start`. This allows our firmware to be used with QEMU's `-kernel` option. This can be invoked like:

```
qemu-system-x86_64
-machine type=q35,accel=kvm \
-cpu host \
-smp 1 \
-m 1024 \
-display none \
-serial stdio \
-kernel target/target/release/hypervisor-fw \
-drive file=clear-28660-kvm.img,format=raw,if=none,id=boot-drive \
-device virtio-blk-pci,drive=boot-drive,disable-legacy=on
```

For some reason, this gets further in the boot than #24, but still doesn't boot all the way to Linux. It can configure the block device and find the kernel, but gets stuck at "Jumping to kernel". I turned off `quiet` mode in the kernel and confirmed that the first line of the kernel console isn't even being printed.

@rbradford, is this the same issue that prevents booting on firecracker? Resetting Virtio Devices?

Signed-off-by: Joe Richey <joerichey@google.com>
Depends on #23 
Fixes #5 

Now you can do the following:
```
cargo xbuild --target target.json --release
sstrip target/target/release/hypervisor-fw
qemu-system-x86_64 \
    -machine type=q35,accel=kvm \
    -cpu host -smp 1 -m 1024 \
    -display none -serial stdio \
    -bios target/target/release/hypervisor-fw \
    -drive id=boot,file=clear-28660-kvm.img,format=raw,if=none \
    -device virtio-blk-pci,drive=boot,disable-legacy=on
```

And the firmware will go from the real-mode reset vector all the way to Rust. Driver bug is preventing boot due to VIRTIO feature negotiation. 
Right now all the tests run on the host. In an ideal world, we would set `.cargo/config`'s `build.target` to be our custom target and have everything (`xbuild`, `xclippy`, `xtest`, etc...) build for the main target. This is [how `blog_os` does it](https://github.com/phil-opp/blog_os/blob/post-10/.cargo/config).

We would have to setup a [custom testing framework](https://os.phil-opp.com/testing/#custom-test-frameworks) however, which could be complex.  We would also have to figure out an easy way to run the tests without too much hassle (maybe once #6 is done).
Per the UEFI spec, calling open on a directory path should work, but right now it will fail, as the code [here](https://github.com/intel/rust-hypervisor-firmware/blob/f52c1a13d52fdf5a12a111ffff40876209de6b2e/src/fat.rs#L574) is only setup to return a file.

The solution to this is probably reworking the filesystem code to better handle working with directories.
Nightly doesn't have clippy at the moment so the CI has currently failed. Although it's possible to fix on a particular version of nightly (via rust-toolchain) that sometimes goes wrong as it pulls down the latest rust-src which might not compile with that compiler.

@joshtriplett do you have any thoughts on how we could move away from nightly or mitigate the risks? A docker container with a known good toolchain already installed?
Right now it's not possible to run the firmware with any non-Firecracker VMM. Ideally we would support a wide range of VMMs. There are many small changes that would fix this:

- [ ] In the ELF binary, advertise support for a common boot specification. This would allow any VMM supporting that spec to run the firmware. Our options are:
  - [Mutliboot/Multiboot2](https://en.wikipedia.org/wiki/Multiboot_specification): this is an older FSF common boot standard. It would also let us work with QEMU's `-kernel` option (might also work with Xen). 
  - [PVH direct boot](https://xenbits.xen.org/docs/unstable/misc/pvh.html): This standard [started with XEN](https://wiki.xen.org/wiki/Xen_Project_Software_Overview#PVH), but support was [added to QEMU](https://patchwork.kernel.org/cover/10715001/) recently. To use this, we just need to advertise `XEN_ELFNOTE_PHYS32_ENTRY` as an [ELFNOTE](https://elixir.bootlin.com/linux/latest/source/include/linux/elfnote.h). Not supporting PVH is why running QEMU with `-kernel` does not work:
    ```
    > qemu-system-x86_64 -kernel target/target/release/hypervisor-fw   
    qemu-system-x86_64: Error loading uncompressed kernel without PVH ELF Note
    ```
  - Note that both of these specs would (probably) require a separate 32-bit entry point. The firmware would then need to setup paging before jumping to the normal 64-bit ELF entry point (i.e. `_start`).

- [ ] Issue #5: support building the firmware as a flat BIOS binary (like `OVMF.fd`), allowing it to be directly loaded/executed by any VMM that supports SeaBIOS/OVMF.

Implementing either of these would let the firmware work on QEMU. I'm not sure about how to get it working for crosvm (it should "just work" with any ELF binary). I can ask around at work Monday to see what the deal is.




Right now the firmware is built into a normal ELF binary, and then booted using Firecracker. However, it would be nice if were possible to build the firmware into a flat BIOS binary that can be directly executed by the processor on reset.

Specifically, the firmware would be loaded at Guest Physical Address `4 GiB - sizeof(binary)` and then execution would begin at the standard x86 [reset vector](https://en.wikipedia.org/wiki/Reset_vector) `0xFFFFFFF0` in Real Mode. This is what SeaBIOS's `bios.bin` and EDK2's `OVMF_CODE.fd`/`OVMF_VARS.fd` builds do.

This would then allow for the binary to be used with any VMM that uses the normal BIOS loading process. This means automatic support for QEMU and XEN. Specifically, using QEMU would then be possible with:
```
qemu-system-x86_64  -drive if=pflash,format=raw,readonly,file=hypervisor-fw
```
## Design Ideas

I would be happy to start exploring this. My idea of the execution flow would be:

1) One instruction at the reset vector: jump to code in (2)
2) Assembly code that:
  b) Deals with A20
  c) Sets up stub IDT/GDT/Paging
  d) Switches to long mode
  e) Jump to `_start` (i.e. the normal ELF entry point)
3) Our normal Rust entry point: `_start`.

This could be done by having two similar `target.json` files, that only differed in refering to different `layout.ld` files. The `layout.fd` file for our flat BIOS build, would just have to make sure that the code for (1) was properly aligned and at the end of the file. This also means that our flat BIOS build would still be an ELF file, which has its advantages. 
