I'm kind of a beginner myself and I had wasted a few minutes when I wasn't able to figure out how I can get the Example Projects to work and from where I can download the .framework files. Therefore, I updated the Frameworks README file to be more descriptive and beginner-friendly.  

Ready to send us a pull request? Please make sure your request is based on the [develop](https://github.com/audiokit/AudioKit/tree/develop) branch of the repository as `master` only holds stable releases.

This lays the groundwork for generating true universal XCFrameworks that contains every single build of AudioKit (iOS, macOS, Apple TV and Catalyst).

This is not complete because to make this actually functional we'll need to rename the `AudioKit` class to something that doesn't clash with the name of the enclosing module. This has large implications needing to patch most examples, playgrounds, tests and docs. It also breaks the API (although in relatively minor way) so it might be better left off for the next major release (AK 5 maybe?)

This is still helpful for the current release because:
- We now generate frameworks with the library evolution feature turned on, which in theory should now solve the issue with binaries becoming incompatible with future versions of Swift
- Fixed a few small things that were getting in the way of successfully building the XCFramework (more importantly several bugs in the compiler and other quirks)

I think the rest of the work to complete XCFramework support should go in a beta branch of some sort, particularly since a lot of the usefulness of this hinges on other things being still pending release: Travis support for Catalina, CocoaPods 1.9 (still in beta) with native XCFramework support. Generally the tooling support even from Xcode still feels pretty "beta".

I have the rest of this ready to commit (and it works even with the new Catalyst examples) but don't think it should to the current develop branch.
We've noticed in our app when we move the playhead position it's starting earlier than the specified time. We do this via:

    player?.stop()
    player?.startTime = position
    player?.play()

I've created a sample project to demonstrate this behavior. [SamplePlayer.zip](https://github.com/AudioKit/AudioKit/files/4017829/SamplePlayer.zip)
To replicate:
1. Run pod install
2. Open the workspace
3. Run the app
Notice a player is started and its currentTime is logged every millisecond
4. Tap "Go Back"
5. Stop the app
6. Search the console for "moving"
Notice the logs show MOVING PLAYHEAD TO 1.2345 and then the player's currentTime logged after that point are before the specified startTime such as
```
13.973310657596372

MOVING PLAYHEAD TO 1.2345

1.2131848072562357
1.2131848072562357
1.2131848072562357
1.2131848072562357
1.2131848072562357
1.2131848072562357
1.2131848072562357
1.2131848072562357
1.2131848072562357
1.2131848072562357
1.2131848072562357
1.223842403628118
1.223842403628118
1.223842403628118
1.223842403628118
1.223842403628118
1.223842403628118
1.223842403628118
1.223842403628118
1.223842403628118
1.223842403628118
1.223842403628118
1.2345226757369614
1.2345226757369614
1.2345226757369614
1.2345226757369614
1.2345226757369614
1.2345226757369614
1.2345226757369614
1.2345226757369614
1.2345226757369614
1.2345226757369614
```

This is problematic in our app because we run some logic based on the player's currentTime. To support looping sections of a song in real-time, we detect when the player's position has reached the end of a loop and move it back to the start time. This is breaking our logic because it starts earlier than the loop's start time, causing us to lose track of the current loop because we've exited the loop and we'll start a new loop a few milliseconds later.
I had a problem that was difficult to diagnose with Microphone tracking, which I resolved with Aurelius' help for [this StackOverflow question](https://stackoverflow.com/questions/59551361/on-macos-catalina-why-do-akfrequencytracker-akmicrophonetracker-give-bogus)  

As of Catalina 10.14, the OS requires user permission to access the microphone, and that requires a specific key in the info.plist file. If the key isn't present the user is not prompted to allow access to the mic, but it is problem (I believe) that AudioKit doesn't throw an exception when the mic can't be accessed, particularly if it is a security issue. By throwing an exception, developers would be clued in immediately to what is going wrong. Otherwise it can be extremely elusive.

In my case, I was aware of security/permission issues with iOS, but since it had been awhile since I'd done app development on macOS, I wasn't aware of the new restriction/requirements.
While connecting an external I/O device , It is batter has a setting for user can select which channels are prefer .
ig. User apogee quartet , quartet has 4 analog input , and 6 analog output , you can select specific I/O channels for recording and monitor . you can select input 1 & 2 for stereo recording or just 1 for 
mono recording.

For current version I test with quartet , if will using default input 1 , still works , but if your audio is via other channels , nothing can be recorded

A setting view with real time chart UI will be great for measuring the input/output volume

![img](http://www.apogeedigital.com/wp-content/uploads/2014/02/maestro-ios-quartet-output-800.jpg)