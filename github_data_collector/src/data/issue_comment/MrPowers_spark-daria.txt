ParquetCompactor is not deleting old files and the input_file_name_parts directory on S3. 

We are using the spark databricks platform, spark 6.2, pyspark and mrpowers:spark-daria:0.36.0-s_2.11. After running ParquetCompactor we have a new big parquet file, but the old files and the input_file_name_parts directory still exists. 

Is it not possible to use the ParquetCompactor on S3?
When I add one more level of nested structure it fails to flatten.
```
"uses the StackOverflow answer format" - {

        val data = Seq(
          Row(
            Row(
              "this",
              "is"
            ),
            "something",
            "cool",
            ";)"
          )
        )

        val schema = StructType(
          Seq(
            StructField(
              "foo",
              StructType(
                Seq(
                  StructField(
                    "bar",
                    StructType(
                      Seq(
                        StructField(
                          "zoo",
                          StringType,
                          true
                        )
                      )
                    )
                  ),
                  StructField(
                    "baz",
                    StringType,
                    true
                  )
                )
              ),
              true
            ),
            StructField(
              "x",
              StringType,
              true
            ),
            StructField(
              "y",
              StringType,
              true
            ),
            StructField(
              "z",
              StringType,
              true
            )
          )
        )

        val df = spark
          .createDataFrame(
            spark.sparkContext.parallelize(data),
            StructType(schema)
          )
          .flattenSchema("_")

        val expectedDF = spark.createDF(
          List(("this", "is", "something", "cool", ";)")),
          List(
            ("foo_bar_zoo", StringType, true),
            ("foo_baz", StringType, true),
            ("x", StringType, true),
            ("y", StringType, true),
            ("z", StringType, true)
          )
        )

        assertSmallDataFrameEquality(
          df,
          expectedDF
        )

      }
```
@MrPowers, @nvander1, 
do you think it's a good idea to add thie project to https://github.com/fthomas/scala-steward to keep the dependencies up to date ?
[Apache Spark itself](https://github.com/apache/spark/blob/master/.github/workflows/master.yml) is using these settings:

```
matrix:
  java: [ '1.8', '11' ]
  hadoop: [ 'hadoop-2.7', 'hadoop-3.2' ]
  exclude:
  - java: '11'
    hadoop: 'hadoop-2.7'
```

I think spark-daria can simply be tested with Java 1.8 and without any Hadoop specified, correct?  I don't think we need to be testing multiple different Java / Hadoop versions.

I just learned that Java 8 and Java 1.8 are the same thing... what?!

I'm not even going to ask why Java 9 and Java 10 aren't included in this discussion.  So confusing!!
This reverts commit 1e7833513d40da6dd695f07a7b721810b1e2522d.

This was merged upstream in spark: https://github.com/apache/spark/pull/24232
I'm getting a strange error. I'm not a regular Scala user, so I may be doing something silly.

First, I start a Spark shell as follows:

```sh
spark-shell --packages "org.apache.hadoop:hadoop-aws:2.7.6,mrpowers:spark-daria:0.32.0-s_2.11"
```

Then I run this code:

```scala
scala> val df = spark.read.parquet("s3a://...")
[Stage 0:>                                                          (0 + 1) 
                                                                            
df: org.apache.spark.sql.DataFrame = [... 96 more fields]

scala> import com.github.mrpowers.spark.daria.sql.DataFrameHelpers
import com.github.mrpowers.spark.daria.sql.DataFrameHelpers

scala> DataFrameHelpers.printAthenaCreateTable(
     |     df,
     |     "my.table",
     |     "s3a://..."
     | )
java.io.FileNotFoundException: /Users/powers/Documents/code/my_apps/spark-daria/target/scala-2.11/scoverage-data/scoverage.measurements.1 (No such file or directory)
  at java.io.FileOutputStream.open0(Native Method)
  at java.io.FileOutputStream.open(FileOutputStream.java:270)
  at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
  at java.io.FileWriter.<init>(FileWriter.java:107)
  at scoverage.Invoker$$anonfun$1.apply(Invoker.scala:42)
  at scoverage.Invoker$$anonfun$1.apply(Invoker.scala:42)
  at scala.collection.concurrent.TrieMap.getOrElseUpdate(TrieMap.scala:901)
  at scoverage.Invoker$.invoked(Invoker.scala:42)
  at com.github.mrpowers.spark.daria.sql.DataFrameHelpers$.printAthenaCreate
Table(DataFrameHelpers.scala:194)
  ... 53 elided
```

The reference to `/Users/powers/` seems strange, and suggests some path from the project author's workstation got mistakenly baked into the package somehow.
There [isn't a regexp_extract_all function in Spark yet](https://issues.apache.org/jira/browse/SPARK-16203).  Pretty surprising that this hasn't been added yet.

There is a [native implementation of regexp_extract_all](https://github.com/apache/spark/pull/21985/files#diff-2c0350957ac4932d3f63796eceaeae08) that hasn't been merged with master yet.

I need this so I'll add a quick & dirty UDF approach.  @nvander1 - feel free to add the Spark native implementation if you'd like.

I'll start thinking about Spark native functions once my Spark Summit talk is over!
spark-daria follows the standard Scala / Java deep nesting package convention that's annoying when importing code.

Users currently need to import code like this: `import com.github.mrpowers.spark.daria.sql.ColumnExt._`

I noticed that some libraries are deviating from these Scala conventions and offering imports like this: `import utest._`.

Maybe we can change the package structure so users can import code like `import mrpowers.daria.sql.ColumnExt._`?  Thoughts @nvander1 / @manuzhang...?

