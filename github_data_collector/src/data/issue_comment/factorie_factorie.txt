Hi! I'm trying to replicate findings of Singh et. al's "Monte Carlo MCMC: Efficient Inference by Approximate Sampling", [link](https://ciir-publications.cs.umass.edu/getpdf.php?id=1053).

Essentially it uses Metropolis-Hastings for entity-resolution (ER) on the CORA citation dataset, and shows a significant speed-up if only a proportion of the factors are sampled.

In the paper, the authors use Samplerank from FACTORIE to create the initial ER model. Given that I've never used Scala before (learning statistics in R), is there a way to port the model from FACTORIE to R? 

If not, how might I implement the proposal function (sum of log-scores of the sample factors, where log-score is cross-product of adjacent variables under consideration) in FACTORIE?

Any advice with regards to methodology would be appreciated. Huge thanks in advance!

Thank you for providing the implementation for topics over time! I am reading the code [here](https://github.com/factorie/factorie/blob/8fa1ffca70600fa390c1a10f7991733404a3456a/src/main/scala/cc/factorie/tutorial/TopicsOverTime.scala) (although I barely knows Scala) and have two questions. Please pardon me if I misunderstood the syntaxes! 

1. [Line 61](https://github.com/factorie/factorie/blob/8fa1ffca70600fa390c1a10f7991733404a3456a/src/main/scala/cc/factorie/tutorial/TopicsOverTime.scala#L61): `val topic2mean = Array.tabulate(numTopics)(i => if (topic2times(i).length > 1) maths.sampleMean(topic2times(i)) else 0.5)`

If I am understanding this correctly, you will give the value of 0.5 to the mean of normalized timestamps for one topic if no tokens have been assigned to this topic. Similarly in line 62, you give the variance 0.25 if this is the case. May I ask why? If I plugin the value 0.5 and 0.25 to solve `alpha` and `beta`, it gives me `alpha=beta=0`


2. [Line 105](https://github.com/factorie/factorie/blob/8fa1ffca70600fa390c1a10f7991733404a3456a/src/main/scala/cc/factorie/tutorial/TopicsOverTime.scala#L105): `val timeSmoothing = Tensor.tabulate(numTopics)(i => { val m = timeMeans(i) + 0.5; m*m*m*m*m*m })`. I did not understand the smoothing step here. Can you please elaborate on this and the following lines?

Thank you very much!
Hi,

When I try to define a factor template that have varying number of neighbors, I got this error

```
Exception in thread "main" java.lang.ClassCastException: cc.factorie.variable.SeqVariable cannot be cast to cc.factorie.variable.VectorVar
	at cc.factorie.infer.BPFactor1Factor2.<init>(BP.scala:222)
	at cc.factorie.infer.BPFactorFactory$.newBPFactor(BP.scala:46)
	at cc.factorie.infer.LoopyBPSummary$$anonfun$apply$2.apply(BP.scala:511)
	at cc.factorie.infer.LoopyBPSummary$$anonfun$apply$2.apply(BP.scala:511)
	at scala.collection.mutable.LinkedHashSet.foreach(LinkedHashSet.scala:93)
	at cc.factorie.infer.LoopyBPSummary$.apply(BP.scala:511)
	at cc.factorie.infer.InferByBPLoopy$.infer(BP.scala:814)
	at cc.factorie.infer.InferByBPLoopy$.infer(BP.scala:810)
	at cc.factorie.optimize.LikelihoodExample.accumulateValueAndGradient(Example.scala:113)
	at cc.factorie.optimize.BatchTrainer$$anonfun$processExamples$1.apply(Trainer.scala:55)
	at cc.factorie.optimize.BatchTrainer$$anonfun$processExamples$1.apply(Trainer.scala:55)
	at scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:74)
	at cc.factorie.optimize.BatchTrainer.processExamples(Trainer.scala:55)
	at cc.factorie.optimize.Trainer$.train(Trainer.scala:406)
	at cc.factorie.optimize.Trainer$.batchTrain(Trainer.scala:441)
```

This is my template
```
new DotTemplate2[TripleLabel, SeqVariable[TripleLabel]]{
  factorName = "SubStructure"
  val nFeature = 1
  val weights: Weights1 = Weights(new DenseTensor1(nFeature))

  def unroll1(label: TripleLabel): Iterable[Factor] = {
    if (label.triple.childrenLabel.isEmpty) Nil
    else Factor(label, new SeqVariable(label.triple.childrenLabel))
  }
  def unroll2(children: SeqVariable[TripleLabel]) = throw new Exception("Should not call this")

  override def statistics(v1: BooleanValue, v2: IndexedSeq[TripleLabel]): Tensor = {
    // return feature here
    val allChildrenIsWrong = if (v2.forall(_.booleanValue == false)) 1.0 else 0.0
    new DenseTensor1(Array(allChildrenIsWrong))
  }
}
```

I have tried VarArg instead of SeqVariable but still have this error. I take a look the code at `cc.factorie.infer.BP.scala` line 44-49

```
      case factor:Factor2[DiscreteVar @unchecked,DiscreteVar @unchecked] =>
        if (varying.contains(factor._1) && varying.contains(factor._2)) new BPFactor2Factor2(factor, new BPEdge(summary.bpVariable(factor._1)), new BPEdge(summary.bpVariable(factor._2)), summary, summary.ring)
        else if (varying.contains(factor._1)) new BPFactor1Factor2(factor.asInstanceOf[Factor2[DiscreteVar,VectorVar]], new BPEdge(summary.bpVariable(factor._1)), summary)
        else if (factor._1.isInstanceOf[DiscreteVar]) new BPFactor1Factor2(factor.asInstanceOf[Factor2[DiscreteVar,VectorVar]], new BPEdge(summary.bpVariable(factor._2)), summary)
        else new BPFactor1Factor2Left(factor.asInstanceOf[Factor2[VectorVar,DiscreteVar]], new BPEdge(summary.bpVariable(factor._2)), summary)
```

It seems that even if I make a sub-class contains all varying neighbors and inherit VectorVar, the code here will treat the second variable of a factor as constant, and won't update the value properly.

Can you please show me the correct way to work though this issue? Any help would be greatly appreciated!
Hey guys! I'm new to Factories.

Is there cubie for DecisionTreeClassifier so that I can serialize/deserialize it?
I want to init ItemizedDirectedModel but I don't know how ot init this Object, use addFactors?
Hi,
I was wondering if you are planning to officially support Scala 2.12?
Scala 2.12 is out for quite some time now and a lot major libraries [support it](https://github.com/scala/make-release-notes/blob/2.12.x/projects-2.12.md).
Any comment on this would be highly appreciated!
tl;dr: I noticed that calling `contains` on `CategoricalDomain` has what I consider to be unexpected behavior and I'm not sure why this occurs or how to fix it.

In my opinion, `domain.index(value) >= 0`, `domain.categories.contains(value)` and `domain.contains(value)` should all return the same result, but they don't, as illustrated in the following slightly-modified version of `TestCategoricalDomain`:

    val domain = new CategoricalDomain[String](List("yes", "no"))
    domain.freeze()
    assert(domain.size == 2) // passes
    // I added the following lines
    assert(domain.index("yes") >= 0) //passes
    assert(domain.categories.contains("yes")) //passes
    assert(domain.contains("yes")) //fails

It seems like it's standard "FACTORIE insider knowledge" to just use one of `domain.index(value) >= 0` or `domain.categories.contains(value)` as a proxy for `contains`, but I think it'd be nice to either fix this issue or at least make it more obvious to the user that `contains` does not work so they should use a different method instead (e.g. throw an informative UnsupportedOperationException).

My first thought was to submit a PR for CategoricalDomain with something like:

    override def contains(category: C): Boolean = indexOrNegativeOne(category) >= 0

If I don't override, the compiler finds a name clash because of type erasure; if I do override, the compiler says the method overrides nothing. So the above is out, unless someone knows how to Scala their way around it.

I'm still not even sure why `contains` doesn't work in the first place, and I've been staring at the debugger for too long. So... I would welcome additional input/advice on how to deal with this small but thorny issue.

Hi, I am trying to compile the project with IntelliJ, but I get:

```
Error:(61, 15) not found: type EnglishLexer
          new EnglishLexer(reader, tokenizeSgml, tokenizeNewline, tokenizeWhitespace, tokenizeAllDashedWords, abbrevPrecedesLowercase,
```

If I compile with the instruction from the README from command line it works, but I would like to use IntelliJ.

Also, how could I set the language of the NER parser to Spanish?

Regards!

For the TAC competition, we should also view the html tags such as <P> or </P> as the line break.
