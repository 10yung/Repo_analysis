Fixes https://github.com/apache/druid/issues/9220.

### Description

The tuningConfig type has changed in https://github.com/apache/druid/pull/8570, but the compaction task should be able to read the spec of the old version for rolling upgrades.

<hr>

This PR has:
- [x] been self-reviewed.
   - [ ] using the [concurrency checklist](https://github.com/apache/druid/blob/master/dev/code-review/concurrency.md) (Remove this item if the PR doesn't have any relation to concurrency.)
- [ ] added documentation for new or modified features or behaviors.
- [ ] added Javadocs for most classes and all non-trivial methods. Linked related entities via Javadoc links.
- [ ] added or updated version, license, or notice information in [licenses.yaml](https://github.com/apache/druid/blob/master/licenses.yaml)
- [ ] added comments explaining the "why" and the intent of the code wherever would not be obvious for an unfamiliar reader.
- [x] added unit tests or modified existing tests to cover new code paths.
- [ ] added integration tests.
- [x] been tested in a test Druid cluster.

<!-- Reviewable:start -->
---
This change isâ€‚[<img src="https://reviewable.io/review_button.svg" height="34" align="absmiddle" alt="Reviewable"/>](https://reviewable.io/reviews/apache/druid/9222)
<!-- Reviewable:end -->

{
  "dataSource": "rd_stack",
  "stream": "rd_stack",
  "partitions": 1,
  "replicas": 1,
  "durationSeconds": 3600,
  "activeTasks": [],
  "publishingTasks": [],
  "latestOffsets": {
    "0": 174
  },
  "minimumLag": {},
  "aggregateLag": 0,
  "offsetsLastUpdated": "2020-01-18T08:27:44.985Z",
  "suspended": false,
  "healthy": false,
  "state": "UNHEALTHY_SUPERVISOR",
  "detailedState": "UNABLE_TO_CONNECT_TO_STREAM",
  "recentErrors": [
    {
      "timestamp": "2020-01-18T08:26:30.003Z",
      "exceptionClass": "org.apache.druid.java.util.common.ISE",
      "message": "org.apache.druid.java.util.common.ISE: Previous sequenceNumber [344019] is no longer available for partition [0]. You can clear the previous sequenceNumber and start reading from a valid message by using the supervisor's reset API.",
      "streamException": true
    },
    {
      "timestamp": "2020-01-18T08:26:59.996Z",
      "exceptionClass": "org.apache.druid.java.util.common.ISE",
      "message": "org.apache.druid.java.util.common.ISE: Previous sequenceNumber [344019] is no longer available for partition [0]. You can clear the previous sequenceNumber and start reading from a valid message by using the supervisor's reset API.",
      "streamException": true
    },
    {
      "timestamp": "2020-01-18T08:27:29.997Z",
      "exceptionClass": "org.apache.druid.java.util.common.ISE",
      "message": "org.apache.druid.java.util.common.ISE: Previous sequenceNumber [344019] is no longer available for partition [0]. You can clear the previous sequenceNumber and start reading from a valid message by using the supervisor's reset API.",
      "streamException": true
    }
  ]
}
Auto Compaction does not work when coordinator is running on 0.17.0 and MiddleManager is running on 0.16.0

### Affected Version

coordinator is running on 0.17.0 and MiddleManager is running on 0.16.0

### Description

Auto Compaction does not work when coordinator is running on 0.17.0 and MiddleManager is running on 0.16.0.
This is due to incompatibility when deserializing the json ingestion spec that is sent to MiddleManager from coordinator. This will automatically fix itself (since auto compaction is auto run) after coordinator upgraded to 0.17.0

`2020-01-18T02:02:34,669 ERROR [TaskMonitorCache-0] org.apache.curator.framework.recipes.cache.PathChildrenCache - 
com.fasterxml.jackson.databind.exc.InvalidTypeIdException: Could not resolve type id 'index' as a subtype of `org.apache.druid.indexing.common.task.batch.parallel.ParallelIndexTuningConfig`: known type ids = [index_parallel, realtime] (for POJO property 'tuningConfig')
 at [Source: (byte[])"{"type":"compact","id":"compact_auto-compact-test-new-4_2020-01-18T02:02:34.633Z","resource":{"availabilityGroup":"compact_auto-compact-test-new-4_2020-01-18T02:02:34.633Z","requiredCapacity":1},"dataSource":"auto-compact-test-new-4","interval":null,"segments":[{"dataSource":"auto-compact-test-new-4","interval":"2015-09-12T00:00:00.000Z/2015-09-13T00:00:00.000Z","version":"2020-01-18T01:33:43.101Z","loadSpec":{"type":"local","path":"/tmp/rolling-upgrade-test/var/druid/segments/auto-compact-test-"[truncated 30732 bytes]; line: 1, column: 30439] (through reference chain: org.apache.druid.indexing.common.task.CompactionTask["tuningConfig"])
        at com.fasterxml.jackson.databind.exc.InvalidTypeIdException.from(InvalidTypeIdException.java:43) ~[jackson-databind-2.10.1.jar:2.10.1]
        at com.fasterxml.jackson.databind.DeserializationContext.invalidTypeIdException(DeserializationContext.java:1758) ~[jackson-databind-2.10.1.jar:2.10.1]
        at com.fasterxml.jackson.databind.DeserializationContext.handleUnknownTypeId(DeserializationContext.java:1265) ~[jackson-databind-2.10.1.jar:2.10.1]
        at com.fasterxml.jackson.databind.jsontype.impl.TypeDeserializerBase._handleUnknownTypeId(TypeDeserializerBase.java:290) ~[jackson-databind-2.10.1.jar:2.10.1]
        at com.fasterxml.jackson.databind.jsontype.impl.TypeDeserializerBase._findDeserializer(TypeDeserializerBase.java:162) ~[jackson-databind-2.10.1.jar:2.10.1]
        at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:113) ~[jackson-databind-2.10.1.jar:2.10.1]
        at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:97) ~[jackson-databind-2.10.1.jar:2.10.1]
        at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeWithType(BeanDeserializerBase.java:1178) ~[jackson-databind-2.10.1.jar:2.10.1]
        at com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:527) ~[jackson-databind-2.10.1.jar:2.10.1]
        at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeWithErrorWrapping(BeanDeserializer.java:528) ~[jackson-databind-2.10.1.jar:2.10.1]
        at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:417) ~[jackson-databind-2.10.1.jar:2.10.1]
        at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1287) ~[jackson-databind-2.10.1.jar:2.10.1]
        at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:326) ~[jackson-databind-2.10.1.jar:2.10.1]
        at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:194) ~[jackson-databind-2.10.1.jar:2.10.1]
        at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:161) ~[jackson-databind-2.10.1.jar:2.10.1]
        at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:130) ~[jackson-databind-2.10.1.jar:2.10.1]
        at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:97) ~[jackson-databind-2.10.1.jar:2.10.1]
        at com.fasterxml.jackson.databind.deser.AbstractDeserializer.deserializeWithType(AbstractDeserializer.java:254) ~[jackson-databind-2.10.1.jar:2.10.1]
        at com.fasterxml.jackson.databind.deser.impl.TypeWrappedDeserializer.deserialize(TypeWrappedDeserializer.java:68) ~[jackson-databind-2.10.1.jar:2.10.1]
        at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4202) ~[jackson-databind-2.10.1.jar:2.10.1]
        at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3266) ~[jackson-databind-2.10.1.jar:2.10.1]
        at org.apache.druid.indexing.worker.WorkerTaskMonitor$1.childEvent(WorkerTaskMonitor.java:165) ~[druid-indexing-service-0.17.0-SNAPSHOT.jar:0.17.0-SNAPSHOT]
        at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:538) [curator-recipes-4.1.0.jar:4.1.0]
        at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:532) [curator-recipes-4.1.0.jar:4.1.0]
        at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:93) [curator-framework-4.1.0.jar:4.1.0]
        at org.apache.curator.shaded.com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:435) [curator-client-4.1.0.jar:?]
        at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:85) [curator-framework-4.1.0.jar:4.1.0]
        at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:530) [curator-recipes-4.1.0.jar:4.1.0]
        at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35) [curator-recipes-4.1.0.jar:4.1.0]
        at org.apache.curator.framework.recipes.cache.PathChildrenCache$9.run(PathChildrenCache.java:808) [curator-recipes-4.1.0.jar:4.1.0]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_232]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_232]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_232]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_232]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_232]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_232]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_232]`
Backport of #9183 to 0.17.0.
This PR adds some structure to the s3 and google extensions docs to indicate that they can be used for either deep storage or ingesting files. They are structured to indicated what configuration is needed for each.

This also makes some stylistic changes to the avro and orc docs.
Update kafka ingestion specs in tutorial docs to use the new inputSpec instead of parseSpec
Update first/ last aggregator docs to remove filterNullValues
See https://github.com/apache/druid/pull/6591, https://github.com/apache/druid/pull/4004#issuecomment-284171911 for details.
azure extensions: task log kill crashes with "Not Implemented"

### Affected Version
0.16.0-incubating

### Description
I have a small 3-master, 2-query, 2-storage node cluster and need to be able to shrink the druid_tasks table.  

I added the following to my overlord configuration:
>  
    # remove old logs
    druid.indexer.logs.kill.enabled=true
    # after: (in ms).
    # 14 days * 24 hours * 60 minutes * 60 seconds * 1000 ms
    druid.indexer.logs.kill.durationToRetain=1209600000`

and when the kill task ran, I got:

> 2020-01-17T22:04:38,578 ERROR [Overlord-Helper-Manager-Exec--0] org.apache.druid.indexing.overlord.helpers.TaskLogAutoCleaner - Failed to clean-up the task logs
java.lang.UnsupportedOperationException: not implemented
        at org.apache.druid.storage.azure.AzureTaskLogs.killOlderThan(AzureTaskLogs.java:159) ~[?:?]
        at org.apache.druid.indexing.overlord.helpers.TaskLogAutoCleaner$1.run(TaskLogAutoCleaner.java:75) [druid-indexing-service-0.16.0-incubating.jar:0.16.0-incubating]
        at org.apache.druid.java.util.common.concurrent.ScheduledExecutors$1.call(ScheduledExecutors.java:55) [druid-core-0.16.0-incubating.jar:0.16.0-incubating]
        at org.apache.druid.java.util.common.concurrent.ScheduledExecutors$1.call(ScheduledExecutors.java:51) [druid-core-0.16.0-incubating.jar:0.16.0-incubating]
        at org.apache.druid.java.util.common.concurrent.ScheduledExecutors$2.run(ScheduledExecutors.java:92) [druid-core-0.16.0-incubating.jar:0.16.0-incubating]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_232]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_232]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_232]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_232]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_232]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_232]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_232]

Is there a plan to implement this?

### Description

Apache Pig dependencies as well as related functionality were removed from the Avro parser in #7810 
We internally use Pig extensively and the Pig specific transformation within the Avro parser extension is useful for us especially for parsing data bags. I would like to add the `fromPigAvroStorage` flag based processing back to the Avro Parser. In terms of tests, I can find a way to test this without adding the Pig dependency back.
I apologize for missing out on raising this concern on the original PR.

Any objections @gianm @Fokko ?
<!-- Replace XXXX with the id of the issue fixed in this PR. Remove this section if there is no corresponding issue. Don't reference the issue in the title of this pull-request. -->

<!-- If you are a committer, follow the PR action item checklist for committers:
https://github.com/apache/druid/blob/master/dev/committer-instructions.md#pr-and-issue-action-item-checklist-for-committers. -->

### Description

Remove unnecessary casts.

<!-- Describe the goal of this PR, what problem are you fixing. If there is a corresponding issue (referenced above), it's not necessary to repeat the description here, however, you may choose to keep one summary sentence. -->

<!-- Describe your patch: what did you change in code? How did you fix the problem? -->

<!-- If there are several relatively logically separate changes in this PR, create a mini-section for each of them. For example: -->

#### Fixed the bug ...
#### Renamed the class ...
#### Added a forbidden-apis entry ...

<!--
In each section, please describe design decisions made, including:
 - Choice of algorithms
 - Behavioral aspects. What configuration values are acceptable? How are corner cases and error conditions handled, such as when there are insufficient resources?
 - Class organization and design (how the logic is split between classes, inheritance, composition, design patterns)
 - Method organization and design (how the logic is split between methods, parameters and return types)
 - Naming (class, method, API, configuration, HTTP endpoint, names of emitted metrics)
-->


<!-- It's good to describe an alternative design (or mention an alternative name) for every design (or naming) decision point and compare the alternatives with the designs that you've implemented (or the names you've chosen) to highlight the advantages of the chosen designs and names. -->

<!-- If there was a discussion of the design of the feature implemented in this PR elsewhere (e. g. a "Proposal" issue, any other issue, or a thread in the development mailing list), link to that discussion from this PR description and explain what have changed in your final design compared to your original proposal or the consensus version in the end of the discussion. If something hasn't changed since the original discussion, you can omit a detailed discussion of those aspects of the design here, perhaps apart from brief mentioning for the sake of readability of this PR description. -->

<!-- Some of the aspects mentioned above may be omitted for simple and small changes. -->

<hr>

This PR has:
- [ ] been self-reviewed.
   - [ ] using the [concurrency checklist](https://github.com/apache/druid/blob/master/dev/code-review/concurrency.md) (Remove this item if the PR doesn't have any relation to concurrency.)
- [ ] added documentation for new or modified features or behaviors.
- [ ] added Javadocs for most classes and all non-trivial methods. Linked related entities via Javadoc links.
- [ ] added or updated version, license, or notice information in [licenses.yaml](https://github.com/apache/druid/blob/master/licenses.yaml)
- [ ] added comments explaining the "why" and the intent of the code wherever would not be obvious for an unfamiliar reader.
- [ ] added unit tests or modified existing tests to cover new code paths.
- [ ] added integration tests.
- [ ] been tested in a test Druid cluster.

<!-- Check the items by putting "x" in the brackets for the done things. Not all of these items apply to every PR. Remove the items which are not done or not relevant to the PR. None of the items from the checklist above are strictly necessary, but it would be very helpful if you at least self-review the PR. -->

<hr>

##### Key changed/added classes in this PR
 * `MyFoo`
 * `OurBar`
 * `TheirBaz`
