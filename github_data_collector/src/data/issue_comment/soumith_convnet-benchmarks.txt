If you build and install tensorflow 1.8 from https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/tree/r1.8-rocm and run the benchmark tests, you will get 0 values either in AMD or Nvidia cards with their latest release public driver.

2018-07-09 20:30:59.599248: Forward across 100 steps, 0.000 +/- 0.000 sec / batch

Is this benchmark no more applicable for tensorflow1.8 latest version?
Is dev is going to update it to work for tensorflow1.8 and higher version for the future?
Operating System: Ubuntu 16.04.3 LTS, Linux kernel 4.13.0
GPU: AMD RX 580
ROCm backend.

```
~/convnet-benchmarks/cltorch$ th imagenet_winners/benchmark.lua 
libthclnn_searchpath    /storage/home/yige/torch-cl/install/lib/lua/5.1/libTHCLNN.so
Running on device: gfx803
Using Advanced Micro Devices, Inc. , OpenCL platform: AMD Accelerated Parallel Processing
Using OpenCL device: gfx803
ModelType: OverFeat[fast]       Kernels: clnn   Input shape: 128x3x231x231
clnn                                    :updateOutput():     673.86
clnn                                 :updateGradInput():     344.01
clnn                               :accGradParameters():     480.27
clnn                                           :Forward:     673.86
clnn                                          :Backward:     824.27
clnn                                             :TOTAL:    1498.13
ModelType: AlexNet      Kernels: clnn   Input shape: 128x3x224x224
clnn                                    :updateOutput():     311.44
clnn                                 :updateGradInput():     158.93
clnn                               :accGradParameters():     623.55
clnn                                           :Forward:     311.44
clnn                                          :Backward:     782.48
clnn                                             :TOTAL:    1093.92
ModelType: VGG Model-A  Kernels: clnn   Input shape: 64x3x224x224
clnn                                    :updateOutput():     671.76
clnn                                 :updateGradInput():     508.20
clnn                               :accGradParameters():    1174.33
clnn                                           :Forward:     671.76
clnn                                          :Backward:    1682.52
clnn                                             :TOTAL:    2354.29
/storage/home/yige/torch-cl/install/bin/luajit: ./imagenet_winners/googlenet.lua:33: attempt to index global 'cudnn' (a nil value)
stack traceback:
        ./imagenet_winners/googlenet.lua:33: in function <./imagenet_winners/googlenet.lua:30>
        imagenet_winners/benchmark.lua:34: in main chunk
        [C]: in function 'dofile'
        ...e/torch-cl/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk
        [C]: at 0x00405e90
```
Hello, would you please help explain this issue?  Thanks in advance.
We found that convnet-benchmarks performance on cupy-2.0.0 is worse than that on cupy-1.0.0.1.  We don't know whether it is problem of cupy or convnet-benchmarks scripts. We reported this issue in https://github.com/cupy/cupy/issues/753, got no response yet.

---------------------details--------------------------
Test Environment:
P100
Test action:
1, install chainer
2, get convnet-benchmarks code:
git clone https://github.com/mitmul/convnet-benchmarks
3, test cases
3.1: case "pip install cupy==1.0.0.1"
(py2-chainer-gpu) [sys_dltest@mlt-gpu200 chainer]$ python train_imagenet.py
alexnet
('Chainer version:', '2.0.0b1')
('CuPy version:', '1.0.0.1')
('CUDA:', True)
('CUDA Version:', u'V8.0.61')
('cuDNN:', True)
('cuDNN Version:', 5110)
('Input data shape:', (128, 3, 224, 224))
('Average Forward: ', 16.15312328338623, ' ms')
('Average Backward: ', 35.27830085754395, ' ms')
('Average Total: ', 51.431424140930176, ' ms')

3.2: case "pip install cupy==2.0.0"
(py2-chainer-gpu) [sys_dltest@mlt-gpu200 chainer]$ python train_imagenet.py
alexnet
('Chainer version:', '2.0.0b1')
('CuPy version:', '2.0.0')
('CUDA:', True)
('cuDNN:', True)
('cuDNN Version:', 5110)
('Input data shape:', (128, 3, 224, 224))
('Average Forward: ', 35.381299591064455, ' ms')
('Average Backward: ', 63.26389694213867, ' ms')
('Average Total: ', 98.64519653320312, ' ms')

3.3: case "pip install cupy==2.0.0rc1"
(py2-chainer-gpu) [sys_dltest@mlt-gpu200 chainer]$ python train_imagenet.py
alexnet
('Chainer version:', '2.0.0b1')
('CuPy version:', '2.0.0rc1')
('CUDA:', True)
('cuDNN:', True)
('cuDNN Version:', 5110)
('Input data shape:', (128, 3, 224, 224))
('Average Forward: ', 35.5438117980957, ' ms')
('Average Backward: ', 63.336796569824216, ' ms')
('Average Total: ', 98.88060836791992, ' ms')

Notice: when run "case cupy==2.0.0*", you need to comment following lines in train_imagenet.py.
#if chainer.cuda.available:
# cuda_v = cupy.cuda.compiler._get_nvcc_version().split()[-1].decode('utf-8')
# print('CUDA Version:', cuda_v)

After trying benchmark_googlenet.py to benchmark, i ran into "**TypeError: Expected int32, got list containing Tensors of type '_Message' instead."**
After searching through i found some links that the tensorflow might not support the old features. Such as tf.concat function.

i think the files uses the old functions from the tensorflow. 




Hi,
I see Tesla GV100 has been given to 15 lucky DNN researchers by Nvidia CEO past week..
so hope someone can post some benchmarks.. :-)

cuDNN v6 is now released and there are clear improvements. How can I go about updating these numbers (and adding more frameworks)? Is it best to just submit a PR with README tables updated? Or did you have different plans for new cuDNN versions?

So far I've got the following for TensorFlow:

```
2017-04-12 11:53:22.136747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 1
2017-04-12 11:53:22.136756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y Y
2017-04-12 11:53:22.136759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 1:   Y Y
2017-04-12 11:53:22.136765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0)
2017-04-12 11:53:22.136768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:1) -> (device: 1, name: TITAN X (Pascal), pci bus id: 0000:03:00.0)
2017-04-12 11:53:23.636099: step 10, duration = 0.017
2017-04-12 11:53:23.808993: step 20, duration = 0.017
2017-04-12 11:53:23.981334: step 30, duration = 0.017
2017-04-12 11:53:24.155021: step 40, duration = 0.017
2017-04-12 11:53:24.329057: step 50, duration = 0.017
2017-04-12 11:53:24.503106: step 60, duration = 0.017
2017-04-12 11:53:24.677380: step 70, duration = 0.017
2017-04-12 11:53:24.851187: step 80, duration = 0.017
2017-04-12 11:53:25.023797: step 90, duration = 0.017
2017-04-12 11:53:25.179516: Forward across 100 steps, 0.017 +/- 0.002 sec / batch
2017-04-12 11:53:26.523299: step 10, duration = 0.050
2017-04-12 11:53:27.033520: step 20, duration = 0.051
2017-04-12 11:53:27.538539: step 30, duration = 0.050
2017-04-12 11:53:28.039816: step 40, duration = 0.050
2017-04-12 11:53:28.547799: step 50, duration = 0.050
2017-04-12 11:53:29.049781: step 60, duration = 0.050
2017-04-12 11:53:29.551407: step 70, duration = 0.050
2017-04-12 11:53:30.055291: step 80, duration = 0.050
2017-04-12 11:53:30.562754: step 90, duration = 0.050
2017-04-12 11:53:31.013226: Forward-backward across 100 steps, 0.050 +/- 0.005 sec / batch
```
Running tensorflow 0.12.head (GPU supported)

No issues when running run_foward. 

This is the output i got when running benchmark_alexnet.py:

2017-04-04 12:28:17.663471: step 10, duration = 0.099
2017-04-04 12:28:18.638741: step 20, duration = 0.097
2017-04-04 12:28:19.610754: step 30, duration = 0.097
2017-04-04 12:28:20.584540: step 40, duration = 0.098
2017-04-04 12:28:21.557400: step 50, duration = 0.097
2017-04-04 12:28:22.528207: step 60, duration = 0.096
2017-04-04 12:28:23.504192: step 70, duration = 0.097
2017-04-04 12:28:24.476781: step 80, duration = 0.097
2017-04-04 12:28:25.449244: step 90, duration = 0.097
2017-04-04 12:28:26.325418: Forward across 100 steps, 0.096 +/- 0.010 sec / batch
WARNING:tensorflow:From /home/atinzad/code/benchmark_alexnet.py:102 in loss.: concat (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2016-12-13.
Instructions for updating:
This op will be removed after the deprecation date. Please switch to tf.concat_v2().
Traceback (most recent call last):

  File "<ipython-input-1-d143c4740e2b>", line 1, in <module>
    runfile('/home/atinzad/code/benchmark_alexnet.py', wdir='/home/atinzad/code')

  File "/usr/local/lib/python2.7/dist-packages/spyder/utils/site/sitecustomize.py", line 866, in runfile
    execfile(filename, namespace)

  File "/usr/local/lib/python2.7/dist-packages/spyder/utils/site/sitecustomize.py", line 94, in execfile
    builtins.execfile(filename, *where)

  File "/home/atinzad/code/benchmark_alexnet.py", line 221, in <module>
    tf.app.run()

  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))

  File "/home/atinzad/code/benchmark_alexnet.py", line 217, in main
    run_benchmark()

  File "/home/atinzad/code/benchmark_alexnet.py", line 206, in run_benchmark
    objective = loss(last_layer, labels)

  File "/home/atinzad/code/benchmark_alexnet.py", line 102, in loss
    concated = tf.concat([indices, labels], 1)

  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 111, in new_func
    return func(*args, **kwargs)

  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py", line 1118, in concat
    return concat_v2(values, concat_dim, name)

  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py", line 1053, in concat_v2
    ).assert_is_compatible_with(tensor_shape.scalar())

  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py", line 756, in assert_is_compatible_with
    raise ValueError("Shapes %s and %s are incompatible" % (self, other))

ValueError: Shapes (2, 128, 1) and () are incompatible