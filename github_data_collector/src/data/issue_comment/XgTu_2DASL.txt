There is no visualize.rar file for google drive :(

Could you please upload it once more?
Could you please publicize the training code? We are not sure about the training details such as how to get the mean and std of parameters and how to set the super parameters about the net. Thanks a lot! 
Hi @XgTu , Thanks for sharing the code.

I am able to successfully run the project however the obtained obj's are just white meshes. The material file for each obj is not getting generated. 

1. In the writeobj.m function i am guessing **options** argument has to be called properly.

2. Should we write texture .jpg to the mtl file (i.e any connection between options.nm_file is texture jpg?)  but in this project we are not writing generating any texture file.

 Can you please tell me if i am wrong and how to create mtl files?

Thank you.

Hello,thanks for your open source code!
I'm getting start to follow the work of PRN, but I cannot really reimplement their work yet.
I've read  your paper on arxiv and noticed that you compared 2DASL with PRN in experiment part.
So I hope you can help me with some problems about the NME metrics.

Do reimplement PRN and get  the results or you just evaluate their model?
How do you calculate 3D NME and do you have any idea about how do PRN calculate 3D NME? 



(I evaluated the model provided in their github on AFLW2000-3D.
The 2D NME is alright.
But I  come up with some problem when calculating 3D NME. 

I use exactly the same normalization factor (bounding box size) and calculate the mean L2 distance between ground truth and predicted results.
But the 68 kpt 3D NME I got is around 6.0%, which is much higher than PRN paper's 4.4% and similar to 6.12% in your paper.

I've tried to refer to the evaluation code of 3DDFA but it seems that they didn't evaluate 3D coordinates.)



Hi Tu,

Thanks for your code. I have obtained 53215 vertices from 3DMM, but I do not which pixels are the 34 2D visible landmarks of AFLW-LFPA. Could you please tell me the relationship between them?  Thanks!
Hi, i recently read your paper, but i have a little confusion about the backward pass. In your paper, you show that you backward pass your predicted 2d landmarks to output x^2d, do you means that you replace x2d with x^2d to generate 2d FLMs and keep the input image unchanged and restart forward training? If i am understanding it in a wrong way, could you please describe the backward pass with more details?
Hello, I am very happy to see your work, I want to achieve a dense alignment of the face, like the following
![image](https://user-images.githubusercontent.com/24581773/60382911-9791c800-9a9c-11e9-9544-c4774813abbd.png)
But I didn't see how to implement this step in your code, and your code is based on Matlab, not Python. Can you provide a Python implementation? This will help understand the meaning of each step, thank you
Hi,

I am kind of new to pytorch, did anyone know which script should I run in order to predict 2D or 3D landmarks for a given facial image?  or should I write a new testing code and use the model such as "2DASL_checkpoint_epoch_allParams_stage2.pth.tar" for this task?

Thanks a lot!
3ddfa：
![test_vertex](https://user-images.githubusercontent.com/30692249/59735487-5b828a00-9288-11e9-90ee-77594d856ab4.gif)

2dasl：
![test_vertex](https://user-images.githubusercontent.com/30692249/59735459-3c83f800-9288-11e9-9ea3-731f1380aac4.gif)

我感觉3ddfa更好，你认为我测试2dasl时问题会出在哪呢。。

Where the code to apply Face swapping？