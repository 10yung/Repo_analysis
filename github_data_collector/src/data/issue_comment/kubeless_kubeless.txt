**Is this a BUG REPORT or FEATURE REQUEST?**:
BUG REPORT

**What happened**:
I've created a new function that just print the date and created a cronjob trigger thats run each minute.
The pod of the the trigger cannot be created due to the following error:

```
Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod "trigger-koby-cronjob-1579087980-z688b": Error response from daemon: OCI runtime create failed: container_linux.go:346: starting container process caused "process_linux.go:449: container init caused \"\"": unknown

Pod sandbox changed, it will be killed and re-created.
```


**What you expected to happen**:
The job will be complted successfully

**How to reproduce it (as minimally and precisely as possible)**:
python file:
```
import datetime
def koby2():
        print "cronJob NEW - %s"%datetime.datetime.now()
```
kubeless function deploy:
```kubeless function deploy koby-cronjob --runtime python2.7 --handler k.koby2 --from-file k.py```

trigger creation:
```kubeless trigger cronjob create koby2 --function koby-cronjob --schedule "* * * * *"```

**Anything else we need to know?**:
kubectl describe pod trigger-koby-cronjob-1579088100-wmd4z

```
Name:               trigger-koby-cronjob-1579088100-wmd4z
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               aks-nodepool1-38957994-vmss000000/10.240.0.4
Start Time:         Wed, 15 Jan 2020 13:36:33 +0200
Labels:             controller-uid=13fadf76-378b-11ea-b94e-462100fdc0cd
                    job-name=trigger-koby-cronjob-1579088100
Annotations:        kubernetes.io/psp: privileged
Status:             Pending
IP:
Controlled By:      Job/trigger-koby-cronjob-1579088100
Containers:
  trigger:
    Container ID:
    Image:         kubeless/unzip@sha256:f162c062973cca05459834de6ed14c039d45df8cdb76097f50b028a1621b3697
    Image ID:
    Port:          <none>
    Host Port:     <none>
    Args:
      curl
      -Lv
       -H "event-id: MUNUDPwFaFpTRkk" -H "event-time: 2020-01-15 11:32:05.158467536 +0000 UTC" -H "event-type: application/json" -H "event-namespace: cronjobtrigger.kubeless.io"
      http://koby-cronjob.default.svc.cluster.local:8080
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Limits:
      cpu:     1m
      memory:  4Mi
    Requests:
      cpu:        1m
      memory:     4Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fjfpg (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  default-token-fjfpg:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-fjfpg
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason                  Age                     From                                        Message
  ----     ------                  ----                    ----                                        -------
  Normal   Scheduled               8m59s                   default-scheduler                           Successfully assigned default/trigger-koby-cronjob-1579088100-wmd4z to aks-nodepool1-38957994-vmss000000
  Warning  FailedCreatePodSandBox  8m51s                   kubelet, aks-nodepool1-38957994-vmss000000  Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod "trigger-koby-cronjob-1579088100-wmd4z": Error response from daemon: OCI runtime create failed: runc did not terminate sucessfully: unknown
  Warning  FailedCreatePodSandBox  6m17s (x12 over 8m42s)  kubelet, aks-nodepool1-38957994-vmss000000  Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod "trigger-koby-cronjob-1579088100-wmd4z": Error response from daemon: OCI runtime create failed: container_linux.go:346: starting container process caused "process_linux.go:449: container init caused \"read init-p: connection reset by peer\"": unknown
  Normal   SandboxChanged          3m42s (x25 over 8m47s)  kubelet, aks-nodepool1-38957994-vmss000000  Pod sandbox changed, it will be killed and re-created.
```


**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:"1", Minor:"14", GitVersion:"v1.14.1", GitCommit:"b7394102d6ef778017f2ca4046abbaa23b88c290", GitTreeState:"clean", BuildDate:"2019-04-08T17:11:31Z", GoVersion:"go1.12.1", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"14", GitVersion:"v1.14.8", GitCommit:"1da9875156ba0ad48e7d09a5d00e41489507f592", GitTreeState:"clean", BuildDate:"2019-11-14T05:19:20Z", GoVersion:"go1.12.10", Compiler:"gc", Platform:"linux/amd64"}
```
- Kubeless version (use `kubeless version`):
```Kubeless version: v1.0.5-dirty```

- Cloud provider or physical cluster:
Cloud provider = Azure - AKS

**Is this a BUG REPORT or FEATURE REQUEST?**:
FEATURE REQUEST

**What happened**:
When pods are terminated, active connections are closed.

**What you expected to happen**:
The function pod should attempt a graceful shutdown on sigterm with some configurable timeout.

**How to reproduce it (as minimally and precisely as possible)**:
1. Create a function that sleeps for 10 seconds or so and returns success.
2. Trigger the function.
3. Intentionally terminate the pod/docker container before the function completes

You'll get a socket hang up error.

**Anything else we need to know?**:
I might like to attempt a PR for this if you decide it's an feature worth having.
From what I can see it looks like it could be added relatively easily to the function proxy following this example: https://gist.github.com/peterhellberg/38117e546c217960747aacf689af3dc2

**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:"1", Minor:"17", GitVersion:"v1.17.0", GitCommit:"70132b0f130acc0bed193d9ba59dd186f0e634cf", GitTreeState:"clean", BuildDate:"2019-12-07T21:20:10Z", GoVersion:"go1.13.4", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"13+", GitVersion:"v1.13.12-gke.16", GitCommit:"d40fd337c7a73c3720b57f23b8d1f21f1b2df7ca", GitTreeState:"clean", BuildDate:"2019-11-25T19:40:16Z", GoVersion:"go1.12.11b4", Compiler:"gc", Platform:"linux/amd64"}
```

- Kubeless version (use `kubeless version`): `Kubeless version: v1.0.4-dirty`
- Cloud provider or physical cluster: GKE

**Is this a BUG REPORT or FEATURE REQUEST?**:
BUG

**What happened**:
```
NAME    NAMESPACE       HANDLER         RUNTIME         DEPENDENCIES                    STATUS       
weather kubeless        test.weather    nodejs12        request: ^2.88.0                0/1 NOT READY
                                                        request-promise-native: ^1.0.8 
```
I'm following the example provided below but am getting a status not ready when deploying the function. 
https://github.com/kubeless/functions/tree/master/incubator/weather

I also visited this comment and applied it but am still getting status not ready
https://github.com/kubeless/kubeless/issues/861#issuecomment-408779664

**What you expected to happen**:
```
NAME    NAMESPACE       HANDLER         RUNTIME         DEPENDENCIES                    STATUS       
weather kubeless        test.weather    nodejs12        request: ^2.88.0                1/1 READY
                                                        request-promise-native: ^1.0.8 
```

I'm also seeing this issue with the serverless-kubeless plugin when packaging functions with dependencies.

**How to reproduce it (as minimally and precisely as possible)**:
Run the weather example

**Anything else we need to know?**:
Using kubeless with minikube and context virtual box.

**Environment**:
- Kubernetes version (use `kubectl version`): 
```
Client Version: version.Info{Major:"1", Minor:"14", GitVersion:"v1.14.8", GitCommit:"211047e9a1922595eaa3a1127ed365e9299a6c23", GitTreeState:"clean", BuildDate:"2019-10-15T12:11:03Z", GoVersion:"go1.12.10", Compiler:"gc", Platform:"darwin/amd64"}
Server Version: version.Info{Major:"1", Minor:"17", GitVersion:"v1.17.0", GitCommit:"70132b0f130acc0bed193d9ba59dd186f0e634cf", GitTreeState:"clean", BuildDate:"2019-12-07T21:12:17Z", GoVersion:"go1.13.4", Compiler:"gc", Platform:"linux/amd64"}
```

- Kubeless version (use `kubeless version`): v1.0.5-dirty

- Cloud provider or physical cluster:
local cluster using minikube and virtualbox

**Is this a BUG REPORT or FEATURE REQUEST?**:
BUG REPORT

**What happened**:
I'm trying to expose a function deployed on a AWS EKS Cluster with http-trigger; either by using kong ingress controller or nginx ingress controller I get a fatal error

**What you expected to happen**:
the ingress creation for the function is created and function gets exposed

**How to reproduce it (as minimally and precisely as possible)**:
`kubeless trigger http create hello-ingress --function-name hello --gateway nginx`
`FATA[0002] Can not create out-of-cluster client: stat :/Users/andreaspoldi/.kube/config: no such file or directory `

**Anything else we need to know?**:
my kubeconfig exists and it is the proper path..other kubeless commands works okay 

```
kubeless get-server-config 
INFO[0002] Current Server Config:                       
INFO[0002] Supported Runtimes are: ballerina0.981.0, dotnetcore2.0, dotnetcore2.1, go1.10, go1.11, go1.12, java1.8, java11, nodejs6, nodejs8, nodejs10, nodejs12, php7.2, php7.3, python2.7, python3.4, python3.6, python3.7, ruby2.3, ruby2.4, ruby2.5, ruby2.6, jvm1.8, nodejs_distroless8, nodejsCE8, vertx1.8 
```

my kubeconfig is generated like this 
`aws eks --region region update-kubeconfig --name cluster_name` as per AWS documentation [https://docs.aws.amazon.com/eks/latest/userguide/create-kubeconfig.html](url)

**Environment**:
- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:"1", Minor:"14", GitVersion:"v1.14.1", GitCommit:"b7394102d6ef778017f2ca4046abbaa23b88c290", GitTreeState:"clean", BuildDate:"2019-04-08T17:11:31Z", GoVersion:"go1.12.1", Compiler:"gc", Platform:"darwin/amd64"}
Server Version: version.Info{Major:"1", Minor:"14+", GitVersion:"v1.14.9-eks-c0eccc", GitCommit:"c0eccca51d7500bb03b2f163dd8d534ffeb2f7a2", GitTreeState:"clean", BuildDate:"2019-12-22T23:14:11Z", GoVersion:"go1.12.12", Compiler:"gc", Platform:"linux/amd64"}

- Kubeless version (use `kubeless version`):
Kubeless version: v1.0.5-dirty
- Cloud provider or physical cluster:
AWS EKS

Thanks
**Is this a BUG REPORT or FEATURE REQUEST?**:
BUG REPORT

**What happened**:
I made the kubeless setup and everything worked fine with examples.
Then i migrated a lambda to it and now when there is a error (I am guessing there is a error btw), I can't see the logs

This is the very first lines of the lambda:
```
def lambda_handler(event, context):
    print('aaaaa')
    print(event)
```

When I call with dummy data it prints as expect, but when I call it with some real data, and I am guerssing there is a error, no output is produced with kubectl logs, and after some time I see:

```
10.244.0.206 - - [06/Jan/2020:18:09:04 +0000] "POST / HTTP/1.1" 408 754 "" "curl/7.58.0" 180/120776
```

If I call the lambda by cli:
```
% kubeless function call xxxxxx --data '{"group_id":"yyyyyyyy"}'
ERRO[0180]                                              
FATA[0180] Request timeout exceeded    
```

**What you expected to happen**:
The minimum I expect is to see some logs...

**How to reproduce it (as minimally and precisely as possible)**:
Not sure here, there is a lot of async on this code.

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```
% kubectl version
Client Version: version.Info{Major:"1", Minor:"16", GitVersion:"v1.16.2", GitCommit:"c97fe5036ef3df2967d086711e6c0c405941e14b", GitTreeState:"clean", BuildDate:"2019-10-15T19:18:23Z", GoVersion:"go1.12.10", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"16", GitVersion:"v1.16.2", GitCommit:"c97fe5036ef3df2967d086711e6c0c405941e14b", GitTreeState:"clean", BuildDate:"2019-10-15T19:09:08Z", GoVersion:"go1.12.10", Compiler:"gc", Platform:"linux/amd64"}
```

- Kubeless version (use `kubeless version`):
```
% kubeless version
Kubeless version: v1.0.5-dirty
```


- Cloud provider or physical cluster:
Digital Ocean
Best of luck for the whole project team!
This is a great start for 2020!
**Is this a BUG REPORT or FEATURE REQUEST?**:

Bug

**What happened**:

When I customize service port. My cronjob not use right value.

```
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2019-12-16T04:20:15Z"
  labels:
    created-by: kubeless
    function: aws-component-monitoring
  name: aws-component-monitoring
  namespace: kubeless
...
spec:
  ports:
  - name: http-function-port
    port: 9100
    protocol: TCP
    targetPort: 9100
...

---
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  creationTimestamp: "2019-12-16T04:20:16Z"
  labels:
    created-by: kubeless
    function: aws-component-monitoring
  name: trigger-aws-component-monitoring
  namespace: kubeless
...
spec:
  concurrencyPolicy: Allow
  failedJobsHistoryLimit: 1
  jobTemplate:
...
            - -Lv
            - ' -H "event-id: 5mAAlIu7XQtXqSo" -H "event-time: 2019-12-16 04:20:16.53058757
              +0000 UTC" -H "event-type: application/json" -H "event-namespace: cronjobtrigger.kubeless.io"'
            - http://aws-component-monitoring.kubeless.svc.cluster.local:8080
            image: kubeless/unzip@sha256:4863100364496255de9bf8722a220dad7143ab277ac72435876eb8c93936e9d7
...
```

**What you expected to happen**:

Used customize port

**How to reproduce it (as minimally and precisely as possible)**:

Create function when customize port, and create trigger cronjob

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`): v1.13.4
- Kubeless version (use `kubeless version`): v1.0.5-dirty
- Cloud provider or physical cluster: AWS (kops)


I'm running self hosted registry with `registry:2.7.1`, it is configured with basic auth. 
Everything works with all docker clients, besides kubeless.
Issue error triggered [here](https://github.com/kubeless/kubeless/blob/f3191b29946bd8a18fcd5ac124e8feb174b786fa/pkg/registry/registry.go#L140).

Error `Unable to build function: Unable to check is target image exists: Unable to extract auth info: Unable to find the property Bearer realm in Basic realm=\"basic-realm\"`

Looks like kubeless expect `www-authenticate` header to have the following properties `Bearer realm`, `service`, `scope` which do not exist with basic auth.

Any plans to support basic auth.?
**FEATURE REQUEST**

I'm looking over the docs and it seems that the only way to specify dependencies is with a `Gopkg.toml` file and using `dep`. Given the norm of dependency management in go moving to modules, are there plans to support using the built in module system (which would also allow using a proxy for private packages)?
**Is this a BUG REPORT or FEATURE REQUEST?**: Bug (Design Flaw)

**What happened**: 
Kafka Controller / Triggers provide no back-pressure and constantly pull in data from the Kafka topics. This is a recipe for disaster - leading to Kubeless basically DDOSing its own functions. If the function and/or trigger is removed from Kubeless, the controller still retains all the messages in memory and continues to try and deliver messages to the missing functions (in error).

This also results in other queues/functions being starved out as the controller continues to try and delivery the defunct, large queue - and does not appear to attempt any round-robin like delivery among all the functions/topics.

This is further confounded by the fact that the controller only has 1 consumer per topic, and therefore you can not scale out the queue throughput beyond 1 partition.

**What you expected to happen**: 
Kafka Controller should implement back-pressure capability and provide scalability beyond 1 consumer/partition. One topic/function getting overloaded should not impact other topics / functions. 

In addition, removing the kafka trigger should clear or stop any pending messages trying to get to those functions.

**How to reproduce it (as minimally and precisely as possible)**: 
Use the python 3.7 target and throw 10-20 thousand messages onto a Kafka topic that Kubeless listens to. Then have another function that expects to here Kafka messages on another topic that Kubeless listens to.

**Anything else we need to know?**:
While fixing the single consumer/partition problem isn't necessarily a blocker for adding some sort of back-pressure capability - its certainly lumped in with the overall design flows with the Kafka Controller. 

**Environment**:
- Kubernetes version (use `kubectl version`): 1.13
- Kubeless version (use `kubeless version`): 1.0.4
- Cloud provider or physical cluster: AWS EKS
