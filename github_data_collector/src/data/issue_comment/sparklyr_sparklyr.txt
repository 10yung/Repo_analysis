This PR adds a wrapper for `Dataset.dropDuplicates` method.

It can be used for efficient deduplication of records when particular choice of row doesn't matter, roughly equivalent to:

```r
distinct(iris, Species, .keep_all = TRUE)
```

with plain `data.frame`, which doesn't seem to play well with `sparklyr`

```r
distinct(iris_tbl, Species, .keep_all = TRUE)
## Error: Can only find distinct value of specified columns if .keep_all is FALSE
```

Additionally it has special meaning for streaming datasets.
# Reporting an Issue with sparklyr

For general programming questions with `sparklyr`, please ask on
[Stack Overflow](http://stackoverflow.com) instead.

Please briefly describe your problem and, when relevant, the output you expect.
Please also provide the output of `utils::sessionInfo()` or
`devtools::session_info()` at the end of your post.

If at all possible, please include a [minimal, reproducible
example](https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example).
The `sparklyr` team will be much more likely to resolve your issue if they are
able to reproduce it themselves locally. If possible, try generating a reproducible
example using the [reprex](https://github.com/tidyverse/reprex) package.

Please delete this preamble after you have read it.

---

your brief description of the problem

```r
spark_version <- "2.1.0"
sc <- spark_connect(master = "local", version = spark_version)

# your reproducible example here
```


```r
sc <- spark_connect(master = "local", version = "3.0.0-preview", packages = "delta")
```
```
Error in if (version < "2.4.2") stop("Delta Lake requires Spark 2.4.2 or newer") : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In max(versions[grepl(version, versions, fixed = TRUE)]) :
  no non-missing arguments, returning NA
```
See https://lfscanning.org/reports/lfai/sparklyr-2020-01-10-9a66b8bb-5c91-4415-987f-9d8afe4aaf93.html
Is it possible to get my kerberos ticket inside an spark_apply loop? I would like to read data from impala/kudu after applying some R functionality to my input. Currently I am able to move the implyr package and the impala jdbc driver towards my spark-apply session, but not the kerberos ticket. I cannot find anywhere whether this is possible or not. If this is possible this would truly extend the possibilities of sparklyr, as one can combine data from multiple tables on the cluster without going forth and back from spark to R.
My code structure would look like this:
`
# kinit in terminal of RStudio
# start up spark session

result <- spark_apply(sc, df,
  function(x){
    library(implyr)
    #do some data manipulation using x using R functions

    #construct an impala query based on the outcomes of the data manipulation

   #Create impala connection named impala_con, currently this fails because I have no kerberos 
     ticket
   impala_data <- dbGetQuery(impala_con, query)
   
  #Do some further manipulations
  return(result)
  },
  packages = TRUE
)
`

Thanks for the help!
Hi, are there plans to include lift as part of ml_fpgrowth in Sparklyr?


Hi,

I'm not sure if this is already supported but I was not able to find it, therefore this feature request. It would be nice to define sliding windows on a stream. This seems to be possible when using pyspark:

```
# Group the data by window and word and compute the count of each group
windowedCounts = words.groupBy(
    window(words.timestamp, "10 minutes", "5 minutes"),
    words.word
).count()
```

I'm not aware of an equivalent of the "window" function within sparklyr. In the pyspark example the desired measure is calculated every 5 seconds over a sliding window of 10 seconds. 
When suppressing the intercept in `ml_linear_regression()` using the `fit_intercept = F` parameter, the returned object does not display the coefficients in the expected way. The coefficients are there in the model object, but it seems like one of the summarization steps is not behaving correctly, probably due to the null intercept. 

Setup 
``` r
library(dplyr)
#> 
#> Attaching package: 'dplyr'
#> The following objects are masked from 'package:stats':
#> 
#>     filter, lag
#> The following objects are masked from 'package:base':
#> 
#>     intersect, setdiff, setequal, union
library(sparklyr)

spark_version <- "2.4.3"

sc <- spark_connect(master = 'local',
                    version = spark_version)

# copy mtcars into spark
mtcars_tbl <- copy_to(sc, 
                      df = mtcars, 
                      overwrite = TRUE)
```

This works as expected:
```r
# fit LM with intercept 
fit_with_intercept <- mtcars_tbl %>%
  ml_linear_regression(mpg ~ wt + cyl, 
                       fit_intercept = TRUE) # param set to TRUE (default)

# Works as expected: 
fit_with_intercept %>% summary()
#> Deviance Residuals:
#>     Min      1Q  Median      3Q     Max 
#> -4.2893 -1.5512 -0.4684  1.5743  6.1004 
#> 
#> Coefficients:
#> (Intercept)          wt         cyl 
#>   39.686261   -3.190972   -1.507795 
#> 
#> R-Squared: 0.8302
#> Root Mean Squared Error: 2.444
```

This does not work correctly. Should return `NULL` for the coefficients
```r
# fit LM without intercept 
fit_without_intercept <- mtcars_tbl %>%
  ml_linear_regression(mpg ~ wt + cyl,
                       fit_intercept = FALSE) # param set to FALSE 

# Does not return coefficients 
fit_without_intercept %>% summary()
#> Deviance Residuals:
#>     Min      1Q  Median      3Q     Max 
#> -13.466  -6.181   1.476  10.597  22.996 
#> 
#> Coefficients:
#> NULL
#> 
#> R-Squared: 0.735
#> Root Mean Squared Error: 10.78

# Coefficients seem to be missing in fit object 
fit_without_intercept$coefficients
#> NULL
```
Coefficients are there, just not pulled into the summary part of the fit object. 
```r
# But coefficients do exist in the model object within the fit object
fit_without_intercept$model$coefficients
#> [1] 1.173788 2.187405

# The coefficients in the model object appear to be the correct coefs 
#   when compared to the regular, in-memory, lm-fit. 
lm(data = mtcars, 
   formula = mpg ~ -1 + wt + cyl) %>% 
  coef()
#>       wt      cyl 
#> 1.173788 2.187405
```

<sup>Created on 2019-12-26 by the [reprex package](https://reprex.tidyverse.org) (v0.3.0)</sup>

<details>

<summary>Session info</summary>

``` r
devtools::session_info()
#> ─ Session info ───────────────────────────────────────────────────────────────
#>  setting  value                       
#>  version  R version 3.6.0 (2019-04-26)
#>  os       Red Hat Enterprise Linux    
#>  system   x86_64, linux-gnu           
#>  ui       X11                         
#>  language (EN)                        
#>  collate  en_US.UTF-8                 
#>  ctype    en_US.UTF-8                 
#>  tz       America/Chicago             
#>  date     2019-12-26                  
#> 
#> ─ Packages ───────────────────────────────────────────────────────────────────
#>  package     * version date       lib source        
#>  askpass       1.1     2019-01-13 [1] CRAN (R 3.6.0)
#>  assertthat    0.2.1   2019-03-21 [1] CRAN (R 3.6.0)
#>  backports     1.1.5   2019-10-02 [1] CRAN (R 3.6.0)
#>  base64enc     0.1-3   2015-07-28 [1] CRAN (R 3.6.0)
#>  callr         3.4.0   2019-12-09 [1] CRAN (R 3.6.0)
#>  cli           2.0.0   2019-12-09 [1] CRAN (R 3.6.0)
#>  config        0.3     2018-03-27 [1] CRAN (R 3.6.0)
#>  crayon        1.3.4   2017-09-16 [1] CRAN (R 3.6.0)
#>  DBI           1.1.0   2019-12-15 [1] CRAN (R 3.6.0)
#>  dbplyr        1.4.2   2019-06-17 [1] CRAN (R 3.6.0)
#>  desc          1.2.0   2018-05-01 [1] CRAN (R 3.6.0)
#>  devtools      2.2.1   2019-09-24 [1] CRAN (R 3.6.0)
#>  digest        0.6.23  2019-11-23 [1] CRAN (R 3.6.0)
#>  dplyr       * 0.8.3   2019-07-04 [1] CRAN (R 3.6.0)
#>  ellipsis      0.3.0   2019-09-20 [1] CRAN (R 3.6.0)
#>  evaluate      0.14    2019-05-28 [1] CRAN (R 3.6.0)
#>  fansi         0.4.0   2018-10-05 [1] CRAN (R 3.6.0)
#>  forge         0.2.0   2019-02-26 [1] CRAN (R 3.6.0)
#>  fs            1.3.1   2019-05-06 [1] CRAN (R 3.6.0)
#>  generics      0.0.2   2018-11-29 [1] CRAN (R 3.6.0)
#>  glue          1.3.1   2019-03-12 [1] CRAN (R 3.6.0)
#>  highr         0.8     2019-03-20 [1] CRAN (R 3.6.0)
#>  htmltools     0.4.0   2019-10-04 [1] CRAN (R 3.6.0)
#>  htmlwidgets   1.5.1   2019-10-08 [1] CRAN (R 3.6.0)
#>  httr          1.4.1   2019-08-05 [1] CRAN (R 3.6.0)
#>  jsonlite      1.6     2018-12-07 [1] CRAN (R 3.6.0)
#>  knitr         1.26    2019-11-12 [1] CRAN (R 3.6.0)
#>  magrittr      1.5     2014-11-22 [1] CRAN (R 3.6.0)
#>  memoise       1.1.0   2017-04-21 [1] CRAN (R 3.6.0)
#>  openssl       1.4.1   2019-07-18 [1] CRAN (R 3.6.0)
#>  pillar        1.4.3   2019-12-20 [1] CRAN (R 3.6.0)
#>  pkgbuild      1.0.6   2019-10-09 [1] CRAN (R 3.6.0)
#>  pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 3.6.0)
#>  pkgload       1.0.2   2018-10-29 [1] CRAN (R 3.6.0)
#>  prettyunits   1.0.2   2015-07-13 [1] CRAN (R 3.6.0)
#>  processx      3.4.1   2019-07-18 [1] CRAN (R 3.6.0)
#>  ps            1.3.0   2018-12-21 [1] CRAN (R 3.6.0)
#>  purrr         0.3.3   2019-10-18 [1] CRAN (R 3.6.0)
#>  r2d3          0.2.3   2018-12-18 [1] CRAN (R 3.6.0)
#>  R6            2.4.1   2019-11-12 [1] CRAN (R 3.6.0)
#>  rappdirs      0.3.1   2016-03-28 [1] CRAN (R 3.6.0)
#>  Rcpp          1.0.3   2019-11-08 [1] CRAN (R 3.6.0)
#>  remotes       2.1.0   2019-06-24 [1] CRAN (R 3.6.0)
#>  rlang         0.4.2   2019-11-23 [1] CRAN (R 3.6.0)
#>  rmarkdown     2.0     2019-12-12 [1] CRAN (R 3.6.0)
#>  rprojroot     1.3-2   2018-01-03 [1] CRAN (R 3.6.0)
#>  rstudioapi    0.10    2019-03-19 [1] CRAN (R 3.6.0)
#>  sessioninfo   1.1.1   2018-11-05 [1] CRAN (R 3.6.0)
#>  sparklyr    * 1.0.5   2019-11-14 [1] CRAN (R 3.6.0)
#>  stringi       1.4.3   2019-03-12 [1] CRAN (R 3.6.0)
#>  stringr       1.4.0   2019-02-10 [1] CRAN (R 3.6.0)
#>  testthat      2.3.1   2019-12-01 [1] CRAN (R 3.6.0)
#>  tibble        2.1.3   2019-06-06 [1] CRAN (R 3.6.0)
#>  tidyselect    0.2.5   2018-10-11 [1] CRAN (R 3.6.0)
#>  usethis       1.5.1   2019-07-04 [1] CRAN (R 3.6.0)
#>  withr         2.1.2   2018-03-15 [1] CRAN (R 3.6.0)
#>  xfun          0.11    2019-11-12 [1] CRAN (R 3.6.0)
#>  yaml          2.2.0   2018-07-25 [1] CRAN (R 3.6.0)
#> 
#> [1] /opt/nfs01/rlibs-readonly/bleed
#> [2] /home/NCIWIN/twells/R/library
#> [3] /usr/lib64/R/library
#> [4] /usr/share/R/library
```

</details>

_First time submitting an github issue. Please let me know if I've botched anything! :)_ 

Hello,

Quick question,

I got these parameters in my random forest in caret:

```
grid<-expand.grid(mtry= c(69), splitrule= c("gini"), min.node.size=1)

RFFirstNorm <- train(Is_Commercial~.,  data = train, method = "ranger",
                   trControl = cv.cntrl,tuneGrid=grid, num.trees = 1500,
                   importance = "permutation")
```

How can I translate these parameter to the random forest in Sparklyr?

And another question will be:


min_instances_per_node sets the depth in the tree as same as min.node.size in R random forest?

Why max.depth in sparklyr random forest does not allow me to put values >30?

Thank you 

Regards


I am fetching twieets from twitter api and loading the data into hive using sparklyr, Problem I am facing with arabic text. In hive text are displaying in non readable format for arabic text.
eg: @TomeiTech @HCT_UAE \u0627\u0644\u0644\u0647 \u064a\u0628\u0627\u0627\u0627\u0627\u0631\u0643 \u0641\u064a\u064a\u064a\u064a\u062c \U0001f62d\u2764\ufe0f\u2764\ufe0f

So I added "Sys.setlocale("LC_ALL", "en_US.UTF-8")" In my R scripts now it displaying arabic text from r cmd when I print:"@TomeiTech @HCT_UAE الله يباااارك فييييج \U0001f62d❤️❤️"

But loading data into sparklyr using copy_to gives this error from R CMD:
<simpleError: java.nio.charset.MalformedInputException: Input length = 1

I am surprised same code works with Rstudio perfectly, But I need to schedule the r script from Rcmd. Please help me fixing this issue here is my working code with Rstudio but not working with R cmd:
library(sparklyr)
library(rtweet)
## name assigned to created app
appname <- "RtSentiment"
## api key (example below is not a real key)
key <- "xxxxxx"
secret <- "xxxxxx"
access_token <- "xxxxxxxx"
access_secret <- "xxxxxxx"
twitter_token <-create_token(
  app = appname,
  consumer_key = key,
  access_token = access_token,
  access_secret = access_secret,
  consumer_secret = secret)
Sys.setlocale("LC_ALL", "en_US.UTF-8")
hct_hnd_twt <- search_tweets(q = "HCT_UAE", n=3,type = "recent",until=Sys.Date())
##Use spark to load data in hive
Sys.setenv(SPARK_HOME = "/opt/cloudera/parcels/CDH/lib/spark")
system("export KRB5CCNAME=/tmp/krb5cc_infabdm_twitter")
system("kinit -p infabdm@AD.HCT.AC.AE -k -t /home/ruser/infabdm.keytab")
##Spark R config
config <- spark_config()
config$spark.executor.cores <- 10
config$spark.executor.memory <- "16G"
sc <- spark_connect(master = "yarn-client", config = config, version = '1.6.0')
hct_hnd_twt <- copy_to(sc, hct_hnd_twt, "hct_hnd_twt",overwrite = TRUE)
DBI::dbGetQuery(sc, "create table r_social_media.hct_handle_tweets_d STORED AS PARQUET as SELECT * FROM hct_hnd_twt")
