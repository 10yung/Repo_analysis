Running prometheus on some unstable environment we want to reduce flushing intervals, doing it with smaller chunks to prevent possible data loss like was mentioned in #2196 comments. For that case would be good to have possibility for tweaking `--storage.tsdb.min-block-duration` and `--storage.tsdb.max-block-duration` from prometheus-operator where not using Thanos compaction.
As discussed in https://github.com/coreos/prometheus-operator/pull/2938
the binary name `lint` is too generic if used outside of the docker
image. So it's been renamed to `po-lint`.
I am getting KubeDeploymentReplicasMismatch alert for almost all deployments. I think their is something wrong with kube-state-metrics.

```
[FIRING:7] Prometheus Event Notification
Alert:  - critical
 Description: 
 Details:
   - alertname: KubeDeploymentReplicasMismatch
   - deployment: aws-alb-ingress-controller (I am getting for other deployments as well)
   - endpoint: http
   - instance: 172.17.23.88:8080
   - job: kube-state-metrics
   - namespace: kube-system
   - pod:prometheus-operator-kube-state-metrics-7f5bff855d-pjvzk
   - prometheus: monitoring/prometheus-operato-prometheus
   - service:prometheus-operator-kube-state-metrics
   - severity: critical
```

Is anyone facing the same issue?
<!--

Feel free to ask questions in #prometheus-operator on Kubernetes Slack!

-->

**What happened?**

Prometheus rule validator allowed Prometheus rule with an incorrect label type to pass the validating webhook validation, this caused prometheus operator failing to start after the pod bounced.

I believe the testSeverity: off is causing the type not to be set correctly as a string. It came through as a boolean and should have been a string. We have seen similar behavior for numbers as well. 

**Did you expect to see some different?**

The prometheus rule with invalid type for the label should have been rejected by rule validator. 

**How to reproduce it (as minimally and precisely as possible)**:
Make a prometheus rule with an alert that has labels with non string values.

Alert that caused this example:

```
- alert: ‘testalert’
        annotations:
          description: This is an alert description
          summary: '{{ $labels.type }} has an error’
        expr: (sum by (type) (increase(metric{type=~”myservice(Carrier|Device)"}[5m]))
          < 1)
        labels:
          env: TEST
          prodSeverity: warning
          team: service-team
          testSeverity: off
```

**Environment**

Openshift 3.11

* Prometheus Operator version:

prometheus-operator:v0.34.0

* Kubernetes version information:

    v1.10.0+b81c8f8

* Kubernetes cluster kind:

    Openshift

* Manifests:

```
insert manifests relevant to the issue
```

* Prometheus Operator Logs:

```
E0115 22:42:33.889919       1 reflector.go:123] github.com/coreos/prometheus-operator/pkg/prometheus/operator.go:467: Failed to list *v1.PrometheusRule: v1.PrometheusRuleList.Items: []*v1.PrometheusRule: v1.PrometheusRule.Spec: v1.PrometheusRuleSpec.Groups: []v1.RuleGroup: v1.RuleGroup.Rules: []v1.Rule: v1.Rule.Labels: ReadString: expects " or n, but found f, error found in #10 byte of ...|everity":off}},{"a|..., bigger context ...|,"team":"service-team","testSeverity":off}},{"alert":"testalert","annota|...
```

**Anything else we need to know?**:

Fix #2960 

Fix broken links and remove spec.version in examples of Exposing Prometheus and Alertmanager documentation
<!--

Feel free to ask questions in #prometheus-operator on Kubernetes Slack!

-->

**What happened?**
Read the documentation :
https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/exposing-prometheus-and-alertmanager.md
Documentation have broken links, spec.version are not up to date.
NGINX Ingress Controller link is broken, NGINX Ingress Controller quick-start is out of date

**Did you expect to see some different?**
A up to date documentation.

**How to reproduce it (as minimally and precisely as possible)**:
NA

**Environment**
NA

Currently when passing `--config-reloader-(cpu|memory)` values those are
used only for limits. This PR sets the same values for requests.
I wasn't sure if the options for the requests should have been passed
with separate options but I thought this was a good compromise in the
meantime.

The initial question was brought up in https://github.com/coreos/kube-prometheus/issues/376
**What is missing?**
The path to alert manager config file is hardcoded (`/etc/alertmanager/config/alertmanager.yaml`), and by default there is a volume mount to `/etc/alertmamager/config`, making it impossible to mount something else to this directory.

**Why do we need it?**
I wanted to use [Kamus](https://kamus.soluto.io) for creating a template configuration file, with a place holder for encrypted data, something like that:
```yaml
receivers:
- name: team-devops-low
  pagerduty_configs:
  - service_key: <%- secrets["pd-team-devops-low"] %>
route:
  group_by:
  - namespace
  group_interval: 5m
  group_wait: 30s
  receiver: team-devops-low
  repeat_interval: 12h
  routes:
  - match:
      namespace: default
    receiver: team-devops-low
```

Everything works, the config file is generated correctly and the place-holder is replaced with the decrypted value - but I can't mount the config file into alert manager.

**Environment**
Kubernetes 1.14
* Prometheus Operator version:

    `quay.io/coreos/prometheus-operator:v0.34.0'

**Anything else we need to know?**:

<!--

Feel free to ask questions in #prometheus-operator on Kubernetes Slack!

-->

**What happened?**
Created a Prometheus object (kind: Prometheus).  The operator creates the StatefulSet and associated config objects and service.  The Prometheus pods associated with the StatefulSet enter a loop where they constantly terminate and restart.

**Did you expect to see some different?**
Expected the Prometheus pods to start successfully and not continually terminate / restart.

**How to reproduce it (as minimally and precisely as possible)**:
Apply the below manifest to the Kubernetes cluster.

**Environment**

* Prometheus Operator version:

``` quay.io/coreos/prometheus-operator:v0.34.0 ```

* Kubernetes version information:

```
Client Version: version.Info{Major:"1", Minor:"17", GitVersion:"v1.17.0", GitCommit:"70132b0f130acc0bed193d9ba59dd186f0e634cf", GitTreeState:"clean", BuildDate:"2019-12-07T21:20:10Z", GoVersion:"go1.13.4", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"17", GitVersion:"v1.17.0", GitCommit:"70132b0f130acc0bed193d9ba59dd186f0e634cf", GitTreeState:"clean", BuildDate:"2019-12-07T21:12:17Z", GoVersion:"go1.13.4", Compiler:"gc", Platform:"linux/amd64"}
```


* Kubernetes cluster kind:

Kubeadm on vSphere ESXi 6.5

* Manifests:

```
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  labels:
    prometheus: k8s
  name: k8s
  namespace: monitoring
spec:
  alerting:
    alertmanagers:
    - name: alertmanager-main
      namespace: monitoring
      port: web
  baseImage: quay.io/prometheus/prometheus
  externalUrl: https://admin.k8s.dev.local/prometheus
  nodeSelector:
    kubernetes.io/os: linux
  podMonitorNamespaceSelector: {}
  podMonitorSelector: {}
  replicas: 2
  resources:
    requests:
      memory: 400Mi
  retention: 30d
  ruleSelector:
    matchLabels:
      prometheus: k8s
      role: alert-rules
  securityContext:
    fsGroup: 2000
    runAsNonRoot: true
    runAsUser: 1000
  serviceAccountName: prometheus-k8s
  serviceMonitorNamespaceSelector: {}
  serviceMonitorSelector: {}
  storage:
    volumeClaimTemplate:
      apiVersion: v1
      kind: PersistentVolumeClaim
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 20Gi
        storageClassName: vsphere-thin
  version: v2.11.0
```

* Prometheus Operator Logs:

```
level=info ts=2020-01-14T17:43:17.016002796Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:43:17.262289325Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:43:17.444519451Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:43:17.621130069Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:43:17.871952399Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:43:18.033185539Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:43:18.211076472Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:43:18.35874237Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:43:27.603057504Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:43:31.341937693Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:43:31.439809231Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:43:31.645902867Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:43:31.840210726Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:43:37.598384666Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:43:41.360857862Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:43:41.676053011Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:43:41.742898031Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:43:41.937110497Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:43:42.319400902Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:43:47.599655017Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:43:51.362766392Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:43:51.449540062Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:43:51.656142501Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:43:51.842817304Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:43:57.604003655Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:44:01.34536578Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:44:01.446015198Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:44:01.786057884Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:44:01.933626666Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:44:07.597842356Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:44:11.378206116Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:44:11.702226655Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
level=info ts=2020-01-14T17:44:11.811890136Z caller=operator.go:1072 component=prometheusoperator msg="sync prometheus" key=monitoring/k8s
```

**Anything else we need to know?**:

Note that the above logs are the only thing that appears in the logfile after logging the creation of the CRD.

I tried copying the manifests created by the operator for the StatefulSet, Service, Configmap, and Secrets and applying them manually (after removing the ownerReferences) and everything worked.  This leads me to believe the issue is with the prometheus-operator somehow.

These are the kubelet logs when running under the prometheus-operator:

```
I0114 12:43:18.185715   12575 reconciler.go:209] operationExecutor.VerifyControllerAttachedVolume started for volume "config" (UniqueName: "kubernetes.io/secret/e678470c-b85d-42a1-b960-0dcddce327e8-config") pod "prometheus-k8s-0" (UID: "e678470c-b85d-42a1-b960-0dcddce327e8")
I0114 12:43:18.185805   12575 reconciler.go:209] operationExecutor.VerifyControllerAttachedVolume started for volume "config-out" (UniqueName: "kubernetes.io/empty-dir/e678470c-b85d-42a1-b960-0dcddce327e8-config-out") pod "prometheus-k8s-0" (UID: "e678470c-b85d-42a1-b960-0dcddce327e8")
I0114 12:43:18.185846   12575 reconciler.go:209] operationExecutor.VerifyControllerAttachedVolume started for volume "prometheus-k8s-token-8qc8j" (UniqueName: "kubernetes.io/secret/e678470c-b85d-42a1-b960-0dcddce327e8-prometheus-k8s-token-8qc8j") pod "prometheus-k8s-0" (UID: "e678470c-b85d-42a1-b960-0dcddce327e8")
I0114 12:43:18.185908   12575 reconciler.go:209] operationExecutor.VerifyControllerAttachedVolume started for volume "pvc-e1af263c-ae5a-4428-a22d-f4b2004ad1fd" (UniqueName: "kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-e1af263c-ae5a-4428-a22d-f4b2004ad1fd.vmdk") pod "prometheus-k8s-0" (UID: "e678470c-b85d-42a1-b960-0dcddce327e8")
I0114 12:43:18.185948   12575 reconciler.go:209] operationExecutor.VerifyControllerAttachedVolume started for volume "prometheus-k8s-rulefiles-0" (UniqueName: "kubernetes.io/configmap/e678470c-b85d-42a1-b960-0dcddce327e8-prometheus-k8s-rulefiles-0") pod "prometheus-k8s-0" (UID: "e678470c-b85d-42a1-b960-0dcddce327e8")
I0114 12:43:18.185983   12575 reconciler.go:209] operationExecutor.VerifyControllerAttachedVolume started for volume "tls-assets" (UniqueName: "kubernetes.io/secret/e678470c-b85d-42a1-b960-0dcddce327e8-tls-assets") pod "prometheus-k8s-0" (UID: "e678470c-b85d-42a1-b960-0dcddce327e8")
E0114 12:43:18.186100   12575 nestedpendingoperations.go:270] Operation for "\"kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-e1af263c-ae5a-4428-a22d-f4b2004ad1fd.vmdk\"" failed. No retries permitted until 2020-01-14 12:43:18.686067805 -0500 EST m=+6901.876606963 (durationBeforeRetry 500ms). Error: "Volume has not been added to the list of VolumesInUse in the node's volume status for volume \"pvc-e1af263c-ae5a-4428-a22d-f4b2004ad1fd\" (UniqueName: \"kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-e1af263c-ae5a-4428-a22d-f4b2004ad1fd.vmdk\") pod \"prometheus-k8s-0\" (UID: \"e678470c-b85d-42a1-b960-0dcddce327e8\") "
W0114 12:43:18.286723   12575 volume_linux.go:45] Setting volume ownership for /var/lib/kubelet/pods/e678470c-b85d-42a1-b960-0dcddce327e8/volumes/kubernetes.io~empty-dir/config-out and fsGroup set. If the volume has a lot of files then setting volume ownership could be slow, see https://github.com/kubernetes/kubernetes/issues/69699
W0114 12:43:18.287949   12575 volume_linux.go:45] Setting volume ownership for /var/lib/kubelet/pods/e678470c-b85d-42a1-b960-0dcddce327e8/volumes/kubernetes.io~empty-dir/wrapped_prometheus-k8s-rulefiles-0 and fsGroup set. If the volume has a lot of files then setting volume ownership could be slow, see https://github.com/kubernetes/kubernetes/issues/69699
W0114 12:43:18.288569   12575 volume_linux.go:45] Setting volume ownership for /var/lib/kubelet/pods/e678470c-b85d-42a1-b960-0dcddce327e8/volumes/kubernetes.io~configmap/prometheus-k8s-rulefiles-0 and fsGroup set. If the volume has a lot of files then setting volume ownership could be slow, see https://github.com/kubernetes/kubernetes/issues/69699
W0114 12:43:18.308974   12575 volume_linux.go:45] Setting volume ownership for /var/lib/kubelet/pods/e678470c-b85d-42a1-b960-0dcddce327e8/volumes/kubernetes.io~empty-dir/wrapped_prometheus-k8s-token-8qc8j and fsGroup set. If the volume has a lot of files then setting volume ownership could be slow, see https://github.com/kubernetes/kubernetes/issues/69699
W0114 12:43:18.309660   12575 volume_linux.go:45] Setting volume ownership for /var/lib/kubelet/pods/e678470c-b85d-42a1-b960-0dcddce327e8/volumes/kubernetes.io~secret/prometheus-k8s-token-8qc8j and fsGroup set. If the volume has a lot of files then setting volume ownership could be slow, see https://github.com/kubernetes/kubernetes/issues/69699
W0114 12:43:18.314339   12575 volume_linux.go:45] Setting volume ownership for /var/lib/kubelet/pods/e678470c-b85d-42a1-b960-0dcddce327e8/volumes/kubernetes.io~empty-dir/wrapped_config and fsGroup set. If the volume has a lot of files then setting volume ownership could be slow, see https://github.com/kubernetes/kubernetes/issues/69699
W0114 12:43:18.314751   12575 volume_linux.go:45] Setting volume ownership for /var/lib/kubelet/pods/e678470c-b85d-42a1-b960-0dcddce327e8/volumes/kubernetes.io~secret/config and fsGroup set. If the volume has a lot of files then setting volume ownership could be slow, see https://github.com/kubernetes/kubernetes/issues/69699
W0114 12:43:18.330381   12575 volume_linux.go:45] Setting volume ownership for /var/lib/kubelet/pods/e678470c-b85d-42a1-b960-0dcddce327e8/volumes/kubernetes.io~empty-dir/wrapped_tls-assets and fsGroup set. If the volume has a lot of files then setting volume ownership could be slow, see https://github.com/kubernetes/kubernetes/issues/69699
W0114 12:43:18.330742   12575 volume_linux.go:45] Setting volume ownership for /var/lib/kubelet/pods/e678470c-b85d-42a1-b960-0dcddce327e8/volumes/kubernetes.io~secret/tls-assets and fsGroup set. If the volume has a lot of files then setting volume ownership could be slow, see https://github.com/kubernetes/kubernetes/issues/69699
I0114 12:43:18.386406   12575 reconciler.go:183] operationExecutor.UnmountVolume started for volume "prometheus-k8s-rulefiles-0" (UniqueName: "kubernetes.io/configmap/e678470c-b85d-42a1-b960-0dcddce327e8-prometheus-k8s-rulefiles-0") pod "e678470c-b85d-42a1-b960-0dcddce327e8" (UID: "e678470c-b85d-42a1-b960-0dcddce327e8")
I0114 12:43:18.386458   12575 reconciler.go:183] operationExecutor.UnmountVolume started for volume "config" (UniqueName: "kubernetes.io/secret/e678470c-b85d-42a1-b960-0dcddce327e8-config") pod "e678470c-b85d-42a1-b960-0dcddce327e8" (UID: "e678470c-b85d-42a1-b960-0dcddce327e8")
I0114 12:43:18.386498   12575 reconciler.go:183] operationExecutor.UnmountVolume started for volume "config-out" (UniqueName: "kubernetes.io/empty-dir/e678470c-b85d-42a1-b960-0dcddce327e8-config-out") pod "e678470c-b85d-42a1-b960-0dcddce327e8" (UID: "e678470c-b85d-42a1-b960-0dcddce327e8")
I0114 12:43:18.386537   12575 reconciler.go:183] operationExecutor.UnmountVolume started for volume "tls-assets" (UniqueName: "kubernetes.io/secret/e678470c-b85d-42a1-b960-0dcddce327e8-tls-assets") pod "e678470c-b85d-42a1-b960-0dcddce327e8" (UID: "e678470c-b85d-42a1-b960-0dcddce327e8")
I0114 12:43:18.386576   12575 reconciler.go:183] operationExecutor.UnmountVolume started for volume "prometheus-k8s-token-8qc8j" (UniqueName: "kubernetes.io/secret/e678470c-b85d-42a1-b960-0dcddce327e8-prometheus-k8s-token-8qc8j") pod "e678470c-b85d-42a1-b960-0dcddce327e8" (UID: "e678470c-b85d-42a1-b960-0dcddce327e8")
W0114 12:43:18.387181   12575 empty_dir.go:418] Warning: Failed to clear quota on /var/lib/kubelet/pods/e678470c-b85d-42a1-b960-0dcddce327e8/volumes/kubernetes.io~configmap/prometheus-k8s-rulefiles-0: ClearQuota called, but quotas disabled
I0114 12:43:18.387533   12575 operation_generator.go:713] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/e678470c-b85d-42a1-b960-0dcddce327e8-prometheus-k8s-rulefiles-0" (OuterVolumeSpecName: "prometheus-k8s-rulefiles-0") pod "e678470c-b85d-42a1-b960-0dcddce327e8" (UID: "e678470c-b85d-42a1-b960-0dcddce327e8"). InnerVolumeSpecName "prometheus-k8s-rulefiles-0". PluginName "kubernetes.io/configmap", VolumeGidValue ""
W0114 12:43:18.387624   12575 empty_dir.go:418] Warning: Failed to clear quota on /var/lib/kubelet/pods/e678470c-b85d-42a1-b960-0dcddce327e8/volumes/kubernetes.io~empty-dir/config-out: ClearQuota called, but quotas disabled
I0114 12:43:18.387677   12575 operation_generator.go:713] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/e678470c-b85d-42a1-b960-0dcddce327e8-config-out" (OuterVolumeSpecName: "config-out") pod "e678470c-b85d-42a1-b960-0dcddce327e8" (UID: "e678470c-b85d-42a1-b960-0dcddce327e8"). InnerVolumeSpecName "config-out". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
I0114 12:43:18.397455   12575 operation_generator.go:713] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/e678470c-b85d-42a1-b960-0dcddce327e8-prometheus-k8s-token-8qc8j" (OuterVolumeSpecName: "prometheus-k8s-token-8qc8j") pod "e678470c-b85d-42a1-b960-0dcddce327e8" (UID: "e678470c-b85d-42a1-b960-0dcddce327e8"). InnerVolumeSpecName "prometheus-k8s-token-8qc8j". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0114 12:43:18.399246   12575 operation_generator.go:713] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/e678470c-b85d-42a1-b960-0dcddce327e8-config" (OuterVolumeSpecName: "config") pod "e678470c-b85d-42a1-b960-0dcddce327e8" (UID: "e678470c-b85d-42a1-b960-0dcddce327e8"). InnerVolumeSpecName "config". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0114 12:43:18.401201   12575 operation_generator.go:713] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/e678470c-b85d-42a1-b960-0dcddce327e8-tls-assets" (OuterVolumeSpecName: "tls-assets") pod "e678470c-b85d-42a1-b960-0dcddce327e8" (UID: "e678470c-b85d-42a1-b960-0dcddce327e8"). InnerVolumeSpecName "tls-assets". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0114 12:43:18.487014   12575 reconciler.go:303] Volume detached for volume "prometheus-k8s-rulefiles-0" (UniqueName: "kubernetes.io/configmap/e678470c-b85d-42a1-b960-0dcddce327e8-prometheus-k8s-rulefiles-0") on node "jdc-ddt-kn06.dev.dtec.int" DevicePath ""
I0114 12:43:18.487052   12575 reconciler.go:303] Volume detached for volume "config" (UniqueName: "kubernetes.io/secret/e678470c-b85d-42a1-b960-0dcddce327e8-config") on node "jdc-ddt-kn06.dev.dtec.int" DevicePath ""
I0114 12:43:18.487070   12575 reconciler.go:303] Volume detached for volume "config-out" (UniqueName: "kubernetes.io/empty-dir/e678470c-b85d-42a1-b960-0dcddce327e8-config-out") on node "jdc-ddt-kn06.dev.dtec.int" DevicePath ""
I0114 12:43:18.487086   12575 reconciler.go:303] Volume detached for volume "tls-assets" (UniqueName: "kubernetes.io/secret/e678470c-b85d-42a1-b960-0dcddce327e8-tls-assets") on node "jdc-ddt-kn06.dev.dtec.int" DevicePath ""
I0114 12:43:18.487102   12575 reconciler.go:303] Volume detached for volume "prometheus-k8s-token-8qc8j" (UniqueName: "kubernetes.io/secret/e678470c-b85d-42a1-b960-0dcddce327e8-prometheus-k8s-token-8qc8j") on node "jdc-ddt-kn06.dev.dtec.int" DevicePath ""
I0114 12:43:18.687934   12575 reconciler.go:209] operationExecutor.VerifyControllerAttachedVolume started for volume "pvc-e1af263c-ae5a-4428-a22d-f4b2004ad1fd" (UniqueName: "kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-e1af263c-ae5a-4428-a22d-f4b2004ad1fd.vmdk") pod "prometheus-k8s-0" (UID: "e678470c-b85d-42a1-b960-0dcddce327e8")
E0114 12:43:18.688033   12575 nestedpendingoperations.go:270] Operation for "\"kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-e1af263c-ae5a-4428-a22d-f4b2004ad1fd.vmdk\"" failed. No retries permitted until 2020-01-14 12:43:19.68800035 -0500 EST m=+6902.878539519 (durationBeforeRetry 1s). Error: "Volume has not been added to the list of VolumesInUse in the node's volume status for volume \"pvc-e1af263c-ae5a-4428-a22d-f4b2004ad1fd\" (UniqueName: \"kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-e1af263c-ae5a-4428-a22d-f4b2004ad1fd.vmdk\") pod \"prometheus-k8s-0\" (UID: \"e678470c-b85d-42a1-b960-0dcddce327e8\") "
I0114 12:43:19.690818   12575 reconciler.go:209] operationExecutor.VerifyControllerAttachedVolume started for volume "pvc-e1af263c-ae5a-4428-a22d-f4b2004ad1fd" (UniqueName: "kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-e1af263c-ae5a-4428-a22d-f4b2004ad1fd.vmdk") pod "prometheus-k8s-0" (UID: "e678470c-b85d-42a1-b960-0dcddce327e8")
E0114 12:43:19.690935   12575 nestedpendingoperations.go:270] Operation for "\"kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-e1af263c-ae5a-4428-a22d-f4b2004ad1fd.vmdk\"" failed. No retries permitted until 2020-01-14 12:43:21.690902199 -0500 EST m=+6904.881441377 (durationBeforeRetry 2s). Error: "Volume has not been added to the list of VolumesInUse in the node's volume status for volume \"pvc-e1af263c-ae5a-4428-a22d-f4b2004ad1fd\" (UniqueName: \"kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-e1af263c-ae5a-4428-a22d-f4b2004ad1fd.vmdk\") pod \"prometheus-k8s-0\" (UID: \"e678470c-b85d-42a1-b960-0dcddce327e8\") "
I0114 12:43:21.697484   12575 reconciler.go:209] operationExecutor.VerifyControllerAttachedVolume started for volume "pvc-e1af263c-ae5a-4428-a22d-f4b2004ad1fd" (UniqueName: "kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-e1af263c-ae5a-4428-a22d-f4b2004ad1fd.vmdk") pod "prometheus-k8s-0" (UID: "e678470c-b85d-42a1-b960-0dcddce327e8")
E0114 12:43:21.697640   12575 nestedpendingoperations.go:270] Operation for "\"kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-e1af263c-ae5a-4428-a22d-f4b2004ad1fd.vmdk\"" failed. No retries permitted until 2020-01-14 12:43:25.697607023 -0500 EST m=+6908.888146181 (durationBeforeRetry 4s). Error: "Volume has not been added to the list of VolumesInUse in the node's volume status for volume \"pvc-e1af263c-ae5a-4428-a22d-f4b2004ad1fd\" (UniqueName: \"kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-e1af263c-ae5a-4428-a22d-f4b2004ad1fd.vmdk\") pod \"prometheus-k8s-0\" (UID: \"e678470c-b85d-42a1-b960-0dcddce327e8\") "
I0114 12:43:25.712555   12575 reconciler.go:209] operationExecutor.VerifyControllerAttachedVolume started for volume "pvc-e1af263c-ae5a-4428-a22d-f4b2004ad1fd" (UniqueName: "kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-e1af263c-ae5a-4428-a22d-f4b2004ad1fd.vmdk") pod "prometheus-k8s-0" (UID: "e678470c-b85d-42a1-b960-0dcddce327e8")
E0114 12:43:25.712653   12575 nestedpendingoperations.go:270] Operation for "\"kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-e1af263c-ae5a-4428-a22d-f4b2004ad1fd.vmdk\"" failed. No retries permitted until 2020-01-14 12:43:33.712619263 -0500 EST m=+6916.903158429 (durationBeforeRetry 8s). Error: "Volume has not been added to the list of VolumesInUse in the node's volume status for volume \"pvc-e1af263c-ae5a-4428-a22d-f4b2004ad1fd\" (UniqueName: \"kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-e1af263c-ae5a-4428-a22d-f4b2004ad1fd.vmdk\") pod \"prometheus-k8s-0\" (UID: \"e678470c-b85d-42a1-b960-0dcddce327e8\") "
I0114 12:43:31.531252   12575 reconciler.go:209] operationExecutor.VerifyControllerAttachedVolume started for volume "config" (UniqueName: "kubernetes.io/secret/b99c0d59-1a3d-498b-8c77-83789f111466-config") pod "prometheus-k8s-0" (UID: "b99c0d59-1a3d-498b-8c77-83789f111466")
I0114 12:43:31.531302   12575 reconciler.go:209] operationExecutor.VerifyControllerAttachedVolume started for volume "tls-assets" (UniqueName: "kubernetes.io/secret/b99c0d59-1a3d-498b-8c77-83789f111466-tls-assets") pod "prometheus-k8s-0" (UID: "b99c0d59-1a3d-498b-8c77-83789f111466")
I0114 12:43:31.531335   12575 reconciler.go:209] operationExecutor.VerifyControllerAttachedVolume started for volume "config-out" (UniqueName: "kubernetes.io/empty-dir/b99c0d59-1a3d-498b-8c77-83789f111466-config-out") pod "prometheus-k8s-0" (UID: "b99c0d59-1a3d-498b-8c77-83789f111466")
I0114 12:43:31.531370   12575 reconciler.go:209] operationExecutor.VerifyControllerAttachedVolume started for volume "prometheus-k8s-rulefiles-0" (UniqueName: "kubernetes.io/configmap/b99c0d59-1a3d-498b-8c77-83789f111466-prometheus-k8s-rulefiles-0") pod "prometheus-k8s-0" (UID: "b99c0d59-1a3d-498b-8c77-83789f111466")
I0114 12:43:31.531410   12575 reconciler.go:209] operationExecutor.VerifyControllerAttachedVolume started for volume "prometheus-k8s-token-8qc8j" (UniqueName: "kubernetes.io/secret/b99c0d59-1a3d-498b-8c77-83789f111466-prometheus-k8s-token-8qc8j") pod "prometheus-k8s-0" (UID: "b99c0d59-1a3d-498b-8c77-83789f111466")
W0114 12:43:31.632186   12575 volume_linux.go:45] Setting volume ownership for /var/lib/kubelet/pods/b99c0d59-1a3d-498b-8c77-83789f111466/volumes/kubernetes.io~empty-dir/wrapped_prometheus-k8s-rulefiles-0 and fsGroup set. If the volume has a lot of files then setting volume ownership could be slow, see https://github.com/kubernetes/kubernetes/issues/69699
```

And these are the logs when not running under the prometheus-operator (for the exact same manifest of the StatefulSet minus the ownerReference):

```
I0114 12:51:21.500275   12575 reconciler.go:209] operationExecutor.VerifyControllerAttachedVolume started for volume "config" (UniqueName: "kubernetes.io/secret/634ec4bd-8776-4599-9ceb-bac83b567f08-config") pod "prometheus-k8s-0" (UID: "634ec4bd-8776-4599-9ceb-bac83b567f08")
I0114 12:51:21.500335   12575 reconciler.go:209] operationExecutor.VerifyControllerAttachedVolume started for volume "config-out" (UniqueName: "kubernetes.io/empty-dir/634ec4bd-8776-4599-9ceb-bac83b567f08-config-out") pod "prometheus-k8s-0" (UID: "634ec4bd-8776-4599-9ceb-bac83b567f08")
I0114 12:51:21.500376   12575 reconciler.go:209] operationExecutor.VerifyControllerAttachedVolume started for volume "prometheus-k8s-token-8qc8j" (UniqueName: "kubernetes.io/secret/634ec4bd-8776-4599-9ceb-bac83b567f08-prometheus-k8s-token-8qc8j") pod "prometheus-k8s-0" (UID: "634ec4bd-8776-4599-9ceb-bac83b567f08")
I0114 12:51:21.500480   12575 reconciler.go:209] operationExecutor.VerifyControllerAttachedVolume started for volume "tls-assets" (UniqueName: "kubernetes.io/secret/634ec4bd-8776-4599-9ceb-bac83b567f08-tls-assets") pod "prometheus-k8s-0" (UID: "634ec4bd-8776-4599-9ceb-bac83b567f08")
I0114 12:51:21.500530   12575 reconciler.go:209] operationExecutor.VerifyControllerAttachedVolume started for volume "prometheus-k8s-rulefiles-0" (UniqueName: "kubernetes.io/configmap/634ec4bd-8776-4599-9ceb-bac83b567f08-prometheus-k8s-rulefiles-0") pod "prometheus-k8s-0" (UID: "634ec4bd-8776-4599-9ceb-bac83b567f08")
I0114 12:51:21.500569   12575 reconciler.go:209] operationExecutor.VerifyControllerAttachedVolume started for volume "pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244" (UniqueName: "kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244.vmdk") pod "prometheus-k8s-0" (UID: "634ec4bd-8776-4599-9ceb-bac83b567f08")
E0114 12:51:21.500645   12575 nestedpendingoperations.go:270] Operation for "\"kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244.vmdk\"" failed. No retries permitted until 2020-01-14 12:51:22.00061511 -0500 EST m=+7385.191154267 (durationBeforeRetry 500ms). Error: "Volume has not been added to the list of VolumesInUse in the node's volume status for volume \"pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244\" (UniqueName: \"kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244.vmdk\") pod \"prometheus-k8s-0\" (UID: \"634ec4bd-8776-4599-9ceb-bac83b567f08\") "
W0114 12:51:21.601560   12575 volume_linux.go:45] Setting volume ownership for /var/lib/kubelet/pods/634ec4bd-8776-4599-9ceb-bac83b567f08/volumes/kubernetes.io~empty-dir/wrapped_prometheus-k8s-rulefiles-0 and fsGroup set. If the volume has a lot of files then setting volume ownership could be slow, see https://github.com/kubernetes/kubernetes/issues/69699
W0114 12:51:21.602381   12575 volume_linux.go:45] Setting volume ownership for /var/lib/kubelet/pods/634ec4bd-8776-4599-9ceb-bac83b567f08/volumes/kubernetes.io~configmap/prometheus-k8s-rulefiles-0 and fsGroup set. If the volume has a lot of files then setting volume ownership could be slow, see https://github.com/kubernetes/kubernetes/issues/69699
W0114 12:51:21.602834   12575 volume_linux.go:45] Setting volume ownership for /var/lib/kubelet/pods/634ec4bd-8776-4599-9ceb-bac83b567f08/volumes/kubernetes.io~empty-dir/config-out and fsGroup set. If the volume has a lot of files then setting volume ownership could be slow, see https://github.com/kubernetes/kubernetes/issues/69699
W0114 12:51:21.621834   12575 volume_linux.go:45] Setting volume ownership for /var/lib/kubelet/pods/634ec4bd-8776-4599-9ceb-bac83b567f08/volumes/kubernetes.io~empty-dir/wrapped_prometheus-k8s-token-8qc8j and fsGroup set. If the volume has a lot of files then setting volume ownership could be slow, see https://github.com/kubernetes/kubernetes/issues/69699
W0114 12:51:21.622533   12575 volume_linux.go:45] Setting volume ownership for /var/lib/kubelet/pods/634ec4bd-8776-4599-9ceb-bac83b567f08/volumes/kubernetes.io~secret/prometheus-k8s-token-8qc8j and fsGroup set. If the volume has a lot of files then setting volume ownership could be slow, see https://github.com/kubernetes/kubernetes/issues/69699
W0114 12:51:21.626828   12575 volume_linux.go:45] Setting volume ownership for /var/lib/kubelet/pods/634ec4bd-8776-4599-9ceb-bac83b567f08/volumes/kubernetes.io~empty-dir/wrapped_tls-assets and fsGroup set. If the volume has a lot of files then setting volume ownership could be slow, see https://github.com/kubernetes/kubernetes/issues/69699
W0114 12:51:21.627151   12575 volume_linux.go:45] Setting volume ownership for /var/lib/kubelet/pods/634ec4bd-8776-4599-9ceb-bac83b567f08/volumes/kubernetes.io~secret/tls-assets and fsGroup set. If the volume has a lot of files then setting volume ownership could be slow, see https://github.com/kubernetes/kubernetes/issues/69699
W0114 12:51:21.636102   12575 volume_linux.go:45] Setting volume ownership for /var/lib/kubelet/pods/634ec4bd-8776-4599-9ceb-bac83b567f08/volumes/kubernetes.io~empty-dir/wrapped_config and fsGroup set. If the volume has a lot of files then setting volume ownership could be slow, see https://github.com/kubernetes/kubernetes/issues/69699
W0114 12:51:21.636526   12575 volume_linux.go:45] Setting volume ownership for /var/lib/kubelet/pods/634ec4bd-8776-4599-9ceb-bac83b567f08/volumes/kubernetes.io~secret/config and fsGroup set. If the volume has a lot of files then setting volume ownership could be slow, see https://github.com/kubernetes/kubernetes/issues/69699
I0114 12:51:22.002094   12575 reconciler.go:209] operationExecutor.VerifyControllerAttachedVolume started for volume "pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244" (UniqueName: "kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244.vmdk") pod "prometheus-k8s-0" (UID: "634ec4bd-8776-4599-9ceb-bac83b567f08")
E0114 12:51:22.002211   12575 nestedpendingoperations.go:270] Operation for "\"kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244.vmdk\"" failed. No retries permitted until 2020-01-14 12:51:23.002175964 -0500 EST m=+7386.192715139 (durationBeforeRetry 1s). Error: "Volume has not been added to the list of VolumesInUse in the node's volume status for volume \"pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244\" (UniqueName: \"kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244.vmdk\") pod \"prometheus-k8s-0\" (UID: \"634ec4bd-8776-4599-9ceb-bac83b567f08\") "
I0114 12:51:23.005462   12575 reconciler.go:209] operationExecutor.VerifyControllerAttachedVolume started for volume "pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244" (UniqueName: "kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244.vmdk") pod "prometheus-k8s-0" (UID: "634ec4bd-8776-4599-9ceb-bac83b567f08")
E0114 12:51:23.005581   12575 nestedpendingoperations.go:270] Operation for "\"kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244.vmdk\"" failed. No retries permitted until 2020-01-14 12:51:25.005542981 -0500 EST m=+7388.196082153 (durationBeforeRetry 2s). Error: "Volume has not been added to the list of VolumesInUse in the node's volume status for volume \"pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244\" (UniqueName: \"kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244.vmdk\") pod \"prometheus-k8s-0\" (UID: \"634ec4bd-8776-4599-9ceb-bac83b567f08\") "
I0114 12:51:25.012922   12575 reconciler.go:209] operationExecutor.VerifyControllerAttachedVolume started for volume "pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244" (UniqueName: "kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244.vmdk") pod "prometheus-k8s-0" (UID: "634ec4bd-8776-4599-9ceb-bac83b567f08")
E0114 12:51:25.013026   12575 nestedpendingoperations.go:270] Operation for "\"kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244.vmdk\"" failed. No retries permitted until 2020-01-14 12:51:29.01299507 -0500 EST m=+7392.203534247 (durationBeforeRetry 4s). Error: "Volume has not been added to the list of VolumesInUse in the node's volume status for volume \"pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244\" (UniqueName: \"kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244.vmdk\") pod \"prometheus-k8s-0\" (UID: \"634ec4bd-8776-4599-9ceb-bac83b567f08\") "
I0114 12:51:29.026417   12575 reconciler.go:209] operationExecutor.VerifyControllerAttachedVolume started for volume "pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244" (UniqueName: "kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244.vmdk") pod "prometheus-k8s-0" (UID: "634ec4bd-8776-4599-9ceb-bac83b567f08")
I0114 12:51:29.030982   12575 operation_generator.go:1245] Controller attach succeeded for volume "pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244" (UniqueName: "kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244.vmdk") pod "prometheus-k8s-0" (UID: "634ec4bd-8776-4599-9ceb-bac83b567f08") device path: "/dev/disk/by-id/wwn-0x6000c29eaefd0010095bd61aa5e1c8a7"
I0114 12:51:29.126835   12575 operation_generator.go:551] MountVolume.WaitForAttach entering for volume "pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244" (UniqueName: "kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244.vmdk") pod "prometheus-k8s-0" (UID: "634ec4bd-8776-4599-9ceb-bac83b567f08") DevicePath "/dev/disk/by-id/wwn-0x6000c29eaefd0010095bd61aa5e1c8a7"
I0114 12:51:30.127435   12575 attacher.go:183] Successfully found attached VMDK "[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244.vmdk".
I0114 12:51:30.127485   12575 operation_generator.go:560] MountVolume.WaitForAttach succeeded for volume "pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244" (UniqueName: "kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244.vmdk") pod "prometheus-k8s-0" (UID: "634ec4bd-8776-4599-9ceb-bac83b567f08") DevicePath "/dev/disk/by-id/wwn-0x6000c29eaefd0010095bd61aa5e1c8a7"
I0114 12:51:30.127502   12575 attacher.go:212] vsphere MountDevice/dev/disk/by-id/wwn-0x6000c29eaefd0010095bd61aa5e1c8a7/var/lib/kubelet/plugins/kubernetes.io/vsphere-volume/mounts/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244.vmdk
I0114 12:51:30.140236   12575 mount_linux.go:287] `fsck` error fsck from util-linux 2.23.2
fsck.ext2: Bad magic number in super-block while trying to open /dev/sdb
/dev/sdb:
The superblock could not be read or does not describe a correct ext2
filesystem.  If the device is valid and it really contains an ext2
filesystem (and not swap or ufs or something else), then the superblock
is corrupt, and you might try running e2fsck with an alternate superblock:
e2fsck -b 8193 <device>
E0114 12:51:30.155479   12575 mount_linux.go:140] Mount failed: exit status 32
Mounting command: systemd-run
Mounting arguments: --description=Kubernetes transient mount for /var/lib/kubelet/plugins/kubernetes.io/vsphere-volume/mounts/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244.vmdk --scope -- mount -t ext4 -o defaults /dev/disk/by-id/wwn-0x6000c29eaefd0010095bd61aa5e1c8a7 /var/lib/kubelet/plugins/kubernetes.io/vsphere-volume/mounts/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244.vmdk
Output: Running scope as unit run-27912.scope.
mount: wrong fs type, bad option, bad superblock on /dev/sdb,
missing codepage or helper program, or other error
In some cases useful info is found in syslog - try
dmesg | tail or so.
I0114 12:51:30.160099   12575 mount_linux.go:322] Disk "/dev/disk/by-id/wwn-0x6000c29eaefd0010095bd61aa5e1c8a7" appears to be unformatted, attempting to format as type: "ext4" with options: [-F -m0 /dev/disk/by-id/wwn-0x6000c29eaefd0010095bd61aa5e1c8a7]
I0114 12:51:32.252633   12575 mount_linux.go:326] Disk successfully formatted (mkfs): ext4 - /dev/disk/by-id/wwn-0x6000c29eaefd0010095bd61aa5e1c8a7 /var/lib/kubelet/plugins/kubernetes.io/vsphere-volume/mounts/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244.vmdk
I0114 12:51:32.271122   12575 operation_generator.go:587] MountVolume.MountDevice succeeded for volume "pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244" (UniqueName: "kubernetes.io/vsphere-volume/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244.vmdk") pod "prometheus-k8s-0" (UID: "634ec4bd-8776-4599-9ceb-bac83b567f08") device mount path "/var/lib/kubelet/plugins/kubernetes.io/vsphere-volume/mounts/[VNXE3200-DEV] kubevols/kubernetes-dynamic-pvc-44e88983-e32d-4b8d-b6ac-ef0a0bd3a244.vmdk"
```

As can be seen, when running under the prometheus-operator as a controller it seems that when the first error is encountered (Volume has not been added to the list of VolumesInUse...) that the operator is aborting the pods (though there is no note of this in the operator logs).  

When not running under the prometheus-operator the same error appears in the logs but the process isn't halted and vSphere eventually gets the volume attached and mounted (though it takes a few seconds).

Is perhaps something in the prometheus-operator aborting the pod creation when any error is returned from the kubelet?  

<!--

Feel free to ask questions in #prometheus-operator on Kubernetes Slack!

-->

**What is missing?**
Common tooling container image. Right now we moved most of the tooling into http://quay.io/coreos/jsonnet-ci, but since new tools are being added (https://github.com/coreos/prometheus-operator/pull/2938) it might be worthwhile to create one common image with all tools in it instead of managing multiple ones.

The easiest solution would be to add `lint` tool to `quay.io/coreos/jsonnet-ci`.
Ideally, we should change the name of that image and change location of Dockerfile to reflect that this is no longer only about jsonnet.

**Why do we need it?**
To simplify tooling management and locking versions for reproducibility

This should solve issues from https://github.com/coreos/prometheus-operator/pull/2938#issuecomment-574119036

/cc @brancz @LiliC