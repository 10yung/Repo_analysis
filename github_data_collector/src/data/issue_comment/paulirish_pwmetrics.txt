### Need additional metrics from lighthouse 
Currently only 5 metrics are exported to Google Sheet:.
1. First Contentful Paint
2. First Meaningful Paint
3. Speed Index
4. First CPU Idle
5. Time to Interactive

There are 6 metrics from Audits of DevTools:
![image](https://user-images.githubusercontent.com/12829001/68702364-c0bd7b80-0599-11ea-81ee-234964aef302.png)

At least export additional one:
 1. Max Potential First Input Delay

Will be great if more are exported:
![image](https://user-images.githubusercontent.com/12829001/68702818-7e486e80-059a-11ea-9306-f5b564f20b43.png)


### Environment
pwmetrics version: 4.2.2
Chrome version: Version 78.0.3904.97 (Official Build) (64-bit)
OS version: Win10X64 1803
### Config / CLI options
pwmetrics --config=pwmetrics-config.js
### Errors
Error while trying to retrieve access token, ENOENT: no such file or directory, mkdir '\Users\my\.credentials\'

Error while trying to retrieve access token, invalid_grant at GoogleOauth.getNewToken
### Solution
After digging out in google-oauth.js, change the line as below:
```
//this.tokenDir = path.join((process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE), '/.credentials/'); // Original 
this.tokenDir = path.join((process.env.USERPROFILE || process.env.HOME || process.env.HOMEPATH), '/.credentials/');
```
Reason:
tokenDir in the original line is equal to \Users\username\\.credentials\\
tokenDir in the new line is equal to C:\Users\username\\.credentials\ , which is the right for Windows.

### Environment
1. pwmetrics version: 4.2.2
2. Chrome version: Version 78.0.3904.97 (Official Build) (64-bit)
3. OS version: Win10X64 1803


Are you considering on adding additional metrics, like Time to First Byte, or is there a way I can add additional metrics on my own?
### Steps to reproduce.
`yarn audit`
### Stack trace
```
┌───────────────┬──────────────────────────────────────────────────────────────┐
│ high          │ Improper Authorization                                       │
├───────────────┼──────────────────────────────────────────────────────────────┤
│ Package       │ googleapis                                                   │
├───────────────┼──────────────────────────────────────────────────────────────┤
│ Patched in    │ >=39.1.0                                                     │
├───────────────┼──────────────────────────────────────────────────────────────┤
│ Dependency of │ pwmetrics                                                    │
├───────────────┼──────────────────────────────────────────────────────────────┤
│ Path          │ pwmetrics > googleapis                                       │
├───────────────┼──────────────────────────────────────────────────────────────┤
│ More info     │ https://www.npmjs.com/advisories/791                         │
└───────────────┴──────────────────────────────────────────────────────────────┘
```

### Environment
1. `pwmetrics` version: `4.2.2`

Included changes:
* Fixes the `@fixme` for choosing the median result by actually basing the decision off the index of the median value instead of looking up the median value
* Adds a new `metric` flag which can specify the metric to be used for the median run calculation, defaulting to `TTFCPUIDLE`
* Also adds support for two special values for the metric flag, although they may be better off broken out into separate flags?
  * `metric=all` will report the medians for all metrics independently instead of the single run for the median of a single metric
  * `metric=average` will report the average for all metrics independently instead of the single run for the median of a single metric

Implements #218 

Would this package be open to alternative calculations of the "median" run?  Currently, it chooses the run with the median `TTFCPUIDLE` value, but that may not necessarily be the median `SI` or `TTI` value either.  Depending on which value the consumer is most concerned with testing, I could see misleading results by always basing the median run off `TTFCPUIDLE`.

I'm wondering if it would be helpful to provide config flags to specify which nmetric to calculate the median run using.  

I'm also tossing around whether I think it would be useful to report the average for each metric independently or not.

Thanks!
Suppose I'm running pwmetrics in a post-merge CI job to track my app's performance over time. I'd like to associate each row of data with the git commit of the code being tested, so I can pinpoint where a regression (or improvement) was introduced. 

If I could have an additional option in the `sheets` options for a `comments` field that would accept a string or function, I could accomplish this as follows:

```typescript
module.exports = {
  sheets: {
    type: 'GOOGLE_SHEETS',
    options: {
      comments: () => require('child_process').execSync('git rev-parse --short HEAD').toString()
    }
};
```

There could be more advanced use cases that would benefit from passing in metadata to the function, but for my particular use case, this simple start would be fine.

I'd be happy to contribute a PR if the feature is approved!
Currently, we are able to publish PWMetrics in the scope of GitHub Package Registry, I'd like to hear from folks if everyone is ok with that. 

cc @paulirish @pedro93 

P.S. I can do operation work.
Since we have an awesome [lighthouse-plugin-field-performance](https://github.com/treosh/lighthouse-plugin-field-performance) for measuring RUM, it will be good to have a flag `field/rum/whatever` which will run url against it and provide real user perf metrics.

Blocked by #211 