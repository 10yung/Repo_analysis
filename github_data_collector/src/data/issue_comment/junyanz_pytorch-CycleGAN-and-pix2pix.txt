In training of generator, if dropout is used in discriminator, the GAN loss for generator calculated will be affected by the dropout effect of discriminator.  

        # GAN loss D_A(G_A(A))
        self.loss_G_A = self.criterionGAN(self.netD_A(self.fake_B), True)
        # GAN loss D_B(G_B(B))
        self.loss_G_B = self.criterionGAN(self.netD_B(self.fake_A), True)

So should we use netD_A.eval() and netD_B.eval() before the calculation of GAN loss in order to eliminate the dropout effect?
E:\Anaconda\python.exe E:/Pycharm_Code/Fruit_count/train.py
Traceback (most recent call last):
  File "E:/Pycharm_Code/Fruit_count/train.py", line 17, in <module>
    torch.cuda.set_device(gpus[0])
  File "E:\Anaconda\lib\site-packages\torch\cuda\__init__.py", line 293, in set_device
    torch._C._cuda_setDevice(-1)
AttributeError: module 'torch._C' has no attribute '_cuda_setDevice'
As already mentioned in [this issue](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/595), the current residual block implementation might not be optimal.

Currently, the residual block is implemented as  **conv > bn > relu > conv > bn > add**.
According to [this paper](https://arxiv.org/abs/1603.05027) by Kaiming He (who is the main author of the original Resnet paper), a better order would be **bn > relu > conv > bn > relu > conv > add**.

I adjusted the ResnetBlock and ResnetGenerator classes in networks.py to use this optimized residual block, and trained default pix2pix with and without this change on the Cityscapes dataset. [Here](https://youtu.be/rQxawUa3YvQ) you can find a result comparison. Left is current pix2pix (without the change) and right is the optimized version.

Overall, I would say this change slightly improves image quality, as the generated images are slightly more detailed (probably due to the additional 6/9 nonlinearities). The effect is very minor though. Also, I neither tested it on CycleGAN, nor conducted any quantitative comparisons.
For net with IN layers, it should be fine when batchsize is larger than 1 when testing.

For net with BN layers, can this also be possible if replacing the BN with IN layers by assigning weights  from corresponding BN layers to those IN layers ? Or just 'Adding a net option' to BN layer, making it perfoms instance normalization (I do current BN does not support it...).

As sometimes, the testing set are very large, and batch-testing mode should benefit a lot.

I trained a cyclegan with images from cityscapes (A) and plant images (all default cyclegan options).
Testing with images from the cityscapes dataset gives the expected results : 
(here from visdom, but running the test.py script gives the same results)
(https://www.imageclef.org/lifeclef/2017/plant)
![image](https://user-images.githubusercontent.com/7610018/71483620-04e2a380-2809-11ea-9ef6-cd9b0084fcc0.png)

However, when I take any new image, for example a random google street view screenshot, it seems as though there are almost no modification to the image: 
Real:
![image](https://user-images.githubusercontent.com/7610018/71483806-cdc0c200-2809-11ea-9113-7e6a00bea26b.png)

Fake:
![image](https://user-images.githubusercontent.com/7610018/71483771-a9fd7c00-2809-11ea-9177-b2a798eb0eda.png)

Is this expected? Does the network need to be trained longer? (I'm only at two epochs, despite showing interesting results using the test dataset).

I am working with images 400x600, and trained a resnet6 pix2pix with --preprocess None. The output images are 200x600, and it is the left cropped image from the former 400x600 image, even though I specify --preprocess None during testing. Any help?
Hi,

Let's assume my datasets have 4000 samples and 8000 samples. When I then start training how does the augmentation work? Is it only for the smallest dataset to balance the datasets? And does it randomly  rotate, translate etc.? Or how does it work?
First, thank you for the great work! I've trained a pix2pix model using my own dataset. Now, I want to use the trained model on another dataset to do the image-to-image translation. My initial understanding is that I need to use the test.py script. Can you provide a little insight into what input flags should I set for the test.py script? Also, will the saved results preserve the file name of the input image? Thanks!
Can anybody please help me how to train the models in YIQ, LAB, HSV colourspaces. I am not able to understand that simply reading the files and using BGR2HSV wont suffice I guess. What changes should be  done as I am a beginner in this package.
Hi,
  I have used your pix2pix GAN model with my custom dataset. But the process of generating image pairs takes a lot of time. I have very less data around 100 image pairs. Even when I tried cycle GAN the results were not great. I came across the term self-supervised learning. My question is whether can I use self-supervision to image translation tasks since I have very little data.