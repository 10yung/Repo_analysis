I'm utilizing R studio and trying to run "report" on the following code to summarize the output in plain text. 

`lm.out <- lm(time ~ year, data=Q2_Data)`

`report(lm.out)`

Utilizing this, I then see the following error:

`Error in UseMethod("format_value") : 
  no applicable method for 'format_value' applied to an object of class "NULL"`

The data I'm utilizing this for is open source as I am experimenting with this package and I've attached it here for reference. 

[olympics copy.sav.zip](https://github.com/easystats/report/files/3923343/olympics.copy.sav.zip)

Let me know if there is something I'm doing wrong here.  I'm fairly new to the R world so it could be entirely on my side but I thought if anyone would know it would be you guys :)

`parameters::model_parameter()`, when using `standardize = "refit"`, no longer returns non-standardized coefficients, but only coefficients, SE and CI based on the refitted model from the standardized data. Only when standardize-method is posthoc/basic or so, then "normal" coefficients, SE and CI _and_ std. coefficients (only) are returned.

This breaks the default report method. Since standardizing is in package _effectsize_ now, I suggest calling `model_parameters()` w/o standardizing and then call `effectsize::standardize_parameters()` separately to build the report.

I can address this issue, or you can do it, if you like. But I think you have a clearer vision of what should be reported in the end.
**Question and context**

This obviously isn't a bug, but I'm trying to work out the logic behind why `print.report_table` requires nested calls to both `insight`, and then `parameters`. There's something I've missed along the way in terms of why format table is contained within `insight` if it's used for pretty printing. Indeed, this could have been discussed in another thread and I've missed it.

```r
#' @export
print.report_table <- function(x, ...) {
  table <- insight::format_table(parameters::parameters_table(x))
  cat(table)
}
```

If I'm not mistaken, in effect the final call ends up being something like this within report:

```R
cat(insight::format_table(parameters::parameters_table(report::report(a_model_object))))
```
I would like better understand this design decision, and the history behind it. Pending that answer, I do think it would be worthwhile documenting some of these processes more explicitly within the functions themselves (i.e., dev notes). This is just a suggestion, but it certainly makes contributing and debugging a little easier!

_For context, the reason I ask this is that I have been experimenting with some Rmarkdown printing, but it's certainly not trivial to implement within the ecosystem of report. I wanted to follow how these tables were generated... here I am!_

Hopefully I haven't missed something written elsewhere in the project. Cheers. ðŸ˜€
I noticed that report() of an anova() object does not provide correct/complete output for regression models (eg lm, lmer, glm).

In particular, it could be very helpful if report() could give an anova-like table of main and interaction effects of lmer or complex multiple lm() as coefficients parameters in these cases is sometimes hard to understand or some coefficients do not make sense empirically (i.e. expected difference for a numerical (age) coefficient of 0 between the levels of a categorical (use/non-use of electronic cigarette) coefficient for the VD (motivation to quit smoking)) and as reference for categorical comparison is taken without control on experimental or theoretical reasons in R.
Use of report for these models could be helpful for providing also effect size from sjstats and other relevant parameters of an anova() object to be reported in papers.

This is secondary but might help to report relevant information: report() does not work for anova() of model comparisons. Honestly, I do not know whether there are guidelines for model comparison reporting in psychology/neuroscience. However, report() could help for picking the right parameters (e.g. AIC, BIC or Chi2 and so on) and correct writing.

Lastly, would it be possible to get main and interaction effects for stan_lm/glm/lmer model with report()? the coefficient parameters output is the same as classic regression models and would be helpful to get the general effect of a predictor instead of its coefficient estimation.

**How could we do it?**
For instance, significance for fixed effects in lmer could be done with anova(model.lmer, dff = "Kenward-Roger) (Luke, 2017) or for other regression models as anova(model.lm) or anova(model.glm, test ="F"). 

I am not an expert in R programming and cannot help with actual coding, this is just a suggestion that I noticed during practical use of report() and R in general.

I appreciate your work!

Luke, S.G. Behav Res (2017) 49: 1494. https://doi.org/10.3758/s13428-016-0809-y

The clean names from parameters are not transferred to the report tables 
DO NOT MERGE THIS BRANCH cause it's like super broken, related to the reorganization with effectsize
Once `correlation` and `estimate` are on CRAN, report will go in, with a high priority to make it citeable. 

I think that it makes sense to associate all contributors of easystats to this publication, as it heavily relies on all of the easystats dependencies.

Here's a list of potential journals (feel free to complete the list):

- Software Journals:
  - [Nature toolbox](https://www.nature.com/nature/articles?type=toolbox) (not sure if possibility of submitting ourselves)
  - [JOSS](https://joss.theoj.org/) (IF = NA, H  = NA, SJR = NA)
  - [JORS](https://openresearchsoftware.metajnl.com/) (IF = NA, H = 2, SJR = NA)
  - [PeerJ Computer Science](https://peerj.com/computer-science/) (IF = NA, H = 10, SJR = 0.87)
  - [SoftwareX](https://www.journals.elsevier.com/softwarex/) (IF = NA, H =10, SJR = 4.54)
  - [Journal of Statistical Software](https://www.jstatsoft.org/index) (IF = NA, H = 115, SJR = 17.57)
- More general journals:
     - [Advances in Methods and Practices in Psychological Science](https://www.psychologicalscience.org/publications/ampps) (IF = NA, H = NA, SJR = NA)
    - [Behavior Research Methods](https://www.psychonomic.org/page/BRM) (IF = NA, H = 114, SJR = 2.69)
    - [Psychological Methods](https://www.apa.org/pubs/journals/met/index) (IF = 8.188, H = 131, SJR = 6.51)
    
    - [Scientific Reports](https://www.nature.com/srep) (IF = 4.525, H = 149, SJR = 1.41)

any opinion and experience are welcomed
Recently install the package and it fails producing the report of a data frame:

> library(report)
> report(iris)
> Error: 'format_value' is not an exported object from 'namespace:parameters'

The same error trying to produce the report with `mtcars`

However, it works well with 

> lm(Sepal.Length ~ Petal.Length + Species, data=iris) %>% 
> report() %>% 
> to_table()


The error popup in a fresh session of R 3.6.1 with the `report` version 0.1.0
To do.
*originally from #38*

- `bayesfactor_inclusion`
    - [x] `text` - only results.
    - [x] `text_full` - Give full explanation about model averaging + posterior probability.
    - [x] `table` - basically `as.data.frame`
    - [x] `table_full` - ***same as `table`??***
    - [x] docs...
    - [x] tests...
- `bayesfactor_models`
    - [x] `text` - method + denominator + {best + worst model}
    - [x] `text_full` - with BF computation method + all models.
    - [x] `table` - basically `as.data.frame`
    - [x] `table_full` - give also "BF01 compared to best model` column.
    - [x] docs...
    - [x] tests...
- `bayesfactor_parameters`
    - [ ] `text` - what null (once), BF for each, by name.
    - [ ] `text_full` - ***same as `text`??***
    - [ ] `table` - basically `as.data.frame` + row for null and side
    - [ ] `table_full` - ***same as `table`??***
    - [ ] docs...
    - [ ] tests...
- `bayesfactor_restricted`
    - [ ] `text` - ***???***
    - [ ] `text_full` - ***???***
    - [ ] `table` - basically `as.data.frame`
    - [ ] `table_full` - ***same as `table`??***
    - [ ] docs...
    - [ ] tests...

### Notes to self:
- Should use `interpret_bf` internally for `text`, with arg `rules`.