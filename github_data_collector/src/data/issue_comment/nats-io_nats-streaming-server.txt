Three-node cluster had been upgraded (all nodes at once due to proto change) and stopped functioning. I've seen another GH issue with "can't start until leader is available" couple of days ago, but option `sync: true` doesn't help much. Same upgrade procedure works well on a cluster with no data. Logs and config are pasted below, please suggest.
Much thanks in advance!


**BEFORE UPGRADE**
```
'tail /var/log/nats/nss.log'
env-nats-01:
    [14373] 2020/01/16 11:11:37.177927 [INF] STREAM: --------- Channels Limits --------
    [14373] 2020/01/16 11:11:37.177942 [INF] STREAM:   Subscriptions:          1000 *
    [14373] 2020/01/16 11:11:37.177960 [INF] STREAM:   Messages     :       1000000 *
    [14373] 2020/01/16 11:11:37.177979 [INF] STREAM:   Bytes        :     976.56 MB *
    [14373] 2020/01/16 11:11:37.177997 [INF] STREAM:   Age          :     unlimited *
    [14373] 2020/01/16 11:11:37.178010 [INF] STREAM:   Inactivity   :     unlimited *
    [14373] 2020/01/16 11:11:37.178051 [INF] STREAM: ----------------------------------
    [14373] 2020/01/16 11:11:37.472781 [INF] 172.21.0.5:35174 - rid:10 - Route connection created
    [14373] 2020/01/16 11:11:37.521777 [INF] 172.21.0.4:49120 - rid:11 - Route connection created
env-nats-03:
    [12234] 2020/01/16 11:11:37.425761 [INF] STREAM:   Messages     :       1000000 *
    [12234] 2020/01/16 11:11:37.425779 [INF] STREAM:   Bytes        :     976.56 MB *
    [12234] 2020/01/16 11:11:37.425798 [INF] STREAM:   Age          :     unlimited *
    [12234] 2020/01/16 11:11:37.425816 [INF] STREAM:   Inactivity   :     unlimited *
    [12234] 2020/01/16 11:11:37.425840 [INF] STREAM: ----------------------------------
    [12234] 2020/01/16 11:11:37.472874 [INF] 172.21.0.5:54086 - rid:10 - Route connection created
    [12234] 2020/01/16 11:11:37.523238 [INF] 172.21.0.3:6222 - rid:11 - Route connection created
    [12234] 2020/01/16 11:11:39.213455 [INF] STREAM: Deleting raft logs from 42199461 to 42199461
    [12234] 2020/01/16 11:11:39.213733 [INF] STREAM: Deletion took 279.6Âµs
env-nats-02:
    [9961] 2020/01/16 11:11:37.882306 [INF] STREAM: --------- Channels Limits --------
    [9961] 2020/01/16 11:11:37.882324 [INF] STREAM:   Subscriptions:          1000 *
    [9961] 2020/01/16 11:11:37.882342 [INF] STREAM:   Messages     :       1000000 *
    [9961] 2020/01/16 11:11:37.882360 [INF] STREAM:   Bytes        :     976.56 MB *
    [9961] 2020/01/16 11:11:37.882378 [INF] STREAM:   Age          :     unlimited *
    [9961] 2020/01/16 11:11:37.882396 [INF] STREAM:   Inactivity   :     unlimited *
    [9961] 2020/01/16 11:11:37.882414 [INF] STREAM: ----------------------------------
    [9961] 2020/01/16 11:11:39.207654 [INF] STREAM: server became leader, performing leader promotion actions
    [9961] 2020/01/16 11:11:39.389969 [INF] STREAM: finished leader promotion actions
```

'curl -s localhost:8223/streaming/serverz | grep state'
```
env-nats-01:
      "state": "CLUSTERED",
env-nats-02:
      "state": "CLUSTERED",
env-nats-03:
      "state": "CLUSTERED",
```

'nats-streaming-server --version'
```
env-nats-03:
    nats-streaming-server version 0.9.2, nats-server version 1.0.7
env-nats-01:
    nats-streaming-server version 0.9.2, nats-server version 1.0.7
env-nats-02:
    nats-streaming-server version 0.9.2, nats-server version 1.0.7
```

**AFTER UPGRADE**
'nats-streaming-server --version'
```
env-nats-02:
    nats-streaming-server version 0.16.2, nats-server: v2.0.4
env-nats-01:
    nats-streaming-server version 0.16.2, nats-server: v2.0.4
env-nats-03:
    nats-streaming-server version 0.16.2, nats-server: v2.0.4
```


'curl -s localhost:8223/streaming/serverz | grep state'
```
env-nats-02:
      "state": "CLUSTERED",
env-nats-03:
	  curl: connection timeout
env-nats-01:
	  curl: connection timeout
```


'tail /var/log/nats/nss.log'
```
env-nats-01:
    [15532] 2020/01/16 11:19:16.503440 [INF] 172.21.0.5:35776 - rid:11 - Router connection closed
    [15532] 2020/01/16 11:19:18.083040 [ERR] STREAM: channel "xxxxxxxx_failure" - unable to restore messages, can't start until leader is available
    [15532] 2020/01/16 11:19:19.619417 [INF] 172.21.0.4:6222 - rid:4 - Router connection closed
    [15532] 2020/01/16 11:19:19.763567 [INF] 172.21.0.4:49722 - rid:12 - Route connection created
    [15532] 2020/01/16 11:19:20.664472 [INF] 172.21.0.4:6222 - rid:13 - Route connection created
    [15532] 2020/01/16 11:19:20.664603 [INF] 172.21.0.4:6222 - rid:13 - Router connection closed
    [15532] 2020/01/16 11:19:21.086269 [ERR] STREAM: channel "xxxxxxxx_failure" - unable to restore messages, can't start until leader is available
    [15532] 2020/01/16 11:19:24.088864 [ERR] STREAM: channel "xxxxxxxx_failure" - unable to restore messages, can't start until leader is available
    [15532] 2020/01/16 11:19:27.056425 [ERR] STREAM: channel "xxxxxxxx_failure" - unable to restore messages, can't start until leader is available
    [15532] 2020/01/16 11:19:30.062032 [ERR] STREAM: channel "xxxxxxxx_failure" - unable to restore messages, can't start until leader is available
env-nats-03:
    [13385] 2020/01/16 11:19:20.093565 [INF] STREAM: Cluster Node ID : env-nats-03
    [13385] 2020/01/16 11:19:20.093586 [INF] STREAM: Cluster Log Path: env-nats-streaming/env-nats-03
    [13385] 2020/01/16 11:19:20.664173 [INF] 172.21.0.3:52446 - rid:10 - Route connection created
    [13385] 2020/01/16 11:19:20.664417 [INF] 172.21.0.3:52446 - rid:10 - Router connection closed
    [13385] 2020/01/16 11:19:20.668360 [INF] 172.21.0.5:54706 - rid:11 - Route connection created
    [13385] 2020/01/16 11:19:20.668519 [INF] 172.21.0.5:54706 - rid:11 - Router connection closed
    [13385] 2020/01/16 11:19:22.096915 [ERR] STREAM: channel "xxxxxxxx_failure" - unable to restore messages, can't start until leader is available
    [13385] 2020/01/16 11:19:25.099436 [ERR] STREAM: channel "xxxxxxxx_failure" - unable to restore messages, can't start until leader is available
    [13385] 2020/01/16 11:19:28.066863 [ERR] STREAM: channel "xxxxxxxx_failure" - unable to restore messages, can't start until leader is available
    [13385] 2020/01/16 11:19:31.070344 [ERR] STREAM: channel "xxxxxxxx_failure" - unable to restore messages, can't start until leader is available
env-nats-02:
    [10906] 2020/01/16 11:18:50.164052 [INF] 172.21.0.4:6222 - rid:39 - Route connection created
    [10906] 2020/01/16 11:18:50.164644 [INF] 172.21.0.4:6222 - rid:39 - Router connection closed
    [10906] 2020/01/16 11:19:15.398990 [INF] 172.21.0.3:36426 - rid:36 - Router connection closed
    [10906] 2020/01/16 11:19:15.770327 [INF] 172.21.0.3:36468 - rid:40 - Route connection created
    [10906] 2020/01/16 11:19:16.502866 [INF] 172.21.0.3:6222 - rid:41 - Route connection created
    [10906] 2020/01/16 11:19:16.502866 [INF] 172.21.0.3:6222 - rid:41 - Router connection closed
    [10906] 2020/01/16 11:19:19.619301 [INF] 172.21.0.4:38584 - rid:38 - Router connection closed
    [10906] 2020/01/16 11:19:19.763688 [INF] 172.21.0.4:38628 - rid:42 - Route connection created
    [10906] 2020/01/16 11:19:20.668102 [INF] 172.21.0.4:6222 - rid:43 - Route connection created
    [10906] 2020/01/16 11:19:20.668381 [INF] 172.21.0.4:6222 - rid:43 - Router connection closed
```

**CONFIGS**
cat /etc/nats-streaming-server.conf
```
port: 4222
http_port: 8223
# pid file
pid_file: "/home/nats/run/nss.pid"

# logging options
debug:   false
trace:   false
logtime: true
log_file: "/var/log/nats/nss.log"

cluster {
  listen: 0.0.0.0:6222
  routes: [ "nats://env-nats-01:6222",   "nats://env-nats-02:6222",   "nats://env-nats-03:6222" ]
}

# NATS Streaming specific configuration
streaming {
  id: env-nats-streaming
  store: file
  dir: store
  cluster {
    node_id: "env-nats-01"
    peers: [ "env-nats-01",   "env-nats-02",   "env-nats-03" ]
    sync: true
  }
}
```
__Background__
We've noticed a behaviour we would like to suggest a change for. We are running a number of subscribers (in both node and python), these are subscribed with a `durable name` a `queue group` and `manual acks`.  When running multiple instances of these subscribers, messages are distributed across the instances and subscribers can drop out and come back fine (durable name).  Not all messages that are sent will be acknowledged, if a piece of work fails its message is not acknowledged and `nats-streaming` will retry later.  

__Request__
We have noticed that if all the subscribers close their connections the first one back gets a huge influx of unacknowledged messages (it can be a lot and we suspect it maybe the entire backlog). Can `nats-streaming` therefore honour the `maxInFlight` setting for that channel when attempting to re-deliver all the unacknowledged messages on resumption?

We understand this is related to #732 and #187.  We don't have any problem with unacknowledged messages being redelivered first on resume, this is actually fine for our use case. But having all messages re-delivered at once is a problem as there could be a LOT of them.  This could send the first listening subscription into a sudden heavy load situation.  

__To replicate__
If we have a running `nats-streaming-server` and we have a subscriber (below in Node) that only acknowledges messages that are strings that equal `bob`. (subscriber.js)

```js
const stan = require('node-nats-streaming');
const os = require('os');

// Client Id
const clientId = `${os.hostname()}.${process.pid}`.replace(/\./g, '-');

// Transport
const transport = stan.connect('test-cluster', clientId);
transport.on('connect', function () {
  // Generic function
  const caller = (msg) => {
    console.log('Function', msg.getSequence(), msg.getData());
    if (msg.getData() === 'bob') {
      console.log('Got a bob message');
      msg.ack()
    }
  }

  // Generic function
  const wrapper = (msg) => {
    caller(msg);
  }
  
  const exit = () => {
    transport.close();
  }

  // Set options
  const options = transport.subscriptionOptions();
  options.setAckWait(2000);
  options.setMaxInFlight(2);
  options.setManualAckMode(true);
  options.setDurableName('nlp.a');

  // Subscribe
  const subscription = transport.subscribe('nlp.a', 'q.nlp.a', options);
  subscription.on('message', wrapper);

  process.on('SIGTERM', exit);
  process.on('SIGINT', exit);
  process.on('SIGQUIT', exit);

  // Done
  console.log('Ready');
})
```

If we run *multiple instances* of this, messages are shared between the instances (queue name), if one instance is stopped and started it will resume and join the group again.  If we send `bob` messages and non bob messages via: (caller.js)

```js
const stan = require('node-nats-streaming');
const os = require('os');

// Client Id
const clientId = `${os.hostname()}.${process.pid}`.replace(/\./g, '-');

// Transport
const transport = stan.connect('test-cluster', clientId);
transport.on('connect', function () {

  for (let i = 0; i < 8; i++) {
    transport.publish('nlp.a', JSON.stringify({ a: 'a' }), function (err, uid) {
      console.log('Message recieved ', uid);
    })    
  }

  for (let i = 0; i < 8; i++) {
    transport.publish('nlp.a', 'bob', function (err, uid) {
      console.log('Bob message recieved ', uid);
    })
  }
})
```
You can see bob messages appearing and being acknowledged and after sometime you can observe all the retries happening.  If you then close both instances of the of the `subscriber.js` and start just one instance of `subsriber.js` again; it receives more that the `maxInFlight` messages (in this example 2).  This number could get very large very quickly in our situation.  

__Environment__
```
nats-streaming-server version 0.16.2, nats-server: v2.0.4
```
I've 3 node nats streaming cluster running and was running fine. Due to some issues, node goes down. Once my nodes have recovered I've tried restarting nats streaming server in all the three nodes but it never comes up.

logs -
[node_1.log](https://github.com/nats-io/nats-streaming-server/files/4057405/node_1.log)
[node_3.log](https://github.com/nats-io/nats-streaming-server/files/4057403/node_3.log)
[node_2.log](https://github.com/nats-io/nats-streaming-server/files/4057404/node_2.log)

config files -
[1_conf.txt](https://github.com/nats-io/nats-streaming-server/files/4057416/1_conf.txt)
[2_conf.txt](https://github.com/nats-io/nats-streaming-server/files/4057417/2_conf.txt)
[3_conf.txt](https://github.com/nats-io/nats-streaming-server/files/4057418/3_conf.txt)

(I had to redeploy the cluster with same config file)
I have a tiny Go application (single  `main` package) which uses NATS Streaming. I have a very small system test that starts an in-process STAN server using `github.com/nats-io/nats-streaming-server/server.RunServer(...)`. Problem is that my application is using the `flag` package which conflicts with `github.com/nats-io/nats-streaming-server/server`. You can find a condensed test that breaks here: https://gist.github.com/JensRantil/41e0665fafdc7619b786f10ce3577033 The output is
```sh
$ go test .
flag provided but not defined: -test.testlogfile
Usage of /var/folders/jk/zh2vp9qj5n5bpzrgb7t_6p700000gn/T/go-build491094738/b001/yo.test:
FAIL	github.com/JensRantil/yo	0.283s
FAIL
```

I think the possibility of starting an in-process STAN in a Go test is really really nice (and I'm 99% sure my test passed a little less than a year ago). Is it too much to ask that you don't rely on [the global `CommandLine` variable](https://devdocs.io/go/flag/index#pkg-variables) but instead instantiate a [`FlagSet`](https://devdocs.io/go/flag/index#FlagSet)?

In case anyone else is hitting this there are two workarounds for this issue:
 * Move the test to a separate package which isn't relying on `flag.CommandLine`.
 * I, and everyone else depending on `github.com/nats-io/nats-streaming-server/server`, start using `flag.FlagSet` myself.
i have setup a 3 node nats cluster on kubernetes using the following manifasts.

[3.txt](https://github.com/nats-io/nats-streaming-server/files/4050742/3.txt)
[1.txt](https://github.com/nats-io/nats-streaming-server/files/4050743/1.txt)
[2.txt](https://github.com/nats-io/nats-streaming-server/files/4050744/2.txt)

the cluster is initiated and the clients can connect but after deleting a pod it starts to print the following logs frequently.

``` 
[1] 2020/01/12 14:48:33.163294 [INF] 10.4.13.0:32974 - rid:818 - Router connection closed
[1] 2020/01/12 14:48:33.163514 [ERR] 10.4.13.0:32974 - rid:818 - attempted to connect to route port
[1] 2020/01/12 14:48:36.568460 [INF] 10.4.13.0:32980 - rid:819 - Route connection created
[1] 2020/01/12 14:48:37.081102 [ERR] 10.4.13.0:32980 - rid:819 - attempted to connect to route port
[1] 2020/01/12 14:48:37.081133 [INF] 10.4.13.0:32980 - rid:819 - Router connection closed
[1] 2020/01/12 14:48:37.081344 [ERR] 10.4.13.0:32980 - rid:819 - attempted to connect to route port
[1] 2020/01/12 14:48:39.892258 [INF] 10.4.13.0:32986 - rid:820 - Route connection created
[1] 2020/01/12 14:48:40.526311 [ERR] 10.4.13.0:32986 - rid:820 - attempted to connect to route port
[1] 2020/01/12 14:48:40.526347 [INF] 10.4.13.0:32986 - rid:820 - Router connection closed
[1] 2020/01/12 14:48:40.526567 [ERR] 10.4.13.0:32986 - rid:820 - attempted to connect to route port
[1] 2020/01/12 14:48:43.646349 [INF] 10.4.13.0:32992 - rid:821 - Route connection created
[1] 2020/01/12 14:48:44.246345 [ERR] 10.4.13.0:32992 - rid:821 - attempted to connect to route port
[1] 2020/01/12 14:48:44.246439 [INF] 10.4.13.0:32992 - rid:821 - Router connection closed
[1] 2020/01/12 14:48:44.246763 [ERR] 10.4.13.0:32992 - rid:821 - attempted to connect to route port
[1] 2020/01/12 14:48:47.411274 [INF] 10.4.13.0:32998 - rid:822 - Route connection created
[1] 2020/01/12 14:48:47.981096 [ERR] 10.4.13.0:32998 - rid:822 - attempted to connect to route port
[1] 2020/01/12 14:48:47.981133 [INF] 10.4.13.0:32998 - rid:822 - Router connection closed
[1] 2020/01/12 14:48:47.981269 [ERR] 10.4.13.0:32998 - rid:822 - attempted to connect to route port
[1] 2020/01/12 14:48:51.017136 [INF] 10.4.13.0:33004 - rid:823 - Route connection created
[1] 2020/01/12 14:48:51.803564 [ERR] 10.4.13.0:33004 - rid:823 - attempted to connect to route port
[1] 2020/01/12 14:48:51.803603 [INF] 10.4.13.0:33004 - rid:823 - Router connection closed
[1] 2020/01/12 14:48:51.803757 [ERR] 10.4.13.0:33004 - rid:823 - attempted to connect to route port
[1] 2020/01/12 14:48:56.528991 [INF] 10.4.13.0:33010 - rid:824 - Route connection created
[1] 2020/01/12 14:48:57.664597 [ERR] 10.4.13.0:33010 - rid:824 - attempted to connect to route port
[1] 2020/01/12 14:48:57.664635 [INF] 10.4.13.0:33010 - rid:824 - Router connection closed
[1] 2020/01/12 14:48:57.664777 [ERR] 10.4.13.0:33010 - rid:824 - attempted to connect to route port
[1] 2020/01/12 14:49:02.510658 [INF] 10.4.13.0:33016 - rid:825 - Route connection created
[1] 2020/01/12 14:49:03.714499 [ERR] 10.4.13.0:33016 - rid:825 - attempted to connect to route port
[1] 2020/01/12 14:49:03.714534 [INF] 10.4.13.0:33016 - rid:825 - Router connection closed
[1] 2020/01/12 14:49:03.714685 [ERR] 10.4.13.0:33016 - rid:825 - attempted to connect to route port

```


it seems that the client attempts to reconnect and the nats cluster seems to think it is already connected
this behavior causes the node containing the pod to become unstable (too much logs and iops)

any idea on how to solve this?



I started the server by "./nats-streaming-server -sc c1.conf" "./nats-streaming-server -sc c2.conf" "./nats-streaming-server -sc c3.conf" 

But there is some error info below, looks like election is failed

**[26589] 2019/12/26 03:47:18.764918 [WRN] STREAM: raft: Election timeout reached, restarting election
[26589] 2019/12/26 03:47:18.765066 [INF] STREAM: raft: Node at stan-cluster.1.stan-cluster [Candidate] entering Candidate state in term 106
[26589] 2019/12/26 03:47:18.770262 [DBG] STREAM: raft: Votes needed: 2
[26589] 2019/12/26 03:47:18.770353 [DBG] STREAM: raft: Vote granted from 1 in term 106. Tally: 1
[26589] 2019/12/26 03:47:20.500925 [INF] STREAM: Shutting down.
[26589] 2019/12/26 03:47:20.501103 [ERR] STREAM: raft: Failed to make RequestVote RPC to {Voter 3 stan-cluster.3.stan-cluster}: nats: connection closed
[26589] 2019/12/26 03:47:20.501220 [DBG] STREAM: connection "_NSS-stan-cluster-raft" has been closed
[26589] 2019/12/26 03:47:20.501252 [ERR] STREAM: raft: Failed to make RequestVote RPC to {Voter 2 stan-cluster.2.stan-cluster}: nats: connection closed
[26589] 2019/12/26 03:47:20.501504 [INF] Initiating Shutdown...
[26589] 2019/12/26 03:47:20.501776 [INF] 127.0.0.1:54580 - rid:6 - Router connection closed
[26589] 2019/12/26 03:47:20.501851 [INF] 127.0.0.1:54646 - rid:8 - Router connection closed**

Here is the config file, the 3 server is running on a same os
[c1.txt](https://github.com/nats-io/nats-streaming-server/files/4001271/c1.txt)
[c2.txt](https://github.com/nats-io/nats-streaming-server/files/4001272/c2.txt)
[c3.txt](https://github.com/nats-io/nats-streaming-server/files/4001273/c3.txt)

and the stan client can't work like this

**root@25da27750f0b:/world_data/go/src/stan.go/examples/stan-pub# ./stan-pub --cluster stan-cluster -s nats://127.0.0.1:4221 svr msg2 
Can't connect: stan: connect request timeout (possibly wrong cluster ID?).
Make sure a NATS Streaming Server is running at: nats://127.0.0.1:4221**
We've the requirement to scale the cluster upto upto 7-11 node (but currently 3). I've benchmarked performance of the NATS streaming server for 1 node and 3 nodes and it seems 1 node nats streaming server outperforms 3 node cluster.
Is the NATS streaming server not horizontally scalable?
I have a 3 node NATS cluster.  fairly basic configuration.  The nodes seem to communicate, as the routing detail (for all 3 nodes) is visible when connecting to an individual node's web server.

However, I have enabled cluster_raft_logging at startup, and it clearly shows that the raft election is not happening.

what am I missing?  I have compared these configurations to some other dev/test clusters, and I can see no obvious differences.   appropriate firewall ports are opened on all 3 nodes,

[nss-node-a.txt](https://github.com/nats-io/nats-streaming-server/files/3952811/nss-node-a.txt)
[nss-node-b.txt](https://github.com/nats-io/nats-streaming-server/files/3952813/nss-node-b.txt)
[nss-node-c.txt](https://github.com/nats-io/nats-streaming-server/files/3952815/nss-node-c.txt)

Hello. We have:
Setup 1:
Single instance of nats-streaming in k8s on c5.2xlarge
In-Memory store
```
writeDeadline: "2s"
maxPayload: 268435456
maxConnections: 500
maxSubscriptions: 100000
maxPending: 268435456 
maxControlLine: 4096
+
    store_limits: {
      max_channels: 25000
      max_msgs: 0
      max_bytes: 0
      max_subs: 0
      max_age: "2m"
      max_inactivity: "1m"
      fds_limit: "1000000"
    }
```
![image](https://user-images.githubusercontent.com/9653011/69359829-0bbe4980-0c92-11ea-8af7-b48342874263.png)
![image](https://user-images.githubusercontent.com/9653011/69360223-d5cd9500-0c92-11ea-95aa-d582cebb80e7.png)
Setup 2: 
nats-operator 3x nodes
nats-streaming-operator 3x nodes 
in k8s on c5.2xlarge
Filestore mounted to emptyDir.medium: Memory (tmpfs)
```
writeDeadline: "2s"
maxPayload: 268435456
maxConnections: 500
maxSubscriptions: 100000
maxPending: 268435456 
maxControlLine: 4096
+
streaming: {
    store_limits: {
        max_channels: 25000
        max_msgs: 0
        max_bytes: 0
        max_subs: 0
        max_age: "2m"
        max_inactivity: "1m"
        fds_limit: "1000000"
    }

    cluster: {
        sync: false
    }

    file_options: {
        sync_on_flush: false
        auto_sync: "0"
    }
}
```
![image](https://user-images.githubusercontent.com/9653011/69361725-b8e69100-0c95-11ea-9959-52cf2b9361c3.png)
![image](https://user-images.githubusercontent.com/9653011/69360992-3a3d2400-0c94-11ea-98cf-52604961d635.png)
in top it was 400-500% cpu usage

Our usual load looks like this:
![image](https://user-images.githubusercontent.com/9653011/69361536-5b524480-0c95-11ea-86eb-669186da938a.png)

k8s nodes have antiAffinity and podAntiAffinity too

Questions:
0) As you can see, performance in cluster mode is ok for some period after start of cluster. But over time it starts "eating" cpu and as result ack time increasing. Do you have any ideas why it happens ? Our setup with single node in that environment works like a charm for 3 months.
1) Can you please advise some configuration options to prevent cluster performance degradation?
2) Why difference between single node and 3x cluster so big ? It is because of raft ?

Thnx
The return value `error` of `enforceLimits` is always nil.

I feel like here should an error returned instead of logging it.
```

	if err := ms.removeFirstMsg(nil, lockFile); err != nil {
			// We are not going to fail the publish, just report
			// the error removing the first message.
			// TODO: Is this the right thing to do?
			ms.log.Errorf("Unable to remove first message: %v", err)
			return nil
		}
```
stores/filestore.go:2951:70

```
func (ms *FileMsgStore) enforceLimits(reportHitLimit, lockFile bool) error {
	// Check if we need to remove any (but leave at least the last added).
	// Note that we may have to remove more than one msg if we are here
	// after a restart with smaller limits than originally set, or if
	// message is quite big, etc...
	maxMsgs := ms.limits.MaxMsgs
	maxBytes := ms.limits.MaxBytes
	for ms.totalCount > 1 &&
		((maxMsgs > 0 && ms.totalCount > maxMsgs) ||
			(maxBytes > 0 && ms.totalBytes > uint64(maxBytes))) {

		// Remove first message from first slice, potentially removing
		// the slice, etc...
		if err := ms.removeFirstMsg(nil, lockFile); err != nil {
			// We are not going to fail the publish, just report
			// the error removing the first message.
			// TODO: Is this the right thing to do?
			ms.log.Errorf("Unable to remove first message: %v", err)
			return nil
		}
		if reportHitLimit && !ms.hitLimit {
			ms.hitLimit = true
			ms.log.Warnf(droppingMsgsFmt, ms.subject, ms.totalCount, ms.limits.MaxMsgs,
				util.FriendlyBytes(int64(ms.totalBytes)), util.FriendlyBytes(ms.limits.MaxBytes))
		}
	}
	return nil
}
```