```
@query_params(
        "_source",
        "_source_excludes",
        "_source_includes",
        "fields",
        "pipeline",
        "refresh",
        "routing",
        "timeout",
        "wait_for_active_shards",
    )
    def bulk(self, body, doc_type=None, index=None, params=None):
```

Posted from the source code, Can I know the reason of why query parameters don't support `version` or `version_type` field.

If don't support the version, all the es document will not encounter version conflict error and keep updating the document which bothers me a lot.

Or if I missing something, that each bulk item can actually specify `version` or `version_type` value.

Thanks for your help~
This PR adds tests as required in #940
**Description**
Since there is no `max_retry` configuration on helpers.parallel_bulk (#645) the default case seems to retry forever and never stop. This is a very strange default behavior and caused my batch processing script to run out of memory.

This situation is caused by having the elasticsearch database run out of storage space which I easily achieved by filling a default elasticsearch docker container with documents until my partition was filled.

That lead to the following error with basic curl insertion.

```
curl -X POST "localhost:9200/test/_doc/?pretty" -H 'Content-Type: application/json' -d'
{
    "key1" : 1,
    "key2": "value2"
}
'
..... # Long wait here until I recieved any result
{
  "error" : {
    "root_cause" : [
      {
        "type" : "cluster_block_exception",
        "reason" : "blocked by: [SERVICE_UNAVAILABLE/2/no master];",
        "suppressed" : [
          {
            "type" : "master_not_discovered_exception",
            "reason" : null
          }
        ]
      }
    ],
    "type" : "cluster_block_exception",
    "reason" : "blocked by: [SERVICE_UNAVAILABLE/2/no master];",
    "suppressed" : [
      {
        "type" : "master_not_discovered_exception",
        "reason" : null
      }
    ]
  },
  "status" : 503
}
```

Since the parallel_bulk method retries on 503 responses and it retires an infinite amount of times this becomes a major issue. In my case I need to ingest a large amount of small documents into ES on periodic schedule. To do this quickly I increased the queue_size and chunk_size of the parallel_bulk call according to the Elasticsearch documentation. In my case an optimal configuration looked like this.

```python
 for success, info in elasticsearch.helpers.parallel_bulk(
                elastic,
                documents,
                max_retries=2,
                queue_size=10, 
                chunk_size=2000,
                raise_on_exception=True,
                raise_on_error=True):
            if not success:
                # Log any exception or database errors.
                self.logger.error("Failed to insert document: %s", info)
```

Despite having `raise_on_exception` and `raise_on_error` set to true this call continues to consume my iterator and fill up my memory despite every single insertion attempt being stuck on infinite retries. I did especially not expect the iterator to continue to be consumed in such a situation.

**Environment**:
Linux - Debian Stretch
Python 3.7 with `pip install elasticsearch==7.1.0`
Elasticsearch database running with `docker run --rm -p 9200:9200 elasticsearch:7.4.2`

**Expected outcome**
That the default configuration would be to retry a limited amount of times and that the iterator stops being consumed until the insertion is either aborted or starts working again. I would also very much appreciate getting #645 fixed so that we have control over the amount of retires.

  
 
Added `bytes` param for nodes function
https://www.elastic.co/guide/en/elasticsearch/reference/current/cat-nodes.html
The formatted documentation on readthedocs.io (https://elasticsearch-py.readthedocs.io/en/master/) is not showing recent updates, and still refers to 7.0.0 as a development version.

When passing basic auth headers the client calls:

`b64encode(b(basic_auth)).decode('utf-8')` via the urllib3 request package.

Here `b` is defined as 

```    
def b(s):
        return s.encode("latin-1")
```

This causes any non latin-1 to be encoded incorrectly and prevents auth.
Currently gzip compression and `http_compress` parameter handling is only implemented for `Urllib3HttpConnection`, so when using alternative `RequestsHttpConnection` the parameter is ignored and the requests are not compressed, which might be unexpected for the users.
The PR adds gzip compression support to `RequestsHttpConnection` class. Compression is enabled by setting `http_compress` constructor parameter to `True` exactly as in the `Urllib3HttpConnection`.
elasticsearch-py has support [here](https://github.com/elastic/elasticsearch-py/blob/master/elasticsearch/serializer.py#L2) for [simplejson](https://github.com/simplejson/simplejson) if it's installed but does not currently include this dependency as an optional extra in the setup file.
After running the bulk() function to update objects.
I noticed that the connections are not being reused and are on CLOSE_WAIT state
Also the garbage collector is not flushing out the connections.
I have closed the connection pool manually which resulted in flushing out of all CLOSE_WAIT connections.

I am wondering why the library is not able to close connection and why am i having so many CLOSE_WAIT connections ?
I have read in few issue reported earlier that the library depends on the gc to collect the connections.
But in my case it is not able to collect the connections, i am wondering why this is happening? 