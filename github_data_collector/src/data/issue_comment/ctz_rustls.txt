Running `openssl ecparam -genkey -name prime256v1` gives output like this:

```
-----BEGIN EC PARAMETERS-----
BggqhkjOPQMBBw==
-----END EC PARAMETERS-----
-----BEGIN EC PRIVATE KEY-----
MHcCAQEEIMAE+BTzAWcOIlF3Ws/rwnGjR/edsqi8yRkDACA5I5qnoAoGCCqGSM49
AwEHoUQDQgAEx5fZWcozQkJM1RxdtZmYRsRCfRPPMZDgxUWcDqqUnIoWPgYpRCFf
s58AWKN3DoGigIC3lxXODamfN6mcZPBg2w==
-----END EC PRIVATE KEY-----
```

The contents of the `-----BEGIN EC PRIVATE KEY-----` block are defined in [RFC 5915](https://tools.ietf.org/html/rfc5915). The `-----BEGIN EC PARAMETERS-----` block just contains the OID of the curve and is redundant with the `parameters` field of the ECPrivateKey block.

It would be helpful if rustls exposed a function that can parse this. Right now the user has to convert it to PKCS#8 (via `openssl pkcs8 -topk8 -nocrypt`) before rustls can consume it.
I couldn't tell if this was possible from scanning the docs. All of the client examples appear to take a `DNSNameRef`.  Lots of m2m environments (e.g. industrial) don't use DNS at all in favor of static IP addresses, and therefore don't use DNS names as the CN or provide the `SubjectAlternativeName` extension at all.

Is it possible to use Rustls w/o the client performing hostname validation via the server CN and/or extensions?

I also have a requirement to parse a custom X.509 extension that contains authz info. Is this possible as well?


Hi, when I was browsing through the code base, my IDE showed a lot of duplicated code.

![image](https://user-images.githubusercontent.com/19969910/71710066-b2057f00-2dfa-11ea-9f94-946891483009.png)

Is this intentional?
libp2p needs to be able to verify certificates that would normally be considered invalid (they contain a critical extension that webpki does not, and should not, know about).  It also needs to return data from this process to the application.  The easiest way to do this is for rustls to not even try to parse the certificate itself.  Instead, it should just treat it as bytes.
This client almost works--it has a few interoperability problems with some servers, but the handshake extension it sends seems credible. It's based on draft -02 of the spec: https://tools.ietf.org/html/draft-ietf-tls-esni-02 . There are newer ones, but Firefox and Cloudflare ship -02 for now. I think there will be some fresh movement around -05 in a few weeks.

I thought I would create this as discussion point, since I found it difficult to guess how you'd like this feature factored into the existing code. I tried to keep most of the ESNI-specific stuff in esni.rs (for now), and I think the stuff in msgs/handshake.rs should be uncontroversial. I'm sure you'll want to change what I've done to sessions, configs, etc.

(edit: I see I've slightly broken the contribution rules here, by starting without opening an issue, although there is one already: issue #199. anyway, there's lots of work to do yet, this is just a small start. still need a server side, some work on what other messages need to be in the handshake, a tool for generating the DNS record, and updates to newer drafts).  
Pretty straightforward. I want to have the SNI saved earlier so that I can access it even if something later down the handshake fails.

My use case is a little complicated, but here it goes. I have a multi-tenant TLS proxy, meaning multiple domains can use me to proxy their TLS connections. I use SNI to determine which server cert my proxy will present, using `ResolvesServerCertUsingSNI`. Alternatively, users of this proxy can have the TLS session be end-to-end, specifically that my TLS proxy is really just a TCP proxy, and I forward every byte between the two parties including the bytes that I read as part of the ClientHello.

So, if the handshake fails, mostly due to ResolvesServerCertUsingSNI not finding a pair, I check to see if the SNI is present in the 'end-to-end' endpoint list. If it is, I just forward the client hello through to the other connection which does the TLS handshake.

Without this change, I was unable to get to the SNI value in the case the ResolvesServerCertUsingSNI comes up empty, which in my application does NOT mean we should drop the connection and completely stop doing things.

Let me know if you have any other questions or if you would like me to add tests!

Note: I manually tested this using the proxy service I am working on and it works fine.
This is a work-in-progress/draft pull request for adding Ed25519 support.

See #52 for more information.

_Notes_

* Some `bogo` related tests are still failing.
* The OpenSSL related commands (like `test-ca/build-a-pki.sh`) depend on [OpenSSL version 1.1.1](https://www.openssl.org/news/openssl-1.1.1-notes.html).  On macOS, for example, prefixing `cargo test` commands with `OPENSSL_DIR="$(brew --prefix openssl@1.1)"` will allow the tests to pass.
Full-duplex mode should allow us to perform both read and write, and they won't block each other.

I believe that there is not much in TLS that prevents us from implementing full-duplex mode. The renegotiation has been removed in TLS 1.3, we can ignore it or throw error like ktls.

You can see some related discussions in tokio. https://github.com/tokio-rs/tokio/issues/1108#issuecomment-519777050
Instead of exposing the raw PRKs, the QUIC API should expose an API that limits which HKDF info is allowed to be used for each key. In particular, it the API should prevent non-QUIC labels from being used. I suggest in particular that for each type of key that needs to be derived for QUIC use, there should be a function which hard-codes the parameters to the HKDF expansion.

This would help make it clearer that non-QUIC keys won't be leaked from Rustls when no key log is being used.
This completes the trifecta of configurability requests: we run a sever that uses rustls, and would like to only enable P256 and X25519 to reduce CPU usage. Relates to #177 and #176 but unlike them there is a big gap between TLS 1.3 and TLS 1.2 in this regard.