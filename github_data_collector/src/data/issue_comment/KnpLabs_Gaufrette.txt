Hello,

i am trying to download and upload files from S3 to local filesystem and vice versa using Knp Gaufrette in a Symfony project.

My code work run on a container in openshift and work fine for little files but with large file (1Go) i have memory issues. I am limited to 1,5Go in my container. I don't understand why my memory is growing so high. Maybe i don't understand well the concept of stream in php but i thought that the file wasn't loaded in memory with stream but loaded chunk by chunk.
With my code i can see that when i do`` 
$srcStream = $this->fs_s3->createStream($filename);
$srcStream->open(new StreamMode('rb+'));
`` my memory is growing with the size of the file.
I also tried `copy('gaufrette://s3/'.$filename,'gaufrette://nfs/'.$filename);` but is is the same.

Am i using stream in the wrong way? Any advice?

Thank you in advance for your help.
Regards
The rename method fails when using non url safe chars in the source parameter.

> Error executing "CopyObject" on "https://****.amazonaws.com/FileWithNonUrlSafeChar%_.pdf"; AWS HTTP error: Client error: `PUT https://eu-west-1-sg-prod-files.s3.eu-west-1.amazonaws.com/FileWithNonUrlSafeChar%_.pdf` resulted in a `400 Bad Request` response:
<Error><Code>InvalidArgument</Code><Message>Invalid copy source encoding.</Message><ArgumentName>x-amz-copy-source</Argu (truncated...)

I just url encode the `CopySource` parameter in the API call
I was testing with google cloud storage and seems that content is uploaded twice using StreamWrapper.

The test:

```php
<?php

use Gaufrette\Adapter\GoogleCloudStorage;
use Gaufrette\Filesystem;
use Gaufrette\StreamWrapper;

require_once(__DIR__ . '/../vendor/autoload.php');

const AUTH_FILE = './google-cloud.json';
const BUCKET_NAME = 'testing';
const LOCAL_FILE = __DIR__.'/test.txt';

// Create filesystem
$googleClient = new \Google_Client();
$googleClient->setAuthConfig(AUTH_FILE);
$googleClient->addScope(\Google_Service_Storage::DEVSTORAGE_FULL_CONTROL);
$service = new \Google_Service_Storage($googleClient);
$adapter = new class($service, BUCKET_NAME) extends GoogleCloudStorage
{
    public function write($key, $content)
    {
        echo 'write strlen ' . strlen($content) . PHP_EOL;

        return parent::write($key, $content);
    }
};
$filesystem = new Filesystem($adapter);

// Register wrapper
$map = StreamWrapper::getFilesystemMap();
$map->set(BUCKET_NAME, $filesystem);

StreamWrapper::register();

// Upload file
copy(LOCAL_FILE, 'gaufrette://' . BUCKET_NAME . '/test.txt');
```

The output is:
```bash
jnunez@xxx:~$ php ./test.php
write strlen 0
write strlen 5
write strlen 5
```

Changing 
```php
copy(LOCAL_FILE, 'gaufrette://' . BUCKET_NAME . '/test.txt');
```
by
```php
$filesystem->write('/test.txt', file_get_contents(LOCAL_FILE));
```

The output is:
```bash
jnunez@xxx:~$ php ./test.php
write strlen 5
```

Is there a bug?
I am using this bundle in a Symfony project to upload files to S3. For some debugging purposes and to know more information about the adapter I am using (S3), I need access to _$service_ property.

But cannot do it at the moment as it's a protected property. Can we make a getter method to get this property?

I can send a pull request if needed.
there's 2 class named 'CreateContainerOptions'

1.
namespace MicrosoftAzure\Storage\Blob\Models;
directory: vendor/microsoft/azure-storage-blob/src/Blob/Models/CreateContainerOptions

2.
namespace MicrosoftAzure\Storage\Blob\Models;
directory: vendor/microsoft/azure-storage/src/Blob/Models/CreateContainerOptions

i don't know if it's a bug, but can you help me Which one should i use?

Related to #618.

Container creation is out of Gaufrette scope. The container should be
created by the deleveloper on its own.
Thus, the multi container mode has been removed, as it was creating
containers on the fly.

## TODO : 
- [x] update doc
By looking at the `UPGRADE.md` file, we can know what are the removed methods.
These methods should trigger a deprecation notice on the `0.x` branch.
We were previously (on `0.x`) creating the containers / buckets automatically when using adapters. This can lead to security flaws in case where the bucket is not created as the adapter's user want to.

Buckets creation is beyound Gaufrette scope and so should not be managed by Gaufrette. Buckets used by Gaufrette shoud be created by the developer (or any other way) and should be properly configured for the developer needs.

Thus, Gaufrette should not automatically create buckets when they do not exist. This will remove the `create` flag passed to adapters's constructor, and the usage of an adapter on an unexisting bucket should raise an exception.

## TODO

- [ ] AwsS3
- [ ] AzureBlobStorage (#622)
- [ ] Ftp
- [x] Local (#620)
- [x] OpenCloud / OpenStack (#533)
- [ ] PhpseclibSftp
- [ ] Update CHANGELOG / UPGRADE guide to indicate about this change
1. https://github.com/KnpLabs/Gaufrette/blob/master/src/Gaufrette/Adapter/Ftp.php#L127
2. https://github.com/KnpLabs/Gaufrette/blob/master/src/Gaufrette/Adapter/Ftp.php#L240
3. https://github.com/KnpLabs/Gaufrette/blob/master/src/Gaufrette/Adapter/Ftp.php#L383

In these 3x places, specific arguments like `-al` or `-alR` are being used when `ftp_rawlist` is being called.

I believe, depending on the FTP server settings, they cause trouble -> directories/files can not be listed and `ftp_rawlist` simply returns empty list.

Here is the code that does not list the files on the server ([Hermes Germany](https://www.myhermes.de) carrier FTP - used by a lot of vendors of course) we use:
```
$adapter = new FtpAdapter($dir, $host, array(
    'port'     => 21,
    'username' => $user,
    'password' => $pass,
    'passive'  => false,
    'create'   => false,
    'mode'     => FTP_BINARY,
    'ssl'      => false,
));
$filesystem = new Filesystem($adapter);
var_dump($filesystem->keys());
```

And here is the native code which works as expected:
```
$connection = ftp_connect($host);

ftp_login($connection, $user, $pass);

var_dump(ftp_rawlist($connection, $dir));
// var_dump(ftp_rawlist($connection, '-alR ' . $dir)); // This is how Gaufrette does it

ftp_close($connection);
```

I believe this arguments must be configurable to resolve mentioned issue - they should be enabled by default to ensure backward compatibility but with an option to disable them.
The meta packages for adapters removed on 1.x should be deprecated. The new adapters introduced in 1.x should have their meta packages.

Also, the `suggest` section of the `composer.json` file should be updated with the new adapters' metapackages.

## To Deprecate
- [ ] `gaufrette/opencloud-adapter` metapackage (adapter removed in #533)
    - [ ] Deprecate the `OpenCloud` adapter
    - [ ] Deprecate the `ObjectStoreFactory` (removed in #533)

## To Create
- [ ] `gaufrette/openstack-adapter` metapackage (adapter introduced in #533)
- [ ] `gaufrette/google-cloud-storage-adapter` metapackage (adapter introduced in #557)