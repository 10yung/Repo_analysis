how to add last model checkpoints during training 
Hi! I can't understand how precision and recall are calculated. 
I would like to plot a P/R curve for different `score_threshold` value, but when I saw the code in `eval.py` I can't understand why FP and TP are calculated this way:
``` python
false_positives = np.cumsum(false_positives)
true_positives  = np.cumsum(true_positives)
```
in particular, I've doubts about np.cumsum(), why it isn't np.sum()?

i think  the algorithm does not take into the pictures without targets, will it cause too few background classes or negative samples compared two-stage algorithm?  

if (x1, y1, x2, y2, class_name) == ('', '', '', '', ''):
        continue
this code from csv_generator.py , can you give me some advices ?


Hi, I was following along [ issue #1114 "TypeError: data type not understood" ]( https://github.com/fizyr/keras-retinanet/issues/1114 )and like user icoicqico was running into a new problem after using the proposed fix in #1114

AttributeError: 'Node' object has no attribute 'output_masks' 

The assumption was that it is related to tf-lite:

> Well, it looks like you got further along, this is probably some tf-lite stuff, I can't really help you with that, I'm guessing a lot of layers in keras-retinanet are not supported

However, I see it unrelated to tf-lite but when loading the Retina Net model with tf.keras in TF2.
I would expect that to work in principle, since Retinanet and TF2.Keras both implement the Keras API?

The relevant code should be:

```
from tensorflow import keras
myCustomObjects = backbone('resnet50').custom_objects
model = keras.models.load_model("/tmp/model_conv.h5", myCustomObjects) 
```

with this stack trace

```
Traceback (most recent call last):
  File "loadKeras.py", line 42, in <module>
    model = keras.models.load_model("/tmp/model_conv.h5", myCustomObjects) 
  File "/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/saving/save.py", line 146, in load_model
    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/saving/hdf5_format.py", line 168, in load_model_from_hdf5
    custom_objects=custom_objects)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/saving/model_config.py", line 55, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/layers/serialization.py", line 102, in deserialize
    printable_module_name='layer')
  File "/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/utils/generic_utils.py", line 191, in deserialize_keras_object
    list(custom_objects.items())))
  File "/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/network.py", line 906, in from_config
    config, custom_objects)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/network.py", line 1852, in reconstruct_from_config
    process_node(layer, node_data)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/network.py", line 1799, in process_node
    output_tensors = layer(input_tensors, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py", line 75, in symbolic_fn_wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/keras/engine/base_layer.py", line 475, in __call__
    previous_mask = _collect_previous_mask(inputs)
  File "/usr/local/lib/python2.7/dist-packages/keras/engine/base_layer.py", line 1441, in _collect_previous_mask
    mask = node.output_masks[tensor_index]
AttributeError: 'Node' object has no attribute 'output_masks'
```

**Seems to me like a problem with TF2**, but I am really just starting with tensor flow.
What do you think?
Any ideas how to fix that?

By the way, I have cloned the repo with commit da8ceaf and have changed the initializer as proposed by [LaurensHagendoorn 
]( https://github.com/fizyr/keras-retinanet/issues/1114#issuecomment-533091841)
Thanks.
Hi, this might not belong here but I'd still like to ask if it's possible to change anchor shapes and sizes when fine tuning my custom model on new dataset. Do we have to maintain the same anchor config throughout all training and fine tuning? Also the same thing about image-min-side and image-max-side. Does this make sense? 
Anyway it throws `IndexError` if I change it without any other change.
I am trying to train model in Google Colab using tensorflow 2.x bundle package for Colab and Getting Error as mention Below:

`AttributeError:'Model' object has no attribute _get_distribution_strategy' `
I am trying to predict image from train model and getting error as below
`2019-12-19 09:54:53.768014: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Incompatible shapes: [1,235356] vs. [1,176517]
	 [[{{node boxes/mul_1}}]]
	 [[filtered_detections/map/while/body/_1/strided_slice_2/_52]]
2019-12-19 09:54:53.769414: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Incompatible shapes: [1,235356] vs. [1,176517]
	 [[{{node boxes/mul_1}}]]`
It takes 200ms inferencing an input of shape (1, 800, 800, 3).
And takes 3seconds inferencing an input of shape  (15, 800, 800, 3)
After applying some tooling to convert the Keras/Retinanet model (.h5) into a Tensorflow model (.pb), I'm facing some issues. When I run the Keras/Retinanet model against an image:
	boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))
the boxes, scores and labels are populated with the expected results. Just for information, the model gives me back the 9 detected objects I was expecting. 

For the Tensorflow/Retinanet model, the situation is quite different.   
	regression, classification = sess.run([tf_classification, tf_regression], feed_dict={tf_input: image})

	filter_detections_layer = keras_retinanet.layers.FilterDetections(nms=True)
	boxes, scores, labels = filter_detections_layer.call([regression, classification])

Both 'regression' and 'classification' have the right format:

	Tensor("classification/concat:0", shape=(?, ?, 3), dtype=float32)
	Tensor("regression/concat:0", shape=(?, ?, 4), dtype=float32)

However, after applying the 'FilterDetections', the boxes, scores and labels are populated with ~240 objects and the bounding boxes have a weird format 

	[[ 0.00330357 -0.50109524 -0.46569705  0.5642642 ]
	 [ 0.00824136 -1.2870306  -0.4677258  -0.2555487 ]
	 [ 0.5877216  -1.5296552   0.06298425 -0.5602327 ]
	 ...

For the same image with the Keras/Retinanet model, bounding boxes are:

	[[827.52124 229.86569 880.2724  363.2836 ]
	 [ 19.23209 294.3282  102.16435 346.58243]
	 [550.0928  279.2105  586.58453 318.4497 ]
	 ...

Looks like I've been missing something. I've been digging into the keras-retinanet code for a couple of days but so far, I'm stuck. Any idea what's wrong in my logic or code ? 


  File "keras_retinanet/bin/train.py", line 528, in <module>
    main()
  File "keras_retinanet/bin/train.py", line 521, in main
    validation_steps = args.steps_for_validation,
AttributeError: 'Namespace' object has no attribute 'steps_for_validation'