Hi！guys, when i run `pip install -r requirements.txt`
The following error :
Building wheels for collected packages: horovod
  Building wheel for horovod (setup.py) ... error
  ERROR: Command errored out with exit status 1:
   command: /home/scw4750/anaconda3/envs/lcx_glow/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '"'"'/tmp/pip-install-qjvktya2/horovod/setup.py'"'"'; __file__='"'"'/tmp/pip-install-qjvktya2/horovod/setup.py'"'"';f=getattr(tokenize, '"'"'open'"'"', open)(__file__);code=f.read().replace('"'"'\r\n'"'"', '"'"'\n'"'"');f.close();exec(compile(code, __file__, '"'"'exec'"'"'))' bdist_wheel -d /tmp/pip-wheel-j5sti5wz --python-tag cp35
       cwd: /tmp/pip-install-qjvktya2/horovod/
  Complete output (97 lines):
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib.linux-x86_64-3.5
  creating build/lib.linux-x86_64-3.5/horovod
  copying horovod/__init__.py -> build/lib.linux-x86_64-3.5/horovod
  creating build/lib.linux-x86_64-3.5/horovod/tensorflow
  copying horovod/tensorflow/__init__.py -> build/lib.linux-x86_64-3.5/horovod/tensorflow
  copying horovod/tensorflow/mpi_ops.py -> build/lib.linux-x86_64-3.5/horovod/tensorflow
  creating build/lib.linux-x86_64-3.5/horovod/torch
  copying horovod/torch/__init__.py -> build/lib.linux-x86_64-3.5/horovod/torch
  copying horovod/torch/mpi_ops.py -> build/lib.linux-x86_64-3.5/horovod/torch
  creating build/lib.linux-x86_64-3.5/horovod/keras
  copying horovod/keras/callbacks.py -> build/lib.linux-x86_64-3.5/horovod/keras
  copying horovod/keras/__init__.py -> build/lib.linux-x86_64-3.5/horovod/keras
  creating build/lib.linux-x86_64-3.5/horovod/common
  copying horovod/common/__init__.py -> build/lib.linux-x86_64-3.5/horovod/common
  creating build/lib.linux-x86_64-3.5/horovod/torch/mpi_lib_impl
  copying horovod/torch/mpi_lib_impl/__init__.py -> build/lib.linux-x86_64-3.5/horovod/torch/mpi_lib_impl
  creating build/lib.linux-x86_64-3.5/horovod/torch/mpi_lib
  copying horovod/torch/mpi_lib/__init__.py -> build/lib.linux-x86_64-3.5/horovod/torch/mpi_lib
  running build_ext
  gcc -pthread -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -std=c++11 -fPIC -O2 -I/home/scw4750/anaconda3/envs/lcx_glow/include/python3.5m -c build/temp.linux-x86_64-3.5/test_compile/test_cpp_flags.cc -o build/temp.linux-x86_64-3.5/test_compile/test_cpp_flags.o
  cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
  gcc -pthread -shared -L/home/scw4750/anaconda3/envs/lcx_glow/lib -Wl,-rpath=/home/scw4750/anaconda3/envs/lcx_glow/lib,--no-as-needed build/temp.linux-x86_64-3.5/test_compile/test_cpp_flags.o -L/home/scw4750/anaconda3/envs/lcx_glow/lib -o build/temp.linux-x86_64-3.5/test_compile/test_cpp_flags.so
  gcc -pthread -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -std=c++11 -fPIC -O2 -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi -pthread -L/usr//lib -L/usr/lib/openmpi/lib -lmpi_cxx -lmpi -ldl -lhwloc -I/home/scw4750/anaconda3/envs/lcx_glow/include/python3.5m -c build/temp.linux-x86_64-3.5/test_compile/test_tensorflow_libs.cc -o build/temp.linux-x86_64-3.5/test_compile/test_tensorflow_libs.o
  cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
  gcc -pthread -shared -L/home/scw4750/anaconda3/envs/lcx_glow/lib -Wl,-rpath=/home/scw4750/anaconda3/envs/lcx_glow/lib,--no-as-needed build/temp.linux-x86_64-3.5/test_compile/test_tensorflow_libs.o -L/home/scw4750/anaconda3/envs/lcx_glow/lib/python3.5/site-packages/tensorflow/core -L/home/scw4750/anaconda3/envs/lcx_glow/lib -ltensorflow_framework -o build/temp.linux-x86_64-3.5/test_compile/test_tensorflow_libs.so
  /usr/bin/ld: cannot find -ltensorflow_framework
  collect2: error: ld returned 1 exit status
  gcc -pthread -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -std=c++11 -fPIC -O2 -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi -pthread -L/usr//lib -L/usr/lib/openmpi/lib -lmpi_cxx -lmpi -ldl -lhwloc -I/home/scw4750/anaconda3/envs/lcx_glow/include/python3.5m -c build/temp.linux-x86_64-3.5/test_compile/test_tensorflow_libs.cc -o build/temp.linux-x86_64-3.5/test_compile/test_tensorflow_libs.o
  cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
  gcc -pthread -shared -L/home/scw4750/anaconda3/envs/lcx_glow/lib -Wl,-rpath=/home/scw4750/anaconda3/envs/lcx_glow/lib,--no-as-needed build/temp.linux-x86_64-3.5/test_compile/test_tensorflow_libs.o -L/home/scw4750/anaconda3/envs/lcx_glow/lib/python3.5/site-packages/tensorflow/core -L/home/scw4750/anaconda3/envs/lcx_glow/lib -o build/temp.linux-x86_64-3.5/test_compile/test_tensorflow_libs.so
  gcc -pthread -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -std=c++11 -fPIC -O2 -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi -pthread -L/usr//lib -L/usr/lib/openmpi/lib -lmpi_cxx -lmpi -ldl -lhwloc -D_GLIBCXX_USE_CXX11_ABI=0 -I/home/scw4750/anaconda3/envs/lcx_glow/lib/python3.5/site-packages/tensorflow/include -I/home/scw4750/anaconda3/envs/lcx_glow/lib/python3.5/site-packages/tensorflow/include/external/nsync/public -I/home/scw4750/anaconda3/envs/lcx_glow/include/python3.5m -c build/temp.linux-x86_64-3.5/test_compile/test_tensorflow_abi.cc -o build/temp.linux-x86_64-3.5/test_compile/test_tensorflow_abi.o
  cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
  gcc -pthread -shared -L/home/scw4750/anaconda3/envs/lcx_glow/lib -Wl,-rpath=/home/scw4750/anaconda3/envs/lcx_glow/lib,--no-as-needed build/temp.linux-x86_64-3.5/test_compile/test_tensorflow_abi.o -L/home/scw4750/anaconda3/envs/lcx_glow/lib/python3.5/site-packages/tensorflow/core -L/home/scw4750/anaconda3/envs/lcx_glow/lib -o build/temp.linux-x86_64-3.5/test_compile/test_tensorflow_abi.so
  building 'horovod.tensorflow.mpi_lib' extension
  creating build/temp.linux-x86_64-3.5/horovod
  creating build/temp.linux-x86_64-3.5/horovod/tensorflow
  gcc -pthread -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/scw4750/anaconda3/envs/lcx_glow/include/python3.5m -c horovod/tensorflow/mpi_ops.cc -o build/temp.linux-x86_64-3.5/horovod/tensorflow/mpi_ops.o -std=c++11 -fPIC -O2 -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi -pthread -L/usr//lib -L/usr/lib/openmpi/lib -lmpi_cxx -lmpi -ldl -lhwloc -I/home/scw4750/anaconda3/envs/lcx_glow/lib/python3.5/site-packages/tensorflow/include -I/home/scw4750/anaconda3/envs/lcx_glow/lib/python3.5/site-packages/tensorflow/include/external/nsync/public -D_GLIBCXX_USE_CXX11_ABI=0
  cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
  g++ -pthread -shared -L/home/scw4750/anaconda3/envs/lcx_glow/lib -Wl,-rpath=/home/scw4750/anaconda3/envs/lcx_glow/lib,--no-as-needed build/temp.linux-x86_64-3.5/horovod/tensorflow/mpi_ops.o -L/home/scw4750/anaconda3/envs/lcx_glow/lib -lpython3.5m -o build/lib.linux-x86_64-3.5/horovod/tensorflow/mpi_lib.cpython-35m-x86_64-linux-gnu.so -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi -pthread -L/usr//lib -L/usr/lib/openmpi/lib -lmpi_cxx -lmpi -ldl -lhwloc -L/home/scw4750/anaconda3/envs/lcx_glow/lib/python3.5/site-packages/tensorflow/core
  INFO: Above error indicates that this PyTorch installation does not support CUDA.
  INFO: Unable to build PyTorch plugin, will skip it.

  Traceback (most recent call last):
    File "/tmp/pip-install-qjvktya2/horovod/setup.py", line 572, in build_extensions
      build_torch_extension(self, options, abi_compile_flags)
    File "/tmp/pip-install-qjvktya2/horovod/setup.py", line 509, in build_torch_extension
      from torch.utils.ffi import create_extension
    File "/home/scw4750/anaconda3/envs/lcx_glow/lib/python3.5/site-packages/torch/utils/ffi/__init__.py", line 1, in <module>
      raise ImportError("torch.utils.ffi is deprecated. Please use cpp extensions instead.")
  ImportError: torch.utils.ffi is deprecated. Please use cpp extensions instead.

  building 'horovod.common.mpi_lib' extension
  creating build/temp.linux-x86_64-3.5/horovod/common
  gcc -pthread -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/scw4750/anaconda3/envs/lcx_glow/include/python3.5m -c horovod/common/common.cc -o build/temp.linux-x86_64-3.5/horovod/common/common.o -std=c++11 -fPIC -O2 -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi -pthread -L/usr//lib -L/usr/lib/openmpi/lib -lmpi_cxx -lmpi -ldl -lhwloc -D_GLIBCXX_USE_CXX11_ABI=0
  cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
  gcc -pthread -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/scw4750/anaconda3/envs/lcx_glow/include/python3.5m -c horovod/common/mpi_message.cc -o build/temp.linux-x86_64-3.5/horovod/common/mpi_message.o -std=c++11 -fPIC -O2 -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi -pthread -L/usr//lib -L/usr/lib/openmpi/lib -lmpi_cxx -lmpi -ldl -lhwloc -D_GLIBCXX_USE_CXX11_ABI=0
  cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
  gcc -pthread -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/scw4750/anaconda3/envs/lcx_glow/include/python3.5m -c horovod/common/operations.cc -o build/temp.linux-x86_64-3.5/horovod/common/operations.o -std=c++11 -fPIC -O2 -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi -pthread -L/usr//lib -L/usr/lib/openmpi/lib -lmpi_cxx -lmpi -ldl -lhwloc -D_GLIBCXX_USE_CXX11_ABI=0
  cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
  In file included from /usr/lib/openmpi/include/mpi.h:253:0,
                   from horovod/common/operations.cc:36:
  /usr/lib/openmpi/include/mpi_portable_platform.h:374:34: warning: invalid suffix on literal; C++11 requires a space between literal and string macro [-Wliteral-suffix]
                _STRINGIFY(__GNUC__)"."_STRINGIFY(__GNUC_MINOR__)"."_STRINGIFY(__GNUC_PATCHLEVEL__)
                                    ^
  /usr/lib/openmpi/include/mpi_portable_platform.h:374:63: warning: invalid suffix on literal; C++11 requires a space between literal and string macro [-Wliteral-suffix]
                _STRINGIFY(__GNUC__)"."_STRINGIFY(__GNUC_MINOR__)"."_STRINGIFY(__GNUC_PATCHLEVEL__)
                                                                 ^
  horovod/common/operations.cc: In function ‘void horovod::common::{anonymous}::PerformOperation(horovod::common::{anonymous}::TensorTable&, horovod::common::MPIResponse)’:
  horovod/common/operations.cc:784:24: error: invalid conversion from ‘const void*’ to ‘void*’ [-fpermissive]
           e.tensor->data(), (int)e.tensor->shape().num_elements(),
                          ^
  In file included from horovod/common/operations.cc:36:0:
  /usr/lib/openmpi/include/mpi.h:1036:20: note: initializing argument 1 of ‘int MPI_Allgatherv(void*, int, MPI_Datatype, void*, int*, int*, MPI_Datatype, MPI_Comm)’
   OMPI_DECLSPEC  int MPI_Allgatherv(void *sendbuf, int sendcount, MPI_Datatype sendtype,
                      ^
  horovod/common/operations.cc:1106:54: error: invalid conversion from ‘const void*’ to ‘void*’ [-fpermissive]
                                 horovod_global.mpi_comm))
                                                        ^
  horovod/common/operations.cc:530:24: note: in definition of macro ‘MPI_CHECK’
       auto mpi_result = (op);                                                    \
                          ^
  In file included from horovod/common/operations.cc:36:0:
  /usr/lib/openmpi/include/mpi.h:1041:20: note: initializing argument 1 of ‘int MPI_Allreduce(void*, void*, int, MPI_Datatype, MPI_Op, MPI_Comm)’
   OMPI_DECLSPEC  int MPI_Allreduce(void *sendbuf, void *recvbuf, int count,
                      ^
  horovod/common/operations.cc: In function ‘void horovod::common::{anonymous}::BackgroundThreadLoop(horovod::common::{anonymous}::HorovodGlobalState&)’:
  horovod/common/operations.cc:1251:33: error: ‘MPI_COMM_TYPE_SHARED’ was not declared in this scope
     MPI_Comm_split_type(mpi_comm, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL,
                                   ^
  horovod/common/operations.cc:1252:34: error: ‘MPI_Comm_split_type’ was not declared in this scope
                         &local_comm);
                                    ^
  error: command 'gcc' failed with exit status 1
  ----------------------------------------
  ERROR: Failed building wheel for horovod
  Running setup.py clean for horovod
Failed to build horovod
Installing collected packages: horovod, setuptools
    Running setup.py install for horovod ... error
    ERROR: Command errored out with exit status 1:
     command: /home/scw4750/anaconda3/envs/lcx_glow/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '"'"'/tmp/pip-install-qjvktya2/horovod/setup.py'"'"'; __file__='"'"'/tmp/pip-install-qjvktya2/horovod/setup.py'"'"';f=getattr(tokenize, '"'"'open'"'"', open)(__file__);code=f.read().replace('"'"'\r\n'"'"', '"'"'\n'"'"');f.close();exec(compile(code, __file__, '"'"'exec'"'"'))' install --record /tmp/pip-record-zkq5olrq/install-record.txt --single-version-externally-managed --compile
         cwd: /tmp/pip-install-qjvktya2/horovod/
    Complete output (97 lines):
    running install
    running build
    running build_py
    creating build
    creating build/lib.linux-x86_64-3.5
    creating build/lib.linux-x86_64-3.5/horovod
    copying horovod/__init__.py -> build/lib.linux-x86_64-3.5/horovod
    creating build/lib.linux-x86_64-3.5/horovod/tensorflow
    copying horovod/tensorflow/__init__.py -> build/lib.linux-x86_64-3.5/horovod/tensorflow
    copying horovod/tensorflow/mpi_ops.py -> build/lib.linux-x86_64-3.5/horovod/tensorflow
    creating build/lib.linux-x86_64-3.5/horovod/torch
    copying horovod/torch/__init__.py -> build/lib.linux-x86_64-3.5/horovod/torch
    copying horovod/torch/mpi_ops.py -> build/lib.linux-x86_64-3.5/horovod/torch
    creating build/lib.linux-x86_64-3.5/horovod/keras
    copying horovod/keras/callbacks.py -> build/lib.linux-x86_64-3.5/horovod/keras
    copying horovod/keras/__init__.py -> build/lib.linux-x86_64-3.5/horovod/keras
    creating build/lib.linux-x86_64-3.5/horovod/common
    copying horovod/common/__init__.py -> build/lib.linux-x86_64-3.5/horovod/common
    creating build/lib.linux-x86_64-3.5/horovod/torch/mpi_lib_impl
    copying horovod/torch/mpi_lib_impl/__init__.py -> build/lib.linux-x86_64-3.5/horovod/torch/mpi_lib_impl
    creating build/lib.linux-x86_64-3.5/horovod/torch/mpi_lib
    copying horovod/torch/mpi_lib/__init__.py -> build/lib.linux-x86_64-3.5/horovod/torch/mpi_lib
    running build_ext
    gcc -pthread -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -std=c++11 -fPIC -O2 -I/home/scw4750/anaconda3/envs/lcx_glow/include/python3.5m -c build/temp.linux-x86_64-3.5/test_compile/test_cpp_flags.cc -o build/temp.linux-x86_64-3.5/test_compile/test_cpp_flags.o
    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
    gcc -pthread -shared -L/home/scw4750/anaconda3/envs/lcx_glow/lib -Wl,-rpath=/home/scw4750/anaconda3/envs/lcx_glow/lib,--no-as-needed build/temp.linux-x86_64-3.5/test_compile/test_cpp_flags.o -L/home/scw4750/anaconda3/envs/lcx_glow/lib -o build/temp.linux-x86_64-3.5/test_compile/test_cpp_flags.so
    gcc -pthread -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -std=c++11 -fPIC -O2 -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi -pthread -L/usr//lib -L/usr/lib/openmpi/lib -lmpi_cxx -lmpi -ldl -lhwloc -I/home/scw4750/anaconda3/envs/lcx_glow/include/python3.5m -c build/temp.linux-x86_64-3.5/test_compile/test_tensorflow_libs.cc -o build/temp.linux-x86_64-3.5/test_compile/test_tensorflow_libs.o
    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
    gcc -pthread -shared -L/home/scw4750/anaconda3/envs/lcx_glow/lib -Wl,-rpath=/home/scw4750/anaconda3/envs/lcx_glow/lib,--no-as-needed build/temp.linux-x86_64-3.5/test_compile/test_tensorflow_libs.o -L/home/scw4750/anaconda3/envs/lcx_glow/lib/python3.5/site-packages/tensorflow/core -L/home/scw4750/anaconda3/envs/lcx_glow/lib -ltensorflow_framework -o build/temp.linux-x86_64-3.5/test_compile/test_tensorflow_libs.so
    /usr/bin/ld: cannot find -ltensorflow_framework
    collect2: error: ld returned 1 exit status
    gcc -pthread -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -std=c++11 -fPIC -O2 -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi -pthread -L/usr//lib -L/usr/lib/openmpi/lib -lmpi_cxx -lmpi -ldl -lhwloc -I/home/scw4750/anaconda3/envs/lcx_glow/include/python3.5m -c build/temp.linux-x86_64-3.5/test_compile/test_tensorflow_libs.cc -o build/temp.linux-x86_64-3.5/test_compile/test_tensorflow_libs.o
    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
    gcc -pthread -shared -L/home/scw4750/anaconda3/envs/lcx_glow/lib -Wl,-rpath=/home/scw4750/anaconda3/envs/lcx_glow/lib,--no-as-needed build/temp.linux-x86_64-3.5/test_compile/test_tensorflow_libs.o -L/home/scw4750/anaconda3/envs/lcx_glow/lib/python3.5/site-packages/tensorflow/core -L/home/scw4750/anaconda3/envs/lcx_glow/lib -o build/temp.linux-x86_64-3.5/test_compile/test_tensorflow_libs.so
    gcc -pthread -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -std=c++11 -fPIC -O2 -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi -pthread -L/usr//lib -L/usr/lib/openmpi/lib -lmpi_cxx -lmpi -ldl -lhwloc -D_GLIBCXX_USE_CXX11_ABI=0 -I/home/scw4750/anaconda3/envs/lcx_glow/lib/python3.5/site-packages/tensorflow/include -I/home/scw4750/anaconda3/envs/lcx_glow/lib/python3.5/site-packages/tensorflow/include/external/nsync/public -I/home/scw4750/anaconda3/envs/lcx_glow/include/python3.5m -c build/temp.linux-x86_64-3.5/test_compile/test_tensorflow_abi.cc -o build/temp.linux-x86_64-3.5/test_compile/test_tensorflow_abi.o
    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
    gcc -pthread -shared -L/home/scw4750/anaconda3/envs/lcx_glow/lib -Wl,-rpath=/home/scw4750/anaconda3/envs/lcx_glow/lib,--no-as-needed build/temp.linux-x86_64-3.5/test_compile/test_tensorflow_abi.o -L/home/scw4750/anaconda3/envs/lcx_glow/lib/python3.5/site-packages/tensorflow/core -L/home/scw4750/anaconda3/envs/lcx_glow/lib -o build/temp.linux-x86_64-3.5/test_compile/test_tensorflow_abi.so
    building 'horovod.tensorflow.mpi_lib' extension
    creating build/temp.linux-x86_64-3.5/horovod
    creating build/temp.linux-x86_64-3.5/horovod/tensorflow
    gcc -pthread -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/scw4750/anaconda3/envs/lcx_glow/include/python3.5m -c horovod/tensorflow/mpi_ops.cc -o build/temp.linux-x86_64-3.5/horovod/tensorflow/mpi_ops.o -std=c++11 -fPIC -O2 -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi -pthread -L/usr//lib -L/usr/lib/openmpi/lib -lmpi_cxx -lmpi -ldl -lhwloc -I/home/scw4750/anaconda3/envs/lcx_glow/lib/python3.5/site-packages/tensorflow/include -I/home/scw4750/anaconda3/envs/lcx_glow/lib/python3.5/site-packages/tensorflow/include/external/nsync/public -D_GLIBCXX_USE_CXX11_ABI=0
    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
    g++ -pthread -shared -L/home/scw4750/anaconda3/envs/lcx_glow/lib -Wl,-rpath=/home/scw4750/anaconda3/envs/lcx_glow/lib,--no-as-needed build/temp.linux-x86_64-3.5/horovod/tensorflow/mpi_ops.o -L/home/scw4750/anaconda3/envs/lcx_glow/lib -lpython3.5m -o build/lib.linux-x86_64-3.5/horovod/tensorflow/mpi_lib.cpython-35m-x86_64-linux-gnu.so -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi -pthread -L/usr//lib -L/usr/lib/openmpi/lib -lmpi_cxx -lmpi -ldl -lhwloc -L/home/scw4750/anaconda3/envs/lcx_glow/lib/python3.5/site-packages/tensorflow/core
    INFO: Above error indicates that this PyTorch installation does not support CUDA.
    INFO: Unable to build PyTorch plugin, will skip it.

    Traceback (most recent call last):
      File "/tmp/pip-install-qjvktya2/horovod/setup.py", line 572, in build_extensions
        build_torch_extension(self, options, abi_compile_flags)
      File "/tmp/pip-install-qjvktya2/horovod/setup.py", line 509, in build_torch_extension
        from torch.utils.ffi import create_extension
      File "/home/scw4750/anaconda3/envs/lcx_glow/lib/python3.5/site-packages/torch/utils/ffi/__init__.py", line 1, in <module>
        raise ImportError("torch.utils.ffi is deprecated. Please use cpp extensions instead.")
    ImportError: torch.utils.ffi is deprecated. Please use cpp extensions instead.

    building 'horovod.common.mpi_lib' extension
    creating build/temp.linux-x86_64-3.5/horovod/common
    gcc -pthread -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/scw4750/anaconda3/envs/lcx_glow/include/python3.5m -c horovod/common/common.cc -o build/temp.linux-x86_64-3.5/horovod/common/common.o -std=c++11 -fPIC -O2 -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi -pthread -L/usr//lib -L/usr/lib/openmpi/lib -lmpi_cxx -lmpi -ldl -lhwloc -D_GLIBCXX_USE_CXX11_ABI=0
    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
    gcc -pthread -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/scw4750/anaconda3/envs/lcx_glow/include/python3.5m -c horovod/common/mpi_message.cc -o build/temp.linux-x86_64-3.5/horovod/common/mpi_message.o -std=c++11 -fPIC -O2 -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi -pthread -L/usr//lib -L/usr/lib/openmpi/lib -lmpi_cxx -lmpi -ldl -lhwloc -D_GLIBCXX_USE_CXX11_ABI=0
    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
    gcc -pthread -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/scw4750/anaconda3/envs/lcx_glow/include/python3.5m -c horovod/common/operations.cc -o build/temp.linux-x86_64-3.5/horovod/common/operations.o -std=c++11 -fPIC -O2 -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi -pthread -L/usr//lib -L/usr/lib/openmpi/lib -lmpi_cxx -lmpi -ldl -lhwloc -D_GLIBCXX_USE_CXX11_ABI=0
    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
    In file included from /usr/lib/openmpi/include/mpi.h:253:0,
                     from horovod/common/operations.cc:36:
    /usr/lib/openmpi/include/mpi_portable_platform.h:374:34: warning: invalid suffix on literal; C++11 requires a space between literal and string macro [-Wliteral-suffix]
                  _STRINGIFY(__GNUC__)"."_STRINGIFY(__GNUC_MINOR__)"."_STRINGIFY(__GNUC_PATCHLEVEL__)
                                      ^
    /usr/lib/openmpi/include/mpi_portable_platform.h:374:63: warning: invalid suffix on literal; C++11 requires a space between literal and string macro [-Wliteral-suffix]
                  _STRINGIFY(__GNUC__)"."_STRINGIFY(__GNUC_MINOR__)"."_STRINGIFY(__GNUC_PATCHLEVEL__)
                                                                   ^
    horovod/common/operations.cc: In function ‘void horovod::common::{anonymous}::PerformOperation(horovod::common::{anonymous}::TensorTable&, horovod::common::MPIResponse)’:
    horovod/common/operations.cc:784:24: error: invalid conversion from ‘const void*’ to ‘void*’ [-fpermissive]
             e.tensor->data(), (int)e.tensor->shape().num_elements(),
                            ^
    In file included from horovod/common/operations.cc:36:0:
    /usr/lib/openmpi/include/mpi.h:1036:20: note: initializing argument 1 of ‘int MPI_Allgatherv(void*, int, MPI_Datatype, void*, int*, int*, MPI_Datatype, MPI_Comm)’
     OMPI_DECLSPEC  int MPI_Allgatherv(void *sendbuf, int sendcount, MPI_Datatype sendtype,
                        ^
    horovod/common/operations.cc:1106:54: error: invalid conversion from ‘const void*’ to ‘void*’ [-fpermissive]
                                   horovod_global.mpi_comm))
                                                          ^
    horovod/common/operations.cc:530:24: note: in definition of macro ‘MPI_CHECK’
         auto mpi_result = (op);                                                    \
                            ^
    In file included from horovod/common/operations.cc:36:0:
    /usr/lib/openmpi/include/mpi.h:1041:20: note: initializing argument 1 of ‘int MPI_Allreduce(void*, void*, int, MPI_Datatype, MPI_Op, MPI_Comm)’
     OMPI_DECLSPEC  int MPI_Allreduce(void *sendbuf, void *recvbuf, int count,
                        ^
    horovod/common/operations.cc: In function ‘void horovod::common::{anonymous}::BackgroundThreadLoop(horovod::common::{anonymous}::HorovodGlobalState&)’:
    horovod/common/operations.cc:1251:33: error: ‘MPI_COMM_TYPE_SHARED’ was not declared in this scope
       MPI_Comm_split_type(mpi_comm, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL,
                                     ^
    horovod/common/operations.cc:1252:34: error: ‘MPI_Comm_split_type’ was not declared in this scope
                           &local_comm);
                                      ^
    error: command 'gcc' failed with exit status 1
    ----------------------------------------
ERROR: Command errored out with exit status 1: /home/scw4750/anaconda3/envs/lcx_glow/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '"'"'/tmp/pip-install-qjvktya2/horovod/setup.py'"'"'; __file__='"'"'/tmp/pip-install-qjvktya2/horovod/setup.py'"'"';f=getattr(tokenize, '"'"'open'"'"', open)(__file__);code=f.read().replace('"'"'\r\n'"'"', '"'"'\n'"'"');f.close();exec(compile(code, __file__, '"'"'exec'"'"'))' install --record /tmp/pip-record-zkq5olrq/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.
Please give me some advice
Bumps [tensorflow-gpu](https://github.com/tensorflow/tensorflow) from 1.8.0 to 1.15.0.
<details>
<summary>Release notes</summary>

*Sourced from [tensorflow-gpu's releases](https://github.com/tensorflow/tensorflow/releases).*

> ## TensorFlow 1.15.0
> # Release 1.15.0
> This is the last 1.x release for TensorFlow. We do not expect to update the 1.x branch with features, although we will issue patch releases to fix vulnerabilities for at least one year.
> 
> ## Major Features and Improvements
> * As [announced](https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/iRCt5m4qUz0), `tensorflow` pip package will by default include GPU support (same as `tensorflow-gpu` now) for the platforms we currently have GPU support (Linux and Windows). It will work on machines with and without Nvidia GPUs. `tensorflow-gpu` will still be available, and CPU-only packages can be downloaded at `tensorflow-cpu` for users who are concerned about package size.
> * TensorFlow 1.15 contains a complete implementation of the 2.0 API in its `compat.v2` module. It contains a copy of the 1.15 main module (without `contrib`) in the `compat.v1` module. TensorFlow 1.15 is able to emulate 2.0 behavior using the `enable_v2_behavior()` function.
> This enables writing forward compatible code: by explicitly importing either `tensorflow.compat.v1` or `tensorflow.compat.v2`, you can ensure that your code works without modifications against an installation of 1.15 or 2.0.
> * `EagerTensor` now supports numpy buffer interface for tensors.
> * Add toggles `tf.enable_control_flow_v2()` and `tf.disable_control_flow_v2()` for enabling/disabling v2 control flow.
> * Enable v2 control flow as part of `tf.enable_v2_behavior()` and `TF2_BEHAVIOR=1`.
> * AutoGraph translates Python control flow into TensorFlow expressions, allowing users to write regular Python inside `tf.function`-decorated functions. AutoGraph is also applied in functions used with `tf.data`, `tf.distribute` and `tf.keras` APIS.
> * Adds `enable_tensor_equality()`, which switches the behavior such that: 
>   * Tensors are no longer hashable.
>   * Tensors can be compared with `==` and `!=`, yielding a Boolean Tensor with element-wise comparison results. This will be the default behavior in 2.0.
> * Auto Mixed-Precision graph optimizer simplifies converting models to `float16` for acceleration on Volta and Turing Tensor Cores. This feature can be enabled by wrapping an optimizer class with `tf.train.experimental.enable_mixed_precision_graph_rewrite()`.
> * Add environment variable `TF_CUDNN_DETERMINISTIC`. Setting to "true" or "1" forces the selection of deterministic cuDNN convolution and max-pooling algorithms. When this is enabled, the algorithm selection procedure itself is also deterministic.
> * TensorRT
>   * Migrate TensorRT conversion sources from contrib to compiler directory in preparation for TF 2.0.
>   * Add additional, user friendly `TrtGraphConverter` API for TensorRT conversion.
>   * Expand support for TensorFlow operators in TensorRT conversion (e.g.
>     `Gather`, `Slice`, `Pack`, `Unpack`, `ArgMin`, `ArgMax`,`DepthSpaceShuffle`). 
>   * Support TensorFlow operator `CombinedNonMaxSuppression` in TensorRT conversion which 
>      significantly accelerates object detection models.
> 
> ## Breaking Changes
> * Tensorflow code now produces 2 different pip packages: `tensorflow_core` containing all the code (in the future it will contain only the private implementation) and `tensorflow` which is a virtual pip package doing forwarding to `tensorflow_core` (and in the future will contain only the public API of tensorflow). We don't expect this to be breaking, unless you were importing directly from the implementation.
> * TensorFlow 1.15 is built using devtoolset7 (GCC7) on Ubuntu 16. This may lead to ABI incompatibilities with extensions built against earlier versions of TensorFlow.
> * Deprecated the use of `constraint=` and `.constraint` with ResourceVariable.
> * `tf.keras`:
>   * `OMP_NUM_THREADS` is no longer used by the default Keras config. To configure the number of threads, use `tf.config.threading` APIs.
>   * `tf.keras.model.save_model` and `model.save` now defaults to saving a TensorFlow SavedModel.
>   * `keras.backend.resize_images` (and consequently, `keras.layers.Upsampling2D`) behavior has changed, a bug in the resizing implementation was fixed.
>   * Layers now default to `float32`, and automatically cast their inputs to the layer's dtype. If you had a model that used `float64`, it will probably silently use `float32` in TensorFlow2, and a warning will be issued that starts with Layer "layer-name" is casting an input tensor from dtype float64 to the layer's dtype of float32. To fix, either set the default dtype to float64 with `tf.keras.backend.set_floatx('float64')`, or pass `dtype='float64'` to each of the Layer constructors. See `tf.keras.layers.Layer` for more information.
>   * Some `tf.assert_*` methods now raise assertions at operation creation time (i.e. when this Python line executes) if the input tensors' values are known at that time, not during the session.run(). When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).
> 
> ## Bug Fixes and Other Changes
> * `tf.estimator`:
>   * `tf.keras.estimator.model_to_estimator` now supports exporting to `tf.train.Checkpoint` format, which allows the saved checkpoints to be compatible with `model.load_weights`.
>   * Fix tests in canned estimators.
>   * Expose Head as public API.
>   * Fixes critical bugs that help with `DenseFeatures` usability in TF2
> * `tf.data`:
>   * Promoting `unbatch` from experimental to core API.
>   * Adding support for datasets as inputs to `from_tensors` and `from_tensor_slices` and batching and unbatching of nested datasets.
> * `tf.keras`:
>   * `tf.keras.estimator.model_to_estimator` now supports exporting to tf.train.Checkpoint format, which allows the saved checkpoints to be compatible with `model.load_weights`.
>   * Saving a Keras Model using `tf.saved_model.save` now saves the list of variables, trainable variables, regularization losses, and the call function.
>   * Deprecated `tf.keras.experimental.export_saved_model` and `tf.keras.experimental.function`. Please use `tf.keras.models.save_model(..., save_format='tf')` and `tf.keras.models.load_model` instead.
>   * Add an `implementation=3` mode for `tf.keras.layers.LocallyConnected2D` and `tf.keras.layers.LocallyConnected1D` layers using `tf.SparseTensor` to store weights,  allowing a dramatic speedup for large sparse models.
></tr></table> ... (truncated)
</details>
<details>
<summary>Changelog</summary>

*Sourced from [tensorflow-gpu's changelog](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md).*

> # Release 1.15.0
> This is the last 1.x release for TensorFlow. We do not expect to update the 1.x branch with features, although we will issue patch releases to fix vulnerabilities for at least one year. 
> 
> ## Major Features and Improvements
> * As [announced](https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/iRCt5m4qUz0), `tensorflow` pip package will by default include GPU support (same as `tensorflow-gpu` now) for the platforms we currently have GPU support (Linux and Windows). It will work on machines with and without Nvidia GPUs. `tensorflow-gpu` will still be available, and CPU-only packages can be downloaded at `tensorflow-cpu` for users who are concerned about package size.
> * TensorFlow 1.15 contains a complete implementation of the 2.0 API in its `compat.v2` module. It contains a copy of the 1.15 main module (without `contrib`) in the `compat.v1` module. TensorFlow 1.15 is able to emulate 2.0 behavior using the `enable_v2_behavior()` function.
> This enables writing forward compatible code: by explicitly importing either `tensorflow.compat.v1` or `tensorflow.compat.v2`, you can ensure that your code works without modifications against an installation of 1.15 or 2.0.
> * EagerTensor now supports numpy buffer interface for tensors.
> * Add toggles `tf.enable_control_flow_v2()` and `tf.disable_control_flow_v2()` for enabling/disabling v2 control flow.
> * Enable v2 control flow as part of `tf.enable_v2_behavior()` and `TF2_BEHAVIOR=1`.
> * AutoGraph translates Python control flow into TensorFlow expressions, allowing users to write regular Python inside `tf.function`-decorated functions. AutoGraph is also applied in functions used with `tf.data`, `tf.distribute` and `tf.keras` APIS.
> * Adds `enable_tensor_equality()`, which switches the behavior such that: 
>   * Tensors are no longer hashable.
>   * Tensors can be compared with `==` and `!=`, yielding a Boolean Tensor with element-wise comparison results. This will be the default behavior in 2.0.
> 
> ## Breaking Changes
> * Tensorflow code now produces 2 different pip packages: `tensorflow_core` containing all the code (in the future it will contain only the private implementation) and `tensorflow` which is a virtual pip package doing forwarding to `tensorflow_core` (and in the future will contain only the public API of tensorflow). We don't expect this to be breaking, unless you were importing directly from the implementation.
> * TensorFlow 1.15 is built using devtoolset7 (GCC7) on Ubuntu 16. This may lead to ABI incompatibilities with extensions built against earlier versions of TensorFlow.
> * Deprecated the use of `constraint=` and `.constraint` with ResourceVariable.
> * `tf.keras`:
>   * `OMP_NUM_THREADS` is no longer used by the default Keras config. To configure the number of threads, use `tf.config.threading` APIs.
>   * `tf.keras.model.save_model` and `model.save` now defaults to saving a TensorFlow SavedModel.
>   * `keras.backend.resize_images` (and consequently, `keras.layers.Upsampling2D`) behavior has changed, a bug in the resizing implementation was fixed.
>   * Layers now default to `float32`, and automatically cast their inputs to the layer's dtype. If you had a model that used `float64`, it will probably silently use `float32` in TensorFlow2, and a warning will be issued that starts with Layer "layer-name" is casting an input tensor from dtype float64 to the layer's dtype of float32. To fix, either set the default dtype to float64 with `tf.keras.backend.set_floatx('float64')`, or pass `dtype='float64'` to each of the Layer constructors. See `tf.keras.layers.Layer` for more information.
>   * Some `tf.assert_*` methods now raise assertions at operation creation time (i.e. when this Python line executes) if the input tensors' values are known at that time, not during the session.run(). When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).
> 
> ## Bug Fixes and Other Changes
> * `tf.estimator`:
>   * `tf.keras.estimator.model_to_estimator` now supports exporting to `tf.train.Checkpoint` format, which allows the saved checkpoints to be compatible with `model.load_weights`.
>   * Fix tests in canned estimators.
>   * Expose Head as public API.
>   * Fixes critical bugs that help with `DenseFeatures` usability in TF2
> * `tf.data`:
>   * Promoting `unbatch` from experimental to core API.
>   * Adding support for datasets as inputs to `from_tensors` and `from_tensor_slices` and batching and unbatching of nested datasets.
> * `tf.keras`:
>   * `tf.keras.estimator.model_to_estimator` now supports exporting to tf.train.Checkpoint format, which allows the saved checkpoints to be compatible with `model.load_weights`.
>   * Saving a Keras Model using `tf.saved_model.save` now saves the list of variables, trainable variables, regularization losses, and the call function.
>   * Deprecated `tf.keras.experimental.export_saved_model` and `tf.keras.experimental.function`. Please use `tf.keras.models.save_model(..., save_format='tf')` and `tf.keras.models.load_model` instead.
>   * Add an `implementation=3` mode for `tf.keras.layers.LocallyConnected2D` and `tf.keras.layers.LocallyConnected1D` layers using `tf.SparseTensor` to store weights,  allowing a dramatic speedup for large sparse models.
>   * Enable the Keras compile API `experimental_run_tf_function` flag by default. This flag enables single training/eval/predict execution path. With this 1. All input types are converted to `Dataset`. 2. When distribution strategy is not specified this goes through the no-op distribution strategy path. 3. Execution is wrapped in tf.function unless `run_eagerly=True` is set in compile.
>   * Raise error if `batch_size` argument is used when input is dataset/generator/keras sequence.
> * `tf.lite`
>   * Add `GATHER` support to NN API delegate.
>   * tflite object detection script has a debug mode.
>   * Add delegate support for `QUANTIZE`.
>   * Added evaluation script for COCO minival.
>   * Add delegate support for `QUANTIZED_16BIT_LSTM`.
>   * Converts hardswish subgraphs into atomic ops.
> * Add support for defaulting the value of `cycle_length` argument of `tf.data.Dataset.interleave` to the number of schedulable CPU cores.
></tr></table> ... (truncated)
</details>
<details>
<summary>Commits</summary>

- [`590d6ee`](https://github.com/tensorflow/tensorflow/commit/590d6eef7e91a6a7392c8ffffb7b58f2e0c8bc6b) Merge pull request [#31861](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/31861) from tensorflow-jenkins/relnotes-1.15.0rc0-16184
- [`b27ac43`](https://github.com/tensorflow/tensorflow/commit/b27ac431aa37cfeb9d5c35cc50081cdb6763a40e) Update RELEASE.md
- [`07bf663`](https://github.com/tensorflow/tensorflow/commit/07bf6634f602757ef0b2106a92c519d09e80157e) Merge pull request [#33213](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/33213) from Intel-tensorflow/mkl-dnn-0.20.6
- [`46f50ff`](https://github.com/tensorflow/tensorflow/commit/46f50ff8a0f099269ac29573bc6ac09d1bc6cab7) Merge pull request [#33262](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/33262) from tensorflow/ggadde-1-15-cp2
- [`49c154e`](https://github.com/tensorflow/tensorflow/commit/49c154e17e9fdfe008f8b0b929d1a729e5939c51) Merge pull request [#33263](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/33263) from tensorflow/ggadde-1-15-final-version
- [`a16adeb`](https://github.com/tensorflow/tensorflow/commit/a16adeb793b587a08958a72cbbf0d338e063a042) Update TensorFlow version to 1.15.0 in preparation for final relase.
- [`8d71a87`](https://github.com/tensorflow/tensorflow/commit/8d71a87b0e3de6d07588f9139660a77271d12498) Add saving of loaded/trained compatibility models in test and fix a compatibi...
- [`8c48aff`](https://github.com/tensorflow/tensorflow/commit/8c48affdf8ec0e5a9c5252f88e63aa5b97daf239) [Intel Mkl] Upgrading MKL-DNN to 0.20.6 to fix SGEMM regression
- [`38ea9bb`](https://github.com/tensorflow/tensorflow/commit/38ea9bbfea423eb968fcc70bc454471277c9537c) Merge pull request [#33120](https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/33120) from tensorflow/perf
- [`a8ef0f5`](https://github.com/tensorflow/tensorflow/commit/a8ef0f5d3bff3fe6f46b821832a4e9073dd7c01d) Automated rollback of commit db7e43192d405973c6c50f6e60e831a198bb4a49
- Additional commits viewable in [compare view](https://github.com/tensorflow/tensorflow/compare/v1.8.0...v1.15.0)
</details>
<br />

[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-gpu&package-manager=pip&previous-version=1.8.0&new-version=1.15.0)](https://help.github.com/articles/configuring-automated-security-fixes)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot ignore this [patch|minor|major] version` will close this PR and stop Dependabot creating any more for this minor/major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/openai/glow/network/alerts).

</details>
Hi, quick issue with mpiexec.  Without it the program runs fine with 1 gpu (am running Horovod within a Docker container), but mpiexec hangs whenever it's invoked.  

I ran a strace and it hangs after this sequence of creating pads; any hints would be appreciated!

`
write(1, "Creating pad 1_1_6_6\n", 21)  = 21
poll([{fd=5, events=POLLIN}, {fd=4, events=POLLIN}, {fd=7, events=POLLIN}, {fd=23, events=POLLIN}, {fd=30, events=POLLIN}, {fd=28, events=POLLIN}, {fd=0, events=POLLIN}, {fd=32, events=POLLIN}, {fd=24, events=POLLIN}], 9, -1) = 1 ([{fd=24, revents=POLLIN}])
read(24, "Creating pad 1_1_4_4\n", 4096) = 21
poll([{fd=5, events=POLLIN}, {fd=4, events=POLLIN}, {fd=7, events=POLLIN}, {fd=23, events=POLLIN}, {fd=30, events=POLLIN}, {fd=28, events=POLLIN}, {fd=0, events=POLLIN}, {fd=32, events=POLLIN}, {fd=24, events=POLLIN}], 9, 0) = 0 (Timeout)
write(1, "Creating pad 1_1_4_4\n", 21)  = 21
poll([{fd=5, events=POLLIN}, {fd=4, events=POLLIN}, {fd=7, events=POLLIN}, {fd=23, events=POLLIN}, {fd=30, events=POLLIN}, {fd=28, events=POLLIN}, {fd=0, events=POLLIN}, {fd=32, events=POLLIN}, {fd=24, events=POLLIN}], 9, -1
`
Bumps [pillow](https://github.com/python-pillow/Pillow) from 5.2.0 to 6.2.0.
<details>
<summary>Release notes</summary>

*Sourced from [pillow's releases](https://github.com/python-pillow/Pillow/releases).*

> ## 6.2.0
> https://pillow.readthedocs.io/en/stable/releasenotes/6.2.0.html
> 
> ## 6.1.0
> https://pillow.readthedocs.io/en/stable/releasenotes/6.1.0.html
> 
> ## 6.0.0
> No release notes provided.
> 
> ## 5.4.1
> No release notes provided.
> 
> ## 5.4.0
> No release notes provided.
> 
> ## 5.3.0
> No release notes provided.
</details>
<details>
<summary>Changelog</summary>

*Sourced from [pillow's changelog](https://github.com/python-pillow/Pillow/blob/master/CHANGES.rst).*

> 6.2.0 (2019-10-01)
> ------------------
> 
> - Catch buffer overruns [#4104](https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4104)
>   [radarhere]
> 
> - Initialize rows_per_strip when RowsPerStrip tag is missing [#4034](https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4034)
>   [cgohlke, radarhere]
> 
> - Raise error if TIFF dimension is a string [#4103](https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4103)
>   [radarhere]
> 
> - Added decompression bomb checks [#4102](https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4102)
>   [radarhere]
> 
> - Fix ImageGrab.grab DPI scaling on Windows 10 version 1607+ [#4000](https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4000)
>   [nulano, radarhere]
> 
> - Corrected negative seeks [#4101](https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4101)
>   [radarhere]
> 
> - Added argument to capture all screens on Windows [#3950](https://github-redirect.dependabot.com/python-pillow/Pillow/issues/3950)
>   [nulano, radarhere]
> 
> - Updated warning to specify when Image.frombuffer defaults will change [#4086](https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4086)
>   [radarhere]
> 
> - Changed WindowsViewer format to PNG [#4080](https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4080)
>   [radarhere]
> 
> - Use TIFF orientation [#4063](https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4063)
>   [radarhere]
> 
> - Raise the same error if a truncated image is loaded a second time [#3965](https://github-redirect.dependabot.com/python-pillow/Pillow/issues/3965)
>   [radarhere]
> 
> - Lazily use ImageFileDirectory_v1 values from Exif [#4031](https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4031)
>   [radarhere]
> 
> - Improved HSV conversion [#4004](https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4004)
>   [radarhere]
> 
> - Added text stroking [#3978](https://github-redirect.dependabot.com/python-pillow/Pillow/issues/3978)
>   [radarhere, hugovk]
> 
> - No more deprecated bdist_wininst .exe installers [#4029](https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4029)
>   [hugovk]
> 
> - Do not allow floodfill to extend into negative coordinates [#4017](https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4017)
>   [radarhere]
></tr></table> ... (truncated)
</details>
<details>
<summary>Commits</summary>

- [`8a30d13`](https://github.com/python-pillow/Pillow/commit/8a30d135378dc6a1c3c08fa4bb9fbc15370feedf) Updated CHANGES.rst [ci skip]
- [`75602d1`](https://github.com/python-pillow/Pillow/commit/75602d12e1b6f2152ab5bd1acfb62a9c8a4a0432) 6.2.0 version bump
- [`4756af9`](https://github.com/python-pillow/Pillow/commit/4756af9c1027ae620eaa9538d6b0dd9b0e844fca) Updated CHANGES.rst [ci skip]
- [`cc16025`](https://github.com/python-pillow/Pillow/commit/cc16025e234b7a7a4dd3a86d2fdc0980698db9cc) Merge pull request [#4104](https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4104) from radarhere/overrun
- [`fb84701`](https://github.com/python-pillow/Pillow/commit/fb8470187a45043c33b1c75e7dca48b38d5db7a6) Merge pull request [#4034](https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4034) from cgohlke/patch-1
- [`b9693a5`](https://github.com/python-pillow/Pillow/commit/b9693a51c99c260bd66d1affeeab4a226cf7e5a5) Merge pull request [#4103](https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4103) from radarhere/dimension
- [`f228d0c`](https://github.com/python-pillow/Pillow/commit/f228d0ccbf6bf9392d7fcd51356ef2cfda80c75a) Merge pull request [#4102](https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4102) from radarhere/decompression
- [`aaf2c42`](https://github.com/python-pillow/Pillow/commit/aaf2c421564fcf96bd030487f09b648f7feb7b67) Merge pull request [#4000](https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4000) from nulano/dpi_fix
- [`b36c1bc`](https://github.com/python-pillow/Pillow/commit/b36c1bc943d554ba223086c7efb502d080f73905) Merge pull request [#4101](https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4101) from radarhere/negative_seek
- [`9a977b9`](https://github.com/python-pillow/Pillow/commit/9a977b975cd871ef9a9128b72414c0de3a292591) Raise error if dimension is a string
- Additional commits viewable in [compare view](https://github.com/python-pillow/Pillow/compare/5.2.0...6.2.0)
</details>
<br />

[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pillow&package-manager=pip&previous-version=5.2.0&new-version=6.2.0)](https://help.github.com/articles/configuring-automated-security-fixes)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot ignore this [patch|minor|major] version` will close this PR and stop Dependabot creating any more for this minor/major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/openai/glow/network/alerts).

</details>
hi,
https://github.com/openai/glow/blob/eaff2177693a5d84a1cf8ae19e8e0441715b82f8/model.py#L576-L584
Why use cross h to get mean and logs? Is there any difference to get mean and logs from its contiguous splits of h?
I guess maybe you want to  "cross split" the featuremap to get the z1, z2 in the previous lines:
https://github.com/openai/glow/blob/eaff2177693a5d84a1cf8ae19e8e0441715b82f8/model.py#L545-L555
As feature feed to split2d are concated from two parts of formerly revnet2d layer, the first half and second half of which are not that equal, doing a "cross split" to get z1, z2 may be a better choice.
Hi, there! I am trying to calculate the log-likelihood of data points.

To do so, I directly modified the provided loss function:
<img width="557" alt="Screen Shot 2019-09-20 at 3 31 48 PM" src="https://user-images.githubusercontent.com/25020683/65308055-cf9c3900-dbbb-11e9-9de2-0fe6d3c8dd13.png">

Then, I tested it on the cifar-10 dataset, and got the following:
![image](https://user-images.githubusercontent.com/25020683/65308207-2275f080-dbbc-11e9-9173-15af03635c19.png)

This result is similar to the one in the paper: https://arxiv.org/abs/1810.09136 "DO DEEP GENERATIVE MODELS KNOW WHAT THEY DON’T KNOW", so I assumed the "logpX" function is correct.

[Problem: ] 
The question is, when I sampled from cifar-10, it seems that the sampled images have overall higher log-likelihood than the real images: (std here has unit 0.1, i.e. 9 implies 0.9. sry for ambiguity)
![image](https://user-images.githubusercontent.com/25020683/65308544-e1321080-dbbc-11e9-9979-938c11ef53ea.png)
![image](https://user-images.githubusercontent.com/25020683/65308553-e3946a80-dbbc-11e9-8c92-eb3770ad7ed8.png)
![image](https://user-images.githubusercontent.com/25020683/65308561-e68f5b00-dbbc-11e9-8df9-fe37f8c03ebe.png)
![image](https://user-images.githubusercontent.com/25020683/65308567-ea22e200-dbbc-11e9-9b8c-36318c9f3329.png)

I am not sure why this happens. Is this a bug, or it is desired? Thank you in advance!




The article briefly describe the architecture, and refer to Real NVP for more details. 

![image](https://user-images.githubusercontent.com/8614529/62634459-37f9c880-b936-11e9-930d-00ae88cc4289.png)

This lead me to believe Glow implemented the Squeeze operation as done by Real NVP. It turns out this is not the case, please see the images below. 

**GLOW**: 
![image](https://user-images.githubusercontent.com/8614529/62634743-afc7f300-b936-11e9-85d6-49b0438e451d.png)

**RealNVP**:
![image](https://user-images.githubusercontent.com/8614529/62634718-a3439a80-b936-11e9-8036-558dd28261db.png)

This can be fixed by changing a single line in ```squeeze2d(..)``` which can be found in tfops.py. 

```
#x = tf.transpose(x, [0, 1, 3, 5, 2, 4]) # differs to RealNVP
x = tf.transpose(x, [0, 1, 3, 2, 4, 5]) # the same as RealNVP
```

Please not similar changes needs to be done for ```unsqueeze2d(..)```. 

Does anyone know if this was intentional? It might be that mixing up channels this way slightly improves performance? 
The flow based model is very very interesting. But it is too GPU hungry. 
I see that some pretrained models are uploaded.
Now I have a question about the pretrained model on CelebA.
For unoptimized model, the input for encoding includes three tensors:
`'enc_x': 'input/image',
'enc_x_d': 'input/downsampled_image',
'enc_y': 'input/label'`
I want to know the relationship between enc_x and enc_x_d. 
If I want to test the model on 32x32 CelebA, how should I use this pretrained model?
In order to perform inference on the trained model the input and output nodes for encode/decode must be known. Does anyone know how to figure this out / what the naming scheme is for a generic model with given hyperparameters? I've tried visualizing my trained model in tensorflow but the graph is extremely cluttered compared to that of the pretrained model making it impossible to decipher outputs / input nodes. Any help would be appreciated. 
i want to train on imagenet dataset,but i can't load the dataset, where shoud i change the correspoding code