Turns out that reducing a whole bunch of numbers to a single value (average for example, or median) means that we're getting rid of a huge amount of information. One way to add context back in without drowning users in raw data is to include a histogram in the output. 

With histograms in the output the user can see the distributions of their two runs to make better informed decisions about the validity of the data.

Here's an example histogram:

```
                              Histogram
              ‚îå                                        ‚îê
   [3.1, 3.2) ‚î§‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 23
   [3.2, 3.3) ‚î§‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 14
   [3.3, 3.4) ‚î§‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 5
   [3.4, 3.5) ‚î§‚ñá‚ñá‚ñá‚ñá‚ñá 3
   [3.5, 3.6) ‚î§‚ñá‚ñá 1
   [3.6, 3.7) ‚î§‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 5
   [3.7, 3.8) ‚î§‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 8
   [3.8, 3.9) ‚î§‚ñá‚ñá‚ñá 2
              ‚îî                                        ‚îò
                              Frequency
```

Here's how the output looks in a test run:

```
üëéüëéüëé(NOT Statistically Significant) üëéüëéüëé

[3054e1d584] "Merge pull request #36506 from kamipo/group_by_with_order_by_virtual_count_attribute" - (0.0201745 seconds)
  SLOWER üê¢üê¢üê¢ by:
      0.8833x [older/newer]
    -13.2063% [(older - newer) / older * 100]
[80f989aece] "Remove duplicated attribute alias resolution in `_select!`" - (0.017821 seconds)

Iterations per sample: 10
Samples: 2
Test type: Kolmogorov Smirnov
Confidence level: 95.0 %
Is significant? (max > critical): false
D critical: 1.7308183826022854
D max: 1.0

Histogram - [3054e1d584] "Merge pull request #36506 from kamipo/group_by_with_order_by_virtual_count_attribute"
                  ‚îå                                        ‚îê
   [0.015, 0.02 ) ‚î§‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 1
   [0.02 , 0.025) ‚î§‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 1
                  ‚îî                                        ‚îò
                                  Frequency

Histogram - [80f989aece] "Remove duplicated attribute alias resolution in `_select!`"
                  ‚îå                                        ‚îê
   [0.017, 0.018) ‚î§‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 1
   [0.018, 0.019) ‚î§‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 1
                  ‚îî                                        ‚îò
                                  Frequency


Output: tmp/compare_branches/2020-01-06-12-23-1578335006-352084000/results.txt
```

> Note it's not that interesting with only two samples



## Caveats

The histograms that are presented do not have identical/correct axis. So while the shape of the histograms are correct in comparison to each other, you cannot place them side by side for an accurate representation because they have different bin sizes and start from a different  value. 

In the short term I do think that the visual data being present with this caveat is better than nothing. In the long term support would need to be added to `unicode_plot` to support this behavior.

## Blockers

There is a bug that raises an error when 2 values are given to generate a histogram if the values have some special undetermined relationship. This is tracked here:

https://github.com/red-data-tools/unicode_plot.rb/issues/24
Need a parser that can accommodate this:

```
  3.635861   0.271070   3.957706 (  4.451950)
  3.658097   0.259989   3.956018 (  6.375668)
  3.645830   0.300098   3.985729 (  4.479861)
  3.717054   0.257627   4.012474 (  4.437075)
  3.518767   0.284179   3.850532 (  4.533361)
  3.647776   0.250607   3.949652 (  6.333169)
  3.650700   0.255125   3.944806 (  4.331832)
  3.599905   0.248381   3.890607 (  4.270693)
  3.589013   0.248963   3.882700 ( 14.741302)
  3.549133   0.243314   3.831065 (  4.752615)
  3.637030   0.265008   3.938134 (  4.583956)
  3.707378   0.262794   4.010824 ( 14.491934)
  3.600069   0.251012   3.887343 (  4.750288)
  3.614917   0.281785   3.935370 (  4.726997)
  3.614017   0.245371   3.895933 (  7.255681)
  3.513302   0.241565   3.791374 (  4.597545)
  3.481929   0.237348   3.764888 (  4.127399)
  3.681426   0.257285   3.976662 (  4.760247)
  3.651148   0.263395   3.952579 (  5.545362)
  3.450599   0.242363   3.728665 (  4.137846)
  3.468851   0.249998   3.766952 (  4.134255)
</html>
  3.608046   0.259281   3.906391 (  4.704833)
  3.695854   0.261036   3.993135 (  4.539016)
  3.474631   0.237228   3.747401 (  4.112216)
  3.658769   0.261520   3.960668 (  4.828371)
  3.520887   0.236373   3.797928 (  4.165172)
  3.461020   0.242898   3.744059 (  4.146483)
```

I think this happens when i'm running a benchmark and my computer goes to sleep and then I wake it up again, but i'm not sure. I guess if that's the case i'm not sure if we should abort or plow through.


The goal of `perf:library` was to be able to provide a pretty solid single point where benchmarks could be run and a single, trustworthy output could be generated.

I've made progress on that goal, but we're not quite there yet. For example I ran the same test 10 different times last night and here's what I saw (on code triage):

```
$ env SHAS_TO_TEST='6aab468,c321bd2' DERAILED_PATH_TO_LIBRARY='.' bundle exec derailed exec perf:library
```

Executed 10 times gave me:

```
üëéüëéüëé(NOT Statistically Significant) üëéüëéüëé

[6aab468] "Fstring gem" - (3.225173 seconds)
  SLOWER üê¢üê¢üê¢ by:
    0.9922x [older/newer]
    -0.7862% [(older - newer) / older * 100]
[c321bd2] "Merge pull request #1188 from codetriage/schneems/upsert-doc-methods" - (3.2000145 seconds)

Iterations per sample: 200
Samples: 200

Test type: Kolmogorov Smirnov
Is significant? (max > critical): false
D critical: 0.12238734153404084
D max: 0.09999999999999998

üëéüëéüëé(NOT Statistically Significant) üëéüëéüëé

[6aab468] "Fstring gem" - (3.210941 seconds)
  FASTER üöÄüöÄüöÄ by:
    1.0020x [older/newer]
    0.1978% [(older - newer) / older * 100]
[c321bd2] "Merge pull request #1188 from codetriage/schneems/upsert-doc-methods" - (3.2173045 seconds)

Iterations per sample: 200
Samples: 200

Test type: Kolmogorov Smirnov
Is significant? (max > critical): false
D critical: 0.12238734153404084
D max: 0.05999999999999994

üëéüëéüëé(NOT Statistically Significant) üëéüëéüëé

[6aab468] "Fstring gem" - (3.208945 seconds)
  SLOWER üê¢üê¢üê¢ by:
    0.9993x [older/newer]
    -0.0660% [(older - newer) / older * 100]
[c321bd2] "Merge pull request #1188 from codetriage/schneems/upsert-doc-methods" - (3.206828 seconds)

Iterations per sample: 200
Samples: 200

Test type: Kolmogorov Smirnov
Is significant? (max > critical): false
D critical: 0.12238734153404084
D max: 0.05999999999999994

üëéüëéüëé(NOT Statistically Significant) üëéüëéüëé

[6aab468] "Fstring gem" - (3.2028555 seconds)
  FASTER üöÄüöÄüöÄ by:
    1.0012x [older/newer]
    0.1245% [(older - newer) / older * 100]
[c321bd2] "Merge pull request #1188 from codetriage/schneems/upsert-doc-methods" - (3.2068465 seconds)

Iterations per sample: 200
Samples: 200

Test type: Kolmogorov Smirnov
Is significant? (max > critical): false
D critical: 0.12238734153404084
D max: 0.07500000000000001

‚ù§Ô∏è ‚ù§Ô∏è ‚ù§Ô∏è  (Statistically Significant) ‚ù§Ô∏è ‚ù§Ô∏è ‚ù§Ô∏è

[6aab468] "Fstring gem" - (3.186942 seconds)
  FASTER üöÄüöÄüöÄ by:
    1.0083x [older/newer]
    0.8270% [(older - newer) / older * 100]
[c321bd2] "Merge pull request #1188 from codetriage/schneems/upsert-doc-methods" - (3.213518 seconds)

Iterations per sample: 200
Samples: 105

Test type: Kolmogorov Smirnov
Is significant? (max > critical): true
D critical: 0.16891057858422326
D max: 0.2380952380952381

üëéüëéüëé(NOT Statistically Significant) üëéüëéüëé

[6aab468] "Fstring gem" - (3.200376 seconds)
  FASTER üöÄüöÄüöÄ by:
    1.0054x [older/newer]
    0.5373% [(older - newer) / older * 100]
[c321bd2] "Merge pull request #1188 from codetriage/schneems/upsert-doc-methods" - (3.217666 seconds)

Iterations per sample: 200
Samples: 200

Test type: Kolmogorov Smirnov
Is significant? (max > critical): false
D critical: 0.12238734153404084
D max: 0.10000000000000003

üëéüëéüëé(NOT Statistically Significant) üëéüëéüëé

[6aab468] "Fstring gem" - (3.204838 seconds)
  FASTER üöÄüöÄüöÄ by:
    1.0013x [older/newer]
    0.1256% [(older - newer) / older * 100]
[c321bd2] "Merge pull request #1188 from codetriage/schneems/upsert-doc-methods" - (3.2088685 seconds)

Iterations per sample: 200
Samples: 200

Test type: Kolmogorov Smirnov
Is significant? (max > critical): false
D critical: 0.12238734153404084
D max: 0.095

üëéüëéüëé(NOT Statistically Significant) üëéüëéüëé

[6aab468] "Fstring gem" - (3.208226 seconds)
  SLOWER üê¢üê¢üê¢ by:
    0.9973x [older/newer]
    -0.2692% [(older - newer) / older * 100]
[c321bd2] "Merge pull request #1188 from codetriage/schneems/upsert-doc-methods" - (3.199613 seconds)

Iterations per sample: 200
Samples: 200

Test type: Kolmogorov Smirnov
Is significant? (max > critical): false
D critical: 0.12238734153404084
D max: 0.07


‚ù§Ô∏è ‚ù§Ô∏è ‚ù§Ô∏è  (Statistically Significant) ‚ù§Ô∏è ‚ù§Ô∏è ‚ù§Ô∏è

[6aab468] "Fstring gem" - (3.165487 seconds)
  FASTER üöÄüöÄüöÄ by:
    1.0200x [older/newer]
    1.9636% [(older - newer) / older * 100]
[c321bd2] "Merge pull request #1188 from codetriage/schneems/upsert-doc-methods" - (3.228888 seconds)

Iterations per sample: 200
Samples: 61

Test type: Kolmogorov Smirnov
Is significant? (max > critical): true
D critical: 0.22160858543889803
D max: 0.26229508196721313

‚ù§Ô∏è ‚ù§Ô∏è ‚ù§Ô∏è  (Statistically Significant) ‚ù§Ô∏è ‚ù§Ô∏è ‚ù§Ô∏è

[6aab468] "Fstring gem" - (3.186347 seconds)
  FASTER üöÄüöÄüöÄ by:
    1.0100x [older/newer]
    0.9945% [(older - newer) / older * 100]
[c321bd2] "Merge pull request #1188 from codetriage/schneems/upsert-doc-methods" - (3.2183525 seconds)

Iterations per sample: 200
Samples: 176

Test type: Kolmogorov Smirnov
Is significant? (max > critical): true
D critical: 0.1304653444395813
D max: 0.17613636363636365

```

## Results of 10 runs

* 7 not significant
    * Faster - 4
    * Slower - 3
* 3 significant
    * Faster - 3
    * Slower - None

So these results aren't too bad. We are consistent in that it seems the runs that say "statistically significant" are all pointing in the right direction (saying the code got faster).

Based off of this set you could say "if it's not significant, then discard the results" and you would end up with 3 tests showing the same thing, which is good.

However here's an older case that I had with the exact same code commits but it was showing statistically significant and the results did not agree with the other 3 tests.

```
‚ù§Ô∏è ‚ù§Ô∏è ‚ù§Ô∏è  (Statistically Significant) ‚ù§Ô∏è ‚ù§Ô∏è ‚ù§Ô∏è

[6aab468] "Fstring gem" - (4.445098688172043 seconds)
  SLOWER by:
    0.7744x [older/newer]
    -29.1292% [(older - newer) / older * 100]
[c321bd2] "Merge pull request #1188 from codetriage/schneems/upsert-doc-methods" - (3.442366107526882 seconds)

Iterations per sample: 200
Samples: 93

Test type: Kolmogorov Smirnov
Is significant? (max > critical): true
D critical: 0.17947750551100136
D max: 0.30107526881720426
```

Yikes, in the prior 10 runs even the statistically significant results are saying that it's only around a 1% difference while this is claiming things got 30% slower.

This was before I switched from average to median. Doing this does make the results not quite so dramatic and 30% slower becomes 9.4% slower. But the problem still persists. This is very far from the "consensus" picture that running the tests 10 times shows.

I think it's statistically significant because it is definitely two different curves, however, the outliers on the "new" data set are massive here is a comparison of tail numbers:

```
Old       New
3.925983  6.560837
4.049389  6.708022
4.065886  6.761255
4.173183  6.842483
4.21175   8.125128
4.212002  8.129534
4.243142  8.302587
4.367608  8.7203
4.769074  10.282378
4.792769  10.284609
4.936911  12.238133
```

> Sorted values, these are not the actual comparison runs, here is raw data https://www.dropbox.com/s/fwhw3cxir18zz2h/problem-data.zip?dl=0


While we could run this `perf:library` test multiple times to see if results agree, I want to be able to only have to run this task once and get a fairly confident üëç or üëé . My question is essentially: Could we massage all of this data in a way so that the  `-29.1292%` result is discredited (or known to be suspicious)? Or flagged somehow.

One thing that stood out to me here is that "old" has a variance of 0.16 while "new" has a variance of 3.5. While it's okay to say that newer is slower, we cannot conclusively say that it is 9.4% slower (which is a lot). 

Here's the "good" result set to compare to: https://www.dropbox.com/s/awlb395xnsl8f6g/derailed-11-runs.zip?dl=0 (with the "bad" data also annotated).

Here's a spreadsheet showing distribution of the "bad" data here https://docs.google.com/spreadsheets/d/19W5PP656bns9K7Oo4bPcu-F7K4IVn0mS306sih9IBMk/edit?usp=sharing
Hey @schneems, üëã 

I was wondering if you would be interested in adding a dependency to `bundler-leak` and using it to check for known leaky dependencies at load time? 

That way, we could use that tool to save people time. They might be using a dependency which is known to be leaky. 

If you are interested, @bronzdoc or me could send a PR to address this idea. 

Thanks! 

Any exec task that  that I run throws this error. Using ruby-2.4.6 on a Mac. 

```
bundle exec derailed exec --help
/Users/stephen.turley/.rvm/gems/ruby-2.4.6/gems/derailed_benchmarks-1.4.1/bin/derailed:30:in `require': /Users/stephen.turley/.rvm/gems/ruby-2.4.6/gems/derailed_benchmarks-1.4.1/lib/derailed_benchmarks/tasks.rb:70: syntax error, unexpected keyword_ensure, expecting keyword_end (SyntaxError)
  ensure
        ^
/Users/stephen.turley/.rvm/gems/ruby-2.4.6/gems/derailed_benchmarks-1.4.1/lib/derailed_benchmarks/tasks.rb:256: syntax error, unexpected keyword_end, expecting end-of-input
        from /Users/stephen.turley/.rvm/gems/ruby-2.4.6/gems/derailed_benchmarks-1.4.1/bin/derailed:30:in `exec'
        from /Users/stephen.turley/.rvm/gems/ruby-2.4.6/gems/thor-0.20.3/lib/thor/command.rb:27:in `run'
        from /Users/stephen.turley/.rvm/gems/ruby-2.4.6/gems/thor-0.20.3/lib/thor/invocation.rb:126:in `invoke_command'
        from /Users/stephen.turley/.rvm/gems/ruby-2.4.6/gems/thor-0.20.3/lib/thor.rb:387:in `dispatch'
        from /Users/stephen.turley/.rvm/gems/ruby-2.4.6/gems/thor-0.20.3/lib/thor/base.rb:466:in `start'
        from /Users/stephen.turley/.rvm/gems/ruby-2.4.6/gems/derailed_benchmarks-1.4.1/bin/derailed:93:in `<top (required)>'
        from /Users/stephen.turley/.rvm/gems/ruby-2.4.6/bin/derailed:23:in `load'
        from /Users/stephen.turley/.rvm/gems/ruby-2.4.6/bin/derailed:23:in `<main>'
        from /Users/stephen.turley/.rvm/gems/ruby-2.4.6/bin/ruby_executable_hooks:24:in `eval'
        from /Users/stephen.turley/.rvm/gems/ruby-2.4.6/bin/ruby_executable_hooks:24:in `<main>'
```
It looks like this code causes the require to not get fired in our core extension https://github.com/rails/rails/blob/192df82f0d96db48a080fc71ef203979f30b4d08/activesupport/lib/active_support/dependencies.rb#L238-L244

That is applied to `Object`.

You can see a failure in  https://github.com/schneems/derailed_benchmarks/pull/148 on https://travis-ci.org/schneems/derailed_benchmarks/jobs/590138082#L412-L417

The high level `require` does not get triggered.

Even if I replace my core extension with this it does not fire:

```ruby
module Kernel
  module_function # rubocop:disable Style/ModuleFunction

  alias_method(:require_without_derailed, :require)
  def require(path)
    raise "require"
  end

  alias_method(:require_relative_without_derailed, :require_relative)
  def require_relative(path)
    raise "require_relative"
  end

  alias_method(:load_without_derailed, :load)
  def load(path, wrap = false)
    raise "load"
  end
end

class Module
  alias_method(:autoload_without_derailed, :autoload)
  def autoload(const, path)
    raise "autoload"
  end
end
```

No idea how bootsnap is working around this in https://github.com/Shopify/bootsnap/blob/master/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb. Either they're not and are silently getting this error or they've mitigated it somewhere. 
The requirements on this project say Ruby 2.1+ but when trying to install this on our 2.2 Ruby system, we are getting the following error:

Gem::RuntimeRequirementNotMetError: heapy requires Ruby version ~> 2.3. The current ruby version is 2.2.0.
An error occurred while installing heapy (0.1.4), and Bundler cannot continue.
Make sure that `gem install heapy -v '0.1.4' --source 'https://rubygems.org/'` succeeds before bundling.

In Gemfile:
  derailed_benchmarks was resolved to 1.3.6, which depends on
    heapy


Is there a work around for this? Or Does the documentation need to be updated?
Is there a way currently to boot a Rails app but *not* hit any endpoints, instead opting for running specific code paths like a Sidekiq background job?
`derailed_benchmarks` detects rails/rack application by `railtie` gem. It isn't completely good option because some gem which can be used in rack application require `railtie`.

For example `standalone_migrations` gem requires `railtie`. So whenever it is used in rack (non-Rails) app then rails setup is invoked instead of rack setup.

I think it will be good to prepare a mechanism which will force in this moment rack setup somehow.