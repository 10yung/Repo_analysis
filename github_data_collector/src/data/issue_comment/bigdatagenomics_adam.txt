

Hi, I try to install this package and use it but I'm constantly failing.
What I did so far:
1) set up Anaconda with PySpark 2.4.4
2) `pip install bdgenomics.adam`
3) `pyadam`
```
['/opt/anaconda/envs/adam/bin/..', '/opt/anaconda/envs/adam/lib/python3.7/site-packages/bdgenomics/adam']
['/opt/anaconda/envs/adam/bin/..', '/opt/anaconda/envs/adam/lib/python3.7/site-packages/bdgenomics/adam']
ls: cannot access /opt/anaconda/envs/adam/lib/python3.7/site-packages/bdgenomics/adam/adam-python/dist: No such file or directory
Failed to find ADAM egg in /opt/anaconda/envs/adam/lib/python3.7/site-packages/bdgenomics/adam/adam-python/dist.
You need to build ADAM before running this program.
```

When I try to use the Python API I get the following result:
```python3
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('abc').getOrCreate()
from bdgenomics.adam.adamContext import ADAMContext
ac = ADAMContext(spark)

Traceback (most recent call last):
  File "/opt/anaconda/envs/adam/lib/python3.7/site-packages/IPython/core/interactiveshell.py", line 3326, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-5-ade8e609ddf7>", line 5, in <module>
    ac = ADAMContext(spark)
  File "/opt/anaconda/envs/adam/lib/python3.7/site-packages/bdgenomics/adam/adamContext.py", line 57, in __init__
    c = self._jvm.org.bdgenomics.adam.rdd.ADAMContext.ADAMContextFromSession(ss._jsparkSession)
TypeError: 'JavaPackage' object is not callable
```
Python 2 will not be maintained past 2020 (https://pythonclock.org/). Many modules already have dropped support. 
```
$ adam-submit transformAlignments \
  -sort_by_reference_position_and_index \
  NA12878.alignedHg38.duplicateMarked.baseRealigned.bam \
  NA12878.alignedHg38.duplicateMarked.baseRealigned.alignments.adam

$ INPUT=NA12878.alignedHg38.duplicateMarked.baseRealigned.alignments.adam \
  OUTPUT=NA12878.alignedHg38.duplicateMarked.baseRealigned.out.cram \
  adam-shell -i convert_parquet_alignments_adam_dataset.scala
...
java.lang.IllegalArgumentException: requirement failed: To save as CRAM, input must be sorted.
  at scala.Predef$.require(Predef.scala:224)
  at org.bdgenomics.adam.rdd.read.AlignmentRecordDataset$$anonfun$saveAsSam$2.apply(AlignmentRecordDataset.scala:896)
  at scala.Option.fold(Option.scala:158)
  at org.apache.spark.rdd.Timer.time(Timer.scala:48)
  at org.bdgenomics.adam.rdd.read.AlignmentRecordDataset.saveAsSam(AlignmentRecordDataset.scala:832)
  at org.bdgenomics.adam.rdd.read.AlignmentRecordDataset$$anonfun$saveAsSam$1.apply$mcV$sp(AlignmentRecordDataset.scala:823)
  at org.bdgenomics.adam.rdd.read.AlignmentRecordDataset$$anonfun$saveAsSam$1.apply(AlignmentRecordDataset.scala:823)
  at org.bdgenomics.adam.rdd.read.AlignmentRecordDataset$$anonfun$saveAsSam$1.apply(AlignmentRecordDataset.scala:823)
  at scala.Option.fold(Option.scala:158)
  at org.apache.spark.rdd.Timer.time(Timer.scala:48)
  at org.bdgenomics.adam.rdd.read.AlignmentRecordDataset.saveAsSam(AlignmentRecordDataset.scala:822)
  ... 65 elided
```

See [convert_parquet_alignments_adam_dataset.scala](https://github.com/heuermh/benchmarks/blob/master/alignments/convert_parquet_alignments_adam_dataset.scala)
Abstract classes
```scala
trait GenomicDataset[T, U <: Product, V <: GenomicDataset[T, U, V]] extends Logging {
  def saveAsParquet(args: SaveArgs): Unit = {
  }
  def saveAsParquet(
    pathName: String,
    blockSize: Int = 128 * 1024 * 1024,
    pageSize: Int = 1 * 1024 * 1024,
    compressCodec: CompressionCodecName = CompressionCodecName.GZIP,
    disableDictionaryEncoding: Boolean = false): Unit
  }
  def saveAsPartitionedParquet(pathName: String,
                               compressCodec: CompressionCodecName = CompressionCodecName.GZIP,
                               partitionSize: Int = 1000000) {
  }
}

abstract class AvroGenomicDataset[T <% IndexedRecord: Manifest, U <: Product, V <: AvroGenomicDataset[T, U, V]] extends GenomicDataset[T, U, V] {

  protected def saveRddAsParquet(args: SaveArgs): Unit = {
  }
  protected def saveRddAsParquet(
    pathName: String,
    blockSize: Int = 128 * 1024 * 1024,
    pageSize: Int = 1 * 1024 * 1024,
    compressCodec: CompressionCodecName = CompressionCodecName.GZIP,
    disableDictionaryEncoding: Boolean = false,
    optSchema: Option[Schema] = None): Unit = SaveAsADAM.time {
  }
  def saveAsParquet(
    pathName: String,
    blockSize: Int = 128 * 1024 * 1024,
    pageSize: Int = 1 * 1024 * 1024,
    compressCodec: CompressionCodecName = CompressionCodecName.GZIP,
    disableDictionaryEncoding: Boolean = false) {
  }
  def saveAsParquet(
    pathName: java.lang.String,
    blockSize: java.lang.Integer,
    pageSize: java.lang.Integer,
    compressCodec: CompressionCodecName,
    disableDictionaryEncoding: java.lang.Boolean) {
  }
  def saveAsParquet(pathName: java.lang.String) {
  }
}
```

Concrete classes
```scala
abstract class CoverageDataset {
  def saveAsParquet(filePath: String,
                    blockSize: Int = 128 * 1024 * 1024,
                    pageSize: Int = 1 * 1024 * 1024,
                    compressCodec: CompressionCodecName = CompressionCodecName.GZIP,
                    disableDictionaryEncoding: Boolean = false) {
  }
  def save(filePath: java.lang.String,
           asSingleFile: java.lang.Boolean,
           disableFastConcat: java.lang.Boolean) = {
  }
}

case class DatasetBoundFeatureDataset {
  override def saveAsParquet(filePath: String,
                             blockSize: Int = 128 * 1024 * 1024,
                             pageSize: Int = 1 * 1024 * 1024,
                             compressCodec: CompressionCodecName = CompressionCodecName.GZIP,
                             disableDictionaryEncoding: Boolean = false) {
  }
}

sealed abstract class FeatureDataset {
  def save(filePath: java.lang.String,
           asSingleFile: java.lang.Boolean,
           disableFastConcat: java.lang.Boolean) {
  }
  def saveAsGtf(fileName: String,
                asSingleFile: Boolean = false,
                disableFastConcat: Boolean = false) = {
  }
  def saveAsGff3(fileName: String,
                 asSingleFile: Boolean = false,
                 disableFastConcat: Boolean = false) = {
  }
  def saveAsUcscBed(fileName: String,
                    asSingleFile: Boolean = false,
                    disableFastConcat: Boolean = false,
                    minimumScore: Double,
                    maximumScore: Double,
                    missingValue: Int = 0) = {
  }
  def saveAsBed(fileName: String,
                asSingleFile: Boolean = false,
                disableFastConcat: Boolean = false) = {
  }
  def saveAsIntervalList(fileName: String,
                         asSingleFile: Boolean = false,
                         disableFastConcat: Boolean = false) = {
  }
  def saveAsNarrowPeak(fileName: String,
                       asSingleFile: Boolean = false,
                       disableFastConcat: Boolean = false) {
  }

case class DatasetBoundFragmentDataset {
  override def saveAsParquet(filePath: String,
                             blockSize: Int = 128 * 1024 * 1024,
                             pageSize: Int = 1 * 1024 * 1024,
                             compressCodec: CompressionCodecName = CompressionCodecName.GZIP,
                             disableDictionaryEncoding: Boolean = false) {
  }
}

sealed abstract class FragmentDataset {
  def save(filePath: java.lang.String) {
  }
}

case class DatasetBoundAlignmentDataset {
  override def saveAsParquet(filePath: String,
                             blockSize: Int = 128 * 1024 * 1024,
                             pageSize: Int = 1 * 1024 * 1024,
                             compressCodec: CompressionCodecName = CompressionCodecName.GZIP,
                             disableDictionaryEncoding: Boolean = false) {
  }
}

sealed abstract class AlignmentDataset {
  def save(args: ADAMSaveAnyArgs,
           isSorted: Boolean = false): Boolean = {
  }
  def save(filePath: java.lang.String,
           isSorted: java.lang.Boolean): java.lang.Boolean = {
  }
  def saveAsSamString(): String = {
  }
  def saveAsSam(
    filePath: String,
    asType: Option[SAMFormat] = None,
    asSingleFile: Boolean = false,
    isSorted: Boolean = false,
    deferMerging: Boolean = false,
    disableFastConcat: Boolean = false): Unit = SAMSave.time {
  }
  def saveAsSam(
    filePath: String,
    asType: Option[SAMFormat],
    asSingleFile: Boolean,
    sortOrder: SAMFileHeader.SortOrder,
    deferMerging: Boolean,
    disableFastConcat: Boolean): Unit = SAMSave.time {
  }
  def saveAsSam(
    filePath: java.lang.String,
    asType: SAMFormat,
    asSingleFile: java.lang.Boolean,
    isSorted: java.lang.Boolean) {
  }
  def saveAsPairedFastq(
    fileName1: String,
    fileName2: String,
    writeOriginalQualityScores: java.lang.Boolean,
    asSingleFile: java.lang.Boolean,
    disableFastConcat: java.lang.Boolean,
    validationStringency: ValidationStringency,
    persistLevel: StorageLevel) {
  }
  def saveAsPairedFastq(
    fileName1: String,
    fileName2: String,
    writeOriginalQualityScores: Boolean = false,
    asSingleFile: Boolean = false,
    disableFastConcat: Boolean = false,
    validationStringency: ValidationStringency = ValidationStringency.LENIENT,
    persistLevel: Option[StorageLevel] = None) {
  }
  def saveAsFastq(
    fileName: String,
    writeOriginalQualityScores: java.lang.Boolean,
    sort: java.lang.Boolean,
    asSingleFile: java.lang.Boolean,
    disableFastConcat: java.lang.Boolean,
    validationStringency: ValidationStringency) {
  }
  def saveAsFastq(
    fileName: String,
    fileName2Opt: Option[String] = None,
    writeOriginalQualityScores: Boolean = false,
    sort: Boolean = false,
    asSingleFile: Boolean = false,
    disableFastConcat: Boolean = false,
    validationStringency: ValidationStringency = ValidationStringency.LENIENT,
    persistLevel: Option[StorageLevel] = None) {
  }
}

case class DatasetBoundReadDataset {
  override def saveAsParquet(filePath: String,
                             blockSize: Int = 128 * 1024 * 1024,
                             pageSize: Int = 1 * 1024 * 1024,
                             compressCodec: CompressionCodecName = CompressionCodecName.GZIP,
                             disableDictionaryEncoding: Boolean = false) {
  }
}

sealed abstract class ReadDataset {
  def save(filePath: java.lang.String, asSingleFile: java.lang.Boolean) {
  }
  def saveAsFastq(filePath: String,
                  asSingleFile: Boolean = false,
                  disableFastConcat: Boolean = false) {
  }
}

case class DatasetBoundSequenceDataset  {
  override def saveAsParquet(filePath: String,
                             blockSize: Int = 128 * 1024 * 1024,
                             pageSize: Int = 1 * 1024 * 1024,
                             compressCodec: CompressionCodecName = CompressionCodecName.GZIP,
                             disableDictionaryEncoding: Boolean = false) {
  }
}

sealed abstract class SequenceDataset {
  def save(
    filePath: java.lang.String,
    asSingleFile: java.lang.Boolean,
    disableFastConcat: java.lang.Boolean) {
  }
  def saveAsFasta(filePath: String,
                  asSingleFile: Boolean = false,
                  disableFastConcat: Boolean = false,
                  lineWidth: Int = 60) {
  }
}

case class DatasetBoundSliceDataset {
  override def saveAsParquet(filePath: String,
                             blockSize: Int = 128 * 1024 * 1024,
                             pageSize: Int = 1 * 1024 * 1024,
                             compressCodec: CompressionCodecName = CompressionCodecName.GZIP,
                             disableDictionaryEncoding: Boolean = false) {
  }
}

sealed abstract class SliceDataset {
  def save(
    filePath: java.lang.String,
    asSingleFile: java.lang.Boolean,
    disableFastConcat: java.lang.Boolean) {
  }
  def saveAsFasta(filePath: String,
                  asSingleFile: Boolean = false,
                  disableFastConcat: Boolean = false,
                  lineWidth: Int = 60) {
  }
}

case class DatasetBoundGenotypeDataset {
  override def saveAsParquet(filePath: String,
                             blockSize: Int = 128 * 1024 * 1024,
                             pageSize: Int = 1 * 1024 * 1024,
                             compressCodec: CompressionCodecName = CompressionCodecName.GZIP,
                             disableDictionaryEncoding: Boolean = false) {
  }
}

sealed abstract class GenotypeDataset {
  def saveVcfHeaders(filePath: String): Unit = {
  }
}

sealed abstract class VariantContextDataset {
  def saveVcfHeaders(filePath: String): Unit = {
  }
  def saveAsParquet(pathName: String,
                    blockSize: Int = 128 * 1024 * 1024,
                    pageSize: Int = 1 * 1024 * 1024,
                    compressCodec: CompressionCodecName = CompressionCodecName.GZIP,
                    disableDictionaryEncoding: Boolean = false) {
  }
  def saveAsVcf(args: ADAMSaveAnyArgs,
                stringency: ValidationStringency = ValidationStringency.LENIENT): Unit = {
  }
  def saveAsVcf(filePath: String): Unit = {
  }
  def saveAsVcf(filePath: String,
                asSingleFile: Boolean,
                deferMerging: Boolean,
                disableFastConcat: Boolean,
                stringency: ValidationStringency): Unit = SaveAsVcf.time {
  }
}

case class DatasetBoundVariantDataset {
  override def saveAsParquet(filePath: String,
                             blockSize: Int = 128 * 1024 * 1024,
                             pageSize: Int = 1 * 1024 * 1024,
                             compressCodec: CompressionCodecName = CompressionCodecName.GZIP,
                             disableDictionaryEncoding: Boolean = false) {
  }
}

sealed abstract class VariantDataset {
  def saveVcfHeaders(filePath: String): Unit = {
  }
}
```

Expected to fail CI.
```
com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Class is not registered: org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus
Note: To register this class use: kryo.register(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus.class);
Serialization trace:
array (scala.collection.mutable.ArrayBuffer)
	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:508)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)
	at com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:37)
	at com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:361)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:302)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)
	at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:456)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
```
See https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala#L488

```scala
// classForName() is expensive in case the class is not found, so we filter the list of
  // SQL / ML / MLlib classes once and then re-use that filtered list in newInstance() calls.
  private lazy val loadableSparkClasses: Seq[Class[_]] = {
    Seq(
      "org.apache.spark.sql.catalyst.expressions.UnsafeRow",
      "org.apache.spark.sql.catalyst.expressions.UnsafeArrayData",
      "org.apache.spark.sql.catalyst.expressions.UnsafeMapData",

      "org.apache.spark.ml.attribute.Attribute",
      "org.apache.spark.ml.attribute.AttributeGroup",
      "org.apache.spark.ml.attribute.BinaryAttribute",
      "org.apache.spark.ml.attribute.NominalAttribute",
      "org.apache.spark.ml.attribute.NumericAttribute",

      "org.apache.spark.ml.feature.Instance",
      "org.apache.spark.ml.feature.LabeledPoint",
      "org.apache.spark.ml.feature.OffsetInstance",
      "org.apache.spark.ml.linalg.DenseMatrix",
      "org.apache.spark.ml.linalg.DenseVector",
      "org.apache.spark.ml.linalg.Matrix",
      "org.apache.spark.ml.linalg.SparseMatrix",
      "org.apache.spark.ml.linalg.SparseVector",
      "org.apache.spark.ml.linalg.Vector",
      "org.apache.spark.ml.stat.distribution.MultivariateGaussian",
      "org.apache.spark.ml.tree.impl.TreePoint",
      "org.apache.spark.mllib.clustering.VectorWithNorm",
      "org.apache.spark.mllib.linalg.DenseMatrix",
      "org.apache.spark.mllib.linalg.DenseVector",
      "org.apache.spark.mllib.linalg.Matrix",
      "org.apache.spark.mllib.linalg.SparseMatrix",
      "org.apache.spark.mllib.linalg.SparseVector",
      "org.apache.spark.mllib.linalg.Vector",
      "org.apache.spark.mllib.regression.LabeledPoint",
      "org.apache.spark.mllib.stat.distribution.MultivariateGaussian"
    ).flatMap { name =>
      try {
        Some[Class[_]](Utils.classForName(name))
      } catch {
        case NonFatal(_) => None // do nothing
        case _: NoClassDefFoundError if Utils.isTesting => None // See SPARK-23422.
      }
}
```