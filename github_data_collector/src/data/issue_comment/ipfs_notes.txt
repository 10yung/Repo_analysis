Continue the work started at https://github.com/ipfs/research/pull/8
feature request(s):

1) Create a new "ipfs add" command that adds a file to the repository in "burn-on-read" mode.  Perhaps the command would be "ipfs add --burn-on-read".  When the content is read for the first time, it is provided, then deleted (maybe with another option for secure deletion).

This command would facilitate using ipfs for relatively easy, moderately private communications, especially for people who run ipfs on their computers.  I could email you a link with a hash of a message that I added to my own ipfs repo, then after you read it, the message would be deleted.  Users who want to be more secure could add a random string at the end of the message (so that a short message is not easily inferred by an intermediary who has a rainbow table), or the message could be encrypted (GPG/PGP, or otherwise).  

The value of these features compared to using Enigmail (or manual GPG/PGP in an email) is that novice recipients typically do not understand how to keep their private key secure, and they don't understand how to keep their message history secure.  When using regular email with GPG, 5 years from now the recipient's computer could be seized and the key could be used to read all the old email traffic.  With the ipfs burn-on-read option, I could email links to a novice, the novice could open them, the message would appear and then disappear so that the history of communications would be exceedingly difficult to recreate.  All you need is an appropriate ipfs web browser add-in.

Burn-on-read (combined with direct network connections so that the CONTENT of the messages bypasses intermediaries such as email providers) might prove to become extremely important as governments consider mandating backdoors to encryption or encrypted devices (and encrypted email) https://www.theverge.com/2018/9/3/17815196/five-eyes-encryption-backdoors-us-uk-australia-nz-canada.

2) To make this even more secure and faster, two features could be added (if they can not already be acheived in some way)...

ADDED FEATURE 1) I'm not sure about the official policy for announcing to the world that a file has been added to a repo, but the burn-on-read messages should not be announced to the network.  For one thing, a private message is not intended for the whole universe, so Spamming all peers with an announcement is just a waste of bandwidth.  The other thing is that quiet operation improves privacy and reduces the chance that a powerful adversary will detect that the hash exists and try to read the message before the intended recipient reads it.  Burn-on-read would be useless without this feature.

ADDED FEATURE 2, VERSION 1) (Easiest for the recipient of a message to read the message) Establish a different format of an ipfs link that looks specifically at the specified address for the specified hash, maybe something like:  `ipfs://205.123.456.119/tcp/4001/QmNsVsMWYbukZjiBiccMjJABCDEFGHIJKLMNOPQRST ` 
Web browser add-ins would have to capture this and fetch it.

ADDED FEATURE 2, VERSION 2) (Easier to program to provide faster functionality for a variety of applications, but clumsier for novice users) Add a command like this:

`ipfs dht find provs QmXgqKTbzdh83pQtKFb19SpMCpDDcKR2ujqk3pKph9aCNF 205.123.456.789 129.987.545.232 321.989.121.111`

In other words, see if this hash can be found at any of the following addresses (e.g., a server farm that hosts content that the provider wants to be found quickly as soon as the content is created, but the hashes are not announced to the universe).

thx for your consideration here.

p.s., the system would also need an expiration date so that if I send you a burn-on-read link and you don't read it in a few days, the server would delete it (Nov 1, 2019).
* Motivation: 

When a file is updated and resync again, decrement its block duplication on nodes all over the world and decrease communication cost (only downloaded the updated blocks) and save storage (only updated section of the file as blocks will be stored).

-----------------

* Problem:

Example, there is a .tar.gz file, which contains a data.txt file, file.tar.gz (~100 GB) stored in my IPFS-repo, which is pulled from another Node-a.

I open the data.txt file and added a single character in a random locations in the file (beginning of the file, middle of the file, and end of the file), and compress it again as file.tar.gz and store it in my IPFS-repo. Here update is only few kilobytes.

[[*]](https://discuss.ipfs.io/t/does-ipfs-provide-block-level-file-copying-feature/6388/4?u=avatar-lavventura) When I deleted a single character at the beginning of a file, since the hash of all 124kb-blocks will be altered, which will lead to download complete file to be downloaded. 

As a result, when node-a wants to re-get the updated tar.gz file a re-sync will take place and whole file will be downloaded all over again. As a result there will be duplication of blocks (~100 GB in this example ) even the change is made only for few kilobytes. **And iteratively this duplication will be distributed to all over the peers, which is very inefficient and consumes high amount of storage and additional communication cost over time.** 

-----------

* Solution: 

Other clouds are try to solve this problem using [Block-Level File Copying](https://superuser.com/a/1368955). On their case like IPFS, since blocklist is considered for "Block-Level File Copying"; when a file is updated (a character is added at the beginning of the file), Dropbox, One-Drive will re-upload the whole file since the first block's hash will be change and it will also affect/change the hash of all the consequent blocks. This doesn't solve the problem.

**=>** I believe better soluiton is to consider between each commits of the files, approach that [git-diff](https://git-scm.com/docs/git-diff) uses could be considered. This will only uploads the changed (diff) parts of the file, that will be few kilobytes on the example I give, and its diffed blocks will be merged when other nodes pull that file. So as communication cost only few kilobytes of that will be transferes and that amount of data will be added to storage will be only few kilobytes as well.

I know that it will be difficult to re-design IPFS's design, but this could be done as a wrapper solution that combines `IPFS` and `git`, and users can use it for very large files based on their needs.

----------

This problem is not considered as priority by IPFS team but at least it should be on the priority.

> IPFS team is considering adding that eventually, but itâ€™s not a priority.


------

Please see discussion I have already opened. Please feel free to add your ideas in to them.

=> [Does IPFS provide block-level file copying feature?](https://discuss.ipfs.io/t/does-ipfs-provide-block-level-file-copying-feature/6388)

=> [Efficiency of IPFS for sharing updated file](https://stackoverflow.com/a/52246029/2402577) 

# Motivation

Users should be able to publish and resolve IPNS records when disconnected from the global network. This is important in two cases:

1. The user is trying to re-resolve content that has been resolved at some point. This is useful for offline document viewing.
2. Two users are offline and are trying to use IPNS.

# Proposal

Libp2p Parts: https://github.com/libp2p/notes/issues/16
* Users should be allowed to publish records when offline. This should _not_ produce an error. However, it might produce a warning.
  * Go-ipfs currently allows this with an `--allow-offline` flag but that's not very user-friendly.
* Users should be allowed to resolve _expired_ records as long as some warning is displayed.
### Problem

At the moment I have 3 IPFS nodes running on my machine embedded by following applications

- [IPFS Desktop](https://github.com/ipfs-shipyard/ipfs-desktop)
- [Textile Desktop](https://github.com/textileio/photos-desktop)
- [Radicle Service](http://www.radicle.xyz/)

And when I'll start test driving [anytype](https://anytype.io/) I'll likely end up with 4 nodes.

It is not ideal, because:

- IPFS is resource intensive (On my Macbook Pro IPFS Node drains battery about 5 times faster than without). That multiplied by 3x (or possibly more as more apps become popular) is terrifying. 
- As user I would like a canonical place for the data stored by apps to be aggregated at, instead it gets scattered around bunch of IPFS nodes. That also means there is no simple way for me to mirror data on other nodes (sever or other device). More broadly current situation does not align with [WebOS](https://github.com/ipfs/roadmap#-webos-d5-e2-i3) vision.
- We are replicating data silos that we hate from the conventional web, sure data isn't gated behind corporate server farm but if user can't easily interact with data trapped in other app embedded node end result is not much different. _(With the caveat that here you can copy dataset across nodes, but propagating changes across nodes is non trivial & apps generally don't have a good story even for doing copying).


### Proposal

`ipfs daemon` and `ipfs` tool and library should by default should attempt to discover if IPFS service is already running and if so just act as a frontend to it, otherwise spawn a service and still act as a frontend to it. That would make all embedders by default share same IPFS node. Dealing with version incompatibilities is going to be an interesting challenge, but even if this would address problem for same versioned nodes that would be a good step forward IMO.
## Essential Use Cases

- IPFS user creates new IPNS folders, files, and related metadata encrypted for privacy at rest by default; providing local record of functional equivalence between `encrypted + key = decrypted` blocks, tracked and linked within a locally-immutable IPLD mDAG by their respective multihash content ID's. 
- Local Access Control configuration API or command set allows gradual structured key sharing with node peers, or pre-defined node groups in shared lockboxes, in keeping with Capability-Based Security standards. 
- Access is granted to the public via structured IPNS manifest distribution, only when global publishing is explicitly intended, using metadata and key-sets signed by the intended public author's public-private key pair. 
  - A network consensus timestamp may be applied at time of public signing, in order to help establish public data source attribution and provenance, if desired by the author.

### Use Cases

- IPFS user creates new IPNS folders, files, and related metadata encrypted for privacy and only uses IPFS and limited IPNS data distribution for remote backup purposes. Encryption keys are thus only ever held locally and never distributed.
- Local access control configuration API allows gradual structured key sharing with known peers, or among pre-existing membership node groups in shared lockboxes, in keeping with Capability-Based Security standards. 
- Decrypted data or necessary decryption keys are only revealed to the public when global publishing is explicitly intended, using metadata and key-sets signed by the public author's public-private key pair, with a network consensus timestamp if public data source attribution and provenance are desired by the author.
  - Upon publication, the node may utilize a group shared public-private key pair to sign all published metadata and keys, in order to gain pseudonymous privacy protection via group membership.
    - Each group lockbox membership or private network is able to collectively determine their own publishing, revocation, membership, local incentives, write access, and blacklist policies via consensus protocol selection and owner/moderator assignments. Each decision will create a different automated control on group node shared key distribution, and map Access Control configuration API calls to Capability Based Security key allocation.
- Nested arrangements and cross-over membership of multiple group lockboxes are possible as a result of key distribution being innately public-private key pair and signature source agnostic. Higher level or external identity confirmation systems are necessary to prevent such complex multi-membership arrangements, as a group defined policy that influences group node Access Control configurations.


### Foundational Features + Functionality

- Finish KeyStore and KeyChain systems, and add group lockbox distribution so access is not node dependent..
- Key exchange Access Control configuration and network API establishes private network and group membership status as a result of encryption at rest, over the wire, and maps into Capability-Based Security models.
- Group lockbox membership ad hoc arrangements allow private networks to form, federate, or subdivide at will.
  - Group members may publish data to the public as a unified group identity via public-private key sharing, which should be automated or managed according to each group's predefined publication policies.
- Every request for a key access needs to occur inside an encrypted LibP2P channel or tunnel, and request messages must be signed by a valid group member or known permitted peer public-private key pair.
- IPNS manifest distribution must not permit accidental leaking of decrypted block or key data prior to Access Control configuration API permitted release.
  - This potential leak issue creates a good argument for keeping IPNS manifest data and distribution separate from (but related to) any encryption key records and management. Even releasing the multihash ID of any decrypted blocks prior to permitted publication represents a potential security threat, so the IPNS manifest records should only reference encrypted block multihashes prior to permitted release.


### Existing Projects + Organizations Working in this Area

`Please note: node and group key management does not create nor enforce any identity or reputation systems directly, but it can enable linking such systems to an Access Control layer as desired by individual group or node owners.`

- Private IPFS networks
- [Peergos](https://github.com/Peergos/Peergos)
- [uPort](https://www.uport.me/)
- [Sovrin](https://sovrin.org/)
- [spherebox](https://www.spherebox.com/en/)

- Google Circles, Groups, and apps "for Work"
- Dropbox Business
- Backblaze Groups
- GitHub private repositories
- Facebook groups
- Discourse
- Slack
- http://sprout.ics.uci.edu/past_projects/gac/index.html

Additional reference materials:
https://docs.google.com/document/d/1htXTZUWPYB3WNsKmvClKQlFn1ByehWTG1WRR5zQkjH8/edit?usp=sharing
https://docs.google.com/presentation/d/1Gfu2J4A5tJZP4V4EmSCsYVVx2cuoo56QWmYDe1ldIwY/edit?usp=sharing
It is entirely possible, especially as IPFS is not yet widely adopted that a peer may have a file, but does not know a corresponding IPFS url, because it just stores the full hash of the file in some file catalog(rather than a pointer to a magic DHT shard, as seems to be the case in bitswap).

To this end I think a peer should also be able to list want hashes to a connected peer.

### Enquiry Setup
1. A peer enquires to connected hosts about what form(s) of catalog hash they have (md5,sha1,sha512). [optional]
2. The peer produces a bare hash wantlist of the form `(hash_type_multicodec,hash)`

### Peering Algorithm
On another peer
1. Read wantlist of peer p
2. Ask a `bare_wantlist_provider` if that file is on file
3. `ipfs add --nocopy <file>` if available
4. Notify peer p of:

  - The corresponding ipfs url, or
  - File Unavailable

### Bare wantlist provider
A bare wantlist provider is a program registered with the ipfs daemon.  Some configuration or environment variable will be set to its path, and accepted hash types.

The wantlist provider will be invocable in the form
`cmdName <hashName>://<hash>`
And shall print the path of the corresponding file, then exit 0
or
If the file is not found (or any other condition for which the peer does not want to serve the file) exit 1.
**Idea:** rustup but for IPFS!

This tool would:

1. Manage multiple IPFS installs (multiple implementations and multiple versions).
2. Provide a virtual `ipfs` command that launches the correct IPFS version (downloading it if necessary).

# Requirements

V1:

* Fetch IPFS over IPFS, when possible.
* Fallback on fetching IPFS over HTTPs.
* Support rolling back to a previous release.

V2:

* Support both js-ipfs and go-ipfs.

V3:

* Support auto update.

# User Story

The user must be able to:

0. Run `ipfs $ARBITRARY_COMMAND` and have that command run on the correct version of IPFS.
1. Run an `ipfs update [latest]` command to update to the latest stable version of their chosen IPFS implementation (go or js).
2. Run `ipfs update vX.Y.Z` to update to a specific version of their chosen IPFS dist.
3. Run `ipfs update $lang-vX.Y.Z` to switch to update to a specific version of `$lang`.
4. Run `ipfs update $lang-latest` to switch to the latest IPFS version in `$lang`.
5. Pass `--transport={ipfs,https,auto}` to `ipfs update` to force a specific transport.

# V1 Design

Like `rustup`, this project should provide a virtual `ipfs` command that downloads the correct IPFS version/distribution and runs it. Unlike `rustup`, it should support downloading IPFS over IPFS if a working version of IPFS is already installed.

* When the `ipfs` command is run:
  * If `ipfs` is installed:
    * Run it.
  * Otherwise:
    * Download IPFS over https from dist.ipfs.io.
* When the `ipfs update` command is run:
  * If `ipfs` is installed:
    * Use it to check for an update by checking `ipfs cat /ipns/dist.ipfs.io/go-ipfs/latest`.
    * If there's an update available, fetch the correct build with IPFS.
  * Otherwise:
    * Check https://dist.ipfs.io/go-ipfs/latest for a new release.
    * If an update is available, fetch the correct build over HTTPS.
  * Save the IPFS binary to a `$IPFS_PATH/releases/$VERSION/$ARCHITECTURE`.
  * Write the current IPFS version to the config file as `Update.Version`.
* When the `ipfs update $VERSION` command is run, do the same thing but use the specified version.

Or as I've affectionally named it: **bitswaxx** which is like bitswap++ concatenated and alpha'd.

I had this idea which is a cross between delegated routing and the "friends of friends" policy that scuttlebutt has. This is likely to highlight my inexperience in network/protocol design and it's more than likely to cover ground and research already discounted...but, if this triggers ideas in someone else, or can be adapted somehow then it will make writing this down worthwhile.

The idea is, get others to fetch content for you, on the bitswap level. In short, your peer's wantlist becomes _your_ wantlist.

Imagine this - if I have 10 peers, and each of those 10 peers has 1,000 peers then I've increased the scope of peers who might have the content I'm looking for from 10 to 10,000.

This comes in 2 flavours:

1. **These peers are my friends, their wantlist is my wantlist**

    We can't just concat ALL wantlists together, because the world will converge on a single dataset and we'll ALL end up storing each other's stuff. Hard disks explode - obvious problem.

    We _could_ do this for peers we deem to be "friends". When we receive their wantlist, it gets added to our wantlist until such time as our friend stops wanting it or we receive it. If we start wanting it ourselves then that trumps the friend want.

    It could allow actual IRL friends to host each others content or allow a public node(s) to befriend all nodes in a particular web/mobile/desktop application.

2. **Hop based wants**

    e.g. "wants" <= 2 hops away are added to my wantlist.

    The hops are configurable. Bitswap is 1 hop based. Bitswaxx would be 2+ hop based. Each "want" would carry a `hop` field - a number that describes how far away the want came from, and is incremented by 1 on receipt.

    A want desired by your node is sent with a `hop` of 0. A want that came from another peer would be sent with whatever the received `hop` was + 1. We'd have to decide what to do with duplicate wants with differing hop counts (take the lowest?).

    Provided the `hop` count for a want you receive is <= 2 (for example) the want will be added to your wantlist. This is of course until such time as the peer who wanted it stops wanting it or we receive it. If we start wanting it ourselves then that trumps the want from the peer.

---

Known issues/open problems:

- Not intelligent
    - Arguably, peers of peers are no more likely to have the content you want than the peers you have and it would be better to intelligently converge on a peer who actually _has_ that content.
        - This is what the DHT does and it is known to be slow. Bitswaxx might be an effective in-between method that's slower than bitswap but faster than DHT.
        - The more peers we can ask for content the more likely we'll find it. Right? We can't actually connect to 10,000 peers simultaneously, but we "can" if we leverage our peer network
- Disk space
    - Storing content (albeit temporarily) will have a considerable disk impact
- Bandwidth
    - Bitswap will become considerably more chatty
    - Multiple peers will receive content they don't want, and may never provide it to the peer that does
- Abuse
    - As with all relays this is open to abuse
    - We're trusting the `hop` field
- Complicated
    - If we want to be able to remove the want from our list (because we don't want it ourselves) we need to track which peers want what - memory issues
It would be fantastic to be able to track dependencies across all the dozens of repos and thousands of issues. It would be fantastic to see what's blocked as easily as the below. See https://github.com/jbenet/random-ideas/issues/37 for a proposal on how to implement this pretty reasonably on top of regular github issues.

@RichardLitt this may be a super valuable "dev workflow" tool to contribute.

---

![](https://trac-hacks.org/raw-attachment/wiki/TracTicketDepgraphPlugin/TracTicketDepgraphPlugin-Screenshot.jpg)
