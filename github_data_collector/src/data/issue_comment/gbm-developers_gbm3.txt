Hi,

Just been trying this package as gbm lacks support for tweedie distribution. Have been suffering a lack of predictive power for the distributions gamma, tweedie and gaussian. I can't share the data but i've gone from a gini of 0.2 on gbm using a lognormal distribution to negative for the three listed above using gbm3. 

Its the same dataset and the code has been modified to have the same parameters, any ideas what could be driving this. 

thanks

Felix
As described in official documentation at the part of basehaz.gbm:

>Arguments:
**cumulative, If TRUE the cumulative survival function will be computed.**
...
Details: 
The proportional hazard model assumes h(t|x)=lambda(t)*exp(f(x)). gbm can estimate the f(x) component via partial likelihood. After estimating f(x), basehaz.gbm can compute the a nonparametric estimate.

Is "the cumulative survival function will be computed" correct? Is there terminology error ? Since the concept of **cumulative survival function** and **cumulative hazard function** is different.

I read the code implemented in `gbm-baseline-hazard.r` and run an example for this function with setting `t.eval=c(10, 20, 30)`, but I got three values in ascending order, which is not consistent to the properties of survival function, so I opened this issue, and same doubts has also been proposed at [stackoverflow](https://stackoverflow.com/questions/52222714/how-can-i-calculate-survival-function-in-gbm-package-analysis)

If this issue is actually existing, I think it would be a wrong guide to someone like me who read the documentation, and the correct version should be **"cumulative, If TRUE the cumulative baseline hazard function will be computed."**

thx!
This is not possible in the old gbm but I recently wrote a function to calculate it. It can yield very different results for the most important variables for different classes. See this question and my answer for the [code](https://stackoverflow.com/q/29637145/3277050)
Hopefully you would just need to translate this from my dplyr/purrr version to base to implement it. If it doesn't make sense to do this this way please let me know!
Hi, team - quick question: Is the `GBMFit` object supported by pmml? Couldn't find any topic on the internet on this.

Posted this question in [SO](https://stats.stackexchange.com/questions/341616/gbm3-gbmfit-class-supported-by-pmml), as well.

Thanks!
Recent advances in interpretability have produced SHAP values for assigning score contributions to features in a consistent manner. Calculating per-feature score contributions was added to XGBoost and LightGBM. It would be great to have it in gbm3 as well:

https://github.com/slundberg/shap


I tried to switch from gbm to gbm3 for quantile regression, but I saw some wrong quantile prediction results from gbm3. See the example below, using the attached toy data [X.zip](https://github.com/gbm-developers/gbm3/files/1564403/X.zip)
```R
library(data.table)

get_tree <- function(g) {
  t <- if (class(g) == "gbm") gbm::pretty.gbm.tree(g)
       else gbm3::pretty_gbm_tree(g)
  t$Node <- as.integer(rownames(t))
  mis <- t$MissingNode + 1
  t <- t[-mis,]
  t$MissingNode <- NULL
  t$RealPrediction <- t$Prediction + g$initF
  t
}

X <- readRDS('X.rds')
str(X)

params <- list(y ~ ., data=X, distribution = list(name="quantile", alpha=0.9), n.trees = 1,
               interaction.depth = 2, n.minobsinnode = 250, shrinkage = 1, bag.fraction = 1)
g0 <- do.call(gbm::gbm, params)
g3 <- do.call(gbm3::gbm, params)

get_tree(g0)
get_tree(g3)

# overall 90% quantile:
quantile(X$y, 0.9, type = 2)
# true 90% quantiles inside the splits:
X[, quantile(y, 0.9, type = 2), .(s1 = a<14.5, s2 = a>=14.5 & b < 0.133)]

# Check the empirical CDF's in the 4th node for g0 and g3:
X[a>=14.5 & b >= 0.133, ecdf(y)(c(1.716003, 1.529294))]
```
The output of it is 
```
> get_tree(g0)
  SplitVar SplitCodePred LeftNode RightNode ErrorReduction Weight  Prediction Node RealPrediction
0        0    14.5000000        1         2      0.6721752   1567  0.03466136    0       1.984051
1       -1     0.1333954       -1        -1      0.0000000    854  0.13339536    1       2.082785
2        1     0.5000000        3         4      1.4643795    713 -0.08359787    2       1.865792
3       -1     0.1578200       -1        -1      0.0000000    273  0.15781996    3       2.107210
4       -1    -0.2333867       -1        -1      0.0000000    440 -0.23338666    4       1.716003
> get_tree(g3)
  SplitVar SplitCodePred LeftNode RightNode ErrorReduction Weight   Prediction Node RealPrediction
0        0    14.5000000        1         2      0.6721752   1567  0.009314278    0       1.958704
1       -1     0.1333954       -1        -1      0.0000000    854  0.133395364    1       2.082785
2        1     0.5000000        3         4      1.4643795    713 -0.112242207    2       1.837148
3       -1     0.1578200       -1        -1      0.0000000    273  0.157819963    3       2.107210
4       -1    -0.4200960       -1        -1      0.0000000    440 -0.420095993    4       1.529294
> 
> # overall 90% quantile:
> quantile(X$y, 0.9, type = 2)
    90% 
1.94939 
> # true 90% quantiles inside the splits:
> X[, quantile(y, 0.9, type = 2), .(s1 = a<14.5, s2 = a>=14.5 & b < 0.133)]
      s1    s2       V1
1: FALSE FALSE 1.716003
2: FALSE  TRUE 2.107210
3:  TRUE FALSE 2.082785
> 
> # Check the empirical CDF's in the 4th node for g0 and g3:
> X[a>=14.5 & b >= 0.133, ecdf(y)(c(1.716003, 1.529294))]
[1] 0.8909091 0.5727273
```
While the true 90% quantiles inside the splits match the leaves from g0 spot on, the node 4 leaf in g3 is wrong, and it corresponds to a 57% empirical quantile.

Referencing issue #146 . This PR allows the user to predict a sparse matrix representing all of the nodes an observation passes through throughout the entire ensemble.
Add option when predicting a GBM3 model to output a sparse matrix of decision tree rules. Essentially output nTrees x (nInternal + nTerminal) nodes matrix of 1s and 0s where 1s indicate the current record passed through that node. 

This would allow users to apply LASSO regression to the sparse node matrix and create simple, weighted rules ensembles as described here:

http://statweb.stanford.edu/~jhf/R_RuleFit.html

Can attempt this myself, but wondering if `gbm_pred` is *the* function to use?

I wonder if anybody can point out how ErrorReduction is calculated in pretty.gbm.tree.r 

https://github.com/gbm-developers/gbm3/blob/62c8dafd87b16fe1d2079cdd5058169f1f08967b/R/pretty-gbm-tree.r#L32

I am trying to calculate it by hand and I cannot find a piece of code which would calculates it for gbm object.

Thanks

No way I can share the data, so not necessarily reproducible.

Data is 20k x 4k, binomial response, 10-fold CV. Gets to end then reports
"Error in `[.data.frame`(data, flag, model$variables$var_names, drop = FALSE) : 
  undefined columns selected"

The columns have had make.names run on them, so it's not weird colnames.

Also, it's sucked up all my RAM and isn't letting go... killing the RStudio session did cause it to let go.

gbm 2.2, R 3.3.1, Ubuntu 16.04.2