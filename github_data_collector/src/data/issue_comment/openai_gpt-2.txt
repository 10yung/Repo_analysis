Hello,

is it possible to train the small 117M GPT-2 model with 6 GB VRAM using FP16?

Recommended Vram is 12 GB so with fp16 I could halve the memory consumption? 
Hi there , There are so many developers want to try gpt-2.
I found a lot of people confuse and suffering for deploy gpt-2 as a web service, 
may be this commit will help them avoid this trouble and save more time focus on the model itself.





Relevant tweet chain: 

https://twitter.com/theshawwn/status/1208169319223480322

https://twitter.com/theshawwn/status/1208171700057186304

Basically, you're prompting the model with <|endoftext|> (a single token with BPE value 50256 or whatever), but the BPE encoder encodes <|endoftext|> as <| end of text|>, five separate tokens. It's completely different.


I am working to make `pb` file of the checkpoint but i don't know the input and output nodes of the model so i am unable to make frozen graph of the model for prediction.
i tried so many ways to make `pb` file. In one of the way i am getting error that `PB file should be less than 2gb`
How to resolve this issue?
Thank you for your time.

allow listing of available models on object storage
I am using GPT2 small model (124M). I have trained the model on real Estate home descriptions samples. So it start generating homes description. I am use the GPT2 following generate function to generate home description samples. **gpt2.generate(sess,length=500,temperature=0.9,nsamples=10,run_name='run1')**

Well the generated description well look like this,

**2700 NW Dogwood St Unit H104 is a condo in Seattle. WA 98146. This 830 square foot condo features 2 bedrooms and 2 bathrooms having 3 stories and 3 floors.This property was built in 1992 and last sold for $40000. .Based on Redfin's Seattle data. we estimate the home's value is $472830. Comparable nearby homes include 2700 NW Dogwood St Unit H101. 2700 NW Dogwood St Unit H202.**

So my question is that how can i give the parameter like **"4 stories 4 floors"** to GPT2 generate function and it will generating the home description samples with this feature (**"4 stories 4 floors"**) and if i give parameter like **2 bedrooms and 2 bathrooms** it will start generating samples with this feature ( **3 bedrooms and 3 bathrooms** )
Can we leverage GPT-2 pre-trained model for WebNLG tasks http://webnlg.loria.fr/pages/challenge.html

The WebNLG challenge consists in mapping data to text
similar to what is being done in https://github.com/tyliupku/wiki2bio.




I get the below errors when running gpt-2. The model runs in the end and seems to work, but is there any way to fix this?

Thanks!

```
2019-11-19 17:13:26.908876: E tensorflow/stream_executor/cuda/cuda_driver.cc:893] failed to alloc 4294967296 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of m
emory

2019-11-19 17:13:26.913409: W .\tensorflow/core/common_runtime/gpu/gpu_host_allocator.h:44] could not allocate pinned host memory of size: 4294967296

2019-11-19 17:13:26.917233: E tensorflow/stream_executor/cuda/cuda_driver.cc:893] failed to alloc 3865470464 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of m
emory

2019-11-19 17:13:26.921787: W .\tensorflow/core/common_runtime/gpu/gpu_host_allocator.h:44] could not allocate pinned host memory of size: 3865470464
2019-11-19 17:13:26.925608: E tensorflow/stream_executor/cuda/cuda_driver.cc:893] failed to alloc 3478923264 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of m
emory

2019-11-19 17:13:26.930056: W .\tensorflow/core/common_runtime/gpu/gpu_host_allocator.h:44] could not allocate pinned host memory of size: 3478923264
```
