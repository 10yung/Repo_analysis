This PR includes @eightysteele's changes from #95 but rebased and with a few little fixes for recent updates.

It also replaces brushfire-finatra with brushfire-finagle, which does more or less the same thing but without the dependency on the EOL-ed Finatra 1 (which won't be published for 2.12). Since this is just an example project (that wasn't compiling anyway) I figured a pretty major change wouldn't be a problem. We could also just delete the dead code, but it seems like a nice thing to have.

I've added an `./iris-srv` example to the README, since it works again. I'd like to aggregate the module in the build, so that we know when it breaks, but because the latest Finagle version isn't built for 2.10 this causes problems for `++` and `+` (we could use sbt-doge to fix this but in my experience it's a pain and in this case probably isn't worth the effort).

More generally it seems like it'd be a good idea to pull all of the example code into a new brushfire-example module, so that stuff like `local.Example` isn't ending up in our published artifacts. If people agree I could do that in this PR, or in a follow-up.
I don't intend to merge this pull request. I'm putting this up here to demonstrate a method for obtaining a functional iterator for trees. This was a way for me to get a bit more familiar with dependent types (since we need to work with the type member of `FullBinaryTreeOps`) and generally play with Scala data structures.

At its cores, this PR introduces a wrapper around a stack of tree nodes, which allows us to iterate through the tree's nodes depth-first.

cc @rob-stripe  @sritchie-stripe @thomas-stripe @erik-stripe @oscar-stripe @kelley-stripe @avi-stripe 
This PR adds some love to the Quick Start example in README. :)
- Fixed an issue where the SNAPSHOT version in `example/iris` didn't match what was in `version.sbt`. 
- Fixed an issue where `org.apache.hadoop` was excluded from runtime by sbt-assembly, causing a `ClassNotFoundException` to be thrown by `example/iris`. Basically the `hadoopClient` in `Deps.scala` was "provided" so it wasn't getting included in `target/scala-2.11/brushfire-scalding-0.7.5-SNAPSHOT-jar-with-dependencies.jar` during builds.

To support faster training (especially in local mode) we could be using compressed Bonsai trees directly in Brushfire. There's really no advantage to Brushfire's native tree type so we should remove it. The idea is that we'd batch our operations for 1 step (eg growing the tree by 1) and make the changes to the Bonsai tree in bulk.

This involves both exposing some new functionality from Bonsai, and replacing the use of trees (and the use of the tree ops type class) in Brushfire.

This PR gives us another way to evaluate how well predictions do
against the actual known distribution. The iris example has been
ported to demonstrated this method in practice.

There is also a small refactoring of the local trainer's validate
method, and some small refactors of other error classes.

This PR moves brushfire-core over to use `spire.algebra.PartialOrder` and `spire.algebra.Order` where applicable. At this point we don't actually _use_ any partial orderings, but the support is there.

Before we merge this I'd like to add some tests to prove this is working, and maybe flesh out support for the _is present_ use-case a bit. What do you all think we need?

Review by @tixxit and @avibryant.

Our tree generators for scalacheck are fairly complex, so if we created a new `brushfire-laws` package or something that included them, then they could be re-used elsewhere.

This is super early work, but we have a CsvTrainerJob, which can run on ~arbitrary CSVs, with the labels provided by the user. The actual types of the values will be inferred before training.

This makes a few related changes to `Evaluator`:
- it decouples it from `Split` and just has it directly operate on `Iterable[T]`, which was the only part of a `Split` it ever cared about anyway.
- this makes it possible to apply an `Evaluator` to a tree or sub-tree, which could be useful in various ways in the future (eg if we want to change `Splitter` to produce candidate sub-trees instead of splits)
- in this context the `Iterable[T]` represents the leaves of the sub-tree, and so it's now labeled as such
- rather than using `Infinity` to represent an unacceptable level of error, it now returns `Option[Double]` so you can return `None` to signal that
- the sign has been reversed (ie this is now an error value not a "goodness" value)
- it doesn't return a new `Split`, just the error
- the root `T` is now also provided. None of the evaluators makes use of it, but there are at least two potential uses I have in mind:
  - considering a split unacceptable based on some ratio of the leaf `T` to the root `T` (eg, "all leaves must contain at least X% of the input Ys")
  - using more global optimization criteria (this comes up in particular in the "explanation tree" use case which I won't explain in more detail here)

This is WIP because I haven't fully updated the training code to make use of it.

attn @tixxit @non

The intent here is to capture the basic mechanics of various training steps - updateTargets, expand, prune, etc - in a way that can be reused in multiple execution environments (local, scalding, spark, ...).

For now, this is all added directly to and used only by the Local trainer. The near-term impact is that the local trainer will stream over its input data in the same way that the distributed trainers do, rather than requiring it to all be loaded into memory. For this PR to be complete, we should move the training steps to their own module (maybe also doing https://github.com/stripe/brushfire/issues/51), and refactor the scalding trainer to use them.

It's very possible that this is too much or too little abstraction - right now it seems a bit overfit to the needs of the specific training steps and platforms we support, and I suspect it will be brittle going forward. (In fact, featureImportance already doesn't work with this, though I can argue that we should move to a TreeTraversal-based strategy for that which would). At the same time, I think _some_ approach like this will be valuable going forward, and I think it's better to start going imperfectly down this path.
