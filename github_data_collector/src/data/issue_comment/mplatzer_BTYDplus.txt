```
Error in mcmc.list(lapply(draws, function(draw) draw$level_1[[i]])) :
  Arguments must be mcmc objects
In addition: Warning message:
In mclapply(1:chains, function(i) run_single_chain(i, cal.cbs, hyper_prior),  :
  scheduled cores 1, 2, 8 did not deliver results, all values of the jobs will be affected
```
I keep getting this failing scheduled cores when trying to run pggg.mcmc.DrawParameters

Example of my parameters is below

```
pggg.mcmc.DrawParameters(cal.cbs = cbs,
                                       mcmc = 10000, burnin = 10000, chains = 8, thin = 20,
                                       mc.cores = parallel::detectCores())
```


how we can deal with big data when we use of BTYDplus? What is the most effective way. we have more than 1 billiard data with 40 million customers. 
Sometimes a sales is registered but but with a total amount of zero. For example, cust number 87, 155 and 227 in the cdnow data. These are considered as sales in the elog2cbs() function, but should be filtered out. 
this may be a dumb one... The model mbg-cnbd-k.R has the ConditionalExpectedTransactions function, where there is code to correct for bias. My question is: why **length >= 100**? is this some rule of thumb? what's the rational behind it? This makes the forecasts conditional to the length of the vector, and not only to their past behaviour. Am I missing something?

I also do not understand the comment: "Only do so, if we can  safely assume that the full customer cohort is passed."

```
# Adjust bias BG/NBD-based approximation by scaling via the Unconditional
# Expectations (for wich we have exact expression). Only do so, if we can
# safely assume that the full customer cohort is passed.
do.bias.corr <- k > 1 && length(x) == length(t.x) 
                            && length(x) == length(T.cal) && length(x) >= 100
```
Hi!

First, thanks a lot for the package. Really useful stuff here.

I noticed what I think is a bug in xbgcnbd.PlotFrequencyInCalibration (and potentially in mcmc.PlotFrequencyInCalibration, but I didn't use it directly).
The behavior at the line 
`x_act <- table(x_act)`
 is not correct, to me, when a member of the series 0:censor is missing in the table, e.g. if our censor is at 52 and nobody made 20,35, or 45 _repeat_ transactions.
It will not _fill with 0_ this frequency values and hence x_act will be smaller than x_ect and due to the way R handle this, the matrix will be ... strange (got a warning in _Rstudio_, didn't in _Visual Studio_)
It can be fixed by using:
`x_act <- table(factor(x_act, levels=c(0:censor)))`

See this _toy example_ to understand what i mean:

```
censor=25

x_act <- c(1:10, 20:30)
x_act[x_act > censor] <- censor
x_act <- table(x_act)
x_est <- 0:censor
mat <- matrix(c(x_act, x_est), nrow = 2, ncol = censor + 1, byrow = TRUE)
mat

x_act2 <- c(1:10, 20:30)
x_act2[x_act2 > censor] <- censor
x_act2 <- table(factor(x_act2,levels=c(0:censor)))
x_est <- 0:censor
mat2 <- matrix(c(x_act2, x_est), nrow = 2, ncol = censor + 1, byrow = TRUE)
mat2
```

I know this is a limit case. But I encountered it analysing some 'small' data subset and thought it would be worth rising it up here.
I'd gladly submit a pull request if you want.

Cheers,
SÃ©bastien

Ps: first post actually on github... please don't hate me if I'm too long...
e.g. for elog2cbs (in addition to data.frame and data.table)
```
structure(list(cust = c(103523979L, 106539747L, 109669649L, 114536229L, 
115893543L, 115959927L, 117271474L, 121815850L, 123091742L, 123545940L, 
126225453L, 126396424L, 126844622L, 126963045L, 132218991L, 133320753L, 
134979819L, 135327695L, 136081180L, 136651599L, 137087039L, 138027869L, 
138717722L, 139202404L, 141638965L, 142495136L, 143705614L, 145281013L, 
147186313L, 147408159L, 149208624L), x = c(0, 0, 0, 0, 1, 0, 
0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 4, 0, 0, 0, 
1, 0, 0, 0), t.x = c(0, 0, 0, 0, 20.1428571428571, 0, 0, 0, 0, 
0, 0, 3, 0, 0, 0.285714285714286, 0, 0, 16.8571428571429, 15.4285714285714, 
0, 0, 0, 0, 9.28571428571429, 0, 0, 0, 16.1428571428571, 0, 0, 
0), litt = c(0, 0, 0, 0, 3.00284974132285, 0, 0, 0, 0, 0, 0, 
1.09861228866811, 0, 0, -1.25276296849537, 0, 0, 2.82477447541035, 
2.73622107806891, 0, 0, 0, 0, 2.06327660482648, 0, 0, 0, 2.78147766965703, 
0, 0, 0), sales = c(154.98, 849.98, 269.94, 54.98, 499.43, 249.99, 
106.98, 944.97, 36.67, 349.97, 24.99, 2149.93, 257.71, 889.95, 
37.97, 29.99, 349.99, 1494.92, 594.92, 977.93, 479.99, 59.99, 
204.96, 7566.69, 54.98, 839.97, 14.99, 2914.91, 192.98, 208.94, 
1029.92), sales.x = c(0, 0, 0, 0, 162.98, 0, 0, 0, 0, 0, 0, 869.95, 
0, 0, 29.98, 0, 0, 274.94, 149.99, 0, 0, 0, 0, 1698.89, 0, 0, 
0, 139.99, 0, 0, 0), first = structure(c(12059, 12056, 12070, 
12079, 12075, 12056, 12077, 12066, 12058, 12070, 12059, 12078, 
12062, 12054, 12055, 12054, 12055, 12071, 12069, 12056, 12077, 
12064, 12062, 12066, 12082, 12080, 12082, 12069, 12062, 12069, 
12059), class = "Date"), T.cal = c(23.8571428571429, 24.2857142857143, 
22.2857142857143, 21, 21.5714285714286, 24.2857142857143, 21.2857142857143, 
22.8571428571429, 24, 22.2857142857143, 23.8571428571429, 21.1428571428571, 
23.4285714285714, 24.5714285714286, 24.4285714285714, 24.5714285714286, 
24.4285714285714, 22.1428571428571, 22.4285714285714, 24.2857142857143, 
21.2857142857143, 23.1428571428571, 23.4285714285714, 22.8571428571429, 
20.5714285714286, 20.8571428571429, 20.5714285714286, 22.4285714285714, 
23.4285714285714, 22.4285714285714, 23.8571428571429), T.star = c(74.7142857142857, 
74.7142857142857, 74.7142857142857, 74.7142857142857, 74.7142857142857, 
74.7142857142857, 74.7142857142857, 74.7142857142857, 74.7142857142857, 
74.7142857142857, 74.7142857142857, 74.7142857142857, 74.7142857142857, 
74.7142857142857, 74.7142857142857, 74.7142857142857, 74.7142857142857, 
74.7142857142857, 74.7142857142857, 74.7142857142857, 74.7142857142857, 
74.7142857142857, 74.7142857142857, 74.7142857142857, 74.7142857142857, 
74.7142857142857, 74.7142857142857, 74.7142857142857, 74.7142857142857, 
74.7142857142857, 74.7142857142857), x.star = c(0L, 0L, 0L, 1L, 
0L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 6L, 1L, 0L, 
0L, 1L, 0L, 2L, 0L, 2L, 0L, 2L, 0L, 0L, 1L), sales.star = c(0, 
0, 0, 54.9, 0, 155.97, 0, 0, 0, 299.94, 0, 379.98, 0, 210.76, 
149.99, 0, 0, 1597.82, 389.98, 0, 0, 393.92, 0, 399.97, 0, 1764.45, 
0, 1899.94, 0, 0, 49.99)), .Names = c("cust", "x", "t.x", "litt", 
"sales", "sales.x", "first", "T.cal", "T.star", "x.star", "sales.star"
), row.names = c(NA, -31L), class = "data.frame")

pggg.draws <- pggg.mcmc.DrawParameters(cbs, mcmc=2500, burnin=500, chains = 1, thin=50)
```

results in
```
Error in pggg_slice_sample("lambda", x = data$x, tx = data$t.x, Tcal = data$T.cal,  :
  slice_sample_cpp loop did not finish
```

When drawing pggg parameters I tend to get a limited (0.5 %) cluster of cases with mean `k`'s between 985 and 999, with absolutely no mean `k` between 50 and 985 (Expected aggregate `k` is around 0.8).

The only thing seemingly setting these cases apart is that they're somewhat regular but rather short-lived, they're probably somewhat overrepresented and must be confusing the algorithm.

The upper bound for `k` slice sampling is set at 1000, which at first sight I don't think is a realistic expectation in any scenario? I would suspect a limit of around 100 to be safer and would adjust the algorithm accordingly.

Speaking of assumptions, it occurred to me that `k`'s aggregate distribution is more likely to follow a lognormal than a gamma distribution. Even in the clumpiest of scenarios, extremely low `k`'s remain less likely than values around 0.5, with a few higher `k` cases always remaining quite likely, a situation the gamma distribution doesn't allow for.
