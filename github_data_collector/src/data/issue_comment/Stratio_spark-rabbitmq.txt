
Why i am getting the issue while trying to connect with rabbit mq 

[ActorRabbitMQSystem-akka.actor.default-dispatcher-2] [akka://ActorRabbitMQSystem/user/$a] connection failed
java.net.ConnectException: Connection refused: connect
i simply took the code and try to connect but not able to connect and get the below error

[akka://ActorRabbitMQSystem/user/$a] connection failed
java.net.ConnectException: Connection refused: connect

org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.AbstractMethodError
        at org.apache.spark.internal.Logging$class.initializeLogIfNecessary(Logging.scala:99)
        at org.apache.spark.streaming.rabbitmq.consumer.Consumer$.initializeLogIfNecessary(Consumer.scala:176)
        at org.apache.spark.internal.Logging$class.log(Logging.scala:46)
        at org.apache.spark.streaming.rabbitmq.consumer.Consumer$.log(Consumer.scala:176)
        at org.apache.spark.streaming.rabbitmq.consumer.Consumer$$anonfun$setUserPassword$1.apply(Consumer.scala:240)
        at org.apache.spark.streaming.rabbitmq.consumer.Consumer$$anonfun$setUserPassword$1.apply(Consumer.scala:238)
        at scala.Option.foreach(Option.scala:257)
        at org.apache.spark.streaming.rabbitmq.consumer.Consumer$.setUserPassword(Consumer.scala:238)
        at org.apache.spark.streaming.rabbitmq.consumer.Consumer$.apply(Consumer.scala:205)
        at org.apache.spark.streaming.rabbitmq.receiver.RabbitMQReceiver$$anonfun$3.apply(RabbitMQInputDStream.scala:60)
        at org.apache.spark.streaming.rabbitmq.receiver.RabbitMQReceiver$$anonfun$3.apply(RabbitMQInputDStream.scala:59)
        at scala.util.Try$.apply(Try.scala:192)
        at org.apache.spark.streaming.rabbitmq.receiver.RabbitMQReceiver.onStart(RabbitMQInputDStream.scala:59)
        at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
        at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
        at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:600)
        at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:590)
        at org.apache.spark.SparkContext$$anonfun$34.apply(SparkContext.scala:2185)
        at org.apache.spark.SparkContext$$anonfun$34.apply(SparkContext.scala:2185)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
        at org.apache.spark.scheduler.Task.run(Task.scala:109)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1609)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1597)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1596)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1596)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
        at scala.Option.foreach(Option.scala:257)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1830)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1779)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1768)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.lang.AbstractMethodError
        at org.apache.spark.internal.Logging$class.initializeLogIfNecessary(Logging.scala:99)
        at org.apache.spark.streaming.rabbitmq.consumer.Consumer$.initializeLogIfNecessary(Consumer.scala:176)
        at org.apache.spark.internal.Logging$class.log(Logging.scala:46)
        at org.apache.spark.streaming.rabbitmq.consumer.Consumer$.log(Consumer.scala:176)
        at org.apache.spark.streaming.rabbitmq.consumer.Consumer$$anonfun$setUserPassword$1.apply(Consumer.scala:240)
        at org.apache.spark.streaming.rabbitmq.consumer.Consumer$$anonfun$setUserPassword$1.apply(Consumer.scala:238)
        at scala.Option.foreach(Option.scala:257)
        at org.apache.spark.streaming.rabbitmq.consumer.Consumer$.setUserPassword(Consumer.scala:238)
        at org.apache.spark.streaming.rabbitmq.consumer.Consumer$.apply(Consumer.scala:205)
        at org.apache.spark.streaming.rabbitmq.receiver.RabbitMQReceiver$$anonfun$3.apply(RabbitMQInputDStream.scala:60)
        at org.apache.spark.streaming.rabbitmq.receiver.RabbitMQReceiver$$anonfun$3.apply(RabbitMQInputDStream.scala:59)
        at scala.util.Try$.apply(Try.scala:192)
        at org.apache.spark.streaming.rabbitmq.receiver.RabbitMQReceiver.onStart(RabbitMQInputDStream.scala:59)
        at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
        at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
        at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:600)
        at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:590)
        at org.apache.spark.SparkContext$$anonfun$34.apply(SparkContext.scala:2185)
        at org.apache.spark.SparkContext$$anonfun$34.apply(SparkContext.scala:2185)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
        at org.apache.spark.scheduler.Task.run(Task.scala:109)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
18/11/16 18:25:59 INFO scheduler.ReceiverTracker: Restarting Receiver 0


 val receiverStream = RabbitMQUtils.createStream(ssc, Map(
      "hosts" -> hosts, 
      "queueName" -> queueName,
      "exchangeName" -> exchangeName,
      "exchangeType" -> exchangeType,
      "**_vHost_**" -> vHost,
      "userName" -> userName,
      "password" -> password
    ))


vhost is wrong
The correct variable name is virtualHost
Please release a version with support for Apache Spark 2.2.x
Fixed correct link to Coverage so it looks nicer.
Updated example code for RabbitMQUtils.createDistributedStream with correct parameters.
Changing from pom to jar packaging to be able to build jar file. Uncertain why this was left as pom, since the project does not contain any other modules.
I couldn't find how to acknowledge a message, suggestion ?

channel.basicAck(...)