Add Support for Spark 2.4.3 and above #242 

The change includes adding support for Apache Spark 2.4.3 and Open JDK 8
It also removed backward compatibility for the Spark Version below 2.3.0. 
There is no feature change hence drop support will not impact any existing users because they will be using the version of older Jar

I am getting below error when checking if point falls within polygon. Here is the error.

org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost, executor driver): java.lang.RuntimeException: Extra(...99 47.0634893610001), [traced - not evaluated])
	at magellan.WKTParser$.parseAll(WKTParser.scala:97)
	at magellan.WKTParser.parseAll(WKTParser.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:

java.lang.RuntimeException: Extra(...99 47.0634893610001), [traced - not evaluated])


Dataset that I using is canada provinces and here it is.

[georef-poly-ca.zip](https://github.com/harsha2010/magellan/files/3277700/georef-poly-ca.zip)

Hi, this is my first ever PR to anything, so please bear with me if I have done something terribly illegal/bad practice :)

I have updated a range of the dependencies to allow for Magellan compatibility with Spark 2.4.3 and Scala 2.11.12. In order to make it work with the new versions of the dependencies I had to change a few things regarding class/object creation.

All tests are passing, when I run "sbt test".

Best Mads
```
val spark = SparkSession.builder
    .appName("Testing Spark DSL")
    .master("local[1]") //build a local cluster
    .getOrCreate()

//  injectRules(spark)

  import spark.implicits._

  val data = Array(("US", "TX", "2018-12-08 00:00:00", 12.0123, "ios", 2, 32.813548, -96.835159),
    ("US", "PA", "2018-12-08 00:00:00", 12.0123, "ios", 183,32.813548, -96.835159),
    ("CA", null, "2018-12-08 00:00:00", 12.0123, "android", 183,32.813548, -96.835159),
    ("GB", null, "2018-12-08 00:00:00", 12.0123, "ios", 2,32.813548, -96.835159),
    ("US", "NC", "2018-12-08 00:00:00", 12.0123, "android", 35,32.813548, -96.835159),
    ("US", "CA", "2018-12-08 00:00:00", 12.0123, null, 2,32.813548, -96.835159),
    ("A", null, "2018-12-08 00:00:00", 12.0123, "android", 183,32.813548, -96.835159),
    ("US", "NY", "2018-12-08 00:00:00", 12.0123, "ios", 2, 32.813548, -96.835159))

  val df1 = spark.sparkContext.parallelize(data).toDF("country", "state", "location_at",
    "horizontal_accuracy", "platform", "app_id", "latitude", "longitude")
    .withColumn("location_at", col("location_at").cast(TimestampType))
  df1.show()
  println(df1.printSchema)

  val filterFilePath = path_to_geojson

  val filteringDS = spark.sqlContext.read.format("magellan")
    .option("magellan.index", "true")
    .option("magellan.index.precision", "15")
    .option("type", "geojson").load(filterFilePath)
    .cache()

  filteringDS.count()
  filteringDS.show(false)

  val filtered = df1
    .withColumn("locationPoint", point(col("longitude"), col("latitude")))
    .join(filteringDS)
    .where(col("locationPoint") within col("polygon"))

  filtered.show()
```

Using the example above, if I just ```injectRules``` I get 0 results. But if I don't use ```injectRules``` I get the proper results.

Also, to note, I've tried different levels of precision in the index but the same issue persisted when injecting the rules.

Geojson file used for testing attached.
[TX.geojson.txt](https://github.com/harsha2010/magellan/files/2927079/TX.geojson.txt)


I would like to run the NYC Taxicab analysis notebook with Azure Databricks but the data is in S3.  How do I save the data into Azure?  Would I save to Azure Data Lake Store and then mount it to Databricks?

Thanks.
I need to use a functionality which is part of the current master but is not a part of the latest release (More specifically, read shapefiles (Polygons and Points) and convert them to GeoJson). I am referring to the asGeoJSON (/src/main/scala/magellan/dsl/package.scala) method for this purpose.

Could you please share when the next release is going to be made?
Hi Ram,
very interesting project.
so, i have a DF with neighborhoods represented as ZCurves in index column:

var index1 = neighborhoods.withColumn("index", $"polygon" index 30)
how to obtain the list of geohashes that represent each zone instead. 
same issue as mentioned here:
https://github.com/harsha2010/magellan/issues/193
he said he could solve it using the toBase32 function but he did not mention how.

can you tell me how to use this functionality (for example with my index1 dataframe above)

thanks a lot and wishing all the best for the project.


Include package in spark using: ./spark-shell --packages harsha2010:magellan:1.0.5-s_2.11

Try: import magellan.coord.NAD83

Error: object coord is not a member of package magellan
