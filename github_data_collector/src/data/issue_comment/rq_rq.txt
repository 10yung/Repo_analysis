I've tried django-rq, but it can't run worker with custom rq queue name, which is not present in Django settings.py. With rq it is possible. But found unhandled exception.

Env: django 2.2.9, rq 1.2.0.

Steps:
 - rqworker was launched in code as `proc = subprocess.Popen([executable + "/rqworker", str(uuid), "--burst", "--name=\"" + uuid + "\""]`
 - Queue configured in code as `Queue(name=uuid, connection=self.redis, is_async=True)`

Error:

```python 01:30:55: Worker rq:worker:"74fa4d0a-0960-40bd-8380-5b9bd01f786e": started, version 1.2.0
01:30:55: *** Listening on 74fa4d0a-0960-40bd-8380-5b9bd01f786e...
01:30:55: Cleaning registries for queue: 74fa4d0a-0960-40bd-8380-5b9bd01f786e
01:30:55: 74fa4d0a-0960-40bd-8380-5b9bd01f786e: start_run({'test':'1'}) (28c187b4-11b2-49b9-bf45-372035d6d824)
01:30:55: Worker rq:worker:"74fa4d0a-0960-40bd-8380-5b9bd01f786e": found an unhandled exception, quitting...
Traceback (most recent call last):
  File "/home/user/venv/lib/python3.7/site-packages/rq/job.py", line 54, in unpickle
    obj = loads(pickled_string)
  File "/home/user/software/loader/methods/my_loader.py", line 8, in <module>
    from loader.models import Model1, Model2, Model3, Model4
  File "/home/user/software/loader/models.py", line 7, in <module>
    class Environments(models.Model):
  File "/home/user/venv/lib/python3.7/site-packages/django/db/models/base.py", line 103, in __new__
    app_config = apps.get_containing_app_config(module)
  File "/home/user/venv/lib/python3.7/site-packages/django/apps/registry.py", line 252, in get_containing_app_config
    self.check_apps_ready()
  File "/home/user/venv/lib/python3.7/site-packages/django/apps/registry.py", line 135, in check_apps_ready
    raise AppRegistryNotReady("Apps aren't loaded yet.")
django.core.exceptions.AppRegistryNotReady: Apps aren't loaded yet.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/venv/lib/python3.7/site-packages/rq/worker.py", line 515, in work
    self.execute_job(job, queue)
  File "/home/user/venv/lib/python3.7/site-packages/rq/worker.py", line 727, in execute_job
    self.fork_work_horse(job, queue)
  File "/home/user/venv/lib/python3.7/site-packages/rq/worker.py", line 667, in fork_work_horse
    self.main_work_horse(job, queue)
  File "/home/user/venv/lib/python3.7/site-packages/rq/worker.py", line 744, in main_work_horse
    raise e
  File "/home/user/venv/lib/python3.7/site-packages/rq/worker.py", line 741, in main_work_horse
    self.perform_job(job, queue)
  File "/home/user/venv/lib/python3.7/site-packages/rq/worker.py", line 866, in perform_job
    self.prepare_job_execution(job, heartbeat_ttl)
  File "/home/user/venv/lib/python3.7/site-packages/rq/worker.py", line 785, in prepare_job_execution
    self.procline(msg.format(job.func_name, job.origin, time.time()))
  File "/home/user/venv/lib/python3.7/site-packages/rq/job.py", line 246, in func_name
    self._unpickle_data()
  File "/home/user/venv/lib/python3.7/site-packages/rq/job.py", line 214, in _unpickle_data
    self._func_name, self._instance, self._args, self._kwargs = unpickle(self.data)
  File "/home/user/venv/lib/python3.7/site-packages/rq/job.py", line 56, in unpickle
    raise UnpickleError('Could not unpickle', pickled_string, e)
rq.exceptions.UnpickleError: ('Could not unpickle', AppRegistryNotReady("Apps aren't loaded yet."))
`

Looks like when creating a scheduled job, the job status is not set to "scheduled". Think this thing may cause new problems when deleting jobs from scheduled registry
The current way the logger is configured in the scheduler impacts other loggers in our project. This probably relates to https://github.com/rq/rq/issues/1177 too.

This PR uses RQ's `setup_loghandlers` instead of calling `logging.basicConfig` directly.
There appears to be a change in logging behavior between 1.1.0 and 1.2.0 that I'm having trouble overriding.  Previously, I was able to get the workers to output JSON output by passing in a CustomWorker that used this logging configuration:

```python
from pythonjsonlogger import jsonlogger

import logging
import progressbar
import sys


class CustomJsonFormatter(jsonlogger.JsonFormatter):
    def add_fields(self, log_record, record, message_dict):
        super(CustomJsonFormatter, self).add_fields(log_record, record, message_dict)
        log_record["@timestamp"] = log_record["asctime"]

        if log_record.get("level"):
            log_record["level"] = log_record["level"].upper()
        else:
            log_record["level"] = record.levelname


logger = logging.getLogger()
logHandler = logging.StreamHandler(sys.stdout)
logger.setLevel(logging.INFO)
logHandler.setLevel(logging.INFO)
FORMAT = (
    "%(asctime)s %(name)s %(levelname)s "
    + "%(message)s %(pathname)s %(exc_info)s %(funcName)s %(lineno)d %(thread)d "
    + "%(process)d %(processName)s %(threadName)s"
)
formatter = CustomJsonFormatter(FORMAT)
logHandler.setFormatter(formatter)
logger.addHandler(logHandler)
```

However, now each worker emits two logs, one correctly formatted to stdout and another using the default Python formatter to stderr.  What's the preferred way to override this and emit one log output again?
Hi
Is there some way to queue a task from PHP directly? Or, over HTTP? Or, in another way?
In addition to queuing, there is a need to wait until the worker's completion, and get the result of his work to PHP. Is it possible?
Used Redis Broker.
Hi, I'm trying to unpickle the rq data present inside the Redis key but it's failing with `invalid load key`

Here's how it looks like
```
 1) "exc_info"
 2) "x\x9c\x03\x00\x00\x00\x00\x01"
 3) "data"
 4) "x\x9c%\x8c1\n\x021\x10E\x05\xc5=\x80\x17\x10l\xb61\x8d\xa0\xee\x19\x04\xaf0\x84dV\x83\x9bd\x9dL\xb6\x13l\x04\x8bt\x8e\xb7\xf1p.\xfa\x8b\xcf\x83\xff\xf8\xf7\xd9\xfb3\xf9\xa7.KBk4\xf5\xc8j$\x97\xe0\x9a1\xa3\xfa5\xb49\x189\x96\xb5\xd74\xb8\xa0|\xb4\xd8\xa9\x10\x03$\xce\x16\x03\x83\xe9rb\xa4\x04\xff)\xb3\xeb\x92\x94\xd5\t\x03\x92f\x84Hp\xc2Q\xd3\xe6\x8c\x16\x92\x89\x84`\xa2\xf7\x8e\xe5%\x0f\xb9I]\xaa\x9c\x90\xc0Y)\xf3\xa6\xd9\xee7;)\xd50^\xba\x18\xe40-\x8b\xde\x99\x0bX\xcd\x1aZ\x8a\x1el4I\x9e\x99E}\x011\xaaM&"
```

Any help?
I'm facing this issues while using rq with tornado app
https://stackoverflow.com/questions/59439905/create-rq-worker-in-python-using-tornado-get-post-request.

successor to #1155 

This PR updates `Queue#enqueue_dependents` such that only those _dependents_ for which all _dependencies_ are complete, will be enqueued. It includes:

- New method on `Job#get_dependencies_statuses` to retrieve the status of all of a jobs _dependencies_.
- A modification to `Queue#enqueue_dependents` (as described above).
- A modification to `Worker#handle_job_success` so that the status of **ALL** successful jobs is persisted to `FINISHED` in Redis.
- A small fix(?) to `Job#set_status` so that it uses a pipeline when appropriate.

Notwithstanding any bugs, I believe the next step here is to modify `Job` such that it accepts more than one dependency(in `depends_on`.
â€¦in case the forked process gets stuck and cannot stop itself.

#323
As a workaround I changed the value HIGHEST_PROTOCOL to 4 in job.py.
Line 27 of jobs.py: 
dumps = partial(pickle.dumps, protocol=4)

rq.exceptions.UnpickleError: ('Could not unpickle', ValueError('unsupported pickle protocol: 5',))

Can the protocol value be set somewhere, as a client of rq?