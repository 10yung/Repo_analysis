Greetings,

Thanks for your great paper. I am wondering about the hyperparameters you used for language modeling experiments. Could you provide information about that?

Thank you!

https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer

https://github.com/mgrankin/over9000/issues/4

I strongly believe that AdaBound would be better if it used RAdam instead of Adam. 
It could merge with Lookahead too and LAMB. 
Then we would have the best of both worlds and a beautiful example of scientific collaboration. 
correct grammar would be "as well as adam"

not sure if you care

![camp](https://user-images.githubusercontent.com/34268595/57842253-bb57c200-77fe-11e9-9a3d-59e34471eb35.png)

I tested three methods in a very simple problem, and got the result as above.

Code are printed here:

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import adabound

class Net(nn.Module):
    
    def __init__(self, dim):
        
        super(Net, self).__init__()
        self.fc1 = nn.Linear(dim, 2*dim)
        self.relu = nn.ReLU(inplace=True)
        self.fc2 = nn.Linear(2*dim, dim)
    
    def forward(self, x):
        
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        
        return x

DIM = 30
epochs = 1000
xini = (torch.ones(1, DIM) * 100)
opti = (torch.zeros(1, DIM) * 100)

lr = 0.01
net = Net(DIM)
objfun = nn.MSELoss()

loss_adab = []
loss_adam = []
loss_sgd = []
for epoch in range(epochs):
    
    if epoch % 100 == 0:
        lr /= 10
    
    optimizer = adabound.AdaBound(net.parameters(), lr) 
    out = net(xini)
    los = objfun(out, opti)
    loss_adab.append(los.detach().numpy())
    
    optimizer.zero_grad()
    los.backward()
    optimizer.step()

lr = 0.01
net = Net(DIM)
objfun = nn.MSELoss()    

for epoch in range(epochs):
    
    if epoch % 100 == 0:
        lr /= 10
    
    optimizer = torch.optim.Adam(net.parameters(), lr) 
    out = net(xini)
    los = objfun(out, opti)
    loss_adam.append(los.detach().numpy())
    
    optimizer.zero_grad()
    los.backward()
    optimizer.step()   
    
lr = 0.001
net = Net(DIM)
objfun = nn.MSELoss()    

for epoch in range(epochs):
    
    if epoch % 100 == 0:
        lr /= 10
    
    optimizer = torch.optim.SGD(net.parameters(), lr, momentum=0.9) 
    out = net(xini)
    los = objfun(out, opti)
    loss_sgd.append(los.detach().numpy())
    
    optimizer.zero_grad()
    los.backward()
    optimizer.step()
 
plt.figure()
plt.plot(loss_adab, label='adabound')
plt.plot(loss_adam, label='adam')
plt.plot(loss_sgd, label='SGD')
plt.yscale('log')
plt.xlabel('epochs')
plt.ylabel('Log(loss)')
plt.legend()
plt.savefig('camp.png', dpi=600)
plt.show() 
Thank you very much for sharing this impressive work. I am somehow receiving the following error:

```
    for group, base_lr in zip(self.param_groups, self.base_lrs):
AttributeError: 'AdaBound' object has no attribute 'base_lrs'
```
https://github.com/wayne391/Image-Super-Resolution/blob/master/src/models/RCAN.py

Just change
`optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, amsgrad=False)`
to
`optimizer = adabound.AdaBound(model.parameters(), lr=1e-4, final_lr=0.1)`

Nan loss in RCAN model, but Adam work fine.
The provided new optimizer is sensitive on tiny batchsize (<4), I am testing on the very simply linear regression, while others performance looks like nice currently.

Path:

![1](https://user-images.githubusercontent.com/10858450/53678905-48d64c00-3cbd-11e9-9583-4cf56c8af760.png)


Loss curve:
![2](https://user-images.githubusercontent.com/10858450/53678907-4d026980-3cbd-11e9-97bb-e889923b8c89.png)


Zoomed Loss curve:
![3](https://user-images.githubusercontent.com/10858450/53678910-4ffd5a00-3cbd-11e9-88f3-15fc1ed8b182.png)


I don't see any reason why this code would not run in a lower version of python.
Could you explain why is there such a requirement?
```
# Applies bounds on actual learning rate
# lr_scheduler cannot affect final_lr, this is a workaround to apply lr decay
final_lr = group['final_lr'] * group['lr'] / base_lr`
```
However lr_scheduler may change param_group['lr'] during training, therefore the final_lr, lower_bound, upper_bound will also be affected. 

Should I not use lr_scheduler and let AbaBound adapts the params to transform from Adam to SGD?

Thank you very much!