hi all 
I have set up a kubernetes cluster with a master node and two worker node. and calico for the network.
tunnels can not ping each others.
Kubernetes Version: 1.17
Calico Version: v3.11.1

Master IP : 1.1.1.1/22
Node1 IP: 2.2.2.2/22
Node2 IP: 3.3.3.3/23
IP's are these numbers, but are valid IP's.

`watch -n 0 kubectl get pods --all-namespaces ` output is :
```
on master : NAMESPACE              NAME                                 READY   STATUS    RESTARTS   AGE
default                nginx-86c57db685-6754h                           1/1     Running   0          178m
default                tvts-c667fd89b-cwfx2                             1/1     Running   0          175m
default                tvts-c667fd89b-wgbnq                             1/1     Running   0          176m
kube-system            calico-kube-controllers-5b644bc49c-zwkk2         1/1     Running   0          31m
kube-system            calico-node-n7s68                                1/1     Running   0          31m
kube-system            calico-node-w4p52                                1/1     Running   0          31m
kube-system            calico-node-zdghr                                1/1     Running   0          31m
kube-system            coredns-6955765f44-9d4bc                         1/1     Running   0          3h4m
kube-system            coredns-6955765f44-x86pf                         1/1     Running   0          3h4m
kube-system            etcd-srvarvan-kube                               1/1     Running   0          3h4m
kube-system            kube-apiserver-srvarvan-kube                     1/1     Running   0          3h4m
kube-system            kube-controller-manager-srvarvan-kube            1/1     Running   0          3h4m
kube-system            kube-proxy-9sbvb                                 1/1     Running   0          3h4m
kube-system            kube-proxy-nc86c                                 1/1     Running   0          179m
kube-system            kube-proxy-xws62                                 1/1     Running   0          178m
kube-system            kube-scheduler-srvarvan-kube                     1/1     Running   0          3h4m
kubernetes-dashboard   dashboard-metrics-scraper-76585494d8-2mcn8       1/1     Running   0          3h1m
kubernetes-dashboard   kubernetes-dashboard-f9bd45cd6-t4j6d             1/1     Running   0          3h1m
```
on node Master (1.1.1.1)
```
sudo calicoctl node status
+-----------------+-------------------+-------+----------+-------------+
|  PEER ADDRESS   |     PEER TYPE     | STATE |  SINCE   |    INFO     |
+-----------------+-------------------+-------+----------+-------------+
| 2.2.2.2         | node-to-node mesh | up    | 11:01:07 | Established |
| 3.3.3.3         | node-to-node mesh | up    | 11:01:09 | Established |
+-----------------+-------------------+-------+----------+-------------+
```
on nodes1 (2.2.2.2):
```
sudo calicoctl node status
+-----------------+-------------------+-------+----------+-------------+
|  PEER ADDRESS   |     PEER TYPE     | STATE |  SINCE   |    INFO     |
+-----------------+-------------------+-------+----------+-------------+
| 1.1.1.1         | node-to-node mesh | up    | 11:01:07 | Established |
| 3.3.3.3        | node-to-node mesh | up    | 11:01:09 | Established |
+-----------------+-------------------+-------+----------+-------------+
```
on nodes2 (3.3.3.3):
```
sudo calicoctl node status
+-----------------+-------------------+-------+----------+-------------+
|  PEER ADDRESS   |     PEER TYPE     | STATE |  SINCE   |    INFO     |
+-----------------+-------------------+-------+----------+-------------+
| 1.1.1.1         | node-to-node mesh | up    | 11:01:07 | Established |
| 2.2.2.2         | node-to-node mesh | up    | 11:01:09 | Established |
+-----------------+-------------------+-------+----------+-------------+
```
the logs of the calico pods for node1 are :
Readiness probe failed: calico/node is not ready: BIRD is not ready: BGP not established with 3.3.3.3,1.1.1.12020-01-18 11:01:31.480 [INFO][160] health.go 156: Number of node(s) with BGP peering established = 0

Readiness probe failed: calico/node is not ready: BIRD is not ready: BGP not established with 1.1.1.1,3.3.3.32020-01-18 11:01:31.480 [INFO][160] health.go 156: Number of node(s) with BGP peering established = 1

and the same log for ndoe2 

in calico.yaml I have set the   IP_AUTODETECTION_METHOD to "interface=eth.*"
```
  - name: IP_AUTODETECTION_METHOD
              value: "interface=eth.*"            
```
also I have test the `value: value: can-reach=8.8.8.8`. but nothing changed.

I dont know what is my fault . tunnels cannot see each other 
and each node only can see its pods, not the other node's pod

also my pod network is 12.0.0.0/16 
but the pods and tunnels get the IP with the /32 netmask like: 12.0.66.128/32
Changed  `watch kubectl get pods --all-namespaces` to `kubectl get pods --all-namespaces --watch`

## Description

This is a documentation fix. The merge would create more accurate documentation, but the change is minor.
## Description

<!-- A few sentences describing the overall goals of the pull request's commits.
Please include
- the type of fix - (e.g. bug fix, new feature, documentation)
- some details on _why_ this PR should be merged
- the details of the testing you've done on it (both manual and automated)
- which components are affected by this PR
- links to issues that this PR addresses
-->

## Related issues/PRs

<!-- If appropriate, include a link to the issue this fixes.
fixes <ISSUE LINK>

If appropriate, add links to any number of PRs documented by this PR
documents <PR LINK>
-->

## Todos

- [ ] Tests
- [ ] Documentation
- [ ] Release note

## Release Note

<!-- Writing a release note:
- By default, no release note action is required.
- If you're unsure whether or not your PR needs a note, ask your reviewer for guidance.
- If this PR requires a release note, update the block below to include a concise note describing
  the change and any important impacts this PR may have.
-->

```release-note
None required
```

I was trying to Setup a Multi-Master Kubernetes setup with external etcd 3 nodes cluster on bare metal REHL 7.2.  Etcd cluster setup got successful. After running the kubeadm init with --pod-cidr 192.168.0.0/16, i had applied calico 3.8 YAML. Containers were very much up and running. But when I'm adding a second master or data node the calico containers which were spun on them are getting in crash back loop. 

**Kube version: 1.17**

Note: I have updated calico.yaml with below, but still the same error.
- name: IP_AUTODETECTION_METHOD
   value: "interface=eth.*"

**Describe POD**

`kubectl describe pod calico-node-hr65k -n kube-system
Name:                 calico-node-hr65k
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 n4tenl-depa0601/10.5.30.104
Start Time:           Wed, 15 Jan 2020 18:31:12 +0530
Labels:               controller-revision-hash=89867b6d7
                      k8s-app=calico-node
                      pod-template-generation=1
Annotations:          scheduler.alpha.kubernetes.io/critical-pod:
Status:               Running
IP:                   10.5.30.104
IPs:
  IP:           10.5.30.104
Controlled By:  DaemonSet/calico-node
Init Containers:
  upgrade-ipam:
    Container ID:  docker://8252f021ff312541ff939bc744e7233c65e7fbf587920b48448746fb54b1184c
    Image:         calico/cni:v3.8.4
    Image ID:      docker-pullable://calico/cni@sha256:040aafdeef49322a1d79f2ec438852af00df7c04d4354fec1acf3e41abb17d12
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/cni/bin/calico-ipam
      -upgrade
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 15 Jan 2020 18:31:13 +0530
      Finished:     Wed, 15 Jan 2020 18:31:13 +0530
    Ready:          True
    Restart Count:  0
    Environment:
      KUBERNETES_NODE_NAME:        (v1:spec.nodeName)
      CALICO_NETWORKING_BACKEND:  <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false
    Mounts:
      /host/opt/cni/bin from cni-bin-dir (rw)
      /var/lib/cni/networks from host-local-net-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-gtp7p (ro)
  install-cni:
    Container ID:  docker://93af302c5ab7a1078347b4b00dfdf793dfd78d028053c55cd0c7a89342e81256
    Image:         calico/cni:v3.8.4
    Image ID:      docker-pullable://calico/cni@sha256:040aafdeef49322a1d79f2ec438852af00df7c04d4354fec1acf3e41abb17d12
    Port:          <none>
    Host Port:     <none>
    Command:
      /install-cni.sh
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 15 Jan 2020 18:31:14 +0530
      Finished:     Wed, 15 Jan 2020 18:31:14 +0530
    Ready:          True
    Restart Count:  0
    Environment:
      CNI_CONF_NAME:         10-calico.conflist
      CNI_NETWORK_CONFIG:    <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false
      KUBERNETES_NODE_NAME:   (v1:spec.nodeName)
      CNI_MTU:               <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false
      SLEEP:                 false
    Mounts:
      /host/etc/cni/net.d from cni-net-dir (rw)
      /host/opt/cni/bin from cni-bin-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-gtp7p (ro)
  flexvol-driver:
    Container ID:   docker://b16b00fa4712f1aee8901484ac2ddd5a31aaae305b988258815b6f731b92ebc7
    Image:          calico/pod2daemon-flexvol:v3.8.4
    Image ID:       docker-pullable://calico/pod2daemon-flexvol@sha256:6dd45f10b54e5d3c9c226504460b12ff3d2a232754c0aa86e8f02d92cf673d94
    Port:           <none>
    Host Port:      <none>
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 15 Jan 2020 18:31:15 +0530
      Finished:     Wed, 15 Jan 2020 18:31:15 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /host/driver from flexvol-driver-host (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-gtp7p (ro)
Containers:
  calico-node:
    Container ID:   docker://2db21392f6fafae9fe847b91ea29f83e09b9a3eaeb6d48f9b7232f158dd2f50a
    Image:          calico/node:v3.8.4
    Image ID:       docker-pullable://calico/node@sha256:ae2a196f15724fe994e41d8992ed01ad7df871eb5529d55e4e7c55ab0d7ef791
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Thu, 16 Jan 2020 16:22:23 +0530
    Last State:     Terminated
      Reason:       Error
      Exit Code:    137
      Started:      Thu, 16 Jan 2020 16:16:11 +0530
      Finished:     Thu, 16 Jan 2020 16:17:20 +0530
    Ready:          False
    Restart Count:  358
    Requests:
      cpu:      250m
    Liveness:   http-get http://localhost:9099/liveness delay=10s timeout=1s period=10s #success=1 #failure=6
    Readiness:  exec [/bin/calico-node -bird-ready -felix-ready] delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      DATASTORE_TYPE:                     kubernetes
      WAIT_FOR_DATASTORE:                 true
      NODENAME:                            (v1:spec.nodeName)
      CALICO_NETWORKING_BACKEND:          <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false
      CLUSTER_TYPE:                       k8s,bgp
      IP:                                 autodetect
      IP_AUTODETECTION_METHOD:            interface=ens192
      CALICO_IPV4POOL_IPIP:               Always
      FELIX_IPINIPMTU:                    <set to the key 'veth_mtu' of config map 'calico-config'>  Optional: false
      CALICO_IPV4POOL_CIDR:               192.168.0.0/16
      CALICO_DISABLE_FILE_LOGGING:        true
      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT
      FELIX_IPV6SUPPORT:                  false
      FELIX_LOGSEVERITYSCREEN:            info
      FELIX_HEALTHENABLED:                true
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/calico from var-lib-calico (rw)
      /var/run/calico from var-run-calico (rw)
      /var/run/nodeagent from policysync (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from calico-node-token-gtp7p (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:
  var-run-calico:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/calico
    HostPathType:
  var-lib-calico:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/calico
    HostPathType:
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  cni-bin-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /opt/cni/bin
    HostPathType:
  cni-net-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:
  host-local-net-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/cni/networks
    HostPathType:
  policysync:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/nodeagent
    HostPathType:  DirectoryOrCreate
  flexvol-driver-host:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
    HostPathType:  DirectoryOrCreate
  calico-node-token-gtp7p:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  calico-node-token-gtp7p
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  beta.kubernetes.io/os=linux
Tolerations:     :NoSchedule
                 :NoExecute
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason     Age                     From                      Message
  ----     ------     ----                    ----                      -------
  Warning  Unhealthy  6m58s (x2430 over 21h)  kubelet, n4tenl-depa0601  Readiness probe failed: calico/node is not ready: felix is not ready: Get http://localhost:9099/readiness: dial tcp 127.0.0.1:9099: connect: connection refused
  Warning  BackOff    118s (x4349 over 21h)   kubelet, n4tenl-depa0601  Back-off restarting failed container`

**Conatiner LOGS**

kubectl logs calico-node-hr65k  -n kube-system -c calico-node
2020-01-16 10:53:31.539 [INFO][8] startup.go 256: Early log level set to info
2020-01-16 10:53:31.540 [INFO][8] startup.go 272: Using NODENAME environment for node name
2020-01-16 10:53:31.540 [INFO][8] startup.go 284: Determined node name: n4tenl-depa0601
2020-01-16 10:53:31.541 [INFO][8] k8s.go 228: Using Calico IPAM
2020-01-16 10:53:31.542 [INFO][8] startup.go 316: Checking datastore connection
2020-01-16 10:54:01.543 [INFO][8] startup.go 331: Hit error connecting to datastore - retry error=Get https://10.96.0.1:443/api/v1/nodes/foo: dial tcp 10.96.0.1:443: i/o timeout
2020-01-16 10:54:32.543 [INFO][8] startup.go 331: Hit error connecting to datastore - retry error=Get https://10.96.0.1:443/api/v1/nodes/foo: dial tcp 10.96.0.1:443: i/o timeout


I have read almost every similar issue raised in GITHUB but still not able to proceed further.


## Description

The GNP `namespaceSelector` and `serviceAccountSelector` should be grouped with `selector`.
<!-- A few sentences describing the overall goals of the pull request's commits.
Please include
- the type of fix - (e.g. bug fix, new feature, documentation)
- some details on _why_ this PR should be merged
- the details of the testing you've done on it (both manual and automated)
- which components are affected by this PR
- links to issues that this PR addresses
-->

## Related issues/PRs

<!-- If appropriate, include a link to the issue this fixes.
fixes <ISSUE LINK>

If appropriate, add links to any number of PRs documented by this PR
documents <PR LINK>
-->

## Todos

- [ ] Tests
- [ ] Documentation
- [ ] Release note

## Release Note

<!-- Writing a release note:
- By default, no release note action is required.
- If you're unsure whether or not your PR needs a note, ask your reviewer for guidance.
- If this PR requires a release note, update the block below to include a concise note describing
  the change and any important impacts this PR may have.
-->

```release-note
None required
```

I have a k8s cluster with version 1.13.6. Then I added a node and the cluster seemed to be working fine, but calico had an error: bird: Netlink: Network is down. Only this new node has this error, the other nodes are normal, then this new node cannot connect to the pods of other nodes, nor can it resolve svc.My calico was upgraded from 3.6.1 to 3.6.5.


## Expected Behavior
I want to solve the problem of bird: Netlink: Network is down

## Current Behavior
Cluster status:
````
[root@k8s-master01 v3.6.5]# kubectl get node
NAME                STATUS   ROLES   AGE    VERSION
k8s-master01.xiqu   Ready    node    243d   v1.13.6
k8s-master02.xiqu   Ready    node    147m   v1.13.6
k8s-master03.xiqu   Ready    node    243d   v1.13.6
k8s-node01.xiqu     Ready    node    243d   v1.13.6
k8s-node02.xiqu     Ready    node    243d   v1.13.6

k8s-master02.xiqu is new node.
````

New Node Route info:
````
[root@k8s-master02 ~]# ip route
default via 10.103.236.254 dev enp7s0f0 proto static metric 100 
10.103.236.0/24 dev enp7s0f0 proto kernel scope link src 10.103.236.178 metric 100 
169.254.0.0/16 dev enp7s0f0 scope link metric 1002 
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 
blackhole 177.245.72.64/26 proto bird 
177.245.72.68 dev cali34878b38246 scope link 
177.245.72.70 dev cali74e0d914f9c scope link
````

Then I started a container on the new node, and then this container could not resolve the Kubernetes service, and could not communicate with the Pods of other nodes.
````
[root@k8s-master01 v3.6.5]# kubectl get po -owide
NAME                            READY   STATUS    RESTARTS   AGE   IP                NODE                NOMINATED NODE   READINESS GATES
fortio-deploy-8ddb4b96d-cckv5   1/1     Running   0          46d   177.253.190.121   k8s-master03.xiqu   <none>           <none>
redis-trib-7f84cc4c56-t8q7f     1/1     Running   0          52m   177.245.72.70     k8s-master02.xiqu   <none>           <none>
[root@k8s-master01 v3.6.5]# kubectl exec -ti redis-trib-7f84cc4c56-t8q7f  -- nslookup kubernetes
nslookup: can't resolve '(null)': Name does not resolve

nslookup: can't resolve 'kubernetes': Try again
command terminated with exit code 1

````

## Context
````
My newly added node cannot communicate with the pods of other nodes, hoping to solve this problem.
````

## Your Environment
* Calico version  --> 3.6.5
* Orchestrator version (e.g. kubernetes, mesos, rkt):
````
[root@k8s-master01 v3.6.5]# kubectl version
Client Version: version.Info{Major:"1", Minor:"13", GitVersion:"v1.13.6", GitCommit:"abdda3f9fefa29172298a2e42f5102e777a8ec25", GitTreeState:"clean", BuildDate:"2019-05-08T13:53:53Z", GoVersion:"go1.11.5", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"13", GitVersion:"v1.13.6", GitCommit:"abdda3f9fefa29172298a2e42f5102e777a8ec25", GitTreeState:"clean", BuildDate:"2019-05-08T13:46:28Z", GoVersion:"go1.11.5", Compiler:"gc", Platform:"linux/amd64"}
````
* Operating System and version: 
````
[root@k8s-master02 ~]# cat /etc/redhat-release 
CentOS Linux release 7.7.1908 (Core)
You have new mail in /var/spool/mail/root
[root@k8s-master02 ~]# uname 
Linux
[root@k8s-master02 ~]# uname -a
Linux k8s-master02.xiqu 4.18.9-1.el7.elrepo.x86_64 #1 SMP Thu Sep 20 09:04:54 EDT 2018 x86_64 x86_64 x86_64 GNU/Linux
````

* calico status
````
[root@k8s-master01 v3.6.5]# kubectl exec -ti calicoctl-6c6fbb8749-snxs7  -n !$ -- calicoctl get node -owide
kubectl exec -ti calicoctl-6c6fbb8749-snxs7  -n kube-system -- calicoctl get node -owide
NAME                ASN         IPV4                IPV6   
k8s-master01.xiqu   (unknown)   10.103.236.177/24          
k8s-master02.xiqu   (unknown)   10.103.236.178/24          
k8s-master03.xiqu   (unknown)   10.103.236.179/24          
k8s-node01.xiqu     (unknown)   10.103.236.175/24          
k8s-node02.xiqu     (unknown)   10.103.236.176/24
````

Calico logs:
````
bird: Netlink: Network is down
bird: Netlink: Network is down
bird: ...
bird: Netlink: Network is down
bird: Netlink: Network is down
bird: ...
2020-01-15 10:49:49.077 [INFO][76] health.go 150: Overall health summary=&health.HealthReport{Live:true, Ready:true}
bird: Netlink: Network is down
bird: Netlink: Network is down
bird: ...
bird: Netlink: Network is down
bird: Netlink: Network is down
bird: ...
2020-01-15 10:49:53.933 [INFO][76] int_dataplane.go 751: Applying dataplane updates
2020-01-15 10:49:53.933 [INFO][76] ipsets.go 223: Asked to resync with the dataplane on next update. family="inet"
2020-01-15 10:49:53.933 [INFO][76] ipsets.go 254: Resyncing ipsets with dataplane. family="inet"
2020-01-15 10:49:53.944 [INFO][76] ipsets.go 304: Finished resync family="inet" numInconsistenciesFound=0 resyncDuration=11.101272ms
2020-01-15 10:49:53.945 [INFO][76] int_dataplane.go 765: Finished applying updates to dataplane. msecToApply=11.640496
bird: Netlink: Network is down
bird: Netlink: Network is down
bird: ...
2020-01-15 10:49:55.165 [INFO][76] health.go 150: Overall health summary=&health.HealthReport{Live:true, Ready:true}
bird: Netlink: Network is down
bird: Netlink: Network is down
bird: ...
bird: Netlink: Network is down
bird: Netlink: Network is down
````

How can I configure a firewall using GlobalNetworkPolicy manifests on interface eth0 for IPv4 and IPv6 in Kubernetes?

I has been configured HostEndpoint

```yaml
---
apiVersion: crd.projectcalico.org/v1
kind: HostEndpoint
metadata:
  name: node1-eth0-ipv4
  labels:
    kubernetes-host: ingress
spec:
  interfaceName: eth0
  node: node1
  expectedIPs:
    - "136.136.136.136"
    - "2001:db8:85a3:8d3:1319:8a2e:370:7348"
```

Applying it with kubectl
```bash
kubectl apply -f manifest.yaml
```

Now creating GlobalNetworkPolicy for IPv4 and IPv6:
```yaml
---
apiVersion: crd.projectcalico.org/v1
kind: GlobalNetworkPolicy
metadata:
  name: drop-node-ingress-ipv6-web
spec:
  order: 0
  selector: has(kubernetes-host)
  ingress:
    - action: Deny
      protocol: TCP
      ipVersion: 6
      destination:
        ports: [8181]
    - action: Deny
      protocol: TCP
      ipVersion: 4
      destination:
        ports: [8181]
```

But there are not any ip6tables rules
```bash
ip6tables-save
```

```# Generated by ip6tables-save v1.6.1 on Wed Jan 15 10:38:30 2020
*nat
:PREROUTING ACCEPT [0:0]
:INPUT ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
:POSTROUTING ACCEPT [0:0]
COMMIT
# Completed on Wed Jan 15 10:38:30 2020
```

## Description
This PR adds a development environment to calico that requires only docker. It is an alternative method to vagrant development environment. It builds calico in a container and then spin up a kind cluster and install the new calico images.

fixes <https://github.com/projectcalico/calico/issues/3079>

## Related issues/PRs
<https://github.com/projectcalico/calico/issues/2929>
New How to for Get Started rework

- Can merge at any time 
- Assign to Casey for final review
- Reviewed by Spike 
- Part of Alex reorg work: Alex provided inputs on scope of doc
## Release Note
<!-- Writing a release note:
- By default, no release note action is required.
- If you're unsure whether or not your PR needs a note, ask your reviewer for guidance.
- If this PR requires a release note, update the block below to include a concise note describing
  the change and any important impacts this PR may have.
-->

```release-note
Calico now [advertises service cluster and external IPs](https://docs.projectcalico.org/master/networking/advertise-service-ips) for IPv6 as well as for IPv4.
```
