Hello, 

I have had some ZeroDivisionErrors trying to get the Rouge-L summary level score for one of my data.

The problem was in the function _union_lcs of rouge.py where the "union longest common subsequence count" was divided by the "combined LCS length".

I added the case when combined_lcs_length was equal to 0 to return 0, and it's working fine now.
(I mean, I added that case locally, I cannot change it in this repository)

Does it sound right ?

```py
def _union_lcs(evaluated_sentences, reference_sentence):
    if len(evaluated_sentences) <= 0:
        raise (ValueError("Collections must contain at least 1 sentence."))

    lcs_union = set()
    reference_words = _split_into_words(reference_sentence)
    combined_lcs_length = 0
    for eval_s in evaluated_sentences:
        evaluated_words = _split_into_words(eval_s)
        lcs = set(_recon_lcs(reference_words, evaluated_words))
        combined_lcs_length += len(lcs)
        lcs_union = lcs_union.union(lcs)

    union_lcs_count = len(lcs_union)
    
    # Here the modification:
    if combined_lcs_length == 0:
        return 0
    union_lcs_value = union_lcs_count / combined_lcs_length
    return union_lcs_value
```
It seems to be irrationally stressing the processor, while Lsa doesn't. What's up?
In command `sumy_eval random modsum/some_name.2.txt --language=english --file=docs/text.txt --format=plaintext` How I can change different parameters of Rouge evaluation like `-a -c 95 -b 665 -m -n 4 -w 1.2`?
I found that sumy will distinguish **heading** and other sentences, so checked the source code and I found that: 
Whether a line is **heading** is decided by **str.isupper()** function.
But in a str composed by Chinese characters, if it contains an uppercase alphabet, the isupper() will return True, but actually it is just a normal sentence instead of heading.

For example:
```
s1 = "你好啊，这儿有N盘蛋糕可以吃。"
s2 = "N你好啊，这儿有盘蛋糕可以吃。"
s3 = "你好啊，这儿有盘蛋糕可以吃。"

s1.isupper()  # True
s2.isupper()  # True
s3.isupper()  # False
```
My corpus contains 300 paragraphs, and the speed is slow. More than 30 mins.
Could you please introduce sumy's performance ? And which stage will make it slow when corpus is large. 
Thanks!

Hi, i read the code of computing term frequency in [LexRank](https://github.com/miso-belica/sumy/blob/dev/sumy/summarizers/lex_rank.py#L68) `metrics[term] = tf / max_tf`.
I don't understand why do u divide the maximum count of word in that sentence rather than use the number of occurrences of the word in the sentence(described in the original lexrank paper), or just normalize it by dividing total count of all words?
Hi master. firstly im very grateful for that python implementation.. But i didnt understand how [Summary Level ROUGE-L](https://github.com/miso-belica/sumy/blob/dev/sumy/evaluation/rouge.py#L254) works via code?

Sentence level type of rouge, we can use more than a sentence for candidate summary.. After that we compute lcs reference summary - Candidate Sentence_1 and Candidate Sentence_2 respectively.

when i try to use it on command prompt, how i do write it?
or how should the structure of  my candidate sentences be (in local.txt  file)?
Just one line, or each line for each sentence?

Thanks.
Hi!
I will be brief, so as not to distract.

When working algorithm produces a row the first proposals from the text.
for example: LexRankSummarizer: `[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]` (numbers are the sequence number proposals in the in the source text.)
Alternative implementation https://github.com/TafadzwaPasipanodya/Nutshell gives the following result on the same text:
Results LexRank: [94, 57, 42, 76, 66, 86, 83, 63]

Regards Alexander

Hi,

I checked the code for Edmundson summarizer. As I figured out it doesn't do anything for English. Basically it suppose to extract cue words and significant words and the words in title and rank the sentences based in these scores and the location. Well, when the input is a raw text file, then the summarizer works based on the location of the sentence. Is that right? There is no method to extract the cue words and significant words as well as title words for the text. So in  this way the implementation is wrong I suppose. Let me know if I did not understand your code or I'm making a mistake? Thanks. 

I use [NLTK](http://nltk.org/) to tokenize text into sentences & words. But that's big package. Maybe something smaller would be better. Something like https://bitbucket.org/trebor74hr/text-sentence/overview
