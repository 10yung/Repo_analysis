The pretrained model provided from you here [https://dl.fbaipublicfiles.com/torchbiggraph/wikidata_translation_v1.tsv.gz](url) is in .tsv format. However, you mention in documentation that tsv format is mainly for small dataset, and the size of that pretrained model after decompressing is around 113GB making it very slow to convert that into a numpy array (as also noted in the documentation)

Is there any way to convert this .tsv file in hdf5 or is there any pre-trained model already in hdf5 format?
## Types of changes

- [ ] Docs change / refactoring / dependency upgrade
- [ ] Bug fix (non-breaking change which fixes an issue)
- [x] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)

## Motivation and Context / Related issue

Add an interface so it's easy to support different edgelist file formats. Add (untested) support for Parquet in resposne to #114.

## How Has This Been Tested (if it applies)

Unit tests pass.

python torchbiggraph/examples/fb15k.py

I haven't tested Parquet yet.

## Checklist

<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->

- [ x] The documentation is up-to-date with the changes I made.
- [x ] I have read the **CONTRIBUTING** document and completed the CLA (see **CONTRIBUTING**).
- [x] All tests passed, and additional code has been covered with new tests.

## Types of changes

<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->

- [ ] Docs change / refactoring / dependency upgrade
- [ x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)

## Motivation and Context / Related issue

<!--- Why is this change required? What problem does it solve? -->
The train failed if compute time or io time is zero.
<!--- Please link to an existing issue here if one exists. -->
https://github.com/facebookresearch/PyTorch-BigGraph/issues/115
<!--- (we recommend to have an existing issue for each pull request) -->

## How Has This Been Tested (if it applies)

It was tested on my local machine with different values of the compute_time and io_time variables.

## Checklist

<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->

- [ ] The documentation is up-to-date with the changes I made.
- [ ] I have read the **CONTRIBUTING** document and completed the CLA (see **CONTRIBUTING**).
- [ ] All tests passed, and additional code has been covered with new tests.

## Steps to reproduce

  1.  Run train.train_and_report_stats with small quantity of data on a fast machine

## Observed Results

Script failed with a stack trace:
  File "d:\a\1\s\ETL\train.py", line 16, in train
    for (i, _, stats, _) in tbg_train.train_and_report_stats(config):
  File "C:\hostedtoolcache\windows\Python\3.7.5\x64\lib\site-packages\torchbiggraph\train.py", line 787, in train_and_report_stats
    f"bucket {total_buckets - remaining} / {total_buckets} : "
ZeroDivisionError: float division by zero
##[error]C:\hostedtoolcache\windows\Python\3.7.5\x64\python.exe failed with return code: 1

It looks that io_time or compute_time is zero.

## Expected Results

 Do not divide on zero time

## Relevant Code

train.train_and_report_stats(config)

This comes from me wanting to try BigGraph on a dataset of moderate size (800mb) but it's a little too large to be exported into tsv files. In particular, I saw that in your converter you iterate over all lines of tsv individually in python which is going to be really slow when over 24 million edges are involved.

Have you considered adding parquet as a supported input file format?
The following table contains the MRR and Hits values I got for the live journal dataset when it was trained with 0.001 learning rate and 5 epochs. Generally, these evaluation metrics results have increased when the number of partitions increases. I observed similar results for two other edge lists. However, the results given in the [paper](https://www.sysml.cc/doc/2019/71.pdf) for FB15k, freebase datasets have negligible changes when the number of partitions increases. Is there any specific reason to clarify this behaviour?

No of partitions | MRR | Hits@1 | Hits@10 | Hits@50
-- | -- | -- | -- | --
1 | 0.762561 | 0.705239 | 0.866899 | 0.955799
2 | 0.761561 | 0.704932 | 0.864738 | 0.954564
4 | 0.767408 | 0.710564 | 0.870976 | 0.957202
8 | 0.80069 | 0.745636 | 0.898649 | 0.966953
16 | 0.80881 | 0.753067 | 0.906994 | 0.969853
32 | 0.808281 | 0.751425 | 0.908283 | 0.970592



Hi,
Can I know the values used as learning rate, batch size etc. when training Freebase knowledge graph with different number of partitions? If you have performed distributed training for other graph datasets too, can I know the hyperparameters with the best performance for each number of partitions?
I am trying to train a few models with PBG and I am wondering how I could get a random baseline of the model. Also, is there any standard/convention on interpreting the performance (e.g. what is a reasonable pos rank/MRR)? 

Thanks!

Currently `FilteredRankingEvaluator` can be used only when `dynamic_rel` is used. I'd like to improve the class to support also the non dynamic mode.

Before I start doing it, there are any caveats you are aware of that I should consider? 