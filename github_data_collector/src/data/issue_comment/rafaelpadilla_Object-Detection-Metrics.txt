I was absolutely shocked to find that line 296 of pascalvoc.py is this:

`shutil.rmtree(savePath, ignore_errors=True)  # Clear folder`

Because of this, I accidentally deleted a very important folder that contained all of my data, documents, and programs from the past month... I am lucky that I have a backup of most of it. Others will not be so lucky, and I'm sure I will not be the last person to assume that specifying the save path as the project's top level directory will be a harmless decision. This line **definitely** should be changed to something like:

`if not os.path.isdir(savePath):
     os.makedirs(savePath)`

or _anything_ besides what it currently is. After this frightening experience, I will be writing a script to check all code downloaded from github for `rmtree` so I never make a mistake like this again... 

With that said, thank you for writing this program. Overall, it is excellent.
I am getting the below error 
Traceback (most recent call last):
  File "pascalvoc.py", line 331, in <module>
    showGraphic=showPlot)
  File "/data/model_assess/Object-Detection-Metrics/lib/Evaluator.py", line 221, in PlotPrecisionRecallCurve
    plt.plot(recall, precision, label='Precision')
  File "/data/anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/pyplot.py", line 3347, in plot
    ax = gca()
  File "/data/anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/pyplot.py", line 984, in gca
    return gcf().gca(**kwargs)
  File "/data/anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/pyplot.py", line 601, in gcf
    return figure()
  File "/data/anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/pyplot.py", line 548, in figure
    **kwargs)
  File "/data/anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/backend_bases.py", line 161, in new_figure_manager
    return cls.new_figure_manager_given_figure(num, fig)
  File "/data/anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/backend_bases.py", line 167, in new_figure_manager_given_figure
    canvas = cls.FigureCanvas(figure)
  File "/data/anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/backends/backend_qt5agg.py", line 24, in __init__
    super(FigureCanvasQTAgg, self).__init__(figure=figure)
  File "/data/anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/backends/backend_qt5.py", line 234, in __init__
    _create_qApp()
  File "/data/anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/backends/backend_qt5.py", line 125, in _create_qApp
    raise RuntimeError('Invalid DISPLAY variable')
RuntimeError: Invalid DISPLAY variable
Can anyone please help me debug this?
does anyone know how to run this library directly on tensorflow object detection checkpoints?

You write:
> In some images there are more than one detection overlapping a ground truth (Images 2, 3, 4, 5, 6 and 7). For those cases the detection with the highest IOU is considered TP and the others are considered FP. This rule is applied by the PASCAL VOC 2012 metric: "e.g. 5 detections (TP) of a single object is counted as 1 correct detection and 4 false detections”.

But in the VOC paper they say:
> Detections output by a method were assigned to groundtruth objects satisfying the overlap criterion in order rankedby the (decreasing) confidence output. Multiple detectionsof the same object in an image were considered false detec-tions e.g. 5 detections of a single object counted as 1 correctdetection and 4 false detection

I.e. they say they sort on confidence but you say you sort on IoU, is this an error or am I missing something?
Hi, in the readme example there are two detections (R, Y) with the same confidence but one of them is labeled TP and the other is labeled FP. When creating the Precision x Recall curve the order of these makes a difference for the final mAP score. Is there a rule on how to order these?
https://github.com/rafaelpadilla/Object-Detection-Metrics/blob/652c29762776b67505c307c506eb6437df015b0a/lib/Evaluator.py#L50

['FP'] but comment False Negative
Hi,
       I would like to calculate threshold values for given precision/recall point using GetPascalVOCMetrics() method. However I'm confused how every box confidence scores are used in conjuction with IOU threhold.

From the code  I can see that only IOU threshold is used to determine if a detection is a FP or TP. The confidence scores seems to be ignored. I assumed that detections' confidence scores are also used to calculate TP/FP however it is seems that they are not. As such I don't know how to extract threshold values per each precision/recall point.

What am I missing?


Seria interessante a implementação do Log Average Miss Rate, já que é utilizado como métrica em detecção de pedestres.
I run eval methods in every epo during training, but i found it runs very slow. So how could I move metrics methods in GPU? thx