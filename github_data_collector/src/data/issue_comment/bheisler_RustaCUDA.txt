Hey,

I implemented a simple kernel (just kopies each pixel of an image) and issues in the lower part of the image:
![grafik](https://user-images.githubusercontent.com/11499442/68044807-5d4d6700-fcd8-11e9-8402-33c6578db52f.png)

The black stripes at the bottom of the image are different on each call, but always get larger from top to bottom. Therefore I assume that `stream.synchronise()?;` has an issue for multi-dimensional kernel launches like this:
```

launch!(module.conv2d<<<(20, 20, 1), (32, 24, 1), 0, stream>>>(
...
)?;
```
(note: the image size is 640x480 pixels)

How can the synchronisation issue be solved?
Should I restrict my kernels to 1-Dimensional block and thread dimensions?

```
launch!(module.conv2d<<<640, 480, 0, stream>>>( ... )?;
```
elimininates the issue.
Deriving generates code like

```
impl<T: Trait> Trait for DevicePointer<T>
```

But pointer primitives in `std` do not constraint like that.
It seems to me that the re-exports in 
https://bheisler.github.io/RustaCUDA/rustacuda/prelude/index.html
are outdated.

I tried to use DeviceBox, which is in rustacuda::memory::*, together with DeviceBuffer, but I couldn't.
I had to include 
pub use rustacuda::memory::*;

Moreover, when replacing 
use rustacuda::prelude::*;
by 

pub use crate::context::Context;
pub use crate::context::ContextFlags;
pub use crate::device::Device;
pub use crate::module::Module;
pub use crate::stream::Stream;
pub use crate::stream::StreamFlags;
pub use crate::CudaFlags;

I could not use DeviceBuffer any longer.


Thanks and cheers!!
Can we please get examples for benchmarking / timing kernels via RustaCUDA ?

I'm not familiar with how to benchmark CUDA code and would love to learn from examples.
Make sure that everything compiles in Rust 2018 mode, then convert the code and all examples/rustdoc comments/tests/etc. to that mode.
Forgot to do this when I was setting up CI initially. Could maybe try adding a Mac OS build too, but I have no idea if they support CUDA at all.
Right now, RustaCUDA only supports very basic usage of Unified Memory, but CUDA provides a complex API for pre-fetching data to a particular device, advising the driver about which device will use a range of data, and so on. RustaCUDA should expose this section of the API.

[Unified Addressing](https://docs.nvidia.com/cuda/archive/8.0/cuda-driver-api/group__CUDA__UNIFIED.html#group__CUDA__UNIFIED)
I'm not entirely sure how these two are related, but RustaCUDA should support them.

[Texture Reference Management](https://docs.nvidia.com/cuda/archive/8.0/cuda-driver-api/group__CUDA__TEXREF.html#group__CUDA__TEXREF)
[Texture Object Management](https://docs.nvidia.com/cuda/archive/8.0/cuda-driver-api/group__CUDA__TEXOBJECT.html#group__CUDA__TEXOBJECT)
[Surface Reference Management](https://docs.nvidia.com/cuda/archive/8.0/cuda-driver-api/group__CUDA__SURFREF.html#group__CUDA__SURFREF)
[Surface Object Management](https://docs.nvidia.com/cuda/archive/8.0/cuda-driver-api/group__CUDA__SURFOBJECT.html#group__CUDA__SURFOBJECT)
CUDA supports complex strided, multidimensional arrays when performing memory transfers. I'm not really sure what they're used for or how they work, but RustaCUDA should support them.

It may also be nice to support copying to/from [ndarray](https://github.com/rust-ndarray/ndarray), if that's feasible.

See the [Memory module](https://docs.nvidia.com/cuda/archive/8.0/cuda-driver-api/group__CUDA__MEM.html#group__CUDA__MEM) for more.
CUDA provides the ability to link together different modules at runtime. It's pretty niche, but it is there so we should expose it through RustaCUDA.

See the [Module module](https://docs.nvidia.com/cuda/archive/8.0/cuda-driver-api/group__CUDA__MODULE.html#group__CUDA__MODULE)