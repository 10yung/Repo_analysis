Hello.
 My question is, whether RCNN can be used for age and gender classification ?
hi, when i run the code 
`GLOG_logtostderr=1 ../../build/tools/finetune_net.bin pascal_finetune_solver.prototxt  ~/rcnn/data/caffe_nets/ilsvrc_2012_train_iter_310k 2>&1 | tee log.txt`

the caffe net initailizition seems stopped suddenly,the log.txt shows like this 
`I0406 16:51:33.727171 26542 finetune_net.cpp:25] Starting Optimization
I0406 16:51:33.727485 26542 solver.cpp:41] Creating training net.
I0406 16:51:33.728703 26542 net.cpp:75] Creating Layer data
I0406 16:51:33.728741 26542 net.cpp:111] data -> data
I0406 16:51:33.728757 26542 net.cpp:111] data -> label
I0406 16:51:33.728803 26542 window_data_layer.cpp:279] Window data layer:
  foreground (object) overlap threshold: 0.5
  background (non-object) overlap threshold: 0.5
  foreground sampling fraction: 0.25
I0406 16:51:33.737037 26542 window_data_layer.cpp:345] num: 0 /home/xuxiangrui/Pascal_voc/celeba/VOCdevkit/VOC2007/JPEGImages/000002.jpg 3 594 423 windows to process: 2762
I0406 16:51:33.914831 26542 window_data_layer.cpp:345] num: 100 /home/xuxiangrui/Pascal_voc/celeba/VOCdevkit/VOC2007/JPEGImages/000189.jpg 3 400 496 windows to process: 2304
I0406 16:51:34.066776 26542 window_data_layer.cpp:345] num: 200 /home/xuxiangrui/Pascal_voc/celeba/VOCdevkit/VOC2007/JPEGImages/000393.jpg 3 640 408 windows to process: 2301
I0406 16:51:34.247323 26542 window_data_layer.cpp:345] num: 300 /home/xuxiangrui/Pascal_voc/celeba/VOCdevkit/VOC2007/JPEGImages/000595.jpg 3 360 640 windows to process: 1185
I0406 16:51:34.380002 26542 window_data_layer.cpp:345] num: 400 /home/xuxiangrui/Pascal_voc/celeba/VOCdevkit/VOC2007/JPEGImages/000808.jpg 3 337 500 windows to process: 2281
I0406 16:51:34.591991 26542 window_data_layer.cpp:354] Number of images: 500
I0406 16:51:34.592015 26542 window_data_layer.cpp:358] class 0 has 1261791 samples
I0406 16:51:34.592025 26542 window_data_layer.cpp:362] Amount of context padding: 16
I0406 16:51:34.592027 26542 window_data_layer.cpp:365] Crop mode: warp
I0406 16:51:34.592031 26542 window_data_layer.cpp:376] output data size: 64,3,227,227
I0406 16:51:34.592034 26542 window_data_layer.cpp:388] Loading mean file from../../data/ilsvrc12/imagenet_mean.binaryproto
I0406 16:51:36.851869 26542 net.cpp:126] Top shape: 64 3 227 227 (9893568)
I0406 16:51:36.851935 26542 net.cpp:126] Top shape: 64 1 1 1 (64)
I0406 16:51:36.851958 26542 net.cpp:157] data does not need backward computation.
I0406 16:51:36.851992 26542 net.cpp:75] Creating Layer conv1
I0406 16:51:36.852007 26542 net.cpp:85] conv1 <- data
I0406 16:51:36.852041 26542 net.cpp:111] conv1 -> conv1
I0406 16:51:36.866598 26542 net.cpp:126] Top shape: 64 96 55 55 (18585600)
I0406 16:51:36.866847 26542 net.cpp:152] conv1 needs backward computation.
I0406 16:51:36.867000 26542 net.cpp:75] Creating Layer relu1
I0406 16:51:36.867081 26542 net.cpp:85] relu1 <- conv1
I0406 16:51:36.867144 26542 net.cpp:99] relu1 -> conv1 (in-place)
I0406 16:51:36.867208 26542 net.cpp:126] Top shape: 64 96 55 55 (18585600)
I0406 16:51:36.867270 26542 net.cpp:152] relu1 needs backward computation.
I0406 16:51:36.867333 26542 net.cpp:75] Creating Layer pool1
I0406 16:51:36.867391 26542 net.cpp:85] pool1 <- conv1
I0406 16:51:36.867465 26542 net.cpp:111] pool1 -> pool1
I0406 16:51:36.867533 26542 net.cpp:126] Top shape: 64 96 27 27 (4478976)
I0406 16:51:36.867555 26542 net.cpp:152] pool1 needs backward computation.
I0406 16:51:36.867578 26542 net.cpp:75] Creating Layer norm1
I0406 16:51:36.867590 26542 net.cpp:85] norm1 <- pool1
I0406 16:51:36.867606 26542 net.cpp:111] norm1 -> norm1
I0406 16:51:36.867746 26542 net.cpp:126] Top shape: 64 96 27 27 (4478976)
I0406 16:51:36.867774 26542 net.cpp:152] norm1 needs backward computation.
I0406 16:51:36.867817 26542 net.cpp:75] Creating Layer conv2
I0406 16:51:36.867832 26542 net.cpp:85] conv2 <- norm1
I0406 16:51:36.867864 26542 net.cpp:111] conv2 -> conv2
I0406 16:51:36.885260 26542 net.cpp:126] Top shape: 64 256 27 27 (11943936)
I0406 16:51:36.885288 26542 net.cpp:152] conv2 needs backward computation.
I0406 16:51:36.885303 26542 net.cpp:75] Creating Layer relu2
I0406 16:51:36.885310 26542 net.cpp:85] relu2 <- conv2
I0406 16:51:36.885320 26542 net.cpp:99] relu2 -> conv2 (in-place)
I0406 16:51:36.885331 26542 net.cpp:126] Top shape: 64 256 27 27 (11943936)
I0406 16:51:36.885340 26542 net.cpp:152] relu2 needs backward computation.
I0406 16:51:36.885351 26542 net.cpp:75] Creating Layer pool2
I0406 16:51:36.885360 26542 net.cpp:85] pool2 <- conv2
I0406 16:51:36.885370 26542 net.cpp:111] pool2 -> pool2
I0406 16:51:36.885381 26542 net.cpp:126] Top shape: 64 256 13 13 (2768896)
I0406 16:51:36.885390 26542 net.cpp:152] pool2 needs backward computation.
I0406 16:51:36.885404 26542 net.cpp:75] Creating Layer norm2
I0406 16:51:36.885413 26542 net.cpp:85] norm2 <- pool2
I0406 16:51:36.885422 26542 net.cpp:111] norm2 -> norm2
I0406 16:51:36.885435 26542 net.cpp:126] Top shape: 64 256 13 13 (2768896)
I0406 16:51:36.885448 26542 net.cpp:152] norm2 needs backward computation.
I0406 16:51:36.885460 26542 net.cpp:75] Creating Layer conv3
I0406 16:51:36.885469 26542 net.cpp:85] conv3 <- norm2
I0406 16:51:36.885479 26542 net.cpp:111] conv3 -> conv3
I0406 16:51:36.906862 26542 net.cpp:126] Top shape: 64 384 13 13 (4153344)
I0406 16:51:36.906883 26542 net.cpp:152] conv3 needs backward computation.
I0406 16:51:36.906889 26542 net.cpp:75] Creating Layer relu3
I0406 16:51:36.906893 26542 net.cpp:85] relu3 <- conv3
I0406 16:51:36.906898 26542 net.cpp:99] relu3 -> conv3 (in-place)
I0406 16:51:36.906903 26542 net.cpp:126] Top shape: 64 384 13 13 (4153344)
I0406 16:51:36.906905 26542 net.cpp:152] relu3 needs backward computation.
I0406 16:51:36.906910 26542 net.cpp:75] Creating Layer conv4
I0406 16:51:36.906913 26542 net.cpp:85] conv4 <- conv3
I0406 16:51:36.906916 26542 net.cpp:111] conv4 -> conv4
I0406 16:51:36.917457 26542 net.cpp:126] Top shape: 64 384 13 13 (4153344)
I0406 16:51:36.917470 26542 net.cpp:152] conv4 needs backward computation.
I0406 16:51:36.917475 26542 net.cpp:75] Creating Layer relu4
I0406 16:51:36.917479 26542 net.cpp:85] relu4 <- conv4
I0406 16:51:36.917482 26542 net.cpp:99] relu4 -> conv4 (in-place)
I0406 16:51:36.917486 26542 net.cpp:126] Top shape: 64 384 13 13 (4153344)
I0406 16:51:36.917490 26542 net.cpp:152] relu4 needs backward computation.
I0406 16:51:36.917495 26542 net.cpp:75] Creating Layer conv5
I0406 16:51:36.917497 26542 net.cpp:85] conv5 <- conv4
I0406 16:51:36.917503 26542 net.cpp:111] conv5 -> conv5
I0406 16:51:36.927706 26542 net.cpp:126] Top shape: 64 256 13 13 (2768896)
I0406 16:51:36.927819 26542 net.cpp:152] conv5 needs backward computation.
I0406 16:51:36.927845 26542 net.cpp:75] Creating Layer relu5
I0406 16:51:36.927855 26542 net.cpp:85] relu5 <- conv5
I0406 16:51:36.927867 26542 net.cpp:99] relu5 -> conv5 (in-place)
I0406 16:51:36.927880 26542 net.cpp:126] Top shape: 64 256 13 13 (2768896)
I0406 16:51:36.927886 26542 net.cpp:152] relu5 needs backward computation.
I0406 16:51:36.927898 26542 net.cpp:75] Creating Layer pool5
I0406 16:51:36.927903 26542 net.cpp:85] pool5 <- conv5
I0406 16:51:36.927911 26542 net.cpp:111] pool5 -> pool5
I0406 16:51:36.927924 26542 net.cpp:126] Top shape: 64 256 6 6 (589824)
I0406 16:51:36.927928 26542 net.cpp:152] pool5 needs backward computation.
I0406 16:51:36.927949 26542 net.cpp:75] Creating Layer fc6
I0406 16:51:36.927958 26542 net.cpp:85] fc6 <- pool5
I0406 16:51:36.927968 26542 net.cpp:111] fc6 -> fc6`



Can anyone help me with this problem?
Hi @rbgirshick ,
When I compiled ninja  with ./configure.py an error report.....(enviroment:linux aarch64,python2.7)
pi@raspberrypi:~/ninja$ ./configure.py --bootstrap
bootstrapping ninja...
./src/browse.cc:1:0: internal compiler error: Segmentation fault
 // Copyright 2011 Google Inc. All Rights Reserved.
 
g++: internal compiler error: Segmentation fault (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-6/README.Bugs> for instructions.
when running:  g++ -MMD -MT build/browse.o -MF build/browse.o.d -g -Wall -Wextra -Wno-deprecated -Wno-missing-field-initializers -Wno-unused-parameter -fno-rtti -fno-exceptions -fvisibility=hidden -pipe '-DNINJA_PYTHON="python"' -O2 -DNDEBUG -DUSE_PPOLL -DNINJA_HAVE_BROWSE -I. -c ./src/browse.cc -o build/browse.o
Traceback (most recent call last):
  File "./configure.py", line 456, in <module>
    objs += cxx('browse', order_only=built('browse_py.h'))
  File "./configure.py", line 282, in cxx
    return n.build(built(name + objext), 'cxx', src(name + '.cc'), **kwargs)
  File "./configure.py", line 164, in build
    self._run_command(self._expand(cmd, local_vars))
  File "./configure.py", line 189, in _run_command
    subprocess.check_call(cmdline, shell=True)
  File "/usr/lib/python2.7/subprocess.py", line 186, in check_call
    raise CalledProcessError(retcode, cmd)
subprocess.CalledProcessError: Command 'g++ -MMD -MT build/browse.o -MF build/browse.o.d -g -Wall -Wextra -Wno-deprecated -Wno-missing-field-initializers -Wno-unused-parameter -fno-rtti -fno-exceptions -fvisibility=hidden -pipe '-DNINJA_PYTHON="python"' -O2 -DNDEBUG -DUSE_PPOLL -DNINJA_HAVE_BROWSE -I. -c ./src/browse.cc -o build/browse.o' returned non-zero exit status 4

this problem occurred at the beginning of the train.The iteration did not start.Could any one give me some help.
how will i be able to start matlab being in rcnn directory ? because i am unable to find matlab in any folder of rcnn . thanks 
I tried the rcnn_demo, and no detections were made because all the scores were negative. What could have happened? I trained a simple model using my own dataset, and the same thing happened.
Hi, 

I have used the RCNN_model that you have provided to get the results on VOC 2007. The thing is I could get MAP = 42.71 with the provided model, while in the paper the lowest claim is 44.2. Also, you claimed MAP = 66 for RCNN in Fast RCNN paper while I have got 40.11 when I trained my network. Since I am using your code without any modification, I think the problem is with the networks that I have used. Is it possible for you to release the trained rcnn models (mat files) that you have used for VOC 2007, 2010 and 2012 using Alexnet and VGG16? 

Thanks so much. 

I have a simple question -- after loading a trained model for 20 classes, can I limit the detection to a single class when I run `rcnn_detect()`?

I tried removing some of the strings in `rcnn_model.classes` but it doesn't work.

I executive the command key=caffe('get_init_key'),then there is an error prompt :

Invalid MEX-file '/home/lrj/MatlabCode/rcnn1/rcnn/external/caffe/matlab/caffe/caffe.mexa64':
/usr/local/MATLAB/R2014a/bin/glnxa64/../../sys/os/glnxa64/libstdc++.so.6: version `GLIBCXX_3.4.20'
not found (required by /usr/lib/x86_64-linux-gnu/libgflags.so.2)

Hi,
I come across the following error, when I run the demo code of rcnn:

```
Error using caffe
Unknown command 'get_weights'

Error in rcnn_load_model (line 36)
rcnn_model.cnn.layers = caffe('get_weights');

```

Could anyone know how to fix it?

Thank you!
