nginx version: nginx/1.16.1
nchan version: 1.2.6

I use `X-Accel-Redirect` header to redirect to subscribe endpoint.

here is my config

```
nchan_max_channel_id_length 2048;

server {
location = /sub_upstream {
        proxy_pass http://upstream_app/subscriber_x_accel_redirect;
        proxy_set_header X-Forwarded-For $remote_addr;
    }

    location ~ /sub/internal/(.*)$ {
        internal; #this location only accessible for internal nginx redirects
        nchan_subscriber;
        nchan_channel_id $1;
        nchan_channel_id_split_delimiter ",";
    }

}
```

And my upstream response is

```
> GET /subscriber_x_accel_redirect HTTP/1.1
> Host: localhost:9099
> User-Agent: curl/7.66.0
> Accept: */*
>
* Mark bundle as not supporting multiuse
< HTTP/1.1 200 OK
< X-Accel-Buffering: no
< X-Accel-Redirect: /sub/internal/1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350
```

ID length is 1291.

When I connect to /sub_upstream, nginx worker will crash

```
2020/01/02 10:12:38 [alert] 1#1: worker process 10 exited on signal 11
```
Doubtless this is my fault in some way, but I could use some help.

Here's my config:

```
  server {
    listen 555 ssl;
    server_name ...removed;

    # SSL settings
...removed

    location = /subscribe {
      nchan_subscriber;
      nchan_channel_id $arg_id;
    }

    location = /publish {
      nchan_publisher;
      nchan_channel_id $arg_id;
    }

    location /nchan_stub_status {
      nchan_stub_status;
    }
  }

```

I'm using the nchan NPM client.  About every second, I can see that a request is being made to my channel, which returns with 101 Switching protocols.  

![image](https://user-images.githubusercontent.com/56696/70556257-70294600-1b78-11ea-95a4-c00af8cf4738.png)

Why would this be happening so frequently?

If I connect using a browser then I rapidly get ERR_EMPTY_RESPONSE errors.

Adding an 'on' handler for errors I see lots of errors saying "Error during WebSocket handshake: net::ERR_CONNECTION_RESET" or "Error during WebSocket handshake: Unexpected response code: 507".  But these are not once per second.  

Much less frequently, I see it return storage errors; I'm not worried about this as I have a periodic restart but I mention it in case it's relevant.

I'm trying to use nchan to create a pub/sub setup where subscribers receive the "current state" on new connections, but start at the newest message. Starting from the first and fast-forwarding would be a prohibitive amount of data for new connections in this case.

Is there any way to add HTTP response code capabilities to nchan_subscribe_request similar to how nchan_publisher_upstream_request handlers can optionally return a response? Or is there another approach to achieving the same thing?

I suppose the response could also be done on nchan_authorize_request instead of nchan_subscribe_request, but neither seems to listen for a callback response.
Are there any plans to add SSL support for Redis connection? Hiredis had recently added it to their master branch.
After issuing HTTP DELETE request to publisher endpoint, the subsequent GET and DELETE requests for the same channel return 404 as described. However the subscribers (both Websockets and EventSource) stay connected and continue to receive POSTed messages.

I use Redis as storage backend with the options:
```
  nchan_redis_storage_mode nostore;
  nchan_redis_nostore_fastpublish on;
``` 
Hi,

We would like to use the Fedora dynamic lib build to host nchan on RedHat OS 7, with nginx 1.15.8, the curretn version. Does the link in https://nchan.io/#install support the OS and nginx version? If not, is this something you can help us, or let me know how to produce the dynamic lib?

Thank you!
Hi!

We are trying to use the `nchan_websocket_client_heartbeat` directive.
Here is the location config:
```
  location = /sub {
        nchan_subscriber websocket;
        nchan_websocket_client_heartbeat '_ping' '_pong';
        nchan_channel_id $arg_id;
  }
```

Here is the excerpt from the log that we observe after "_ping" is sent:

```
2019/08/27 18:46:08 [debug] 9614#9614: BUFCHAINPOOL:0000564E4CC66A10 bcs 2 (rec. 0), files 0 (rec. 0)
2019/08/27 18:46:08 [debug] 9614#9614: BUFCHAINPOOL:0000564E4CC66A10 bcs 3 (rec. 0), files 0 (rec. 0)
2019/08/27 18:46:08 [debug] 9614#9614: BUFCHAINPOOL:0000564E4CC66A10 bcs 0 (rec. 3), files 0 (rec. 0)
2019/08/27 18:46:08 [debug] 9614#9614: SUB:WEBSOCKET:0000564E4CCBF760 dequeue
2019/08/27 18:46:08 [debug] 9614#9614: SPOOL:sub 0000564E4CCBF760 dequeue callback
2019/08/27 18:46:08 [debug] 9614#9614: MEMSTORE:00: Chanhead gc add 0000564E4CD77980 /00732dd9-6494-4bb9-9d89-2ca8d293581c: sub count == 0 after spooler dequeue
2019/08/27 18:46:08 [debug] 9614#9614: MEMSTORE:00: Chanhead churn withdraw 0000564E4CD77980 /00732dd9-6494-4bb9-9d89-2ca8d293581c
2019/08/27 18:46:08 [debug] 9614#9614: REAPER: withdraw chanhead churner 0000564E4CD77980
2019/08/27 18:46:08 [debug] 9614#9614: MEMSTORE:00: chanhead_gc max -1 count 0
2019/08/27 18:46:08 [debug] 9614#9614: MEMSTORE:00: message GC results: started with 0, walked 0, deleted 0 msgs
2019/08/27 18:46:08 [debug] 9614#9614: MEMSTORE:00: not ready to reap /00732dd9-6494-4bb9-9d89-2ca8d293581c, 5 sec left
2019/08/27 18:46:08 [debug] 9614#9614: REAPER: reap chanhead 0000564E4CD77980 later (waiting to be reaped: 1)
2019/08/27 18:46:08 [debug] 9614#9614: REAPER: reap chanhead again later (remaining: 1)
2019/08/27 18:46:08 [debug] 9614#9614: SUB:WEBSOCKET:0000564E4CCBF760 destroy for req 0000564E4CCF9D50
```
Version: 1.2.5

Is the `nchan_websocket_client_heartbeat` directive allowed in pure nchan_subscriber location?

What we observe is that after the client sends "_ping", websocket is closed from the server side and client reconnects. No _pong is received. Seems like we are doing something wrong... But what?
After nchan running for a few days at our dev env we are facing to publish latency, about 20 sec, and it's growing. 
Also even not able to subscribe to channel. When open at browser, i can see at network tab that request at pending state.
Nchan is behind nginx-ingress. Checked logs of nchan and nginx-ingress, there is no line that i tried to access data by subscribe url. Only restart helps

Here is stats
`
total published messages: 828
stored messages: 0
shared memory used: 56K
shared memory limit: 131072K
channels: 1
subscribers: 4
redis pending commands: 0
redis connected servers: 4
total interprocess alerts received: 67497
interprocess alerts in transit: 239
interprocess queued alerts: 239
total interprocess send delay: 0
total interprocess receive delay: 0
nchan version: 1.2.6`

`Active connections: 15 
server accepts handled requests
 1812 1812 11314 
Reading: 0 Writing: 14 Waiting: 1 `
Hi.

Is planed used for storage messages Redis Stream?
https://redis.io/topics/streams-intro

Redis Stream, used as universal storage messages for my microservices.
Thanks.
I've updated nchan from
> `nchan` built from source, commit sha `d65a5c3`

to latest 1.2.6.

While we had some issues with nginx worker crashing in the past, it's been quite stable in last 3-4months, no crashes at all.

Now, in just about 30 minutes after version change, the worker crashed
Here are the details (though, as I used compiled deb packages, there are less details than usual)

```
$> lsb_release -a
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 14.04.5 LTS
Release:	14.04
Codename:	trusty
```

```
$> uname -a
Linux app.example.com 4.4.0-31-generic #50~14.04.1-Ubuntu SMP Wed Jul 13 01:07:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
```

```
$> cat /proc/version
Linux version 4.4.0-31-generic (buildd@lgw01-43) (gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04.3) ) #50~14.04.1-Ubuntu SMP Wed Jul 13 01:07:32 UTC 2016
```

```
$> nginx -V
nginx version: nginx/1.10.1
built with OpenSSL 1.0.1f 6 Jan 2014
TLS SNI support enabled
configure arguments: --with-cc-opt='-g -O2 -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security -D_FORTIFY_SOURCE=2' --with-ld-opt='-Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-z,now' --prefix=/usr/share/nginx --conf-path=/etc/nginx/nginx.conf --http-log-path=/var/log/nginx/access.log --error-log-path=/var/log/nginx/error.log --lock-path=/var/lock/nginx.lock --pid-path=/run/nginx.pid --modules-path=/usr/lib/nginx/modules --http-client-body-temp-path=/var/lib/nginx/body --http-fastcgi-temp-path=/var/lib/nginx/fastcgi --http-proxy-temp-path=/var/lib/nginx/proxy --http-scgi-temp-path=/var/lib/nginx/scgi --http-uwsgi-temp-path=/var/lib/nginx/uwsgi --with-debug --with-pcre-jit --with-ipv6 --with-http_ssl_module --with-http_stub_status_module --with-http_realip_module --with-http_auth_request_module --with-http_v2_module --with-http_dav_module --with-threads --with-http_addition_module --with-http_flv_module --with-http_geoip_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_image_filter_module --with-http_mp4_module --with-http_random_index_module --with-http_secure_link_module --with-http_sub_module --with-http_xslt_module --with-mail --with-mail_ssl_module --with-stream --with-stream_ssl_module --add-module=/build/nginx/debian/modules/headers-more-nginx-module --add-module=/build/nginx/debian/modules/nginx-auth-pam --add-module=/build/nginx/debian/modules/nginx-cache-purge --add-module=/build/nginx/debian/modules/nginx-dav-ext-module --add-module=/build/nginx/debian/modules/nginx-development-kit --add-module=/build/nginx/debian/modules/nginx-echo --add-module=/build/nginx/debian/modules/ngx-fancyindex --add-module=/build/nginx/debian/modules/nchan --add-module=/build/nginx/debian/modules/nginx-lua --add-module=/build/nginx/debian/modules/nginx-upload-progress --add-module=/build/nginx/debian/modules/nginx-upstream-fair --add-module=/build/nginx/debian/modules/ngx_http_substitutions_filter_module
```
```
$> curl -X GET -H "Accept: text/json" http://10.xxx.xxx.90/nchan_stub_status
total published messages: 1292
stored messages: 9
shared memory used: 32K
shared memory limit: 131072K
channels: 25
subscribers: 8
redis pending commands: 0
redis connected servers: 0
total interprocess alerts received: 0
interprocess alerts in transit: 0
interprocess queued alerts: 0
total interprocess send delay: 0
total interprocess receive delay: 0
nchan version: 1.2.6
```

nginx error log:
```
ker process: /build/nginx/debian/modules/nchan/src/store/spool.c:479: spool_fetch_msg: Assertion `spool->msg_status == MSG_INVALID' failed.
2019/06/29 13:12:37 [notice] 10880#10880: signal 17 (SIGCHLD) received
2019/06/29 13:12:37 [alert] 10880#10880: worker process 10881 exited on signal 6 (core dumped)
2019/06/29 13:12:37 [notice] 10880#10880: start worker process 12335
ker process: /build/nginx/debian/modules/nchan/src/store/spool.c:479: spool_fetch_msg: Assertion `spool->msg_status == MSG_INVALID' failed.
2019/06/29 13:15:05 [notice] 10880#10880: signal 17 (SIGCHLD) received
2019/06/29 13:15:05 [alert] 10880#10880: worker process 12335 exited on signal 6 (core dumped)
2019/06/29 13:15:05 [notice] 10880#10880: start worker process 12438
```


```
$> gdb /usr/sbin/nginx  CoreDump 
GNU gdb (Ubuntu 7.7.1-0ubuntu5~14.04.3) 7.7.1
Copyright (C) 2014 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type "show copying"
and "show warranty" for details.
This GDB was configured as "x86_64-linux-gnu".
Type "show configuration" for configuration details.
For bug reporting instructions, please see:
<http://www.gnu.org/software/gdb/bugs/>.
Find the GDB manual and other documentation resources online at:
<http://www.gnu.org/software/gdb/documentation/>.
For help, type "help".
Type "apropos word" to search for commands related to "word"...
Reading symbols from /usr/sbin/nginx...(no debugging symbols found)...done.
[New LWP 10881]
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
Core was generated by `nginx: worker process                           '.
Program terminated with signal SIGABRT, Aborted.
#0  0x00007f9e513e5c37 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
56	../nptl/sysdeps/unix/sysv/linux/raise.c: No such file or directory.
(gdb) where
#0  0x00007f9e513e5c37 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007f9e513e9028 in __GI_abort () at abort.c:89
#2  0x00007f9e513debf6 in __assert_fail_base (fmt=0x7f9e51533058 "%s%s%s:%u: %s%sAssertion `%s' failed.\n%n", 
    assertion=assertion@entry=0x595248 "spool->msg_status == MSG_INVALID", 
    file=file@entry=0x594e90 "/build/nginx/debian/modules/nchan/src/store/spool.c", line=line@entry=479, 
    function=function@entry=0x5955d0 "spool_fetch_msg") at assert.c:92
#3  0x00007f9e513deca2 in __GI___assert_fail (assertion=0x595248 "spool->msg_status == MSG_INVALID", 
    file=0x594e90 "/build/nginx/debian/modules/nchan/src/store/spool.c", line=479, function=0x5955d0 "spool_fetch_msg") at assert.c:101
#4  0x000000000050e4d8 in ?? ()
#5  0x000000000050debf in ?? ()
#6  0x00000000005170a2 in ?? ()
#7  0x000000000051ac83 in nchan_memstore_handle_get_message_reply ()
#8  0x000000000051ae85 in ?? ()
#9  0x000000000051b1f4 in ?? ()
#10 0x000000000051ad71 in ?? ()
#11 0x000000000050e313 in ?? ()
#12 0x000000000050ec28 in ?? ()
#13 0x0000000000519d51 in ?? ()
#14 0x000000000050281c in nchan_subscriber_subscribe ()
#15 0x0000000000502bf5 in nchan_subscriber_authorize_subscribe_request ()
#16 0x00000000004ef756 in nchan_pubsub_handler ()
#17 0x0000000000461a4f in ngx_http_core_content_phase ()
#18 0x000000000045bf55 in ngx_http_core_run_phases ()
#19 0x0000000000467e83 in ngx_http_process_request ()
#20 0x0000000000468836 in ?? ()
#21 0x0000000000450489 in ?? ()
#22 0x0000000000446517 in ngx_process_events_and_timers ()
#23 0x000000000044de55 in ?? ()
#24 0x000000000044c940 in ngx_spawn_process ()
#25 0x000000000044e0c4 in ?? ()
#26 0x000000000044edff in ngx_master_process_cycle ()
#27 0x0000000000427c8f in main ()
```

FYI: this is the second dump of the two, I still have this in `core_pattern`
```
|/usr/share/apport/apport %p %s %c %d %P
```

I'm about to revert my version change for now, let me know if you need anything else.