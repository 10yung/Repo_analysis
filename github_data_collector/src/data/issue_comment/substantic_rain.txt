TODO:
- check Python package build
- fix DataObject destructor in Python client
- use better defaults for listen addr/port
- run websockets on same port as HTTP (or switch governors to Websockets)
- change Rust client (#47) to use Websockets
@all 

hi all:
I have read the doc on https://substantic.github.io/rain/docs/index.html. 

The example given in `get-started` as below

```python3
from rain.client import Client, tasks, blob

# Connect to server
client = Client("localhost", 7210)

# Create a new session
with client.new_session() as session:

    # Create task (and two data objects)
    task = tasks.Concat((blob("Hello "), blob("world!"),))

    # Mark that the output should be kept after submit
    task.output.keep()

    # Submit all crated tasks to server
    session.submit()

    # Wait for completion of task and fetch results and get it as bytes
    result = task.output.fetch().get_bytes()

    # Prints 'Hello world!'
    print(result)
```

This is so amusing. 

But `Python3.5+` support `async/await` syntax, does this project support this syntax?

in roadmap I find this task `Update in the Python API (using aiohttp for async API) (@gavento) [medium]`. Is it in order to support `async/await1 syntax?

After supporting `async/await` syntax, the example maybe shows as below 

```python3
from rain.client import Client, tasks, blob

# Connect to server
client = Client("localhost", 7210)

# Create a new session
async with client.new_session() as session:

    # Create task (and two data objects)
    task = tasks.Concat((blob("Hello "), blob("world!"),))

    # Mark that the output should be kept after submit
    task.output.keep()

    # Submit all crated tasks to server
    session.submit()

    # Wait for completion of task and fetch results and get it as bytes
    result = await task.output.fetch().get_bytes()

    # Prints 'Hello world!'
    print(result)
```
A document to track the directions from 0.3, replacing #26. Our mid- and long-term goals, their *[priority]*, (asignee) and any sub-tasks.

*Any help is welcome with mentoring available for most tasks!*

## Remaining enhancements from v0.3

**Will be updated after prioritization discussion.**

### Client-side protocols
Replace capnp RPC and the current monitoring dashboard HTTP API with common protocol.
Part of #11 (more discussion there) but specific to the public API. 
- [ ] Design the API calls (@gavento) *[medium]*
- [ ] Implement in the server (@gavento) *[medium]*
- [ ] Update in the Python API (using [aiohttp](https://aiohttp.readthedocs.io/en/stable/) for async API) (@gavento) *[medium]*
- [ ] Update the dashboard (@gavento) *[medium]* (#38)

### Improve the dashboard with more information and post-mortem analysis
- [ ] Design and revamp the dashborad. Depends on the client API development (@gavento) *[medium/low]* (#38)
- [x] Include stats for task/object groups and possibly names/labels from #32 *[low]* (#38)

### Fix current bugs
- [ ] #7 (occurs under heavy load only) *[medium]*
- [ ] #13 (seems to be bound to Exoscale deployment) *[high]*

### Custom tasks (subworkers) in more languages
- [ ] Python subworker as a library *[low]* (run standalone scripts as opposed to defining them in the client only)

### Easier deployment in the cloud
- [ ] Deployment in the amazon cloud (@vojtechcima) *[medium]* (#37)

### Packaging for easier deployment 
Multiple options, priorities may vary. (@spirali)
- [ ] [AppImage](https://appimage.org/)/Snap packages *[low]* (we already have static binaries)
  * Snapcraft has a [rust plugin](https://docs.snapcraft.io/reference/plugins/rust)
- [ ] Deb/other distro packages *[low]*
  * There is [cargo-deb](https://github.com/mmstick/cargo-deb)

### Improve Python API
Pythonize the client API.
- [ ] Draft content-type loaders/extensions (@gavento) *[low]*
- [x] Task/object groups and names/labels (#32) *[low]*

### Improve testing infrastructure
- [ ] Scripts/containers/... to test deployment and running in a network. (@vojtechcima) *[medium]*
  * Test `rain start` and running on OpenStack, Exoscale, AWS. Does not have to be a part of CI (even for running locally). Depends on / part of #37.

### More real-world code examples
Lower priority, best based on real use-cases. Ideas: numpy subtasks, C++/Rust subworkers 

## Enhancements to revisit in the (not so distant) future

* Integration with some popular libraries
  * [Apache Arrow](https://arrow.apache.org/) content-type
    * Basic type and loading is implemented. We could add more operations (filter, split, merge, ...)
  * [XGBoost](https://xgboost.readthedocs.io/en/latest/) tasks, etc ...
  * *Why not now:* Not clear what would be the demand
* Worker configuration files (needed for common (CPU) and special resources (GPU), different subworker locatins and configurations, ...)
  * Partially done
  * *Why not now:* Needs to be thought-through (esp. w.r.t. resources), not needed now
* Separate session construction and running (save/load session)
  * *Why not now:* Not clear what would be the use-cases, not difficult when API stabilized
* Clients in other languages: Rust, C++, Java, ...
  * *Why not now:* Not clear what would be the demand. Easier after the protocol/Python API stabilization.
* Scale the scheduler, benchmarks
  * There is a benchmark in `utils/bench/simple_task_scaling.py`. The results as of 0.2 are [here](https://docs.google.com/spreadsheets/d/1RNaUzZ_4kbONS-7o4093fjIc-t3i3eqS410BNWjphGU/edit?usp=sharing).
  * *Why not now:* While eventually crucial, the scheduler is sufficient when there are <1000 tasks to be scheduled at once.

This PR introduces a Rust client for the cluster with a synchronous API that mirrors the Python client.

The client is used to implement a stop command for the cluster. The terminateServer RPC call on the scheduler is now implemented with a trivial `exit(0)`, later it should probably do a more graceful shutdown.

I'm going to need help with my arch-nemesis (storing closures in structs) to somehow implement `Drop` for `Session` (which needs access to the client).
Currently a crash of a subworker may crash a worker, and a crash of a worker may crash the server. We need to improve this. However, we are *not* aiming for infrastructure resiliency now. Subworker crash may still fail the task (and so also the session) and worker crash may still lose all the objects and fail all involved sessions. The main goal is to keep the server running and deliver a graceful error.

A robust failure handling will open up the road to retrying tasks (possibly on different workers) and later to worker crash resiliency.
Improve the online monitoring to:
* Show session list summary with more info
* Show more statistics and graphs, e.g. aggregation by task type or group (proposed in #32)
* Show a timeline by groups or with significant (all or e.g. named?) events.
* Only display the dependency graph optionally and for reasonably small graphs
  * As a future improvement, we could display the dependency graph of groups, clusters, task types etc.)
* See the full details of every task and object (on demand)
* Examine history of the events in some way (a timeline may be enough)
* Do all of this for finished and closed sessions via re-running the server with the sa

This depends on a streaming API (#11) and may require a complete UI framework.
While Rain startup on PBS already has basic support, it would be great to have scripts to setup and control (at least teardown) deployment on some of: CloudStack API (Exoscale), AWS, Goole cloud, ... 

@vojtechcima is already working on that - Vojta, would you please take over and fill some plans and status?

Add task attribute to allow only a subset of workers. An empty attribute 

Usage:
* Working on files local to a worker (either reading or writing).
  * Useful for `open` and `store` tasks (but not limited to them).
* Selecting suitable worker subset for a taks when (virtual) resources are not appropriate.
  * However, for GPUs and other features, we should use resources.
* Testing.

Constant data objects already have a similar feature for placement, but this API is not planned to be published (and it is not really relevant for computed data objects).

Alternative: Every worker could be a resource, required by the task. However, this is not ergonomic and we do not have a clear picture of how to do (virtual) resources in the right way.
Introduce optional task/data object groups and names for better monitoring and possibly debugging. Currently, the data objects have a `label` indicating the role it plays in the producer task (e.g. `log`, `score`, ...).

Introduce:

* task and data object name - arbitrary string, optional, to be set for any tasks the user wants to distinguish later by their name. May not be unique in the session but is intended to be unique. Always set by the user.
  * Goal: be able to distinguish special tasks in the graph and to query them.
  * Importance: low
  * Perhaps we need to think about possible uses and name vs label on objects (not to be confusing)

* task and data object groups - a textual label splitting the nodes into *disjoint* groups (including the `None` group).
  * Goal: to be able to monitor statistics of different task/DO groups, meaningful progress reporting and stats aggregation
  * Advantage: easy to report compact per-group aggregates, even store them in the log.
  * Variant: add arbitrary `tags`, aggregate by tags combination. Drawbacks are added complexity (to many tag combinations) and less monitoring meaning (the stats no longer add to all nodes).


Work in progress update on Attributes.

Waits on internal attributes discussion and cleaup.