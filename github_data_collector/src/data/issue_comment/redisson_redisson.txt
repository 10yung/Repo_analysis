in my app, sometimes there are some dead locks in redis
java.lang.IllegalMonitorStateException: attempt to unlock lock, not locked by current thread by node id: 6e3d30b7-83b1-433a-9335-5e8b9916d3ac thread-id: 731 
    at org.redisson.RedissonLock.lambda$unlockAsync$3(RedissonLock.java:580) 
    at org.redisson.misc.RedissonPromise.lambda$onComplete$0(RedissonPromise.java:187) 
    at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577) 
    at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:570) 
    at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:549) 
    at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490) 
    at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615) 
    at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604) 
    at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104) 
    at org.redisson.misc.RedissonPromise.trySuccess(RedissonPromise.java:82) 
    at org.redisson.command.RedisExecutor.handleReference(RedisExecutor.java:483) 
    at org.redisson.command.RedisExecutor.handleSuccess(RedisExecutor.java:476) 
    at org.redisson.command.RedisExecutor.handleResult(RedisExecutor.java:461) 
    at org.redisson.command.RedisExecutor.checkAttemptPromise(RedisExecutor.java:447) 
    at org.redisson.command.RedisExecutor.lambda$execute$3(RedisExecutor.java:169) 
    at org.redisson.misc.RedissonPromise.lambda$onComplete$0(RedissonPromise.java:187) 
    at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577) 
    at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:570) 
    at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:549) 
    at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490) 
    at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615) 
    at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604) 
    at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104) 
    at org.redisson.misc.RedissonPromise.trySuccess(RedissonPromise.java:82) 
    at org.redisson.client.handler.CommandDecoder.completeResponse(CommandDecoder.java:444) 
    at org.redisson.client.handler.CommandDecoder.handleResult(CommandDecoder.java:439) 
    at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:370) 
    at org.redisson.client.handler.CommandDecoder.decodeCommand(CommandDecoder.java:196) 
    at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:134) 
    at org.redisson.client.handler.CommandDecoder.decode(CommandDecoder.java:104) 
    at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:493) 
    at io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:366) 
    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:271) 
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:377) 
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363) 
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:355) 
    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) 
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:377) 
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363) 
    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) 
    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) 
    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) 
    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) 
    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) 
    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) 
    at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) 
    at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) 
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) 
    at java.lang.Thread.run(Thread.java:748)
We are interested in running Sentinel behind a L4 proxy terminating TLS.  This sentinel will report a master at `IP:master_port`.  This master is behind a second L4 proxy terminating TLS on a port we plan to infer from the reported `master_port`.   Ideally, Redisson would offer Sentinel + Master over TLS, with the ability to configure a port mapping function to take `master_port` and translate it to the L4 proxy port terminating TLS in front of the master.   As an aside, our model does not require us to translate either the sentinel or master IP, but the Lettuce example below fully allows that.

Here is the equivalent Lettuce 5.2.2 (due out 2/11/2020) code: 

```
package com.example;

import io.lettuce.core.RedisClient;
import io.lettuce.core.api.StatefulRedisConnection;
import io.lettuce.core.api.sync.RedisCommands;
import io.lettuce.core.internal.HostAndPort;
import io.lettuce.core.resource.ClientResources;
import io.lettuce.core.resource.DnsResolvers;
import io.lettuce.core.resource.MappingSocketAddressResolver;
import org.junit.Test;

public class TestApp {

    @Test
    public void testLettuceSentinel() {
        final MappingSocketAddressResolver portMapper = MappingSocketAddressResolver.create(DnsResolvers.UNRESOLVED,
                                                                                            hostAndPort -> {
                                                                                                int port = hostAndPort.port;
                                                                                                switch (port) {
                                                                                                    case 6380:
                                                                                                        port = port + 1;  
                                                                                                        break;
                                                                                                }
                                                                                                System.out.printf("mapping: %s to %s:%d\n", hostAndPort.toString(), hostAndPort.hostText, port);
                                                                                                return HostAndPort.of(hostAndPort.hostText, port);
                                                                                            });

        final ClientResources clientResources = ClientResources.builder().socketAddressResolver(portMapper).build();

        final RedisClient redisClient = RedisClient.create(clientResources, "rediss-sentinel://192.168.99.1:26381/0#mymaster");

        try (final StatefulRedisConnection<String, String> connection = redisClient.connect()) {
            System.out.println("connected");

            final RedisCommands<String, String> redisCommands = connection.sync();
            redisCommands.clientSetname("blah");

            final String result = redisCommands.set("foo", "bar");
            System.out.printf("set foo result=%s\n", result);

            final String foo = redisCommands.get("foo");
            System.out.printf("get foo=%s\n", foo);
        }
        redisClient.shutdown();
    }
}
```

**Describe alternatives you've considered**

Many.  Including putting the sentinel / master cluster behind an F5 load balancer terminating TLS and let the F5 choose the master.  Clients program to the F5 over plain Redis API over TLS.

 RLock 
lock.isHeldByCurrentThread() verification  before unlocking
but sometimes exception

org.redisson.client.RedisException: Unexpected exception while processing command
        at org.redisson.command.CommandAsyncService.convertException(CommandAsyncService.java:400)
        at org.redisson.command.CommandAsyncService.get(CommandAsyncService.java:204)
        at org.redisson.RedissonObject.get(RedissonObject.java:94)
        at org.redisson.RedissonLock.isHeldByThread(RedissonLock.java:521)
        at org.redisson.RedissonLock.isHeldByCurrentThread(RedissonLock.java:515)
Add a customizer interface that can be implemented by beans wishing to customize the RedissonClient auto configuration
<!--
Сonsider Redisson PRO https://redisson.pro version for advanced features and support by SLA.
-->

Might be connected to https://github.com/redisson/redisson/issues/2120, but I wasn't too sure about that, so I decided to create a new issue instead.

**Expected behavior**

No memory leak?

**Actual behavior**

SentinelConnectionManager is leaking memory because it stores too many connections to the sentinels in the `nodeConnections` map.

We are using a singleton Redisson instance for connecting to our replicated Redis instance (3 nodes, 3 sentinels). After running for some days, we are experiencing high GC times and OOM errors. Upon investigating the heap dump, we noticed that the SentinelConnectionManager stores nearly 50,000 connection to the same 3 sentinels in its `nodeConnections` map. However, the `sentinels` map itself only contains the expected 3 entries.

<img width="593" alt="sentinelconnectionmanager_heap" src="https://user-images.githubusercontent.com/19145587/72245770-9451ca00-35f1-11ea-8516-bb3aeca96700.png">

I tried analyzing and reproducing the leak myself, but I wasn't too successful since I am not familiar with your code. Would be great if you could have a look what could cause such a behavior :)

Unfortunately can't provide the heap dump since it might contain sensitive data.


**Steps to reproduce or test case**

N/A

**Redis version**

4.0.2

**Redisson version**

3.11.3

**Redisson configuration**

```
config.useSentinelServers().setMasterName("mymaster")
    .addSentinelAddress(redisSentinelAddress)
    .setPassword("")
    .setReadMode(ReadMode.MASTER)
    .setSubscriptionMode(SubscriptionMode.MASTER)
    .setMasterConnectionPoolSize(64)
    .setRetryInterval(2000)
    .setRetryAttempts(3)
    .setConnectTimeout(1000)
    .setTimeout(5000);
```
<!--
Сonsider Redisson PRO https://redisson.pro version for advanced features and support by SLA.
-->

**Expected behavior**
We are using Redisson with AWS elasticache redisson cluster. Redisson client throws exception from some of the hosts. Only way to recover is to reboot the EC2 instances/restart the application. Slaves are reachable from the EC2. 

**Actual behavior**
Redisson client fails with the following exception eventhough slaves are reachable from EC2.

**Caused by: org.redisson.client.RedisConnectionException: SlaveConnectionPool no available Redis entries.  Disconnected hosts: [10.0.129.98/10.0.129.98:6379]**
        at org.redisson.connection.pool.ConnectionPool.get(ConnectionPool.java:219) ~[Redisson-3.11.x.jar:?]
        at org.redisson.connection.pool.SlaveConnectionPool.get(SlaveConnectionPool.java:30) ~[Redisson-3.11.x.jar:?]
        at org.redisson.connection.balancer.LoadBalancerManager.nextConnection(LoadBalancerManager.java:248) ~[Redisson-3.11.x.jar:?]
        at org.redisson.connection.MasterSlaveEntry.connectionReadOp(MasterSlaveEntry.java:464) ~[Redisson-3.11.x.jar:?]
        at org.redisson.connection.MasterSlaveConnectionManager.connectionReadOp(MasterSlaveConnectionManager.java:607) ~[Redisson-3.11.x.jar:?]
        at org.redisson.command.RedisExecutor.getConnection(RedisExecutor.java:645) ~[Redisson-3.11.x.jar:?]
        at org.redisson.command.RedisExecutor.execute(RedisExecutor.java:116) ~[Redisson-3.11.x.jar:?]
        at org.redisson.command.RedisExecutor$2.run(RedisExecutor.java:245) ~[Redisson-3.11.x.jar:?]
        at io.netty.util.HashedWheelTimer$HashedWheelTimeout.expire(HashedWheelTimer.java:663) ~[netty-common-4.1.21.Final.jar:?]
        at io.netty.util.HashedWheelTimer$HashedWheelBucket.expireTimeouts(HashedWheelTimer.java:738) ~[netty-common-4.1.21.Final.jar:?]
        at io.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:466) ~[netty-common-4.1.21.Final.jar:?]
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.21.Final.jar:?]

**Steps to reproduce or test case**
Exception happens very randomly in production. 

**Redis version**
5.0.6
**Redisson version**
3.11.6
**Redisson configuration**
config.setNettyThreads(256)
                .useClusterServers()
                .addNodeAddress(redisUrl)
                .setTimeout(200)
                .setRetryInterval(50)
                .setMasterConnectionPoolSize(128)
                .setMasterConnectionMinimumIdleSize(32)
                .setSlaveConnectionPoolSize(128)
                .setSlaveConnectionMinimumIdleSize(32)
                .setPingConnectionInterval(5000)
                .setKeepAlive(true);
<!--
Сonsider Redisson PRO https://redisson.pro version for advanced features and support by SLA.
-->
Use RDelayedQueue hit an bug.
The code is as below:
                RBlockingQueue<DelayJob> blockingFairQueue = redissonClient.getBlockingQueue(topicName);
                RDelayedQueue<DelayJob> delayedQueue = redissonClient.getDelayedQueue(blockingFairQueue);
                delayedQueue.offer(delayJob, 5, TimeUnit.SECONDS);

**Expected behavior**
I expect the job can be executed after 5 second.

**Actual behavior**
In my prod environment, I found almost all job is executed after 5 second, but only a few jobs were executed immediately(do not delay 5 seconds)

**Steps to reproduce or test case**
It is hard to reproducde, only a few jobs were executed immediately in each day

**Redis version**
redis_version:3.2.12
**Redisson version**
3.11.6
**Redisson configuration**

In Spring Boot RedissonAutoConfiguration there is the need for post customizing the autoconfigured RedissonClient.

Would you consider a pull request with this functionality?
**Is your feature request related to a problem? Please describe.**
Redission collection is used in my application (single jar) and want to package needed classes without all optional dependencies to reduce app size.

**Describe the solution you'd like**
Split feature (optional dependencies are used) as sub module.
**Describe alternatives you've considered**
Remove all optional dependencies java classes from my jar (Follow dependency document in wiki). But I don't think it is right approach and whether have potential problems.  