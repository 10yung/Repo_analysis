Hello, I have some stupid problem. I built all, made "sudo make install", and i want to integrate libfann.so in android studio. Where can i take libfann.so file or how i can build it?
hello

i search for using FANN with GPU, and found only one example and now understand this
http://leenissen.dk/fann/html_latest/files2/gpu-txt.html

can you show more examples for GPU ?

request simple-robot-mushroom-xor etc with gpu use

thank you
hello

i have no trouble on linux with fann + erlang
(there i get executable file "fannerl" in fannerl/priv and all is ok)

but i do not fully understand how install fann on windows 7

i install visual studio community 2019, install c++ there, install erlang, this is ok
next i choose VS2010 directory inside fann_dir, open the fann.sln solution, click "build" in VS

visual studio says all ok, compiling ok

but i do not understand what next?
help me please

after compiling fann i see next dll --
```
E:\fann_2.2.0\bin

fanndouble.dll
fanndoubled.dll
fannfixed.dll
fannfixedd.dll
fannfloat.dll
fannfloatd.dll
```

what of thet needs for erlang in
```
E:\fannerl\src\fannerl.erl
at line 1584 --
PrivDir = code:priv_dir(fannerl),
```
?

fannerl in erlang compiles fine --
```
E:\fannerl>rebar3 compile
===> Verifying dependencies...
===> Compiling fannerl
```

but tests not run ok
```
E:\fannerl>rebar3 eunit
===> Verifying dependencies...
===> Compiling fannerl
===> Performing EUnit tests...
======================== EUnit ========================
file "fannerl.app"
  application 'fannerl'
    module 'fannerl'
      module 'fannerl_tests'
        Tests that fannerl can be started and stopped accordingly
          fannerl_tests: fannerl_start_stop_test_...=CRASH REPORT==== 8-May-2019::02:09:04.701000 ===
  crasher:
    initial call: fannerl:init/1
    pid: <0.139.0>
    registered_name: fannerl
    exception error: enoent
      in function  open_port/2
         called as open_port({spawn,"e:\fannerl\priv"},
                             [{packet,2},nouse_stdio,binary,exit_status])
      in call from fannerl:init/1 (e:/fannerl/src/fannerl.erl, line 1604)
    ancestors: [<0.130.0>]
    message_queue_len: 0
    messages: []
    links: [<0.130.0>]
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 376
    stack_size: 27
    reductions: 225
  neighbours:
    neighbour:
      pid: <0.134.0>
      registered_name: []
      initial_call: {eunit_proc,group_leader_process,1}
      current_function: {eunit_proc,group_leader_loop,3}
      ancestors: []
      message_queue_len: 0
      links: [<0.130.0>]
      trap_exit: false
      status: waiting
      heap_size: 233
      stack_size: 7
      reductions: 11
      current_stacktrace: [{eunit_proc,group_leader_loop,3,
                              [{file,"eunit_proc.erl"},{line,584}]}]
    neighbour:
      pid: <0.130.0>
      registered_name: []
      initial_call: {erlang,apply,2}
      current_function: {proc_lib,sync_wait,2}
      ancestors: []
      message_queue_len: 0
      links: [<0.134.0>,<0.139.0>,<0.129.0>]
      trap_exit: false
      status: waiting
      heap_size: 6772
      stack_size: 91
      reductions: 8752
      current_stacktrace: [{proc_lib,sync_wait,2,[{file,"proc_lib.erl"},{line,350}]},
                  {fannerl_tests,fannerl_start_stop,0,
                                 [{file,"e:/fannerl/test/fannerl_tests.erl"},
                                  {line,165}]},
                  {eunit_test,run_testfun,1,
                              [{file,"eunit_test.erl"},{line,71}]},
                  {eunit_proc,run_test,1,[{file,"eunit_proc.erl"},{line,510}]},
                  {eunit_proc,with_timeout,3,
                              [{file,"eunit_proc.erl"},{line,335}]},
                  {eunit_proc,handle_test,2,
                              [{file,"eunit_proc.erl"},{line,493}]},
                  {eunit_proc,tests_inorder,3,
                              [{file,"eunit_proc.erl"},{line,435}]},
                  {eunit_proc,with_timeout,3,
                              [{file,"eunit_proc.erl"},{line,325}]}]
*skipped*
undefined
*unexpected termination of test process*
::{enoent,[{erlang,open_port,
                   [{spawn,"e:\fannerl\priv"},
                    [{packet,2},nouse_stdio,binary,exit_status]],
                   [{file,"erlang.erl"},{line,2213}]},
           {fannerl,init,1,[{file,"e:/fannerl/src/fannerl.erl"},{line,1604}]},
           {proc_lib,init_p_do_apply,3,[{file,"proc_lib.erl"},{line,249}]}]}

=======================================================
  Failed: 0.  Skipped: 0.  Passed: 0.
One or more tests were cancelled.
===> Error running tests
```

help me please
thanks




I have a net with 7 layers:
`320 320 320 320 320 320 1`

Created using the code:
```
std::vector<unsigned int> layers;
layers.push_back(320);
for (int i = 0; i < 5; ++i)
    layers.push_back(320);
layers.push_back(1);
ann.create_standard_array(7, layers.data());
```

In the net file, it is saved as:
`layer_sizes=321 321 321 321 321 321 2 `
...which I assumed has no bearing on actual performance.
Except, that it does â€” while the input & output getting functions return the correct appropriate values, `get_total_neurons()` returns the ones saved in the net file, instead of real neurons.
tested for MinGW-gcc 8.3 and MSVC2017
e.g. keras.initializers.RandomNormal
[The function](https://github.com/libfann/fann/blob/b211dc3db3a6a2540a34fbe8995bf2df63fc9939/src/fann_train.c#L772-L846) has several TODO comments regarding the correctness of the algorithm's implementation. We should review these lines and either fix them or remove the comments.
Closes #85.
Closes #110.

The iRProp- algorithm as given in its original paper does not have
weight-backtracking: If the sign of the derivative changes in an
iteration, the weights should not be updated at all.

This PR should not be merged yet, it is still missing a unit test for the new behavior. Unfortunately, I'm a bit lost as to how we could unit-test this change. @andersfylling, do you maybe have an idea?
I had a overflow-error with the 64bit fannfloat.dll.
rprop_increase_factor and rprop_decrease_factor were similar to 0
Fixed it with this issue from ache7: https://github.com/libfann/fann/issues/85

fann_train.c
```
void fann_update_weights_irpropm(struct fann *ann, unsigned int first_weight, unsigned int past_end)
{
	fann_type *train_slopes = ann->train_slopes;
	fann_type *weights = ann->weights;
	fann_type *prev_steps = ann->prev_steps;
	fann_type *prev_train_slopes = ann->prev_train_slopes;

	double prev_step, next_step, slope, prev_slope, same_sign;

	float increase_factor = ann->rprop_increase_factor;	/*1.2; */
	float decrease_factor = ann->rprop_decrease_factor;	/*0.5; */
	float delta_min = ann->rprop_delta_min;	/*0.0; */
	float delta_max = ann->rprop_delta_max;	/*50.0; */

	unsigned int i = first_weight;

	for(; i != past_end; i++)
	{
 
		prev_step = fann_max(prev_steps[i], (fann_type) 0.0001);	/* prev_step may not be zero because then the training will stop */
    
    
    
		slope = train_slopes[i];
		prev_slope = prev_train_slopes[i];
    
     
		same_sign = prev_slope * slope;

// https://github.com/libfann/fann/issues/85
		next_step = prev_step;
		if (same_sign > 0)
			next_step = fann_min(prev_step * increase_factor, delta_max);
		else if (same_sign < 0) {
			next_step = fann_max(prev_step * decrease_factor, delta_min);
			slope = 0;
		}

		if (slope < 0) {
			weights[i] -= next_step;
			if (weights[i] < -1500) weights[i] = -1500;
		}
		else if (slope > 0) {
			weights[i] += next_step;
			if (weights[i] > 1500) weights[i] = 1500;
		}

    /*
		if(same_sign >= 0.0)
			next_step = fann_min(prev_step * increase_factor, delta_max);
		else
		{
			next_step = fann_max(prev_step * decrease_factor, delta_min);
			slope = 0;
		}
   
     
    
		if(slope < 0)
		{
			weights[i] -= next_step;
			if(weights[i] < -1500)
				weights[i] = -1500;
		}
		else
		{
			weights[i] += next_step;
			if(weights[i] > 1500)
				weights[i] = 1500;
		}
	*/
     
		/*if(i == 2){
		 * printf("weight=%f, slope=%f, next_step=%f, prev_step=%f\n", weights[i], slope, next_step, prev_step);
		 * } */

		/* update global data arrays */
		prev_steps[i] = next_step;
		prev_train_slopes[i] = slope;
		train_slopes[i] = 0.0;

	}
}
```
