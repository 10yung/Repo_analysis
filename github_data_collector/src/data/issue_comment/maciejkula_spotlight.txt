I am trying to predict the implicit sequential model. 

```
data=[['useID','docid','timestamp']]

from sklearn import preprocessing
le_usr = preprocessing.LabelEncoder() # user encoder
le_itm = preprocessing.LabelEncoder() # item encoder

# shift item_ids with +1 (but not user_ids):
item_ids = (le_itm.fit_transform(data['docid'])+1).astype('int32')
user_ids = (le_usr.fit_transform(data['userID'])).astype('int32')


from spotlight.interactions import Interactions
implicit_interactions = Interactions(user_ids, item_ids, timestamps=data.timestamp)

from spotlight.cross_validation import user_based_train_test_split, random_train_test_split
train, test = user_based_train_test_split(implicit_interactions, 0.3)
```

Now I use the following code to train:

from spotlight.sequence.implicit import ImplicitSequenceModel

```
sequential_interaction = train.to_sequence()
implicit_sequence_model = ImplicitSequenceModel(use_cuda=False, n_iter=1, loss='bpr', representation='pooling')
implicit_sequence_model.fit(sequential_interaction, verbose=True)
```

But after this lets say i want to predict for "user_ids" (one hot encoded at the top) 1000, I dont know how to use the predict function.

As per my understanding, predict function should take the input of sequence for that user, but how to find out that sequence for that user?

`sequential_interaction` when used with` .sequences()` doesnt give me which row belongs to which user.

Can you please provide an example to use predict on this data ? Any example using implicit sequence model would be helpful.

Thanks

My questions might seem dumb, but I was trying to understand the project. As seen in the documentation, various parameters go into the model. I was thinking if there is an easy way to do Hyperparameter tuning in spotlight? 
I am wondering if Spotlight includes the iMF model proposed in _'Collaborative Filtering for Implicit Feedback Datasets'_ by **Yifan Hu** (not BPR)? 
LSTM based sequential model seems to fit data really well on first epoch, but validation accuracy doesn't improve after first epoch and even loss halts improving after second epoch. I tried different optimizer paramters like lr, l2, etc., but it doesn't improve beyond certain point.

@maciejkula Any idea how can improve that?
Can I know why you use the last k as the target and :-k as the sequences. When in sequence recommendation case, isn't it is better to use only the last element in the sequence as the target? CMIIW
Tensorboard is now supported by PyTorch. Does spotlight support it or will do so in the future? And what is a suggested way of monitoring model training (training and validation loss/metrics at step n) when using spotlight models?

Thank you very much!
I'm trying to download spotlight on my Mac with Python 3.7 and I'm getting this error:

```
bash-3.2# conda install -c maciejkula -c pytorch spotlight=0.1.5
Collecting package metadata (current_repodata.json): done
Solving environment: failed with current_repodata.json, will retry with next repodata source.
Initial quick solve with frozen env failed.  Unfreezing env and trying again.
Solving environment: failed with current_repodata.json, will retry with next repodata source.
Collecting package metadata (repodata.json): done
Solving environment: failed
Initial quick solve with frozen env failed.  Unfreezing env and trying again.
Solving environment: failed

UnsatisfiableError: The following specifications were found
to be incompatible with the existing python installation in your environment:

  - spotlight=0.1.5 -> python[version='>=2.7,<2.8.0a0,>=3.5,<3.6.0a0,>=3.6,<3.7.0a0']

If python is on the left-most side of the chain, that's the version you've asked for.
When python appears to the right, that indicates that the thing on the left is somehow
not available for the python version you are constrained to.  Your current python version
is (python=3.7).  Note that conda will not change your python version to a different minor version
unless you explicitly specify that.

The following specifications were found to be incompatible with each other:



Package wheel conflicts for:
spotlight=0.1.5 -> scikit-learn -> scipy -> numpy[version='>=1.11.3,<2.0a0,>=1.15.1,<2.0a0,>=1.9.3,<2.0a0'] -> mkl_random -> numpy-base[version='>=1.0.2,<2.0a0'] -> python[version='>=2.7,<2.8.0a0,>=3.5,<3.6.0a0,>=3.6,<3.7.0a0'] -> pip -> wheel
python=3.7 -> pip -> wheel
Package certifi conflicts for:
python=3.7 -> pip -> wheel -> setuptools -> certifi[version='>=2016.09']
spotlight=0.1.5 -> scikit-learn -> scipy -> numpy[version='>=1.11.3,<2.0a0,>=1.15.1,<2.0a0,>=1.9.3,<2.0a0'] -> mkl_random -> numpy-base[version='>=1.0.2,<2.0a0'] -> python[version='>=2.7,<2.8.0a0,>=3.5,<3.6.0a0,>=3.6,<3.7.0a0'] -> pip -> wheel -> setuptools -> certifi[version='>=2016.09']
Package pip conflicts for:
spotlight=0.1.5 -> scikit-learn -> scipy -> numpy[version='>=1.11.3,<2.0a0,>=1.15.1,<2.0a0,>=1.9.3,<2.0a0'] -> mkl_random -> numpy-base[version='>=1.0.2,<2.0a0'] -> python[version='>=2.7,<2.8.0a0,>=3.5,<3.6.0a0,>=3.6,<3.7.0a0'] -> pip
python=3.7 -> pip
Package setuptools conflicts for:
spotlight=0.1.5 -> scikit-learn -> scipy -> numpy[version='>=1.11.3,<2.0a0,>=1.15.1,<2.0a0,>=1.9.3,<2.0a0'] -> mkl_random -> numpy-base[version='>=1.0.2,<2.0a0'] -> python[version='>=2.7,<2.8.0a0,>=3.5,<3.6.0a0,>=3.6,<3.7.0a0'] -> pip -> wheel -> setuptools
python=3.7 -> pip -> wheel -> setuptools
```
Hello, 

I just notice that you start count id from 1, which lead to one more dimension abuse in both user and item.
For example, the number of user and item in Movielens 1M is 6040 and 3706.
Actually, your final processed dataset , including Scipy matrix is  in 6041*3707 shape.

This might be a tiny problem.

Hello,
I was wondering if you have any plan to refactor the code and make it more look like a framework like fastai, ignite, etc. 
Right now we have to implement the training loop for each new model and this is not efficient. If we could use callbacks, it would be more easy to use more complicated training approaches(like anneal learning rate) and at the same time use something like TensorFX and even logging without headache.
Hi!

I'm trying to train an implicit sequential model on click stream data, but as soon as I try to evaluate (e.g. using MRR, or Precision & Recall) after having trained the model, it throws an error:

    mrr = spotlight.evaluation.mrr_score(implicit_sequence_model, test, train)

    ValueErrorTraceback (most recent call last)
    <ipython-input-78-349343a26e9b> in <module>
    ----> 1 mrr = spotlight.evaluation.mrr_score(implicit_sequence_model, test, train)

    ~/.local/lib/python3.7/site-packages/spotlight/evaluation.py in mrr_score(model, test, train)
         45             continue
         46
    ---> 47         predictions = -model.predict(user_id)
         48
         49         if train is not None:

    ~/.local/lib/python3.7/site-packages/spotlight/sequence/implicit.py in predict(self, sequences, item_ids)
        316
        317         self._check_input(item_ids)
    --> 318         self._check_input(sequences)
        319
        320         sequences = torch.from_numpy(sequences.astype(np.int64).reshape(1, -1))

    ~/.local/lib/python3.7/site-packages/spotlight/sequence/implicit.py in _check_input(self, item_ids)
        188
        189         if item_id_max >= self._num_items:
    --> 190             raise ValueError('Maximum item id greater '
        191                              'than number of items in model.')
        192

    ValueError: Maximum item id greater than number of items in model.

Perhaps the error is obvious, but I can't pinpoint what I'm doing wrong, so below I'll describe as concisely as possible, what I'm doing.

## Comparison of experimental with synthetic data

I tried generating synthetic data and use that instead of my experimental data, and then it works. This lead me to compare the data structure of the synthetic data with my experimental:

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 1:</span> Synthetic data with N=100 unique users, M=1k unique items, and Q=10k interactions</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">user_id</th>
<th scope="col" class="org-left">item_id</th>
<th scope="col" class="org-left">timestamp</th>
</tr>
</thead>

<tbody>
<tr>
<td class="org-left">0</td>
<td class="org-left">958</td>
<td class="org-left">1</td>
</tr>


<tr>
<td class="org-left">0</td>
<td class="org-left">657</td>
<td class="org-left">2</td>
</tr>


<tr>
<td class="org-left">0</td>
<td class="org-left">172</td>
<td class="org-left">3</td>
</tr>


<tr>
<td class="org-left">1</td>
<td class="org-left">129</td>
<td class="org-left">4</td>
</tr>


<tr>
<td class="org-left">1</td>
<td class="org-left">.</td>
<td class="org-left">5</td>
</tr>


<tr>
<td class="org-left">1</td>
<td class="org-left">.</td>
<td class="org-left">6</td>
</tr>


<tr>
<td class="org-left">.</td>
<td class="org-left">.</td>
<td class="org-left">.</td>
</tr>


<tr>
<td class="org-left">.</td>
<td class="org-left">.</td>
<td class="org-left">.</td>
</tr>


<tr>
<td class="org-left">.</td>
<td class="org-left">.</td>
<td class="org-left">.</td>
</tr>


<tr>
<td class="org-left">.</td>
<td class="org-left">.</td>
<td class="org-left">.</td>
</tr>


<tr>
<td class="org-left">N</td>
<td class="org-left">.</td>
<td class="org-left">Q-2</td>
</tr>


<tr>
<td class="org-left">N</td>
<td class="org-left">.</td>
<td class="org-left">Q-1</td>
</tr>


<tr>
<td class="org-left">N</td>
<td class="org-left">459</td>
<td class="org-left">Q</td>
</tr>
</tbody>
</table>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 2:</span> Experimental data, N=2.5M users, M=20k items, Q=14.8M interactions</caption>

<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">user_id</th>
<th scope="col" class="org-right">item_id</th>
<th scope="col" class="org-right">timestamp</th>
</tr>
</thead>

<tbody>
<tr>
<td class="org-right">725397</td>
<td class="org-right">3992</td>
<td class="org-right">0</td>
</tr>


<tr>
<td class="org-right">2108444</td>
<td class="org-right">10093</td>
<td class="org-right">1</td>
</tr>


<tr>
<td class="org-right">2108444</td>
<td class="org-right">10093</td>
<td class="org-right">2</td>
</tr>


<tr>
<td class="org-right">1840496</td>
<td class="org-right">15616</td>
<td class="org-right">3</td>
</tr>


<tr>
<td class="org-right">1792861</td>
<td class="org-right">16551</td>
<td class="org-right">4</td>
</tr>


<tr>
<td class="org-right">1960701</td>
<td class="org-right">16537</td>
<td class="org-right">5</td>
</tr>


<tr>
<td class="org-right">1140742</td>
<td class="org-right">6791</td>
<td class="org-right">6</td>
</tr>


<tr>
<td class="org-right">2074022</td>
<td class="org-right">4263</td>
<td class="org-right">.</td>
</tr>


<tr>
<td class="org-right">2368959</td>
<td class="org-right">19258</td>
<td class="org-right">.</td>
</tr>


<tr>
<td class="org-right">2368959</td>
<td class="org-right">17218</td>
<td class="org-right">.</td>
</tr>


<tr>
<td class="org-right">.</td>
<td class="org-right">.</td>
<td class="org-right">.</td>
</tr>


<tr>
<td class="org-right">.</td>
<td class="org-right">.</td>
<td class="org-right">Q-1</td>
</tr>


<tr>
<td class="org-right">.</td>
<td class="org-right">.</td>
<td class="org-right">Q</td>
</tr>
</tbody>
</table>

1. Both data sets have users indexed from `[0..N-1]`, but my experimental is not sorted on `user_ids` as is the case for the synthetic data.

2. Both data sets have `item_ids` indexed from `[1..M]`, yet it only throws the "ValueError: Maximum item id greater than number of items in model." for my experimental data.

3. I've re-shaped my timestamps to be just the data frame index after sorting on time, so this is also as in the synthetic data set. (Previously my timestamps were in seconds since 1970 of the event, and some events were simultaneous, i.e. order arbitrary/degenerate state.


## Code for processing the experimental data:

```python
# pandas dataframe with unique string identifier for users ('session_id'), 
# and 'Article number' for item_id, and 'timestamp' for event
df = df.sort_values(by=['timestamp']).reset_index(drop=True)


# encode string identifiers for users and items to integer values:
from sklearn import preprocessing
le_usr = preprocessing.LabelEncoder() # user encoder
le_itm = preprocessing.LabelEncoder() # item encoder

# shift item_ids with +1 (but not user_ids):
item_ids = (le_itm.fit_transform(df['Article number']) + 1).astype('int32')
user_ids = (le_usr.fit_transform(df['session_id'])     + 0).astype('int32')


from spotlight.interactions import Interactions
implicit_interactions = Interactions(user_ids, item_ids, timestamps=df.index.values)

from spotlight.cross_validation import user_based_train_test_split, random_train_test_split
train, test = random_train_test_split(implicit_interactions, 0.2)
```


## Code for training the model:

``` python
from spotlight.sequence.implicit import ImplicitSequenceModel
sequential_interaction = train.to_sequence()
implicit_sequence_model = ImplicitSequenceModel(use_cuda=True, n_iter=10, loss='pointwise', representation='pooling')
implicit_sequence_model.fit(sequential_interaction, verbose=True)

import spotlight.evaluation
mrr = spotlight.evaluation.mrr_score(implicit_sequence_model, test, train)
```

## Questions on input format:

Here are some questions I thought might pinpoint the error, in where my data might differ from the synthetic data set:

1. Is there any purpose, or even harm, to include users with only a single interaction?

2. Does the model allow a user have multiple events with the same timestamp-value?

3. As long as `(userid,itemid,timestamp)` triplets pair up, does row-ordering matter?
