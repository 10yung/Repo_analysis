Very useful package.  I have a minor suggestion: in `add_pi` (and possibly other functions - I haven't checked), an error is thrown if `tb` does not include a column for the response variable.  The actual values in the column are ignored, but the column has to be present.  I'd guess this is due to some internal code similar to:
`X <- formula(fit) %>%  model.matrix(data = tb)`
used to get the design matrix for simulation-based prediction intervals.  Using a few more steps should eliminate the need to include the response.  Something like:
`chr_formula <- formula(fit) %>% deparse() %>% strsplit(' ') %>% getElement(1) `
`X <- as.formula(chr_formula[-1]) %>% model.matrix(data = tb)`
I noticed this specifically with a Poisson GLM.  `add_ci` did not require the response to be in `tb`.
Dear John,<br>
I am running GLMM models with `family=Gamma` and I want to use `add_ci()` to compute confidence intervals around predictions (many thanks for developing this by the way).<br>Unfortunately, `add_ci()` *often* encounters the following error:
`Error in quantile.default(newX[, i], ...) : missing values and NaN's not allowed if 'na.rm' is FALSE`

*often* but not *always*; I can re-run `add_ci()` and it might fail or not.<br>

And it seems to be associated with this type of warning message:
`In lme4::bootMer(fit, my_pred, nsim = nSims, type = "parametric",  :  some bootstrap runs failed (1/100)`

Here is the code Iâ€™m running:
`modfinal<-glmer(cumulative_duration ~ winter + month + s_cumul_dura_small + big_treat + 
    (1 | id_first_feederpair), data=data_FRICOE, family = Gamma(link = "log"))`,
with `cumulative_duration` being the time (in seconds) spent by the common chaffinch (*Fringilla coelebs*) on a feeder.

`newdatpred <- expand.grid(winter=as.factor("4")`
`,month=as.factor("Jan")`
`,s_cumul_dura_small=0`
`,big_treat=as.factor(c("NONE","PICPIC"))`
`,id_first_feederpair=as.factor("0060564"))`

`add_ci(newdatpred,modfinal,alpha=0.01,response=T,includeRanef=F,type="boot",nSims=100,names=c("lowBCI","uppBCI"))`
(Note: *nSims* set to 100 for speed reason, but planning on using 1000 for final results)

I hope I'm giving enough useful details on this. I'm happy to share more as needed.
I attach the data table *data_FRICOE.txt* if it can help reproduce this error (several runs of `add_ci()` might be required given it does not always yield this error)
[data_FRICOE.txt](https://github.com/jthaman/ciTools/files/3657637/data_FRICOE.txt)

Trying `add_ci` on a model with a formula involving `I(...)`, i.e.
```
library(ciTools)
library(lme4)

data <- data.frame(x = c(1, 2, 3, 4), y = c(1, 3, 10, 15), gr = (1, 1, 2, 2))
mod <- lmer(y ~ I(x^2) + (1|gr), data)
add_ci(data, mod)
```
results in `Error in x^2: non-numeric argument to binary operator`.  Attempting `add_ci(data, mod, type = "parametric")` results in ``Error in `[[<-.data.frame`(`*tmp*`, names[1], value = numeric(0)) : replacement has 0 rows, data has 4``.   The code completes in other cases, but returns spurious confidence intervals and the warning `longer object length is not a multiple of shorter object length`.  At least in the parametric case, the problem appears to be in `get_x_matrix_mermod`.

Trying `add_ci` on a model with no fixed effects, i.e.
```
data <- data.frame(x = rep(seq(5), 2), y = c(rnorm(5), rnorm(5, mean = 0.5)), gr = c(rep(1, 5), rep(2, 5)))
mod <- lmer(y ~ (1 | gr), data)
data %>% add_ci(mod, type = "parametric")
```
results in `Error in reformulate(attributes(terms(fit))$term.labels) : 'termlabels' must be a character vector of length at least one`.  Again, the problem appears to be in `get_x_matrix_mermod`.
Beta regression is a good alternative to binomial regression when the number of trials is unknown. I would like to support this model!
Some users don't fit mixed models! 
For certain methods like add_pi.glm and add_quantile.glm, a small parametric bootstrap is performed to generate the desired statistic. Presently if these two methods are used, a separate simulation is run for each method. If the number of replicates (nSims) and new data (tb) are not changed, it would be nice to store and reuse the simulation data.

The benefits of storing simulation data are twofold:
1. We can speed up our computations if many methods are run in sequence
2. add_pi, add_qauntile, and add_probs will be consistent within an analysis --- each statistic calculated on the same simulation data.

Though we could just handle the second point by setting a seed.



 
Was also wondering if it would be hard to also support mgcv::gam (generalized additive models) and scam (shape constrained additive models) models? The recipe to calculate confidence and prediction intervals for these should be quite similar to that for GLMs right?
I'm finding that we cannot use add_ci.lmer for "big data". I tried an example from the mermod vignette with 200,000 observations and found that R couldn't put the new data frame into memory. Here's the example I tried:

```
## linear example

x_gen_mermod <- function(ng = 8, nw = 5){
  n <- ng * nw
  x2 <- runif(n)
  group <- rep(as.character(1:ng), each = nw)
  return(tibble::tibble(x2 = x2,
                        group = group))
}

mm_pipe <- function(tb, ...){
  model.matrix(data = tb, ...)
}

get_validation_set <- function(tb, sigma, sigmaG, beta, includeRanef, groupIntercepts){
  vm <- sample_n(tb, 5, replace = F)[rep(1:5, each = 100), ]
  vf <- bind_rows(vm, tb) %>%
    select(-group) %>%
    mm_pipe(~.*.)
  vf <- vf[1:500, ]
  vGroups <- if(!includeRanef) rnorm(500, 0, sigmaG) else groupIntercepts[as.numeric(vm$group)]
  vm[["y"]] <- vf %*% beta + vGroups + rnorm(500, mean = 0, sd = sigma)
  vm
}

y_gen_mermod <- function(tb, sigma = 1, sigmaG = 1, delta = 1, includeRanef = FALSE, validationPoints = FALSE){
  groupIntercepts <- rnorm(length(unique(tb$group)), 0, sigmaG)
  tf <- tb %>%
    dplyr::select(-group) %>%
    mm_pipe(~.*.)
  beta <- rep(delta, ncol(tf))
  if(validationPoints)  {
    vm <- get_validation_set(tb, sigma, sigmaG, beta, includeRanef, groupIntercepts)
  }
  tb[["y"]] <- tf %*% beta + groupIntercepts[as.numeric(tb$group)] + rnorm(nrow(tb), mean = 0, sd = sigma)
  tb[["truth"]] <- tf %*% beta + groupIntercepts[as.numeric(tb$group)] * (includeRanef)
  if(validationPoints) return(list(tb = tb, vm = vm)) else return(tb)
}


tb <- x_gen_mermod(10, 20000) %>%
    y_gen_mermod()

fit2 <- lmer(y ~ x2 + (1|group) , data = tb)

tb %>% add_ci(fit2, type = "parametric", includeRanef = TRUE, names = c("LCB", "UCB"))
```

Lmer works just fine on an example data set this large, but ciTools chokes and spits out 

```
Error: cannot allocate vector of size 298.0 Gb
```

We need to re-examine how we are storing things in memory and see if we can do something more efficient. I'm not sure if this bug affects the other methods as well.
Would it also be possible for the package to support confidence and prediction intervals for gls models?
I would like to have some methods for inflated Poisson and NegBin models. I'm not sure how these models are commonly fit in R (what package or functions are used?). I think this issue is less important than #28 and #25. 