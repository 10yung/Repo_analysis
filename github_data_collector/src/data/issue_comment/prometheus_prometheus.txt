Except common, which is handled in #6661 

Signed-off-by: Julien Pivotto <roidelapluie@inuits.eu>

<!--
    Don't forget!
    
    - If the PR adds or changes a behaviour or fixes a bug of an exported API it would need a unit/e2e test.
    
    - Where possible use only exported APIs for tests to simplify the review and make it as close as possible to an actual library usage.
    
    - No tests are needed for internal implementation changes.
    
    - Performance improvements would need a benchmark test to prove it.
    
    - All exposed objects should have a comment.
    
    - All comments should start with a capital letter and end with a full stop.
 -->
Signed-off-by: Julien Pivotto <roidelapluie@inuits.eu>

<!--
    Don't forget!
    
    - If the PR adds or changes a behaviour or fixes a bug of an exported API it would need a unit/e2e test.
    
    - Where possible use only exported APIs for tests to simplify the review and make it as close as possible to an actual library usage.
    
    - No tests are needed for internal implementation changes.
    
    - Performance improvements would need a benchmark test to prove it.
    
    - All exposed objects should have a comment.
    
    - All comments should start with a capital letter and end with a full stop.
 -->
Inspired by https://github.com/prometheus/prometheus/pull/6347 and
https://github.com/prometheus/prometheus/pull/6347#issuecomment-570151979

Signed-off-by: Julien Pivotto <roidelapluie@inuits.eu>

<!--
    Don't forget!
    
    - If the PR adds or changes a behaviour or fixes a bug of an exported API it would need a unit/e2e test.
    
    - Where possible use only exported APIs for tests to simplify the review and make it as close as possible to an actual library usage.
    
    - No tests are needed for internal implementation changes.
    
    - Performance improvements would need a benchmark test to prove it.
    
    - All exposed objects should have a comment.
    
    - All comments should start with a capital letter and end with a full stop.
 -->
<!--
    Don't forget!
    
    - If the PR adds or changes a behaviour or fixes a bug of an exported API it would need a unit/e2e test.
    
    - Where possible use only exported APIs for tests to simplify the review and make it as close as possible to an actual library usage.
    
    - No tests are needed for internal implementation changes.
    
    - Performance improvements would need a benchmark test to prove it.
    
    - All exposed objects should have a comment.
    
    - All comments should start with a capital letter and end with a full stop.
 -->

This sentence was missing from this endpoint but it is present in the rest of them. I think it might make sense to add a new rule to the PR template, just in the case of new API endpoints. 


## Bug Report
This is a similar issue as https://github.com/prometheus/prometheus/issues/6408 and https://github.com/prometheus/prometheus/issues/6595.

**What did you do?**
I am running Prometheus with default retention and TSDB settings. It is scraping ~4k metrics per second.

**What did you expect to see?**
I expected the WAL to stay small and new compact TSDB block to be created.

**What did you see instead? Under which circumstances?**
Under steady load of ~4k metrics/second, the compaction seems to run forever, leading to the WAL and RAM filling up until OOM. Due to this checkpointing also fails, Prometheus crashes unclean and the cycle repeats.

**Environment**

* System information:

Production: Linux 4.9.0-4-amd64 x86_64
Reproduced on: Darwin 18.7.0 x86_64

* Prometheus version:

Production: Docker image of 2.15.2
Reproduced with: 669592a2c4d59697ce3f654db2c1e7d5e3d42714

* Alertmanager version:

not relevant

* Prometheus configuration file:

not relevant

* Alertmanager configuration file:

not relevant

* Reproduced with code:

```go
package main

import (
	"github.com/prometheus/prometheus/tsdb"
)

func main() {
	db, err := tsdb.OpenDBReadOnly("data", nil)
	if err != nil {
		panic(err)
	}
	defer db.Close()

	err = db.FlushWAL("data")
	if err != nil {
		panic(err)
	}
}
```

* Logs:

no relevant errors or messages in log, no output is printing during compaction running forever at 100% of one CPU core.

* Probable root cause:

During my reproduction I let the FlushWAL run until the RAM usage of my little program no longer went down (starting at ~8 GB after TSDB and WAL load, down to ~500 MB). Then I paused the program with the debugger and detected the following situation:

I am running into the case of batchNames staying empty for every iteration of the top level for-loop here:
https://github.com/prometheus/prometheus/blob/669592a2c4d59697ce3f654db2c1e7d5e3d42714/tsdb/index/index.go#L815-L827

The first label position is 20814, while maxPostings is 19476 and that causes it to break on the first iteration of the inner for-loop, leaving names[] untouched and batchNames empty and that causes it to get stuck in an endless loop.
Signed-off-by: Julien Pivotto <roidelapluie@inuits.eu>

<!--
    Don't forget!
    
    - If the PR adds or changes a behaviour or fixes a bug of an exported API it would need a unit/e2e test.
    
    - Where possible use only exported APIs for tests to simplify the review and make it as close as possible to an actual library usage.
    
    - No tests are needed for internal implementation changes.
    
    - Performance improvements would need a benchmark test to prove it.
    
    - All exposed objects should have a comment.
    
    - All comments should start with a capital letter and end with a full stop.
 -->
This will make big queries that only touch the head faster,
though queries that touch both the head and a block will still
be the same speed. This probably won't help much with graphing
unless the range is under an hour, however it should make most
recording rules faster. 

PromQL benchmarks for histograms show only 2-3% improvement, but
they're only over 1k series. Looks like we'd be saving about 1s for a head-only
query that touches 1M series, so a 1M series histogram_quantile(rate(5m)) 
would go from around 12s to 11s.
This changes exposes the `stripeSize` const as a configurable option in tsdb. Not passing in the option will result in the tsdb using the default stripe size that is uses today.

This is an important change for Cortex's new experimental TSDB feature as it allows us to reduce the memory footprint of TSDB's as they are scaled to many nodes. More info about this memory usage can be found [here](https://github.com/cortexproject/cortex/pull/1947)

@codesome @krasi-georgiev 
config:
scrape_configs:
- job_name: sub-prometheus
  ...
  scrape_interval: 1m
  scrape_timeout: 1m
  metrics_path: /federate
  scheme: http
  consul_sd_configs:
  - server: consul:8700 
    tag_separator: ','
    scheme: http
    allow_stale: true
    refresh_interval: 1m
    tags:
    - kubernetes
  relabel_configs:

prometheus: version 2.10.0

log:
level=error ts=2020-01-16T07:25:56.075Z caller=consul.go:360 component="discovery manager scrape" discovery=consul msg="Error refreshing service list" err="Get http://consul:8500/v1/catalog/services?index=227057&stale=&wait=30000ms: context canceled"


code:
// Watch the catalog for new services we would like to watch. This is called only
// when we don't know yet the names of the services and need to ask Consul the
// entire list of services.
func (d *Discovery) watchServices(ctx context.Context, ch chan<- []*targetgroup.Group, lastIndex *uint64, services map[string]func()) {
	catalog := d.client.Catalog()
	level.Debug(d.logger).Log("msg", "Watching services", "tags", d.watchedTags)

	t0 := time.Now()
	opts := &consul.QueryOptions{
		WaitIndex:  *lastIndex,
		WaitTime:   watchTimeout,
		AllowStale: d.allowStale,
		NodeMeta:   d.watchedNodeMeta,
	}
	srvs, meta, err := catalog.Services(opts.WithContext(ctx))
	elapsed := time.Since(t0)
	rpcDuration.WithLabelValues("catalog", "services").Observe(elapsed.Seconds())

	if err != nil {
		level.Error(d.logger).Log("msg", "Error refreshing service list", "err", err)
		rpcFailuresCount.Inc()
		time.Sleep(retryInterval)
		return
	}
	// If the index equals the previous one, the watch timed out with no update.
	if meta.LastIndex == *lastIndex {
		return
	}
	*lastIndex = meta.LastIndex

	// Check for new services.
	for name := range srvs {
		// catalog.Service() returns a map of service name to tags, we can use that to watch
		// only the services that have the tag we are looking for (if specified).
		// In the future consul will also support server side for service metadata.
		// https://github.com/hashicorp/consul/issues/1107
		if !d.shouldWatch(name, srvs[name]) {
			continue
		}
		if _, ok := services[name]; ok {
			continue // We are already watching the service.
		}

		wctx, cancel := context.WithCancel(ctx)
		d.watchService(wctx, ch, name)
		services[name] = cancel
	}

	// Check for removed services.
	for name, cancel := range services {
		if _, ok := srvs[name]; !ok {
			// Call the watch cancellation function.
			cancel()
			delete(services, name)

			// Send clearing target group.
			select {
			case <-ctx.Done():
				return
			case ch <- []*targetgroup.Group{{Source: name}}:
			}
		}
	}
}


Another potential exemplar storage option, this time without messing with the Appender interface. Hooking up `ExemplarStorage` with other components, such as scrape, is left for a future PR. 

Signed-off-by: Callum Styan <callumstyan@gmail.com>