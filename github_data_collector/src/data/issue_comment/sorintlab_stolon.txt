Use k8s client patch method to update pod annotations (in some versions
replacing the whole pod object will return an error)
As described in #748 I propose the following changes to improve the documentation of the syncTimeout.
As multiple users have experienced the hard way, the default timeout is currently an undocumented arbitrary number of minutes that can easily become a problem if you need to recover from a full backup a day ago and need to replay a large number of transactions.

Out of scope: A nicer solution might be to monitor the progress of the recovery, but I'm not sure how to implement that. Yet another option may be to remove the timeout entirely.

For now the least I can do is try and improve the user experience by improving the documentation as suggested previously:

@johannesboon Feel free to open an RFE issue to request this to be documented and also a PR to add this to the doc.

_Originally posted by @sgotti in https://github.com/sorintlab/stolon/issues/685#issuecomment-563191033_
<!-- **NOTE:** Please submit only bug reports or RFE via the issue tracker. For other question or if unsure ask on the mailing list or gitter channel: https://github.com/sorintlab/stolon#contacts -->

### Submission type

<!-- Delete the inappropriate option below: -->

 - Bug report
 - Request for enhancement (RFE)


### Environment
kubernetes

#### Stolon version
0.14.0
#### Additional environment information if useful to understand the bug
Stolon keeper stops working and I see following messages

2019-12-11T19:42:36.214Z        ERROR   cmd/keeper.go:669       cannot get configured pg parameters     {"error": "dial unix /tmp/.s.PGSQL.5432: connect: connection refused"}
2019-12-11T19:42:38.715Z        ERROR   cmd/keeper.go:669       cannot get configured pg parameters     {"error": "dial unix /tmp/.s.PGSQL.5432: connect: connection refused"}
2019-12-11T19:42:40.112Z        INFO    cmd/keeper.go:1441      our db requested role is master
2019-12-11T19:42:40.121Z        INFO    postgresql/postgresql.go:313    starting database
2019-12-11 19:42:40.205 UTC [1218801] FATAL:  pre-existing shared memory block (key 5432001, ID 98304) is still in use
2019-12-11 19:42:40.205 UTC [1218801] HINT:  Terminate any old server processes associated with data directory "/stolon-data/postgres".
2019-12-11T19:42:40.321Z        ERROR   cmd/keeper.go:1460      failed to start postgres        {"error": "postgres exited unexpectedly"}
2019-12-11T19:42:41.216Z        ERROR   cmd/keeper.go:669       cannot get configured pg parameters     {"error": "dial unix /tmp/.s.PGSQL.5432: connect: connection refused"}
2019-12-11T19:42:43.717Z        ERROR   cmd/keeper.go:669       cannot get configured pg parameters     {"error": "dial unix /tmp/.s.PGSQL.5432: connect: connection refused"}
2019-12-11T19:42:45.412Z        INFO    cmd/keeper.go:1441      our db requested role is master
2019-12-11T19:42:45.420Z        INFO    postgresql/postgresql.go:313    starting database
2019-12-11 19:42:45.427 UTC [1218806] FATAL:  pre-existing shared memory block (key 5432001, ID 98304) is still in use
2019-12-11 19:42:45.427 UTC [1218806] HINT:  Terminate any old server processes associated with data directory "/stolon-data/postgres".

<!-- Bug report -->

### Expected behaviour you didn't see
Stolon postgres should have been up
### Unexpected behaviour you saw
Stolon postgres instance is down
### Steps to reproduce the problem
Over period of time, stolon shows this behavior

<!-- RFE -->

### Enhancement Description

With this change, we ensure that the replica pods of the same
deployment do not get assigned to the same node. Fot this, we have
employed k8s anti-affinity rule.
<!-- **NOTE:** Please submit only bug reports or RFE via the issue tracker. For other question or if unsure ask on the mailing list or gitter channel: https://github.com/sorintlab/stolon#contacts -->

### Submission type

<!-- Delete the inappropriate option below: -->

 - Request for enhancement (RFE)

<!-- RFE -->

### Enhancement Description

Currently there is a single way to init or restore standby node via pg_basebackup with [hard-coded options](https://github.com/sorintlab/stolon/blob/master/internal/postgresql/postgresql.go#L966-L969). This becomes problematic both for users [preferring to add custom options](https://github.com/sorintlab/stolon/issues/649) to `pg_basebackup` and for enterprises with big databases (TB+) wishing to restore data from external backup through high-throughput network and to not have any impact on master at restore process.
I propose to specify the option in cluster config (or for nodes, in particular) that describes the custom sequence of strategies to restore standby node as it's implemented in [patroni](https://github.com/zalando/patroni/blob/master/docs/replica_bootstrap.rst#building-replicas). I see it have to be specified as array (sequence) of objects each of which consists of `dataRestoreCommand` field describing data restoring and optional `archiveRecoverySettings` that describes WAL recovery. Custom recovery stops as soon as any first strategy succeeds. 

Any thoughts, ideas or objections?
Given the resolutions to the following issues: #562 #604, I felt that having an explicit doc would increase visibility for the solution. Hence this PR.
In sentinel, when **timer.Since(kih.Timer) > s.sleepInterval** will delete this keeperInfo, but the time between twice check is already  s.sleepInterval. so should it more than s.sleepInterval? Correct me if i am wrong
```
else if kih.Seen && timer.Since(kih.Timer) > s.sleepInterval {
					//Remove since it wasn't updated
					delete(tmpKeepersInfo, ki.UID)
				}
```
Currently, `sleepInterval` is respected by `keeper` and `sentinel`. It is not there for proxy. @sgotti Correct me if I am wrong. Can we add it proxy ?
We don't have timeout for libkv based stores(we are using consul) at [here](https://github.com/sorintlab/stolon/blob/master/internal/store/libkv.go#L55). Can we add it? Is there any reason for not having it?