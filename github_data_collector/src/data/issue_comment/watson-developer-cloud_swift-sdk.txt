### Summary

This PR adds the following functionality:

- Support for new parameters in SpeechToTextV1 `recognize()` method
- Addition of new voice models in TextToSpeechV1
- Travis build updates for improved stability

### Other Information

This will result in the release of `3.2.0` of the Swift SDK


Thanks for contributing to the Watson Developer Cloud!
let audioSession = AVAudioSession.sharedInstance()
    try? audioSession.setCategory(AVAudioSession.Category.playAndRecord, mode: AVAudioSession.Mode.default, options: [.defaultToSpeaker, .allowBluetooth,.allowBluetoothA2DP])
    do {
        try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
    } catch {
        print("Error in the session")
    }

let utterance = AVSpeechUtterance(string: text)
utterance.voice = AVSpeechSynthesisVoice(language: "en-US")
synth.speak(utterance)                            
synth.continueSpeaking()

I tried using only . allowBluetoothA2DP but after doing so the audio cannot be heard in the car's bluetooth speaker.
### When you open an issue for a feature request, please add as much detail as possible:

Currently, the interface exposed in `SpeechToTextV1/SpeechToText+Recognize.swift` only leaves a `SpeechToTextSession` alive for the time that it takes to transcribe a `Data` blob.

We should add support to send smaller chunks of data in realtime as a part of one session, to support streaming audio applications that are not driven via the microphone.
The link in the current readme:
https://github.com/watson-developer-cloud/swift-sdk/blob/master/docs/3.0.0-migration-guide.md

is broken.  I believe the link should be to:
https://github.com/watson-developer-cloud/swift-sdk/blob/master/MIGRATION-V3.md

:)
