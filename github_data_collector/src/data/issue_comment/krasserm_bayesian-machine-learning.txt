Thank you very much for this very descriptive and detailed implementation of BNNs. I am trying to implement BNN for a custom function which takes multiple inputs and produces one output. How can I define noise in such a case?
Thank you very much for your implementation. It's highly helpful.

I'm attempting to learn different functions using the BNN model. I restrict the range of training values to be discontinuous. For example, in an attempt to learn a polynomial function (2*x^2 +2), I slightly change the number of neurons and number of layers for better prediction and train the network with equally sampled values from -4 to -1 and from 1 to 4. When testing the network with equally sampled values from -6 to 6, I would expect the Epistemic uncertainty to be high at the range of values that the network has not seen during the training, i.e from -6 to -4, from -1 to 1 and from 4 to 6. 

Observations after testing:
1, The predicted mean is close to the expected test value in the trained range ([-4,-1], [1,4]) as expected
2, The variance of predicted values in the untrained range ([-6,-4], [4,6]) is high as expected

3, The variance of predicted values in the untrained range ([-1,1]), is lower which is contradictory to the concept of determining epistemic uncertainty through BNN.   Could there be any explanation for this and Is there some way to make the model to have high variance in all the ranges of untrained data? 

![Polynomial function](https://user-images.githubusercontent.com/17695906/68542668-682a8a80-03af-11ea-8259-352503e165c9.png)
  

I have also done trained the exact model you have presented for 'Sin()' function with discontinuous range of training data and have observed the same issue
![Sin_incomplete](https://user-images.githubusercontent.com/17695906/68543086-f2282280-03b2-11ea-8918-20d46961a873.png)

Thanks a lot in advance !!
 
Just a quick introduction - [PlaidML](https://github.com/plaidml/plaidml) is a backend that can be used to allow GPU-based learning on different hardware (in my case, a Mac with an AMD GPU). If I use PlaidML as a backend: 

```
import os
os.environ["KERAS_BACKEND"] = "plaidml.keras.backend"
from keras import backend as K
from keras import activations, ini
```

And then attempt to use the the DenseVariational layer:

```
class DenseVariational(Layer):
    def __init__(self, output_dim, kl_loss_weight, activation=None, **kwargs):
        self.output_dim = output_dim
        self.kl_loss_weight = kl_loss_weight
        self.activation = activations.get(activation)
        super().__init__(**kwargs)
        
    def build(self, input_shape):
        self._trainable_weights.append(prior_params)
        self.kernel_mu = self.add_weight(name='kernel_mu', shape=(input_shape[1], self.output_dim), initializer=initializers.normal(stddev=prior_sigma), trainable=True)
        self.bias_mu = self.add_weight(name='bias_mu', shape=(self.output_dim,), initializer=initializers.normal(stddev=prior_sigma), trainable=True)
        self.kernel_rho = self.add_weight(name='kernel_rho', shape=(input_shape[1], self.output_dim), initializer=initializers.constant(0.0), trainable=True)
        self.bias_rho = self.add_weight(name='bias_rho', shape=(self.output_dim,), initializer=initializers.constant(0.0), trainable=True)
        super().build(input_shape)
        
    def call(self, x):
        kernel_sigma = tf.math.softplus(self.kernel_rho)
        kernel = self.kernel_mu + kernel_sigma * tf.random.normal(self.kernel_mu.shape)
        bias_sigma = tf.math.softplus(self.bias_rho)
        bias = self.bias_mu + bias_sigma * tf.random.normal(self.bias_mu.shape)
        self.add_loss(self.kl_loss(kernel, self.kernel_mu, kernel_sigma) + self.kl_loss(bias, self.bias_mu, bias_sigma))
        return self.activation(K.dot(x, kernel) + bias)
        
    def compute_output_shape(self, input_shape):
        return (input_shape[0], self.output_dim)
        
    def kl_loss(self, w, mu, sigma):
        variational_dist = tf.distributions.Normal(mu, sigma)
        return self.kl_loss_weight * K.sum(variational_dist.log_prob(w) - log_mixture_prior_prob(w))
```

I get the following error when constructing a sequential model:

>  model.add(DenseVariational(1, kl_loss_weight=kl_loss_weight, activation='sigmoid'))
> Traceback (most recent call last):
>   File "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py", line 541, in make_tensor_proto
>     str_values = [compat.as_bytes(x) for x in proto_values]
>   File "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py", line 541, in <listcomp>
>     str_values = [compat.as_bytes(x) for x in proto_values]
>   File "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/util/compat.py", line 71, in as_bytes
>     (bytes_or_text,))
> TypeError: Expected binary or unicode string, got <tile.Value dense_variational_1/dense_variational_2/dense_variational_3/dense_variational_4/dense_variational_5/dense_variational_6/dense_variational_7/dense_variational_8/dense_variational_9/dense_variational_10/dense_variational_11/dense_variational_12/dense_variational_13/dense_variational_14/dense_variational_15/dense_variational_16/dense_variational_17/dense_variational_18/dense_variational_19/dense_variational_20/dense_variational_21/dense_variational_22/dense_variational_23/dense_variational_24/dense_variational_25/dense_variational_26/dense_variational_27/dense_variational_28/dense_variational_29/kernel_rho Tensor FLOAT32(512, 1)>
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File "<stdin>", line 1, in <module>
>   File "/usr/local/lib/python3.7/site-packages/keras/engine/sequential.py", line 181, in add
>     output_tensor = layer(self.outputs[0])
>   File "/usr/local/lib/python3.7/site-packages/keras/engine/base_layer.py", line 457, in __call__
>     output = self.call(inputs, **kwargs)
>   File "<stdin>", line 17, in call
>   File "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_nn_ops.py", line 11532, in softplus
>     "Softplus", features=features, name=name)
>   File "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py", line 531, in _apply_op_helper
>     raise err
>   File "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py", line 528, in _apply_op_helper
>     preferred_dtype=default_dtype)
>   File "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 1297, in internal_convert_to_tensor
>     ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
>   File "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py", line 286, in _constant_tensor_conversion_function
>     return constant(v, dtype=dtype, name=name)
>   File "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py", line 227, in constant
>     allow_broadcast=True)
>   File "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py", line 265, in _constant_impl
>     allow_broadcast=allow_broadcast))
>   File "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py", line 545, in make_tensor_proto
>     "supported type." % (type(values), values))
> TypeError: Failed to convert object of type <class 'plaidml.tile.Value'> to Tensor. Contents: dense_variational_1/dense_variational_2/dense_variational_3/dense_variational_4/dense_variational_5/dense_variational_6/dense_variational_7/dense_variational_8/dense_variational_9/dense_variational_10/dense_variational_11/dense_variational_12/dense_variational_13/dense_variational_14/dense_variational_15/dense_variational_16/dense_variational_17/dense_variational_18/dense_variational_19/dense_variational_20/dense_variational_21/dense_variational_22/dense_variational_23/dense_variational_24/dense_variational_25/dense_variational_26/dense_variational_27/dense_variational_28/dense_variational_29/kernel_rho Tensor FLOAT32(512, 1). Consider casting elements to a supported type.

This may just come with the package if the underlying issue is similar to [here](https://github.com/plaidml/plaidml/issues/122), but is there any way to make these layers function without tying them so close to Tensorflow directly? 
Hi, 

Thanks a lot for sharing your code on Bayesian NN, it sure was very useful. However, when experimenting on regression problems > 2 variables and classification problems, the model does not seem to learn anything. The loss pretty much remains constant after a few iterations. This is especially true for Relu activation. To show this, I am attaching a zip file where, when you run the file example_regression.py, you get the following plot. 

![tmp](https://user-images.githubusercontent.com/2194256/64827245-4e6bf300-d578-11e9-9f2a-5835161004c1.png)
[BayesianNN_Problem.zip] (https://github.com/krasserm/bayesian-machine-learning/files/3607763/BayesianNN_Problem.zip)

As you can see, the model does not learn the shape when we have 2 features. This actually changes, if I change the activation from ‘relu’ to ‘tanh’! any reason why? 

Additionally, I also tried your model for classifying fashion mnist data, and unfortunately, the model does not learn anything here and simply produces an accuracy of 10%

I am not sure as to what is wrong here. Any help is greatly appreciated!

Thanks
Vineeth

I just want to say thanks for this repo! Great explanations for an interesting topic!