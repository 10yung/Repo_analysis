<!--
Thank you for reporting an issue.

*IMPORTANT* -  *before* creating a new issue please look around:
 - Borgbackup documentation: http://borgbackup.readthedocs.io/en/stable/index.html
 - FAQ: https://borgbackup.readthedocs.io/en/stable/faq.html
 and
 - open issues in Github tracker: https://github.com/borgbackup/borg/issues
  
If you cannot find a similar problem, then create a new issue.

Please fill in as much of the template as possible.
-->

## Have you checked borgbackup docs, FAQ, and open Github issues?

Yes

## Is this a BUG / ISSUE report or a QUESTION?

QUESTION

## System information. For client/server mode post info for both machines.

#### Your borg version (borg -V).

borg 1.1.10

## Question

I have a client that backups to a remote location (server) in append only mode (`borg serve` is run with append only flag). This means prunes do not actually free disk space, everything is fine.

Now on the server I can access that repository, too, and see the transaction log. How can I apply the transaction log so files are actually deleted, disk space is freed etc.? Is there a command for this? Something like "everything what the client requested in append only mode is fine, let's commit it now"?

## Have you checked borgbackup docs, FAQ, and open Github issues?

Yes

## Is this a BUG / ISSUE report or a QUESTION?

BUG

## System information. For client/server mode post info for both machines.

#### Your borg version (borg -V).
1.1.9

#### Operating system (distribution) and version.
Raspbian
#### Hardware / network configuration, and filesystems used.
raspberry pi
#### How much data is handled by borg?
lots
#### Full borg commandline that lead to the problem (leave away excludes and passwords)
borg prune -v --list --stats --dry-run --keep-last=5 --keep-daily=7 --keep-weekly=26 --keep-yearly=-1 ssh://pi@192.168.1.21/media/piRaidDrive/Borg/octoPi

## Describe the problem you're observing.
Message :
usage: borg [-h] [-V] <command> ...
borg: error: unrecognized arguments: --keep-last=5

#### Can you reproduce the problem? If so, describe how. If not, describe troubleshooting steps you took before opening the issue.
Yes I can reproduce the problem with keep-last, keep-secondly and keep-minutely. It begins to work with keep-hourly.

The same command, launched locally on the server
borg prune -v --list --stats --dry-run --keep-last=5 --keep-daily=7 --keep-weekly=26 --keep-yearly=-1 /media/piRaidDrive/Borg/octoPi
works .
The server and the client have the same borg version


#### Include any warning/errors/backtraces from the system logs

None

<!--
Thank you for reporting an issue.

*IMPORTANT* -  *before* creating a new issue please look around:
 - Borgbackup documentation: http://borgbackup.readthedocs.io/en/stable/index.html
 - FAQ: https://borgbackup.readthedocs.io/en/stable/faq.html
 and
 - open issues in Github tracker: https://github.com/borgbackup/borg/issues
  
If you cannot find a similar problem, then create a new issue.

Please fill in as much of the template as possible.
-->

## Have you checked borgbackup docs, FAQ, and open Github issues?

Yes. And I discussed it in borgbackup@python.org (&ldquo;Isn't locking broken, because stale lock removal doesn't comply with the locking protocol?&rdquo;) with the confirmation that the issue most probably exists.

## Is this a BUG&nbsp;/ ISSUE report or a QUESTION?

BUG.

I did not (yet) observe any event resulting from it, and I don't know if anybody did. However, the risk of repository damage definitely exists.

Furthermore, the requirement for the filesystem on which the repository resides,<sup><a name="cite-1"/>[1](#user-content-footnote-1)</sup>

> mkdir(2) should be atomic, since it is used for locking

is useless because of this bug.

## System information. For client/server mode post info for both machines.

The bug is basical and affects all platforms and file systems.

However, what I observed in order to confirm it is how the implementation deals with the file system in order to acquire and to release a lock and to kill stale locks. The information below relates to these observations.

#### Your borg version (borg -V).

1.1.7 and&nbsp;1.1.10

#### Operating system (distribution) and version.

Linux&nbsp;4.15.0-65-generic, Ubuntu&nbsp;18.04.3&nbsp;LTS

#### Hardware&nbsp;/ network configuration, and file systems used.

ext4 (mounted rw,relatime), tmpfs (mounted rw,nosuid,nodev,relatime)

#### How much data is handled by borg?

empty and almost empty repository

#### Full borg commandline that lead to the problem (leave away excludes and passwords)

The exact commandline doesn't matter. I tried the commands `borg list` and `borg mount` as processes creating a shared lock, and `borg with-lock` and `borg create` as processes creating an exclusive lock. See also [Can you reproduce the problem?&nbsp;&hellip;](#reproduce).

## Describe the problem you're observing.

The removal of stale locks violates the locking procedure as described in the documentation,<sup><a name="cite-2"/>[2](#user-content-footnote-2)</sup> <sup><a name="cite-3"/>[3](#user-content-footnote-3)</sup>

> If the process can create the lock.exclusive directory for a resource, it has the lock for it. If creation fails (because the directory has already been created by some other process), lock acquisition fails.

Let's assume two concurrent borg processes, _A_ and _B_. _A_ gets the lock. _B_ sees the lock.exclusive directory, thinking it be stale, removes it and recreates it, now thinking it owns the lock, too.

Apparently, in order to prevent this situation,

> Inside the lock directory, there is a file indicating hostname, process id and thread id of the lock holder.

So it seems that another process would be able to reliably check whether the process which holds the lock is alive or dead. But this is not the case, because creating the lock.exclusive directory and creating this process identifier is not an atomic operation. There is still a chance that a concurrent process sees an empty (hence an apparently stale) directory, i.e. after directory creation and before process identifier creation. So the requirement of atomicity of mkdir (I would call it consistency, what is needed here) is useless&mdash;even when the lock initially doesn't exist.

### <a name="reproduce"/>Can you reproduce the problem? If so, describe how. If not, describe troubleshooting steps you took before opening the issue.

The bug can be seen like this, for example:

```
$ export BORG_HOST_ID=hostname@1234567890
$ borg init -e none testrepo
$ mkdir testrepo/lock.exclusive
$ touch testrepo/lock.exclusive/$BORG_HOST_ID.12345-0
$ strace -e trace=%file -f -o borg.log borg list testrepo
Killed stale lock hostname@1234567890.12345-0.
```

A stale lock (same host id, non-existent process id) is created in a repository. Then `borg list` is called, wrapped by a `strace` command. In the `strace` output (borg.log) one finds the following system calls dealing with file system I/O:

```
...
1  1525  stat("/home/user/testrepo", {st_mode=S_IFDIR|0700, st_size=4096, ...}) = 0
2  1525  mkdir("/home/user/testrepo/lock.exclusive", 0777) = -1 EEXIST (File exists)
3  1525  stat("/home/user/testrepo/lock.exclusive/hostname@1234567890.1525-0", 0x7ffdab3fc670) = -1 ENOENT (No such file or directory)
4  1525  openat(AT_FDCWD, "/home/user/testrepo/lock.exclusive", O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 4
5  1525  unlink("/home/user/testrepo/lock.exclusive/hostname@1234567890.12345-0") = 0
6  1525  rmdir("/home/user/testrepo/lock.exclusive") = 0
7  1525  mkdir("/home/user/testrepo/lock.exclusive", 0777) = 0
8  1525  openat(AT_FDCWD, "/home/user/testrepo/lock.exclusive/hostname@1234567890.1525-0", O_WRONLY|O_CREAT|O_TRUNC|O_CLOEXEC, 0666) = 4
...
```

In line&nbsp;2, borg tries to create the lock.exclusive directory&mdash;unsuccessfully, because the lock.exclusive directory already exists. Then borg looks if its own process identifier is in the directory, I don't know why. However, the stale lock removal can be observed in lines 4&ndash;6: The directory is read and the stale process identifier is found (line&nbsp;4). After having checked that there is no process&nbsp;12345, the process identifier is deleted in line&nbsp;5. It is finished with the removal of the lock.exclusive directory in line&nbsp;6. In line&nbsp;7, the lock acquisition is tried again&mdash;this time successfully. After that, the own process identifier is created. This confirms the documented and assumed logic.

Now, if a concurrent process would make its lock acquisition between line&nbsp;5 and&nbsp;6 or between line&nbsp;7 and&nbsp;8, both processes would think they have the lock.

## Suggested approach: _&ldquo;renaming strategy&rdquo;_

In order to eliminate that chance and to make the lock acquisition a safe operation as a whole, one can create a randomly named temporary directory within the repo directory, then create the process identifier within this temporary directory (this is not atomic, but isolated and without any race condition), and then rename the temporary directory to &ldquo;lock.exclusive.&rdquo; The locking protocol has to be changed to this renaming being successful.

The stale lock removal procedure can then remain as it is, and it can be applied when the lock acquisition fails, as already implemented.

### Requirements

For this strategy in order to work, it is required that the renaming of a directory to &ldquo;lock.exclusive&rdquo; is only successful if &ldquo;lock.exclusive&rdquo; did not exist before or was empty. So if multiple concurrent processes call this at a time, only that call may be signaled success whose source directory becomes the destination directory, and all the other consistencies are required which are taken as granted such as that the inode of the directory and its contents don't change during the renaming. This is the requirement which replaces the requirement [1](#user-content-footnote-1).

(The atomicity of the renaming operation such as described in `rename`<sup><a name="cite-4"/>[4](#user-content-footnote-4)</sup> of Unix-like platforms is not required: If the lock.exclusive directory existed before but was empty, we have no problem with any time gap during which &ldquo;lock.exclusive&rdquo; does not exist.)

### Which Python function can be used?

With regard to (almost) POSIX compliant operating systems, the system call `rename` is the candidate of choice. It will only succeed if the name did not exist before or was an empty directory.

With regard to Windows, the system call `MoveFile`<sup><a name="cite-5"/>[5](#user-content-footnote-5)</sup> or `MoveFileX` is the candidate of choice. It will only succeed if the name did not exist before.

As far as I can see, the Python function `os.rename(src, dst,*, src_dir_fd=None, dst_dir_fd=None)`<sup><a name="cite-6"/>[6](#user-content-footnote-6)</sup> is the one which has to be used for the renaming strategy, because it uses `rename` on Unix like platforms and `MoveFileX` on Windows. And what is even more important: Its behaviour is specified exactly as stated here.

### Comparison: Is renaming a directory as consistent as creation of a directory concerning the OS system calls, or could the situation get worse by changing the locking strategy to renaming?

I tried to prepare a comparison beyond the specification level, i.e. looking at real implementations, but I couldn't, sorry. I looked at the glibc source code, where the `rename` method is implemented, together with a colleage. As soon as I find crucial details, I will catch up with it here.

However, the situation cannot be made worse in effect by replacing the current strategy with the renaming strategy, because the current strategy is not safe already. But it would be a waste of time if the change wouldn't be an improvement. So we have to require that `os.rename(src, dst, *, src_dir_fd=None, dst_dir_fd=None)` works as specified to the same extent as directory creation, since this is what the bug will fix.

Can we rely on that?&mdash;No, of course not. There is no way of guaranteeing that, already because of the big number of the supporting platforms. But we _should_ rely on that. Reasons:

- The two functions `mkdir` and `rename` are part of the same API (on Windows, it's the same situation), do similar things, and use the same functionality themselves (e.g. `link`&nbsp;/ `unlink` on Unix like systems), so we can expect them to be treated with the same attention.
- This API is very, very old and mature. A colleague sent the Win32.hlp file (Footnote&nbsp;[5](#user-content-footnote-5) shows a screenshot of a page of it) to me which is a documentation of the Windows system calls. The file is a quarter of a century old. Concerning our question, it didn't change since then. I found a man page of `rename`, basically saying the same as the current one, written in the last millenium. This API stems from the seventies.
- The old-fashioned way of implementing a lock file protocol or similar things by this write-replace&nbsp;/ rename technique which makes use of the atomicity&nbsp;/ consistency of one or the other function is a common and well-known pattern.<sup><a name="cite-7"/>[7](#user-content-footnote-7)</sup> Prominent applications rely on it.<sup><a name="cite-8"/>[8](#user-content-footnote-8)</sup>
- It would be considered a very serious bug if one could delete a non-empty directory by `mkdir` or `rename`.
- It would be a bug of the operating system. At the moment, it is Borg which has the bug.

#### Some remarks about network file systems

To be added&hellip;

&#x1f596; Peace and long life!

-------
<a name="footnote-1"/><sup>1</sup> [&uarr;](#user-content-cite-1)&ensp;https://borgbackup.readthedocs.io/en/1.1.10/usage/general.html#file-systems
<a name="footnote-2"/><sup>2</sup> [&uarr;](#user-content-cite-2)&ensp;I will call it &ldquo;locking protocol&rdquo; here, because it is a protocol, irrespective of whether it is intended to be a public protocol or not.
<a name="footnote-3"/><sup>3</sup> [&uarr;](#user-content-cite-3)&ensp;https://borgbackup.readthedocs.io/en/1.1.10/internals/data-structures.html#lock-files
<a name="footnote-4"/><sup>4</sup> [&uarr;](#user-content-cite-4)&ensp;for example, see https://linux.die.net/man/2/rename
<a name="footnote-5"/><sup>5</sup> [&uarr;](#user-content-cite-5)&ensp;![MoveFile](https://user-images.githubusercontent.com/59793129/72566918-0574d400-38b5-11ea-9808-7853291851ac.png)
<a name="footnote-6"/><sup>6</sup> [&uarr;](#user-content-cite-6)&ensp;http://docs.python.org/library/os.html#os.rename
<a name="footnote-7"/><sup>7</sup> [&uarr;](#user-content-cite-7)&ensp;for example, see http://stupidpythonideas.blogspot.com/2014/07/getting-atomic-writes-right.html or https://yakking.branchable.com/posts/atomic-file-creation-tmpfile/ or https://blog.gocept.com/2013/07/15/reliable-file-updates-with-python/
<a name="footnote-8"/><sup>8</sup> [&uarr;](#user-content-cite-8)&ensp;I remember that I learned this technique from qmail.
## Have you checked borgbackup docs, FAQ, and open Github issues?

Yes

## Is this a BUG / ISSUE report or a QUESTION?

Probably a bug.

## System information. For client/server mode post info for both machines.

#### Your borg version (borg -V).

`1.1.9`, repository was initially create with version `1.0.9`.

#### Operating system (distribution) and version.

Debian stretch.

#### Hardware / network configuration, and filesystems used.

Local directory on ZFS. Hardware is a crummy multi-slot USB disk enclosure, but ZFS shows 0 I/O errors on the pool.

#### How much data is handled by borg?

2 to 2.5 TiB.

## Describe the problem you're observing.

_Backstory_: I have a repository which was originally created using Borg `1.0.9` and most data in the repository has been inserted using that version. Then, for some reason, the chunk index was lost (when running `borg create` it started rebuilding it). Knowing that I'm using an older version and that this might be an issue which has already been solved, I upgraded to version `1.1.9` (from the official Debian repository `stretch-backports`) and let it run again and rebuild the chunk index. All output shown below are from version `1.1.9`.

Now the repository is in a state where `borg create` fails with `No such file or directory: '/srv/backup/apu.borg/data/42/425105'` (see full log below). This is the full output of the first time the error appeared. It beging by finishing rebuilding the chunk index and then terminated with the error:

```
jan 12 22:49:20 apu borg-backup.sh[20261]: Creating archive at "/srv/backup/apu.borg::{now:%Y-%m-%d-%H%M%S}"             
jan 12 23:00:49 apu borg-backup.sh[20261]: Synchronizing chunks cache...                                                       
jan 12 23:00:51 apu borg-backup.sh[20261]: Archives: 783, w/ cached Idx: 66, w/ outdated Idx: 0, w/o cached Idx: 717.    
jan 12 23:00:51 apu borg-backup.sh[20261]: Reading cached archive chunk index for 2017-11-26-065635 ...                        
jan 12 23:01:03 apu borg-backup.sh[20261]: Merging into master chunks index ...                         
[...]
jan 13 07:25:26 apu borg-backup.sh[20261]: Fetching and building archive index for 2018-09-08-071309 ...                                                          
jan 13 07:25:44 apu borg-backup.sh[20261]: Merging into master chunks index ...                                                                                                            
jan 13 07:25:44 apu borg-backup.sh[20261]: Fetching and building archive index for 2019-09-05-082930 ...                                                          
jan 13 07:25:56 apu borg-backup.sh[20261]: Merging into master chunks index ...                                                                                                            
jan 13 07:25:57 apu borg-backup.sh[20261]: Reading cached archive chunk index for 2018-11-12-081615 ...                                                                                    
jan 13 07:25:57 apu borg-backup.sh[20261]: Merging into master chunks index ...                                                                                                                
jan 13 07:25:58 apu borg-backup.sh[20261]: Fetching and building archive index for 2018-11-16-072405 ...                                                          
jan 13 07:26:18 apu borg-backup.sh[20261]: Merging into master chunks index ...                                                                                                            
jan 13 07:26:18 apu borg-backup.sh[20261]: Done.                                                        
jan 13 07:29:36 apu borg-backup.sh[20261]: Local Exception                                                                     
jan 13 07:29:36 apu borg-backup.sh[20261]: Traceback (most recent call last):                                  
jan 13 07:29:36 apu borg-backup.sh[20261]:   File "/usr/lib/python3/dist-packages/borg/archiver.py", line 4455, in main
jan 13 07:29:36 apu borg-backup.sh[20261]:     exit_code = archiver.run(args)                           
jan 13 07:29:36 apu borg-backup.sh[20261]:   File "/usr/lib/python3/dist-packages/borg/archiver.py", line 4387, in run                                            
jan 13 07:29:36 apu borg-backup.sh[20261]:     return set_ec(func(args))                                                                                                                   
jan 13 07:29:36 apu borg-backup.sh[20261]:   File "/usr/lib/python3/dist-packages/borg/archiver.py", line 154, in wrapper                                         
jan 13 07:29:36 apu borg-backup.sh[20261]:     return method(self, args, repository=repository, **kwargs)                                                         
jan 13 07:29:36 apu borg-backup.sh[20261]:   File "/usr/lib/python3/dist-packages/borg/archiver.py", line 557, in do_create                                                                
jan 13 07:29:36 apu borg-backup.sh[20261]:     create_inner(archive, cache)                                                                                                                
jan 13 07:29:36 apu borg-backup.sh[20261]:   File "/usr/lib/python3/dist-packages/borg/archiver.py", line 520, in create_inner                                                                                                                                                                                                                                                                                                           
jan 13 07:29:36 apu borg-backup.sh[20261]:     read_special=args.read_special, dry_run=dry_run, st=st)                                                                                                                                                                                                                                                                                                                                   
jan 13 07:29:36 apu borg-backup.sh[20261]:   File "/usr/lib/python3/dist-packages/borg/archiver.py", line 634, in _process                                                                                                      
jan 13 07:29:36 apu borg-backup.sh[20261]:     read_special=read_special, dry_run=dry_run)                                                                                                 
jan 13 07:29:36 apu borg-backup.sh[20261]:   File "/usr/lib/python3/dist-packages/borg/archiver.py", line 634, in _process                                                                                                                                                                                                                                                                                                               
jan 13 07:29:36 apu borg-backup.sh[20261]:     read_special=read_special, dry_run=dry_run)                                                                                                                                                                                                                                                                                                                                               
jan 13 07:29:36 apu borg-backup.sh[20261]:   File "/usr/lib/python3/dist-packages/borg/archiver.py", line 634, in _process                                                                                                      
jan 13 07:29:36 apu borg-backup.sh[20261]:     read_special=read_special, dry_run=dry_run)                                                                        
jan 13 07:29:36 apu borg-backup.sh[20261]:   File "/usr/lib/python3/dist-packages/borg/archiver.py", line 634, in _process
jan 13 07:29:36 apu borg-backup.sh[20261]:     read_special=read_special, dry_run=dry_run)                                                                                                 
jan 13 07:29:36 apu borg-backup.sh[20261]:   File "/usr/lib/python3/dist-packages/borg/archiver.py", line 634, in _process                                                                 
jan 13 07:29:36 apu borg-backup.sh[20261]:     read_special=read_special, dry_run=dry_run)              
jan 13 07:29:36 apu borg-backup.sh[20261]:   File "/usr/lib/python3/dist-packages/borg/archiver.py", line 634, in _process 
jan 13 07:29:36 apu borg-backup.sh[20261]:     read_special=read_special, dry_run=dry_run)                                                                        
jan 13 07:29:36 apu borg-backup.sh[20261]:   File "/usr/lib/python3/dist-packages/borg/archiver.py", line 634, in _process                                                                 
jan 13 07:29:36 apu borg-backup.sh[20261]:     read_special=read_special, dry_run=dry_run)              
jan 13 07:29:36 apu borg-backup.sh[20261]:   File "/usr/lib/python3/dist-packages/borg/archiver.py", line 608, in _process
jan 13 07:29:36 apu borg-backup.sh[20261]:     status = archive.process_file(path, st, cache)                            
jan 13 07:29:36 apu borg-backup.sh[20261]:   File "/usr/lib/python3/dist-packages/borg/archive.py", line 1034, in process_file                                    
jan 13 07:29:36 apu borg-backup.sh[20261]:     self.chunk_file(item, cache, self.stats, backup_io_iter(self.chunker.chunkify(fd, fh)))                                                                                                                                                                                                                                                                                                   
jan 13 07:29:36 apu borg-backup.sh[20261]:   File "/usr/lib/python3/dist-packages/borg/archive.py", line 962, in chunk_file                                                                                                                                                                                                                                                                                                              
jan 13 07:29:36 apu borg-backup.sh[20261]:     item.chunks.append(chunk_processor(data))                                                                                                                                                                                                                                                  
jan 13 07:29:36 apu borg-backup.sh[20261]:   File "/usr/lib/python3/dist-packages/borg/archive.py", line 950, in chunk_processor                                  
jan 13 07:29:36 apu borg-backup.sh[20261]:     chunk_entry = cache.add_chunk(self.key.id_hash(data), data, stats, wait=False)                                                              
jan 13 07:29:36 apu borg-backup.sh[20261]:   File "/usr/lib/python3/dist-packages/borg/cache.py", line 899, in add_chunk
jan 13 07:29:36 apu borg-backup.sh[20261]:     self.repository.put(id, data, wait=wait)                                  
jan 13 07:29:36 apu borg-backup.sh[20261]:   File "/usr/lib/python3/dist-packages/borg/repository.py", line 1088, in put       
jan 13 07:29:36 apu borg-backup.sh[20261]:     self.prepare_txn(self.get_transaction_id())                               
jan 13 07:29:36 apu borg-backup.sh[20261]:   File "/usr/lib/python3/dist-packages/borg/repository.py", line 533, in prepare_txn
jan 13 07:29:36 apu borg-backup.sh[20261]:     self._rebuild_sparse(segment)                            
jan 13 07:29:36 apu borg-backup.sh[20261]:   File "/usr/lib/python3/dist-packages/borg/repository.py", line 863, in _rebuild_sparse
jan 13 07:29:36 apu borg-backup.sh[20261]:     self.compact[segment] += self.io.segment_size(segment)                    
jan 13 07:29:36 apu borg-backup.sh[20261]:   File "/usr/lib/python3/dist-packages/borg/repository.py", line 1324, in segment_size
jan 13 07:29:36 apu borg-backup.sh[20261]:     return os.path.getsize(self.segment_filename(segment))                    
jan 13 07:29:36 apu borg-backup.sh[20261]:   File "/usr/lib/python3.5/genericpath.py", line 50, in getsize               
jan 13 07:29:36 apu borg-backup.sh[20261]:     return os.stat(filename).st_size                                          
jan 13 07:29:36 apu borg-backup.sh[20261]: FileNotFoundError: [Errno 2] No such file or directory: '/srv/backup/apu.borg/data/42/425105'
jan 13 07:29:36 apu borg-backup.sh[20261]: Platform: Linux apu 4.9.0-11-amd64 #1 SMP Debian 4.9.189-3+deb9u2 (2019-11-11)x86_64
jan 13 07:29:36 apu borg-backup.sh[20261]: Linux: debian 9.11                                                            
jan 13 07:29:36 apu borg-backup.sh[20261]: Borg: 1.1.9  Python: CPython 3.5.3                                                                                                              
jan 13 07:29:36 apu borg-backup.sh[20261]: PID: 20263  CWD: /
jan 13 07:29:36 apu borg-backup.sh[20261]: sys.argv: ['/usr/bin/borg', 'create', '--verbose', '--stats', '--exclude', '/proc', '--exclude', '/sys', '--exclude', '/dev', '--exclude', '/tmp', '--exclude', '/run', '--exclude', '/mnt', '--exclude', '/media', '--exclude', '/var/swap,', '--exclude', '/var/lib/docker', '--exclude', '/srv/backup', '--exclude', '/root/.cache', '--exclude', '/home/*/.cache', '--exclude', '/home/mic
hi/transmission', '/srv/backup/apu.borg::{now:%Y-%m-%d-%H%M%S}', '/']
jan 13 07:29:36 apu borg-backup.sh[20261]: SSH_ORIGINAL_COMMAND: None
```

The missing file does indeed not exists:

```
$ ls -d /srv/backup/apu.borg/data/42/425105                           
ls: cannot access '/srv/backup/apu.borg/data/42/425105': No such file or directory
```

But I have hourly snapshots of the filesystem containing the repository and the file did exist at some point in the past but was somehow deleted, I assume by Borg itself. So I can easily restore this specific file:

```
root@apu ~$ ls /srv/backup/.zfs/snapshot/*/apu.borg/data/42/425105
/srv/backup/.zfs/snapshot/snappy-2020-01-11-140003/apu.borg/data/42/425105
/srv/backup/.zfs/snapshot/snappy-2020-01-11-150002/apu.borg/data/42/425105
```

To find out whether more files have gone missing, I ran `borg check` (without first restoring the missing file). But to my surprise, it did not find any problems:

```
$ borg --debug check --repository-only /srv/backup/apu.borg/
using builtin fallback logging configuration                                    
35 self tests completed in 0.98 seconds                                         
Starting repository check                                               
Read committed index of transaction 425109                                      
Segment transaction is    425109                                                
Determined transaction is 425109                                                
Found 399578 segments                                                       
Starting repository index check                                                 
Completed repository check, no problems found.                          
```

Running `borg create` after that still produces the same error message as above:

```
$ ./borg-backup.sh
Creating archive at "/srv/backup/apu.borg::{now:%Y-%m-%d-%H%M%S}"
Local Exception
Traceback (most recent call last):
[...]
  File "/usr/lib/python3.5/genericpath.py", line 50, in getsize
    return os.stat(filename).st_size
FileNotFoundError: [Errno 2] No such file or directory: '/srv/backup/apu.borg/data/42/425105'

Platform: Linux apu 4.9.0-11-amd64 #1 SMP Debian 4.9.189-3+deb9u2 (2019-11-11) x86_64
Linux: debian 9.11
Borg: 1.1.9  Python: CPython 3.5.3
PID: 27195  CWD: /root
sys.argv: ['/usr/bin/borg', 'create', '--verbose', '--stats', '--exclude', '/proc', '--exclude', '/sys', '--exclude', '/dev', '--exclude', '/tmp', '--exclude','/run', '--exclude', '/mnt', '--exclude', '/media', '--exclude', '/var/swap,', '--exclude', '/var/lib/docker', '--exclude', '/srv/backup', '--exclude', '/root/.cache', '--exclude', '/home/*/.cache', '--exclude', '/home/michi/transmission','/srv/backup/apu.borg::{now:%Y-%m-%d-%H%M%S}', '/']
SSH_ORIGINAL_COMMAND: None
```

Should I not have used `--repository-only` with `borg check`? I feared that it would take even longer, the `borg check` command above ran for about 40 h.

I would like to know how to either fix the repository (e.g. know which files are missing and that the existing ones are in good shape) or know that it can't be fixed (because there are files missing I can't restore).

#### Can you reproduce the problem? If so, describe how. If not, describe troubleshooting steps you took before opening the issue.

I'm not sure how to reproduce this. The problem seems to have been caused by an older Borg version.
Make the buzhash chunker more resistant to fingerprinting by introducing a permutation of the buzhash table, as discussed at https://github.com/borgbackup/borg/issues/3687
- BORG_SECURITY_DIR: Added "Will move with BORG_CONFIG_DIR variable unless specified.".
- BORG_SECURITY_DIR: re-positioned immediately below BORG_CONFIG_DIR
- BORG_CACHE_DIR: moved to precede CONFIG_DIR and SECURITY_DIR.

Took a cue from #4835 

For issue #4788 


From a mailing list post of mine:

[The borg user asking copied existing data to a new filesystem.]

> Should I move to "--files-cache=mtime,size,inode" (Modify) to avoid long initial backup times when I resume my daily backups over ssh?

The problem is that ctime and inode number of all files have changed due to the copying to a new filesystem.

So, what's left is only "size", which [if it is the only criteria used] is rather weak.

If you are absolutely sure that all the files are identical as before (so even a weak "--files-cache=size" would be no problem), you could use that for the first backup.

Note: it is also important that the absolute paths of the files do not change.

https://github.com/borgbackup/borg/blob/1.1.9/src/borg/cache.py#L970

The code there deals with a change of the inode number in case of "cache hits" (read the comment above that line).

It does not deal with the ctime change though, but we could think about whether it makes sense to add some "C" and "M" modes that ignore the ctime/mtime for change detection, but update the cmtime value in the cache to either the current ctime ("C") or the current mtime ("M") of the file. With that, the 2nd backup from the new filesystem could go back to the usual --files-cache=size,ctime,inode without triggering a full backup.

Sadly, without that change and in a situation like yours, you could not enable ctime/mtime for change detection without triggering a full backup, but only --files-cache=inode,size (which also is a bit weak).

This is a question, a request to update the documentation, and a feature request all in one.

**Question:** What happens when `borg create` can't read a file? Does it act as if the file didn't exist?

**Feature request:** Assuming that is indeed what happens, could we have an option that makes borg treat the broken file/folder as if it wasn't modified since the last backup?

**Documentation:** This doesn't seem to be documented anywhere, so the docs should be updated with this information.
## Have you checked borgbackup docs, FAQ, and open Github issues?

Yes

## Is this a BUG / ISSUE report or a QUESTION?

BUG

## System information. For client/server mode post info for both machines.

#### Your borg version (borg -V).

```
$ borg -V
borg 1.2.0a8.dev61+gfc96fc4b
```

#### Operating system (distribution) and version.

Arch Linux

#### Hardware / network configuration, and filesystems used.

- borg host: btrfs RAID5 on HDD
- borg client: btrfs on SSD
- `borg create` over SSH, `borg recreate` locally on host

#### How much data is handled by borg?

```
$ du -hs /mnt/data/Backups/Hosts/able/borg
609G    /mnt/data/Backups/Hosts/able/borg
```

#### Full borg commandline that lead to the problem (leave away excludes and passwords)

```
$ export BORG_PASSCOMMAND="pass misc/borg"
$ export BORG_REPO="/mnt/data/Backups/Hosts/able/borg"
$ borg recreate --exclude-caches --keep-exclude-tags --progress --stats ::able-2019-08-21
```

## Describe the problem you're observing.

Using `borg recreate --exclude-caches` on a single specific archive in my repository raises a KeyError.

#### Can you reproduce the problem? If so, describe how. If not, describe troubleshooting steps you took before opening the issue.

The problem can be reproduced on this specific archive 100% of the time.

#### Include any warning/errors/backtraces from the system logs

```
$ export BORG_PASSCOMMAND="pass misc/borg"
$ export BORG_REPO="/mnt/data/Backups/Hosts/able/borg"
$ borg recreate --exclude-caches --keep-exclude-tags --progress --stats ::able-2019-08-21
recreate is an experimental feature.
Type 'YES' if you understand this and want to continue: YES
Local Exception
Traceback (most recent call last):
  File "/usr/lib/python3.8/site-packages/borg/archiver.py", line 4476, in main
    exit_code = archiver.run(args)
  File "/usr/lib/python3.8/site-packages/borg/archiver.py", line 4407, in run
    return set_ec(func(args))
  File "/usr/lib/python3.8/site-packages/borg/archiver.py", line 165, in wrapper
    return method(self, args, repository=repository, cache=cache_, **kwargs)
  File "/usr/lib/python3.8/site-packages/borg/archiver.py", line 1570, in do_recreate
    if not recreater.recreate(name, args.comment, args.target):
  File "/usr/lib/python3.8/site-packages/borg/archive.py", line 1887, in recreate
    self.matcher_add_tagged_dirs(archive)
  File "/usr/lib/python3.8/site-packages/borg/archive.py", line 2042, in matcher_add_tagged_dirs
    file = open_item(archive, cachedir_masters[item.source])
KeyError: 'arch/var/lib/flatpak/repo/objects/50/7741d5f44a0b8c6d81e907afb1160bb7f3aba3e44779d2bcb0b04a556a70f7.file'

Platform: Linux stratofortress.nexus.i.intelfx.name 5.4.7-arch1-1 #1 SMP PREEMPT Tue, 31 Dec 2019 17:20:16 +0000 x86_64
Linux: Unknown Linux
Borg: 1.2.0a8.dev61+gfc96fc4b  Python: CPython 3.8.1 msgpack: 0.6.2
PID: 2148059  CWD: /mnt/data/Backups/Hosts/able/borg
sys.argv: ['/usr/bin/borg', 'recreate', '--exclude-caches', '--keep-exclude-tags', '--progress', '--stats', '::able-2019-08-21']
SSH_ORIGINAL_COMMAND: None
```

## Additional notes:

Mounting the problematic archive reveals that `arch/var/lib/flatpak/repo/objects/50/7741d5f44a0b8c6d81e907afb1160bb7f3aba3e44779d2bcb0b04a556a70f7.file` contains valid cachedir tag contents, and adding a `print()` to the crash site reveals that this file is a hardlink destination for a properly named cachedir tag:

```
$ borg recreate --exclude-caches --keep-exclude-tags --progress --stats ::able-2019-08-21
recreate is an experimental feature.                                                                               
Type 'YES' if you understand this and want to continue: YES                                                        
XXX: going to crash:
        item.path=arch/var/lib/flatpak/runtime/org.freedesktop.Platform/x86_64/18.08/365d62760000772f8cf401cfb6389252a2c1e2a0ebb39d99302fb730abfd8b74/files/cache/fontconfig/CACHEDIR.TAG
        item.source=arch/var/lib/flatpak/repo/objects/50/7741d5f44a0b8c6d81e907afb1160bb7f3aba3e44779d2bcb0b04a556a70f7.file
Local Exception
<...>
```

As I understand the situation, the crash occurs when a `CACHEDIR.TAG` is a hardlink to another file that is _not_ named `CACHEDIR.TAG` (which can and will happen with deduplicating content-addressable stores such as flatpak, ostree etc).

I have worked around this issue locally by simply skipping tag content verification when the hardlink cannot be resolved, but I don't see a good way to fix this properly without either loading the whole archive contents into the hashmap or making `matcher_add_tagged_dirs()` quadratic.

Any ideas?