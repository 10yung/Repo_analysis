I'm trying to switch to ABBA from optimizely, cause i don't mind writing jquery test code on my own. Still, the main problem is that i'd like to define experiments on server side, not in js code snippet.

Current solution lacks flexibility, because i have to change the code, every time i want to change my test. It's an issue when you work with several clients, and don't controll website code.

I could manage this in Google Tag Manager, but A/B test scripts should load ASAP, and in GTM you cannot controll the order of firing scripts, and all A/B test vendors say, that GTM is not the right tool if you want to change page contents.

So the solution would be to add a parameter to javascript request, and concatenate test code, or request it from standard js.
You'd also have to add possibility to create experiments on the backend, but if now they're created on first request, it should be easy to simply make this request on the backend.

do you think it's possible?

Is there a reason why you didn't specify any versions for the gems in Gemfile? I'm asking, because it can lead to unexpected behaviour if you always retrieve the latest versions of the gems.

Is there a stable gemset with defined versions that you would suggest?

Odd that this happens only when running ABBA as a rack app under NGINX/Passenger, but when I do, I get this error message:

```
Error compiling CSS asset

Encoding::InvalidByteSequenceError: "\xE2" on US-ASCII
    (in /var/code/abba/app/assets/stylesheets/experiment.css.styl)

    (/var/code/abba/vendor/bundle/ruby/1.9.1/gems/execjs-1.4.0/lib/execjs/external_runtime.rb:173:in `read'
```

I understand that issue may be more appropriately answered in the ExecJS or the Stylus project, maybe even Passenger!  I'm not sure exactly which of these projects this is best addressed in.

This doesn't happen when running under thin.

However, by adding this line at the top of the `config.ru` file, the issue is solved for me given my preferred deployment setup:

```
Encoding.default_external = 'UTF-8'
```

Thought I'd open this issue in part to see if others have this problem.

This works with the latest Cedar stack on Heroku.

Thanks

In my head I expected this framework to report unique visitors, not total.  So if the same user refreshes the page multiple times, I'm seeing a visit for each page load whereas I expect to see 1 visit.

I'm still pretty new to AB testing philosophy... am I "doing it wrong" to expect unique visits?  If not, would you accept a patch for that?

Thanks!
--Bob

I was getting an error while deploying to Heroku

ruby: symbol lookup error: /app/vendor/bundle/ruby/2.0.0/gems/eventmachine-1.0.0/lib/rubyeventmachine.so: undefined symbol: rb_enable_interrupt

Given the fact that Ruby is not explicitly specified, it was using 2.0.0.

Got it to work by specifying ruby '1.9.3' in Gemfile

Please update to Ruby 2.0.0

Hi,

I'm trying to extend your chart by a line showing the visitors/converstions over time in order to have a better feeling for the shown values.

The data is already selected but I have very serious problems to get the data into the chart.
I'm new to d3 and maybe I oversee the point where you put the elements into the chart.

Could you please elaborate a little bit how the chart works?

Thanks!!

Add a callback to Abba.complete() that fires when tracking image has loaded.

Use case: you want to record a completion when user clicks something.  In many cases (eg. signup or purchase button) this links to a different page, but you don't want to navigate away until the completion was recorded.

Full disclosure: I authored the [original ABBA library](http://www.thumbtack.com/labs/abba/).

This looks like a great library for people who want to run A/B tests themselves, I haven't seen any other package that takes care of the whole stack and makes it this easy.

However, I'm wary of the statistics here, for a few reasons:
- The package appears to give one-tailed p-values. This is debatable, but I believe that doesn't line up with how most people interpret their A/B tests and thus leads to an excess of false positives.
- The package supports multiple comparisons but includes no correction or warning for it. Granted, very few A/B testing packages address this, but it can also lead to a (potentially serious) excess of false positives.
- The package computes a table of four normal tail probabilities itself using a fairly crude method:
  
  ```
  a         = 50.0
  norm_dist = []
  (0.0..3.1).step(0.01) { |x| norm_dist << [x, a += 1 / Math.sqrt(2 * Math::PI) * Math::E ** (-x ** 2 / 2)] }
  Z_TO_PROBABILITY = [90, 95, 99, 99.9].map { |pct| [norm_dist.find { |x,a| a >= pct }.first, pct] }.reverse
  ```
  
  resulting in
  
  ```
  z-value  computed tail prob  actual tail prob
  2.75     .001                .0030
  2.26     .01                 .0119
  1.63     .05                 .0516
  1.27     .1                  .1020
  ```
  
  For the largest z-value, the computed tail probability is low by a factor of 3, which can be substantial for people trying to run high-confidence tests -- again, it's overestimating confidence which can again lead to excess false positives. It'd be simpler and more accurate to hardcode a lookup table -- tables of these values are [readily available](http://en.wikipedia.org/wiki/Standard_normal_table). (One could also make use of a statistical library or directly code a numerical approximation to the standard normal CDF.)
- The package uses the non-pooled Z-test for two proportions in case where I believe the pooled test is more appropriate (because the null hypothesis is that the control and the variation have the same conversion rate). This tends to overestimate Z-values and thus, again, confidences.
- The Z-test can be inaccurate for small samples, though this doesn't actually contribute much error since the package requires that the total number of conversions is at least 25 which keeps us in pretty safe territory. With highly uneven sampling weights we could get into small-sample trouble, but it's definitely less of an issue.

As an example of the potential for numerical accuracy issues, consider an experiment with 6/500 conversions in baseline and 20/500 conversions in one variation. The package reports 99.9% confidence, or a one-tailed p-value <= 0.001. [Fisher's Exact Test](http://www.graphpad.com/quickcalcs/contingency1.cfm) gives a one-tailed p-value of 0.0042. The [original ABBA](http://www.thumbtack.com/labs/abba/#Baseline=6%2C500&Variation+1=20%2C500&abba%3AintervalConfidenceLevel=0.95&abba%3AuseMultipleTestCorrection=true) gives a two-tailed p-value of 0.0063, corresponding to a one-tailed p-value of roughly 0.0033. So we're underestimating the one-tailed p-value by a factor of 3-4 and the two-tailed p-value (which is probably more appropriate) by a factor of 6-8. This is pretty substantial -- our long-run false-positive rate will be 6-8x higher than we expect (ignoring multiple testing issues).

I think this would be a really awesome contribution to the world of A/B testing with some more robust statistics. I'd suggest using the (original) ABBA JS library (or perhaps a port of the Python version to Ruby), which also gets you some nice confidence intervals on proportions and improvements. Together the two would make a pretty sweet solution to do-it-yourself A/B testing.

Looks like you are using a 1-tailed test to calculate the probability. Is that correct? I see both ways used in different A/B testing tools (Optimzely use a 1-tailed test, I believe).

Curious as to the thinking behind the choice.
