in the docker-hadoop project , i can not build the dockerfile with debian:8 , but i can use debian:9 with the 3.1.3 Dockerfile  to build the hadoop 2.x version. you can upgrade the hadoop 2.x Dockerfile?

ps : in the dockerfile you can add a code line as follow:
RUN ln -s /opt/hadoop-$HADOOP_VERSION /opt/hadoop
then i can see the version in the docker containner.
As title.
I am trying to deploy hadoop on kubernetes using the images build by you. the problem I am facing is that the values of "hadoop.env" that I am passing using configmap not working properly. Means the variable are reflected in hdfs and core sites but the value part are missing. can you please help me to sort this out. I have paste screen dumps below.

Screendumps:

<property><name>dfs.webhdfs.enabled</name><value></value></property>
<property><name>dfs.permissions.enabled</name><value></value></property>
<property><name>dfs.namenode.datanode.registration.ip-hostname-check</name><value></value></property>
<property><name>hadoop.proxyuser.sdc.hosts</name><value></value></property>
<property><name>hadoop.proxyuser.sdc.groups</name><value></value></property>

Thanks in advance.
Hi,
I want to scale the hadoop datanode to 3, I run the command `docker-compose up --scale datanode=3`. But the resulting cluster still has only one datanode. 
 Could you please give me some suggestions on how to run the cluster with multiple datanodes?

Thanks,
I'm completely new to Hadoop, and I found this repo because I had the thought, "Wow, installing Hadoop is hard, and all I want is HDFS. Surely there's got to be an easier way to do this. Maybe someone made a Docker container!"

Indeed, this repo does an amazing job of getting all the complicated details out of the way. But there's a number of questions that are left unanswered after getting this running. Thought it'd be useful to list them here:
 - Why is only the namenode port (9870) forwarded in `docker-compose.yml`? I opened up the other ports (8088, 8042, 9864, and 8188) in the other services and can access all the UIs now.
 - How do I connect other hosts to the one I spun up with `docker-compose up`? It'd be amazing to be able to do something like `docker-compose up` (and maybe another command) on another host and have them connect.
 - How do I get started with uploading data to HDFS? I tried using "Upload" in `http://localhost:9870`->`Utilities->Browse the file system, and it failed. 
 
These questions will probably be answered just by working with Hadoop more, but I thought they could help you guys if you're looking to address the new crowd. Lots of university students, especially those doing data science/engineering, are starting to feel the need to get familiar with tools like this.

Can someone help me troubleshoot the issue I'm having with `make wordcount`?

Running that command yields this error:

> docker run --network docker-hadoop_default --env-file hadoop.env bde2020/hadoop-base: hdfs dfs -mkdir -p /input/
> docker: invalid reference format.
> See 'docker run --help'.
> make: *** [wordcount] Error 125
`make wordcount` failed in my environment with UnknownHostException error.
How can I fix it?

Any help would be appreciated.
Thanks.

```
java.lang.IllegalArgumentException: java.net.UnknownHostException: historyserver
	at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:445)
	at org.apache.hadoop.yarn.util.timeline.TimelineUtils.buildTimelineTokenService(TimelineUtils.java:163)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceInit(YarnClientImpl.java:186)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.serviceInit(ResourceMgrDelegate.java:111)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.<init>(ResourceMgrDelegate.java:105)
	at org.apache.hadoop.mapred.YARNRunner.<init>(YARNRunner.java:152)
	at org.apache.hadoop.mapred.YarnClientProtocolProvider.create(YarnClientProtocolProvider.java:34)
	at org.apache.hadoop.mapreduce.Cluster.initialize(Cluster.java:130)
	at org.apache.hadoop.mapreduce.Cluster.<init>(Cluster.java:109)
	at org.apache.hadoop.mapreduce.Cluster.<init>(Cluster.java:102)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1540)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1536)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapreduce.Job.connect(Job.java:1536)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1564)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)
	at WordCount.main(WordCount.java:59)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
Caused by: java.net.UnknownHostException: historyserver
	... 27 more
```
I  have set up docker swarm cluster , use the following configuration file  to deploy hdfs cluster on the overlay network named test in my swarm cluster .
version: '3'
services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.1.2-java8
    volumes:
      - namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env
    networks:
      - hbase
    ports:
      - 9870:9870
    deploy:
      mode: replicated
      replicas: 1
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.hostname == 10-20-28-111
      labels:
        traefik.docker.network: hbase
        traefik.port: 9870

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.1.2-java8
    volumes:
      - datanode:/hadoop/dfs/data
    env_file:
      - ./hadoop.env
    networks:
      - hbase
    environment:
      SERVICE_PRECONDITION: "namenode:9000"
    deploy:
      restart_policy:
        condition: on-failure
      labels:
        traefik.docker.network: hbase
        traefik.port: 9866
      placement:
        constraints:
          - node.hostname == 10-20-28-111
volumes:
  datanode:
  namenode:

networks:
  hbase:
    external:
      name: test

I can see registed datanode  from namenode web ui after successful deployment,
but the datanode hostname and ip don't match.And i can't access datanode 9866
 port through the datanode ip,but i can access it via  datanode hostname.
Can you tell me why and give me a hand?Thank you!
According to below doc this port should be exposed
https://ambari.apache.org/1.2.5/installing-hadoop-using-ambari/content/reference_chap2_1.html