tispark extensions 写数据不支持sql，不是很方便使用，提供我们给delta 添加sql 方案，供参考。
通过spark extensions ParserInterface 扩展sql语法，
![image](https://user-images.githubusercontent.com/1145830/72117220-d8da2d00-3387-11ea-81e4-10d12204b1ec.png)

```scala
case class DeleteTableCommand(table: TableIdentifier,
                              where: Option[String]) extends RunnableCommand {

  override def run(sparkSession: SparkSession): Seq[Row] = {

    //只能对 Delta Table 执行 Delete 操作
    DeltaUtils.deltaTableCheck(sparkSession, table, "DELETE")

    val deltaTable = DeltaUtils.getDeltaTable(sparkSession, table)
    if (where.isEmpty) {
      deltaTable.delete()
    } else {
      deltaTable.delete(where.get)
    }
    deltaTable.generate("symlink_format_manifest")
    Seq.empty[Row]
  }
}
```

```scala
/**
  * 通过 delta scala api 进行更新
  **/
case class UpdateTableCommand(table: TableIdentifier,
                              upset: Map[String, String],
                              where: Option[String]) extends RunnableCommand {

  override def run(sparkSession: SparkSession): Seq[Row] = {

    //只能对 Delta Table 执行 Update 操作
    DeltaUtils.deltaTableCheck(sparkSession, table, "UPDATE")

    val deltaTable = DeltaUtils.getDeltaTable(sparkSession, table)
    if (where.isEmpty) {
      deltaTable.updateExpr(upset)
    } else {
      deltaTable.updateExpr(where.get, upset)
    }
    deltaTable.generate("symlink_format_manifest")
    Seq.empty[Row]
  }
}
```
BAZEL support has not been maintained for a long time, also it is no longer documented in README, purpose to remove it from this project.

@zhexuany @ilovesoup @marsishandsome 
TiDB/TiKV used a new row format. TiSpark should support it.

RFC: https://github.com/pingcap/tidb/blob/master/docs/design/2018-07-19-row-format.md

Relating PRs:
https://github.com/pingcap/tidb/pull/7597 
https://github.com/pingcap/tidb/pull/12634
通过tispark 把hive 和tidb 表融入在一起，spark sql创建视图，视图sql可能只有tidb表，hive表，或者tidb和hive表同时存在，目前不支持tidb 表创建的视图，读取会报类型错误
support `in` pushdown in tispark (see chspark)
**Describe the bug**
```scala
[2019-12-09T22:27:14.461Z] - Write Empty data
[2019-12-09T22:27:14.716Z] 19/12/10 06:27:14 WARN TaskSetManager: Stage 163 contains a task of very large size (757 KB). The maximum recommended task size is 100 KB.
[2019-12-09T22:27:14.973Z] 19/12/10 06:27:14 WARN TaskSetManager: Stage 165 contains a task of very large size (757 KB). The maximum recommended task size is 100 KB.
[2019-12-09T22:27:15.229Z] 19/12/10 06:27:15 WARN TaskSetManager: Stage 166 contains a task of very large size (757 KB). The maximum recommended task size is 100 KB.
[2019-12-09T22:29:21.625Z] 19/12/10 06:29:19 WARN TaskSetManager: Stage 200 contains a task of very large size (757 KB). The maximum recommended task size is 100 KB.
[2019-12-09T22:29:21.625Z] 19/12/10 06:29:20 WARN TaskSetManager: Stage 202 contains a task of very large size (757 KB). The maximum recommended task size is 100 KB.
[2019-12-09T22:29:21.625Z] 19/12/10 06:29:20 WARN TaskSetManager: Stage 203 contains a task of very large size (757 KB). The maximum recommended task size is 100 KB.
[2019-12-09T22:32:13.483Z] 19/12/10 06:31:59 ERROR DAGIterator: Process region tasks failed, remain 0 tasks not executed due to
[2019-12-09T22:32:13.483Z] com.pingcap.tikv.exception.GrpcException: retry is exhausted.
[2019-12-09T22:32:13.483Z] 	at com.pingcap.tikv.util.ConcreteBackOffer.doBackOff(ConcreteBackOffer.java:142)
[2019-12-09T22:32:13.483Z] 	at com.pingcap.tikv.region.RegionStoreClient.handleCopResponse(RegionStoreClient.java:490)
[2019-12-09T22:32:13.483Z] 	at com.pingcap.tikv.region.RegionStoreClient.coprocess(RegionStoreClient.java:450)
[2019-12-09T22:32:13.483Z] 	at com.pingcap.tikv.operation.iterator.DAGIterator.process(DAGIterator.java:196)
[2019-12-09T22:32:13.483Z] 	at com.pingcap.tikv.operation.iterator.DAGIterator.lambda$submitTasks$1(DAGIterator.java:70)
[2019-12-09T22:32:13.483Z] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2019-12-09T22:32:13.483Z] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2019-12-09T22:32:13.483Z] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2019-12-09T22:32:13.484Z] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2019-12-09T22:32:13.484Z] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2019-12-09T22:32:13.484Z] 	at java.lang.Thread.run(Thread.java:748)
[2019-12-09T22:32:13.484Z] Caused by: com.pingcap.tikv.exception.LockException
[2019-12-09T22:32:13.484Z] 	... 10 more
[2019-12-09T22:32:13.484Z] 19/12/10 06:31:59 ERROR Executor: Exception in task 3.0 in stage 229.0 (TID 1226)
[2019-12-09T22:32:13.484Z] com.pingcap.tikv.exception.TiClientInternalException: Error reading region:
[2019-12-09T22:32:13.484Z] 	at com.pingcap.tikv.operation.iterator.DAGIterator.doReadNextRegionChunks(DAGIterator.java:169)
[2019-12-09T22:32:13.484Z] 	at com.pingcap.tikv.operation.iterator.DAGIterator.readNextRegionChunks(DAGIterator.java:146)
[2019-12-09T22:32:13.484Z] 	at com.pingcap.tikv.operation.iterator.DAGIterator.hasNext(DAGIterator.java:92)
[2019-12-09T22:32:13.484Z] 	at org.apache.spark.sql.tispark.TiRowRDD$$anon$1.hasNext(TiRowRDD.scala:75)
[2019-12-09T22:32:13.484Z] 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.coprocessorrdd_nextBatch_0$(Unknown Source)
[2019-12-09T22:32:13.484Z] 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2019-12-09T22:32:13.484Z] 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2019-12-09T22:32:13.484Z] 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
[2019-12-09T22:32:13.484Z] 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
[2019-12-09T22:32:13.484Z] 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
[2019-12-09T22:32:13.484Z] 	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
[2019-12-09T22:32:13.484Z] 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:306)
[2019-12-09T22:32:13.484Z] 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:304)
[2019-12-09T22:32:13.484Z] 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)
[2019-12-09T22:32:13.484Z] 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)
[2019-12-09T22:32:13.484Z] 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2019-12-09T22:32:13.484Z] 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
[2019-12-09T22:32:13.484Z] 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
[2019-12-09T22:32:13.484Z] 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2019-12-09T22:32:13.484Z] 	at org.apache.spark.scheduler.Task.run(Task.scala:123)
[2019-12-09T22:32:13.484Z] 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
[2019-12-09T22:32:13.484Z] 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
[2019-12-09T22:32:13.484Z] 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
[2019-12-09T22:32:13.484Z] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2019-12-09T22:32:13.484Z] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2019-12-09T22:32:13.484Z] 	at java.lang.Thread.run(Thread.java:748)
[2019-12-09T22:32:13.484Z] Caused by: java.util.concurrent.ExecutionException: com.pingcap.tikv.exception.RegionTaskException: Handle region task failed:
[2019-12-09T22:32:13.484Z] 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[2019-12-09T22:32:13.484Z] 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
[2019-12-09T22:32:13.484Z] 	at com.pingcap.tikv.operation.iterator.DAGIterator.doReadNextRegionChunks(DAGIterator.java:164)
[2019-12-09T22:32:13.484Z] 	... 25 more
[2019-12-09T22:32:13.484Z] Caused by: com.pingcap.tikv.exception.RegionTaskException: Handle region task failed:
[2019-12-09T22:32:13.484Z] 	at com.pingcap.tikv.operation.iterator.DAGIterator.process(DAGIterator.java:209)
[2019-12-09T22:32:13.484Z] 	at com.pingcap.tikv.operation.iterator.DAGIterator.lambda$submitTasks$1(DAGIterator.java:70)
[2019-12-09T22:32:13.484Z] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2019-12-09T22:32:13.484Z] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2019-12-09T22:32:13.484Z] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2019-12-09T22:32:13.484Z] 	... 3 more
[2019-12-09T22:32:13.484Z] Caused by: com.pingcap.tikv.exception.GrpcException: retry is exhausted.
[2019-12-09T22:32:13.484Z] 	at com.pingcap.tikv.util.ConcreteBackOffer.doBackOff(ConcreteBackOffer.java:142)
[2019-12-09T22:32:13.484Z] 	at com.pingcap.tikv.region.RegionStoreClient.handleCopResponse(RegionStoreClient.java:490)
[2019-12-09T22:32:13.484Z] 	at com.pingcap.tikv.region.RegionStoreClient.coprocess(RegionStoreClient.java:450)
[2019-12-09T22:32:13.484Z] 	at com.pingcap.tikv.operation.iterator.DAGIterator.process(DAGIterator.java:196)
[2019-12-09T22:32:13.484Z] 	... 7 more
[2019-12-09T22:32:13.484Z] Caused by: com.pingcap.tikv.exception.LockException
[2019-12-09T22:32:13.484Z] 	... 10 more
[2019-12-09T22:32:13.484Z] 19/12/10 06:31:59 WARN TaskSetManager: Lost task 3.0 in stage 229.0 (TID 1226, localhost, executor driver): com.pingcap.tikv.exception.TiClientInternalException: Error reading region:
[2019-12-09T22:32:13.484Z] 	at com.pingcap.tikv.operation.iterator.DAGIterator.doReadNextRegionChunks(DAGIterator.java:169)
[2019-12-09T22:32:13.484Z] 	at com.pingcap.tikv.operation.iterator.DAGIterator.readNextRegionChunks(DAGIterator.java:146)
[2019-12-09T22:32:13.484Z] 	at com.pingcap.tikv.operation.iterator.DAGIterator.hasNext(DAGIterator.java:92)
[2019-12-09T22:32:13.484Z] 	at org.apache.spark.sql.tispark.TiRowRDD$$anon$1.hasNext(TiRowRDD.scala:75)
[2019-12-09T22:32:13.484Z] 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.coprocessorrdd_nextBatch_0$(Unknown Source)
[2019-12-09T22:32:13.484Z] 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2019-12-09T22:32:13.485Z] 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2019-12-09T22:32:13.485Z] 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
[2019-12-09T22:32:13.485Z] 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
[2019-12-09T22:32:13.485Z] 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
[2019-12-09T22:32:13.485Z] 	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
[2019-12-09T22:32:13.485Z] 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:306)
[2019-12-09T22:32:13.485Z] 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:304)
[2019-12-09T22:32:13.485Z] 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)
[2019-12-09T22:32:13.485Z] 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)
[2019-12-09T22:32:13.485Z] 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2019-12-09T22:32:13.485Z] 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
[2019-12-09T22:32:13.485Z] 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
[2019-12-09T22:32:13.485Z] 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2019-12-09T22:32:13.485Z] 	at org.apache.spark.scheduler.Task.run(Task.scala:123)
[2019-12-09T22:32:13.485Z] 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
[2019-12-09T22:32:13.485Z] 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
[2019-12-09T22:32:13.485Z] 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
[2019-12-09T22:32:13.485Z] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2019-12-09T22:32:13.485Z] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2019-12-09T22:32:13.485Z] 	at java.lang.Thread.run(Thread.java:748)
[2019-12-09T22:32:13.485Z] Caused by: java.util.concurrent.ExecutionException: com.pingcap.tikv.exception.RegionTaskException: Handle region task failed:
[2019-12-09T22:32:13.485Z] 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[2019-12-09T22:32:13.485Z] 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
[2019-12-09T22:32:13.485Z] 	at com.pingcap.tikv.operation.iterator.DAGIterator.doReadNextRegionChunks(DAGIterator.java:164)
[2019-12-09T22:32:13.485Z] 	... 25 more
[2019-12-09T22:32:13.485Z] Caused by: com.pingcap.tikv.exception.RegionTaskException: Handle region task failed:
[2019-12-09T22:32:13.485Z] 	at com.pingcap.tikv.operation.iterator.DAGIterator.process(DAGIterator.java:209)
[2019-12-09T22:32:13.485Z] 	at com.pingcap.tikv.operation.iterator.DAGIterator.lambda$submitTasks$1(DAGIterator.java:70)
[2019-12-09T22:32:13.485Z] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2019-12-09T22:32:13.485Z] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2019-12-09T22:32:13.485Z] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2019-12-09T22:32:13.485Z] 	... 3 more
[2019-12-09T22:32:13.485Z] Caused by: com.pingcap.tikv.exception.GrpcException: retry is exhausted.
[2019-12-09T22:32:13.485Z] 	at com.pingcap.tikv.util.ConcreteBackOffer.doBackOff(ConcreteBackOffer.java:142)
[2019-12-09T22:32:13.485Z] 	at com.pingcap.tikv.region.RegionStoreClient.handleCopResponse(RegionStoreClient.java:490)
[2019-12-09T22:32:13.485Z] 	at com.pingcap.tikv.region.RegionStoreClient.coprocess(RegionStoreClient.java:450)
[2019-12-09T22:32:13.485Z] 	at com.pingcap.tikv.operation.iterator.DAGIterator.process(DAGIterator.java:196)
[2019-12-09T22:32:13.485Z] 	... 7 more
[2019-12-09T22:32:13.485Z] Caused by: com.pingcap.tikv.exception.LockException
[2019-12-09T22:32:13.485Z] 	... 10 more
[2019-12-09T22:32:13.485Z] 
[2019-12-09T22:32:13.485Z] 19/12/10 06:31:59 ERROR TaskSetManager: Task 3 in stage 229.0 failed 1 times; aborting job
[2019-12-09T22:32:13.485Z] 19/12/10 06:31:59 ERROR DAGIterator: Process region tasks failed, remain 0 tasks not executed due to
[2019-12-09T22:32:13.485Z] com.pingcap.tikv.exception.GrpcException: retry is exhausted.
[2019-12-09T22:32:13.485Z] 	at com.pingcap.tikv.util.ConcreteBackOffer.doBackOff(ConcreteBackOffer.java:142)
[2019-12-09T22:32:13.485Z] 	at com.pingcap.tikv.region.RegionStoreClient.handleCopResponse(RegionStoreClient.java:490)
[2019-12-09T22:32:13.485Z] 	at com.pingcap.tikv.region.RegionStoreClient.coprocess(RegionStoreClient.java:450)
[2019-12-09T22:32:13.485Z] 	at com.pingcap.tikv.operation.iterator.DAGIterator.process(DAGIterator.java:196)
[2019-12-09T22:32:13.485Z] 	at com.pingcap.tikv.operation.iterator.DAGIterator.lambda$submitTasks$1(DAGIterator.java:70)
[2019-12-09T22:32:13.485Z] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2019-12-09T22:32:13.485Z] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2019-12-09T22:32:13.485Z] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2019-12-09T22:32:13.485Z] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2019-12-09T22:32:13.485Z] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2019-12-09T22:32:13.485Z] 	at java.lang.Thread.run(Thread.java:748)
[2019-12-09T22:32:13.485Z] Caused by: com.pingcap.tikv.exception.LockException
[2019-12-09T22:32:13.485Z] 	... 10 more
[2019-12-09T22:32:13.485Z] 19/12/10 06:31:59 WARN TaskSetManager: Lost task 1.0 in stage 229.0 (TID 1224, localhost, executor driver): TaskKilled (Stage cancelled)
[2019-12-09T22:32:13.485Z] - Write large amount of data *** FAILED ***
[2019-12-09T22:32:13.485Z]   org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 229.0 failed 1 times, most recent failure: Lost task 3.0 in stage 229.0 (TID 1226, localhost, executor driver): com.pingcap.tikv.exception.TiClientInternalException: Error reading region:
[2019-12-09T22:32:13.485Z] 	at com.pingcap.tikv.operation.iterator.DAGIterator.doReadNextRegionChunks(DAGIterator.java:169)
[2019-12-09T22:32:13.485Z] 	at com.pingcap.tikv.operation.iterator.DAGIterator.readNextRegionChunks(DAGIterator.java:146)
[2019-12-09T22:32:13.486Z] 	at com.pingcap.tikv.operation.iterator.DAGIterator.hasNext(DAGIterator.java:92)
[2019-12-09T22:32:13.486Z] 	at org.apache.spark.sql.tispark.TiRowRDD$$anon$1.hasNext(TiRowRDD.scala:75)
[2019-12-09T22:32:13.486Z] 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.coprocessorrdd_nextBatch_0$(Unknown Source)
[2019-12-09T22:32:13.486Z] 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2019-12-09T22:32:13.486Z] 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2019-12-09T22:32:13.486Z] 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
[2019-12-09T22:32:13.486Z] 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
[2019-12-09T22:32:13.486Z] 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
[2019-12-09T22:32:13.486Z] 	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
[2019-12-09T22:32:13.486Z] 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:306)
[2019-12-09T22:32:13.486Z] 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:304)
[2019-12-09T22:32:13.486Z] 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)
[2019-12-09T22:32:13.486Z] 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)
[2019-12-09T22:32:13.486Z] 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2019-12-09T22:32:13.486Z] 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
[2019-12-09T22:32:13.486Z] 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
[2019-12-09T22:32:13.486Z] 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2019-12-09T22:32:13.486Z] 	at org.apache.spark.scheduler.Task.run(Task.scala:123)
[2019-12-09T22:32:13.486Z] 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
[2019-12-09T22:32:13.486Z] 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
[2019-12-09T22:32:13.486Z] 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
[2019-12-09T22:32:13.486Z] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[2019-12-09T22:32:13.486Z] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[2019-12-09T22:32:13.486Z] 	at java.lang.Thread.run(Thread.java:748)
[2019-12-09T22:32:13.486Z] Caused by: java.util.concurrent.ExecutionException: com.pingcap.tikv.exception.RegionTaskException: Handle region task failed:
[2019-12-09T22:32:13.486Z] 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[2019-12-09T22:32:13.486Z] 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
[2019-12-09T22:32:13.486Z] 	at com.pingcap.tikv.operation.iterator.DAGIterator.doReadNextRegionChunks(DAGIterator.java:164)
[2019-12-09T22:32:13.486Z] 	... 25 more
[2019-12-09T22:32:13.486Z] Caused by: com.pingcap.tikv.exception.RegionTaskException: Handle region task failed:
[2019-12-09T22:32:13.486Z] 	at com.pingcap.tikv.operation.iterator.DAGIterator.process(DAGIterator.java:209)
[2019-12-09T22:32:13.486Z] 	at com.pingcap.tikv.operation.iterator.DAGIterator.lambda$submitTasks$1(DAGIterator.java:70)
[2019-12-09T22:32:13.486Z] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2019-12-09T22:32:13.486Z] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2019-12-09T22:32:13.486Z] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2019-12-09T22:32:13.486Z] 	... 3 more
[2019-12-09T22:32:13.486Z] Caused by: com.pingcap.tikv.exception.GrpcException: retry is exhausted.
[2019-12-09T22:32:13.486Z] 	at com.pingcap.tikv.util.ConcreteBackOffer.doBackOff(ConcreteBackOffer.java:142)
[2019-12-09T22:32:13.486Z] 	at com.pingcap.tikv.region.RegionStoreClient.handleCopResponse(RegionStoreClient.java:490)
[2019-12-09T22:32:13.486Z] 	at com.pingcap.tikv.region.RegionStoreClient.coprocess(RegionStoreClient.java:450)
[2019-12-09T22:32:13.486Z] 	at com.pingcap.tikv.operation.iterator.DAGIterator.process(DAGIterator.java:196)
[2019-12-09T22:32:13.486Z] 	... 7 more
[2019-12-09T22:32:13.486Z] Caused by: com.pingcap.tikv.exception.LockException
[2019-12-09T22:32:13.486Z] 	... 10 more
[2019-12-09T22:32:13.486Z] 
[2019-12-09T22:32:13.486Z] Driver stacktrace:
[2019-12-09T22:32:13.486Z]   at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
[2019-12-09T22:32:13.486Z]   at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
[2019-12-09T22:32:13.486Z]   at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
[2019-12-09T22:32:13.486Z]   at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
[2019-12-09T22:32:13.486Z]   at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
[2019-12-09T22:32:13.486Z]   at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
[2019-12-09T22:32:13.486Z]   at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
[2019-12-09T22:32:13.486Z]   at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
[2019-12-09T22:32:13.486Z]   at scala.Option.foreach(Option.scala:257)
[2019-12-09T22:32:13.486Z]   at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
[2019-12-09T22:32:13.486Z]   ...
```
**Spark and TiSpark version info**
<!-- What version of Spark and TiSpark are you using? (Provide Spark version and run `spark.sql(“select ti_version()”).show(false)` in spark-shell) -->
affected versions: Master branch, release-2.2

<!--
**Additional context**
Add any other context about the problem here.
You may also provide TiDB version here if it is related to the issue.
-->

**Describe the bug**
TiSession was cached by distinct value of SparkConf. However, it is possible that multiple SparkSession share the same SparkConf, thus the SparkSession that TiSession points to is not guaranteed to be the same as `SparkSession.getActiveSession/SparkSession.getDefaultSession()`.

**Spark and TiSpark version info**
<!-- What version of Spark and TiSpark are you using? (Provide Spark version and run `spark.sql(“select ti_version()”).show(false)` in spark-shell) -->

master, release-2.2
<!--
**Additional context**
Add any other context about the problem here.
You may also provide TiDB version here if it is related to the issue.
-->

schedule task close to Data (TiKV)
Github already provided free CI/CD action. This PR enables a simple maven build action and we extend it later.



support columnar batch execution with spark-3.0
- in this PR https://github.com/pingcap/tispark/pull/1263
- or in this branch https://github.com/pingcap/tispark/tree/feature/support-spark-3.0-rebase-columnar

please use this profile `spark-3.0-scala-2.12` and uncomment the following code to enable test with spark-3.0

```
                    /*MVN_PROFILE_SCALA_2_12_TEST.each { MVN_TEST_PROFILE ->
                        sh """
                            export MAVEN_OPTS="-Xmx6G -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=51M"
                            mvn clean test ${MVN_PROFILE} ${MVN_PROFILE_SCALA_2_12} ${MVN_TEST_PROFILE} -Dtest=moo ${mvnStr}
                        """
                    }*/
```


see: https://github.com/apache/spark/pull/25008/files/8c285e57837bc1ccb87c0d894805ceb5d16f1299