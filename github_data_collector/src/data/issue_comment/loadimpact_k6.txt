This is a proposal for adding a new, standard metric to k6 called `http_req_errors` that records the number of failed HTTP requests.

_Some_ of this functionality can be achieved today with a custom metric, but it's so basic that I suggest it should be included by default. 

## Feature Description

Successful requests often have longer `http_req_duration` than failed requests. This metric would allow k6 to display a separate `http_req_duration` for successful requests and separate `http_req_duration_errors` for failed requests.

It's not possible to define which HTTP status codes should be considered errors because these codes are context-specific. For example, HTTP-404 is not an error if someone is testing for 404s. 
This problem can be solved by adding additional configuration options, as described below.

The biggest advantage of this functionality is the UX improvement. 
Currently, k6 doesn't distinguish between failed and successful requests, therefore the terminal output for a test that has only failed requests is not distinguishable from a successful test.  

With this change, one could see at first glance that the test was/wasn't successful without adding checks and thresholds.
 
## Suggested Solution (optional)

```javascript
let options = {
   http_errors: ["status>399"],  // default setting
   http_errors_exclude: [404, 403, 1099]
};
```

k6 terminal output could look similar to this:
```
    http_reqs..................: 10      0/s
    http_reqs_errors...........: 2 (20%)      2/s
    http_req_duration..........: avg=10.13s   min=10.13s   med=10.13s   max=10.13s   p(90)=10.13s   p(95)=10.13s  
    http_req_duration_errors...: avg=1.13s   min=1.13s   med=1.13s   max=1.03s   p(90)=10.13s   p(95)=10.13s  

```
This implements a couple of features discussed in #1279:

- General cleanup of the right-side progress bar text: shorter, split into two columns, correct alignment. Initially I experimented with tabwriter to achieve this simply with tabs, but there would always be an edge case that would unexpectedly cause columns to shift further right, so tabs weren't reliable. The alignments done manually here with `fmt` padding unfortunately complicate the render code quite a bit, so let me know if it can be simplified. I still think it's worth it as columns make the progress much easier to follow, even though they don't strictly contain homogeneous elements.
- Adding a "status" column and field to `ProgressBar` to indicate when the executor is stopping, done, etc. This is shown after the executor name, for lack of a better place, and takes up a fixed 2 chars. There's a minor bug in that the interrupted status is sometimes not rendered when pressing `^C`. Some kind of race condition... Related to this, should we wait for `max(gracefulStop)` before exiting, and force-exit with another `^C`? I think that would be better UX.

Here's what it looks like currently:
![2020-01-16-174117_983x298_scrot](https://user-images.githubusercontent.com/1009277/72552998-3b578f80-3898-11ea-9751-ad63abcdd2f0.png)
The discussions surrounding https://github.com/loadimpact/k6/issues/1295 and https://github.com/loadimpact/k6/pull/1307 made me realize that we might have an issue with work partitioning.... Some of the executors have 2 different attributes that have to be distributed when we partition them with execution segments (https://github.com/loadimpact/k6/issues/997) - the number of VUs they have at any given moment, and the work they have to do. The `per-vu-iterations`, `constant-looping-vus`, and `variable-looping-vus` don't have this problem, since for them, the only important thing we need to segment/partition is the number of VUs - the work each VU has to do isn't affected, it's constant. 

But for `shared-iterations`, `constant-arrival-rate`, and `variable-arrival-rate`, things are a bit different. Say that we have the following configuration:
```js
export let options = {
    executionSegment: "1/3:2/3",
    execution: {
        shared_iters: {
            type: "shared-iterations",
            vus: 2,
            iterations: 3,
            maxDuration: "10s",
        },
    },
};
```

This is the execution segment that has no VUs, but a single iteration... Similarly, with the `arrival-rate` executors it will probably be even easier to end up having a segment with work but no VUs... So, I think we may need to have a 2-tier partitioning for these executors:
1. Partition the workers (i.e. VUs) first
2. Based on the number of allotted workers for that segment, partition the actual work (iterations or iterations/s respectively).

So, in the above example, this is how the iterations and VUs are currently partitioned if we split the work into thirds, and also how I think they should:

| InstID | Segment | VUs | Iterations now | Iterations after fix |
|--------|---------|---|--------------------|-----|
| 1      | (0 - ⅓] | 1 | 1 | 1  |
| 2      | (⅓ - ⅔] | 0 | **1** | 0 |
| 3      | (⅔ - 1] | 1 | 1 | 2 |

For the arrival-rate executors things like this would probably happen even more often, since iters/s are practically infinitely divisible. Say that we have 2 iter/s with 2 VUs, and we again want to split that execution into thirds:
```js
export let options = {
    executionSegment: "1/3:2/3",
    execution: {
        constant_arr_rate: {
            type: "constant-arrival-rate",
            rate: 2,
            duration: "10s",
            preAllocatedVUs: 2,
            maxVUs: 2,
        },
    },
};
```

We'd have something like this:

| InstID | Segment | VUs | Iters/s now | Iters/s after fix |
|--------|---------|---|--------------------|-----|
| 1      | (0 - ⅓] | 1 | ⅔ | 1  |
| 2      | (⅓ - ⅔] | 0 | **⅔** | 0 |
| 3      | (⅔ - 1] | 1 | ⅔ | 1 |

These changes should be fairly easy and efficient to achieve with the execution segments, though of course, serious testing would be required to validate it...
Server using [Socket.IO](https://github.com/socketio/socket.io)

> Socket.IO is not a WebSocket implementation. Although Socket.IO indeed uses WebSocket as a transport when possible, it adds some metadata to each packet: the packet type, the namespace and the ack id when a message acknowledgement is needed. That is why a WebSocket client will not be able to successfully connect to a Socket.IO server, and a Socket.IO client will not be able to connect to a WebSocket server (like ws://echo.websocket.org) either. Please see the protocol specification [here](https://github.com/socketio/socket.io-protocol).

Useing  k6  ws module  connect will throw an error:

> ERRO[0000] GoError: websocket: bad handshake

This seemingly normal script won't work as most people expect:
```js
import http from "k6/http";

export let options = {
    duration: "5s",
    thresholds: {
        "iterations": ["count == 2"],
        "http_reqs": ["count == 2"],
    },
};

export default function () {
    let response = http.get(
        "https://httpbin.test.loadimpact.com/delay/3",
        { timeout: "2s" },
    );
}
```

Because the request timeout doesn't understand stringy durations, `"2s"` would be converted to `2` (because JavaScript :man_facepalming: ), so k6 would understand it as "2 milliseconds". So it's going to start to make a ton of requests instead of just 3...

However this script also doesn't work like how you'd expect either:
```js
import http from "k6/http";

export let options = {
    duration: 5000,
    thresholds: {
        "iterations": ["count == 2"],
        "http_reqs": ["count == 2"],
    },
};

export default function () {
    let response = http.get(
        "https://httpbin.test.loadimpact.com/delay/3",
        { timeout: 2000 },
    );
}
```

k6 runs, doesn't abort with a configuration error or something like that... but it interprets the `5000` value for `duration` as 5000 nanoseconds, i.e. 0.005 milliseconds, so it just immediately aborts... :man_facepalming: This is just a consequence of the way we've wrapped Go's `time.Duration` (which is nanosecond-based) in our [`types.Duration` custom type](https://github.com/loadimpact/k6/blob/7b6b2a684b7f26bbcf1b8549db89614ac398ad54/lib/types/types.go#L87) (that parses things like `"2s"`). Only, when it doesn't get a string, it [falls back](https://github.com/loadimpact/k6/blob/7b6b2a684b7f26bbcf1b8549db89614ac398ad54/lib/types/types.go#L118) to the nanosecond code...

So, I think we should unify and make everything saner... To do that with minimal breaking changes, I propose that we:
- use the same duration type (`types.Duration` / `types.NullDuration`?) everywhere we accept user-specified duration
- modify it so it either accepts valid string durations, or integer durations **in milliseconds**, everything else should be an error

So, the only breaking change is that users won't be able to specify the global options in nanoseconds, which won't be a big loss and I hope nobody has done...
I noted that when I use an powerful machines to stress another two servers, the requesting rate fluctuates periodically with two cpu cores pinned in 100%:
![image](https://user-images.githubusercontent.com/3319979/71971105-e044ed80-3244-11ea-88cc-57a76f5c1e8f.png)

cpu% in high position of period:
![image](https://user-images.githubusercontent.com/3319979/71971788-4da54e00-3246-11ea-8b56-5da13385402c.png)


cpu% in low position of period:
![image](https://user-images.githubusercontent.com/3319979/71970597-f7371000-3243-11ea-8772-499cb30cba8d.png)


This discussion is a nice starting point for what's currently suboptimal: https://github.com/loadimpact/k6/pull/1007#discussion_r332447323

I also noticed that the `lib` package depends on the `js/compiler` package... `go list -f '{{ join .Imports "\n" }}'  github.com/loadimpact/k6/lib | sort` produces:
```
...
github.com/loadimpact/k6/js/compiler
github.com/loadimpact/k6/lib/fsext
github.com/loadimpact/k6/lib/types
github.com/loadimpact/k6/loader
github.com/loadimpact/k6/stats
github.com/loadimpact/k6/ui/pb
...
```
it's not related to the new executors, but this shouldn't be the case
The usage report in #1007 is currently broken and disabled. Because we want to maximally preserve user privacy and not send any potentially sensitive data, we can't just package the new `execution` option and send that, since it could contain sensitive data we don't want to know.

Instead, we need to move the sending of the usage report from before the start of the script to the end of the execution and send only a rough summary of what was actually executed. So:
- the `k6_version`, `goos` and `goarch` JSON fields remain unchanged
- `vus_max`, `iterations` and `duration` would get their values from the [`ExecutionState`](https://github.com/loadimpact/k6/blob/f6be35cfa006c56954d6821e0f94f4863f063e45/lib/execution.go#L123)
- `st_duration` will be removed, since it doesn't make a lot of sense
- we might want to also add a field for the used executor types, but only their types and number, no names or any other details
Users should be able to inject environment variables and metric tags per-executor, as well as be able to specify a custom function to be executed instead of the `default` one. Something like this:
```js
export let options = {
    execution: {
        someKey: {
            type: "constant-looping-vus",
            vus: 10,
            duration: "60s",

            // Inject/overwrite environment variable key-value pairs
            env: {
                blah: "whatever",
                key1: "value1",
                key2: "value2"
            },

            // Allow users to execute some other *exported* function besides the default one
            exec: "someExportedFunctionThatsNotTheDefault",

            // The ability to inject additional metric tags
            tags: { tag1: "value1", tag2: "value2" },
        },
    },
}
```

These things would allow us to relatively gracefully support multiple scenarios in the same script, a very frequently requested feature.
This comment describes how execution segments don't work correctly with the arrival-rate executors: https://github.com/loadimpact/k6/pull/1007#issuecomment-522883821

And the other known bug is specifically with the variable-arrival-rate executor, described and somewhat fixed in https://github.com/loadimpact/k6/pull/1285 and demonstrated in this screencast: https://asciinema.org/a/xZca9sVUpmFfqBU0IFVCPRYZO