hi all

If my keras network look like
        model = Sequential()
        model.add(GRU(
            params.recurrent_units=20, activation='linear',
            input_shape=(pr.n_features=20, pr.feature_size=13), dropout=0.2, name='net'
        ))
        model.add(Dense(1, activation='sigmoid'))

model's summary
Layer (type)                 Output Shape              Param #
net (GRU)                    (None, 20)                2040
dense_1 (Dense)              (None, 1)                 21

how to create the network structure for tiny-dnn?
I don't know how to fill the below variables
    recurrent(tiny_dnn::gru(input_size, hidden_size), seq_len, params);
Thanks

Tiny-dnn is header-only.Can I just type "#include <tiny_dnn/tiny_dnn.h>"and add the headfile into the project?
But I encounter this problem that "tiny_dnn/tiny_dnn.h:no such file or directory" whatever I use visual stdio 2019 on windows or arm-linux-gcc on linux for accorss compliation.And also there seems something wrong in the tiny_dnn.h.
![1571040269(1)](https://user-images.githubusercontent.com/56524467/66736656-4d6d0080-ee59-11e9-8854-98485dadded7.jpg)

![1571040097(1)](https://user-images.githubusercontent.com/56524467/66736512-e2bbc500-ee58-11e9-9d72-a4f9769f0b02.jpg)


Please find the error message below:

[ 34%] Building CXX object examples/CMakeFiles/example_mnist_train.dir/mnist/train.cpp.o
In file included from /home/febin/Storage/tiny-dnn-master/tiny_dnn/tiny_dnn.h:43:0,
                 from /home/febin/Storage/tiny-dnn-master/examples/mnist/train.cpp:10:
/home/febin/Storage/tiny-dnn-master/tiny_dnn/layers/l2_normalization_layer.h: In member function ‘virtual void tiny_dnn::l2_normalization_layer::back_propagation(const std::vector<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64> > >*>&, const std::vector<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64> > >*>&, std::vector<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64> > >*>&, std::vector<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64> > >*>&)’:
/home/febin/Storage/tiny-dnn-master/tiny_dnn/layers/l2_normalization_layer.h:109:15: warning: unused variable ‘prev_delta’ [-Wunused-variable]
     tensor_t &prev_delta     = *in_grad[0];
               ^~~~~~~~~~
/home/febin/Storage/tiny-dnn-master/tiny_dnn/layers/l2_normalization_layer.h:110:15: warning: unused variable ‘curr_delta’ [-Wunused-variable]
     tensor_t &curr_delta     = *out_grad[0];
               ^~~~~~~~~~
/home/febin/Storage/tiny-dnn-master/tiny_dnn/layers/l2_normalization_layer.h:112:18: warning: unused variable ‘num_samples’ [-Wunused-variable]
     const size_t num_samples = curr_out.size();
                  ^~~~~~~~~~~

/tmp/ccJwUsAg.s: Assembler messages:
/tmp/ccJwUsAg.s: Internal error (Segmentation fault).
Please report this bug.
examples/CMakeFiles/example_mnist_train.dir/build.make:62: recipe for target 'examples/CMakeFiles/example_mnist_train.dir/mnist/train.cpp.o' failed
make[2]: *** [examples/CMakeFiles/example_mnist_train.dir/mnist/train.cpp.o] Error 1


I dont see this issue posted here !! 
When building: examples/CMakeFiles/example_cblas_mlp.dir/backends/cblas/test_mlp.cpp.o
on test_mlp.cpp line 35: error: too few arguments to function ‘int rand_r(unsigned int*)’
```
       input.push_back(rand_r() / RAND_MAX);
```

On my glibc,  rand_r was declared as:
 extern int rand_r (unsigned int *__seed) __THROW;

Fix:
```
       input.push_back(rand_r(&seed) / RAND_MAX);
```


can we use both test function and get_loss functions at a time epoch enumerate? If I use like that I Am getting loss as nan values.
I found one bug in multi class cross entropy that log(0)=nan.so whenever   std::log(y[i]); should change to std::log(y[i]+1e-20);
instead of 1e-20 we can add any small value

how much I can keep b1_t and b2_t?
I am trying to match the performance with keras I could not able to find the b1_t and b2_t constant values?
Could you publish an example of training for semantic segmentation where the pictures are located as files in a folder? Thank you
Hello,
I am trying to train semantic segmentation net.
The net was build but I get exception in deconv2d function (file backend_tiny.h) when calling to backward_activation:
`backward_activation(*out_grad[0], *out_data[0], curr_delta);`
Message is: Microsoft C++ exception: std::bad_function_call at memory location 0x00000000011FB640..
The exception is in the STD library area.
I am using the latest tiny-dnn code.
Hi:
Is there anybody can help me ?
I use the g++ option "static" to compile the examples, it can compile successfully but when  I run the example benchmark_all it shows segment fault.
thanks in advance :)