![image](https://user-images.githubusercontent.com/51399917/72327395-f989e600-36eb-11ea-8145-4575aa35b31a.png)
How to solve it?
Does anyone get the same results as in the paper on WMT14 en-de dataset?
Who can tell me where this path can be changed?
I have installed this package, but I don't know where to change the path below.
Thank you!!!
![image](https://user-images.githubusercontent.com/36886184/71321748-048c9480-24f9-11ea-825a-2970cce632ca.png)

Hi,

I saw that you slightly changed your code and now write:

`self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)`

Can you tell me the reason for using `bias=False`? I first thought you changed it in order for the zeros in the input to be preserved. Although padding_idx is used in nn.Embedding, you afterwards add the positional encoding which leads to all values in the input to be non-zero. 

Thanks in advance for your reply!
could you add the bleu score result on IWSLT14 de-en dataset for fast comparable result with other implementation

thanks for your great work. 

Let me check one point about **head's order** when extracting and visualizing attention weight.

I personally checked internal behavior in `MultiheadAttention()`.
first, prepare temporary params and tensor for test as below, 

```python
# params
minibatch = 32
d_model = 10
seq_len = 20
n_head = 2
d_k = 10
d_v = 10

# dummy inputs for check
ipt = torch.randn(minibatch, seq_len, d_model) # torch.Size([32, 20, 10])
```

and checked the size in before changing tensor size of query, key and value, and after that in `MultiheadAttention()`

```python
class MultiHeadAttention(nn.Module):
    ''' Multi-Head Attention module '''

    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):
        super().__init__()
    ...

    def forward(self, q, k, v, mask=None):

        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head

        sz_b, len_q, _ = q.size()
        sz_b, len_k, _ = k.size()
        sz_b, len_v, _ = v.size()

        residual = q

        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)
        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)
        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)
        print(q.size(), k.size(), v.size())
        print(q[0][0])

        q = q.permute(2, 0, 1, 3).contiguous().view(-1, len_q, d_k) # (n*b) x lq x dk
        k = k.permute(2, 0, 1, 3).contiguous().view(-1, len_k, d_k) # (n*b) x lk x dk
        v = v.permute(2, 0, 1, 3).contiguous().view(-1, len_v, d_v) # (n*b) x lv x dv

        print(q.size(), k.size(), v.size())
        print(q[0][0])
        print(q[32][0])

    ...
```

and output is

```
torch.Size([32, 20, 2, 10]) torch.Size([32, 20, 2, 10]) torch.Size([32, 20, 2, 10])
# [batch: 0, seq_position: 0, head: 0], [batch: 0, seq_position: 0, head: 1 ]
tensor([[-0.8109,  0.1689, -0.6094,  1.7303,  0.0122, -0.3231, -1.4111,  0.8645,
          0.5258, -2.0746],
        [ 1.0508,  0.2750,  1.0371,  1.3258,  1.3777,  0.1417,  0.4130,  1.8987,
         -1.0220,  1.3690]], grad_fn=<SelectBackward>)
torch.Size([64, 20, 10]) torch.Size([64, 20, 10]) torch.Size([64, 20, 10])
# [batch: 0, head: 0, seq_position: 0], [batch: 0, head: 1, seq_position: 0]
tensor([-0.8109,  0.1689, -0.6094,  1.7303,  0.0122, -0.3231, -1.4111,  0.8645,
         0.5258, -2.0746], grad_fn=<SelectBackward>)
tensor([ 1.0508,  0.2750,  1.0371,  1.3258,  1.3777,  0.1417,  0.4130,  1.8987,
        -1.0220,  1.3690], grad_fn=<SelectBackward>)
```

It looks like next head is following one batch(?)(sorry, it is so ambiguous expression)

so I interpreted like that after `view()`,  output tensor shape is like below,

```
head_0, batch_0
head_0, batch_1
...
head_0, batch_(minibatch_size)
...
head_1, batch_0
head_1, batch_1
...
```

so, I visualize attention weight of two head for first sequence of batch like below,

```python
_, attn_weight = model.(enc|dec)oder.forward([inputs], return_attns=True)
print(attn_weight[0]) # first head
print(attn_weight[0+batch_size]) # second head
```

is this collect?


Please, how to add Attention weight plot to this code. Thanks in advance
Thank you for the code! By default, Pytorch linear layers use uniform Kaiming initialization, which is meant to be used before a ReLU activation with uniform [+1, -1] inputs. However, this initialization is used at the output projections of the attention and feed-forward submodules. With this initialization, training loss does not decrease on the Ubuntu conversation corpus. I found that switching to Xavier (smaller init) lead to convergence, while even smaller values (xavier * 1/100) lead to faster convergence. This is not surprising considering each submodule uses skip connections (x+f(x)), so smaller outputs lead to a similar variance between the input and output of a given Transformer layer, which is what these initialization methods aim to achieve. The LayerNorm corrects any changes in activation variance, but this does not stop layer outputs from initializing as increasingly non-linear early in training, which can complicate gradients. Also, I don't believe Attention Is All You Need mentions initialization, but I could be wrong. Thanks.
What is the shape of the parameter `word_prob` that is input to the method `advance` in the beam search module? I am wondering what `num_words` is? Is this the target vocab size?

Also, why do we get the beam a score belongs to by computing `best_score_ids / num_words`. 