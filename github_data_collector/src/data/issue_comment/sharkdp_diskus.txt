Maybe I were doing it wrong.
The computed directory is my clippy build.
```console
% /usr/bin/du -sch
4.5G    .
4.5G    total
% diskus
4.73 GB (4,727,521,280 bytes)
% hyperfine diskus '/usr/bin/du -sch'
Benchmark #1: diskus
  Time (mean ± σ):     115.8 ms ±  28.6 ms    [User: 2.601 s, System: 0.592 s]
  Range (min … max):    69.1 ms … 156.9 ms    19 runs

Benchmark #2: /usr/bin/du -sch
  Time (mean ± σ):      22.8 ms ±   2.8 ms    [User: 5.5 ms, System: 17.4 ms]
  Range (min … max):    14.2 ms …  26.9 ms    163 runs

Summary
  '/usr/bin/du -sch' ran
    5.07 ± 1.40 times faster than 'diskus'
```

# Meta

* diskus: b2e4cf9 but with `cargo update`
As I suggested on a commit
This is a rudimentary implementation, and just outputs the total number of files found as well.  It includes directories in the count as well, and is very naive in its reporting in that sense.  We leverage diskus for reporting and this metric is helpful for us.

Haven't submitted many PRs before, so let me know if you need other things added.
This should fix #27.  I basically removed `rayon` and handled the threads manually using `crossbeam`.  I also took the liberty to make the `diskus` binary use the `diskus` library instead of just `mod`ing `walk.rs`.
With a sufficiently large directory, the threads can overflow their stacks because `walk()` is recursive.  I encountered a similar issue when rewriting the `du` implementation in [`uutils/coreutils`](https://github.com/uutils/coreutils), so I decided to test `diskus` and found the same problem.
One of the most common use-cases for `du -hs` by far for me (and also what `dust` caters to), is to do `du -hs *` to find the largest directories in a given directory (usually the current one). It doesn't seem like `diskus` can currently operate in that mode? It'd be awesome if that kind of "tell me what's large" quick-n-dirty mode could be supported somehow!