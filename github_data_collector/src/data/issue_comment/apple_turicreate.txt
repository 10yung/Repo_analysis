## Upstream dependencies

#2826, which removes redundant MXnet **Python** code after major 6.0 release. 

---

## Goal, goal, goal, ale, ale, ale

partially resolve #2312 

1. make our current python codebase fully pep8 compatible.
2. provide GitHub hook to check pep8 compatibility for future commits.
3. build the team culture for the pep8 coding style for our python codebase.

In order to fully resolve #2312, we need to run a `linter`, which can be only done incrementally. I will cover this later.

---

## Methodology

No magic, but [blackI[0] magic. I

'm pretty confident about the tool [black][0] and its reformatting result because of,

> Also, as a temporary safety measure, Black will check that the reformatted code still produces a valid AST that is equivalent to the original. This slows it down. If you're feeling confident, use --fast.

---

## Step by step

- [ ] make source code pep8 compliant

[black][0] the `python` dir first to make sure the `test` directory is left intact. If all tests are passed, I'm confident to move to the next step.

```
All done! ✨ 🍰 ✨
281 files reformatted, 17 files left unchanged.
```

---

- [ ] make test code pep8

[0]: https://github.com/psf/black
[1]: https://www.python.org/dev/peps/pep-0008/
I used LibriSpeech to classify the speakers. I broke up my data and trained it with CreateML, and with 147 classes, the outcome is excellent, or at least it claims 98% on testing.

However, I would like to make the model updatable, and this is something I can't do with CreateML since it uses a GLM at the end.

So I tried using the TC sound classifier since it has a neural network classifier at the end with updatable dense layers. I went through the sample project in the documentation and everything worked fine with TC's sound classifier. Then I tried to use my own data, the same I used with CreateML, and it never learns at all.

Is there something I might be missing? Is CreateML's approach super different than TC's? I've looked at both models with CoreML tools and see that they're both pipeline classifiers, and that they start with a preprocessing model, a feature abstractor, and then, for TC, the neural network, and for CreateML, the GLM.

Any recommendations on getting TC's sound classifier to work with my data? They're 16000 sample rate waves with silence removed and then divided into 1 second files.

Thanks!
Until #2936 gets resolved, marked the failing non-deterministic test as xfail.
The output is not reproducible i.e, The topics give different words with every run. 
There is no random seed parameter in tc.topic_model.create.

Could there be a good reason for this? I want the results to be reproducible.

Thanks
- Remove documentation building from make_wheel.sh
- Create standalone script to make documentation
- Creaet standalone GitLab job

Fixes #284 
Fixes the second bullet point of https://github.com/apple/turicreate/issues/2460#issuecomment-543303328
## TL;DR
The perf regression is caused by that the TF graph is initialized (adapted) for each image input during stylization (predict). The change will make the TF graph only initialized once during a prediction.

The way to do it is to define the image H and W as the dynamic dimension value instead of static.

resolves #2798 on the CPU path.

---

## review suggestion
When you review the code, please ignore src/python/turicreate/test/test_style_transfer.py because it mainly has formatting changes.

---

## report

load the same weights and predict on the same image.

```
In [9]: print(content_sf)
+------------------------+
|  content_feature_name  |
+------------------------+
| Height: 403 Width: 311 |
+------------------------+
```

### conclusion

* compared with 6.0: from 800ms -> 500ms.
* compared with 5.8:
  * on single image predict, 800ms -> 1290ms, expected slow down since initializing the graph will take 400ms.
  * batch stylizing images: 750ms -> 540 + (**880** / `number_of_images * number_of_styles`)ms, the initialization cost is amortized by subsequent calls to the `stylize`. **880** is the estimated data preparation (440ms) and graph initialization (440ms) overhead, which is pretty accurate, IMO, for my machine. We can do better to see how much we can reduce the data preparation overhead at the CPP implementation level.

---

### 5.8 result

since everything runs in the python layer, the model state is reserved and no extra initialization is needed,

```
In [6]: %timeit img = model.stylize(content_sf["content_feature_name"], style=0)
751 ms ± 8.32 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

This profile is pretty accurate for `stylize` since there is **no** overhead of neural net architecture initialization.

---

### 6.0 result

if I run predict on singe image with a single style, the overhead of graph construction (880 ms) is not amortized,

```
+------------------+--------------+------------------+
| Images Processed | Elapsed Time | Percent Complete |
+------------------+--------------+------------------+
| 1                | 539.738ms    | 100.00%          |
+------------------+--------------+------------------+
1.29 s ± 20.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

```

But if we run in a batch manner with data replicated 127 times,

```
In [9]: print(content_sf)
+------------------------+
|  content_feature_name  |
+------------------------+
| Height: 403 Width: 311 |
| Height: 403 Width: 311 |
| Height: 403 Width: 311 |
| Height: 403 Width: 311 |
| Height: 403 Width: 311 |
| Height: 403 Width: 311 |
| Height: 403 Width: 311 |
| Height: 403 Width: 311 |
| Height: 403 Width: 311 |
| Height: 403 Width: 311 |
+------------------------+
[128 rows x 1 columns]
Note: Only the head of the SFrame is printed.
You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.
```

and then feed it into predict (stylization) compuation flow,

```
In [8]: %timeit img = model.stylize(content_sf["content_feature_name"], style=0)
+------------------+--------------+------------------+
| Images Processed | Elapsed Time | Percent Complete |
+------------------+--------------+------------------+
| 1                | 577.957ms    | 0.78%            |
| 2                | 1.04s        | 1.56%            |
| 3                | 1.48s        | 2.34%            |
| 4                | 1.93s        | 3.12%            |
| 5                | 2.38s        | 3.91%            |
| 10               | 4.69s        | 7.81%            |
| 20               | 9.97s        | 15.62%           |
| 30               | 14.91s       | 23.44%           |
| 40               | 20.05s       | 31.25%           |
| 50               | 24.93s       | 39.06%           |
| 60               | 29.94s       | 46.88%           |
| 70               | 35.06s       | 54.69%           |
| 80               | 40.54s       | 62.50%           |
| 90               | 45.77s       | 70.31%           |
| 100              | 50.89s       | 78.12%           |
| 110              | 55.98s       | 85.94%           |
| 120              | 1m 1s        | 93.75%           |
+------------------+--------------+------------------+
1min 10s ± 7.07 s per loop (mean ± std. dev. of 7 runs, 1 loop each)

```

The performance is averaged to ` 546.875` ms for each image input, which is approximate 540 + 880 / 128.
There is a unit test failure (`test_create_empty_dataset`) in OSOD that fails non-deterministically with two different errors:


On macOS 10.14 with Python 2.7:
```
__________ OneObjectDetectorSmokeTest.test_create_with_empty_dataset ___________

self = <turicreate.test.test_one_shot_object_detector.OneObjectDetectorSmokeTest testMethod=test_create_with_empty_dataset>

    def test_create_with_empty_dataset(self):
        with self.assertRaises(_ToolkitError):
>           tc.one_shot_object_detector.create(self.train[:0], target=self.target)

test_one_shot_object_detector.py:138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../toolkits/one_shot_object_detector/one_shot_object_detector.py:64: in create
    data, target, backgrounds)
../toolkits/one_shot_object_detector/util/_augmentation.py:54: in preview_synthetic_training_data
    backgrounds = backgrounds.apply(lambda im: _tc.image_analysis.resize(
../data_structures/sarray.py:1841: in apply
    dryrun = [fn(i) for i in self.head(100) if i is not None]
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

im = '{'

    backgrounds = backgrounds.apply(lambda im: _tc.image_analysis.resize(
        im,
>       int(im.width/2),
        int(im.height/2),
        im.channels
        ))
E   AttributeError: 'str' object has no attribute 'width'

../toolkits/one_shot_object_detector/util/_augmentation.py:56: AttributeError
```

On macOS 10.14 with Python 3.7:
```
__________ OneObjectDetectorSmokeTest.test_create_with_empty_dataset ___________

self = <turicreate.test.test_one_shot_object_detector.OneObjectDetectorSmokeTest testMethod=test_create_with_empty_dataset>

    def test_create_with_empty_dataset(self):
        with self.assertRaises(_ToolkitError):
>           tc.one_shot_object_detector.create(self.train[:0], target=self.target)

test_one_shot_object_detector.py:138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../toolkits/one_shot_object_detector/one_shot_object_detector.py:64: in create
    data, target, backgrounds)
../toolkits/one_shot_object_detector/util/_augmentation.py:54: in preview_synthetic_training_data
    backgrounds = backgrounds.apply(lambda im: _tc.image_analysis.resize(
../data_structures/sarray.py:1841: in apply
    dryrun = [fn(i) for i in self.head(100) if i is not None]
../data_structures/sarray.py:1841: in <listcomp>
    dryrun = [fn(i) for i in self.head(100) if i is not None]
../data_structures/sarray.py:854: in generator
    ret = self.__proxy__.iterator_get_next(elems_at_a_time)
cy_sarray.pyx:273: in turicreate._cython.cy_sarray.UnitySArrayProxy.iterator_get_next
    ???
cy_sarray.pyx:276: in turicreate._cython.cy_sarray.UnitySArrayProxy.iterator_get_next
    ???
cy_flexible_type.pyx:1755: in turicreate._cython.cy_flexible_type.pylist_from_flex_list
    ???
cy_flexible_type.pyx:1823: in turicreate._cython.cy_flexible_type.pyobject_from_flexible_type
    ???
cy_cpp_utils.pxd:68: in turicreate._cython.cy_cpp_utils.cpp_to_str
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   UnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 4: invalid continuation byte

cy_cpp_utils.pyx:82: UnicodeDecodeError
```

Repro steps:
- Run the Python unit test
This [unit test](https://github.com/apple/turicreate/blob/d8aa46a5e79966029796b22ab52616dccfbc2e64/src/python/turicreate/test/test_json.py#L360) should be rewritten or removed. 

Failure example 1:
```
_________________________ JSONTest.test_arbitrary_json _________________________

self = <turicreate.test.test_json.JSONTest testMethod=test_arbitrary_json>

    @hypothesis.settings(derandomize=True, suppress_health_check=[hypothesis.HealthCheck.too_slow])
>   @hypothesis.given(hypothesis_json)
    def test_arbitrary_json(self, json_obj):

test_json.py:359: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../hypothesis/core.py:599: in execute
    % (test.__name__, text_repr[0])
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <hypothesis.core.StateForActualGivenExecution object at 0x13401e710>
message = "Hypothesis test_arbitrary_json(self=<turicreate.test.test_json.JSONTest testMethod=test_arbitrary_json>, json_obj=[{'t': 0}]) produces unreliable results: Falsified on the first call but did not on a subsequent one"

    def __flaky(self, message):
        if len(self.falsifying_examples) <= 1:
>           raise Flaky(message)
E           hypothesis.errors.Flaky: Hypothesis test_arbitrary_json(self=<turicreate.test.test_json.JSONTest testMethod=test_arbitrary_json>, json_obj=[{'t': 0}]) produces unreliable results: Falsified on the first call but did not on a subsequent one

../../hypothesis/core.py:769: Flaky
```

Example 2:
```
Falsifying example: test_arbitrary_json(self=<turicreate.test.test_json.JSONTest testMethod=test_arbitrary_json>, json_obj=[{'Y': [{'2AB': -0.0,
    '7h': 13,
    'A': '',
    'CkEm': 0.5,
    'EUUY': -2.220446049250313e-16,
    'JBbEZ': '',
    'LgZcU': '',
    'Y': '',
    'Zvgdo': '',
    'ddDng': '',
    'o1Qj6jrL': ''}]}])
Unreliable test timings! On an initial run, this test took 305.89ms, which exceeded the deadline of 200.00ms, but on a subsequent run it took 11.93 ms, which did not. If you expect this sort of variability in your test timings, consider turning deadlines off for this test by setting deadline=None.
```
Comparing run time across runs doesn't make sense. In many cases the unit test returns early if the random input isn't in the correct format.
Fixes issues with CoreML integration on OSX 10.13.