I failed to make polynote work with Scala 2.12 and Spark 2.4.4 ( my current docker container is https://github.com/antonkulaga/bigdata-docker/tree/master/containers/polynote ). Probably the classpath issue, as it cannot find org.apache libraries. Strangly the same stuff used to work for me for 2.11.x
![polynote_issue](https://user-images.githubusercontent.com/842436/72658414-41915d00-39b9-11ea-93f7-9b5d75f57587.png)

A bit messy but I figure let's get it out there and iterate. Addresses #334 
* Fixed dangling listeners that keep trying to reactivate a notebook that you closed
* Added an error display when there's a failure trying to load a notebook
* A bit more logging around notebook sessions
* Misc websocket cleanup
Addresses #57 
When the whole page is filled with cells and a new cell is added, it shows at the bottom of the window. Like some other notebooks namely Jupyter, is it possible to scroll that cell little bit so it will be in the center of the window?
Please check the difference by checking out these screenshots. 

1. Polynote (new cell at the bottom and not scrollable)

![polynote](https://user-images.githubusercontent.com/6255161/72496654-4090ec00-3850-11ea-8ffd-470db885927a.png)




2. Other notebook (new cell is scrollable kept in the center)

![other](https://user-images.githubusercontent.com/6255161/72496656-4090ec00-3850-11ea-83f4-b235be59400f.png)


I followed the instruction to run it on docker, created the `config.ym`l and the `notebooks` directory, but wasn't able to run it properly.

```
$ docker run --rm -it -p 127.0.0.1:8192:8192 -p 127.0.0.1:4040-4050:4040-4050 -v `pwd`/config.yml:/opt/config/config.yml -v `pwd`/notebooks:/opt/notebooks/ polynote/polynote:latest --config 
java -cp polynote.jar:polynote.jar:/opt/polynote/deps/scala-library-2.11.12.jar:/opt/polynote/deps/polynote-runtime.jar:/opt/polynote/deps/scala-reflect-2.11.12.jar:/opt/polynote/deps/polynote-spark-runtime.jar:/opt/polynote/deps/portable-scala-reflect_2.11-0.1.0.jar:/opt/polynote/deps/scala-collection-compat_2.11-2.1.1.jar:/opt/polynote/deps/scala-xml_2.11-1.2.0.jar:/opt/polynote/deps/scala-compiler-2.11.12.jar -Djava.library.path=/usr/local/lib/python3.7/dist-packages/jep polynote.Main --config
Exception in thread "main" zio.FiberFailure: Fiber failed.
An unchecked error was produced.
java.lang.IllegalArgumentException: Unknown argument --config
	at polynote.server.Server$.polynote$server$Server$$parseArgs(Server.scala:205)
	at polynote.server.Server$$anonfun$run$1.apply(Server.scala:86)
	at polynote.server.Server$$anonfun$run$1.apply(Server.scala:86)
	at zio.internal.FiberContext.evaluateNow(FiberContext.scala:280)
	at zio.internal.FiberContext.zio$internal$FiberContext$$run$body$1(FiberContext.scala:596)
	at zio.internal.FiberContext$$anonfun$6.run(FiberContext.scala:596)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Fiber:2 was supposed to continue to:
  a future continuation at polynote.server.Server$$anonfun$run$2.apply(Server.scala:86)

Fiber:2 execution trace:
  at zio.ZIO$$anonfun$orDieWith$1.apply(ZIO.scala:669)
  at scala.Predef$$anon$1.apply(Predef.scala:389)
  at zio.ZIOFunctions$$anonfun$fromEither$1.apply(ZIO.scala:2125)
  at polynote.server.Server$$anonfun$run$1.apply(Server.scala:86)

Fiber:2 was spawned by:

Fiber:1 was supposed to continue to:
  a future continuation at polynote.app.App$$anonfun$main$1$$anonfun$apply$3.apply(App.scala:56)
  a future continuation at polynote.app.App$$anonfun$main$1.apply(App.scala:56)

Fiber:1 ZIO Execution trace: <empty trace>

Fiber:1 was spawned by: <empty trace>
```
It seems to be impossible to edit an equation after entering it initially. I can't figure out a way to bring up the TeX editor for the equation already made. 
It would be very useful to be able to set some environment variables for kernels - the problem is that this would only work in remote mode...
Should we add a delete button to the cell toolbar?
Right now if you want to launch a Spark-enabled interpreter you need to have some Spark configuration in the notebook. We should just have a checkbox that toggles whether to launch Spark or not. 