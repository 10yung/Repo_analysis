I need to download (and upload to S3) a really large file (1.5 GB). I need to run this on a serverless environment so I may not necessarily have access to a local disk. How can I do this with mechanize? Thanks.

I just tried the following code and this code is still running for last 30 minutes.

```ruby
agent.pluggable_parser.default = Mechanize::Download
agent.get(url).body
```

FIxes #547 
`save` and `load` of `Mechanize::CookieJar` accept `|` as a file path. It leads to OS Command Injection.

# How to reproduce

PoC of `save` is the following.

```ruby
require 'mechanize'

url = 'https://example.com/'

agent = Mechanize.new
page = agent.get(url)
login_form = page.forms[0]
login_form.field_with(name: 'email').value = 'xxx'
login_form.field_with(name: 'password').value = 'zzz'
page = login_form.submit

agent.cookie_jar.save('| touch /tmp/evil.txt')
```

Then, `/tmp/evil.txt` will be created. `load` is the same.

# Environments

- Mechanize: master branch
- Ruby: MRI (ruby 2.4.4p296 (2018-03-28 revision 63013) [x86_64-darwin17])

From [RFC 7235 Appendix A](https://tools.ietf.org/html/rfc7235#appendix-A):

> The "realm" parameter is no longer always required on challenges;
  consequently, the ABNF allows challenges without any auth parameters.

This patch makes the 'realm' parameter optional for Basic auth.

Having said that, I'm not too sure whether this would be in conflict with [RFC 7617 section 2.](https://tools.ietf.org/html/rfc7617#section-2) where it says the following about Basic auth:

> The authentication parameter 'realm' is REQUIRED ([[RFC7235],
      Section 2.2](https://tools.ietf.org/html/rfc7235#section-2.2)).

Feel free to reject this PR if we'd rather keep to the words of RFC 7617 and/or I have misinterpreted RFC 7235.  Otherwise, comments and suggestions welcome if improvements needed.

- [Possibly relevant SO link](https://stackoverflow.com/a/35339784)
This PR updates the CI matrix to use latest JRuby, 9.2.6.0.

[JRuby 9.2.6.0 release blog post](https://www.jruby.org/2019/02/11/jruby-9-2-6-0.html)

---

- This does not solve any of the three Zlib Gzip test failures, 
- This does not introduce any new failures.

<details>

```
  1) Error:
TestMechanize#test_get_gzip:
IOError: footer is not found
    org/jruby/ext/zlib/JZlibRubyGzipReader.java:576:in `eof'
    org/jruby/ext/zlib/JZlibRubyGzipReader.java:582:in `eof?'
    /home/travis/build/sparklemotion/mechanize/lib/mechanize/http/agent.rb:1183:in `auto_io'
    /home/travis/build/sparklemotion/mechanize/lib/mechanize/http/agent.rb:447:in `content_encoding_gunzip'
    /home/travis/build/sparklemotion/mechanize/lib/mechanize/http/agent.rb:830:in `response_content_encoding'
    /home/travis/build/sparklemotion/mechanize/lib/mechanize/http/agent.rb:296:in `fetch'
    /home/travis/build/sparklemotion/mechanize/lib/mechanize.rb:464:in `get'
    /home/travis/build/sparklemotion/mechanize/test/test_mechanize.rb:551:in `test_get_gzip'
  2) Error:
TestMechanize#test_content_encoding_hooks_body_io:
Zlib::GzipFile::NoFooter: footer is not found
    org/jruby/ext/zlib/JZlibRubyGzipReader.java:349:in `read'
    /home/travis/build/sparklemotion/mechanize/test/test_mechanize.rb:574:in `external_cmd'
    /home/travis/build/sparklemotion/mechanize/test/test_mechanize.rb:582:in `block in test_content_encoding_hooks_body_io'
    /home/travis/build/sparklemotion/mechanize/lib/mechanize/http/agent.rb:392:in `block in hook_content_encoding'
    org/jruby/RubyArray.java:1792:in `each'
    /home/travis/build/sparklemotion/mechanize/lib/mechanize/http/agent.rb:391:in `hook_content_encoding'
    /home/travis/build/sparklemotion/mechanize/lib/mechanize/http/agent.rb:294:in `fetch'
    /home/travis/build/sparklemotion/mechanize/lib/mechanize.rb:464:in `get'
    /home/travis/build/sparklemotion/mechanize/test/test_mechanize.rb:585:in `test_content_encoding_hooks_body_io'
  3) Error:
TestMechanize#test_content_encoding_hooks_header:
IOError: footer is not found
    org/jruby/ext/zlib/JZlibRubyGzipReader.java:576:in `eof'
    org/jruby/ext/zlib/JZlibRubyGzipReader.java:582:in `eof?'
    /home/travis/build/sparklemotion/mechanize/lib/mechanize/http/agent.rb:1183:in `auto_io'
    /home/travis/build/sparklemotion/mechanize/lib/mechanize/http/agent.rb:447:in `content_encoding_gunzip'
    /home/travis/build/sparklemotion/mechanize/lib/mechanize/http/agent.rb:830:in `response_content_encoding'
    /home/travis/build/sparklemotion/mechanize/lib/mechanize/http/agent.rb:296:in `fetch'
    /home/travis/build/sparklemotion/mechanize/lib/mechanize.rb:464:in `get'
    /home/travis/build/sparklemotion/mechanize/test/test_mechanize.rb:569:in `test_content_encoding_hooks_header'

```

</details>
Credential headers (Authorization and Cookie) are now forwarded to cross-site when redirection. Should be cleared these headers before redirection to protect leaking credential info.
mechanize v2.7.6 socksify patch no working, mechanize v2.7.5 working.

```
require "socksify"
require 'socksify/http'

Socksify::debug = true

class Mechanize::HTTP::Agent
  public
  def set_socks addr, port
    set_http unless @http
    class << @http
      attr_accessor :socks_addr, :socks_port

      def http_class
        Net::HTTP.SOCKSProxy(socks_addr, socks_port)
      end
    end

    @http.socks_addr = addr
    @http.socks_port = port
  end
end
```
It seems to be `require 'kconv'` (Kanji converter in the standard Ruby libraries) is not used anywhere at all.

This found in truffle-ruby which hasn't been `kconv` yet in this issue https://github.com/oracle/truffleruby/issues/1439

This thing was introduced in this commit https://github.com/sparklemotion/mechanize/commit/5c8e802f35d53c355e4840c972ba0113f15b58e2
I saw the other issue about a memory leak in net-http-persistent 3.0.0 but even after rolling back to 2.9.4 I still see this behaviour.

````
require 'mechanize'

def meminfo
  pid, size = `ps ax -o pid,rss | grep -E "^[[:space:]]*#{$$}"`.strip.split.map(&:to_i) ; return size
end

mech = Mechanize.new
x = 0
while 1
  mech.get 'https://www.google.com'
  m = meminfo
  if m != x then x = m ; p x ; end
end
````

Simply loading a page seems to endlessly consume RAM. Surely this isn't expected behaviour?