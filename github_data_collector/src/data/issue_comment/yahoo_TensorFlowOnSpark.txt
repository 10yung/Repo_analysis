**Environment:**
 - Python version [e.g. 2.7, 3.6] 3.6
 - Spark version [e.g. 2.1, 2.3.1] 2.4.4
 - TensorFlow version [e.g. 1.5, 1.9.0] 1.14
 - TensorFlowOnSpark version [e.g. 1.1, 1.3.2] master
 - Cluster version [e.g. Standalone, Hadoop 2.8, CDH5] Hadoop 2.8.5

I am running the hadoop/spark installation on AWS EMR at the moment.

**Describe the bug:**

I am trying to run mnist example and I having an issue when performing the data prep, using the tensorflow_datasets package. In my code, `mnist_data_setup.py` loads the data to HDFS as opposed to local file system as seen below,

```
import tensorflow_datasets as tfds
mnist, info = tfds.load('mnist', with_info=True, data_dir='hdfs://default/user/hadoop/tensorflow_datas')
```

Perhaps the exception (shown below) is not pertaining to TensorflowOnSpark directly, but I wanted to see @leewyang can provide some advise/assistance here. Appreciate your time.

**Logs:**

I am receiving the following when running the spark application.
```
loadFileSystems error:
(unable to get stack trace for java.lang.NoClassDefFoundError exception: ExceptionUtils::getStackTrace error.)
hdfsBuilderConnect(forceNewInstance=0, nn=default, port=0, kerbTicketCachePath=(NULL), userName=(NULL)) error:
```

**Spark Submit Command Line:**

I have tried various variations, including providing `LD_LIBRARY_PATH` to the executor env. 

```
${SPARK_HOME}/bin/spark-submit  --deploy-mode cluster \
--queue default --num-executors 4 \
--conf spark.executorEnv.CLASSPATH=$(hadoop classpath --glob) \
--executor-memory 4G --archives mnist/mnist.zip#mnist \
--jars hdfs:///user/${USER}/tensorflow-hadoop-1.10.0.jar,hdfs:///user/${USER}//spark-tensorflow-connector_2.11-1.10.0.jar \
TensorFlowOnSpark/examples/mnist/mnist_data_setup.py \
--output cluster --format tfr
```

I have performed the `hadoop classpath --glob` and verified that the full list of jars are present on both master and slave nodes.


Weird part is that when running the same python snippet on pyspark shell (after setting up `CLASSPATH`), it seems run perfectly fine.

```
import tensorflow_datasets as tfds
mnist, info = tfds.load('mnist', with_info=True, data_dir='hdfs://default/user/hadoop/tensorflow_datas')
```

Is there a known limitation around the length that can be passed via Spark Submit?

Additionally see a related issue [here](https://jira.apache.org/jira/browse/HDFS-4552).
Initial code to leverage Spark 3 GPU resource allocation.

I confirm that this contribution is made under the terms of the license found in the root directory of this repository's source tree and that I have the authority necessary to make this contribution on behalf of its copyright owner.

when execute 

```
export PYTHON_ROOT=./Python
export LD_LIBRARY_PATH=${PATH}
export PYSPARK_PYTHON=${PYTHON_ROOT}/bin/python
export SPARK_YARN_USER_ENV="PYSPARK_PYTHON=Python/bin/python"
export PATH=${PYTHON_ROOT}/bin/:$PATH
export QUEUE=queue_search
export SPARK_HOME=/data/soft/spark2.4
${SPARK_HOME}/bin/spark-submit \
--master yarn \
--deploy-mode cluster \
--queue ${QUEUE} \
--num-executors 4 \
--executor-memory 27G \
--py-files TensorFlowOnSpark/tfspark.zip,TensorFlowOnSpark/examples/mnist/spark/mnist_dist.py \
--conf spark.dynamicAllocation.enabled=false \
--conf spark.yarn.maxAppAttempts=1 \
--archives hdfs:///user/${USER}/Python36Centos6.zip#Python \
TensorFlowOnSpark/examples/mnist/spark/mnist_spark.py \
--images mnist/csv/train/images \
--labels mnist/csv/train/labels \
--mode train \
--model mnist_model
```


org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/data2/emr/yarn/local/usercache/zhaosl/appcache/application_1567671041846_91289/container_e02_1567671041846_91289_01_000003/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/data2/emr/yarn/local/usercache/zhaosl/appcache/application_1567671041846_91289/container_e02_1567671041846_91289_01_000003/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/data2/emr/yarn/local/usercache/zhaosl/appcache/application_1567671041846_91289/container_e02_1567671041846_91289_01_000001/pyspark.zip/pyspark/rdd.py", line 2499, in pipeline_func
  File "/data2/emr/yarn/local/usercache/zhaosl/appcache/application_1567671041846_91289/container_e02_1567671041846_91289_01_000001/pyspark.zip/pyspark/rdd.py", line 2499, in pipeline_func
  File "/data2/emr/yarn/local/usercache/zhaosl/appcache/application_1567671041846_91289/container_e02_1567671041846_91289_01_000001/pyspark.zip/pyspark/rdd.py", line 2499, in pipeline_func
  File "/data2/emr/yarn/local/usercache/zhaosl/appcache/application_1567671041846_91289/container_e02_1567671041846_91289_01_000001/pyspark.zip/pyspark/rdd.py", line 352, in func
  File "/data2/emr/yarn/local/usercache/zhaosl/appcache/application_1567671041846_91289/container_e02_1567671041846_91289_01_000001/pyspark.zip/pyspark/rdd.py", line 801, in func
  File "/data2/emr/yarn/local/usercache/zhaosl/appcache/application_1567671041846_91289/container_e02_1567671041846_91289_01_000001/tfspark.zip/tensorflowonspark/TFSparkNode.py", line 404, in _train
AttributeError: 'AutoProxy[get_queue]' object has no attribute 'put'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
spark-submit \
  --verbose \
  $(pwd)/TensorFlowOnSpark/examples/mnist/mnist_data_setup.py \
  --output /user/hdfs/jupyter/mnist_kerberos/csv

Errors with:

python: can't open file '/TensorFlowOnSpark/examples/mnist/spark/mnist_spark.py': [Errno 2] No such file or directory

Thanks


Normally, TFoS launches the TF process as the user who started the Spark application.  However, we've seen in some cases that it appears that the TF process is running as the "yarn" user, which can cause issues with writing the model file to the user's HDFS directory.

The workaround is to create an HDFS directory that grants write permissions to the "yarn" user.