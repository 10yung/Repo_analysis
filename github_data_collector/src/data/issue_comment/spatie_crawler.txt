Hello,
I receive this answer when I'm using the crawler like this:
![image](https://user-images.githubusercontent.com/46482024/69042170-6ee17f00-09f9-11ea-9baf-4d890be9e357.png)
![image](https://user-images.githubusercontent.com/46482024/69042202-7c970480-09f9-11ea-81c1-9b97222e9bcd.png)

How to continue to a page that gets status code 302 and redirected?
I'm creating this issue to keep track of the tasks for v5.

## New features

- [ ] proper support for persistent (database-backed) queues (#137, #200), allowing to perform a crawl in discrete steps at different points in time, and resume an interrupted crawl; note that this would benefit a lot from #237 (putting a failed URL back onto the persistent crawl queue) if you're willing to revisit this
- [ ] automatically set the `Referer` header
- [ ] (to be discussed) ship a PDO implementation of a database-backed crawl queue, which would also serve as a reference implementation

## Fixes

- [ ] fix the performance issues (#210, #241, and as a result, #236) (positive side effect: removing one dependency)

## Code improvements

- [X] Get rid of `$id` in `CrawlUrl`: Guzzle's `Pool` [can yield string keys](https://github.com/guzzle/guzzle/pull/1203) as well, we do not need an int, and can use the URL as the key to greatly simplify the code - **implemented** in #244
- [ ] Improve performance of `CollectionCrawlQueue` by using an indexed array rather than [looping through the URLs](https://github.com/spatie/crawler/blob/67974a810f6a342c333c4e348fcfad7c1375ffde/src/CrawlQueue/CollectionCrawlQueue.php#L103-L117)
- [ ] Add return types wherever possible (example: `CrawlQueue::getFirstPendingUrl() : ?CrawlUrl`)
- [ ] Change signature of `CrawlQueue::has` to `(CrawlUrl $crawlUrl)` to be consistent with the other methods
- [ ] Decouple `CrawlRequestFailed|FulfilledClass` from Guzzle by replacing it with an interface; this will allow better static analysis than a invokable class name, and make the contract more explicit, such as: `public function crawlFailed(RequestException $exception, CrawlUrl $url)`
- [ ] Make `CrawlProfile` an interface instead of an abstract class

## Cleanup

- [ ] remove deprecated methods (`Crawler::endsWith()`)
- [ ] If possible, get rid of `CrawlQueue::get()`, introduced in #244 as a temporary workaround for Guzzle's inability to use objects as keys
I've been experiencing very slow crawling after a few hundred pages.

To ensure that the problem did not come from the client-server connection, I used a [cache middleware](https://github.com/Kevinrob/guzzle-cache-middleware) with a [greedy cache strategy](https://github.com/Kevinrob/guzzle-cache-middleware#greedy-caching), and performed a dry run to store all the pages into the cache before doing any performance testing.

Here's a scatter plot of the time it takes to crawl a single page:

![crawl-time-plot](https://user-images.githubusercontent.com/1952838/59940143-334a8500-945a-11e9-9d7b-bd393d72d01e.png)

**It takes an average of 10s to process a single page after only 300 pages!**

To understand what made it this slow, I ran the xdebug profiler, here is the result:

![xdebug-profile](https://user-images.githubusercontent.com/1952838/59940275-93412b80-945a-11e9-929e-fa4b367f6d98.png)

It looks like recursive calls to `Crawler::addToDepthTree()` are eating 95% of the CPU and make the crawling shockingly slow.

Before I dig further into the code (I'm not yet familiar with it), **is there any easy fix that comes to your mind, to speed this process up?**

*Note: I cannot attach the cachegrind file, as it's 2.5GB.*