Thanks a bunch for this package. I sometimes have a hyperparameter value that is a Nonetype, but TBX errors when it is passed in. 

I was wondering what the right way to handle this is. Maybe in summary.py, this case should be handled as a `type=DataType.DATA_TYPE_UNSET`?

```
In [2]: from tensorboardX import summary

In [3]: summary.hparams
Out[3]: <function tensorboardX.summary.hparams(hparam_dict=None, metric_dict=None)>

In [4]: summary.hparams({"a": None}, {"score": 1})
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
<ipython-input-4-92757b992968> in <module>
----> 1 summary.hparams({"a": None}, {"score": 1})

~/miniconda3/lib/python3.7/site-packages/tensorboardX/summary.py in hparams(hparam_dict, metric_dict)
    105
    106         if not isinstance(v, int) or not isinstance(v, float):
--> 107             v = make_np(v)[0]
    108             ssi.hparams[k].number_value = v
    109

~/miniconda3/lib/python3.7/site-packages/tensorboardX/x2num.py in make_np(x)
     32         return check_nan(prepare_mxnet(x))
     33     raise NotImplementedError(
---> 34         'Got {}, but expected numpy array or torch tensor.'.format(type(x)))
     35
     36

NotImplementedError: Got <class 'NoneType'>, but expected numpy array or torch tensor.
```
Trying to get the PR AUC from tensorboardx, seems like we only have PR so far ... implement the methods equivalent to tf.metrics.auc
I'd like to annotate on logs, especially what epoch a log belongs to.

Iteration-level (or each nth-iteration) logging makes it hard to know what epoch a specific data point is. If I do epoch-level logging, it leads to a low number of observations and I have to wait quite long for information. 

An obvious solution would be to provide an additional parameter or add tuple support for global_step. Is there a reason why this is not possible yet?

This issue is related to https://github.com/lanpa/tensorboardX/issues/269 , but I don't think the questions was properly answered.
I am experiencing crashes/freezing in my keras code. The code freezes without an error, it just stops. 

Code i am using

`self.writer = SummaryWriter("Tensy")`
`self.writer.add_scalars('Softmax', {"Action 1" : predicted_action[0][0], "Action 2" : predicted_action[0][1], "Action 3" : predicted_action[0][2]}, self.gstep)	`

I've calculated that it is around the flush time that it crashes wheter its the first flush round or later in the run... nonetheless it crashes every run, making my run proccess unreliable to do a long run. If it succeeds to save, then there is nothing wrong with the file itself. Are there any known issues of tensorboardx freezing code? I didn't want to report this as a bug, because it might just me making some mistake.

Hope someone can help



**Describe the bug**
For function `add_embedding()`, when the input label is using utf-8 Chinese characters, it can export tensorboard log file. However, when you check the projector using `tensorboard --logdir runs`, it crashed to load data, raising exception

```txt
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xb0 in position 0: invalid start byte
``` 

**Minimal runnable code to reproduce the behavior**
```
# -*- coding: utf-8 -*-
from tensorboardX import SummaryWriter

writer = SummaryWriter()
writer.add_embedding(np.random.random([2, 3]), ['‰∏Ä', '‰∫å'])
writer.close()
```
Note that '‰∏Ä', '‰∫å' are Chinese characters.

**Expected behavior**
The exported tensorboard log file uses utf-8 encode, so that tensorboard can show Chinese or other non-English words.


**Environment**
protobuf           3.9.2
tensorboardX       1.9
torch              1.3.0


**Python environment**
Anaconda py3.6
## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:

1.Run my script below:
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
# from torch.utils.tensorboard import SummaryWriter
from tensorboardX import SummaryWriter

# bug 1: bool type inputs
class Net_1(nn.Module):
    def __init__(self, dropout=0.5):
        super(Net_1, self).__init__()
        self.fc1 = nn.Linear(120, 84)
        self.fc2 = nn.Linear(84, 10)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, use_dropout=False):
        x = F.relu(self.fc1(x))
        if use_dropout:
            x = self.dropout(x)  # or other operations ....
        x = F.relu(self.fc2(x))
        return x

with SummaryWriter("bugs") as w:
    net = Net_1()
    input_x = torch.randn((2,120))
    w.add_graph(net, (input_x, True))


# bug 2: None type inputs (might be argument's default value)
class Net_2(nn.Module):
    def __init__(self):
        super(Net_2, self).__init__()
        self.fc1 = nn.Linear(120, 84)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(120, 84)
        self.fc4 = nn.Linear(84, 10)

    def forward(self, x, y=None, z=None):
        x = F.relu(self.fc1(x))
        if y is not None:
            y = F.relu(self.fc2(y))
            x = x + y
        if z is not None:
            z = F.relu(self.fc3(z))
            x = x + z
        x = F.relu(self.fc4(x))
        return x

with SummaryWriter("bugs") as w:
    net = Net_2()
    input_x = torch.randn((2,120))
    input_y = None
    input_z = torch.randn((2,120))
    w.add_graph(net, (input_x, input_y, input_z))


# bug 3: List type inputs (dict, or other python build-in types like int,str,... may also meet this question)
class Net_3(nn.Module):
    def __init__(self):
        super(Net_3, self).__init__()
        self.fc_list = [nn.Linear(120, 120) for _ in range(10)]
        self.fc_n = nn.Linear(120, 10)

    def forward(self, x, index:list=None):
        if index is not None:
            for i in index:
                x = F.relu(self.fc_list[i](x))
        x = F.relu(self.fc_n(x))
        return x

with SummaryWriter("bugs") as w:
    net = Net_3()
    input_x = torch.randn((2, 120))
    index = [1, 5, 1, 7, 0]
    w.add_graph(net, (input_x, index))

```

and you can see the trace(take bug 3 as an example):
```
Error occurs, No graph saved
Traceback (most recent call last):
  File "<input>", line 1, in <module>
  File "/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_bundle/pydev_umd.py", line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File "/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py", line 18, in execfile
    exec(compile(contents+"\n", file, 'exec'), glob, loc)
  File "/Users/wangyuanzheng/Downloads/xxxxxxx/project/albert_pytorch/dev/add_graph_bug.py", line 25, in <module>
    w.add_graph(net, (input_x, True))
  File "/Users/wangyuanzheng/anaconda3/envs/CCFBigData-torch/lib/python3.7/site-packages/torch/utils/tensorboard/writer.py", line 682, in add_graph
    self._get_file_writer().add_graph(graph(model, input_to_model, verbose))
  File "/Users/wangyuanzheng/anaconda3/envs/CCFBigData-torch/lib/python3.7/site-packages/torch/utils/tensorboard/_pytorch_graph.py", line 239, in graph
    raise e
  File "/Users/wangyuanzheng/anaconda3/envs/CCFBigData-torch/lib/python3.7/site-packages/torch/utils/tensorboard/_pytorch_graph.py", line 234, in graph
    trace = torch.jit.trace(model, args)
  File "/Users/wangyuanzheng/anaconda3/envs/CCFBigData-torch/lib/python3.7/site-packages/torch/jit/__init__.py", line 858, in trace
    check_tolerance, _force_outplace, _module_class)
  File "/Users/wangyuanzheng/anaconda3/envs/CCFBigData-torch/lib/python3.7/site-packages/torch/jit/__init__.py", line 997, in trace_module
    module._c._create_method_from_trace(method_name, func, example_inputs, var_lookup_fn, _force_outplace)
RuntimeError: Type 'Tuple[Tensor, bool]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced (toTraceableIValue at ../torch/csrc/jit/pybind_utils.h:298)
frame #0: c10::Error::Error(c10::SourceLocation, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 135 (0x110c479e7 in libc10.dylib)
frame #1: torch::jit::toTraceableIValue(pybind11::handle) + 1280 (0x110246740 in libtorch_python.dylib)
frame #2: torch::jit::toTypedStack(pybind11::tuple const&) + 31 (0x1102e7edf in libtorch_python.dylib)
frame #3: void pybind11::cpp_function::initialize<torch::jit::script::initJitScriptBindings(_object*)::$_16, void, torch::jit::script::Module&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, pybind11::function, pybind11::tuple, pybind11::function, bool, pybind11::name, pybind11::is_method, pybind11::sibling>(torch::jit::script::initJitScriptBindings(_object*)::$_16&&, void (*)(torch::jit::script::Module&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, pybind11::function, pybind11::tuple, pybind11::function, bool), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::__invoke(pybind11::detail::function_call&) + 147 (0x11031e4e3 in libtorch_python.dylib)
frame #4: pybind11::cpp_function::dispatcher(_object*, _object*, _object*) + 3372 (0x10fe57d3c in libtorch_python.dylib)
<omitting python frames>
```
<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->
writer.add_graph should run normally.

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:
```
wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py
# For security purposes, please check the contents of collect_env.py before running it.
python collect_env.py
```

 Collecting environment information...
PyTorch version: 1.3.0
Is debug build: No
CUDA used to build PyTorch: None
OS: Mac OSX 10.14.6
GCC version: Could not collect
CMake version: Could not collect
Python version: 3.7
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
Versions of relevant libraries:
[pip] numpy==1.17.2
[pip] torch==1.3.0
[pip] torchvision==0.4.1
[conda] torch                     1.3.0                    pypi_0    pypi
[conda] torchvision               0.4.1                    pypi_0    pypi

## Additional context

<!-- Add any other context about the problem here. -->
1.TensorboardX.SummaryWriter.add_graph has the same bug as torch.utils.tensorboard.SummaryWriter.add_graph
2.Besides this bug, I hope add_graph could accept not only a tuple as positional arguments, but also a dict as keyword arguments for the model.forward()'s input
add_hparam creates a subdirectory with name time.time(). In tensorboard, this shows up as an individual run. Is this intended behavior? I think it would be much cleaner to put it in the same events file, or -if that is not possible- at least in the same directory of the actual run.

Having add_scalars as different runs makes sense, I guess. Different behavior could be implemented using Custom Scalars. But for HParams, I do not see why it should be a separate run
When I projecting an embedding with different labels, for example:
```python
writer.add_embedding(same_embedding, labels_str_two,
                             tag=f'labels_str_two')
writer.add_embedding(same_embedding, labels_str_one, tag='labels_str_one')
```
I got two different pictures, just like these two pictures. So why relatively distances between points are different when projecting one embedding with different labels?

![image](https://user-images.githubusercontent.com/22348625/66125565-dbefaf80-e619-11e9-97e0-9a9538b07d68.png)
![image](https://user-images.githubusercontent.com/22348625/66125575-e447ea80-e619-11e9-88da-c7364888626f.png)



Hi there, 
I am not sure if you mentioned it somewhere, but I was thinking about adding information about hyperparameters to my summaries. Problem is, that I already have 20+ training runs and I would just like to reopen those files and add the information to them. 

I can write a script for that, but is it possible to open and add information to an existing summary?

Thank you,
Christian