I am having quite a bit of trouble trying to create a minimum working example for the RTS game, in which I can get a single state, visualize its variables and pass an action back to the environment.

I took a look at the Basic Usage section of the `README.md`, but it looks like the code is not supposed to be used, just interpreted as pseudocode. For example, I don't know where functions such as `Init()` come from, or what is the type and attributes of the `context` variable.

I then took a look at `train.py` and `eval.py`, which use the RTS, but the code is highly abstracted and everything is passed as dictionaries, whose values don't seem to have an explicit documentation. Also, these scripts don't show how a user can get a state and return an action to the environment (or how to work with batches).

The closest I came to an answer was the [documentation of the Python wrapper](http://yuandong-tian.com/html_elf/wrapper-python.html). The page explicitly mentions variables with names such as `["s", "pi", "r", "a"]`, which I believe are states, rewards, actions and policies, but there is not a full example depicting how to get to these values, only code excerpts which must be pieced together and make no sense on their own.

I believe the project could benefit greatly from a documented minimum working example guiding users on creating their own agent. Maybe create a random agent for the RTS game, which others can then replace for their own agents.

I would like to add that I was able to compile the project and run the train and eval examples both on CPU and GPU, on Python 3.7 under Linux. My frustration comes from being unable to customize anything.
This PR aims to fix the CI build of the repo on Travis.

I have listed my findings on #136. Basically, Tower Defense (TD) and Capture the Flag (CF) are not compiling since October 2018, due to a commit that made breaking changes in the elf base library in order to add features to the main RTS game (MC).

It looks to me as though TD and CF are abandoned, so I've fixed the build of the MC module (which was also broken for a different reason) on `.travis.yml` and created this PR to discuss what to do with TD and CF.
I was trying to fix the CI build. I noticed that an error occurs in the .travis.yml file in the following lines:

```
- (mkdir rts/build_MC && cd rts/build_MC && cmake .. -DGAME_DIR=../game_MC && make)
- (mkdir rts/build_CF && cd rts/build_CF && cmake .. -DGAME_DIR=../game_CF && make)
- (mkdir rts/build_TD && cd rts/build_TD && cmake .. -DGAME_DIR=../game_TD && make)
```

The first line had a problem related to using the wrong Python executable, so I fixed it by pointing it towards what looks like the right executable inside the Travis container using `-DPYTHON_EXECUTABLE=/opt/pyenv/shims/python`.

The following two lines, however, are attempting to compile the contents of the build_CF and build_TD directories, which appear to have incomplete files. Can anyone tell me what these directories contain?
In this PR, I've deleted all mentions to the `async` keyword in the master branch of ELF to resume compatibility of the repository with Python 3.7

ELF uses `async` as a name for variables and function parameters. In Python 3.7, `async` has become a reserved word. The word is only used as argument to the `copy_` and `cuda` methods, which are methods from the `pytorch.Tensor` object. But these methods don't use the value of the `async` argument for anything.

By removing mentions to the now-reserved keyword, no change in the behavior of the package is expected. I've tested training a model using `./train_minirts.sh` on Python 3.7 with both CPU and GPU and everything worked. I believe this fixes #124 and #127.

I created a new PR because I used my master branch as the head repository in #134, which was a bad idea on my side.
 HI,Can you provide the code for pytorch version 0.4.0 ?
When I trained, I found that the computer memory occupancy was increasing.I don't know what the problem is.
Now there is only selfplay or training with AI-SIMPLE.
If I want to train with different type RL models, e.g. a player using ppo and the other one using A3C,
how can I deal with it?
I started training miniRTS - like 2 weeks ago. Without interrupting, for now I have (just the last part pasted):

```
[trainer] actor count: 4750/5000
new_record: False
count: 2905
best_win_rate: 0.7521728917077578
str_win_rate: [2905] Win rate: 0.727 [3103/1165/4268], Best win rate: 0.752 [2754]
str_acc_win_rate: Accumulated win rate: 0.667 [8208765/4101445/12310210]
100%|█████████| 5000/5000 [07:34<00:00, 11.00it/s]
[2019-04-20 14:31:03.794200][64] Iter[2906]: 
Save to ./
Filename = ./save-726740.bin
Command arguments train.py --batchsize 64 --freq_update 1 --players type=AI_NN,fs=50,args=backup/AI_SIMPLE|start/500|decay/0.99;type=AI_SIMPLE,fs=20 --num_games 1024 --tqdm --T 20 --additional_labels id,last_terminal --trainer_stats winrate --keys_in_reply V --gpu 0
[2906] Time spent = 454386.121000 ms
actor: 4750/5000
train: 250/5000
2906:V_err[4750]: avg: 0.03603, min: 0.00120[3002], max: 0.13680[1537]
2906:acc_reward[4750]: avg: 0.33279, min: 0.12162[3137], max: 0.51011[116]
2906:cost[250]: avg: 1.84572, min: 1.75103[225], max: 1.91308[153]
2906:entropy_pi[4750]: avg: -1.81812, min: -1.97498[4702], max: -1.58594[3505]
2906:init_reward[250]: avg: 0.33657, min: 0.18084[165], max: 0.47968[192]
2906:nll_pi[4750]: avg: 1.82787, min: 1.50373[3047], max: 2.29496[1670]
2906:predicted_V[4750]: avg: 0.33951, min: -1.25702[2216], max: 1.27786[2678]
2906:reward[4750]: avg: 0.00634, min: -0.04688[1216], max: 0.06250[1082]
2906:total_entropy[4750]: avg: -1.81812, min: -1.97498[4702], max: -1.58594[3505]
2906:total_nll[4750]: avg: 1.82787, min: 1.50373[3047], max: 2.29496[1670]

[trainer] actor count: 4750/5000
new_record: False
count: 2906
best_win_rate: 0.7521728917077578
str_win_rate: [2906] Win rate: 0.724 [3070/1173/4243], Best win rate: 0.752 [2754]
str_acc_win_rate: Accumulated win rate: 0.667 [8211835/4102618/12314453]
```

For now I do not now if there is any 'soft option to finish' in current state to get result for research.

Info about workstation:
Memory: 8 GiB
CPU: Intel Core i5-4590 3.30GHz x4
FeForce GTX 960 2GB GDDR5 (128 bit)
OS: Ubuntu 18.04.2 LTS
`async` is a reserved word in Python 3.7

So, I think change `install instruction` like the following
> conda create -n elf python=3.6 (or 3.x)

or change parameter name('async') to the other.
Hi, thanks for this great project!

I was wondering that which version of pytorch should I use for atari?
I use 0.2 and it works well for minirts.
But when I eval atari breakout.bin with pretrained model, these messages would appear:

> Load from /home/ubuntu/atari_breakout.bin
/home/ubuntu/miniconda3/envs/elf/lib/python3.6/site-packages/torch/serialization.py:286: SourceChangeWarning: source code of class 'model.Model_ActorCritic' has changed. you can retieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.
  warnings.warn(msg, SourceChangeWarning)
Traceback (most recent call last):
  File "eval.py", line 24, in <module>
    model = env["model_loaders"][0].load_model(GC.params)
  File "/home/ubuntu/ELF/rlpytorch/model_loader.py", line 82, in load_model
    model.load(self.load, omit_keys=omit_keys)
  File "/home/ubuntu/ELF/rlpytorch/model_base.py", line 93, in load
    data = torch.load(filename)
  File "/home/ubuntu/miniconda3/envs/elf/lib/python3.6/site-packages/torch/serialization.py", line 231, in load
    return _load(f, map_location, pickle_module)
  File "/home/ubuntu/miniconda3/envs/elf/lib/python3.6/site-packages/torch/serialization.py", line 379, in _load
    result = unpickler.load()
ModuleNotFoundError: No module named 'rlmethod_common'

I guess this is because I use a wrong pytorch version for atari? 
And if I use a model trained by myself, it will just get stuck in 0%:
(I don't use a GPU for atari)

> Running ROM file...
Random seed is 23468651
Action set: 0 1 3 4 11 12
Version:  1f790173095cd910976d9f651b80beb872ec5d12_staged
Num Actions:  6
#recv_thread = 4
Group 0:
  Collector[0] Batchsize: 32 Info: [gid=0][T=1][name=""]
  Collector[1] Batchsize: 32 Info: [gid=1][T=1][name=""]
  Collector[2] Batchsize: 32 Info: [gid=2][T=1][name=""]
  Collector[3] Batchsize: 32 Info: [gid=3][T=1][name=""]
Load from /home/ubuntu/save-831.bin
Loaded = /home/ubuntu/save-831.bin
  0%|                     | 0/500 [13:24<?, ?it/s]