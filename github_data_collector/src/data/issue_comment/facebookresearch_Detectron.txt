Hello, I have successfully built the docker with the Dockerfile. But when i tried to run the infer_simply.py, I faced this error. Does anyone know how to fix this? Thanks in advance.

> root@localhost:/detectron# python tools/infer_simple.py --cfg deploy_file/R50FPN.yaml --output-dir /tmp/detectron-visualizations --image-ext jpg --wts deploy_file/model_final.pkl demo
Found Detectron ops lib: /usr/local/caffe2_build/lib/libcaffe2_detectron_ops_gpu.so
WARNING cnn.py:  40: [====DEPRECATE WARNING====]: you are creating an object from CNNModelHelper class which will be deprecated soon. Please use ModelHelper object with brew module. For more information, please refer to caffe2.ai and python/brew.py, python/brew_test.py for more information.
INFO net.py:  59: Loading weights from: deploy_file/model_final.pkl
I0110 01:42:28.716251    47 net_dag_utils.cc:118] Operator graph pruning prior to chain compute took: 9.7481e-05 secs
I0110 01:42:28.716440    47 net_dag.cc:61] Number of parallel execution chains 63 Number of operators = 232
I0110 01:42:28.726867    47 net_dag_utils.cc:118] Operator graph pruning prior to chain compute took: 6.9024e-05 secs
I0110 01:42:28.726986    47 net_dag.cc:61] Number of parallel execution chains 30 Number of operators = 188
I0110 01:42:28.728276    47 net_dag_utils.cc:118] Operator graph pruning prior to chain compute took: 9.689e-06 secs
I0110 01:42:28.728312    47 net_dag.cc:61] Number of parallel execution chains 5 Number of operators = 18
INFO infer_simple.py: 113: Processing demo/15673749081_767a7fa63a_k.jpg -> /tmp/detectron-visualizations/15673749081_767a7fa63a_k.jpg.pdf
terminate called after throwing an instance of 'caffe2::EnforceNotMet'
  what():  [enforce fail at conv_op_cudnn.cc:572] status == CUDNN_STATUS_SUCCESS. 8 vs 0. , Error at: /var/lib/jenkins/workspace/caffe2/operators/conv_op_cudnn.cc:572: CUDNN_STATUS_EXECUTION_FAILED Error from operator: 
input: "gpu_0/data" input: "gpu_0/conv1_w" output: "gpu_0/conv1" name: "" type: "Conv" arg { name: "kernel" i: 7 } arg { name: "exhaustive_search" i: 0 } arg { name: "pad" i: 3 } arg { name: "order" s: "NCHW" } arg { name: "stride" i: 2 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: "CUDNN"
*** Aborted at 1578620552 (unix time) try "date -d @1578620552" if you are using GNU date ***
PC: @     0x7fce729dd428 gsignal
*** SIGABRT (@0x2f) received by PID 47 (TID 0x7fcdc1fff700) from PID 47; stack trace: ***
    @     0x7fce72d83390 (unknown)
    @     0x7fce729dd428 gsignal
    @     0x7fce729df02a abort
    @     0x7fce6c39e84d __gnu_cxx::__verbose_terminate_handler()
    @     0x7fce6c39c6b6 (unknown)
    @     0x7fce6c39c701 std::terminate()
    @     0x7fce6c3c7d38 (unknown)
    @     0x7fce72d796ba start_thread
    @     0x7fce72aaf41d clone
    @                0x0 (unknown)
Aborted (core dumped)

is it possible to write output file in .npy format?

### Actual results
I download `gtFine_trainvaltest.zip, leftImg8bit_trainvaltest.zip, leftImg8bit_trainvaltest_foggy.zip, leftImg8bit_trainval_rain.zip` from [https://www.cityscapes-dataset.com/](https://www.cityscapes-dataset.com/). And I use `tools/convert_cityscapes_to_coco.py`, also succeed to convert the datasets.
Then I want to use the faster rcnn in mmdetection to do the control experiment, but the results are too low. In train, I use images in `leftImg8bit_trainvaltest.zip`, and in val or test, I use images in `leftImg8bit_trainvaltest_foggy.zip` or `leftImg8bit_trainval_rain.zip`. So, I want to ask for the `.json`, or maybe I forget to do something?
The results I do is :

dataset  | Person | Rider | Car | Bus | Truck | Bicycle | Mcycle | Train | mAP
-- | -- | -- | -- | -- | -- | -- | -- | -- | --
rainy | 1.2 | 1.5 | 1.8 | 1.8 | 0.3 | 1.0 | 0 | 0 | 1.8
foggy | 9.1 | 9.4 | 14.2 | 9.4 | 4.5 | 7.5 | 4.2 | 1.2 | 14.6

### Expected results
I read the paper, and their results (in foggy) are:

Paper name | Person | Rider | Car | Bus | Truck | Bicycle | Mcycle | Train | mAP
-- | -- | -- | -- | -- | -- | -- | -- | -- | --
A Robust Learning Approach to Domain Adaptive Object Detection | 31.69 | 39.41 | 45.81 | 23.86 | 39.34 | 20.64 | 22.26 | 32.36 | 31.92
Domain Adaptive Faster R-CNN for Object Detection in the Wild | 17.8 | 23.6 | 27.1 | 23.8 | 11.9 | 22.8 | 14.4 | 9.1 | 18.8





I find some bugs in this file which is named by Detectron/detectron/datasets/voc_eval.py.
From line 213 on:
   # compute precision recall
    fp = np.cumsum(fp)
    tp = np.cumsum(tp)
    rec = tp / float(npos)
    # avoid divide by zero in case the first detection matches a difficult
    # ground truth
    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)
    ap = voc_ap(rec, prec, use_07_metric)

we can see that the rec and prec is  not a list but a float number.This means the calculation of ap is not the area of PR curve.

So could you please tell me how to fix or make sense of this question.

A recent Detectron commit added an import statement that throws an Exception on my system when using tools/convert_pkl_to_pb.py:

```
$ cd Detectron
$ python3.6 tools/convert_pkl_to_pb.py -h
Traceback (most recent call last):
  File "tools/convert_pkl_to_pb.py", line 44, in <module>
    from caffe2.caffe2.fb.predictor import predictor_exporter, predictor_py_utils
ModuleNotFoundError: No module named 'caffe2.caffe2'
```
Commit: https://github.com/facebookresearch/Detectron/commit/a6a835f5b8208c45d0dce217ce9bbda915f44df7#diff-077e2ae71af52724d94c3522525d2e7fR44

Specifically, this is the problematic import:
from caffe2.caffe2.fb.predictor import predictor_exporter, predictor_py_utils

I am using this pytorch github commit: c3c0dcf6e35efdb843587a03522c37a7d1184539
I verified that, even on the latest pytorch commit (756f279d95d14729e52ccbc4a8d4167725dcaac3), there is no python package `caffe2.caffe2.fb.predictor`.

One simple workaround fix is to replace the problematic import with the following:
`from caffe2.python.predictor import predictor_exporter, predictor_py_utils`

I'm unsure if `caffe2.caffe2.fb.predictor` contains special functionality required for the logfiledb stuff.

### Expected results

What did you expect to see?

I expected convert_pkl_to_pb.py to not throw an ImportError

### Actual results

What did you observe instead?

I saw an ImportError.

### Detailed steps to reproduce

Clone + install Detectron (and pytorch/caffe2), using the above git hashes.
Then, run the following commands:
```
cd Detectron/
python3.6 tools/convert_pkl_to_pb.py -h
```

### System information
This issue is not related to CUDA/GPU.

* Operating system: Ubuntu 16.04
* Compiler version: ?
* CUDA version: ?
* cuDNN version: ?
* NVIDIA driver version: ?
* GPU models (for all devices if they are not all the same): ?
* `PYTHONPATH` environment variable: ?
* `python --version` output: Python 3.6.3
* Anything else that seems relevant: ?

I have already run my own mask rcnn, including training and testing. 
Now, I want to add a new branch to the network to make another prediction parallel to cls, bbox, and mask. However, the caffe2 program confuses me a lot, I don't know where should I make a change...

So... could you please give me some advice? Thanks..
I run convert_pkl_to_pb.py successfully but only get faster rcnn pb model, without mask layers.
Please help. 

Hi,
I use "tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml" to training,
Q1: Why the AP of result is 0 or -1 ? 
(I had checked my json file was corrected.) 
VOC2COCO
------------------------------------------------------
import os
import argparse
import json
import xml.etree.ElementTree as ET
from typing import Dict, List
from tqdm import tqdm
import re


def get_label2id(labels_path: str) -> Dict[str, int]:
    """id is 1 start"""
    with open(labels_path, 'r') as f:
        labels_str = f.read().split()
    labels_ids = list(range(1, len(labels_str)+1))
    return dict(zip(labels_str, labels_ids))


def get_annpaths(ann_dir_path: str = None,
                 ann_ids_path: str = None,
                 ext: str = '',
                 annpaths_list_path: str = None) -> List[str]:
    # If use annotation paths list
    if annpaths_list_path is not None:
        with open(annpaths_list_path, 'r') as f:
            ann_paths = f.read().split()
        return ann_paths

    # If use annotaion ids list
    ext_with_dot = '.' + ext if ext != '' else ''
    with open(ann_ids_path, 'r') as f:
        ann_ids = f.read().split()
    ann_paths = [os.path.join(ann_dir_path, aid+ext_with_dot) for aid in ann_ids]
    return ann_paths


def get_image_info(annotation_root, extract_num_from_imgid=True):
    path = annotation_root.findtext('path')
    if path is None:
        filename = annotation_root.findtext('filename')
    else:
        filename = os.path.basename(path)
    img_name = os.path.basename(filename)
    img_id = os.path.splitext(img_name)[0]
    if extract_num_from_imgid and isinstance(img_id, str):
        img_id = int(re.findall(r'\d+', img_id)[0])

    size = annotation_root.find('size')
    width = int(size.findtext('width'))
    height = int(size.findtext('height'))

    image_info = {
        'file_name': filename,
        'height': height,
        'width': width,
        'id': img_id
    }
    return image_info


def get_coco_annotation_from_obj(obj, label2id):
    label = obj.findtext('name')
    #assert label in label2id, f"Error: {label} is not in label2id !"
    
    category_id = label2id[label]
    bndbox = obj.find('bndbox')
    xmin = int(bndbox.findtext('xmin')) 
    ymin = int(bndbox.findtext('ymin')) 
    xmax = int(bndbox.findtext('xmax')) 
    ymax = int(bndbox.findtext('ymax')) 
    #assert xmax > xmin and ymax > ymin, f"Box size error !: (xmin, ymin, xmax, ymax): {xmin, ymin, xmax, ymax}"
    o_width = xmax - xmin
    o_height = ymax - ymin
    ann = {
        'area': o_width * o_height,
        'iscrowd': 0,
        'bbox': [xmin, ymin, o_width, o_height],
        'category_id': category_id,
        'ignore': 0,
        'segmentation': []  # This script is not for segmentation
    }
    return ann


def convert_xmls_to_cocojson(annotation_paths: List[str],
                             label2id: Dict[str, int],
                             output_jsonpath: str,
                             extract_num_from_imgid: bool = True):
    output_json_dict = {
        "images": [],
        "type": "instances",
        "annotations": [],
        "categories": []
    }
    bnd_id = 1  # START_BOUNDING_BOX_ID, TODO input as args ?
    print('Start converting !')
    for a_path in tqdm(annotation_paths):
        # Read annotation xml
        ann_tree = ET.parse(a_path)
        ann_root = ann_tree.getroot()

        img_info = get_image_info(annotation_root=ann_root,
                                  extract_num_from_imgid=extract_num_from_imgid)
        img_id = img_info['id']
        output_json_dict['images'].append(img_info)

        for obj in ann_root.findall('object'):
            ann = get_coco_annotation_from_obj(obj=obj, label2id=label2id)
            ann.update({'image_id': img_id, 'id': bnd_id})
            output_json_dict['annotations'].append(ann)
            bnd_id = bnd_id + 1

    for label, label_id in label2id.items():
        category_info = {'supercategory': 'none', 'id': label_id, 'name': label}
        output_json_dict['categories'].append(category_info)

    with open(output_jsonpath, 'w') as f:
        output_json = json.dumps(output_json_dict)
        f.write(output_json)

#python3
def main():
    parser = argparse.ArgumentParser(#test_20/ #train/
        description='This script support converting voc format xmls to coco format json')
    parser.add_argument('--ann_dir', type=str, default='ch/train/xml/',
                        help='path to annotation files directory. It is not need when use --ann_paths_list')
    parser.add_argument('--ann_ids', type=str, default='ch/train/name.txt',
                        help='path to annotation files ids list. It is not need when use --ann_paths_list')
    parser.add_argument('--ann_paths_list', type=str, default='ch/train/path.txt',
                        help='path of annotation paths list. It is not need when use --ann_dir and --ann_ids')
    parser.add_argument('--labels', type=str, default='ch/train/class.txt',
                        help='path to label list.')
    parser.add_argument('--output', type=str, default='ch/train/train.json', help='path to output json file')
    parser.add_argument('--ext', type=str, default='', help='additional extension of annotation file')
    args = parser.parse_args()
    label2id = get_label2id(labels_path=args.labels)
    ann_paths = get_annpaths(
        ann_dir_path=args.ann_dir,
        ann_ids_path=args.ann_ids,
        ext=args.ext,
        annpaths_list_path=args.ann_paths_list
    )
    convert_xmls_to_cocojson(
        annotation_paths=ann_paths,
        label2id=label2id,
        output_jsonpath=args.output,
        extract_num_from_imgid=True
    )
if __name__ == '__main__':
    main()
-------------------------------------------------------------
Q2 : Why "accuracy_cls" is 0.98 but the classification of test are error ?
Q3 : What is "Filtered 0 roidb entries: 56370 -> 56370" mean  ?

Thanks
When I run ‘’setup.py' ,  I get an error: can't find fvcore.

```
Searching for fvcore@ git+https://github.com/facebookresearch/fvcore.git
Reading https://pypi.org/simple/fvcore/
Couldn't find index page for 'fvcore' (maybe misspelled?)
Scanning index of all packages (this may take a while)
Reading https://pypi.org/simple/
Download error on https://pypi.org/simple/: [Errno -2] Name or service not known -- Some packages may not be found!
No local packages or working download links found for fvcore@ git+https://github.com/facebookresearch/fvcore.git
error: Could not find suitable distribution for Requirement.parse('fvcore@ git+https://github.com/facebookresearch/fvcore.git')
```


I try to manually installing it :
`pip install git+https://github.com/facebookresearch/fvcore.git`
and i got feedback：
`Successfully built fvcore`

but after that, the same question still exist, what shhould i do?  

Default mode for tempfile is w+b, but yaml.dump gives a string