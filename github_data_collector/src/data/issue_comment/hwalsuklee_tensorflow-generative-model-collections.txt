Thank you for sharing your codes.

I found that there is no weight clipping in the paper Least Squares Generative Adversarial Networks. The weight clipping is used in the paper Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities. Obviously, your code is the former.

I tried both situations, and the performance is better without weight clipping.

I changed the network structure and adjusted the super-parameters to apply to data cifar-10. I thought maybe someone needed it, so I left the url of my code.
https://github.com/AliceAria/Performance-comparison-of-GAN-on-cifar-10

Thanks again.
Thanks for your code! I found that  Discriminator los sfunction of w-gan is the same as  Discriminator los sfunction of gan in your code,but actually  not! Pleace check it!
at 113 line in WGAN_GP, I recommend changing the code

`alpha = tf.random_uniform(shape=self.inputs.get_shape(), minval=0.,maxval=1.)`
to
`alpha = tf.random_uniform(shape=[BATCH_SIZE,1,1,1], minval=0.,maxval=1.)`

Because It  must be created **one** alpha value for each batch
Hey @hwalsuklee,

Thanks for sharing your code which helps me a lot. I have a question about the linear function in ops.py

I would be appreciated if you can explain what you are doing in the this function.

```
def linear(input_, output_size, scope=None, stddev=0.02, bias_start=0.0, with_w=False):
    shape = input_.get_shape().as_list()

    with tf.variable_scope(scope or "Linear"):
        matrix = tf.get_variable("Matrix", [shape[1], output_size], tf.float32,
                 tf.random_normal_initializer(stddev=stddev))
        bias = tf.get_variable("bias", [output_size],
        initializer=tf.constant_initializer(bias_start))
        if with_w:
            return tf.matmul(input_, matrix) + bias, matrix, bias
        else:
            return tf.matmul(input_, matrix) + bias
```

Also, It would be great if you add some comments when you are implementing; to be much easier for others to understand.

Thanks.
I've noticed an inefficiency in the CGAN code. When we append the one-hot encoded labels to the image, they influence the training gradients a lot. Instead, I've noticed that scaling the one-hot encoded labels down by a factor of 0.01 or even 0.001 helps the CGAN converge around twice as fast.

That would mean changing opts.py's conv_cond_concat function. My hack was to change ```return concat([x, y*tf.ones([x_shapes[0], x_shapes[1], x_shapes[2], y_shapes[3]])], 3)``` to ```return concat([x, 0.001*y*tf.ones([x_shapes[0], x_shapes[1], x_shapes[2], y_shapes[3]])], 3)``` and that worked well for me. I'm not too sure about in general though, perhaps try adding batch norm?
Make sure it works in win
Thanks for your contribution!
A problem occurs when I am training ACGAN without tuning any parameters which is shown as follows:
As the training process goes on, the loss of D is declining, but the loss of G is increasing on the contrary...and the gap between two of them is pretty large. I do not know how to solve it...
![image](https://user-images.githubusercontent.com/16879220/42943275-da3dc8d8-8b94-11e8-8a48-b0cd9e8c6893.png)

thanks you code , when i read the code , the discriminator use sigmoid output, but you also return D_real_logits ,and when caluate the loss real or fake ,you use D_real_logits or D_fake_logits before sigmoid output ; why you don't use the D_real or D_fake ,the sigmoid ouput? 
`        # get loss for discriminator
        d_loss_real = - tf.reduce_mean(D_real_logits)
        d_loss_fake = tf.reduce_mean(D_fake_logits)`

thank ,can you help me?
Thanks for the nicely orgnized code, I learned a lot from it. But I have question about the CVAE's input, you just append y's 1-hot code to the last dimension of x, so it makes the original data up to 11 times larger (the image shape becomes 28 * 28 * 11). But why not just append y in the middle of the encoding process, say, when we get a flat vector after several conv2d transformation, then we could append y to this flat vector?
This is a great collection of generative models for TensorFlow, all nicely wrapped in a common class interface. I'd like to use this as a basis for ongoing work I'm migrating to TensorFlow. I'm interested in using this code not only to test MNIST models, but also as a way of generating a series of reference models using several other datasets which can be reused and shared.

So as a first proposed change, I'd like to separate the batch_size from the model definition to instead be a runtime variable by using a placeholder. This allows:

 1) a trained model can be later opened without knowing the batch_size used at training time
 2) the encoder/decoder can be called on x/z of variable length
 3) a trained model can be later refined via transfer learning at a different batch_size

I've done a quick version of this for the VAE model and verified that this still works on that model (at least on the latest TensorFlow) and enables (1) and (2) above. If you are open to the spirit of this change, I'm happy to rework the implementation if you'd like this cleaned up further.