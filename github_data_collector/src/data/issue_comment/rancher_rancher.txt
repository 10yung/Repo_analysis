I can't start Windows node within same network because Rancher agent can't connect to server's external IP which it resolves. It should resolve to local ip to connect. One solution would be to use custom DNS server and set DNS starting docker. Preferred way is to use docker run --add-host flag, but it doesn't work. Error: FATA: https://www.myserver.com is not accessible

`PowerShell -NoLogo -NonInteractive -Command "& {docker run --add-host www.myserver.com:10.20.54.14 -v c:/:c:/host rancher/rancher-agent:v2.3.4 bootstrap --server https://www.myserver.com --token xxx --worker | iex}"`
None

<!--
Please search for existing issues first, then read https://rancher.com/docs/rancher/v2.x/en/contributing/#bugs-issues-or-questions to see what we expect in an issue
For security issues, please email security@rancher.com instead of posting a public issue in GitHub. You may (but are not required to) use the GPG key located on Keybase.
-->

**What kind of request is this (question/bug/enhancement/feature request):**
Bug

**Steps to reproduce (least amount of steps as possible):**
1. Create a new custom cluster with 2 control plane, 3 etcd, and 2 worker nodes.
2. Shut down one of the etcd nodes. To accomplish this, I stopped docker on the VM.
3. Attempt to add a new etcd node by running the docker run... on a new host.

**Result:**
The UI reports:
```
Failed to reconcile etcd plane: Failed to add etcd member [etcd-ip-10-0-2-19] to etcd cluster
```
Rancher logs show:
```
2020/01/17 22:47:12 [INFO] cluster [c-jpzp5] provisioning: [add/etcd] Adding member [etcd-ip-10-0-2-19] to etcd cluster
2020/01/17 22:47:12 [ERROR] ClusterController c-jpzp5 [cluster-provisioner-controller] failed with : Failed to reconcile etcd plane: Failed to add etcd member [etcd-ip-10-0-2-19] to etcd cluster
```
Every 5 minutes it attempts to add the etcd member but fails with the same error.

**Other details that may be helpful:**
If the unhealthy etcd node is deleted in the UI, the new etcd does get added successfully.

**Environment information**
- Rancher version (`rancher/rancher`/`rancher/server` image tag or shown bottom left in the UI):
v2.3.4
- Installation option (single install/HA):
single install
<!--
If the reported issue is regarding a created cluster, please provide requested info below
-->

**Cluster information**
- Cluster type (Hosted/Infrastructure Provider/Custom/Imported):
custom
- Machine type (cloud/VM/metal) and specifications (CPU/memory):
t3a.medium on AWS
- Kubernetes version (use `kubectl version`):

```
> kubectl version
Client Version: version.Info{Major:"1", Minor:"16", GitVersion:"v1.16.1", GitCommit:"d647ddbd755faf07169599a625faf302ffc34458", GitTreeState:"clean", BuildDate:"2019-10-02T17:01:15Z", GoVersion:"go1.12.10", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"16", GitVersion:"v1.16.4", GitCommit:"224be7bdce5a9dd0c2fd0d46b83865648e2fe0ba", GitTreeState:"clean", BuildDate:"2019-12-11T12:37:43Z", GoVersion:"go1.12.12", Compiler:"gc", Platform:"linux/amd64"}
```

- Docker version (use `docker version`):

```
$ docker version
Client: Docker Engine - Community
 Version:           19.03.5
 API version:       1.40
 Go version:        go1.12.12
 Git commit:        633a0ea838
 Built:             Wed Nov 13 07:22:05 2019
 OS/Arch:           linux/amd64
 Experimental:      false

Server: Docker Engine - Community
 Engine:
  Version:          19.03.5
  API version:      1.40 (minimum version 1.12)
  Go version:       go1.12.12
  Git commit:       633a0ea838
  Built:            Wed Nov 13 07:28:45 2019
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          v1.2.10
  GitCommit:        b34a5c8af56e510852c35414db4c1f4fa6172339
 runc:
  Version:          1.0.0-rc8+dev
  GitCommit:        3e425f80a8c931f88e6d94a8c831b9d5aa481657
 docker-init:
  Version:          0.18.0
  GitCommit:        fec3683
```

Right now scale testing framework only measures cluster and project listing. Add listing for all v3 resources to testing frame work.
Automate scaling tests using Jenkins
**What kind of request is this (question/bug/enhancement/feature request):** bug 


**Steps to reproduce (least amount of steps as possible):**
- Deploy a cluster
- Through the API run CIS Scan on the cluster with the config map value set as 
`{"config.json": "{'skip':{'rke-cis-1.4':['1.1.2']}}"}`

**Actual Result:**
The scan does NOT skip the test.

**Expected Result:**
The Scan should skip the test - 1.1.2


**Other details that may be helpful:**


**Environment information**
- Rancher version (`rancher/rancher`/`rancher/server` image tag or shown bottom left in the UI): master-head
- Installation option (single install/HA): single

<!--
Please search for existing issues first, then read https://rancher.com/docs/rancher/v2.x/en/contributing/#bugs-issues-or-questions to see what we expect in an issue
For security issues, please email security@rancher.com instead of posting a public issue in GitHub. You may (but are not required to) use the GPG key located on Keybase.
-->

**What kind of request is this (question/bug/enhancement/feature request):**
Bug

**Steps to reproduce (least amount of steps as possible):**
Create an ingress that includes TLS.

**Result:**

The "host" field is changed from the yaml supplied value to the `${ingress_name}.${namespace}.${ip}.xip.io`. The desired result is to maintain the value provided in the yaml file.

**Other details that may be helpful:**

This issue was visible when trying to install harbor using helm in a cluster on a private network (nginx ingress, no configuration for letsencrypt). Logs of the session:

First cleanup previous install, the change appears to be persistent:

```
[bmitch@rke-1 ~]$ kubectl delete -n harbor -f foo-http.yaml
ingress.extensions "foo-ingress" deleted
```

Try applying an ingress without a TLS config:
```
[bmitch@rke-1 ~]$ cat foo-http.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:                              
  labels:                            
    app: foo
  name: foo-ingress                                        
spec:                                    
  rules:                                       
  - host: foo.harbor.192.168.237.64.xip.io                                    
    http:                     
      paths: 
      - backend:
          serviceName: harbor-harbor-portal
          servicePort: 80                              
        path: /                                                                                                                                                                                                                                                                 
                                                                                                                                       
[bmitch@rke-1 ~]$ kubectl apply -n harbor -f foo-http.yaml
ingress.extensions/foo-ingress created
[bmitch@rke-1 ~]$ kubectl get -n harbor -o yaml ingress.extensions/foo-ingress
apiVersion: extensions/v1beta1
kind: Ingress      
metadata:          
  annotations:             
    kubectl.kubernetes.io/last-applied-configuration: |                     
      {"apiVersion":"extensions/v1beta1","kind":"Ingress","metadata":{"annotations":{},"labels":{"app":"foo"},"name":"foo-ingress","namespace":"harbor"},"spec":{"rules":[{"host":"foo.harbor.192.168.237.64.xip.io","http":{"paths":[{"backend":{"serviceName":"harbor-harbor-p
ortal","servicePort":80},"path":"/"}]}}]}}
  creationTimestamp: "2020-01-17T20:33:03Z"
  generation: 1                           
  labels:                                                                                                                                                                                                                                                                      
    app: foo
  name: foo-ingress                                                                                                                                                                                                                                                            
  namespace: harbor                                                                                                                                                                                                                                                            
  resourceVersion: "474568"
  selfLink: /apis/extensions/v1beta1/namespaces/harbor/ingresses/foo-ingress
  uid: 66dfe1af-dbf1-49d1-aedc-b2520d326344
spec:     
  rules:                               
  - host: foo.harbor.192.168.237.64.xip.io
    http:
      paths:      
      - backend:                                                              
          serviceName: harbor-harbor-portal
          servicePort: 80
        path: /
status:       
  loadBalancer: {}    
```

Add the TLS config from harbor:
```
[bmitch@rke-1 ~]$ cat foo-https.yaml                                                                                                                                                                                                                                   [61/1790]
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  labels:
    app: foo
  name: foo-ingress
spec:
  rules:
  - host: foo.harbor.192.168.237.64.xip.io
    http:
      paths:
      - backend:
          serviceName: harbor-harbor-portal
          servicePort: 80
        path: /
  tls:
  - hosts:
    - core.harbor.192.168.237.64.xip.io
    secretName: harbor-harbor-ingress

[bmitch@rke-1 ~]$ kubectl apply -n harbor -f foo-https.yaml
ingress.extensions/foo-ingress configured
```

On first glance, this appears ok, the host is still `foo.harbor...`:
```
[bmitch@rke-1 ~]$ kubectl get -n harbor -o yaml ingress.extensions/foo-ingress
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"extensions/v1beta1","kind":"Ingress","metadata":{"annotations":{},"labels":{"app":"foo"},"name":"foo-ingress","namespace":"harbor"},"spec":{"rules":[{"host":"foo.harbor.192.168.237.64.xip.io","http":{"paths":[{"backend":{"serviceName":"harbor-harbor-p
ortal","servicePort":80},"path":"/"}]}}],"tls":[{"hosts":["core.harbor.192.168.237.64.xip.io"],"secretName":"harbor-harbor-ingress"}]}}
  creationTimestamp: "2020-01-17T20:33:03Z"
  generation: 2
  labels:
    app: foo
  name: foo-ingress
  namespace: harbor
  resourceVersion: "474671"
  selfLink: /apis/extensions/v1beta1/namespaces/harbor/ingresses/foo-ingress
  uid: 66dfe1af-dbf1-49d1-aedc-b2520d326344
spec:
  rules:
  - host: foo.harbor.192.168.237.64.xip.io
    http:
      paths:
      - backend:
          serviceName: harbor-harbor-portal
          servicePort: 80
        path: /
  tls:
  - hosts:
    - core.harbor.192.168.237.64.xip.io
    secretName: harbor-harbor-ingress
status:
  loadBalancer: {}
```

But after a second, the resource is updated to `foo-ingress.harbor...`:
```
[bmitch@rke-1 ~]$ kubectl get -n harbor -o yaml ingress.extensions/foo-ingress
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    field.cattle.io/publicEndpoints: '[{"addresses":["192.168.237.64"],"port":80,"protocol":"HTTP","serviceName":"harbor:harbor-harbor-portal","ingressName":"harbor:foo-ingress","hostname":"foo-ingress.harbor.192.168.237.64.xip.io","path":"/","allNodes":true}]'
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"extensions/v1beta1","kind":"Ingress","metadata":{"annotations":{},"labels":{"app":"foo"},"name":"foo-ingress","namespace":"harbor"},"spec":{"rules":[{"host":"foo.harbor.192.168.237.64.xip.io","http":{"paths":[{"backend":{"serviceName":"harbor-harbor-p
ortal","servicePort":80},"path":"/"}]}}],"tls":[{"hosts":["core.harbor.192.168.237.64.xip.io"],"secretName":"harbor-harbor-ingress"}]}}
  creationTimestamp: "2020-01-17T20:33:03Z"
  generation: 3
  labels:
    app: foo
  name: foo-ingress
  namespace: harbor
  resourceVersion: "474725"
  selfLink: /apis/extensions/v1beta1/namespaces/harbor/ingresses/foo-ingress
  uid: 66dfe1af-dbf1-49d1-aedc-b2520d326344
spec:
  rules:
  - host: foo-ingress.harbor.192.168.237.64.xip.io
    http:
      paths:
      - backend:
          serviceName: harbor-harbor-portal
          servicePort: 80
        path: /
  tls:
  - hosts:
    - core.harbor.192.168.237.64.xip.io
    secretName: harbor-harbor-ingress
status:
  loadBalancer:
    ingress:
    - ip: 192.168.237.64
    - ip: 192.168.237.65
    - ip: 192.168.237.66
```

This is particularly bad when one ingress configuration is used for two different hostnames as the harbor helm install does. At this point, from the command line and from the GUI, I'm unable to configure the host in the ingress, it keeps getting modified back to the generated value.

**Environment information**
- Rancher version (`rancher/rancher`/`rancher/server` image tag or shown bottom left in the UI): v2.3.3
- Installation option (single install/HA): single rancher instance managing a cluster with 3 managers and 3 workers (this is a lab environment)

<!--
If the reported issue is regarding a created cluster, please provide requested info below
-->

**Cluster information**
- Cluster type (Hosted/Infrastructure Provider/Custom/Imported): Custom
- Machine type (cloud/VM/metal) and specifications (CPU/memory): VM, 2 vCPU, 4G ram each
- Kubernetes version (use `kubectl version`):

```
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"17", GitVersion:"v1.17.1", GitCommit:"d224476cd0730baca2b6e357d144171ed74192d6", GitTreeState:"clean", BuildDate:"2020-01-14T21:04:32Z", GoVersion:"go1.13.5", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"16", GitVersion:"v1.16.3", GitCommit:"b3cbbae08ec52a7fc73d334838e18d17e8512749", GitTreeState:"clean", BuildDate:"2019-11-13T11:13:49Z", GoVersion:"go1.12.12", Compiler:"gc", Platform:"linux/amd64"}
```

- Docker version (use `docker version`):

```
[root@rke-0 rancher]# docker version
Client: Docker Engine - Community
 Version:           19.03.5
 API version:       1.40
 Go version:        go1.12.12
 Git commit:        633a0ea
 Built:             Wed Nov 13 07:25:41 2019
 OS/Arch:           linux/amd64
 Experimental:      false

Server: Docker Engine - Community
 Engine:
  Version:          19.03.5
  API version:      1.40 (minimum version 1.12)
  Go version:       go1.12.12
  Git commit:       633a0ea
  Built:            Wed Nov 13 07:24:18 2019
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.2.10
  GitCommit:        b34a5c8af56e510852c35414db4c1f4fa6172339
 runc:
  Version:          1.0.0-rc8+dev
  GitCommit:        3e425f80a8c931f88e6d94a8c831b9d5aa481657
 docker-init:
  Version:          0.18.0
  GitCommit:        fec3683

```

Changes proposed to validate KDM changes: 

In addition to the current onTag job that gets triggered in jenkins , we need another jenkins job to call a validation test that should do the following:

Deploy Rancher server with current RC build. Create custom cluster with all K8s versions available. (Currently no K8s version is set which results in only the default K8s version being deployed)
Deploy last released version ( can we rely on latest tag for this) .
Change kdm to point to “dev” branch.
Create custom cluster with all K8s versions available.

Record cluster details for each of the cluster that gets created on both the rancher server instances so that it can be used by the jenkins jobs to trigger automation runs .
