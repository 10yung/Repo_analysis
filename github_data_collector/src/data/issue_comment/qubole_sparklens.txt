While reading the for example the _Printing Application timeline_, looking at 535 jobs it is hard to relate a number to an action in the Spark program. In the Spark UI there is a solution for this: set the spark.job.description and/or spark.jobGroup.id in the driver for every (eager) action you want the Spark executors to do (see the answers to [https://stackoverflow.com/questions/39123314/how-to-add-custom-description-to-spark-job-for-displaying-in-spark-web-ui](https://stackoverflow.com/questions/39123314/how-to-add-custom-description-to-spark-job-for-displaying-in-spark-web-ui))

Is it possible to add an option to Sparklens to choose to report the spark.job.description and/or spark.jobGroup.id with its Job id?
### qubole/sparklens now has a Chat Room on Gitter

@iamrohit has just created a chat room. You can visit it here: [https://gitter.im/qubole-sparklens/community](https://gitter.im/qubole-sparklens/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&content=body_link).

This pull-request adds this badge to your README.md:


[![Gitter](https://badges.gitter.im/qubole-sparklens/community.svg)](https://gitter.im/qubole-sparklens/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=body_badge)

If my aim is a little off, please [let me know](https://gitlab.com/gitlab-org/gitter/readme-badger/issues).

Happy chatting.


PS: [Click here](https://gitter.im/settings/badger/opt-out) if you would prefer not to receive automatic pull-requests from Gitter in future.

- Sparklens is compiled against spark 2.0.0
- Spark 2.0.0 has a dependency on json4s 3.2.11 wherein Spark 2.4.0 onwards json4s 3.5.3 is used
- In the later version the method definition of org.json4s.JsonMethods.parse() has changed
- Fix uses reflection to resolve this backward incompatibility
- Added a framework for simplifying adding any metrics to Sparklens report
- Mapping the nodes of the sparkplan to the stages they were executed as a part of
- Autodetecting the skew join in the data

Still a Work In Progress!
Following metrics related to the Driver are now reported - 

1.  driverHeapMax                    => Max Heap memory allocated to the driver JVM
2. driverMaxHeapCommitted  => Max Heap memory committed to the driver JVM
3. driverMaxHeapUsed            => Max Heap memory used by the driver JVM
4. driverCPUTime                     => Total CPU Time allocated to the driver JVM
5. driverGCCount                      => Total major GCs happened during the lifetime of the JVM
6. driverGCTime                        => Part of driverCPUTime spent in the GC

Sample output for a simple application.

```

 Time spent in Driver vs Executors
 Driver WallClock Time    00m 11s   79.24%
 Executor WallClock Time  00m 03s   20.76%
 Total WallClock Time     00m 15s


 DriverMetrics (driverMetrics)
                NAME                        Value
 driverCPUTime                           10216.7 hh
 driverGCCount                                   11
 driverGCTime                              301.0 ms
 driverHeapMax                               1.0 GB
 driverMaxHeapCommitted                    480.0 MB
 driverMaxHeapUsed                         308.9 MB
```


When I upload the sparklens report json file to [sparklens](http://sparklens.qubole.com/), I don't get any reports delivered. I have even checked my spam folders.
Hi ,
I want to use Sparklens while running Spark Notebooks in a Databricks cluster . Is that possible currently ? 
Hi,

First of all, amazing project!

From the report generated from http://sparklens.qubole.com/ , I see the ideal executor plot where it plot "the minimal number of executors (ideal) which could have finished the same work in same amount of wall clock time" 

I am curious what are the formulas, equations for such plot. If you can give me some explanation on how you guys approach it, that would be great. Thanks!
Hi,
I tried specifying an S3 location as spark.sparklens.data.dir and I am getting the following error.
The issue is reproduced in every job.

As per my understanding, this is the code which is dumping the spark lens json file - https://github.com/qubole/sparklens/blob/master/src/main/scala/com/qubole/sparklens/QuboleJobListener.scala#L135
This should work with S3 directories as well.

19/02/27 07:23:18 ERROR Utils: uncaught error in thread SparkListenerBus, stopping SparkContext
java.lang.ExceptionInInitializerError
	at com.amazon.ws.emr.hadoop.fs.files.TemporaryDirectoriesGenerator.createAndTrack(TemporaryDirectoriesGenerator.java:145)
	at com.amazon.ws.emr.hadoop.fs.files.TemporaryDirectoriesGenerator.createTemporaryDirectories(TemporaryDirectoriesGenerator.java:94)
	at com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.create(S3NativeFileSystem.java:642)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:932)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:913)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:810)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:799)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.create(EmrFileSystem.java:171)
	at com.qubole.sparklens.QuboleJobListener.dumpData(QuboleJobListener.scala:136)
	at com.qubole.sparklens.QuboleJobListener.onApplicationEnd(QuboleJobListener.scala:164)
	at org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent(SparkListenerBus.scala:57)
	at org.apache.spark.scheduler.LiveListenerBus.doPostEvent(LiveListenerBus.scala:36)
	at org.apache.spark.scheduler.LiveListenerBus.doPostEvent(LiveListenerBus.scala:36)
	at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:63)
	at org.apache.spark.scheduler.LiveListenerBus.postToAll(LiveListenerBus.scala:36)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(LiveListenerBus.scala:94)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:79)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:79)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:78)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1279)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:77)
Caused by: java.lang.IllegalStateException: Shutdown in progress
	at java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:66)
	at java.lang.Runtime.addShutdownHook(Runtime.java:211)
	at com.amazon.ws.emr.hadoop.fs.files.TemporaryDirectoryShutdownHook.<clinit>(TemporaryDirectoryShutdownHook.java:17)
	... 22 more
19/02/27 07:23:18 ERROR Utils: throw uncaught fatal error in thread SparkListenerBus
java.lang.ExceptionInInitializerError
	at com.amazon.ws.emr.hadoop.fs.files.TemporaryDirectoriesGenerator.createAndTrack(TemporaryDirectoriesGenerator.java:145)
	at com.amazon.ws.emr.hadoop.fs.files.TemporaryDirectoriesGenerator.createTemporaryDirectories(TemporaryDirectoriesGenerator.java:94)
	at com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.create(S3NativeFileSystem.java:642)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:932)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:913)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:810)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:799)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.create(EmrFileSystem.java:171)
	at com.qubole.sparklens.QuboleJobListener.dumpData(QuboleJobListener.scala:136)
	at com.qubole.sparklens.QuboleJobListener.onApplicationEnd(QuboleJobListener.scala:164)
	at org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent(SparkListenerBus.scala:57)
	at org.apache.spark.scheduler.LiveListenerBus.doPostEvent(LiveListenerBus.scala:36)
	at org.apache.spark.scheduler.LiveListenerBus.doPostEvent(LiveListenerBus.scala:36)
	at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:63)
	at org.apache.spark.scheduler.LiveListenerBus.postToAll(LiveListenerBus.scala:36)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(LiveListenerBus.scala:94)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:79)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:79)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:78)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1279)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:77)
Caused by: java.lang.IllegalStateException: Shutdown in progress
	at java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:66)
	at java.lang.Runtime.addShutdownHook(Runtime.java:211)
	at com.amazon.ws.emr.hadoop.fs.files.TemporaryDirectoryShutdownHook.<clinit>(TemporaryDirectoryShutdownHook.java:17)
	... 22 more
19/02/27 07:23:18 INFO SparkContext: SparkContext already stopped.
Exception in thread "SparkListenerBus" java.lang.ExceptionInInitializerError
	at com.amazon.ws.emr.hadoop.fs.files.TemporaryDirectoriesGenerator.createAndTrack(TemporaryDirectoriesGenerator.java:145)
	at com.amazon.ws.emr.hadoop.fs.files.TemporaryDirectoriesGenerator.createTemporaryDirectories(TemporaryDirectoriesGenerator.java:94)
	at com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.create(S3NativeFileSystem.java:642)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:932)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:913)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:810)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:799)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.create(EmrFileSystem.java:171)
	at com.qubole.sparklens.QuboleJobListener.dumpData(QuboleJobListener.scala:136)
	at com.qubole.sparklens.QuboleJobListener.onApplicationEnd(QuboleJobListener.scala:164)
	at org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent(SparkListenerBus.scala:57)
	at org.apache.spark.scheduler.LiveListenerBus.doPostEvent(LiveListenerBus.scala:36)
	at org.apache.spark.scheduler.LiveListenerBus.doPostEvent(LiveListenerBus.scala:36)
	at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:63)
	at org.apache.spark.scheduler.LiveListenerBus.postToAll(LiveListenerBus.scala:36)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(LiveListenerBus.scala:94)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:79)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:79)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:78)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1279)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:77)
Caused by: java.lang.IllegalStateException: Shutdown in progress
	at java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:66)
	at java.lang.Runtime.addShutdownHook(Runtime.java:211)
	at com.amazon.ws.emr.hadoop.fs.files.TemporaryDirectoryShutdownHook.<clinit>(TemporaryDirectoryShutdownHook.java:17)
	... 22 more
19/02/27 07:23:18 INFO YarnClusterSchedulerBackend: Shutting down all executors
I am trying to run sparklens on event logs of my application. 

I am using following command

```
./bin/spark-submit \
	--packages qubole:sparklens:0.2.0-s_2.11 \
	--master local[0] \
	--class com.qubole.sparklens.app.ReporterApp \
	qubole-dummy-arg file:///Users/shasidhar/interests/sparklens/eventlog.txt source=history
```

I see following output in console

```
Ivy Default Cache set to: /Users/shasidhar/.ivy2/cache
The jars for the packages stored in: /Users/shasidhar/.ivy2/jars
:: loading settings :: url = jar:file:/Users/shasidhar/interests/spark/spark-2.3.0-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
qubole#sparklens added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0
	confs: [default]
	found qubole#sparklens;0.2.0-s_2.11 in spark-packages
:: resolution report :: resolve 177ms :: artifacts dl 5ms
	:: modules in use:
	qubole#sparklens;0.2.0-s_2.11 from spark-packages in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent
	confs: [default]
	0 artifacts copied, 1 already retrieved (0kB/6ms)
2019-01-03 15:46:11 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Warning: Local jar /Users/shasidhar/interests/spark/spark-2.3.0-bin-hadoop2.7/qubole-dummy-arg does not exist, skipping.

2019-01-03 15:46:52 INFO  ShutdownHookManager:54 - Shutdown hook called
2019-01-03 15:46:52 INFO  ShutdownHookManager:54 - Deleting directory /private/var/folders/3t/rfd2djjs1yg30mhmw8z_s7tw0000gp/T/spark-7a992110-6a4f-44f4-9473-1ddade11b53a
```

What exactly I need to look at after this? Does it generate sparklens json file? If yes, where I can see the output file? 