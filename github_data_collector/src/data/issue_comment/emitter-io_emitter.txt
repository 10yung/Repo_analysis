I'm confused to run emitter quickly. when I know how to start, I update the docker run part, I hope I can be helpful.
## Problem
```shell 
# github.com/emitter-io/emitter/config
go/pkg/mod/github.com/emitter-io/emitter@v2.0.437+incompatible/config/config.go:66:14: undefined: config.VaultConfig
```
## environment
```shell
 go env
GO111MODULE="on"
GOARCH="amd64"
GOBIN=""
GOCACHE="/Users/abser/Library/Caches/go-build"
GOENV="/Users/abser/Library/Application Support/go/env"
GOEXE=""
GOFLAGS=""
GOHOSTARCH="amd64"
GOHOSTOS="darwin"
GONOPROXY=""
GONOSUMDB=""
GOOS="darwin"
GOPATH="/Users/abser/go"
GOPRIVATE=""
GOPROXY="https://goproxy.cn"
GOROOT="/usr/local/go"
GOSUMDB="sum.golang.org"
GOTMPDIR=""
GOTOOLDIR="/usr/local/go/pkg/tool/darwin_amd64"
GCCGO="gccgo"
AR="ar"
CC="clang"
CXX="clang++"
CGO_ENABLED="1"
GOMOD="/dev/null"
CGO_CFLAGS="-g -O2"
CGO_CPPFLAGS=""
CGO_CXXFLAGS="-g -O2"
CGO_FFLAGS="-g -O2"
CGO_LDFLAGS="-g -O2"
PKG_CONFIG="pkg-config"
GOGCCFLAGS="-fPIC -m64 -pthread -fno-caret-diagnostics -Qunused-arguments -fmessage-length=0 -fdebug-prefix-map=/var/folders/m0/q27qpd7x3r14p94qxs73k7mw0000gn/T/go-build078196314=/tmp/go-build -gno-record-gcc-switches -fno-common"
```
## Complete log
```shell
ArideMacBook-Air:~ abser$ go get -u github.com/emitter-io/emitter && emitter
go: finding github.com/emitter-io/emitter v2.0.437+incompatible
go: downloading github.com/emitter-io/emitter v2.0.437+incompatible
go: extracting github.com/emitter-io/emitter v2.0.437+incompatible
go: finding github.com/emitter-io/config v1.0.0
go: downloading github.com/emitter-io/config v1.0.0
go: finding github.com/hashicorp/go-sockaddr v1.0.2
go: finding golang.org/x/crypto latest
go: downloading github.com/hashicorp/go-sockaddr v1.0.2
go: finding github.com/karlseguin/ccache v2.0.3+incompatible
go: finding github.com/weaveworks/mesh latest
go: finding github.com/axiomhq/hyperloglog latest
go: finding github.com/kelindar/tcp v1.0.0
go: extracting github.com/emitter-io/config v1.0.0
go: downloading github.com/karlseguin/ccache v2.0.3+incompatible
go: downloading github.com/kelindar/tcp v1.0.0
go: downloading golang.org/x/crypto v0.0.0-20200109152110-61a87790db17
go: extracting github.com/kelindar/tcp v1.0.0
go: extracting github.com/hashicorp/go-sockaddr v1.0.2
go: finding github.com/kelindar/binary v1.0.8
go: downloading github.com/weaveworks/mesh v0.0.0-20191105120815-58dbcc3e8e63
go: downloading github.com/axiomhq/hyperloglog v0.0.0-20191112132149-a4c4c47bc57f
go: extracting github.com/karlseguin/ccache v2.0.3+incompatible
go: downloading github.com/kelindar/binary v1.0.8
go: finding github.com/kelindar/process latest
go: finding github.com/valyala/fasthttp v1.7.1
go: downloading github.com/valyala/fasthttp v1.7.1
go: extracting github.com/axiomhq/hyperloglog v0.0.0-20191112132149-a4c4c47bc57f
go: extracting github.com/weaveworks/mesh v0.0.0-20191105120815-58dbcc3e8e63
go: downloading github.com/kelindar/process v0.0.0-20170730150328-69a29e249ec3
go: extracting github.com/kelindar/binary v1.0.8
go: extracting golang.org/x/crypto v0.0.0-20200109152110-61a87790db17
go: extracting github.com/kelindar/process v0.0.0-20170730150328-69a29e249ec3
go: extracting github.com/valyala/fasthttp v1.7.1
go: downloading github.com/dgryski/go-metro v0.0.0-20180109044635-280f6062b5bc
go: downloading github.com/stretchr/objx v0.1.0
go: extracting github.com/dgryski/go-metro v0.0.0-20180109044635-280f6062b5bc
go: extracting github.com/stretchr/objx v0.1.0
go: downloading github.com/klauspost/compress v1.8.2
go: downloading github.com/valyala/bytebufferpool v1.0.0
go: downloading golang.org/x/net v0.0.0-20190827160401-ba9fcec4b297
go: extracting github.com/valyala/bytebufferpool v1.0.0
go: extracting golang.org/x/net v0.0.0-20190827160401-ba9fcec4b297
go: extracting github.com/klauspost/compress v1.8.2
go: downloading github.com/klauspost/cpuid v1.2.1
go: extracting github.com/klauspost/cpuid v1.2.1
go: finding github.com/klauspost/compress v1.9.7
go: finding github.com/klauspost/cpuid v1.2.2
go: finding github.com/davecgh/go-spew v0.0.0-20180830191138-d8f796af33cc
go: finding github.com/dgryski/go-metro latest
go: finding gopkg.in/yaml.v2 v2.2.7
go: finding golang.org/x/net latest
go: finding github.com/valyala/bytebufferpool v1.0.0
go: finding golang.org/x/sys latest
go: downloading golang.org/x/net v0.0.0-20191209160850-c0dbc17a3553
go: downloading gopkg.in/yaml.v2 v2.2.7
go: downloading golang.org/x/sys v0.0.0-20200107162124-548cf772de50
go: downloading github.com/klauspost/compress v1.9.7
go: extracting gopkg.in/yaml.v2 v2.2.7
go: extracting golang.org/x/net v0.0.0-20191209160850-c0dbc17a3553
go: extracting golang.org/x/sys v0.0.0-20200107162124-548cf772de50
go: extracting github.com/klauspost/compress v1.9.7
# github.com/emitter-io/emitter/config
go/pkg/mod/github.com/emitter-io/emitter@v2.0.437+incompatible/config/config.go:66:14: undefined: config.VaultConfig
```
Hi, I run emitter in a 8G container, when the connection grows to 418000, emitter will restart because of memory leak. My question is why one connection cost so large memory? Is that true I can only add 418000 connection in one broker(8G memory)?
I have been trying to use SSL in EKS by using a certificate from AWS however I cant reach the broker at 8883 (MQTTS) but I can in 1883. It would be great to see a demo of this if possible. There are no errors in the brokers logs.

- tcp://mqtt.domain.io:1883 --> works
- ssl://mqtt.domain.io:8883 --> does not work - it hangs

In broker yaml I have ports:
```
containers:
      - env:
        ....
        - name: EMITTER_LISTEN
          value: ":1883"
        - name: EMITTER_TLS_LISTEN
          value: ":8883"
        - name: EMITTER_TLS_HOST
          value: "domain.io"
        - name: EMITTER_TLS_EMAIL
          value: "email@domain.io"
ports:
          - containerPort: 8080
          - containerPort: 8883
          - containerPort: 1883
          - containerPort: 443
          - containerPort: 4000
          - containerPort: 80
```
and in the load balancer yaml:
```
metadata:
  name: broker-loadbalancer
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: "certificateArn"
    service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: "ELBSecurityPolicy"
    service.beta.kubernetes.io/aws-load-balancer-ssl-ports: "8883"
...
ports:
    - port: 1883
      targetPort: 1883
      name: mqtt
    - port: 8883
      targetPort: 8883
      name: mqtts
    - port: 80
      targetPort: 80
      name: http
```
Am I missing something?

I am not sure if the load balancer requieres the annotations part, but I also removed them to see if it works but it didnt.

Emitter logs:
```
...
2019/12/23 18:10:40 [service] starting the listener (0.0.0.0:1883)
2019/12/23 18:10:40 [service] starting the listener (0.0.0.0:8080)
2019/12/23 18:10:40 [tls] setting up certificates with dircache cache
2019/12/23 18:10:40 [service] exposing autocert TLS validation on :80
2019/12/23 18:10:40 [service] starting the listener (0.0.0.0:8883)
2019/12/23 18:10:40 [service] service started
...
```
Running netstat in the pods:

- broker-0:
```
kubectl exec -it broker-0 -- sh
~ # netstat
Active Internet connections (w/o servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       
tcp        0      0 broker-0.broker.default.svc.cluster.local:39417 broker-1.broker.default.svc.cluster.local:4000 ESTABLISHED 
tcp        0      0 broker-0.broker.default.svc.cluster.local:8883 ip-10-0-193-164.ec2.internal:62668 TIME_WAIT   
tcp        0      0 broker-0.broker.default.svc.cluster.local:8883 ip-10-0-193-164.ec2.internal:62564 TIME_WAIT   
tcp        0      0 broker-0.broker.default.svc.cluster.local:8883 ip-10-0-128-108.ec2.internal:6950 TIME_WAIT   
tcp        0      0 broker-0.broker.default.svc.cluster.local:8883 ip-10-0-128-108.ec2.internal:6934 TIME_WAIT   
tcp        0      0 broker-0.broker.default.svc.cluster.local:8883 ip-10-0-193-164.ec2.internal:62576 TIME_WAIT   
tcp        0      0 broker-0.broker.default.svc.cluster.local:8883 ip-10-0-128-108.ec2.internal:6948 TIME_WAIT   
tcp        0      0 broker-0.broker.default.svc.cluster.local:8883 ip-10-0-193-164.ec2.internal:62610 TIME_WAIT   
tcp        0      0 broker-0.broker.default.svc.cluster.local:8883 ip-10-0-193-164.ec2.internal:62674 TIME_WAIT   
tcp        0      0 broker-0.broker.default.svc.cluster.local:8883 ip-10-0-128-108.ec2.internal:6958 TIME_WAIT   
tcp        0      0 broker-0.broker.default.svc.cluster.local:8883 ip-10-0-193-164.ec2.internal:62594 TIME_WAIT   
tcp        0      0 broker-0.broker.default.svc.cluster.local:8883 ip-10-0-128-108.ec2.internal:6956 TIME_WAIT   
tcp        0      0 broker-0.broker.default.svc.cluster.local:8883 ip-10-0-193-164.ec2.internal:62612 TIME_WAIT   
tcp        0      0 broker-0.broker.default.svc.cluster.local:8883 ip-10-0-193-164.ec2.internal:62608 TIME_WAIT   
tcp        0      0 broker-0.broker.default.svc.cluster.local:8883 ip-10-0-193-164.ec2.internal:62574 TIME_WAIT   
tcp        0      0 broker-0.broker.default.svc.cluster.local:8883 ip-10-0-193-164.ec2.internal:62566 TIME_WAIT   
tcp        0      0 broker-0.broker.default.svc.cluster.local:8883 ip-10-0-128-108.ec2.internal:6920 TIME_WAIT   
tcp        0      0 broker-0.broker.default.svc.cluster.local:8883 ip-10-0-193-164.ec2.internal:62614 TIME_WAIT   
tcp        0      0 broker-0.broker.default.svc.cluster.local:8883 ip-10-0-128-108.ec2.internal:6936 TIME_WAIT   
tcp        0      0 broker-0.broker.default.svc.cluster.local:8883 ip-10-0-193-164.ec2.internal:62624 TIME_WAIT   
tcp        0      0 broker-0.broker.default.svc.cluster.local:8883 ip-10-0-193-164.ec2.internal:62584 TIME_WAIT   
tcp        0      0 broker-0.broker.default.svc.cluster.local:8883 ip-10-0-193-164.ec2.internal:62618 TIME_WAIT   
tcp        0      0 broker-0.broker.default.svc.cluster.local:8883 ip-10-0-193-164.ec2.internal:62570 TIME_WAIT   
tcp        0      0 broker-0.broker.default.svc.cluster.local:8883 ip-10-0-128-108.ec2.internal:6910 TIME_WAIT   
tcp        0      0 broker-0.broker.default.svc.cluster.local:8883 ip-10-0-128-108.ec2.internal:6926 TIME_WAIT   
tcp        0      0 broker-0.broker.default.svc.cluster.local:8883 ip-10-0-128-108.ec2.internal:6918 TIME_WAIT   
tcp        0      0 broker-0.broker.default.svc.cluster.local:8883 ip-10-0-128-108.ec2.internal:6942 TIME_WAIT   
Active UNIX domain sockets (w/o servers)
Proto RefCnt Flags       Type       State         I-Node Path
```
- broker-1:
```
kubectl exec -it broker-1 -- sh
~ # netstat
Active Internet connections (w/o servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       
tcp        0      0 broker-1.broker.default.svc.cluster.local:8883 ip-10-0-193-164.ec2.internal:62664 TIME_WAIT   
tcp        0      0 broker-1.broker.default.svc.cluster.local:8883 ip-10-0-128-108.ec2.internal:6954 TIME_WAIT   
tcp        0      0 broker-1.broker.default.svc.cluster.local:8883 ip-10-0-193-164.ec2.internal:62622 TIME_WAIT   
tcp        0      0 broker-1.broker.default.svc.cluster.local:8883 ip-10-0-128-108.ec2.internal:afs3-prserver TIME_WAIT   
tcp        0      0 broker-1.broker.default.svc.cluster.local:8883 ip-10-0-128-108.ec2.internal:6982 TIME_WAIT   
tcp        0      0 broker-1.broker.default.svc.cluster.local:8883 ip-10-0-128-108.ec2.internal:7010 TIME_WAIT   
tcp        0      0 broker-1.broker.default.svc.cluster.local:8883 ip-10-0-128-108.ec2.internal:afs3-kaserver TIME_WAIT   
tcp        0      0 broker-1.broker.default.svc.cluster.local:8883 ip-10-0-128-108.ec2.internal:6940 TIME_WAIT   
tcp        0      0 broker-1.broker.default.svc.cluster.local:4000 broker-0.broker.default.svc.cluster.local:39417 ESTABLISHED 
tcp        0      0 broker-1.broker.default.svc.cluster.local:8883 ip-10-0-128-108.ec2.internal:afs3-update TIME_WAIT   
tcp        0      0 broker-1.broker.default.svc.cluster.local:8883 ip-10-0-128-108.ec2.internal:6988 TIME_WAIT   
tcp        0      0 broker-1.broker.default.svc.cluster.local:8883 ip-10-0-128-108.ec2.internal:6938 TIME_WAIT   
tcp        0      0 broker-1.broker.default.svc.cluster.local:8883 ip-10-0-128-108.ec2.internal:6928 TIME_WAIT   
tcp        0      0 broker-1.broker.default.svc.cluster.local:8883 ip-10-0-128-108.ec2.internal:6924 TIME_WAIT   
tcp        0      0 broker-1.broker.default.svc.cluster.local:8883 ip-10-0-193-164.ec2.internal:62666 TIME_WAIT   
tcp        0      0 broker-1.broker.default.svc.cluster.local:8883 ip-10-0-193-164.ec2.internal:62636 TIME_WAIT   
tcp        0      0 broker-1.broker.default.svc.cluster.local:8883 ip-10-0-128-108.ec2.internal:6932 TIME_WAIT   
tcp        0      0 broker-1.broker.default.svc.cluster.local:8883 ip-10-0-128-108.ec2.internal:6996 TIME_WAIT   
Active UNIX domain sockets (w/o servers)
Proto RefCnt Flags       Type       State         I-Node Path
```
add HTTPJson method to generate channel key with http post;  
add onHTTPPublishJson method to publish message with http post

These codes provide http api for other language, for example delphi, php ...
### first question
I created a chat topic like 
```go
ga, _ := c.GenerateKey("", "xm/chat/group/a/", "rwlsp", 0)
err = c.Presence(ga, "xm/chat/group/a/", true, true)
if err != nil {
	panic(err)
}
```
I created several client to connect emitter-server, like 
```go
err := c.Presence("" ,"xm/chat/group/a/", true, true)
if err != nil {
	panic(err)
}

err = c.Subscribe("" ,"xm/chat/group/a/", nil)
if err != nil {
	panic(err)
}
``` 

then 10 seonds ago , i unsubscribe it

```go
err := c.Unsubscribe(key, channel)
if err != nil {
	fmt.Println(err)
}
```

i cannot receive subscribe/unsubscribe but i can receive status from each client.

i want to receive every client when they subscribe or unsubscribe's message.

### second question

```go
func getOpts() []func(*emitter.Client) {
	return []func(*emitter.Client){
		emitter.WithAutoReconnect(true),
		emitter.WithBrokers("tcp://127.0.0.1:8788"),
		emitter.WithClientID("sys"),
		emitter.WithPassword("123456"),
		emitter.WithUsername("sys-user"),
	}
}
```

I create client with clientid and password,username, but when i publish message like this

```go
c.Publish(key, channel, "hello a", emitter.WithTTL(86400*30), emitter.WithoutEcho(), emitter.WithLast(999), emitter.WithAtLeastOnce())
```

then one client disconnect ,when its online ,it can also receive the old messages, but i want is receive the message that the client never received before.

See title.
Hi, I can't seem to get presence changes working. I tried it 3 different ways and all I get is the initial status, but not the changes (change value is true). 

In the [demo-presence](https://github.com/kelindar/demo-presence) after clicking online/offline repeatedly, the "Now Online" remains empty the entire time, and here is the console output:

```
emitter: connected
app.js:64 {time: 1575448858, event: "status", channel: "presence-demo/d797ea7e/", who: Array(0)}
```

I also tested it with sdk-integration-test in go by setting the changes parameter to true in the Presence call, and swapping the order of clientA() and clientB(), again no presence changes.
I am wondering about something... you need a key to publish a message and subscribe. You don't seem to need a key to actually connect. So what happens to a subscription when the key times out?

**Example**
I create a key with a TTL of 600. I then subscribe to a channel and wait for messages. What happens to that subscription once the 600 seconds times out? Obviously, I could not publish a message because I would need to provide a valid key in order to publish. But what happens to that subscription that is using my connection and waiting on messages? Does it just stop receiving messages?

The reason I ask is.. I want to an app to know when it needs to go get a new key. An easy thing to do is to check for TTL expiration anytime the app is about to publish a message. This way, we don't have to periodically check the key's TTL. The problem I see is for subscriptions. Since we are simply waiting for messages, we don't have an opportunity to "check". We have to assume that the subscription is still valid and receiving "events".

If the key expires, will an event be raised on the connection or the channel?
Hi Team, 

I am exploring the use of emitter for one of our use cases which involves pub sub.

Things that I did:

1. Cloned the repository
2. Built it locally
3. At first run, License and Secret was generated
4. Added the license in emitter.conf
5. Browse to 127.0.0.1:8080/keygen
5. Added the secret, gave the channel name as 'mqtt/' and set the ttl to 30000
6. Got the channel key
7. Connected to 127.0.0.1:8080 using PAHO MQTT
8. Subscribe to channel via <channel_key>/mqtt/
9. Publish a message to <channel_key>/mqtt/

After doing all this, I do not receive message on the subscribed channel. 

**I tried a few things, What worked for me is this! Is this an ideal scenario?**

Channel Details:
channel: testchannel/
key    : bBSJoEcSt9rbI0Bew9QZti31tT7s_ok1

**_First Case :_** 
Subscribe to channel as **bBSJoEcSt9rbI0Bew9QZti31tT7s_ok1/testchannel/**

Publish a message to channel **bBSJoEcSt9rbI0Bew9QZti31tT7s_ok1/testchannel/**

>_I did not receive the message_

**_Second Case :_** 
Subscribe to channel as **testchannel/**

Publish a message to channel **bBSJoEcSt9rbI0Bew9QZti31tT7s_ok1/testchannel/**

>_I did not receive the message_

**_Third Case :_** 

C1 : Subscribe to channel as **bBSJoEcSt9rbI0Bew9QZti31tT7s_ok1/testchannel/**
C2 : Subscribe to channel as **testchannel/**

Publish a message to channel **bBSJoEcSt9rbI0Bew9QZti31tT7s_ok1/testchannel/**

>_I received the message only on C2 i.e **testchannel/**_. No message received on C1.

Disconnect C1 i.e **bBSJoEcSt9rbI0Bew9QZti31tT7s_ok1/testchannel/**

Publish a message to channel **bBSJoEcSt9rbI0Bew9QZti31tT7s_ok1/testchannel/**

> I do not receive any message on either of the channels.


------

I have seen this video as well to check how channels and keys work!
>https://www.youtube.com/watch?v=OcdL_454XT0