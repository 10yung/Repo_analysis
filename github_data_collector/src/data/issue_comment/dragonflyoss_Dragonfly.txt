
# Weekly Report of Dragonfly

This is a weekly report of Dragonfly. It summarizes what have changed in the project during the passed week, including pr merged, new contributors, and more things in the future. 
It is all done by @AliGHRobot which is an AI robot.  See: https://github.com/pouchcontainer/pouchrobot.
## Repo Update 

| Watch | Star | Fork | Contributors | New Issues | Closed Issues |
|:-----:|:----:|:----:|:------------:|:----------:|:-------------:|
|194 (↑1)|4525 (↑15)|602 (↑8)|62 (↑3)|0|0|


## PR Update
		
Thanks to contributions from community, Dragonfly team merged **21** pull requests in the repository last week. All these pull requests could be divided into **feature**, **bugfix**, **doc**, **test** and **others**:
		
### feature 🆕 🔫 

* feat: generate task per http range ([#1180](https://github.com/dragonflyoss/Dragonfly/pull/1180))

### bugfix 🐛 🔪 

* bugfix: retry multi times if failed to report pieces ([#1185](https://github.com/dragonflyoss/Dragonfly/pull/1185))
* bugfix: update cdn fail when the content is being read and the source server is down ([#1152](https://github.com/dragonflyoss/Dragonfly/pull/1152))

### doc 📜 📝 

* docs: auto generate Dragonfly cli/api/contributors docs via code ([#1188](https://github.com/dragonflyoss/Dragonfly/pull/1188))
* docs: auto generate Dragonfly cli/api/contributors docs via code ([#1182](https://github.com/dragonflyoss/Dragonfly/pull/1182))
* docs: auto generate Dragonfly cli/api/contributors docs via code ([#1169](https://github.com/dragonflyoss/Dragonfly/pull/1169))
* docs: add more detail steps in workspace preparation ([#1156](https://github.com/dragonflyoss/Dragonfly/pull/1156))
* docs: update docs about deploy with Physical Machines ([#1120](https://github.com/dragonflyoss/Dragonfly/pull/1120))
* docs: add a doc about using dragonfly with harbor registry ([#1028](https://github.com/dragonflyoss/Dragonfly/pull/1028))
* docs: optimize the config documents ([#1024](https://github.com/dragonflyoss/Dragonfly/pull/1024))

### test ✅ ☑️ 

* test: add unit test case for func GetAsBitset ([#1187](https://github.com/dragonflyoss/Dragonfly/pull/1187))
* test: add unit test case for func GetAsMap ([#1184](https://github.com/dragonflyoss/Dragonfly/pull/1184))
* test: add util test for atomiccount ([#1178](https://github.com/dragonflyoss/Dragonfly/pull/1178))
* test: add unit test case for func GetAsBool ([#1176](https://github.com/dragonflyoss/Dragonfly/pull/1176))
* test: add test case in unit test TestParseFilter ([#1173](https://github.com/dragonflyoss/Dragonfly/pull/1173))
* test: add unit test case for func GetAsString ([#1172](https://github.com/dragonflyoss/Dragonfly/pull/1172))
* test: add unit test case for func GetAsInt ([#1170](https://github.com/dragonflyoss/Dragonfly/pull/1170))

### others

* code clean: remove tmp file (dfget-31028-1577713727.584.tmp-027362658) ([#1183](https://github.com/dragonflyoss/Dragonfly/pull/1183))
* add unit test for GetMsgByCode ([#1179](https://github.com/dragonflyoss/Dragonfly/pull/1179))
* Securing many http links to https links ([#1174](https://github.com/dragonflyoss/Dragonfly/pull/1174))
* Configure dfclient/dfdaemon/dfget log file path via 'logConfig.path' property ([#1145](https://github.com/dragonflyoss/Dragonfly/pull/1145))

## Code Review Statistics 🐞 🐞 🐞 
This project encourages everyone to participant in code review, in order to improve software quality. Every week @pouchrobot would automatically help to count pull request reviews of single github user as the following. So, try to help review code in this project.

| Contributor ID | Pull Request Reviews |
|:--------: | :--------:|
|@lowzj|23|
|@Starnop|4|
|@xujihui1985|1|
|@YanzheL|1|


## New Contributors 🎖 🎖 🎖 

We have no new contributors in this project this week.
Dragonfly team encourages everything about contribution from community.
For more details, please refer to https://github.com/dragonflyoss/Dragonfly/blob/master/CONTRIBUTING.md . 🍻


 Thank all of you!
Signed-off-by: Hu Shuai <hus.fnst@cn.fujitsu.com>

<!-- 
Please make sure you have read and understood the contributing guidelines;
https://github.com/dragonflyoss/dragonfly/blob/master/CONTRIBUTING.md -->

### Ⅰ. Describe what this PR did


### Ⅱ. Does this pull request fix one issue?
<!--If that, add "fixes #xxxx" below in the next line, for example, fixes #15. Otherwise, add "NONE" -->


### Ⅲ. Why don't you add test cases (unit test/integration test)? (你真的觉得不需要加测试吗？)



### Ⅳ. Describe how to verify it


### Ⅴ. Special notes for reviews



Signed-off-by: 楚贤 <chuxian.mjj@antfin.com>

<!-- 
Please make sure you have read and understood the contributing guidelines;
https://github.com/dragonflyoss/dragonfly/blob/master/CONTRIBUTING.md -->

### Ⅰ. Describe what this PR did

Add `ClientStreamWriter` to implement Streaming

### Ⅱ. Does this pull request fix one issue?
<!--If that, add "fixes #xxxx" below in the next line, for example, fixes #15. Otherwise, add "NONE" -->

This PR is pre-commit for Proposal https://github.com/dragonflyoss/Dragonfly/issues/1164

### Ⅲ. Why don't you add test cases (unit test/integration test)? (你真的觉得不需要加测试吗？)

### Ⅳ. Describe how to verify it

### Ⅴ. Special notes for reviews


Signed-off-by: YanzheL <lee.yanzhe@yanzhe.org>

<!-- 
Please make sure you have read and understood the contributing guidelines;
https://github.com/dragonflyoss/dragonfly/blob/master/CONTRIBUTING.md -->

### Ⅰ. Why do you propose this PR

The current implementation of HTTPS hijacking is simple but not correct.

It uses user provided TLS server cert `df.crt` and `df.key` to decrypt HTTPS connection. However, this cert cannot be a CA, which means every TLS connection is encrypted by the same cert from user point of view.

This is not a standard behavior of a HTTPS Man-In-The-Middle proxy, and will cause various issues.

1. The cert used by HTTPS hijacking cannot be automatically verified by applications because the Common Name (or Server-Alternative-Names) is always same and it doesn't match the host of every connection. So user have to configure their applications manually to force-ignore the TLS verification.

   This prevents `dfclient` be used as a general system-level HTTPS proxy without affecting user applications.

2. Some applications cannot be configured to trust a specific TLS cert or ignore TLS verification error (e.g.  Google Chrome). Instead, the only way to achieve it is to configure them to trust the CA, or add the CA to system trust store.

### II. Describe what this PR did

1. If `df.crt` and `df.key` is a CA key-pair, then `dfclient` use it to issue leaf TLS certs for every connection whose host matches pre-configured hijacking rules.

   User can either add the CA to system trust store, or configure individual application to trust it. 

   Since the common name of leaf cert is set as the target host, the connection will be verified by user application automatically as normal.

2. If `df.crt` and `df.key` is not a CA key-pair, the behavior of `dfclient` is same as the old way: this cert is used in hijacking instead of generating new certs per connection.

3. So this PR is fully backward-compatible and will NOT break user applications.


### ⅡI. Does this pull request fix one issue?
<!--If that, add "fixes #xxxx" below in the next line, for example, fixes #15. Otherwise, add "NONE" -->

Potential issues are stated above.

### IV. Potential use cases

1. Maybe now we can cache HTTPS docker registries (Probably fixes #525).

   The dfdaemon acts as a decrypting MITM HTTPS proxy, so it can 'see' the request body of docker image pull requests to remote private HTTPS registries. If we can see the body, then we can cache it as well.

2. Generic HTTPS caching proxy, just like squid. Cache anything in a distributed way, and not just for HTTP contents. We can also use dfdaemon to speed up normal webpage loading in browser.


### V. Why don't you add test cases (unit test/integration test)? (你真的觉得不需要加测试吗？)

I'm working on unit tests, but currently I don't have much time....
For now, I just tested this feature in container, and everything seems good.

### VI. Describe how to verify it

1. Prepare a self-signed CA with private key.

2. Configure dfdaemon to use this key pair, and also configure the hijack rules, proxy rules.

   ```yaml
   proxies:
     - regx: blobs/sha256.* # Caching docker images
     - regx: '.*\.png' # Caching png files.
   hijack_https:
     cert: ca.crt
     key: ca.key
     hosts:
       - regx: '.*'    # Decrypt all sites, for test.
   ```

3. Setup dfdaemon as your system's http(s) proxy.

   ```shell
   export HTTP_PROXY=http://127.0.0.1:65001; export HTTPS_PROXY=http://127.0.0.1:65001;
   ```

4. Check the TLS cert return by target site.

   ```shell
   curl -vkL https://alibaba.com -o /dev/null
   ```

   This command will report that the TLS cert of `alibaba.com`is signed by your CA.

   The dfdaemon log will indicate that it is downloading png files of `alibaba.com`


### ⅤII. Special notes for reviews

This PR reuses CA's private key (and signature algorithm) to generate per-connection certs. I think it can save key-generation overhead and there is no need to use new TLS private key for every connection since the generated cert is temporal (Default valid time is 24 hours).

As for security, if the private key of CA is leaked someway, generating new private key for every connection will not improve security (maybe???). So reusing CA private key does not bring much security issues.

If this is not appropriate, I can change it.
## Question
<!-- You can ask any question about this project -->

--showbar command is supported in python，why not supprted in go？

# Weekly Report of Dragonfly

This is a weekly report of Dragonfly. It summarizes what have changed in the project during the passed week, including pr merged, new contributors, and more things in the future. 
It is all done by @AliGHRobot which is an AI robot.  See: https://github.com/pouchcontainer/pouchrobot.
## Repo Update 

| Watch | Star | Fork | Contributors | New Issues | Closed Issues |
|:-----:|:----:|:----:|:------------:|:----------:|:-------------:|
|193 (↑1)|4510 (↑9)|594 (↑5)|59 (↑1)|0|0|


## PR Update
		
Thanks to contributions from community, Dragonfly team merged **8** pull requests in the repository last week. All these pull requests could be divided into **feature**, **bugfix**, **doc**, **test** and **others**:
		
### bugfix 🐛 🔪 

* fix: use StringArrayVar to parse http header ([#1166](https://github.com/dragonflyoss/Dragonfly/pull/1166))

### doc 📜 📝 

* docs: auto generate Dragonfly cli/api/contributors docs via code ([#1169](https://github.com/dragonflyoss/Dragonfly/pull/1169))

### test ✅ ☑️ 

* test: add unit test case for func GetAsInt ([#1170](https://github.com/dragonflyoss/Dragonfly/pull/1170))
* test: add unit test case for func Remove ([#1167](https://github.com/dragonflyoss/Dragonfly/pull/1167))
* test: add unit test case for func ParsePieceIndex ([#1160](https://github.com/dragonflyoss/Dragonfly/pull/1160))

### others

* fix typo: fix comment for func Print ([#1165](https://github.com/dragonflyoss/Dragonfly/pull/1165))
* fix typo: fix comment for func ParseNodesString ([#1163](https://github.com/dragonflyoss/Dragonfly/pull/1163))
* Refactor: make version subcommand as a public subcommand ([#1159](https://github.com/dragonflyoss/Dragonfly/pull/1159))

## Code Review Statistics 🐞 🐞 🐞 
This project encourages everyone to participant in code review, in order to improve software quality. Every week @pouchrobot would automatically help to count pull request reviews of single github user as the following. So, try to help review code in this project.

| Contributor ID | Pull Request Reviews |
|:--------: | :--------:|
|@lowzj|10|
|@xujihui1985|1|
|@truongnh1992|1|
|@Starnop|1|


## New Contributors 🎖 🎖 🎖 

We have no new contributors in this project this week.
Dragonfly team encourages everything about contribution from community.
For more details, please refer to https://github.com/dragonflyoss/Dragonfly/blob/master/CONTRIBUTING.md . 🍻


 Thank all of you!
## Question

看FAQ中说明如果在dfdaemon中通过registry设置为第三方仓库，那么下载就是从第三方仓库拉取镜像，但是我测试并没有生效，我的dfdaemon改成`dfdaemon --registry http://xxx.xx.73.71:5000 --node xxx.xx.73.72:8002,xxx.xx.73.72:8004`,通过`docker pull xxx`,结果并没有通过dfdaemon下载镜像，而是直接下载的，dfdaemon.log中没有任何日志记录。

## Backgrounds

Now the client of Dragonfly will random read and write disk multiple times during the downloading process. 
For directly using `dfget` to download a file:
* `dfget` random writes a piece into disk after downloading it
* `dfget server` random reads the piece from disk to share it
* `dfget` sequential reads the file from disk after downloading to do checksum

And for using `dfdaemon` to pull images, there're extra disk IO by Dragonfly:
* `dfdaemon` sequential reads the file from disk to send it to `dockerd`

It's not a problem when the host has a local disk. But it will be a potential 
 bottleneck when the Dragonfly client runs on a virtual machine with a cloud disk, all the disk IO will become network IO which has a bad performance when read/write at the same time.

So a solution is needed to reduce the IO times generated by Dragonfly.

## Idea

**P2P Streaming** is a P2P based on Streaming, which sends the data downloaded by using p2p pattern to the user directly, in order to achieve the purpose of reading and writing to disk as few as possible.

### P2P Streaming Data Flow

This diagram describes the p2p streaming data flow.

![image](https://user-images.githubusercontent.com/2419214/72035542-2cd70a00-32d3-11ea-982f-a903218c8b7f.png)

* `Piece Data Cache` stores the pieces' data in memory that can be shared to the other peers. A piece's data should be putted into this cache after downloading, and be evicted according to the `LRU` strategy when the cache is full.
* `StreamIO` sends pieces' data to callers in ascending order of `piece's number`.
* In the scenario of using `dfdaemon` to pulling images and others , the `dfdaemon` and `dfget` should be merged into one process that can reduce the time of starting `dfget` process.
* Also `dfget` can be as an individual process to download files directly.

### P2P Streaming Sliding Window

The `P2P Streaming Sliding Window` is designed to control the number of pieces of a file that can be scheduled and downloaded to avoid unlimited memory usage. This idea comes from tcp sliding window, but its minimal transmission unit is a `piece` not a `byte`.

![image](https://user-images.githubusercontent.com/2419214/72036461-e505b200-32d5-11ea-929e-0955ce5be993.png)

* `Memory Cache` is the `Piece Data Cache` to share pieces in the p2p network. The larger the cache, the higher the p2p transmission efficiency.



### Ⅰ. Issue Description
SuperNode 日志中记录的时间戳是UTC+0的时间非本地时间，例如
**机器当前时间**
$date
Mon Jan  6 12:12:37 CST 2020
**supernode 输出时间：**
2020-01-06 04:12:40.338 WARN sign:110318

### Ⅱ. Describe what happened


### Ⅲ. Describe what you expected to happen


### Ⅳ. How to reproduce it (as minimally and precisely as possible)

1.
2.
3.

### Ⅴ. Anything else we need to know?


### Ⅵ. Environment:

- dragonfly version:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:

## Question
现象：多个docker pull请求下载时，偶尔会出现某个节点下载失败。
场景：本轮请求13台设备在18:00发起docker pull请求，12台设备59-60s后下载成功，但是一台失败并且无限轮询持续请求，持续失败。
supernode log：
2020-01-02 18:02:31.521 INFO sign:2801 : gc peer: start to deal with peer: tst4-xxx.com-xxx.xx.73.65-1577958913537379557

2020-01-02 18:08:32.552 WARN sign:2801 : failed to get dfget task by dstCID(xxx.xx.73.65-10966-1577959254.445) and taskID(d92205d4ce2007422736d99a34c53d8263b157a8362c4894e1f926c5b406362f), and the srcCID is xxx.xx.73.2-12305-1577959255.676, err: failed to get key xxx.xx.73.65-10966-1577959254.445@d92205d4ce2007422736d99a34c53d8263b157a8362c4894e1f926c5b406362f from map: {"Code":0,"Msg":"data not found"}
dfclient log:
2020-01-02 19:18:03.494 INFO sign:12305-1577959255.676 : pull piece task({"taskID":"d92205d4ce2007422736d99a34c53d8263b157a8362c4894e1f926c5b406362f","superNode":"xxx.xx.72.238:8002","dstCid":"xxx.xx.73.65-10966-1577959254.445","range":"167772160-171966463","result":503,"status":701,"pieceSize":4194304,"pieceNum":40}) result:{"code":602,"msg":"taskID(d92205d4ce2007422736d99a34c53d8263b157a8362c4894e1f926c5b406362f) clientID(172.22.73.2-12305-1577959255.676): {\"Code\":9,\"Msg\":\"peer should wait\"}"} and sleep 1.327s

这里xxx.xx.73.65同样也是一台发起请求的节点，看到这里我的认知是supernode让xxx.xx.73.2从xxx.xx.73.65下载相应的piece，但是supernode在18:08:31时gc了节点的信息导致xxx.xx.73.2再来拉取信息时失败。

我想咨询下为什么会出现这种情况，这应该不算合理的情况吧，请问还需要提供什么信息吗？

我自己这边也结合源码定位

