
![image](https://user-images.githubusercontent.com/37984270/57641483-bca4a700-75d2-11e9-8364-520627223f7b.png)
The generator loss diverges and does not converge
Even after adding Gaussian noise to input the G_loss diverges. Please Help


Hey guys, I'm using the conditional-gan code for tensorflow version to generate garment drawings, the training images is RGB png file, labels are vectors like this [1,2,3,4,...,2]ï¼ŒI just changed the feeding training data from mnist to my data, but the generated images are shapeless, I don't know why, someone helps me? thanks!!
The attached is the modified code and images generated.


[code+generated images.zip](https://github.com/wiseodd/generative-models/files/2897643/code%2Bgenerated.images.zip)

Hello WiseOdd,

I have recognise you vanilla VAE, which seems pretty neat despite of the fact that does not work as I remember a Gassian noise sparse model would work.

I have recently read [1606.05908](https://arxiv.org/pdf/1606.05908.pdf) where VAE are explained quite good more or less.
Threre is the PDF for X described as expectation value over $P(Z)$. Now, when you update your parameters of the Gaussian, normally the variance is a diagonal matrix of $\sigma^2$.
So, of cause one could think  about a variance for each point in the dimensionality of X, but I am not qute sure that is the proper common idea behind the variance of the Gaussian distribution in generative models.

$P(X) = \frac{1}{D_z} \sum_{\forall z \in Z} P(X|Z, \Theta) P(Z)$ using this annotation, the expectation for $\sigma^2$ based on the data would be something similar to $\sum_{\forall n \in N} \braket{X-\mu(z;\Theta)|X-\mu(z;\Theta)}$, right?

Maybe you can take a look on that code. Because your variance seems to appear as a matrix instead of a value or set of values for the Gaussian noise model.

So, did you thought explicitly about a full covariance matrix or was it just trial and error in this case?

If trying out expectation value(s) for the variance, let me know, what your experience is.

regards,
Markus

change the deprecated torch.Varialbe api into torch.tensor with requires_grad ==true
hey first of all  thank you for your great job it's very clear in general and helpful 


My first question is your loop enumerate V_s which is something completely random does it has to enumerate in X_mb
https://github.com/wiseodd/generative-models/blob/b930d5fa9e2f69adfd4ea8ec759f38f6ce6da4c2/RBM/rbm_binary_pcd.py#L54


My second I think in this line you have to change v_s to v_prime like CD if not why ? 
https://github.com/wiseodd/generative-models/blob/b930d5fa9e2f69adfd4ea8ec759f38f6ce6da4c2/RBM/rbm_binary_pcd.py#L57
SystemExit: 2
/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.
  warn("To exit: use 'exit', 'quit', or Ctrl-D.", stacklevel=1)
I can see the implementation computes KL div between q(z|x,c) and p(z)=N(0,I) but the original formulation has conditional prior p(z|c). 

Can you explain why you are still using zero mean unit Gaussian prior?
This replaces 0-dim tensors with item() to get a Python number from the tensors containing a single value.
Hi,
Can you explain why multiply 0.5 from gradient ?
```
    # Average the gradients
    for p in D_shared.parameters():
        p.grad.data = 0.5 * p.grad.data

```