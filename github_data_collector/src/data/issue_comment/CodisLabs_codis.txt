你好，codis3线上运行的时候，出现运行中slot分配错乱的问题。

总共30个分组，每个分组的slot显示都是35个或34个，运行都很平稳。
昨天线上出现个问题，就是30个分组里有8个分组的数据量突然异常上涨。
排查了一下，业务写入没有问题。通过slot监控写入的1024个key显示，__codis_mt_slot_527_50B这个key原本应该落在group23，却在group2中也有，删除后监控写入的__codis_mt_slot_527_50B还是会在group2出现，group2出现了102个__codis_mt_slot_**。今天再次以这种方法确认又没问题了。

通过导出多个分组key也显示同一个key会有2份

请帮忙解答一下，感觉是个大坑哎

codis dashboard进程挂掉,有什么方法可以在dashboard挂掉时，自动重启dashboard
我排查了哨兵服务日志及codis服务 未发现任何问题 且 codis服务正常使用，但是UI报错
![image](https://user-images.githubusercontent.com/41365187/71334258-56283400-2578-11ea-9524-410c6c532f39.png)
以下是
curl  http://woqx-prd-codis01:18080/api/topom/sentinels/info/172.16.0.237:26379
{
    "arch_bits": "64",
    "blocked_clients": "0",
    "client_biggest_input_buf": "0",
    "client_longest_output_list": "0",
    "config_file": "/usr/local/codis/./conf/sentinel.conf",
    "connected_clients": "6",
    "evicted_keys": "0",
    "executable": "/usr/local/codis/./bin/redis-sentinel",
    "expired_keys": "0",
    "gcc_version": "4.8.4",
    "hz": "16",
    "instantaneous_input_kbps": "0.47",
    "instantaneous_ops_per_sec": "9",
    "instantaneous_output_kbps": "9.09",
    "keyspace_hits": "0",
    "keyspace_misses": "0",
    "latest_fork_usec": "0",
    "lru_clock": "12822",
    "master0": "name=codis-woqx-prd-2,status=ok,address=172.16.0.237:6380,slaves=1,sentinels=1",
    "master1": "name=codis-woqx-1,status=ok,address=172.16.0.236:6380,slaves=1,sentinels=1",
    "master2": "name=codis-woqx-3,status=ok,address=172.16.0.238:6380,slaves=1,sentinels=1",
    "master3": "name=codis-woqx-2,status=ok,address=172.16.0.237:6380,slaves=1,sentinels=1",
    "master4": "name=codis-woqx-prd-3,status=ok,address=172.16.0.238:6380,slaves=1,sentinels=1",
    "master5": "name=codis-woqx-prd-1,status=ok,address=172.16.0.236:6380,slaves=1,sentinels=1",
    "migrate_cached_sockets": "0",
    "multiplexing_api": "epoll",
    "os": "Linux 2.6.32-696.16.1.el6.x86_64 x86_64",
    "process_id": "10596",
    "pubsub_channels": "1",
    "pubsub_patterns": "0",
    "redis_build_id": "2bdb8aa56be3fbc2",
    "redis_git_dirty": "0",
    "redis_git_sha1": "f8bc4e32",
    "redis_mode": "sentinel",
    "redis_version": "3.2.9",
    "rejected_connections": "0",
    "run_id": "dc8ec227f0dec83db9422731a2dbae13a5122dd2",
    "sentinel_masters": "6",
    "sentinel_running_scripts": "0",
    "sentinel_scripts_queue_length": "0",
    "sentinel_simulate_failure_flags": "0",
    "sentinel_tilt": "0",
    "sync_full": "0",
    "sync_partial_err": "0",
    "sync_partial_ok": "0",
    "tcp_port": "26379",
    "total_commands_processed": "272979680",
    "total_connections_received": "532533",
    "total_net_input_bytes": "13289936062",
    "total_net_output_bytes": "281684568018",
    "uptime_in_days": "461",
    "uptime_in_seconds": "39876601",
    "used_cpu_sys": "63347.82",
    "used_cpu_sys_children": "0.00",
    "used_cpu_user": "55139.51",
    "used_cpu_user_children": "0.00"
}
Sentinels日志
10596:X 18 Dec 11:17:26.160 # +set master codis-woqx-3 172.16.0.238 6380 parallel-syncs 1
10596:X 18 Dec 11:17:26.160 # +set master codis-woqx-3 172.16.0.238 6380 down-after-milliseconds 30000
10596:X 18 Dec 11:17:26.160 # +set master codis-woqx-3 172.16.0.238 6380 failover-timeout 300000
10596:X 18 Dec 11:17:26.160 # +set master codis-woqx-3 172.16.0.238 6380 auth-pass redis1234
10596:X 18 Dec 11:17:26.165 # +set master codis-woqx-1 172.16.0.236 6380 parallel-syncs 1
10596:X 18 Dec 11:17:26.165 # +set master codis-woqx-1 172.16.0.236 6380 down-after-milliseconds 30000
10596:X 18 Dec 11:17:26.165 # +set master codis-woqx-1 172.16.0.236 6380 failover-timeout 300000
10596:X 18 Dec 11:17:26.165 # +set master codis-woqx-1 172.16.0.236 6380 auth-pass redis1234
10596:X 18 Dec 11:17:26.168 # +set master codis-woqx-2 172.16.0.237 6380 parallel-syncs 1
10596:X 18 Dec 11:17:26.168 # +set master codis-woqx-2 172.16.0.237 6380 down-after-milliseconds 30000
10596:X 18 Dec 11:17:26.168 # +set master codis-woqx-2 172.16.0.237 6380 failover-timeout 300000
10596:X 18 Dec 11:17:26.168 # +set master codis-woqx-2 172.16.0.237 6380 auth-pass redis1234
10596:X 18 Dec 11:17:26.171 * +slave slave 172.16.0.238:6381 172.16.0.238 6381 @ codis-woqx-2 172.16.0.237 6380
10596:X 18 Dec 11:17:26.175 * +slave slave 172.16.0.237:6381 172.16.0.237 6381 @ codis-woqx-1 172.16.0.236 6380
10596:X 18 Dec 11:17:26.178 * +slave slave 172.16.0.236:6381 172.16.0.236 6381 @ codis-woqx-3 172.16.0.238 6380
10596:X 23 Dec 11:13:06.725 # -monitor master codis-woqx-3 172.16.0.238 6380
10596:X 23 Dec 11:13:06.729 # -monitor master codis-woqx-2 172.16.0.237 6380
10596:X 23 Dec 11:13:06.732 # -monitor master codis-woqx-1 172.16.0.236 6380
10596:X 23 Dec 11:13:06.743 # +monitor master codis-woqx-2 172.16.0.237 6380 quorum 2
10596:X 23 Dec 11:13:06.746 # +monitor master codis-woqx-3 172.16.0.238 6380 quorum 2
10596:X 23 Dec 11:13:06.749 # +monitor master codis-woqx-1 172.16.0.236 6380 quorum 2
10596:X 23 Dec 11:13:06.749 # +set master codis-woqx-2 172.16.0.237 6380 parallel-syncs 1
10596:X 23 Dec 11:13:06.749 # +set master codis-woqx-2 172.16.0.237 6380 down-after-milliseconds 30000
10596:X 23 Dec 11:13:06.749 # +set master codis-woqx-2 172.16.0.237 6380 failover-timeout 300000
10596:X 23 Dec 11:13:06.749 # +set master codis-woqx-2 172.16.0.237 6380 auth-pass redis1234
10596:X 23 Dec 11:13:06.752 # +set master codis-woqx-3 172.16.0.238 6380 parallel-syncs 1
10596:X 23 Dec 11:13:06.752 # +set master codis-woqx-3 172.16.0.238 6380 down-after-milliseconds 30000
10596:X 23 Dec 11:13:06.752 # +set master codis-woqx-3 172.16.0.238 6380 failover-timeout 300000
10596:X 23 Dec 11:13:06.752 # +set master codis-woqx-3 172.16.0.238 6380 auth-pass redis1234
10596:X 23 Dec 11:13:06.755 # +set master codis-woqx-1 172.16.0.236 6380 parallel-syncs 1
10596:X 23 Dec 11:13:06.755 # +set master codis-woqx-1 172.16.0.236 6380 down-after-milliseconds 30000
10596:X 23 Dec 11:13:06.755 # +set master codis-woqx-1 172.16.0.236 6380 failover-timeout 300000
10596:X 23 Dec 11:13:06.755 # +set master codis-woqx-1 172.16.0.236 6380 auth-pass redis1234
10596:X 23 Dec 11:13:06.803 * +slave slave 172.16.0.238:6381 172.16.0.238 6381 @ codis-woqx-2 172.16.0.237 6380
10596:X 23 Dec 11:13:06.809 * +slave slave 172.16.0.236:6381 172.16.0.236 6381 @ codis-woqx-3 172.16.0.238 6380
10596:X 23 Dec 11:13:06.814 * +slave slave 172.16.0.237:6381 172.16.0.237 6381 @ codis-woqx-1 172.16.0.236 6380

您好！ 想获取一个这个版本的codis，麻烦您授权下哈！
http://www.juvenxu.com/2015/03/20/experiences-on-zookeeper-ops/ 
http://0xffff.me/blog/2014/11/11/codis-de-she-ji-yu-shi-xian-part-2/
these two links is invalid, could anyone can update ?
2019/12/06 10:37:24 main.go:104: [WARN] set ncpu = 4, max-ncpu = 12
2019/12/06 10:37:24 main.go:128: [WARN] option --dashboard = 192.168.101.10:18080
2019/12/06 10:37:24 proxy.go:91: [WARN] [0xc4200a88f0] create new proxy:
{
    "token": "2aa5bee4d10e9479851e3a91a9cc221b",
    "start_time": "2019-12-06 10:37:24.266633566 +0800 CST m=+0.008529628",
    "admin_addr": "ilink-master:11080",
    "proto_type": "tcp4",
    "proxy_addr": "ilink-master:19000",
    "product_name": "codis-demo",
    "pid": 32314,
    "pwd": "/home/qianli/icloud/codis/bin",
    "sys": "Linux ilink-master 3.10.0-514.el7.x86_64 #1 SMP Tue Nov 22 16:42:41 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux",
    "hostname": "ilink-master",
    "datacenter": ""
}
2019/12/06 10:37:24 proxy.go:378: [WARN] [0xc4200a88f0] admin start service on [::]:11080
2019/12/06 10:37:24 main.go:193: [WARN] create proxy with config
proto_type = "tcp4"
proxy_addr = "0.0.0.0:19000"
admin_addr = "0.0.0.0:11080"
jodis_name = ""
jodis_addr = ""
jodis_auth = ""
jodis_timeout = "20s"
jodis_compatible = false
product_name = "codis-demo"
product_auth = ""
session_auth = ""
proxy_datacenter = ""
proxy_max_clients = 1000
proxy_max_offheap_size = "1gb"
proxy_heap_placeholder = "256mb"
backend_ping_period = "5s"
backend_recv_bufsize = "128kb"
backend_recv_timeout = "30s"
backend_send_bufsize = "128kb"
backend_send_timeout = "30s"
backend_max_pipeline = 20480
backend_primary_only = false
backend_primary_parallel = 1
backend_replica_parallel = 1
backend_keepalive_period = "75s"
backend_number_databases = 16
session_recv_bufsize = "128kb"
session_recv_timeout = "30m"
session_send_bufsize = "64kb"
session_send_timeout = "30s"
session_max_pipeline = 10000
session_keepalive_period = "75s"
session_break_on_failure = false
metrics_report_server = ""
metrics_report_period = "1s"
metrics_report_influxdb_server = ""
metrics_report_influxdb_period = "1s"
metrics_report_influxdb_username = ""
metrics_report_influxdb_password = ""
metrics_report_influxdb_database = ""
metrics_report_statsd_server = ""
metrics_report_statsd_period = "1s"
metrics_report_statsd_prefix = ""
2019/12/06 10:37:24 proxy.go:402: [WARN] [0xc4200a88f0] proxy start service on 0.0.0.0:19000
2019/12/06 10:37:24 main.go:206: [WARN] option --pidfile = /home/qianli/icloud/codis/bin/codis-proxy.pid
2019/12/06 10:37:24 main.go:229: [WARN] [0xc4200a88f0] proxy waiting online ...
2019/12/06 10:37:24 main.go:340: [WARN] rpc online proxy failed
[error]: [Remote Error] store: load proxy failed
    4   /home/gopath/src/github.com/CodisLabs/codis/pkg/topom/topom_cache.go:70
            github.com/CodisLabs/codis/pkg/topom.(*Topom).refillCache
    3   /home/gopath/src/github.com/CodisLabs/codis/pkg/topom/topom.go:270
            github.com/CodisLabs/codis/pkg/topom.(*Topom).newContext
    2   /home/gopath/src/github.com/CodisLabs/codis/pkg/topom/topom_proxy.go:48
            github.com/CodisLabs/codis/pkg/topom.(*Topom).OnlineProxy
    1   /home/gopath/src/github.com/CodisLabs/codis/pkg/topom/topom_api.go:259
            github.com/CodisLabs/codis/pkg/topom.(*apiServer).OnlineProxy
    0   /home/gopath/src/github.com/CodisLabs/codis/pkg/topom/topom_api.go:82
            github.com/CodisLabs/codis/pkg/topom.(*apiServer).OnlineProxy-fm

`eval "local flag = 0 local targetScore = redis.call('get', KEYS[1]) local currentCount = 0 if (not targetScore) then redis.call('set', KEYS[1], 0) if (ARGV[2] ~= nil and ARGV[2] ~= '') then redis.call('expire', KEYS[1], tonumber(ARGV[2]))     end else currentCount = tonumber(targetScore) end local limitCount = tonumber(ARGV[1]) if (currentCount < limitCount or limitCount == 0) then     currentCount = tonumber(redis.call('incr', KEYS[1])) flag = 1  end local ret = {} ret[1] = flag ret[2] = currentCount return ret" 1 lua_expire_test  1 -1`
1）When we executed the above lua script on version 3.2.8, we found the master-slave data inconsistent
  

**master :  ttl lua_expire_test    result: -1
     slave   :  ttl lua_expire_test     result: -2**


2）Then we try again and execute the `monitor `command on the slave
`1575463540.730292 [0 xx.xx.xx.xx:6380] "eval" "local flag = 0 local targetScore = redis.call('get', KEYS[1]) local currentCount = 0 if (not targetScore) then redis.call('set', KEYS[1], 0) if (ARGV[2] ~= nil and ARGV[2] ~= '') then redis.call('expire', KEYS[1], tonumber(ARGV[2]))     end else currentCount = tonumber(targetScore) end local limitCount = tonumber(ARGV[1]) if (currentCount < limitCount or limitCount == 0) then     currentCount = tonumber(redis.call('incr', KEYS[1])) flag = 1  end local ret = {} ret[1] = flag ret[2] = currentCount ret[3]=tonumber(ARGV[2]) return ret" "1" "lua_expire_test" "1" "-1"
1575463540.730380 [0 lua] "get" "lua_expire_test"
1575463540.730391 [0 lua] "set" "lua_expire_test" "0"
1575463540.730411 [0 lua] "expire" "lua_expire_test" "-1"
1575463540.730419 [0 lua] "incr" "lua_expire_test"`

There seems to be nothing wrong with the sync script.

Could you check whether this is expected behavior or an issue ? Thanks.


[myid:] - INFO [main-SendThread(test001:2181):ClientCnxn$SendThread@1025] - Opening socket connection to server test001/10.10.7.235:2181. Will not attempt to authenticate using SASL (unknown error)
一直报[myid：]-信息[main-SendThread（test001：2181）：ClientCnxn $ SendThread @ 1025]-打开与服务器test001 / 10.10.7.235：2181的套接字连接。 不会尝试使用SASL进行身份验证（未知错误）

早上一开始能连上，下午一直报这个错误，连不上zookeeper
有没有大佬能给我解决一下

