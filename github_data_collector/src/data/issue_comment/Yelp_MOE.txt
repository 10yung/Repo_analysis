After building from source in a virutal environment i get the following error when running the following example code:

    # Sample 20 points
    for i in range(20):
        # Use MOE to determine what is the point with highest Expected Improvement to use next
        next_point_to_sample = gp_next_points(exp)[0]  # By default we only ask for one point
        # Sample the point from our objective function, we can replace this with any function
        value_of_next_point = function_to_minimize(next_point_to_sample)

        print "Sampled f({0:s}) = {1:.18E}".format(str(next_point_to_sample), value_of_next_point)

        # Add the information about the point to the experiment historical data to inform the GP
        exp.historical_data.append_sample_points([SamplePoint(next_point_to_sample, value_of_next_point, 0.01)])  # We can add some noise

ERROR:
  File "sd.py", line 24, in <module>
    next_point_to_sample = gp_next_points(exp)[0]  # By default we only ask for one point
  File "/home/hallab/moe_e/MOE/moe/easy_interface/simple_endpoint.py", line 54, in gp_next_points
    json_response = call_endpoint_with_payload(rest_host, rest_port, endpoint, json_payload, testapp)
  File "/home/hallab/moe_e/MOE/moe/easy_interface/simple_endpoint.py", line 24, in call_endpoint_with_payload
    with contextlib.closing(urllib2.urlopen(request)) as f:
  File "/usr/lib/python2.7/urllib2.py", line 154, in urlopen
    return opener.open(url, data, timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
urllib2.URLError: <urlopen error [Errno 111] Connection refused>
My collaborator added some covariance functions (using polynomial exponential and matern kernels) to the gpp_covariance.cpp and gpp_covariance.hpp and the python wrapper covariance.py and also in domain.py

however when attempting to build moe with these changes -- cmake succeeds meaning the C++ code seems good, but I don't understand how the python code gets compiled and placed in the appropriate place. Therefore I don't know how to build moe so that I can use these new covariance functions through the python interface. 

Could some detail be added to the documentation regarding adding custom functions via C+++ with wrappers?
Hey I noticed there haven't been any commits since January 2017. I was just curious if this project was essentially abandoned by the developers? Or was it deemed sufficient for use? Or was the project brought internally back to Yelp? 
The algorithm looks a lot similar to spearmint, which also use GP + EI.

Just curious if there is any comparison or is it simply a commericial version of spearmint?

I found a 69MiB `moe.log` file (which contains logs since 2014-06-28 till 2014-11-18), 79MiB `.git` repo, `.DS_Store` files, vim temp files (end with `~`), etc. in the final docker image. All of them are useless for a production-ready docker image.

Besides, once built, building tools are no longer useful, so I suggest you use two-steps build that copy the generated binary files into another image.
Hi there, I have been trying to run a project which is built upon the MOE
there is an error showing that
**import moe.build.GPP as C_GP
ImportError: No module named build.GPP**
I am under MOE/moe/optimal_learning/python/cpp_wrappers/gaussian_process.py 
I check the file and there is no build file under moe.
What should I do?
This is a collection of minor fixes that resolve:
 * A compilation error
 * Dramatically reduce the amount of warnings generated by GCC 7.x
I am assuming that MOE doesn't support user-defined prior mean functions, and that the only way to incorporate prior knowledge into MOE is by carefully adding fake points with a large error on them?
First of all, I am really amazed by the wonderful, neat interface of MOE. I appreciate it.

I don't understand why there are noise_variance attribute in [SamplePoint](http://yelp.github.io/MOE/moe.optimal_learning.python.html?highlight=samplepoint#moe.optimal_learning.python.data_containers.SamplePoint).

In other words, why does every datapoint have their own noise value? I don't think it is about **multi-fidelity** setting. If so, it would be truly amazing though.

So, what does noise_variance assigned to each data point mean, and what value should I plug in to it?