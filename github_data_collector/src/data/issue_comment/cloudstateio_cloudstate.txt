Here: https://github.com/cloudstateio/cloudstate#client-api
https://discuss.lightbend.com/t/changing-maximum-response-size-getting-error-resource-exhausted-compressed-grpc-message-exceeds-maximum-size-4194304/5337

**_What did you see?_**

https://cloudstate.io/docs/user/lang/index.html#3rd-party-libraries
<img width="519" alt="Screenshot 2020-01-10 at 11 42 38" src="https://user-images.githubusercontent.com/12616445/72147211-4f822500-339e-11ea-8088-172eb805f1df.png">

**_What did you expect to see?_**
more emojions

**_Additional Information_**
#170 might have introduced that.
Consider adding support in the Cloudstate protocol for propagating request and response metadata (think for instance headers) so that it is possible to inspect and return things like HTTP or HTTP/2 headers for a given request.

Open questions:

Should this apply to ALL requests/responses?
Should they be preserved no matter what?
Do we only allow standard headers?
Do we only allow custom headers?
Do we only allow specific headers?
Do we exclude specific headers?
Can we achieve most of the use-cases without changing user code or the protocol?
Do we only allow it for some specific types? (think HttpBody requests)

Discuss!
Currently, there's no simple way to run the proxy locally, which means there's no way to run a stateful service locally and talk to it using its gRPC interface.

Of course, you can run minikube or something like that locally, but for development, you may want to run your function with a debugger, for example, so you want to run it on your host machine, not in a container.

For a development environment, I think the basic idea would be to run the proxy in docker, and have it call into the function running on the host container. There's a few challenges with this, the biggest one is that there's no standard, cross platform way for a docker container to know the IP address of the host, see:

https://nickjanetakis.com/blog/docker-tip-65-get-your-docker-hosts-ip-address-from-in-a-container

So we'll probably need some tooling to facilitate this. Let's say we had a `cloudstate` command, I would imagine something like this:

```
cloudstate run-proxy --http-port=9000 --user-function-port=8080
```

This would roughly translate to running:

```
docker run --rm -e USER_FUNCTION_PORT=8080 -e USER_FUNCTION_HOST=<host-ip-address> -p 9000:9000 cloudstateio/cloudstate-proxy-native-dev-mode:latest
```

Where `<host-ip-address>` was `host.docker.internal` on OSX and Windows, and the output of `ip -4 addr show docker0 | grep -Po 'inet \K[\d.]+'` on Linux. Right now, the `USER_FUNCTION_HOST` environment variable isn't read, so we'd need to add support for that. 9000/8080 could be the defaults, so if you were just testing one function you could use `cloudstate run-proxy`, but then if you wanted to run multiple, you'd specify ports.

Having run the above, you could now instantiate a client for your gRPC interface and connect to it on `localhost:9000`. Easy.

We might also consider whether we want to support bootstrapping a cluster, this would be a lot more involved of course. It may make more sense to simply say if you want to test with a cluster, use minikube.
ref: https://kubernetes.io/blog/2019/06/20/crd-structural-schema/

* As CRDs have become v1 stable now in K8s, they will require a schema going forward. 
* Having a schema means we can generate API documentation and users know exactly what fields are available in the CR.

This has some further implications to the current CRD structure. We can't use a common `config` for `StatefulStore`, for example. It needs to be split up into structured schemas for each type, like a `postgresConfig`, and so on.

The current CRD is defined like this:

```yaml
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: statefulstores.cloudstate.io
spec:
  group: cloudstate.io
```

This seems to break convention of defining a CRD group as a sub-domain of the  owned domain. Examples are [Istio](https://github.com/istio/istio/blob/f1e6a60efc3c42b5b4fe7a15f8a66e97352314e1/install/kubernetes/helm/istio-init/files/crd-mixer.yaml#L15) which uses `config.istio.io`, [Prometheus](https://github.com/coreos/prometheus-operator/blob/f94728d25fb61a466e3736e640be6415ed0545ed/example/prometheus-operator-crd/podmonitor.crd.yaml#L8) which uses `monitoring.coreos.com`.

I ran into this because `kubebuilder` expects the API group to be a sub-domain of the CRD's domain. So it thinks this CRD's domain is `io`. One common sub-domain used for group is `crd`, so we could consider changing it to `crd.cloudstate.io`.

I think technically this is allowed, but as convention seems to indicate using a sub-domain, we should consider changing it to prevent future incompatibilities with tools and changes to k8s itself.
No one knows what CRDTs are, and they just introduce a layer of unfamiliarity. We should rename them, eg, call the replicated entities. This would mean updating all of the APIs to use this terminology, as well as all the docs.
â€¦ implement it as a pure lang-support alternative.


Related to https://github.com/cloudstateio/cloudstate/issues/51