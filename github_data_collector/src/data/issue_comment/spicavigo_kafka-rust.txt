my producer conf is :
```
let producer = Producer::from_hosts(settings::BROKER.to_owned()
            .split(',')
            .map(|s| s.trim().to_owned())
            .collect())
            .with_ack_timeout(Duration::from_secs(1))
            .with_required_acks(RequiredAcks::One)
            .with_compression(compression)
            .create()
            .expect("CREATE_KAFKA_PRODUCER_ERROR");
```
sometimes have:
```
tcp               CLOSE-WAIT             1                   0                                                        192.168.49.107:36000           10.22.73.9:9092
tcp               CLOSE-WAIT             1                   0                                                        192.168.49.107:57004          10.22.73.24:9092
tcp               CLOSE-WAIT             1                   0                                                        192.168.49.107:42256          10.22.73.20:9092
tcp               CLOSE-WAIT             1                   0                                                        192.168.49.107:57042          10.22.73.24:9092
tcp               CLOSE-WAIT             1                   0                                                        192.168.49.107:56762          10.22.73.30:9092
tcp               CLOSE-WAIT             1                   0                                                        192.168.49.107:35984           10.22.73.9:9092
tcp               CLOSE-WAIT             1                   0                                                        192.168.49.107:56752          10.22.73.30:9092
tcp               CLOSE-WAIT             1                   0                                                        192.168.49.107:56768          10.22.73.30:9092
tcp               CLOSE-WAIT             1                   0                                                        192.168.49.107:56746          10.22.73.30:9092
tcp               CLOSE-WAIT             1                   0                                                        192.168.49.107:57010          10.22.73.24:9092
tcp               CLOSE-WAIT             1                   0                                                        192.168.49.107:35986           10.22.73.9:9092
tcp               CLOSE-WAIT             1                   0                                                        192.168.49.107:57028          10.22.73.24:9092
tcp               CLOSE-WAIT             1                   0                                                        192.168.49.107:57052          10.22.73.24:9092
tcp               CLOSE-WAIT             1                   0                                                        192.168.49.107:35970           10.22.73.9:9092
tcp               CLOSE-WAIT             1                   0                                                        192.168.49.107:56740          10.22.73.30:9092
tcp               CLOSE-WAIT             1                   0                                                        192.168.49.107:57014          10.22.73.24:9092
tcp               CLOSE-WAIT             1                   0                                                        192.168.49.107:57012          10.22.73.24:9092
tcp               CLOSE-WAIT             1                   0                                                        192.168.49.107:42258          10.22.73.20:9092
tcp               CLOSE-WAIT             1                   0                                                        192.168.49.107:35982           10.22.73.9:9092
tcp               CLOSE-WAIT             1                   0                                                        192.168.49.107:57032          10.22.73.24:9092
tcp               CLOSE-WAIT             1                   0                                                        192.168.49.107:57006          10.22.73.24:9092
tcp               CLOSE-WAIT             1                   0                                                        192.168.49.107:57026          10.22.73.24:9092
tcp               CLOSE-WAIT             1                   0                                                        192.168.49.107:42260          10.22.73.20:9092
tcp               CLOSE-WAIT             1                   0                                                        192.168.49.107:42250          10.22.73.20:9092
tcp               CLOSE-WAIT             1                   0                                                        192.168.49.107:42252          10.22.73.20:9092
```
All TCP Connect is CLOSE-WAIT
The option for the number of partitions is `--partitions`, not
`--partition`.
Currently the following methods of Kafka vanilla producer API are not supported:

1.  the **enable.idempotence** configuration 
2. initTransactions()
3. beginTransaction()
4. commitTransaction()
5. abortTransaction()
6. sendOffsetsToTransaction() 

Implementing this methods will unlock Kafka transactional features like atomic publishing into several topics and exactly-once delivery semantic.

Source: https://kafka.apache.org/22/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html

What is the proper way of maintaining a "reliable" consumer? And by reliable I mean that it should be available during whole program execution. Particularly, if there's an error on some call, it should try to recover, if possible (e.g. in case of loosing connection, it should reconnect to the broker)

Should the consumer::Consumer re-constructed once again (and again, ...) in such a case? Or it might be some other way of handling that?
Hello

The `Error` type doesn't play very nice with the `failure` crate. Would it be possible, once this https://github.com/rust-lang-nursery/error-chain/pull/241 lands and is published, update the version of error chain used?

Thank you
I'm using the Consumer class to read a stream of messages and every so often there's a rogue big one. When this happens, my loop's `consumer.poll()` call returns an `Err` value. Once this happens, the consumer doesn't move past this offset and all subsequent calls to `consumer.poll()` just return the same error. The loop (in greatly elided form looks like:

```
loop {
    match consumer.poll() {
        Ok(message_sets) => {
            for message_set in message_sets.iter() {
                for m in message_set.messages() {
                    // Do stuff with messages
                }
                consumer.consume_messageset(message_set).unwrap();
            }
            consumer.commit_consumed().unwrap();
        },
        Err(e) => {
            // MessageSizeTooBig, etc
        }
    }
}
```

The questions are:

1. What does this error mean when it's received by the consumer? Is it that the broker rejected a message from the producer, or that the consumer tried to fetch one and it (meaning this program) can't accommodate it? Admittedly, this is more of a general Kafka question.

2. Is there a way to move past the error and continue processing more messages? It doesn't look like there's a messageset or message to consume and commit if you end up in the `Err` match arm.

3. My understanding is that what you get when you poll() is a bunch of message sets, potentially multiple sets containing multiple messages. Is there a way to not lose an entire batch of messages when this happens? 
`examples/topic-metadata` prints for example:

```
> ./topic-metadata --brokers host:port --topics topic-name
topic          p-id l-id                        (l-host)     earliest       latest       (size)
topic-name      0    1            (xxx.xxx.xxx.xxx:port)      7261198      7261198          (0)
topic-name      1    2            (xxx.xxx.xxx.xxx:port)      7328851      7328851          (0)
topic-name      2    0            (xxx.xxx.xxx.xxx:port)      7269629      7269629          (0)
```

we'd also like to see a partition's configured replicas and in-sync-replicas; these correspond to the [partition metadata fields `partition` and `isr`](https://github.com/spicavigo/kafka-rust/blob/master/src/protocol/metadata.rs#L54).
I'm still working on the LZ4 compression supports, it not a finalized design, please help to review code and give your comments.

Kafka use an [incomplete implementation](https://cwiki.apache.org/confluence/display/KAFKA/KIP-57+-+Interoperable+LZ4+Framing) of [LZ4 frame format](https://github.com/lz4/lz4/wiki/lz4_Frame_format.md), I doubt whether we need a more complete implementation or just follow Kafka did.

Besides, are there any document about how to generate test-data and write integration test?
`kafka-rust` is using some system/non-rust dependencies, such as miniz(flate2), libsnappy (snappy-sys), and libopenssl (openssl-sys) etc, which cause the deployment was limited by those native libraries.
```
kafka v0.5.0
├── byteorder v0.5.3
├── crc v1.4.0
│   └── lazy_static v0.2.6
├── flate2 v0.2.17
│   ├── libc v0.2.21
│   └── miniz-sys v0.1.9
│       └── libc v0.2.21 (*)
├── fnv v1.0.5
├── log v0.3.7
├── openssl v0.8.3
│   ├── bitflags v0.7.0
│   ├── lazy_static v0.2.6 (*)
│   ├── libc v0.2.21 (*)
│   └── openssl-sys v0.7.17
│       └── libc v0.2.21 (*)
├── ref_slice v1.1.1
└── snappy v0.4.0
    ├── libc v0.2.21 (*)
    └── snappy-sys v0.1.0
        └── libc v0.2.21 (*)
```

I'm working on the PR #148 to replace `snappy` with `snap` in pure rust, I think we may replace the other  system/non-rust dependencies in future. 

For example, replace `openssl` with `native-tls`, which may lack some features compare with openssl, but I believe it will be ready sooner or later, because [a lot of rust crates](https://crates.io/crates/native-tls/reverse_dependencies) switch to use it
This is medium-difficulty for any newcomers, as it requires a somewhat intimate familiarity with the library.

Currently, much of the code that requires doing actual networking calls to a Kafka server are not tested. We can take advantage of Docker and Travis to enable integration testing the networking methods. I did some research and playing around today, and I came up with a setup that seems to work.

Basically, we set up a `docker-compose.yml` that uses a public Kafka container from the Docker registry, and give the Kafka version as a parameter. Then we set up our `.travis.yml` to run the tests for each Kafka version we want to test against. This will allow us not only to ensure sure the code works as intended, but also verify compatibility across as many versions of Kafka as we'd like. I realize that this crate is still pre-1.0, so retroactively adding tests for *everything* might be overkill, since the API is still subject to change, but it should help with getting at least the most important methods tested for correctness.

For testing locally, you would just need to have Docker and Compose installed and then, before running the cargo integration tests:

1. Set the `KAFKA_VER` environment variable in your shell
2. Bring up the desired Kafka container with `docker-compose up -d`

[This commit](https://github.com/dead10ck/kafka-rust/commit/25633eb0b5ab4a78732e20c0c726562f3a763d8b) in a branch of my fork showcases the code changes necessary. It uses the latest versions of 0.8, 0.9, and 0.10, and includes a simple integration test for loading the `KafkaClient` metadata. This is what [a Travis build](https://travis-ci.org/dead10ck/kafka-rust/builds/178968497) would look like.

What do you think?

# Update
The above has been implemented and merged in #143! Let's keep this issue around to track progress of integration test backfilling.