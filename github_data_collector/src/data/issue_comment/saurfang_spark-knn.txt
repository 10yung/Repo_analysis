It would be nice to get a release of this so that it can be used with Scala 2.12 and Spark 2.4.X.
val knn = new KNNRegression().setTopTreeSize(df.count().toInt / 500)
                                     .setFeaturesCol("features")
                                     .setPredictionCol("predicted")
                                     .setK(1)

This throws the following exception,

Caused by: java.lang.ClassNotFoundException: org.apache.spark.ml.param.shared.HasInputCols$class
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)

As part our CICD pipeline, we have a daily build that runs on relatively small amounts of data. As part of this, we discovered an interesting bug; as part of the method estimateTau, there is the following line:

```scala
val y = DenseVector(estimators.map { case (_, d) => math.log(d) })
```

In this case, d is the average distance between points. We are finding that on the small data used in our daily build, beta can exceed 0. When this happens, yMax, which is defined as:

```scala
val yMax = breeze.linalg.max(y)
```
is below negative one, and subsequently used as the bufferSize. 

Specifically, the following appears in the log:

```bash
ERROR KNN: Unable to estimate Tau with positive beta: 0.1577160047542901. This maybe because data is too small.
Setting to -1.3153582722102333 which is the maximum average distance we found in the sample.
This may leads to poor accuracy. Consider manually set bufferSize instead.
You can also try setting balanceThreshold to zero so only metric trees are built.
```

(this does not cause the code to stop, and it continues)

```bash
Exception in thread "main" java.lang.IllegalArgumentException: knn_2166a4d536d3 parameter bufferSize given invalid value -1.3153582722102333
```

This then causes an error and the pipeline stops.

From my understanding, very low average distances would always cause errors if beta exceeds 0.
Is there a way to extract the K nearest neighbors from Training samples from the KNN model in the Scala version?
while fitting training data, on what parameter does top tree size, leave size and sub tree leave size depends?
followed whatever was there
val training = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt").toDF()
val knn = new KNNClassifier()
  .setTopTreeSize(training.count().toInt / 500)
  .setK(10)
 1st error : TopTreeSize is invalid 0 (since total count of training sample is 100)
let say we set manually TreeSize as 1
then it throws an exception while running knn.fit(training)

java.util.NoSuchElementException: Failed to find a default value for inputCols
  at org.apache.spark.ml.param.Params$$anonfun$getOrDefault$2.apply(params.scala:652)
  at org.apache.spark.ml.param.Params$$anonfun$getOrDefault$2.apply(params.scala:652)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.ml.param.Params$class.getOrDefault(params.scala:651)
  at org.apache.spark.ml.PipelineStage.getOrDefault(Pipeline.scala:42)
  at org.apache.spark.ml.param.Params$class.$(params.scala:658)
  at org.apache.spark.ml.PipelineStage.$(Pipeline.scala:42)
  at org.apache.spark.ml.knn.KNN.fit(KNN.scala:383)


  
It appears that `spark-knn` needs to transform dense vectors into their sparse form. This creates a limitation when using `spark-knn` for very wide, sparse datasets such as document-term matrices used in NLP.  

Is there any interest in supporting sparse vectors within `spark-knn`?
Would you please advise how to get K nearest neighbors for each data point using the python interface? 
I believe this is possible based on " KNN itself is also exposed for advanced usage which returns arbitrary columns associated with found neighbors. "

I ran KNNClassifier on my local machine with 5000 rows data, and I got stackoverflow errors. The version of this KNN is v0.1.1. How to avoid this stackoverflow?
Just faced the issue and the reason was that the number of points (defaults to `1000`) was higher than the number of records in the training dataset. Perhaps obvious for ML practitioners, but I spent few minutes debugging to nail it down.

It'd be nice to know it before fitting a model or get a more user-friendly error message.

```
Exception in thread "main" java.lang.IllegalArgumentException: requirement failed: Sampling fraction (333.3333333333333) must be on interval [0, 1]
	at scala.Predef$.require(Predef.scala:224)
	at org.apache.spark.util.random.BernoulliSampler.<init>(RandomSampler.scala:148)
	at org.apache.spark.rdd.RDD$$anonfun$sample$2.apply(RDD.scala:495)
	at org.apache.spark.rdd.RDD$$anonfun$sample$2.apply(RDD.scala:490)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.sample(RDD.scala:490)
	at org.apache.spark.ml.knn.KNN.fit(KNN.scala:387)
```