Python 2's maintenance ends in 2020, and that happens to be now (there should be [one last release of Python 2 in april](https://adtmag.com/articles/2020/01/09/python-2-end-of-life.aspx), and then nothing).

We may start thinking about terminating support for Python 2 in MechanicalSoup.

I'd like to (but I can live without it) have MechanicalSoup 1.0 support Python 2, so that an easy way for people who really need it with MechanicalSoup corresponds to a version with a round number. Anyway, we can start writing a PR to drop support and remove any code needed only for Python 2. A quick grep for `Python 2` and `__future__` shows that there should be only very marginal change (i.e. only lines to remove, and `.travis-ci.yml` to adapt).
[Poetry](https://python-poetry.org/) and [Pipenv](https://pipenv.kennethreitz.org/en/latest/) both allow in particular pinning dependency versions, hence should help us to debug subtle issues related to versions like #299.

Poetry is the most recent one, uses the new standard `pyproject.toml` and covers stuff that pipenv doesn't like publishing to PyPI, so it's probably the best option, but I never used either of them so I may be wrong.

Step 1 is to try it and see how cool/uncool it is.
I use the proxy this way, it seem didn't work.

```
import mechanicalsoup

proxies = {
    "http": "socks5://127.0.0.1:49260",
    "https": "socks5://127.0.0.1:49260"
}
url = 'https://whatismyip.com/'
browser = mechanicalsoup.StatefulBrowser()

browser.session.proxies = proxies   # This seem can't work

page = browser.open(url)  #, verify=False
print(page)
print(page.text)
```

Where's a working demo about this? Thank you!
Current implementation parses response.text both when content-type is not text/html or empty
This leads parsing data when content-type is binary data such as application/x-gzip.
Parse only when content-type not passed.
It might be helpful to get documents translated version.
cause, many Asians look forward to your 'MechanicalSoup'!

Do you have or Do you need documents translations?

Thanks for reading :)
I'm getting an error for the following test in a local build:

tests/test_browser.py::test_enctype_and_file_submit[multipart/form-data-False-<input name="pic" type="file" />] FAILED

```
>               assert found_in == "files"
E               AssertionError: assert 'form' == 'files'
E                 - form
E                 + files
```

This build uses the following dependency versions:

requests (2.21.0)
beautifulsoup4 (4.7.1)

Note that TravisCI is using `requests-2.22.0` and `beautifulsoup4-4.8.0`.
I want to make get request and receive chunked response, then parse each chunk body to get information from it. But at the end of get request invoked following chain: add_soup(response) -> Browser.__looks_like_html(response) -> response.text, that forces to load all chunks into memory. Is add_soup method required when request has "stream=True"?

This is a high-severity vulnerability affecting urllib3 versions prior to 1.25. See https://github.com/urllib3/urllib3/issues/1553 for more info.

We depend on urllib3 through requests, which has an open issue for supporting urllib3 1.25 (see https://github.com/kennethreitz/requests/issues/5065). Once this issue has been resolved and a new release is made, I think we should update our minimum requirement for requests.

Should we do any backporting to our current release, or just look to do a new release soon?
The function `_request(self, form, url=None, **kwargs)` from the Browser class retrieves data and files from kwargs argument :

`data = kwargs.pop("data", dict())`
`files = kwargs.pop("files", dict())`

However, this way of submitting data or files is not tested nor documented.