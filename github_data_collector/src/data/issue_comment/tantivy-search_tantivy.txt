Updates the requirements on [time](https://github.com/time-rs/time) to permit the latest version.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href="https://github.com/time-rs/time/releases">time's releases</a>.</em></p>
<blockquote>
<h2>v0.2.2</h2>
<h2>Bugs fixed</h2>
<p><a href="https://github-redirect.dependabot.com/time-rs/time/issues/200">#200</a> has been fixed.</p>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li>See full diff in <a href="https://github.com/time-rs/time/commits/v0.2.2">compare view</a></li>
</ul>
</details>
<br />


Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a "Dependabot enabled" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Automerge options (never/patch/minor, and dev/runtime dependencies)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>
This PR implements what was briefly discussed on Gitter, a `ReadOnlySource` implementation that uses a `Read + Seek` based API.

Please do note that this is for now a initial implementation, a proof of concept. Plenty of places do questionable things or are a bit sloppy and need to be improved but, it can serve for us as a starting point for discussion. That being said it does work correctly with the current `Directory` implementations and passes all tests.

The motivation for a `Read + Seek` based implementation is [this](https://github.com/matrix-org/seshat/blob/master/src/encrypted_dir.rs#L564) `Directory` implementation that transparently encrypts/decrypts files that Tantivy reads/writes. This could be quite important for client applications that provide full text search over sensitive data.

I am sure that there are more use-cases for such an `Read + Seek` based API, and making the `Directory` more flexible is a welcome improvement.

A downside of this API is that allocating memory buffers and copying data is necessary instead of the direct access that we get with the slice based approach.
This seems like a reasonable start to this.  Would close #706 , but I'd imagine we want to do some more work here. 
This ticket is about fixing the heuristic of the `LogMergePolicy`.
There is actually two different problems in this ticket.

# Problem 1

The relative benefit of merging segments diminishes as the segment gets bigger and bigger.
We should add a configurable `max_segment_size` expressed in number of documents to limit the size of two segments that can be considered for merge.

# Problem 2

The current merge policy only takes in account the size (in number of documents) to merge two segments. We do not take in account the number of deleted documents.

This can lead to the following problem. A very large segment is created, and the index consists in 
this large segment and a lot of small segments.

Until the small segments get merged up to the point when `min_merge_size` large segment are created, the large segment never gets merged.
This can take a very long time (e.g. months). In the meanwhile, deletes also happen and this mammoth segment may actually contain a very large ratio of deletes.

The heuristic should be changed to take in account the ratio of deleted file to avoid this problem. 
Tantivy needs a thread pool to handle documents which  added by user's add_document thread for each index writer ( not user's add_document thread direactly like lucene) .

I think it will needs a lot of threads when indexing multiple indices, which will cause high load averge.
I have been asked about the scoring algorithm that tantivy uses and realised that neither I, nor the documentation have a canonical description for it apart from:

> The larger the number, the more relevant the document to the search

https://docs.rs/tantivy/0.10.3/tantivy/type.Score.html

I think it will be great to add more information and run through an example query on an index to show why queries return results in that order and how a user might debug specific queries. 

## Who do we expect to read this? 
People building a full-text search engine are interested in efficiently storing and ranking documents against queries. The score of each document is arguably THE most important data type that we return to users in every query. I expect most users of tantivy will want to read about the Score type at one point or another. 

### 2 types of users:
1) knowledgeable about building search engines and wants to confirm the validity of tantivy's scoring algorithm - expect to see tf/idf, BM25 and other known 
2) someone for whom tantivy might be the first experience building a search application with little background on document scoring - want answers to specific questions and some further reading material.

### Questions these users want to answer:

- [ ] Why are search results in this order? What is this score field? Why is it a float? 
- [ ] How does each subquery in the full query (eg. q: "title:president AND (body:Obama OR body:barack) AND year:<2008") contribute to the final score of a document
- [ ] I want to boost/expected a specific document higher up in the set of results for a given query - how do I do that? 


## Suggested style of documentation
Prose: A detailed high-level explanation for document scoring - how is each query scored, how are scores of different sub-queries combined. 
Code: doc-test (doesn't need to assert/test anything) that walks through an example of debugging a unexpectedly low-ranking document, using `Query::explain` and showing how the example query can be re-written. 

## Provide further reading material
Give links to tf-idf, BM25 wikipedia pages and the [Query::explain](https://docs.rs/tantivy/0.10.3/tantivy/query/trait.Query.html#method.explain) method


## If you do this ticket, you will learn:
- The full life-cycle of a tantivy query from query to score per document
- tantivy helper methods for debugging such queries
- writing concise, yet informative documentation for power-users and amateurs at the same time
If an application has more than one server, having one thread pool per index is not a good idea.
We should probably externalize the executor so that it can used with several `IndexReader`.
Tantivy is great. We're developing a ES-like(much simpler and specific to our needs) system based on tantivy. Currently, we're doing some optimization and we need some info about the format of various tantivy files like .idx and .term. Is there any doc available or should we read some source code? Thanks!
Two users requested to easily access the Top K int values of an index. A typical usage could be to display the last N documents of a mailbox for instance.

This ticket requires to tackle https://github.com/tantivy-search/fst/issues/3 first. 
I want to get the largest number from a indexed u64 filed, but from the API seems I need first get the filed count then use RangeQuery to search, Or is there other more efficient way?
which more like equivalent to `order by <filed> desc limit 1` in sql.

```
//#1
let count = ...
let query = RangeQuery::new_u64(field, (count-1..count));
searcher.search(&query, &TopDocs::with_limit(1));
//#2
searcher.search(&AllQuery, &TopDocs::with_limit(1)); //this will scan all the docs?

//let query = RangeQuery::new_u64(field, -1); is there any implementation like this. 
```