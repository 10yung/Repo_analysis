I have been trying to install `leela-zero` on my arch for the whole afternoon. The result is frustrating. I kept getting the complaint

> C++ exception with description "No suitable OpenCL device found"

At first I thought it's because I'm missing some dependencies, but it does not seem to be like. I'm using an old laptop manufactured in 2007.. perhaps it lacks some device? GPU too old?

### Detailed complaints

`$ yay leela-zero-git` gives
```
#...
[100%] Linking CXX executable tests
[100%] Built target tests
Running main() from gtest_main.cc
[==========] Running 13 tests from 2 test cases
[----------] Global test environment set-up.
BLAS Core: built-in Eigen 3.3.7 library.
Detecting residual layers...v1...8 channels1 blocks.
Initializing OpenCL (autodetecting precision).
Detected 1 OpenCL platforms.
Platform version: OpenCL 1.1 Mesa 19.3.2
Platform profile: FULL_PROFILE
Platform name:     Clover
Platform vendor:   Mesa
Error getting device(s): clGetDeviceIDs: -1
unknown file: Failure
C++ exception with description "No suitable OpenCL device found." thrown in auxiliary test code (environments or even listeners).
==> ERROR: A failure occurred in check().
     Aborting...
Error making: leela-zero-git
```


`$ clinfo` gives
```
Number of platforms                               1
  Platform Name                                   Clover
  Platform Vendor                                 Mesa
  Platform Version                                OpenCL 1.1 Mesa 19.3.2
  Platform Profile                                FULL_PROFILE
  Platform Extensions                             cl_khr_icd
  Platform Extensions function suffix             MESA

  Platform Name                                   Clover
Number of devices                                 0

NULL platform behavior
  clGetPlatformInfo(NULL, CL_PLATFORM_NAME, ...)  Clover
  clGetDeviceIDs(NULL, CL_DEVICE_TYPE_ALL, ...)   
  clCreateContext(NULL, ...) [default]            No devices found in platform
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_DEFAULT)  No devices found in platform
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CPU)  No devices found in platform
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_GPU)  No devices found in platform
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ACCELERATOR)  No devices found in platform
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CUSTOM)  No devices found in platform
  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ALL)  No devices found in platform

ICD loader properties
  ICD loader Name                                 OpenCL ICD Loader
  ICD loader Vendor                               OCL Icd free software
  ICD loader Version                              2.2.12
  ICD loader Profile                              OpenCL 2.2
```
Hello all

Firstly, I would like to thank developers for great work and open sourcing the project. It is an invaluable educational tool!

Basically I was browsing Leela Zero code to learn how it all works and wanted to clarify that understanding of parallelism model is correct:

__leela-zero-server__ single server manages whole crowd sourcing effort
* accumulates games from clients
* performs neural network training

__autogtp__ is a main client, single executable run locally
* spins up `m_gpus*m_games` instances of `leelaz` executable
* communicates with each `leelaz` using GTP protocol via OS pipes
* communication happens _every move_

__leelaz__ executable manages single game
* receives _per move_ commands from autogtp (or other client, e.g. GUI)
* runs by default `gpu_count=1` GPU threads (each thread grabs 1x GPU, `OpenCLScheduler.cpp:122`)
* for tree search: runs multiple MCTS threads on a single tree

Would you say above description accurate?

Thanks in advance!
https://lifein19x19.com/viewtopic.php?f=18&t=17187
I managed to install Leela Zero on RPi4 - 4Gb with Raspbian Buster, and it run pretty well.  With the following setting for leelaz: `--gtp -w 40b.gz --noponder -p 3200 -timemanage off` it took aprox 40 sec. for a move.
So:
1. Install NodeJS
`
curl -sL https://deb.nodesource.com/setup_13.x | sudo bash -
sudo apt install nodejs
`
2. Install LeelaZero
`
cd ~
sudo apt install clinfo && clinfo
git clone https://github.com/leela-zero/leela-zero
cd leela-zero
git submodule update --init --recursive
sudo apt install cmake g++ libboost-dev libboost-program-options-dev libboost-filesystem-dev opencl-headers ocl-icd-libopencl1 ocl-icd-opencl-dev zlib1g-dev
mkdir build && cd build
cmake -DUSE_CPU_ONLY=1 ..
cmake --build .
`
3. Install Sabaki
`
cd ~
git clone https://github.com/SabakiHQ/Sabaki
cd Sabaki
npm install
npm run build
npm start
`
4. Install LizGoban
`
cd ~
git clone https://github.com/kaorahi/lizgoban
cd lizgoban
npm install
npm start
`
5. Copy to lizgoban/external directory, leelaz executable and network.gz weights file...

I tried before as gui quarry and kigo but I had issues with them (exits unexpectedly)
Thanks a lot for GCP leading a huge project of training Leela-zero and the result is really amazing! @gcp

Since there are new weight still comes out and many fans hope to know if there is any method to keep Leela-zero growing.

Here is one of my suggestion:

1. Rent a server for Leela-zero, and I think the fees could be collected by donation or some other ways.  First, we need GCP's approvement. It is important.
2. Find volunteers to keep the training(produce new network, adjust the learning rate, blocks or visits etc. )
3. If there is any new method that is useful for upgrade training(for example we can learn some new techniques from SAI or Katago etc), the folunteers could try it in the training of Leela-zero. 

Just hope that Leela-zero could be live longer and become stronger!

Any suggestion is appreciated~







I am trying to connect with AutoGTP v18 for the first time and I get this response

AutoGTP v18
Using 1 game thread(s) per device.
Starting tuning process, please wait...
Network connection to server failed.
NetworkException: Curl returned non-zero exit code 23
Retrying in 30 s.

Can anyone help?

Thanks
968231 total match games (0 in past 24 hours, 0 in past hour).

Start Date | Network Hashes | Wins / Losses | Games | SPRT
-- | -- | -- | -- | --
2019-12-28 09:20 | a5e02580 VS 877d874d | 114 : 131 (46.53%) | 245 / 400 | fail


If I want to expand the network, using net2 will make MSE and other data abnormal. The situation is very bad
Hi,
I tried to run the tests after compilation, but leela-zero died with a floating point exception. If I try to start the binary directly I get the same problem.

```
$ ./tests
Running main() from gtest_main.cc
[==========] Running 13 tests from 2 test cases.
[----------] Global test environment set-up.
BLAS Core: built-in Eigen 3.3.7 library.
Detecting residual layers...v1...8 channels...1 blocks.
Initializing OpenCL (autodetecting precision).
Detected 1 OpenCL platforms.
Platform version: OpenCL 1.1 Mesa 19.3.1
Platform profile: FULL_PROFILE
Platform name:    Clover
Platform vendor:  Mesa
Device ID:     0
Device name:   AMD VERDE (DRM 2.50.0, 5.4.3-arch1-1, LLVM 9.0.0)
Device type:   GPU
Device vendor: AMD
Device driver: 19.3.1
Device speed:  775 MHz
Device cores:  10 CU
Device score:  1111
Selected platform: Clover
Selected device: AMD VERDE (DRM 2.50.0, 5.4.3-arch1-1, LLVM 9.0.0)
with OpenCL 1.1 capability.
Half precision compute support: Yes.
Tensor Core support: No.
OpenCL: using fp16/half or tensor core compute support.

Started OpenCL SGEMM tuner.
Will try 290 valid configurations.
[1]    131702 floating point exception (core dumped)  ./tests
```
As hardware details are already listed above I don't know what to add. If you need additional
information pleas let me know.

Edit: Added stack trace


```
#0  0x00007fea816969f3 in ?? () from /usr/lib/gallium-pipe/pipe_radeonsi.so
#1  0x00007fea81696be0 in ?? () from /usr/lib/gallium-pipe/pipe_radeonsi.so
#2  0x00007fea81697790 in ?? () from /usr/lib/gallium-pipe/pipe_radeonsi.so
#3  0x00007fea816979d7 in ?? () from /usr/lib/gallium-pipe/pipe_radeonsi.so
#4  0x00007fea816564a2 in ?? () from /usr/lib/gallium-pipe/pipe_radeonsi.so
#5  0x00007fea816565f1 in ?? () from /usr/lib/gallium-pipe/pipe_radeonsi.so
#6  0x00007fea81656d54 in ?? () from /usr/lib/gallium-pipe/pipe_radeonsi.so
#7  0x00007fea81659e9c in ?? () from /usr/lib/gallium-pipe/pipe_radeonsi.so
#8  0x00007fea8a9d54b8 in ?? () from /usr/lib/libMesaOpenCL.so.1
#9  0x00007fea8a9d1080 in ?? () from /usr/lib/libMesaOpenCL.so.1
#10 0x00007fea8a9d1bc3 in ?? () from /usr/lib/libMesaOpenCL.so.1
#11 0x00007fea8a9d2341 in ?? () from /usr/lib/libMesaOpenCL.so.1
#12 0x00007fea8a9c0959 in ?? () from /usr/lib/libMesaOpenCL.so.1
#13 0x00007fea8b94f66e in clEnqueueNDRangeKernel () from /usr/lib/libOpenCL.so.1
#14 0x00005585bf6411f3 in cl::CommandQueue::enqueueNDRangeKernel (offset=..., events=0x0, event=0x7ffc9355c3b0, local=..., global=..., kernel=<synthetic pointer>..., this=<synthetic pointer>) at /home/nerf/git/leela/src/CL/cl2.hpp:7972
#15 Tuner<half_float::half>::tune_sgemm[abi:cxx11](int, int, int, int, int) (this=0x7ffc9355c990, m=8, n=25, k=8, batch_size=36, runs=<optimized out>) at /home/nerf/git/leela/src/Tuner.cpp:533
#16 0x00005585bf64223d in Tuner<half_float::half>::load_sgemm_tuners[abi:cxx11](int, int, int, int) (this=0x7ffc9355c990, m=8, n=25, k=8, batch_size=36) at /usr/include/c++/9.2.0/ext/new_allocator.h:89
#17 0x00005585bf657386 in OpenCL<half_float::half>::initialize (this=0x5585c0f0f4c0, channels=8, batch_size=1) at /home/nerf/git/leela/src/Tuner.cpp:731
#18 0x00005585bf6579c5 in OpenCLScheduler<half_float::half>::initialize (this=0x5585c0f0f3e0, channels=8) at /usr/include/c++/9.2.0/bits/unique_ptr.h:352
#19 0x00005585bf66aa7b in Network::init_net (this=0x7fea8aa84010, channels=8, pipe=...) at /usr/include/c++/9.2.0/bits/unique_ptr.h:352
#20 0x00005585bf672d7a in Network::select_precision (this=0x7fea8aa84010, channels=8) at /usr/include/c++/9.2.0/bits/move.h:74
#21 0x00005585bf673628 in Network::initialize (this=0x7fea8aa84010, playouts=<optimized out>, weightsfile=...) at /home/nerf/git/leela/src/Network.cpp:573
#22 0x00005585bf6a3194 in LeelaEnv::SetUp (this=<optimized out>) at /home/nerf/git/leela/src/tests/gtests.cpp:87
#23 0x00005585bf6b9c6c in testing::internal::SetUpEnvironment(testing::Environment*) ()
#24 0x00005585bf6d6edd in void (*std::for_each<__gnu_cxx::__normal_iterator<testing::Environment* const*, std::vector<testing::Environment*, std::allocator<testing::Environment*> > >, void (*)(testing::Environment*)>(__gnu_cxx::__normal_iterator<testing::Environment* const*, std::vector<testing::Environment*, std::allocator<testing::Environment*> > >, __gnu_cxx::__normal_iterator<testing::Environment* const*, std::vector<testing::Environment*, std::allocator<testing::Environment*> > >, void (*)(testing::Environment*)))(testing::Environment*) ()
#25 0x00005585bf6d0601 in void testing::internal::ForEach<std::vector<testing::Environment*, std::allocator<testing::Environment*> >, void (*)(testing::Environment*)>(std::vector<testing::Environment*, std::allocator<testing::Environment*> > const&, void (*)(testing::Environment*)) ()
#26 0x00005585bf6b9ed7 in testing::internal::UnitTestImpl::RunAllTests() ()
#27 0x00005585bf6d6594 in bool testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) ()
#28 0x00005585bf6cfeb5 in bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) ()
#29 0x00005585bf6b89fa in testing::UnitTest::Run() ()
#30 0x00005585bf6a6f1b in RUN_ALL_TESTS() ()
#31 0x00005585bf6a6ea9 in main ()

```
A big difference between the training methods of MuZero and AlphaZero is:  Muzero uses K consequential  steps samples, and AlphaZero uses only one step samples. Muzero's paper has shown the advantage of this training approach. 

My suggestion is this:

Modify chunkparser.py, let parse() generate K batches data once, the samples of this K batches form one batch of consequential K positions of different games 

Modify tfprocess.py, using gradient accumulation method, updating network parameters with the averaged gradients of K batches.

In MuZero, K=5

I do not have the resource  for training, maybe someone here can try. The coding can be done within an hour.