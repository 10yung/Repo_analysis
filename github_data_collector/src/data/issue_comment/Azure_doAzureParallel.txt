Fix timeout error message in waitForTasksToComplete()
For a job with 1000 tasks, from a time point, all the remaining tasks will fail with the following errors 
> Error in save(list = names(.GlobalEnv), file = outfile, version = version,  : 
  error writing to connection
> Calls: quit -> sys.save.image -> save.image -> save
> Execution halted

This error happens randomly to me, so I cannot give a minimal reproducible example. Sometimes, If I submit the exactly same job again, all the tasks can be successfully completed. 
So I believe it is possibly due to problems of Azure.

I am not familiar with Azure, therefore I am not sure what info  I should provide here. Please let me know if any additional info is helpful.

Thanks.


Proposed fix for #363
When adding a task, the creation of the upload token for the standard job files like the file `1.txt` containing the log of task 1 ignores if the user maybe has set a longer expiry time for their output files.

In my case I have long running tasks and changed the following line from the documentation example
https://github.com/Azure/doAzureParallel/blob/2b8f388dc476a3a51aa71c3ab1165d080ad08fe8/docs/72-persistent-storage.md#L63
to
```R
# make token expire 20 days after creation
writeToken <- storageClient$generateSasToken("w", "c", outputFolder, end = Sys.time() + 60 * 60 * 24 * 20) 
```

So the SAS token for my output files now has the `se` parameter in the URI like this:
`[...]&se=2019-08-18T11%3A11%3A48Z&[...]`
while in the URI for 1.txt it looks like this:
`[...]&se=2019-07-31T11%3A11%3A59Z&[...]`
so my own upload gets ignored because the task crashes with a FileUploadAccessDenied Error for the 1.txt before my upload is run.

I am not sure if this is definied here
https://github.com/Azure/doAzureParallel/blob/96bfc226628a5f0e58c52d2f0e4f76b76e3bf2e9/R/batch-api.R#L79
or here
https://github.com/Azure/doAzureParallel/blob/96bfc226628a5f0e58c52d2f0e4f76b76e3bf2e9/R/doAzureParallel.R#L560-L566

If you could point me in the right direction I could try a fix.
Is there a reason this line references a release from September 2018?

https://github.com/Azure/doAzureParallel/blob/9e9b4942f40e957140ca415a917963193b5e4dc9/R/cluster.R#L207

On the same topic, the run command seems outdated as well:
https://github.com/Azure/doAzureParallel/blob/9e9b4942f40e957140ca415a917963193b5e4dc9/R/batch-api.R#L237-L242

For comparison, see https://github.com/Azure/batch-insights/blob/master/README.md#linux

Hello,

I am able to run the sample "getting started" code but when I attempt to load a package from CRAN, my nodes fail to start. Can anyone show me what I might be doing wrong?

Here is my cluster file. This is the same file from "getting started" but I've included the "hypervolume" package.

````
{
  "name": "hv",
  "vmSize": "Standard_D2_v2",
  "maxTasksPerNode": 2,
  "poolSize": {
    "dedicatedNodes": {
      "min": 0,
      "max": 0
    },
    "lowPriorityNodes": {
      "min": 5,
      "max": 10
    },
    "autoscaleFormula": "QUEUE"
  },
  "containerImage": "rocker/tidyverse:latest",
  "rPackages": {
    "cran": ["hypervolume"],
    "github": [],
    "bioconductor": []
  },
  "commandLine": [],
  "subnetId": ""
}
````

I run the following:

````
library(doAzureParallel)
setCredentials('credentials.json')
cluster <- makeCluster("cluster.json")
````

But all my nodes fail to boot. Here is an example:

````
1: In .showNodesFailure(nodesWithFailures) :
  The following 1 nodes failed while running the start task:
tvm-2487789449_4-20190722t172941z-p
````
This package has some other dependencies, including `rgeos` which can be finicky to install. I've tried specifying just `rgeos` by itself but that also fails. Is there a better way to install packages?

Thank you!
I am experiencing the known issue with autoscale and github package installation, where the error message is:
```
Error: HTTP error 403.
  API rate limit exceeded for 52.*******. (But here's the good news: Authenticated requests get a higher rate limit. Check out the documentation for more details.)

  Rate limit remaining: 0/60
  Rate limit reset at: 2019-06-24 22:49:36 UTC

  To increase your GitHub API rate limit
  - Use `usethis::browse_github_pat()` to create a Personal Access Token.
  - Use `usethis::edit_r_environ()` and add the token as `GITHUB_PAT`.
Execution halted
```

However, I have set the githubAuthenticationToken in the credentials.json file. Is the environmental variable not yet set when the github install occurs with the packages are specified in the cluster.json file? 

Possibly relevant: I am using a custom docker image (but I want to install the packages from git as I am iterating on package implementation). 

I am not sure how to make a reproducible example, but it occurs when scaling up from 1 to ~400 nodes. Here is my cluster.json in case it helps to reproduce:

```
  "name": "psmirnov",
  "vmSize": "Standard_D2_v3",
  "maxTasksPerNode": 4,
  "poolSize": {
    "dedicatedNodes": {
      "min": 1,
      "max": 1
    },
    "lowPriorityNodes": {
      "min": 0,
      "max": 5000
    },
    "autoscaleFormula": "QUEUE"
  },
  "containerImage": "bhklab/pharmacogx:v3",
  "rPackages": {
    "cran": ["MASS", "tictoc", "mvtnorm", "abind", "polynom", "memoise", "purrr", "matrixStats"],
    "github": ["bhklab/mCI", "bhklab/fastCI"],
    "bioconductor": []
  },
  "commandLine": [],
  "subnetId": ""
}
```
The `sharedKeys` object is not necessary to select the `storageAccount` object. Also, the `storageAccountName` is already an object in this script, but was referred as a string.
Before submitting a bug please check the following:
- [ ] Start a new R session
- [ ] Check your credentials file
- [ ] Install the latest doAzureParallel package
- [ ] Submit a minimal, reproducible example 
- [ ] run `sessionInfo()`

**Description**
Hello, This is my first time writing an issue at github as a first time user of Azure batch cluster. As a beginner, I started with trying the given example in this tutorial.

https://docs.microsoft.com/en-us/azure/batch/tutorial-r-doazureparallel

I have just copy pasted, exactly as it is. It seems everything running fine.  All the 5 nodes are up and running. The Progress: section shows 100%. But, When it comes to Merging then I see following error. 

| Progress: 100.00% (10/10) | Running: 0 | Queued: 0 | Completed: 10 | Failed: 0 |
Tasks have completed. Merging results...An error has occurred in the merge task of the job 'job20190510161411'. Error handling is set to 'stop' and has proceeded to terminate the job. The user will have to handle deleting the job. If this is not the correct behavior, change the errorhandling property to 'pass'  or 'remove' in the foreach object. Use the 'getJobFile' function to obtain the logs. For more information about getting job logs, follow this link: https://github.com/Azure/doAzureParallel/blob/master/docs/90-troubleshooting.md#viewing-files-directly-from-compute-nodeError in e$fun(obj, substitute(ex), parent.frame(), e$data) : 
  object 'results' not found

After going through some other issues in this page, I stopped merging by using  enableCloudCombine = FALSE option. Then I get following error.

Error in e$fun(obj, substitute(ex), parent.frame(), e$data) : 
  object 'results' not found

Any help is highly appreciated.

Thanks 
Moeen

**Description**
If you do not set up an Azure storage container for every foreach loop you get the cryptic error message `Error in base64(txt, FALSE, mode) : decoding from base64 failed`

It happened to me, but after google it I only found this previous closed issue https://github.com/Azure/doAzureParallel/issues/297

It is a stupid mistake, but it should have a clear error message. An improvement could be that the storage account information should be checked when the clusters it is spin up.