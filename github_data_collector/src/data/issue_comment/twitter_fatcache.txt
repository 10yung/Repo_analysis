We're evaluating the potential benefits of deploying Fatcache and I've run into a problem I haven't been able to resolve.  The code compiles cleanly on the platform we're using, Ubuntu 16/Trusty, and the server process start just fine.  But if I run performance tests with "memtier_benchmark" the server process crashes with a segmentation fault for any non-trivial test configuration.

I've tried running minimal configurations that use all defaults, I've compiled the v0.1.1 tag and the lastest code from the GitHub repo.  And I've compiled with full debugging enabled and optimization (O0 flag) disabled.  The process crashes each time no matter which configuration I use.

I'm hoping it's a known issue or something dumb I'm doing to setup/configure Fatcache.  But since I haven't found anything by searching public documents related to Fatcache I wanted to post here to look for clues.

Here's the stack trace from "gdb" in case it helps.

Program received signal SIGSEGV, Segmentation fault.
__memcpy_sse2_unaligned () at ../sysdeps/x86_64/multiarch/memcpy-sse2-unaligned.S:36
36	../sysdeps/x86_64/multiarch/memcpy-sse2-unaligned.S: No such file or directory.
(gdb) where
#0  __memcpy_sse2_unaligned () at ../sysdeps/x86_64/multiarch/memcpy-sse2-unaligned.S:36
#1  0x000000000040ccfc in mbuf_copy (mbuf=0x5889f60, pos=0x7ffff6ff0cf0 "", size=8144) at fc_mbuf.c:233
#2  0x000000000040ce1c in mbuf_copy_from (mhdr=0x737b68, pos=0x7ffff6ff0cf0 "", size=2347828697) at fc_mbuf.c:267
#3  0x000000000040c46f in rsp_send_value (ctx=0x7fffffffea00, conn=0x634ab0, msg=0x735a30, it=0x7ffff1ef104c, cas=423) at fc_response.c:338
#4  0x000000000040aaae in req_process_get (ctx=0x7fffffffea00, conn=0x634ab0, msg=0x735a30) at fc_request.c:212
#5  0x000000000040b8ae in req_process (ctx=0x7fffffffea00, conn=0x634ab0, msg=0x735a30) at fc_request.c:548
#6  0x000000000040b9f6 in req_recv_done (ctx=0x7fffffffea00, conn=0x634ab0, msg=0x735a30, nmsg=0x0) at fc_request.c:610
#7  0x0000000000409898 in msg_parsed (ctx=0x7fffffffea00, conn=0x634ab0, msg=0x735a30) at fc_message.c:168
#8  0x0000000000409c99 in msg_parse (ctx=0x7fffffffea00, conn=0x634ab0, msg=0x735a30) at fc_message.c:317
#9  0x0000000000409e85 in msg_recv_chain (ctx=0x7fffffffea00, conn=0x634ab0, msg=0x735a30) at fc_message.c:383
#10 0x0000000000409f30 in msg_recv (ctx=0x7fffffffea00, conn=0x634ab0) at fc_message.c:417
#11 0x000000000040415f in core_recv (ctx=0x7fffffffea00, conn=0x634ab0) at fc_core.c:77
#12 0x000000000040454f in core_core (ctx=0x7fffffffea00, conn=0x634ab0, events=1) at fc_core.c:157
#13 0x00000000004046f2 in core_loop (ctx=0x7fffffffea00) at fc_core.c:215
#14 0x000000000041396d in main (argc=13, argv=0x7fffffffeb08) at fc.c:746
The release link in the README points to the Google code archive, and there hasn't been any tarball release since 2013!

Maybe you should make a new one, or simply get rid of release tarballs if this does not make sense.
libfc is non-thread-safe lib, for using it as a independent storage engine
hi @manjuraj 

* allow to set max simultaneous connections
* server shouldn't close listen fd when accept function occur error
like  the following snippet: 
 n = pwrite(fd, slab, size, off);
    if (n < size) {
        log_error("pwrite fd %d %zu bytes at offset %"PRId64" failed: %s",
                  fd, size, off, strerror(errno));
        return FC_ERROR;
    }

when n<size ,pread or pwrite may interrupt by signal ,errno is equal to EINTR,
it should process on writing,not return an error?

Hi,

I am testing fatcache with libmemcached. I have write a simple program, and I found that during the working process, the program consumes a lot of my memory. My computer has 64G memory, and the program can take all these memories. 

The setup is like this:
OS: ubuntu 14.04
database: mysql 5.6
C++ library: libmemcached
Memory: 64GB
SSD: Use a usb driver flash instead

By debugging the program, I found that the set operation needs a lot of memory. So, do you have any idea about this problem? Is there any problem with libmemcached? Looking forward for your replay. Thank you very much!

Best!

Hi,

I received the following kernel error while running the fatcache:

```
 kernel: Out of socket memory
```

Setup:
- 96 GB RAM
- 1 TB SSD
- 8 fatcache instances each with ~21GB of memory each `-m 21760 -i 64` (12 core box)
- Running a "memslap" client creating thousands of connections (~36 K connections) to fatcache
- When the free memory left is around 5 GB, I see the above error and memslap stops
- 2.6.32 kernel

Let me know if you need any other information

Hi,  I noticed that GETs do not result in writes to the disk. This makes sense for the common case.

I'm wondering if there are cases where fatcache would be required to write to disk in the future? e.g. support CAS, maintain state of cache across power cycle.

BTW, Is there a plan to support "persistent" fatcache that keeps state across power cycle?  Is a persistent memcache useful in real deployments?  

Thanks.

Hi, is fatcache currently in production use?  I'm interested in contributing to optimizing memcache for SSDs, and would like to start with performance analysis to expose optimization possibilities.  Should I start with fatcache or is there another deployment you would recommend? Thanks.

In your summary, you write that you have code in place to check for SHA-1 collisions. I suggest getting rid of that code and its performance overhead.

There are simply no collisions for reasonably short keys. The chance of accidentally producing a hash collision for SHA-1 is astronomically small. Even purposely finding SHA-1 collisions with brute force takes ca. 2^80 instructions. Just do the math on how long your system would need to be running for that to happen.

The only reason for doing the check would be to protect against more elaborate attacks which a user might be able to inject. It is unlikely that SHA-1 will be susceptible to preimage attacks (which would be necessary to, for example, retrieve other chosen data from the cache) any time soon (not even MD5 is). More likely but still a far fetch would be a collision attack on SHA-1 that would allow a complexity attack on your system. If that ever becomes a potential threat, you should migrate to a better hash as checking won't help you.
