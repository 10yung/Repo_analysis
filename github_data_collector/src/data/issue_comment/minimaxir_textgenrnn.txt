I'm absolutely not an expert but wouldn't the Adam-optimizer better to use for textgenrnn than RMSprop?  
This will use native Tensorflow Keras and supports Tensorflow 2.1
Hello

Is there anyway to feed some "text" while generating at runtime?
For example, while generating texts, can we feed some sample sentence/sentences into the algorithm,  so that, it can create "texts" based on given text?

It crashes after about 100 lines when I try to train a new Model on my GPU. This doesn't happen when I use my CPU.
Im using: Python 3.6, TF-GPU 1.15, CUDA 10

```Python
from textgenrnn import textgenrnn
textgen = textgenrnn()
textgen.reset()
textgen.train_from_file('roll.txt', num_epochs=5)
print("done")
textgen.save("thistest.h5")
textgen.generate()
```
```
Epoch 1/5
2019-12-30 22:57:34.996742: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2019-12-30 22:57:35.225537: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
331/900 [==========>...................] - ETA: 33s - loss: 1.9191
```


Hi there,

First of all great library, I've been having lots of fun with it!

Is it expected behaviour that the vocab.json file doesn't get updated once it's been created from a new_model?

I have the following workflow:
* With ~1200 txt files, train_from_largetext_file on #1 with new_model=True and word_level=True
* Stop the script, which saves out the weights, vocab and config files
* Start the script loading in the saved weights, vocab and config files, and for the remaining txt files, loop through each and train_from_largetext_file with new_model=False and word_level=True

My expectation is that for each new file it trains, it would contribute to the vocab file and refine the model. But the reality is the vocab file doesn't get updated, so new words found in the new texts do not become part of the model. Is this right?

Thanks again for your work.

and how to deal with Chinese
I had made textgenrnn with external Keras since native TF was missing features. Now that there is parity, I am OK with merging it back into native TF with TF 2.0 support. textgenrnn does not use much custom Keras code so it should be a relatively simply change; the concern is not breaking old models, which may be possible due to the `SavedModel` change.

TF 2.1 also has TPU/mixed precision support which will be very helpful for training performance.