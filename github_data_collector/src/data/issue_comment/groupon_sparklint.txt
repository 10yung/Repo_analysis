Attempting to run the embedded listener in spark-shell as:

`spark-shell --master=spark://localhost:7077 --conf spark.extraListeners=com.groupon.sparklint.SparklintListener --packages com.groupon.sparklint:sparklint-spark201_2.11:1.0.8`

Results in shell exiting with:

`java.lang.NoClassDefFoundError: org/http4s/package$HttpService$`

```
Caused by: java.lang.ClassNotFoundException: org.http4s.package$HttpService$
  at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
  ... 92 more
```


```
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.3
      /_/

Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_212)

$ java -version
openjdk version "1.8.0_212"
OpenJDK Runtime Environment (build 1.8.0_212-b04)
OpenJDK 64-Bit Server VM (build 25.212-b04, mixed mode)
```

I have set up sparklint in server mode using their docker image and added the spark event log directory of my streaming jobs as a source, but the spark lint UI shows that the jobs are 0% complete and 0 active 
![image](https://user-images.githubusercontent.com/20191599/62520460-43bc9080-b84b-11e9-9b70-463c2562f532.png)

Is this a configuration issue or some problem with the docker image? I have followed the instruction on their wiki. Appreciate any pointers on this.
We would like to monitor live spark jobs in server mode. We want all spark jobs to be in one place... When you run in live mode there is only one SparkLint UI per job? We would like all finished and running jobs to be under one SparkLint UI. 

We are able to run in live mode where there is only one job per UI. In server mode it seems to only show spark jobs that have finished? 

Is there a way to see all jobs old and running in server mode?
Hi @roboxue Can sparklint work on standalone spark cluster ? Thanks XD ~
Hi, 
So I've just started using Databricks and I'm pretty stoked about being able to use sparklint and checking on my jobs. 

I currently upload my jars to Databricks and then invoke the methods that I am interested in. 

I tried including sparklint from the sbt file as mentioned, but the compilation always fails. 
(Let me know if you would want to see the logs in detail. I will upload them if you would like to)

I then compiled only the sparklint code and uploaded it as a separate jar, attached it to my cluster,  and in the other jar that holds my code, I added the lines:
` spark.conf.set("spark.extraListeners", "com.groupon.sparklint.SparklintListener")
  spark.conf.set("spark.sparklint.port", "4242")`

However, I am unable to access the port or see any sort of indication in the logs that sparklint has been active during the job execution. 

Can you please help me get this going on Databricks?

Hi,

Can sparklint also show this on the dashboard?
1) heap memory utilization for driver and executors
2) Amount of data spilled to disk

Thanks,
Manish
It would be super nice if app can recommend spark-submit parameters for the jobs.

One small comment: The necessity of pressing play/fast forward buttons is not intuitive. Firstly, I closed the application thinking that it isn't working. A small popup that shows those buttons may be convenient.

Overall, great app! It helps a lot, thank you so much. 
Keep up the good work!
I have tried a few combinations when starting the UI
```
docker run -v /path/to/logs/dir:/logs -p 23763:23763 roboxue/sparklint -d /logs && open localhost:23763
```

But if I have .lz4 files in the directory, they are not picked up as sources.
I notice there is some code in sparklint that uses the lz4 codec.

Hi ,

Tried the instruction provided in https://github.com/groupon/sparklint . When I do a spark submit I do see the logs which says the below . But when I hit the driver IP with port I cannot see the UI. Could you please help me . Are there any additional configs to be applied ?

Logs

17/08/17 14:30:16 INFO nio1.NIO1SocketServerGroup: Service bound to address /0:0:0:0:0:0:0:0:23763
17/08/17 14:30:16 INFO sparklint.Sparklint: Server started on 23763
17/08/17 14:30:16 INFO spark.SparkContext: Registered listener com.groupon.sparklint.SparklintListener
I am unable to add the history server link which is hosted on Microsoft Azure HDInsight cloud. I get the error 

> unexpected HTTP status: 401 Credentials missing or Auth is not Basic

Is there any way to provide the credentials?