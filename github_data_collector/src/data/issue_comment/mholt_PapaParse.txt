I've been studying PapaParse's code from 2 days. It is difficult to understand for a programmer, without proper architecture defined in the docs. I have to read to code and interpret the concept, which could be wrong also!! 
When I parse remote file `file` variable in `complete` callback is just a string with filename instead of [File](https://developer.mozilla.org/en-US/docs/Web/API/File).

```javascript
complete: function(results, file) {
	console.log("Parsing complete:", results, file);
}
```

How can I pass [File](https://developer.mozilla.org/en-US/docs/Web/API/File) ?
Hello, how are you?

I've been using PapaParse in Nodejs to parse big remote files with streams. The library is great, thank you very much for your dedication.

One thing I'm trying to achieve is to be able to retrieve the **errors** field of an output in PapaParse's stream. When you process the data in one go (that is, when not using a stream), the output you get is of the following structure:
```
{
   data: [...],
   errors: [...],
   meta: {...}
}
```
However, when piping PapaParse's read stream to a writable stream, I don't receive an object with **errors** and **meta** fields, but a parsed row only. I would like to know if there is any way of receiving this whole object for each parsed row as I'm very interested in the **TooFewFields** and **TooManyFields** errors.

This is important for me because I would like to maintain the same error checking interface when parsing with and without PapaParse's stream, even if it means having an impact in the parsing performance.

Happy holidays!
Full error:
```
Uncaught DOMException: Failed to execute 'open' on 'XMLHttpRequest': Invalid URL
    at l._readChunk (blob:http://localhost:8080/0b8053c6-ba21-4708-8fbe-cf0ee7c7f2ed:1:7241)
    at l._nextChunk (blob:http://localhost:8080/0b8053c6-ba21-4708-8fbe-cf0ee7c7f2ed:1:6870)
    at l.stream (blob:http://localhost:8080/0b8053c6-ba21-4708-8fbe-cf0ee7c7f2ed:1:6977)
    at Object.parse (blob:http://localhost:8080/0b8053c6-ba21-4708-8fbe-cf0ee7c7f2ed:1:1097)
    at f.onmessage (blob:http://localhost:8080/0b8053c6-ba21-4708-8fbe-cf0ee7c7f2ed:1:18176)
```

The code - which works fine when I don't include `worker: true`:
```
function downloadWeather(location: string) {
  weather[location] = [];
  let rowNumber = 0;
  Papa.parse(`/data/WeatherRaw${location}.csv`, {
    download: true,
    dynamicTyping: true,
    header: true,
    worker: true,
    step(row: any) {
      weather[location].push(row.data as RawWeatherType);
    },
    complete() {
      console.log('Weather downloaded for ' + location);
    },
  });
}
```

The only two things I can think of are a bug in the library (unlikely), or that I need to set up something special in webpack to work with webworkers? Here's my webpack file: https://github.com/toddmedema/electrify/blob/master/shared/webpack.shared.js
Is there any way to use a promise instead of a callback?  I've gotten to where I hate callbacks.
## Issue

If I pass a CSV stream to `Papa.parse` that contains special characters, it sometimes breaks the special characters so they show up as e.g. ��.

## How to reproduce

See example at:
https://repl.it/repls/ArcticTreasuredLine

Press "Run" at the top of the page

### What should happen?

There should only be output of `ä` character

### What happened?

There's random occurrences of `��`

## Root cause

These two lines are responsible for this issue:
https://github.com/mholt/PapaParse/blob/ae73d2a96639beec58a83326de6bd8e8ca0c02b3/papaparse.js#L863
https://github.com/mholt/PapaParse/blob/ae73d2a96639beec58a83326de6bd8e8ca0c02b3/papaparse.js#L506

So when papaparse reads a chunk, it directly calls `.toString` of that chunk.

However a chunk consists of bytes, and some utf8-characters are two bytes long:
* `ä` consists of two bytes: `11000011` and `10100100`
* `a` (and other "regular" characters) is just one byte `01100001`

Now if the chunk splits right between a multi-byte character like `ä`, papaparse calls `toString` to both parts of the character distinctly, and produces two weird characters:

`11000011` (from end of first chunk) transforms to `�`
`10100100` (from start of second chunk) transforms to `�`

## How to fix this issue?

If received chunks of bytes, the concatenation should be done in bytes too, e.g. using `Buffer.concat`. Papaparse should not call `toString` before it has split the stream to lines, so the `_partialLine` remains as a buffer rather than a string type.
Hi,

When I am handling csv file which is originally encoded with Windows-1252 (cp1252) via e.g. demo website: https://www.papaparse.com/demo and choose ISO-8859-15 as encoding in options, the special chars are decoded wrongly. I tested it for euro sign, which has 128 value in Windows-1252 and 164 in ISO-8859-15.

Parsing cp1252 encoded file with euro signs as ISO-8859-15 should end up displaying some other character in place of euro. However euro sign is visible in results.

What is more it doesn't work like that if I upload ISO-8859-15 encoded file and choose cp1252 as encoding. Euro sign is that case replaced with some other character. 


Would love to get your feedback on my wishlist for Papaparse. We're going to dedicate some time for the team @ Flatfile before the end of year to move anything on this list we're agreed on to a next release.

### 1. Migrate to ES7 + Typescript or Flow
This is something that, after a deep audit of the source code we feel is necessary to ensure long term reliability. In a data oriented library like this, type strength will allow for easier reasoning about the code, add stability and prevent unseen bugs. ES7 will allow for more readable code

> Since not everybody understands TS or Flow deeply enough to have confidence to contribute. My recommendation is to keep the config permissive to allow for vanillaJS contributions and have core contributors add stronger typing before merging to master.

### 2. Separate NodeJS build from Browser build
This will allow for a lighter package when using in the browser (vast majority based on cursory analysis), as well as open more freedom to invest in optimizations for each stack independently. These could either be distributed as different packages (eg. `@papaparse/core` and `@papaparse/node`) or a second build in the same package. (eg. (`import PapaParse from 'papaparse/node'`)

### 3. Reduce core sugar and add plugin framework
We've noticed a lot of the open issues relate to desired support of edge cases or unique data scenarios that shouldn't be treated as part of the core "csv parser" but are entirely legitimate use cases. The goal here is to distribute a core package with _common functionality_ and allow users to choose additional use cases as needed.

Candidates:
- `@papaparse/http` - adapter for downloading or streaming data from web - can be optimized separately for nodejs and browser as well as opens up for other adapters for things like S3 with plenty of optimizations.
- `@papaparse/types` - split out the typecasting logic, there's a lot of room for improvement here w/better understanding of boolean types, dates, etc. But it doesn't make sense to invest that into a core csv parser library.
- `@papaparse/unparse` - there's been a decent amount of confusion with users about how different configurations relate to parse vs. unparse. These are also distinctly different problems to solve for.

Future Candidates:
Things like `detect-encoding` to auto detect file encoding, `generous-escaping` (for the common unescaped quotes situation), and many other user requests. Additionally framework specific components like an HoC for React could be awesome.

### 4. Improved docs
Would love to see updated searchable docs with both auto generated API references as well as guides, fiddles, and improved demo. I'm a fan of docusaurus for this. We'd be happy to contribute content & design here.

### 5. Reorganized source code
With almost 2000 lines in papaparse.js it's time to tackle deconstructing that a bit into components that are easier to reason about. Since 4b16215353aa256da44c48160441e91ef0254340 6 years ago (335 lines) when most of the tools we have at our disposal today weren't available, we haven't changed much. Time for a `src` folder! Let's follow https://sourcemaking.com/refactoring as a guide

### 6. Tests, coverage, cross-browser testing & CI based distribution
We should take advantage of setting up the Sauce testing matrix so we don't break things in old browsers as we go. Also, it'd be great if we could use Github Actions to auto deploy master and release candidates to npm / bower / etc. In addition we should improve unit test coverage in addition to the mainly acceptance testing we have now.

### 7. Other: Pipes, Promises, etc.
- Piping for easier composition of logic, also relatively required for plugin framework.
- Promises because it's 2019 (shim for legacy browsers)
- Allow for some dependencies and ensure fossa.com scans are run on them - let's not re-invent all the wheels.
- Functional first - no classes unless necessary
- Package decomposition - As seen in the above example, switching to an npm org w/multiple packages and likely using something like [lerna](https://github.com/lerna/lerna) to manage the packages.

### 8. Backwards compatibility adapter
Because this would be a pretty robust overhaul, we should publish an adapter that's fully backwards compatible with re-composed elements. Possibly `@papaparse/legacy` - allowing people to move forwards without a complete overhaul. It could also identify the things they aren't using and give them a custom migration checklist.

We're happy to take on the work of this overhaul here at Flatfile - so keep in mind we're not asking for a lot of work from the community. But do please provide feedback on all of this, we want to chart a path forwards that makes sense to everybody.

Also, what do **you** want to see? Comment with new ideas or criticisms / approval of the above.
https://github.com/mholt/PapaParse/issues/738
I am trying to download a csv.gz file and then be able to treat the data rows by rows on a node server. I don't mind the method (request, https...) but somehow I do not manage to get clean data that Papa Parse would parse normally. Now I am getting some encoded strangeness.

My last try is the following :

```
try {
  var streamHttp = await new Promise((resolve, reject) => {
    var buffer = [];
    https.get(url, res => {
      res.setEncoding("binary");
      let gunzip = zlib.createGzip();
      res.pipe(gunzip);
      gunzip
        .on("data", data => {
          buffer.push(data.toString());
        })
        .on("end", () => {
          resolve(buffer.join(""));
        });
    });
  });
} catch (e) {
  console.log(e);
}

Papa.parse(streamHttp, {
  delimiter: ",",
  step: row => {
    console.log(row);
  },
  complete: results => {
    console.log("complete");
  }
});
```
but the result I am getting in the console log is full of "�>�g��x�9�)" so I guess I have a problem with encoding.

The url I use is not directly to a .csv.gz file but to an automatically generated address ending in "/" where the file is. I don't know if it matters or not. I do not mind a totally different approach than mine, just trying to find a solution.

Thanks in advance and keep up the great work with papa parse !

If I get the solution I'd be glad to propose a mini example to be added to the doc.