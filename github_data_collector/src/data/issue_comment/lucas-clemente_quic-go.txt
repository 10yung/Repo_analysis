
Fixes #2258, which was deleted by Github. Depends on #2293.
Hey there,

I've been working on a HTTP1 to HTTP3 proxy to transport allow NGINX to connect to a HTTP3 upstream. I have been testing this by connecting to both Cloudflare.com using HTTP3 and the Quiche based NGINX server patches.

After a while (few hundred requests) and sometimes even immediately I start to run into `STREAM_STATE_ERROR` errors. And the client won't be able to recover on its own without creating a new round tripper instance. This is when connecting to the same host. This is on quic-go version v0.14.1.

When it breaks while connecting to the NGINX based server it will for example still work to Cloudflare.com, so I assume something got stuck in the connection state and the client isn't cleaning up because of the error.

A code sample essentially boils down to this:

```go
package main

import (
	"github.com/lucas-clemente/quic-go/http3"
	"log"
	"net/http"
)

func main() {
	hclient := &http.Client{
		Transport: &http3.RoundTripper{},
	}

	http.HandleFunc("/", func(w http.ResponseWriter, request *http.Request) {
		request.RequestURI = ""
		request.URL.Host = request.Host
		request.URL.Scheme = "https"

		response, err := hclient.Do(request)
		if err != nil {
			http.Error(w, http.StatusText(http.StatusInternalServerError) + ": "+ err.Error(), http.StatusInternalServerError)

			return
		}

		if err := response.Write(w); err != nil {
			log.Printf("Write error: %+v", err)

			http.Error(w, http.StatusText(http.StatusInternalServerError) + ": "+ err.Error(), http.StatusInternalServerError)

			return
		}
	})
	log.Fatal(http.ListenAndServe(":8080", nil))
}
```
Fixes #2065.
As shown by #2275, it's not enough to just detect that 0-RTT packets were sent, we also need to somehow detect that they were actually processed. Otherwise, we can't distinguish between a successful and a reject 0-RTT connection.
any plan  H3 to support  trailer? It's important for file transfer streaming over H3 to present final status

This seems to be caused by the internal send buffer of the UDP socket becoming full causing Linux kernel to return EAGAIN for writes. The golang netpoller handles this by waiting until the socket is writable ([source](https://github.com/golang/go/blob/release-branch.go1.13/src/internal/poll/fd_unix.go#L331)). This eventually causes Session to freeze on SendQueue.Send().
Stream write buffer like (*TCPConn) SetWriteBuffer.
It turns out that the Linux kernel's UDP receive buffer is too small by default (around 200k). In order to support higher bandwidths, we need a significantly higher value.

It would be good if we could set this on a per-connection basis. Not sure if Go allows us to do this.