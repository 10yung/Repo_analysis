error validating "STDIN": error validating data: ValidationError(Deployment.spec): missing required field "selector" in io.k8s.api.apps.v1.DeploymentSpec; if you choose to ignore these errors, turn validation off with --validate=false

I tried to update from v1beta to apps/v1
for Kubernetes1.16+
But the deployment file fails.

I've installed 3 node kubernetes cluster and deployed Gluster cluster on it. But At heketi-deployment, It got failed. The kubernetes logs say unable to access heketi.db located at /var/lib/heketi/heketi.db.

Here are the logs from kubernetes pod.

```
Setting up heketi database
  File: /var/lib/heketi/heketi.db
  Size: 16384     	Blocks: 32         IO Block: 4096   regular file
Device: fe01h/65025d	Inode: 1185900     Links: 1
Access: (0644/-rw-r--r--)  Uid: (    0/    root)   Gid: (    0/    root)
Access: 2020-01-11 08:18:25.152360099 +0000
Modify: 2020-01-11 08:18:25.148359972 +0000
Change: 2020-01-11 08:18:25.148359972 +0000
 Birth: 2020-01-11 08:18:25.132359465 +0000
failed to dump db: Could not construct dump from DB: Could not construct dump from DB: Unable to access list
error: Unable to export db. DB contents may not be valid
Heketi v9.0.0-136-g587dc593
ERROR: Unable to parse configuration: EOF
```
I am using Openstack for my kubernetes Node: I have 1 Mster and 2 worker in it.

I have 2 volumes of 200GB each attached to the each worker- dev/vdb.

While installing the gluster-kubernetes I am getting the below Error:

`Error: Failed to allocate new volume: No space
command terminated with exit code 255
Failed on setup openshift heketi storage
This may indicate that the storage must be wiped and the GlusterFS nodes must be reset.`

Configuration: kubernetes 1.16 / coreos


`core@test-eu-k8s-k8s-master-nf-1 ~/gluster-kubernetes $ kubectl get nodes
NAME                          STATUS   ROLES    AGE   VERSION
test-eu-k8s-k8s-master-nf-1   Ready    master   19d   v1.16.2
test-eu-k8s-k8s-node-nf-1     Ready    <none>   19d   v1.16.2
test-eu-k8s-k8s-node-nf-2     Ready    <none>   19d   v1.16.2`

My topology.json.sample
`{
  "clusters": [
    {
      "nodes": [
        {
          "node": {
            "hostnames": {
              "manage": [
                "test-eu-k8s-k8s-node-nf-1"
              ],
              "storage": [
                "172.21.97.105"
              ]
            },
            "zone": 1
          },
          "devices": [
            "/dev/vdb"
          ]
        },
        {
          "node": {
            "hostnames": {
              "manage": [
                "test-eu-k8s-k8s-node-nf-2"
              ],
              "storage": [
                "172.21.97.102"
              ]
            },
            "zone": 1
          },
          "devices": [
            "/dev/vdb"
          ]
        }
      ]
    }
  ]
}`



**The error:**

[Y]es, [N]o? [Default: Y]: y
Using Kubernetes CLI.
Using namespace "default".
Checking for pre-existing resources...
  GlusterFS pods ... not found.
  deploy-heketi pod ... not found.
  heketi pod ... not found.
  gluster-s3 pod ... not found.
Creating initial resources ... serviceaccount/heketi-service-account created
clusterrolebinding.rbac.authorization.k8s.io/heketi-sa-view created
clusterrolebinding.rbac.authorization.k8s.io/heketi-sa-view labeled
OK
node/test-eu-k8s-k8s-node-nf-1 labeled
node/test-eu-k8s-k8s-node-nf-2 labeled
daemonset.apps/glusterfs created
Waiting for GlusterFS pods to start ... OK
secret/heketi-config-secret created
secret/heketi-config-secret labeled
service/deploy-heketi created
deployment.apps/deploy-heketi created
Waiting for deploy-heketi pod to start ... OK
Creating cluster ... ID: ad68d55094021aab45e1787211bddb9e
Allowing file volumes on cluster.
Allowing block volumes on cluster.
Creating node test-eu-k8s-k8s-node-nf-1 ... ID: 286a9afcb058acd6ca548ebd12d05f97
 ^[[DAdding device /dev/vdb ... OK
Creating node test-eu-k8s-k8s-node-nf-2 ... ID: 7643d57e7a2a417e70e32c85e7acdb18
Adding device /dev/vdb ... OK
heketi topology loaded.
Error: Failed to allocate new volume: No space
command terminated with exit code 255
Failed on setup openshift heketi storage
This may indicate that the storage must be wiped and the GlusterFS nodes must be reset.
k8s 1.16+ no longer supports `extensions/v1beta1` for DaemonSet and Deployment objects.  The api versions for these have been updated to `apps/v1` to match the documentation in kubernetes:  [DaemonSet](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/), [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)

selectors were added to the daemonset because the selector property is no longer optional.

<!-- Reviewable:start -->
---
This change is [<img src="https://reviewable.io/review_button.svg" height="34" align="absmiddle" alt="Reviewable"/>](https://reviewable.io/reviews/gluster/gluster-kubernetes/629)
<!-- Reviewable:end -->

gk-deploy outputs the message : 
```
Deploying GlusterFS pods.
sed -e 's/storagenode\: glusterfs/storagenode\: 'glusterfs'/g' /root/kube/gluster-kubernetes/deploy/kube-templates/glusterfs-daemonset.yaml | /usr/bin/kubectl -n gk create -f - 2>&1
error: unable to recognize "STDIN": no matches for kind "DaemonSet" in version "extensions/v1beta1"
Waiting for GlusterFS pods to start ...
Checking status of pods matching '--selector=glusterfs=pod':
```
It seems that for v1.16+ we should use extensions/v1 and no more v1beta
https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.16.md#deprecations-and-removals



<!-- Reviewable:start -->
This change is [<img src="https://reviewable.io/review_button.svg" height="34" align="absmiddle" alt="Reviewable"/>](https://reviewable.io/reviews/gluster/gluster-kubernetes/626)
<!-- Reviewable:end -->

- updated deprecated and removed kubernetes 1.16 API beta endpoints. (deprecated since 1.9 - 1.10)
- replaced with proper official endpoints.
- fixed deployment template selectors. (necessary since 1.16)
- removed deprecated kubectl --show-all argument

this is production tested

<!-- Reviewable:start -->
---
This change is [<img src="https://reviewable.io/review_button.svg" height="34" align="absmiddle" alt="Reviewable"/>](https://reviewable.io/reviews/gluster/gluster-kubernetes/625)
<!-- Reviewable:end -->

Versions:
k8s: v1.15.4  -- heketi: v9.0.0, glusterfs: 6.5+

We are facing the same problem as in #404  when undeploying an application with tens of pvcs/pvs. It looks like pvc and pv are gone.
kubectl get pvc or kubectl get pv ---> no resources found

But both volumes in heketi as well as - the corresponding glusterfs-dynamic services are not still showing up -- not removed:
heketi-cli volume list or heketi-cli topology info --> show the volumes there. Also gluster volume list
and for dynamic services:

kubectl get svc --all-namespaces
NAMESPACE NAME TYPE CLUSTE
R-IP EXTERNAL-IP PORT(S) AGE
default glusterfs-dynamic-067b2708-8ffc-4f39-a5ee-67b05675fb73 ClusterIP 10.254
.1.187 1/TCP 3d18h
default glusterfs-dynamic-08ae158a-0064-42ed-b24b-8713186379ef ClusterIP 10.254
.160.55 1/TCP 3d18h
default glusterfs-dynamic-125a572f-d9a1-4ce1-b948-f1c1676cec16 ClusterIP 10.254
.180.65 1/TCP 3d18h
.....
For me this makes no sense since it says 'If the volume is found, then throw an error and abort..?'

https://github.com/gluster/gluster-kubernetes/blob/7246eb4053c8c5336e4da68d86b76124d435eb3e/deploy/gk-deploy#L870-L873
Fix for [issue 618](https://github.com/gluster/gluster-kubernetes/issues/618)

<!-- Reviewable:start -->
---
This change is [<img src="https://reviewable.io/review_button.svg" height="34" align="absmiddle" alt="Reviewable"/>](https://reviewable.io/reviews/gluster/gluster-kubernetes/621)
<!-- Reviewable:end -->
