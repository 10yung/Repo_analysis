when I run the notebook (05_pytorch_backend/cnn_acceleration_tensorly_and_pytorch.ipynb), I find that the tucker decomposition convolutional layer runs slow. The decomposed VGG16 runs slower than the original VGG16? why? how to evaluate the runtime and calculate the speedup?
This is based on @jcrist's notebook from https://gist.github.com/jcrist/f7f0682ed01f12e96f9a40d8862b2477. I have added an example using `parafac` to the end of the notebook. I didn't know of any real life example sparse tensors that can be factored easily (the nips tensor that Jim used in the notebook seems to have a very large rank), so I constructed one using a random sparse factorization. If you know of a better example, let me know. 

This requires https://github.com/tensorly/tensorly/pull/85.