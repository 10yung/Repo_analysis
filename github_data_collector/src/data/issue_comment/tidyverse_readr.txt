Before, the documentation for param `x` in `write_delim()` inside `R/write.R` **was reused** for `write_file()`, `write_lines()` and `format_*()` – which is obviously wrong. This PR redefines/overwrites the param `x` documentation where appropriate.
Hello

When reading numerical data where commas are wrongfully used instead of points, `read_table2` seems to be silently removing commas leading to a modification of numerical values. For example, `read_table2` turns the value "19,1" to 191 without any warning (line 38 of the original dataset):

```r
library(readr)
sparrows <- read_table2("https://raw.githubusercontent.com/frousseu/ECL749/master/data/sparrows.txt",n_max=50)

## Parsed with column specification:
## cols(
##  `"Sex"` = col_character(),
## `"wingcrd"` = col_double(),
## `"wt"` = col_number()
## )

sparrows[38,]

##  A tibble: 1 x 3
##  `"Sex"` `"wingcrd"` `"wt"`
##  <chr>         <dbl>  <dbl>
##  1 "\"m\""          57    191

```
Thanks!

The second example is confusing: https://github.com/tidyverse/readr/issues/1007

And it for such an useful function, it would be nice to have a few more examples of realistic usage.
R version 3.5.3 (2019-03-11)
Platform: x86_64-pc-linux-gnu (64-bit)
readr: 1.3.1.9000

I compiled readr from source (hence at version 1.3.1.9)  to see if the problem went away, but it did not make any difference. 


Hi,

This problem stumped me for a few days as I thought I had hardware and or system library problems.

I was using readr::read_csv() to open a large gzipped CSV file with about 500,000 rows and 300 columns. I regularly check the size of the returned data.frame as I've been performing multi-stage processing/cleaning and save the data between stages.

For example:

```r
df <- readr::read_csv('mydata.csv.gz', guess=10000)
```

On one occasion, I found that the data frame was only about 200,000 rows and I was getting one or two warnings about parsing the file. 

I thought I might have accidentally removed too many rows in a previous stage or used the wrong delimiter somehow. I unzipped the csv.gz file from a bash terminal and read the file in using base R read_csv(). Much to my relief I found that all the expected number of rows and columns were present, so the csv.gz file was fine.

The odd thing was that almost every time I ran the readr::read_csv() function to read the csv.gz file, the row and column that produced the parsing error was different. It was never consistent. Hence, I thought I had some sort of hardware problem or an odd system library issue.

I eventually found that my /tmp directory was almost full. Such that readr:read_csv() would fill the /tmp folder and I would get the parsing errors and a smaller tibble than expected.

Below is a reproducible example for a GNU/Linux system. 

We can create a small temporary directory using tmpfs as root to illustrate the problem.

```shell
# mkdir /tmp_r
# mount -t tmpfs -o size=1M tmpfs /tmp_r/
# df
Filesystem           Type      Size  Used Avail Use% Mounted on
...
tmpfs                tmpfs     1.0M     0  1.0M   0% /tmp_r

```

From my understanding, readr decompresses files to the tempdir() location so it can parse the file and guess the column types. To manually override the tempdir() location, I found the [unixtools](https://github.com/s-u/unixtools) library to be the easiest approach. We'll also change the temporary session path (R_SESSION_TMPDIR) for good measure.

```r
library(devtools)
devtools::install_github("s-u/unixtools")
library(unixtools)
unixtools::set.tempdir('/tmp_r')
tempdir()
# [1] "/tmp_r"
Sys.setenv(R_SESSION_TMPDIR = '/tmp_r')
Sys.getenv('R_SESSION_TMPDIR')
# [1] "/tmp_r"
```

Create a sufficiently large data.frame and save it as a CSV file.

```r
df <- data.frame(matrix(0, nrow=1000000, ncol=20))
write.csv(df, 'mytable.csv', row.names = F, quote = T )

```

Read the csv file back into R to check if it worked 

```r
df_csv <- read.csv( 'mytable.csv' )
dim(df_csv)
# [1] 1000000      20
```

I could have used write.csv(gzfile()) to compress the file, but it's easier do it manually in the terminal.

```shell
$ gzip -c mytable.csv > mytable.csv.gz
```

Base read.csv() and gzfile() has no problem reading the file. This is important because readr also uses gzfile() and I wanted to rule out gzfile() being the cause of the problem.

```r
df_csv_gz <- read.csv( gzfile('mytable.csv.gz') )
dim(df_csv_gz)
# [1] 1000000      20
```

However, running readr:read_csv() produces warnings and a tibble with fewer rows than expected.
```r
df_csv_gz <- readr::read_csv( 'mytable.csv.gz' )
# Parsed with column specification:
#   cols(
#     .default = col_double()
#   )
# See spec(...) for full column specifications.
# Warning: 1 parsing failure.
# row col   expected     actual         file
# 26212  -- 20 columns 13 columns <connection>
dim(df_csv_gz)
# [1] 26212    20

```
The tempdir() is not completely filled on the first run as you would expect.
If you re-running the above command two times tempdir is filled with three files.

```shell
# ls -l /tmp_r
total 1.0M
-rw-r--r-- 1 user1 user1 1016K Dec 26 12:12 file45439fd235a
-rw-r--r-- 1 user1 user1  8.0K Dec 26 12:12 file4544bae6c09
-rw-r--r-- 1 user1 user1     0 Dec 26 12:12 file454604032cd
```

Running the read_csv() for a forth time results in an empty data.frame and no messages at all.
```r
df_csv_gz <- readr::read_csv( 'mytable.csv.gz' )
dim(df_csv_gz)
# [1] 0 0
```

If we increase the \tmp_r directory size, the problem is solved.

```shell
# rm -rf /tmp_r/*
# mount -t tmpfs -o remount,size=50M tmpfs /tmp_r/
# df
Filesystem           Type      Size  Used Avail Use% Mounted on
...
tmpfs                tmpfs     50M     0  50M   0% /tmp_r
```

```r
df_csv_gz <- readr::read_csv( 'mytable.csv.gz' )
# Parsed with column specification:
#   cols(
#     .default = col_double()
#   )
# See spec(...) for full column specifications.
dim(df_csv_gz)
# [1] 1000000      20

```
I would have expected readr:readr() to check if gzfile() successfully decompressed the file before trying to parse it. 

If you need any further information, please let me know.
See comments in the reprex.

```r
library(readr)
library(tibble)

# there is an empty string and an NA value in the data
dat <- tibble(
  x = c(NA, ""),
  y = c(1, 1)
)

print(dat)
#> # A tibble: 2 x 2
#>   x         y
#>   <chr> <dbl>
#> 1 <NA>      1
#> 2 ""        1

write_delim(dat,
            "dat.csv",
            delim = ",",
            na = "",
            col_names = F)

dat <- readLines("dat.csv")

# write_delim() creates a csv file where the empty string is quoted, while the NA value is not
cat(dat, sep = "\n")
#> ,1
#> "",1

# however, read_delim() does not respect the difference, and treats both as NAs
dat <- read_delim(
  "dat.csv",
  col_names = c("x", "y"),
  col_types = cols(x = col_character(),
                   y = col_double()),
  locale = locale(encoding = "UTF8"),
  delim = ",",
  quote = "\"",
  na = ""
)

print(dat)
#> # A tibble: 2 x 2
#>   x         y
#>   <chr> <dbl>
#> 1 <NA>      1
#> 2 <NA>      1
```

write_csv uses the display option digits.secs to determine the precision of timestamps when writing csv's and will cut off decimal seconds when exporting without any sort of warning (write.csv appears to do the same). Also there seems to be minor precision issues with reading and writing timestamps.
```

library(lubridate)
library(readr)
Sys.setenv(TZ='UTC')
options(digits.secs=6) #Set to display 6 decimal places
packageVersion("readr")
#[1] ‘1.3.1’

t1<- tibble(time= mdy_hms("1/1/2019_12:30:05", tz = "UTC")+microseconds(500))
t1$time[1] #"2019-01-01 12:30:05.0004 UTC" #Displayed incorrectly
t1$time[1] == ymd_hms("2019-01-01 12:30:05.0005") #TRUE (Actually equal to .0005 despite incorrect display)
write_csv(t1, "test_time.csv") #write csv with display set to 6 decimal places

options(digits.secs=2) #Set to only display 2 decimal places
write_csv(t1, "test_time2.csv") #write csv with display set to 2 decimal places

options(digits.secs=6) #Reset back to display 6 decimal places

test1<- read_csv("test_time.csv")
test1_text<- read_csv("test_time.csv", col_types= "c") #read in as character
test2<-  read_csv("test_time2.csv")
test2_text<- read_csv("test_time2.csv", col_types= "c") #read in as character


t1$time[1]==ymd_hms("2019-01-01 12:30:05.0005") #TRUE
test1_text$time[1] #"2019-01-01T12:30:05.000499Z" (6 digits but slightly off)
test1$time[1]==ymd_hms("2019-01-01 12:30:05.000499") #TRUE
test2_text$time[1] #"2019-01-01T12:30:05.00Z" (Only written 2 decimal places)
test2$time[1]==ymd_hms("2019-01-01 12:30:05.00") #TRUE
```

The read_lines_raw is not handling correctly the CR and LF characters in UTF-16LE files.

```r
raw_crlf = iconv("ab\r\n12",from="UTF-8",to="UTF-16LE", toRaw = TRUE)[[1]]
raw_cr = iconv("ab\r12",from="UTF-8",to="UTF-16LE", toRaw = TRUE)[[1]]
raw_lf = iconv("ab\n12",from="UTF-8",to="UTF-16LE", toRaw = TRUE)[[1]]

readr::read_lines_raw(raw_crlf)
#> [[1]]
#> [1] 61 00 62 00
#> 
#> [[2]]
#> [1] 00
#> 
#> [[3]]
#> [1] 00 31 00 32 00
readr::read_lines_raw(raw_cr)
#> [[1]]
#> [1] 61 00 62 00
#> 
#> [[2]]
#> [1] 00 31 00 32 00
readr::read_lines_raw(raw_lf)
#> [[1]]
#> [1] 61 00 62 00
#> 
#> [[2]]
#> [1] 00 31 00 32 00
```

`writeLines()` shows the example data in a more representative way than `cat()`

``` r
library(readr)
fwf_sample <- readr_example("fwf-sample.txt")

cat(read_lines(fwf_sample))
#> John Smith          WA        418-Y11-4111 Mary Hartford       CA        319-Z19-4341 Evan Nolan          IL        219-532-c301

writeLines(read_lines(fwf_sample))
#> John Smith          WA        418-Y11-4111
#> Mary Hartford       CA        319-Z19-4341
#> Evan Nolan          IL        219-532-c301
```

<sup>Created on 2019-10-24 by the [reprex package](https://reprex.tidyverse.org) (v0.3.0)</sup>

Issue #163 was supposed to have fixed the problem of reading compressed files from urls. However, I am getting an error when trying to read files from this ftp server. The result is not the .csv file inside the .zip file. When I download the .zip first, and then `read_csv()` from local folder, it works.

```r
read_csv("ftp://ftp.rmpc.org/pub/data/CS041_ODFW_2017.zip")
```
Hi,

I was wondering if there is a way to access password protected links(of the data file) through readr? I used to use http links to read in tsv files via read_tsv function earlier until my institution made those links password protected and hence all my scripts broke. This feature if available or could be made possible will be very helpful.

Thanks,

Ashu