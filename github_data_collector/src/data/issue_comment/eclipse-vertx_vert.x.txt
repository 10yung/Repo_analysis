

### Version

3.8

### Context

Currently you have the choice between callback hell, ending with complex business code at tabulation 8192, or to rely on complex future headaches.

In both cases, you may/will end up, even with best efforts, with un-readable and/or un-maintainable complex business code.

RUST (https://rust-lang.github.io/async-book/01_getting_started/04_async_await_primer.html) and python 3 (https://docs.python.org/3/library/asyncio-task.html) have support for async / await syntax, which basically hides this boring level of useless complexity.

Some JS stuff try to mitigate also this problem (https://github.com/pmlopes/vertx3-nashorn.next/blob/master/examples/async-await/src/server.js).

I assume vertx, java server side, critically need this.

I didnt find any clues about this, so i open this bug. Feel free to do what you want with it, but i assume bypassing this may be a very bad idea for vertx.

### Do you have a reproducer?

Refer to some samples, like https://stackoverflow.com/questions/49799865/how-to-refactor-chain-of-asynchronous-calls-in-vertx-to-avoid-the-callback-hell


* Link to github project/gist

### Steps to reproduce

Use vertx.

### Extra

Vertx 3.8 and before.

Hi

Is it possible to extend `HttpServerRequest.getParam` method so it takes default value for request parameter and returns it if parameter is not there. I'm happy to create a PR for that.

Thanks,
Tim
Hello 

We get rare NPE's on high load in Http1xClientConnection
Vert.x: 3.8.4
Stacktrace:
```
java.lang.NullPointerException: null
	at io.vertx.core.http.impl.Http1xClientConnection$StreamImpl.access$2000(Http1xClientConnection.java:237)
	at io.vertx.core.http.impl.Http1xClientConnection.handleResponseEnd(Http1xClientConnection.java:628)
	at io.vertx.core.http.impl.Http1xClientConnection.handleHttpMessage(Http1xClientConnection.java:584)
	at io.vertx.core.http.impl.Http1xClientConnection.handleMessage(Http1xClientConnection.java:566)
	at io.vertx.core.impl.ContextImpl.executeTask(ContextImpl.java:369)
	at io.vertx.core.impl.EventLoopContext.execute(EventLoopContext.java:43)
	at io.vertx.core.impl.ContextImpl.executeFromIO(ContextImpl.java:232)
	at io.vertx.core.net.impl.VertxHandler.channelRead(VertxHandler.java:173)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352)
	at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:438)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:328)
	at io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:387)
	at io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:358)
	at io.netty.handler.codec.http.HttpClientCodec$Decoder.channelInactive(HttpClientCodec.java:285)
	at io.netty.channel.CombinedChannelDuplexHandler.channelInactive(CombinedChannelDuplexHandler.java:223)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:257)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:243)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:236)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1417)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:257)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:243)
	at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:913)
	at io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:819)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:510)
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:413)
	at io.netty.util.concurrent.SingleThreadEventExecutor$6.run(SingleThreadEventExecutor.java:1050)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:834)
```
Return default value instead of null if query param is not present. 
 - removed unused imports

Improvement based on: [vertx-web#1473](https://github.com/vert-x3/vertx-web/issues/1473)


Currently the options to build Vertx provide a `clustered` boolean with the following behavior:

- calling `vertx(options)` with `clustered` set to `false` builds a non clustered instance
- calling `vertx(options)` with `clustered` set to `true` throw an `IllegalArgumentException`
- calling `clusteredVertx(options)` with `clustered` set to `true` builds a clustered instance
- calling `clusteredVertx(options)` with `clustered` set to `false` does set `clustered` to `true` and then builds a clustered instance

Given this behaviour, the usefulness of this setting seems very low and instead it could be removed in Vert.x 4 with no actual impact other than not throwing an `IllegalStateException` when `clustered` is set to `true` and the `vertx(options)` method is called.

When I use `CompositeFuture#all(List<Future> futures)`, it's really boring to add `@SuppressWarnings("rawtypes")`.

How about to change the argument to `List<Future<?>> futures`, just like that of `CompositeFutureImpl`'s?

And same to `CompositeFuture#any` and `CompositeFuture#join`.

Sorry my English is not good enough.
vertx-core : bodyHandler.
It will be called twice.
for a sample GET request called twice.
for a sample submited form , called twice.
As a result, this handle will run two thousand times if a thousand people request the page.
And I did the following, is that right ???
var buff = Buffer.buffer();
request.handler(buffer->{
buff.appendBuffer(buffer);
request.endHandler(/*TODO*/)//using endHandler inside handler
});

When attempting to bind more than one HTTP server to (different) UNIX domain sockets, only the first socket will be opened and all none of the others will be opened.

Some sample code to replicate this (in Kotlin, but this exact code isn't particularly important):

```kotlin
val f1 = File("/tmp/f1.socket")
val f2 = File("/tmp/f2.socket")
val opts = VertxOptions()
opts.preferNativeTransport = true
val vertx = Vertx.vertx(opts)

f1.delete()
f2.delete()

val sem = java.util.concurrent.Semaphore(2)
sem.acquire(2)

vertx.createHttpServer().apply {
    requestHandler { }
    listen(SocketAddress.domainSocketAddress(f1.canonicalPath)) {
        println("S1 started: " + it.succeeded())
        sem.release()
    }
}

vertx.createHttpServer().apply {
    requestHandler { }
    listen(SocketAddress.domainSocketAddress(f2.canonicalPath)) {
        println("S2 started: " + it.succeeded())
        sem.release()
    }
}

// Wait for both servers to start
sem.acquire(2)

println("F1 exists: " + f1.exists())
println("F2 exists: " + f2.exists())

exitProcess(0)
```

Which outputs:

```
S1 started: true
S2 started: true
F1 exists: true
F2 exists: false
```

It seems (having run through it with a debugger) that the following fragment from `HttpServerImpl` is at fault:

```java
    sslHelper.setApplicationProtocols(applicationProtocols);
    Map<ServerID, HttpServerImpl> sharedHttpServers = vertx.sharedHttpServers();
    synchronized (sharedHttpServers) {
      this.actualPort = port; // Will be updated on bind for a wildcard port
      id = new ServerID(port, host);
      HttpServerImpl shared = sharedHttpServers.get(id); // Caching issue here
      if (shared == null || port == 0) {
        serverChannelGroup = new DefaultChannelGroup("vertx-acceptor-channels", GlobalEventExecutor.INSTANCE);
        ServerBootstrap bootstrap = new ServerBootstrap();
```

The servers are cached by the unique combination of their port and hostname. Since domain sockets always have a host of `localhost` and a port of `-1`, only the first socket be created and all subsequent domain sockets will be found in the map.

I'm on Vertx 3.8.4, and while I haven't tried building from source it appears the relevant code is identical in master.

```
io.vertx.core.eventbus.impl.clustered.ClusteredEventBus
SEVERE: Failed to remove sub
com.hazelcast.core.HazelcastInstanceNotActiveException: Hazelcast instance is not active!
	at com.hazelcast.spi.AbstractDistributedObject.throwNotActiveException(AbstractDistributedObject.java:105)
	at com.hazelcast.spi.AbstractDistributedObject.lifecycleCheck(AbstractDistributedObject.java:100)
	at com.hazelcast.spi.AbstractDistributedObject.getNodeEngine(AbstractDistributedObject.java:94)
	at com.hazelcast.multimap.impl.ObjectMultiMapProxy.remove(ObjectMultiMapProxy.java:128)
	at io.vertx.spi.cluster.hazelcast.impl.HazelcastAsyncMultiMap.lambda$remove$8(HazelcastAsyncMultiMap.java:169)
	at io.vertx.core.impl.ContextImpl.lambda$executeBlocking$2(ContextImpl.java:316)
	at io.vertx.core.impl.TaskQueue.run(TaskQueue.java:76)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:834)
```

It seems that the reason is that ```EventBusImpl#unregisterAll``` calls `HandlerRegistration#unregister` with null `doneHandler`. This leads to the cluster leaving before the EventBus is shutted down.
Signed-off-by: Paulo Lopes <pmlopes@gmail.com>

This PR is a proposal to support same site policy for cookies (server side only). There is no parsing involved (as that is a feature only applicable to browsers).

The Same Site policy is supported by all major browsers and a security feature to prevent cross site request forgery.

Although this isn't present in netty (yet) discussions seem to show that this will only land on the 5.x branch which means vertx can't offer the current owasp recommendation on cookies https://www.owasp.org/index.php/SameSite 

As a workaround this PR allows users to build secure cookie based applications and once support lands upstream we can easily drop this and fully support it.

Also note that the chrome team will be "strict" with this flag:

https://www.chromium.org/updates/same-site
