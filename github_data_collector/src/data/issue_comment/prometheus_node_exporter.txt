This fills in some missing freebsd diskstats. Tested on freebsd 11.3 (stable).
This requires root, so it shouldn't be used.

This closes #1246
/etc/systemd/system/node-exporter.service

[Unit]
Description=Node exporter
After=network-online.target

[Service]
User=root
#Restart=on-failure
Restart=always
RestartSec=15
#Change this line if you download the 
#Prometheus on different path user
ExecStart=/node_exporter/node_exporter

[Install]
WantedBy=multi-user.target


systemctl status node-exporter.service

● node-exporter.service - Node exporter
   Loaded: loaded (/etc/systemd/system/node-exporter.service; disabled; vendor preset: enabled)
   Active: activating (auto-restart) (Result: exit-code) since Wed 2020-01-15 10:24:59 IST; 6s ago
  Process: 17904 ExecStart=/node_exporter/node_exporter (code=exited, status=203/EXEC)
 Main PID: 17904 (code=exited, status=203/EXEC)


### Host operating system: output of `uname -a`
```
[le@server]: ~/src/util-compose>$ uname -a
Linux server 5.4.8-arch1-1 #1 SMP PREEMPT Sat, 04 Jan 2020 23:46:18 +0000 x86_64 GNU/Linux
```
### node_exporter version: output of `node_exporter --version`
```
/ $ node_exporter --version
node_exporter, version 0.18.1 (branch: HEAD, revision: 3db77732e925c08f675d7404a8c46466b2ece83e)
  build user:       root@b50852a1acba
  build date:       20190604-16:41:18
  go version:       go1.12.5
```

### node_exporter command line flags
```
  nodeexporter:
    image: prom/node-exporter
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
      - /mnt/data:/mnt/data:ro
      - /mnt/data/appdata:/mnt/data/appdata:ro
      - /tmp/textcollector:/tmp/textcollector
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc|var/lib/docker/.+)($$|/)'
      - '--collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs|tmpfs|nsfs)$$'
      - '--collector.textfile.directory=/tmp/textcollector'
      - '--collector.systemd'
    restart: always
    expose:
      - 9100
```

### Are you running node_exporter in Docker?
Yes

### What did you do that produced an error?
Check exported metrics

### What did you expect to see?
Proper reporting on root filesystem bytes

### What did you see instead?
same values for `/` than for `/mnt/data/appdata`

```
node_filesystem_avail_bytes{device="/dev/mapper/ae54c1a7-5d17-45de-a99e-26920e5f442f",fstype="ext4",mountpoint="/mnt/data/appdata"} 1.13407520768e+11
node_filesystem_avail_bytes{device="/dev/mapper/vg0-data",fstype="ext4",mountpoint="/mnt/data"} 8.686425993216e+12
node_filesystem_avail_bytes{device="/dev/sda3",fstype="ext4",mountpoint="/"} 1.13407520768e+11
```

I remember, I used to have a `/rootfs` mountpoint instead of `/`, but that vanished long time ago.

Can reproduce this on 2 machines.
Reuse the Go-only implementation already in place for FreeBSD (#385) on
Darwin, DragonflyBSD, NetBSD and OpenBSD.

Tested on all affected platforms.
<!--
	Please note: GitHub issues should only be used for feature requests and
	bug reports. For general usage/help/discussions, please refer to one of:

	- #prometheus on freenode
	- the Prometheus Users list: https://groups.google.com/forum/#!forum/prometheus-users

	Before filing a bug report, note that running node_exporter in Docker is
	not recommended, for the reasons detailed in the README:

	https://github.com/prometheus/node_exporter#using-docker

	Finally, also note that node_exporter is focused on *NIX kernels, and the
	WMI exporter should be used instead on Windows.

	For bug reports, please fill out the below fields and provide as much detail
	as possible about your issue.  For feature requests, you may omit the
	following template.
-->
### Host operating system: output of `uname -a`

Linux hostname 4.4.0-116-generic #140-Ubuntu SMP Mon Feb 12 21:23:04 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

### node_exporter version: output of `node_exporter --version`
node_exporter, version 0.15.2+ds (branch: debian/sid, revision: 0.15.2+ds-1)
  build user:       pkg-go-maintainers@lists.alioth.debian.org
  build date:       20171214-15:26:08
  go version:       go1.9.2

### node_exporter command line flags
--collector.mountstats

### Are you running node_exporter in Docker?
No

### What did you do that produced an error?
Nothing special, just scrape node exporter

### What did you expect to see?
Mountstats metrics

### What did you see instead?

No mountstats metrics. In logs:
```
Jan 07 14:15:10 hostname prometheus-node-exporter[22123]: time="2020-01-07T14:15:10Z" level=error msg="ERROR: mountstats collector failed after 0.005747s: failed to parse mountstats: not enough information for NFS stats: [obfuscated...] source="collector.go:123"
```

This is caused by [following line](https://github.com/prometheus/procfs/blob/314d69e47b2ad51b7f9fc48aed1b706fab8de26d/mountstats.go#L340), because my mountstats contain following line (sorry I had to obfuscate it, this is not real output, but it is multiline):

```
        impl_id:        name='XXX 10.7-STABLE #0 r325575+d8sf789df(HEAD): Fri May 5 12:42:01 EDT 2018
    user@hostname:/path/to/something
',domain='something.com',date='1789840000,0'
        caps:   ...
``` 

I believe, that this is caused, because there are no 2 fields in some line and procfs parser just fails. If I remove replace the ` return nil, fmt.Errorf("not enough information for NFS stats: %v", ss)` by `continue`, everything works as expected.

Thanks for any help.
This closes #770

Only textfile and time collector but that's a start.
Currently Node Exporter has a metric called: node_uname_info which of course exposes uname info. While this is nice, it does not help if you are running different OSes which could have similar uname info.

Therefore it would be nice to include a similar node_os_release_info metric which would provide information regarding the OS release/version of the node.

This information can be easily gotten from files like:
/etc/release
/etc/os-release
/etc/version

While this functionality could be added through a textfile collector script, I think it would make sense to add it to the base exporter as uname is already supported here.



<!--
	Please note: GitHub issues should only be used for feature requests and
	bug reports. For general usage/help/discussions, please refer to one of:

	- #prometheus on freenode
	- the Prometheus Users list: https://groups.google.com/forum/#!forum/prometheus-users

	Before filing a bug report, note that running node_exporter in Docker is
	not recommended, for the reasons detailed in the README:

	https://github.com/prometheus/node_exporter#using-docker

	Finally, also note that node_exporter is focused on *NIX kernels, and the
	WMI exporter should be used instead on Windows.

	For bug reports, please fill out the below fields and provide as much detail
	as possible about your issue.  For feature requests, you may omit the
	following template.
-->
### Host operating system: output of `uname -a`
Linux lga-kubnode470 4.4.205-1.el7.elrepo.x86_64 #1 SMP Fri Nov 29 10:10:01 EST 2019 x86_64 x86_64 x86_64 GNU/Linux

### node_exporter version: output of `node_exporter --version`
<!-- If building from source, run `make` first. -->
node_exporter, version 0.18.0 (branch: HEAD, revision: f97f01c46cfde2ff97b5539b7964f3044c04947b)
  build user:       root@77cb1854c0b0
  build date:       20190509-23:12:18
  go version:       go1.12.5

### node_exporter command line flags
<!-- Please list all of the command line flags -->
* --path.procfs=/host/proc
* --path.sysfs=/host/sys
* --collector.textfile.directory=/var/log/pulsepoint/prometheus

### Are you running node_exporter in Docker?
<!-- Please note the warning above. -->
Node exporter is running in docker in kubernetes cluster:
        image: "prom/node-exporter:v0.18.0"

Node exporter daemonset manifest was created from stable helm chart:
There are 3 volumes:
name: proc
        hostPath: /proc -> container readonly: /host/proc
        hostPath: /sys -> container readonly: /host/sys
        hostPath: /var/log/pulsepoint/prometheus -> container: /var/log/pulsepoint/prometheus (used for text files metrics)
No kubernetes security context
hostNetwork: true
hostPID: true

### What did you do that produced an error?
We are running k8s cluster with [kube-router](https://github.com/cloudnativelabs/kube-router) based networking. So we are heavily using ipvs.
At some point we started to observe significant number of errors from all node-collectors about duplicate ipvs metrics (starting and heaving max value at 7:00 AM and then fading within an hour):
```
time="2019-12-19T12:25:10Z" level=error msg="
error gathering metrics: 324 error(s) occurred:
* [from Gatherer #2] collected metric "node_ipvs_backend_connections_active" { label:<name:"local_address" value:"10.203.128.184" > label:<name:"local_port" value:"9001" > label:<name:"proto" value:"TCP" > label:<name:"remote_address" value:"10.204.57.184" > label:<name:"remote_port" value:"9001" > gauge:<value:0 > } was collected before with the same name and label values
...
" source="log.go:172"
```
And we have many of that type for node_ipvs_backend_weight, node_ipvs_backend_connections_active, node_ipvs_backend_connections_inactive
We have scrape interval 15 seconds and 500 nodes cluster.

### What did you expect to see?
No errors

### What did you see instead?
Bunch of errors starting from some specific time with the same pattern: start at 7:00 AM and then fading within an hour.

The degree to which a disk has extra work it can't service. This value
should ideally not exceed 1. Beyond this value would indicate that work
is getting queued up. In cloud environments this is also likely to be
amplified by providers throttling operations if I/O exceeds allocated
IOPs.

'diskAlertSelector' added to support adjustment of alert scope.
    
Signed-off-by: trotttrotttrott <trott@odaacabeef.com>

~~By default this is only applied to "sda" devices as they're typically
the root volume of a Linux machine. Root volumes are targeted in
particular because often the cause for saturation is a single workload.
In a k8s context, this will likely cause issues for co-located
workloads. A common solution is to assign the saturating workload to a
PVC so its disk usage is isolated and the PVC disk can be tuned
accordingly.~~

~~Signed-off-by: trotttrotttrott <chris.trott@grafana.com>~~

@SuperQ @discordianfish wanted to put this out there as it's something I'm experimenting with alerting on. Seemed like it would be useful for other folks using the mixin.