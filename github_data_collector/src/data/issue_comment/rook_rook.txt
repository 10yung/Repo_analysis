<!-- Please take a look at our [Contributing](https://rook.io/docs/rook/master/development-flow.html)
documentation before submitting a Pull Request!
Thank you for contributing to Rook! -->

**Description of your changes:**
Updates and clarifications to the Rook governance including:
- Add a steering committee to oversee the project starting with three members: @jbw976 @dyusupov @travisn 
- Rename approvers to maintainers, who have push access to the repo
- Clarify process for adding and removing maintainers
- Move @bassam to emeritus maintainer
- Add reviewers for YugabyteDB
- Remove inactive reviewers
- Other clarifications for maintainer and reviewer responsibilities

Per requirements for current Rook governance, this must be approved by majority of current Rook maintainers before this is merged or the changes take effect.

**Checklist:**

- [ ] Reviewed the developer guide on [Submitting a Pull Request](https://rook.io/docs/rook/master/development-flow.html#submitting-a-pull-request)
- [ ] Documentation has been updated, if necessary.
- [ ] Unit tests have been added, if necessary.
- [ ] Integration tests have been added, if necessary.
- [ ] Pending release notes updated with breaking and/or notable changes, if necessary.
- [ ] Upgrade from previous release is tested and upgrade user guide is updated, if necessary.
- [ ] Code generation (`make codegen`) has been run to update object specifications, if necessary.
- [ ] Comments have been added or updated based on the standards set in [CONTRIBUTING.md](https://github.com/rook/rook/blob/master/CONTRIBUTING.md#comments)
- [ ] Add the flag for skipping the CI if this PR does not require a build. See [here](https://github.com/rook/rook/blob/master/INSTALL.md#skip-ci) for more details.

[skip ci]
<!-- **Are you in the right place?**
1. For issues or feature requests, please create an issue in this repository.
2. For general technical and non-technical questions, we are happy to help you on our [Rook.io Slack](https://slack.rook.io/).
3. Did you already search the existing open issues for anything similar? -->

**Is this a bug report or feature request?**
* Feature Request

**What should the feature do:**
Allow specification of `min_size` for Ceph pools (RBD and CephFS).

**What is use case behind this feature:**
In our Rook operated Ceph cluster (4 worker/storage nodes, 2 workers per datacenter, `failureDomain` set to `zone`, node labels set accordingly) with `.spec.replicated.size` set to `2` in `CephBlockPool` we see `min_size` automatically set to `2` on pools as well. This prevents I/O when one datacenter is lost although data is available. Manually setting `min_size` to `1` recovers from this situation and allows data access even when one site is lost. See https://docs.ceph.com/docs/jewel/rados/operations/pools/#set-the-number-of-object-replicas and https://docs.ceph.com/docs/jewel/rados/operations/pools/#size.

**Environment**:
<!-- Specific environment information that helps with the feature request -->
```
[root@rook-ceph-tools-595594bf67-hp5pv /]# ceph osd tree
ID  CLASS WEIGHT  TYPE NAME          STATUS REWEIGHT PRI-AFF 
 -1       0.09357 root default                               
 -4       0.04678     zone dc1                               
-11       0.02339         host n0204                         
  2   ssd 0.02339             osd.2      up  1.00000 1.00000 
 -3       0.02339         host n0205                         
  1   ssd 0.02339             osd.1      up  1.00000 1.00000 
 -8       0.04678     zone dc2                               
 -7       0.02339         host n0206                         
  0   ssd 0.02339             osd.0      up  1.00000 1.00000 
-13       0.02339         host n0207                         
  3   ssd 0.02339             osd.3      up  1.00000 1.00000 
```

CephBlockPool:

```yaml
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: rbdpool
  namespace: rook-ceph
spec:
  crushRoot: ""
  deviceClass: ""
  erasureCoded:
    algorithm: ""
    codingChunks: 0
    dataChunks: 0
  failureDomain: zone
  replicated:
    min_size: 1
    size: 2
```

```
$ kubectl exec -ti -n rook-ceph rook-ceph-tools-595594bf67-hp5pv bash
# ceph osd pool get rbdpool min_size
min_size: 2
# ceph osd pool get rbdpool size    
size: 2
# ceph -v
ceph version 14.2.6 (f0aa067ac7a02ee46ea48aa26c6e298b5ea272e9) nautilus (stable)
```

**Is this a bug report or feature request?**
<!-- Remove only one -->
* Bug Report

  
**Bug Report**

Rook rewrites configs for external ceph cluster repeatedly every few seconds.

**Expected behavior:**

Only rewrite configs when cluster changes.

**Deviation from expected behavior:**
```
2020-01-17 14:03:25.162995 I | op-mon: ClusterInfo is now Empty, refilling it from status.MonMap.Mons
2020-01-17 14:03:25.163036 I | op-mon: new external mon ceph5 found: 10.1.1.5:6789, adding it
2020-01-17 14:03:25.163052 I | op-mon: new external mon ceph6 found: 10.1.1.6:6789, adding it
2020-01-17 14:03:25.163064 I | op-mon: new external mon ceph7 found: 10.1.1.7:6789, adding it
2020-01-17 14:03:25.173956 I | op-mon: saved mon endpoints to config map map[data:ceph5=10.1.1.5:6789,ceph6=10.1.1.6:6789,ceph7=10.1.1.7:6789 maxMonId:-1 mapping:{"node":{}} csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.1.1.5:6789","10.1.1.6:6789","10.1.1.7:6789"]}]]
2020-01-17 14:03:25.180472 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2020-01-17 14:03:25.180747 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph
2020-01-17 14:04:11.090181 I | op-mon: ClusterInfo is now Empty, refilling it from status.MonMap.Mons
2020-01-17 14:04:11.090223 I | op-mon: new external mon ceph5 found: 10.1.1.5:6789, adding it
2020-01-17 14:04:11.090238 I | op-mon: new external mon ceph6 found: 10.1.1.6:6789, adding it
2020-01-17 14:04:11.090249 I | op-mon: new external mon ceph7 found: 10.1.1.7:6789, adding it
2020-01-17 14:04:11.100412 I | op-mon: saved mon endpoints to config map map[maxMonId:-1 mapping:{"node":{}} csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.1.1.5:6789","10.1.1.6:6789","10.1.1.7:6789"]}] data:ceph5=10.1.1.5:6789,ceph6=10.1.1.6:6789,ceph7=10.1.1.7:6789]
2020-01-17 14:04:11.106600 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2020-01-17 14:04:11.106910 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph
2020-01-17 14:04:56.987005 I | op-mon: ClusterInfo is now Empty, refilling it from status.MonMap.Mons
2020-01-17 14:04:56.987059 I | op-mon: new external mon ceph5 found: 10.1.1.5:6789, adding it
2020-01-17 14:04:56.987075 I | op-mon: new external mon ceph6 found: 10.1.1.6:6789, adding it
2020-01-17 14:04:56.987087 I | op-mon: new external mon ceph7 found: 10.1.1.7:6789, adding it
2020-01-17 14:04:56.997593 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.1.1.5:6789","10.1.1.6:6789","10.1.1.7:6789"]}] data:ceph5=10.1.1.5:6789,ceph6=10.1.1.6:6789,ceph7=10.1.1.7:6789 maxMonId:-1 mapping:{"node":{}}]
2020-01-17 14:04:57.003829 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2020-01-17 14:04:57.004133 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph
2020-01-17 14:05:42.858947 I | op-mon: ClusterInfo is now Empty, refilling it from status.MonMap.Mons
2020-01-17 14:05:42.859009 I | op-mon: new external mon ceph5 found: 10.1.1.5:6789, adding it
2020-01-17 14:05:42.859028 I | op-mon: new external mon ceph6 found: 10.1.1.6:6789, adding it
2020-01-17 14:05:42.859044 I | op-mon: new external mon ceph7 found: 10.1.1.7:6789, adding it
2020-01-17 14:05:42.871217 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.1.1.5:6789","10.1.1.6:6789","10.1.1.7:6789"]}] data:ceph5=10.1.1.5:6789,ceph6=10.1.1.6:6789,ceph7=10.1.1.7:6789 maxMonId:-1 mapping:{"node":{}}]
2020-01-17 14:05:42.877706 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2020-01-17 14:05:42.877999 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph
2020-01-17 14:06:28.772799 I | op-mon: ClusterInfo is now Empty, refilling it from status.MonMap.Mons
2020-01-17 14:06:28.772840 I | op-mon: new external mon ceph5 found: 10.1.1.5:6789, adding it
2020-01-17 14:06:28.772855 I | op-mon: new external mon ceph6 found: 10.1.1.6:6789, adding it
2020-01-17 14:06:28.772868 I | op-mon: new external mon ceph7 found: 10.1.1.7:6789, adding it
2020-01-17 14:06:28.784259 I | op-mon: saved mon endpoints to config map map[mapping:{"node":{}} csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.1.1.7:6789","10.1.1.5:6789","10.1.1.6:6789"]}] data:ceph5=10.1.1.5:6789,ceph6=10.1.1.6:6789,ceph7=10.1.1.7:6789 maxMonId:-1]
2020-01-17 14:06:28.789822 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2020-01-17 14:06:28.790111 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph
```

**How to reproduce it (minimal and precise):**
<!-- Please let us know any circumstances for reproduction of your bug. -->

Use rook 1.2.1 with a external cluster with 3 mons.
```
apiVersion: v1
kind: Namespace
metadata:
  name: {{ rook_namespace }}
---
apiVersion: v1
kind: Secret
metadata:
  name: rook-ceph-mon
  namespace: {{ rook_namespace }}
type: Opaque
data:
  cluster-name: {{ rook_clustername | b64encode }}
  fsid: {{ rook_external_fsid | b64encode }}
  admin-secret: {{ rook_external_admin_secret | b64encode }}
  mon-secret: {{ rook_external_mon_secret | b64encode }}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: rook-ceph-mon-endpoints
  namespace: {{ rook_namespace }}
data:
  data: "{{ rook_external_ceph_mon_data }}"
  mapping: "{}"
  maxMonId: "2"
---
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: {{ rook_clustername }}
  namespace: {{ rook_namespace }}
spec:
  external:
    enable: true
  dataDirHostPath: /var/lib/rook
  # providing an image is optional, do this if you want to create other CRs (rgw, mds, nfs)
  cephVersion:
    image: ceph/ceph:v14.2.6 # MUST match external cluster version
```
**Environment**:
* OS: CentOS 7
* Kernel : 3.10.0-1062.9.1.el7.x86_64
* Cloud provider or hardware configuration: Bare Metal
* Rook version: v1.2.1
* Storage backend version: ceph 14.2.6
* Kubernetes version: v1.17.0
* Kubernetes cluster type: Bare Metal
* Storage backend status: HEALTH_OK

<!-- **Are you in the right place?**
1. For issues or feature requests, please create an issue in this repository.
2. For general technical and non-technical questions, we are happy to help you on our [Rook.io Slack](https://slack.rook.io/).
3. Did you already search the existing open issues for anything similar? -->

**Is this a bug report or feature request?**
* Feature Request

**What should the feature do:**

It would be a must have to be able to set (optionally) on the CephCluster CRD two fields:
- public_network
- cluster_network

Those two configs fields would represent the same one as public network and cluster network in the ceph config global section (that is to say network adresses in the CIDR notation).

Rook could thanks to that try to discover the interface ip addresses to use for each OSD, checking the ip is contained in one of those CIDRs.

Today I am doing something like this using rook-config-override configmap and running an ansible pass outside rook logic.

Generated config looks like:

```
kubectl get cm rook-config-override -n rook-ceph -o yaml
apiVersion: v1
data:
  config: |
    [global]
    public network = 10.205.0.0/16
    cluster network = 10.206.0.0/16
    [osd.1]
    cluster addr = 10.206.0.1
    public ip = 10.205.0.1
    [osd.2]
    cluster addr = 10.206.0.1
    public ip = 10.205.0.1
    [osd.3]
    cluster addr = 10.206.0.1
    public ip = 10.205.0.1
```

The ansible playbook fragment I am currently using to do this looks like the following one:

```
- name: Ceph tasks
  hosts: "{{ groups['kube-master'][0] }}"
  become: true
  tasks:
    - name: Get nodes OSDs
      shell: "kubectl get pods -n rook-ceph --field-selector spec.nodeName={{ item.1 }} -l app=rook-ceph-osd -o=jsonpath='{range .items[*]}{\"[osd.\"}{.metadata.labels.ceph-osd-id}{\"]\\ncluster addr = {{ hostvars[item.1].netplan_configuration.network.ethernets.eno50.addresses[0] | ipaddr('address') }}\\npublic ip = {{ hostvars[item.1].ansible_host }}\\n\"}{end}'"
      changed_when: False
      register: nodes_osds
      with_indexed_items: "{{ groups['all'] }}"
    - k8s_raw:
        state: present
        resource_definition: |
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: rook-config-override
            namespace: "{{ item }}"
          data:
            config: |
              [global]
              public network = {{ ceph_public_network }}
              cluster network = {{ ceph_cluster_network }}
          {{ nodes_osds.results | map(attribute='stdout') | map('regex_replace', '(.*)', '    \1') | join('\n') | regex_replace('\\n', "\n") }}
        kubeconfig: /etc/kubernetes/admin.conf
        verify_ssl: no
      with_items:
        - "rook-ceph"
```

**What is use case behind this feature:**

It is a good practice with ceph to use two distinct networks, one for ceph client exchanges with the OSDs, another one for data sync between OSDs.

**Environment**:
<!-- Specific environment information that helps with the feature request -->

<!-- **Are you in the right place?**
1. For issues or feature requests, please create an issue in this repository.
2. For general technical and non-technical questions, we are happy to help you on our [Rook.io Slack](https://slack.rook.io/).
3. Did you already search the existing open issues for anything similar? -->

**Is this a bug report or feature request?**
* Bug Report

**Deviation from expected behavior:**
After the first monitor starts, the  no further monitors join the quorum and die one after another.

**Expected behavior:**
Three monitors form quorum

**How to reproduce it (minimal and precise):**
<!-- Please let us know any circumstances for reproduction of your bug. -->
Kubernetes v1.16.3 deployed using kubespray v2.12.0 on ubuntu 18.04.3 LTS (Bionic Beaver)
Using `master` of rook and any version of ceph (tested with v13, v14 and master)

**File(s) to submit**:
**cluster.yaml**
```
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: ceph/daemon-base:latest-master
    allowUnsupported: true
  dataDirHostPath: /var/lib/rook
  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  mon:
    count: 3
    allowMultiplePerNode: false
  dashboard:
    enabled: true
    ssl: true
  monitoring:
    enabled: false
    rulesNamespace: rook-ceph
  network:
    hostNetwork: false
  rbdMirroring:
    workers: 0
  crashCollector:
    disable: true
  annotations:
  resources:
  removeOSDsIfOutAndSafeToRemove: false
  storage: # cluster level storage configuration and selection
    useAllNodes: true
    useAllDevices: true
    config:
  disruptionManagement:
    managePodBudgets: false
    osdMaintenanceTimeout: 30
    manageMachineDisruptionBudgets: false
    machineDisruptionBudgetNamespace: openshift-machine-api
```

**Operator Logs**
```
2020-01-17 08:57:41.607282 I | rookcmd: starting Rook v1.1.0-beta.0.769.ga42d822 with arguments '/usr/local/bin/rook ceph operator'
2020-01-17 08:57:41.607325 I | rookcmd: flag values: --add_dir_header=false, --alsologtostderr=false, --csi-attacher-image=quay.io/k8scsi/csi-attacher:v1.2.0, --csi-ceph-image=quay.io/cephcsi/cephcsi:v1.2.2, --csi-cephfs-plugin-template-path=/etc/ceph-csi/cephfs/csi-cephfsplugin.yaml, --csi-cephfs-provisioner-dep-template-path=/etc/ceph-csi/cephfs/csi-cephfsplugin-provisioner-dep.yaml, --csi-cephfs-provisioner-sts-template-path=/etc/ceph-csi/cephfs/csi-cephfsplugin-provisioner-sts.yaml, --csi-driver-name-prefix=, --csi-enable-cephfs=true, --csi-enable-grpc-metrics=true, --csi-enable-rbd=true, --csi-kubelet-dir-path=/var/lib/kubelet, --csi-provisioner-image=quay.io/k8scsi/csi-provisioner:v1.4.0, --csi-rbd-plugin-template-path=/etc/ceph-csi/rbd/csi-rbdplugin.yaml, --csi-rbd-provisioner-dep-template-path=/etc/ceph-csi/rbd/csi-rbdplugin-provisioner-dep.yaml, --csi-rbd-provisioner-sts-template-path=/etc/ceph-csi/rbd/csi-rbdplugin-provisioner-sts.yaml, --csi-registrar-image=quay.io/k8scsi/csi-node-driver-registrar:v1.1.0, --csi-snapshotter-image=quay.io/k8scsi/csi-snapshotter:v1.2.2, --enable-discovery-daemon=true, --enable-flex-driver=false, --enable-machine-disruption-budget=false, --help=false, --kubeconfig=, --log-flush-frequency=5s, --log-level=INFO, --log_backtrace_at=:0, --log_dir=, --log_file=, --log_file_max_size=1800, --logtostderr=true, --master=, --mon-healthcheck-interval=45s, --mon-out-timeout=10m0s, --operator-image=, --service-account=, --skip_headers=false, --skip_log_headers=false, --stderrthreshold=2, --v=0, --vmodule=
2020-01-17 08:57:41.607328 I | cephcmd: starting operator
2020-01-17 08:57:41.642111 I | op-discover: rook-discover daemonset started
2020-01-17 08:57:41.644068 I | operator: rook-provisioner ceph.rook.io/block started using ceph.rook.io flex vendor dir
I0117 08:57:41.644161       6 leaderelection.go:217] attempting to acquire leader lease  rook-ceph/ceph.rook.io-block...
2020-01-17 08:57:41.644333 I | operator: rook-provisioner rook.io/block started using rook.io flex vendor dir
2020-01-17 08:57:41.644339 I | operator: Watching all namespaces for cluster CRDs
2020-01-17 08:57:41.644343 I | op-cluster: start watching clusters in all namespaces
I0117 08:57:41.644356       6 leaderelection.go:217] attempting to acquire leader lease  rook-ceph/rook.io-block...
2020-01-17 08:57:41.644357 I | op-cluster: Enabling hotplug orchestration: ROOK_DISABLE_DEVICE_HOTPLUG=false
2020-01-17 08:57:41.644446 I | operator: setting up the controller-runtime manager
I0117 08:57:41.656163       6 leaderelection.go:227] successfully acquired lease rook-ceph/ceph.rook.io-block
I0117 08:57:41.656209       6 controller.go:769] Starting provisioner controller ceph.rook.io/block_rook-ceph-operator-678887c8d-bgp75_6b53ea81-3907-11ea-b87e-5222bbc6e924!
I0117 08:57:41.656220       6 event.go:209] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"rook-ceph", Name:"ceph.rook.io-block", UID:"d7b1e8e1-8e17-44be-8591-f192640a16dd", APIVersion:"v1", ResourceVersion:"10984", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' rook-ceph-operator-678887c8d-bgp75_6b53ea81-3907-11ea-b87e-5222bbc6e924 became leader
I0117 08:57:41.659014       6 leaderelection.go:227] successfully acquired lease rook-ceph/rook.io-block
I0117 08:57:41.659081       6 controller.go:769] Starting provisioner controller rook.io/block_rook-ceph-operator-678887c8d-bgp75_6b53f7f1-3907-11ea-b87e-5222bbc6e924!
I0117 08:57:41.659090       6 event.go:209] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"rook-ceph", Name:"rook.io-block", UID:"8aafeffe-ec1a-4492-a734-1a0c5558ab50", APIVersion:"v1", ResourceVersion:"10985", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' rook-ceph-operator-678887c8d-bgp75_6b53f7f1-3907-11ea-b87e-5222bbc6e924 became leader
2020-01-17 08:57:42.054435 I | operator: starting the controller-runtime manager
I0117 08:57:42.256490       6 controller.go:818] Started provisioner controller ceph.rook.io/block_rook-ceph-operator-678887c8d-bgp75_6b53ea81-3907-11ea-b87e-5222bbc6e924!
I0117 08:57:43.059297       6 controller.go:818] Started provisioner controller rook.io/block_rook-ceph-operator-678887c8d-bgp75_6b53f7f1-3907-11ea-b87e-5222bbc6e924!
2020-01-17 08:57:48.739568 I | op-cluster: starting cluster in namespace rook-ceph
2020-01-17 08:57:48.823248 I | ceph-csi: CSIDriver object created for driver "rook-ceph.rbd.csi.ceph.com"
2020-01-17 08:57:48.834916 I | ceph-csi: CSIDriver object created for driver "rook-ceph.cephfs.csi.ceph.com"
2020-01-17 08:57:48.834925 I | operator: successfully started Ceph CSI driver(s)
2020-01-17 08:57:54.835147 I | op-cluster: detecting the ceph image version for image ceph/daemon-base:latest-master...
2020-01-17 08:57:57.732704 I | op-cluster: Detected ceph image version: "15.0.0-9337 octopus"
2020-01-17 08:57:57.732752 W | op-cluster: unsupported ceph version detected: "15.0.0-9337 octopus", pursuing
2020-01-17 08:57:57.734387 E | cephconfig: clusterInfo: <nil>
2020-01-17 08:57:57.734432 I | op-cluster: CephCluster "rook-ceph" status: "Creating".
2020-01-17 08:57:57.742599 I | op-mon: start running mons
2020-01-17 08:57:57.744290 I | exec: Running command: ceph-authtool --create-keyring /var/lib/rook/rook-ceph/mon.keyring --gen-key -n mon. --cap mon 'allow *'
2020-01-17 08:57:57.818240 I | exec: Running command: ceph-authtool --create-keyring /var/lib/rook/rook-ceph/client.admin.keyring --gen-key -n client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mgr 'allow *' --cap mds 'allow'
2020-01-17 08:57:57.851456 I | op-mon: creating mon secrets for a new cluster
2020-01-17 08:57:57.865422 I | op-mon: saved mon endpoints to config map map[maxMonId:-1 mapping:{"node":{}} csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":[]}] data:]
2020-01-17 08:57:58.254621 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2020-01-17 08:57:58.254948 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph
2020-01-17 08:57:59.453976 I | op-mon: targeting the mon count 3
2020-01-17 08:57:59.461363 I | op-mon: sched-mon: created canary deployment rook-ceph-mon-a-canary
2020-01-17 08:58:00.656469 I | op-mon: sched-mon: canary monitor deployment rook-ceph-mon-a-canary scheduled to hans
2020-01-17 08:58:00.656479 I | op-mon: assignmon: mon a assigned to node hans
2020-01-17 08:58:00.662578 I | op-mon: sched-mon: created canary deployment rook-ceph-mon-b-canary
2020-01-17 08:58:01.053970 I | op-mon: sched-mon: canary monitor deployment rook-ceph-mon-b-canary scheduled to ludwig
2020-01-17 08:58:01.054013 I | op-mon: assignmon: mon b assigned to node ludwig
2020-01-17 08:58:01.060969 I | op-mon: sched-mon: created canary deployment rook-ceph-mon-c-canary
2020-01-17 08:58:01.453676 I | op-mon: sched-mon: canary monitor deployment rook-ceph-mon-c-canary scheduled to benni
2020-01-17 08:58:01.453687 I | op-mon: assignmon: mon c assigned to node benni
2020-01-17 08:58:01.453692 I | op-mon: assignmon: cleaning up canary deployment rook-ceph-mon-a-canary and canary pvc
2020-01-17 08:58:01.453695 I | op-k8sutil: removing deployment rook-ceph-mon-a-canary if it exists
2020-01-17 08:58:01.460674 I | op-k8sutil: Removed deployment rook-ceph-mon-a-canary
2020-01-17 08:58:01.467372 I | op-k8sutil: rook-ceph-mon-a-canary still found. waiting...
2020-01-17 08:58:03.473191 I | op-k8sutil: rook-ceph-mon-a-canary still found. waiting...
2020-01-17 08:58:05.477920 I | op-k8sutil: confirmed rook-ceph-mon-a-canary does not exist
2020-01-17 08:58:05.477961 I | op-mon: assignmon: cleaning up canary deployment rook-ceph-mon-b-canary and canary pvc
2020-01-17 08:58:05.477976 I | op-k8sutil: removing deployment rook-ceph-mon-b-canary if it exists
2020-01-17 08:58:05.485287 I | op-k8sutil: Removed deployment rook-ceph-mon-b-canary
2020-01-17 08:58:05.496840 I | op-k8sutil: rook-ceph-mon-b-canary still found. waiting...
2020-01-17 08:58:07.502458 I | op-k8sutil: rook-ceph-mon-b-canary still found. waiting...
2020-01-17 08:58:09.507465 I | op-k8sutil: rook-ceph-mon-b-canary still found. waiting...
2020-01-17 08:58:11.512975 I | op-k8sutil: rook-ceph-mon-b-canary still found. waiting...
2020-01-17 08:58:13.518712 I | op-k8sutil: rook-ceph-mon-b-canary still found. waiting...
2020-01-17 08:58:15.523974 I | op-k8sutil: rook-ceph-mon-b-canary still found. waiting...
2020-01-17 08:58:17.528659 I | op-k8sutil: confirmed rook-ceph-mon-b-canary does not exist
2020-01-17 08:58:17.528699 I | op-mon: assignmon: cleaning up canary deployment rook-ceph-mon-c-canary and canary pvc
2020-01-17 08:58:17.528714 I | op-k8sutil: removing deployment rook-ceph-mon-c-canary if it exists
2020-01-17 08:58:17.536449 I | op-k8sutil: Removed deployment rook-ceph-mon-c-canary
2020-01-17 08:58:17.543326 I | op-k8sutil: rook-ceph-mon-c-canary still found. waiting...
2020-01-17 08:58:19.547350 I | op-k8sutil: confirmed rook-ceph-mon-c-canary does not exist
2020-01-17 08:58:19.547394 I | op-mon: creating mon a
2020-01-17 08:58:19.560878 I | op-mon: mon "a" endpoint are [v2:10.233.41.56:3300,v1:10.233.41.56:6789]
2020-01-17 08:58:19.572647 I | op-mon: saved mon endpoints to config map map[data:a=10.233.41.56:6789 maxMonId:2 mapping:{"node":{"a":{"Name":"hans","Hostname":"hans","Address":"10.7.0.12"},"b":{"Name":"ludwig","Hostname":"ludwig","Address":"10.7.0.9"},"c":{"Name":"benni","Hostname":"benni","Address":"10.7.0.6"}}} csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.233.41.56:6789"]}]]
2020-01-17 08:58:19.597086 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2020-01-17 08:58:19.599989 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph
2020-01-17 08:58:19.607926 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2020-01-17 08:58:19.607970 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph
2020-01-17 08:58:19.612609 I | op-mon: 0 of 1 expected mon deployments exist. creating new deployment(s).
2020-01-17 08:58:19.623780 I | op-mon: waiting for mon quorum with [a]
2020-01-17 08:58:19.630772 I | op-mon: mon a is not yet running
2020-01-17 08:58:19.630808 I | op-mon: mons running: []
2020-01-17 08:58:19.631122 I | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/913880231
2020-01-17 08:58:24.430269 I | op-mon: Monitors in quorum: [a]
2020-01-17 08:58:24.430279 I | op-mon: mons created: 1
2020-01-17 08:58:24.430329 I | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/707914714
2020-01-17 08:58:24.670094 I | op-mon: waiting for mon quorum with [a]
2020-01-17 08:58:24.683399 I | op-mon: mons running: [a]
2020-01-17 08:58:24.683460 I | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/295698289
2020-01-17 08:58:24.924804 I | op-mon: Monitors in quorum: [a]
2020-01-17 08:58:24.924886 I | exec: Running command: ceph config set global mon_allow_pool_delete true --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/085748252
2020-01-17 08:58:25.189220 I | exec: Running command: ceph config set global rbd_default_features 3 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/604384459
2020-01-17 08:58:25.462175 I | op-mon: creating mon b
2020-01-17 08:58:25.497952 I | op-mon: mon "a" endpoint are [v2:10.233.41.56:3300,v1:10.233.41.56:6789]
2020-01-17 08:58:25.506751 I | op-mon: mon "b" endpoint are [v2:10.233.39.112:3300,v1:10.233.39.112:6789]
2020-01-17 08:58:25.519421 I | op-mon: saved mon endpoints to config map map[data:a=10.233.41.56:6789,b=10.233.39.112:6789 maxMonId:2 mapping:{"node":{"a":{"Name":"hans","Hostname":"hans","Address":"10.7.0.12"},"b":{"Name":"ludwig","Hostname":"ludwig","Address":"10.7.0.9"},"c":{"Name":"benni","Hostname":"benni","Address":"10.7.0.6"}}} csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.233.41.56:6789","10.233.39.112:6789"]}]]
2020-01-17 08:58:25.527459 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2020-01-17 08:58:25.527535 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph
2020-01-17 08:58:25.535462 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2020-01-17 08:58:25.535524 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph
2020-01-17 08:58:25.540613 I | op-mon: 1 of 2 expected mon deployments exist. creating new deployment(s).
2020-01-17 08:58:25.544887 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed
2020-01-17 08:58:25.549119 I | op-k8sutil: updating deployment rook-ceph-mon-a
2020-01-17 08:58:27.567617 I | op-k8sutil: finished waiting for updated deployment rook-ceph-mon-a
2020-01-17 08:58:27.579312 I | op-mon: waiting for mon quorum with [a b]
2020-01-17 08:58:27.599827 I | op-mon: mon b is not yet running
2020-01-17 08:58:27.599866 I | op-mon: mons running: [a]
2020-01-17 08:58:27.600081 I | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/239813038
2020-01-17 08:58:27.906082 I | op-mon: Monitors in quorum: [a]
2020-01-17 08:58:27.906103 I | op-mon: mons created: 2
2020-01-17 08:58:27.906171 I | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/759025461
2020-01-17 08:58:28.169320 I | op-mon: waiting for mon quorum with [a b]
2020-01-17 08:58:28.185305 I | op-mon: mon b is not yet running
2020-01-17 08:58:28.185351 I | op-mon: mons running: [a]
2020-01-17 08:58:33.194075 I | op-mon: mons running: [a b]
2020-01-17 08:58:33.194278 I | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/073683216
2020-01-17 08:58:33.478205 W | op-mon: failed to find initial monitor b in mon map
2020-01-17 08:58:33.478217 W | op-mon: monitor b is not in quorum list
2020-01-17 08:58:38.496704 I | op-mon: mons running: [a b]
2020-01-17 08:58:38.496900 I | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/317412655
===== block repeats a few times
2020-01-17 09:00:56.310791 W | op-mon: failed to find initial monitor b in mon map
2020-01-17 09:00:56.310801 W | op-mon: monitor b is not in quorum list
2020-01-17 09:01:01.328716 I | op-mon: mons running: [a b]
2020-01-17 09:01:01.328900 I | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/516398780
2020-01-17 09:01:01.594866 W | op-mon: failed to find initial monitor b in mon map
2020-01-17 09:01:01.594877 W | op-mon: monitor b is not in quorum list
2020-01-17 09:01:01.594968 E | op-cluster: failed to create cluster in namespace "rook-ceph". failed to start the mons: failed to start mon pods: failed to wait for mon quorum: exceeded max retry count waiting for monitors to reach quorum
2020-01-17 09:01:06.835214 I | op-cluster: detecting the ceph image version for image ceph/daemon-base:latest-master...
2020-01-17 09:01:09.466709 I | op-cluster: Detected ceph image version: "15.0.0-9337 octopus"
2020-01-17 09:01:09.466761 W | op-cluster: unsupported ceph version detected: "15.0.0-9337 octopus", pursuing
2020-01-17 09:01:09.474245 I | op-mon: parsing mon endpoints: a=10.233.41.56:6789,b=10.233.39.112:6789
2020-01-17 09:01:09.474412 I | op-mon: loaded: maxMonID=2, mons=map[a:0xc000fdf660 b:0xc000fdf8a0], mapping=&{Node:map[b:0xc000804a20 c:0xc000804a50 a:0xc0008049f0]}
2020-01-17 09:01:09.475106 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2020-01-17 09:01:09.475370 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph
2020-01-17 09:01:09.475520 I | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/593267947
2020-01-17 09:01:09.756800 I | op-cluster: CephCluster "rook-ceph" status: "Creating".
2020-01-17 09:01:09.774145 I | op-mon: start running mons
2020-01-17 09:01:09.781531 I | op-mon: parsing mon endpoints: a=10.233.41.56:6789,b=10.233.39.112:6789
2020-01-17 09:01:09.781687 I | op-mon: loaded: maxMonID=2, mons=map[a:0xc000fad500 b:0xc000fad540], mapping=&{Node:map[b:0xc00095f350 c:0xc00095f3b0 a:0xc00095f2c0]}
2020-01-17 09:01:09.792529 I | op-mon: saved mon endpoints to config map map[data:a=10.233.41.56:6789,b=10.233.39.112:6789 maxMonId:2 mapping:{"node":{"a":{"Name":"hans","Hostname":"hans","Address":"10.7.0.12"},"b":{"Name":"ludwig","Hostname":"ludwig","Address":"10.7.0.9"},"c":{"Name":"benni","Hostname":"benni","Address":"10.7.0.6"}}} csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.233.41.56:6789","10.233.39.112:6789"]}]]
2020-01-17 09:01:10.130145 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2020-01-17 09:01:10.130241 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph
2020-01-17 09:01:11.330611 I | op-mon: targeting the mon count 3
2020-01-17 09:01:11.337700 I | op-mon: sched-mon: created canary deployment rook-ceph-mon-d-canary
2020-01-17 09:01:12.532002 I | op-mon: sched-mon: canary monitor deployment rook-ceph-mon-d-canary scheduled to benni
2020-01-17 09:01:12.532048 I | op-mon: assignmon: mon d assigned to node benni
2020-01-17 09:01:12.532071 I | op-mon: assignmon: cleaning up canary deployment rook-ceph-mon-d-canary and canary pvc
2020-01-17 09:01:12.532090 I | op-k8sutil: removing deployment rook-ceph-mon-d-canary if it exists
2020-01-17 09:01:12.536133 I | op-k8sutil: Removed deployment rook-ceph-mon-d-canary
2020-01-17 09:01:12.539583 I | op-k8sutil: rook-ceph-mon-d-canary still found. waiting...
2020-01-17 09:01:14.544709 I | op-k8sutil: rook-ceph-mon-d-canary still found. waiting...
2020-01-17 09:01:16.548667 I | op-k8sutil: confirmed rook-ceph-mon-d-canary does not exist
2020-01-17 09:01:16.548887 I | exec: Running command: ceph config set global mon_allow_pool_delete true --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/081288014
2020-01-17 09:01:16.781132 I | exec: Running command: ceph config set global rbd_default_features 3 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/338749525
2020-01-17 09:01:17.001133 I | op-mon: creating mon d
2020-01-17 09:01:17.027733 I | op-mon: mon "a" endpoint are [v2:10.233.41.56:3300,v1:10.233.41.56:6789]
2020-01-17 09:01:17.055068 I | op-mon: mon "b" endpoint are [v2:10.233.39.112:3300,v1:10.233.39.112:6789]
2020-01-17 09:01:17.065616 I | op-mon: mon "d" endpoint are [v2:10.233.17.5:3300,v1:10.233.17.5:6789]
2020-01-17 09:01:17.076527 I | op-mon: saved mon endpoints to config map map[data:a=10.233.41.56:6789,b=10.233.39.112:6789,d=10.233.17.5:6789 maxMonId:3 mapping:{"node":{"a":{"Name":"hans","Hostname":"hans","Address":"10.7.0.12"},"b":{"Name":"ludwig","Hostname":"ludwig","Address":"10.7.0.9"},"c":{"Name":"benni","Hostname":"benni","Address":"10.7.0.6"},"d":{"Name":"benni","Hostname":"benni","Address":"10.7.0.6"}}} csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.233.39.112:6789","10.233.17.5:6789","10.233.41.56:6789"]}]]
2020-01-17 09:01:17.211268 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2020-01-17 09:01:17.211607 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph
2020-01-17 09:01:17.618854 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2020-01-17 09:01:17.619215 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph
2020-01-17 09:01:17.628022 I | op-mon: 2 of 3 expected mon deployments exist. creating new deployment(s).
2020-01-17 09:01:17.633156 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed
2020-01-17 09:01:17.637946 I | op-k8sutil: updating deployment rook-ceph-mon-a
2020-01-17 09:01:19.657234 I | op-k8sutil: finished waiting for updated deployment rook-ceph-mon-a
2020-01-17 09:01:19.662660 I | op-mon: deployment for mon rook-ceph-mon-b already exists. updating if needed
2020-01-17 09:01:19.667963 I | op-k8sutil: updating deployment rook-ceph-mon-b
2020-01-17 09:01:21.691679 I | op-k8sutil: finished waiting for updated deployment rook-ceph-mon-b
2020-01-17 09:01:21.703744 I | op-mon: waiting for mon quorum with [a b d]
2020-01-17 09:01:21.740501 I | op-mon: mon d is not yet running
2020-01-17 09:01:21.740546 I | op-mon: mons running: [a b]
2020-01-17 09:01:21.740737 I | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/709984688
2020-01-17 09:01:22.011003 I | op-mon: Monitors in quorum: [a]
2020-01-17 09:01:22.011014 I | op-mon: mons created: 3
2020-01-17 09:01:22.011069 I | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/463933775
2020-01-17 09:01:22.249222 I | op-mon: waiting for mon quorum with [a b d]
2020-01-17 09:01:22.275633 I | op-mon: mon d is not yet running
2020-01-17 09:01:22.275684 I | op-mon: mons running: [a b]
2020-01-17 09:01:27.301474 I | op-mon: mons running: [a b d]
2020-01-17 09:01:27.301702 I | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/764417122
2020-01-17 09:01:27.612094 W | op-mon: failed to find initial monitor b in mon map
2020-01-17 09:01:27.612106 W | op-mon: monitor b is not in quorum list
2020-01-17 09:01:32.638241 I | op-mon: mons running: [a b d]
2020-01-17 09:01:32.638456 I | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/067493977
===== repeats a few times
2020-01-17 09:03:50.610037 W | op-mon: failed to find initial monitor b in mon map
2020-01-17 09:03:50.610048 W | op-mon: monitor b is not in quorum list
2020-01-17 09:03:55.636255 I | op-mon: mons running: [a b d]
2020-01-17 09:03:55.636440 I | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/712259310
2020-01-17 09:03:55.894999 W | op-mon: failed to find initial monitor b in mon map
2020-01-17 09:03:55.895008 W | op-mon: monitor b is not in quorum list
2020-01-17 09:03:55.895033 E | op-cluster: failed to create cluster in namespace "rook-ceph". failed to start the mons: failed to start mon pods: failed to wait for mon quorum: exceeded max retry count waiting for monitors to reach quorum
2020-01-17 09:04:00.835137 I | op-cluster: detecting the ceph image version for image ceph/daemon-base:latest-master...
2020-01-17 09:04:03.558532 I | op-cluster: Detected ceph image version: "15.0.0-9337 octopus"
2020-01-17 09:04:03.558569 W | op-cluster: unsupported ceph version detected: "15.0.0-9337 octopus", pursuing
2020-01-17 09:04:03.566232 I | op-mon: parsing mon endpoints: a=10.233.41.56:6789,b=10.233.39.112:6789,d=10.233.17.5:6789
2020-01-17 09:04:03.566364 I | op-mon: loaded: maxMonID=3, mons=map[b:0xc000bc53e0 d:0xc000bc5480 a:0xc000bc5360], mapping=&{Node:map[a:0xc000bf3980 b:0xc000bf39b0 c:0xc000bf39e0 d:0xc000bf3a10]}
2020-01-17 09:04:03.566810 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2020-01-17 09:04:03.567070 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph
2020-01-17 09:04:03.567199 I | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/259428725
2020-01-17 09:04:03.815338 I | op-cluster: CephCluster "rook-ceph" status: "Creating".
2020-01-17 09:04:03.833406 I | op-mon: start running mons
2020-01-17 09:04:03.840912 I | op-mon: parsing mon endpoints: a=10.233.41.56:6789,b=10.233.39.112:6789,d=10.233.17.5:6789
2020-01-17 09:04:03.841109 I | op-mon: loaded: maxMonID=3, mons=map[a:0xc0000cb9a0 b:0xc0000cba20 d:0xc0000cba80], mapping=&{Node:map[a:0xc000ba7830 b:0xc000ba7860 c:0xc000ba7890 d:0xc000ba78c0]}
2020-01-17 09:04:03.851888 I | op-mon: saved mon endpoints to config map map[data:a=10.233.41.56:6789,b=10.233.39.112:6789,d=10.233.17.5:6789 maxMonId:3 mapping:{"node":{"a":{"Name":"hans","Hostname":"hans","Address":"10.7.0.12"},"b":{"Name":"ludwig","Hostname":"ludwig","Address":"10.7.0.9"},"c":{"Name":"benni","Hostname":"benni","Address":"10.7.0.6"},"d":{"Name":"benni","Hostname":"benni","Address":"10.7.0.6"}}} csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.233.39.112:6789","10.233.17.5:6789","10.233.41.56:6789"]}]]
2020-01-17 09:04:03.980544 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2020-01-17 09:04:03.980776 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph
2020-01-17 09:04:05.179606 I | op-mon: targeting the mon count 3
2020-01-17 09:04:05.179825 I | exec: Running command: ceph config set global mon_allow_pool_delete true --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/010510416
2020-01-17 09:04:05.405106 I | exec: Running command: ceph config set global rbd_default_features 3 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/836333935
2020-01-17 09:04:05.620996 I | op-mon: checking for basic quorum with existing mons
2020-01-17 09:04:06.582201 I | op-mon: mon "a" endpoint are [v2:10.233.41.56:3300,v1:10.233.41.56:6789]
2020-01-17 09:04:07.181902 I | op-mon: mon "b" endpoint are [v2:10.233.39.112:3300,v1:10.233.39.112:6789]
2020-01-17 09:04:07.782443 I | op-mon: mon "d" endpoint are [v2:10.233.17.5:3300,v1:10.233.17.5:6789]
2020-01-17 09:04:08.780645 I | op-mon: saved mon endpoints to config map map[data:d=10.233.17.5:6789,a=10.233.41.56:6789,b=10.233.39.112:6789 maxMonId:3 mapping:{"node":{"a":{"Name":"hans","Hostname":"hans","Address":"10.7.0.12"},"b":{"Name":"ludwig","Hostname":"ludwig","Address":"10.7.0.9"},"c":{"Name":"benni","Hostname":"benni","Address":"10.7.0.6"},"d":{"Name":"benni","Hostname":"benni","Address":"10.7.0.6"}}} csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.233.41.56:6789","10.233.39.112:6789","10.233.17.5:6789"]}]]
2020-01-17 09:04:09.393271 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2020-01-17 09:04:09.393602 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph
2020-01-17 09:04:09.785523 I | cephconfig: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2020-01-17 09:04:09.785808 I | cephconfig: generated admin config in /var/lib/rook/rook-ceph
2020-01-17 09:04:09.799886 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed
2020-01-17 09:04:09.809811 I | op-k8sutil: updating deployment rook-ceph-mon-a
2020-01-17 09:04:11.829618 I | op-k8sutil: finished waiting for updated deployment rook-ceph-mon-a
2020-01-17 09:04:11.829675 I | op-mon: waiting for mon quorum with [a b d]
2020-01-17 09:04:11.855457 I | op-mon: mons running: [a b d]
2020-01-17 09:04:11.855640 I | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/906679298
2020-01-17 09:04:12.119617 I | op-mon: Monitors in quorum: [a]
2020-01-17 09:04:12.125043 I | op-mon: deployment for mon rook-ceph-mon-b already exists. updating if needed
2020-01-17 09:04:12.130193 I | op-k8sutil: updating deployment rook-ceph-mon-b
2020-01-17 09:04:14.157047 I | op-k8sutil: finished waiting for updated deployment rook-ceph-mon-b
2020-01-17 09:04:14.157100 I | op-mon: waiting for mon quorum with [a b d]
2020-01-17 09:04:14.182876 I | op-mon: mons running: [a b d]
2020-01-17 09:04:14.183157 I | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/202669945
2020-01-17 09:04:14.436540 I | op-mon: Monitors in quorum: [a]
2020-01-17 09:04:14.442205 I | op-mon: deployment for mon rook-ceph-mon-d already exists. updating if needed
2020-01-17 09:04:14.447487 I | op-k8sutil: updating deployment rook-ceph-mon-d
2020-01-17 09:04:16.471732 I | op-k8sutil: finished waiting for updated deployment rook-ceph-mon-d
2020-01-17 09:04:16.471786 I | op-mon: waiting for mon quorum with [a b d]
2020-01-17 09:04:16.490998 I | op-mon: mons running: [a b d]
2020-01-17 09:04:16.491174 I | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/832341636
2020-01-17 09:04:16.753685 I | op-mon: Monitors in quorum: [a]
2020-01-17 09:04:16.753697 I | op-mon: mons created: 3
2020-01-17 09:04:16.753748 I | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/313627155
2020-01-17 09:04:16.989208 I | op-mon: waiting for mon quorum with [a b d]
2020-01-17 09:04:16.995972 I | op-mon: mons running: [a b d]
2020-01-17 09:04:16.996043 I | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/238187606
2020-01-17 09:04:17.242758 I | op-mon: Monitors in quorum: [a]
2020-01-17 09:04:17.242849 I | exec: Running command: ceph auth get-or-create-key client.csi-rbd-provisioner mon profile rbd mgr allow rw osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/807687613
2020-01-17 09:04:17.541890 I | exec: Running command: ceph auth get-or-create-key client.csi-rbd-node mon profile rbd osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/114472440
2020-01-17 09:04:17.795433 I | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-provisioner mon allow r mgr allow rw osd allow rw tag cephfs metadata=* --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/071043319
2020-01-17 09:04:18.064208 I | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-node mon allow r mgr allow rw osd allow rw tag cephfs *=* mds allow rw --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/059901930
2020-01-17 09:04:18.391212 I | ceph-csi: created kubernetes csi secrets for cluster "rook-ceph"
2020-01-17 09:04:18.391256 I | exec: Running command: ceph auth get-or-create-key client.crash mon allow profile crash mgr allow profile crash --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/442661441
2020-01-17 09:04:18.607342 I | ceph-crashcollector-controller: created kubernetes crash collector secret for cluster "rook-ceph"
2020-01-17 09:04:18.607381 I | exec: Running command: ceph version --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/143053996
2020-01-17 09:04:18.838518 I | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/684738587
2020-01-17 09:04:19.112549 I | exec: Running command: ceph mon enable-msgr2 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/449156286
2020-01-17 09:04:19.320858 I | cephclient: successfully enabled msgr2 protocol
2020-01-17 09:04:19.320872 I | op-mgr: start running mgr
2020-01-17 09:04:19.320927 I | exec: Running command: ceph auth get-or-create-key mgr.a mon allow * mds allow * osd allow * --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/168209669
2020-01-17 09:04:20.484970 I | op-mgr: dashboard service started
2020-01-17 09:04:20.485248 I | exec: Running command: ceph mgr module enable dashboard --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/129670816
2020-01-17 09:04:20.485296 I | op-mgr: successful modules: mgr module(s) from the spec
2020-01-17 09:04:20.485346 I | exec: Running command: ceph mgr module enable prometheus --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/250947967
2020-01-17 09:04:20.485425 I | exec: Running command: ceph config get mgr.a mgr/dashboard/server_addr --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/047869650
2020-01-17 09:04:20.485535 I | exec: Running command: ceph mgr module enable crash --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/009795081
2020-01-17 09:04:20.485668 I | exec: Running command: ceph mgr module enable rook --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/309750740
2020-01-17 09:04:20.723039 I | exec: Error ENOENT:
2020-01-17 09:04:20.723152 I | exec: Running command: ceph config rm mgr.a mgr/dashboard/server_addr --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/723888931
2020-01-17 09:04:20.940511 I | exec: Running command: ceph config get mgr.a mgr/dashboard/a/server_addr --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/636720678
2020-01-17 09:04:21.178586 I | exec: Error ENOENT:
2020-01-17 09:04:21.178715 I | exec: Running command: ceph config rm mgr.a mgr/dashboard/a/server_addr --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/780556621
2020-01-17 09:04:21.463748 I | exec: Running command: ceph config get mgr.a mgr/prometheus/server_addr --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/560197704
2020-01-17 09:04:21.688259 I | exec: Error ENOENT:
2020-01-17 09:04:21.688368 I | exec: Running command: ceph config rm mgr.a mgr/prometheus/server_addr --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/927389447
2020-01-17 09:04:21.745318 I | op-mgr: successful modules: prometheus
2020-01-17 09:04:21.764374 I | exec: Running command: ceph mgr module enable orchestrator_cli --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/147381946
2020-01-17 09:04:21.783835 I | exec: module 'crash' is already enabled (always-on)
2020-01-17 09:04:21.783884 I | op-mgr: successful modules: crash
2020-01-17 09:04:21.899484 I | exec: Running command: ceph config get mgr.a mgr/prometheus/a/server_addr --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/840057553
2020-01-17 09:04:22.116164 I | exec: Error ENOENT:
2020-01-17 09:04:22.116251 I | exec: Running command: ceph config rm mgr.a mgr/prometheus/a/server_addr --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/698156540
2020-01-17 09:04:22.340131 I | op-mgr: successful modules: http bind settings
2020-01-17 09:04:22.911878 I | exec: module 'orchestrator_cli' is already enabled (always-on)
2020-01-17 09:04:22.912199 I | exec: Running command: ceph orchestrator set backend rook --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/345968939
2020-01-17 09:04:23.256173 I | exec: no valid command found; 10 closest matches:
mon feature ls {--with-value}
mon add <name> <IPaddr[:port]>
mon rm <name>
mon stat
mon getmap {<int[0-]>}
fs set-default <fs_name>
mon dump {<int[0-]>}
fs rm_data_pool <fs_name> <pool>
osd getcrushmap {<int[0-]>}
pg scrub <pgid>
Error EINVAL: invalid command
2020-01-17 09:04:23.256237 I | cephclient: command failed. trying again...
2020-01-17 09:04:25.735938 I | exec: Running command: ceph dashboard create-self-signed-cert --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/660612750
2020-01-17 09:04:26.049027 I | exec: no valid command found; 10 closest matches:
mon feature ls {--with-value}
mon add <name> <IPaddr[:port]>
mon rm <name>
mon stat
mon getmap {<int[0-]>}
fs set-default <fs_name>
mon dump {<int[0-]>}
fs rm_data_pool <fs_name> <pool>
osd getcrushmap {<int[0-]>}
pg scrub <pgid>
Error EINVAL: invalid command
2020-01-17 09:04:26.049105 I | op-mgr: dashboard module is not ready yet. trying again...
2020-01-17 09:04:28.256658 I | exec: Running command: ceph orchestrator set backend rook --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/708575381
2020-01-17 09:04:28.574047 I | exec: no valid command found; 10 closest matches:
mon feature ls {--with-value}
mon add <name> <IPaddr[:port]>
mon rm <name>
mon stat
mon getmap {<int[0-]>}
fs set-default <fs_name>
mon dump {<int[0-]>}
fs rm_data_pool <fs_name> <pool>
osd getcrushmap {<int[0-]>}
pg scrub <pgid>
Error EINVAL: invalid command
2020-01-17 09:04:28.574106 I | cephclient: command failed. trying again...
2020-01-17 09:04:31.049503 I | exec: Running command: ceph dashboard create-self-signed-cert --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/480808688
2020-01-17 09:04:31.348546 I | exec: no valid command found; 10 closest matches:
mon feature ls {--with-value}
mon add <name> <IPaddr[:port]>
mon rm <name>
mon stat
mon getmap {<int[0-]>}
fs set-default <fs_name>
mon dump {<int[0-]>}
fs rm_data_pool <fs_name> <pool>
osd getcrushmap {<int[0-]>}
pg scrub <pgid>
Error EINVAL: invalid command
2020-01-17 09:04:31.348610 I | op-mgr: dashboard module is not ready yet. trying again...
2020-01-17 09:04:33.574446 I | exec: Running command: ceph orchestrator set backend rook --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/018828687
2020-01-17 09:04:33.933710 I | exec: no valid command found; 10 closest matches:
mon feature ls {--with-value}
mon add <name> <IPaddr[:port]>
mon rm <name>
mon stat
mon getmap {<int[0-]>}
fs set-default <fs_name>
mon dump {<int[0-]>}
fs rm_data_pool <fs_name> <pool>
osd getcrushmap {<int[0-]>}
pg scrub <pgid>
Error EINVAL: invalid command
2020-01-17 09:04:33.933788 I | cephclient: command failed. trying again...
2020-01-17 09:04:36.349033 I | exec: Running command: ceph dashboard create-self-signed-cert --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/292274594
2020-01-17 09:04:36.738625 I | exec: no valid command found; 10 closest matches:
mon feature ls {--with-value}
mon add <name> <IPaddr[:port]>
mon rm <name>
mon stat
mon getmap {<int[0-]>}
fs set-default <fs_name>
mon dump {<int[0-]>}
fs rm_data_pool <fs_name> <pool>
osd getcrushmap {<int[0-]>}
pg scrub <pgid>
Error EINVAL: invalid command
2020-01-17 09:04:36.738681 I | op-mgr: dashboard module is not ready yet. trying again...
2020-01-17 09:04:38.934063 I | exec: Running command: ceph orchestrator set backend rook --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/055464089
2020-01-17 09:04:39.303060 I | exec: no valid command found; 10 closest matches:
mon feature ls {--with-value}
mon add <name> <IPaddr[:port]>
mon rm <name>
mon stat
mon getmap {<int[0-]>}
fs set-default <fs_name>
mon dump {<int[0-]>}
fs rm_data_pool <fs_name> <pool>
osd getcrushmap {<int[0-]>}
pg scrub <pgid>
Error EINVAL: invalid command
2020-01-17 09:04:39.303134 I | cephclient: command failed. trying again...
2020-01-17 09:04:41.739087 I | exec: Running command: ceph dashboard create-self-signed-cert --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/889120036
2020-01-17 09:04:42.128122 I | exec: no valid command found; 10 closest matches:
mon feature ls {--with-value}
mon add <name> <IPaddr[:port]>
mon rm <name>
mon stat
mon getmap {<int[0-]>}
fs set-default <fs_name>
mon dump {<int[0-]>}
fs rm_data_pool <fs_name> <pool>
osd getcrushmap {<int[0-]>}
pg scrub <pgid>
Error EINVAL: invalid command
2020-01-17 09:04:42.128191 I | op-mgr: dashboard module is not ready yet. trying again...
2020-01-17 09:04:44.303492 I | exec: Running command: ceph orchestrator set backend rook --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/624538675
2020-01-17 09:04:44.645209 I | exec: no valid command found; 10 closest matches:
mon feature ls {--with-value}
mon add <name> <IPaddr[:port]>
mon rm <name>
mon stat
mon getmap {<int[0-]>}
fs set-default <fs_name>
mon dump {<int[0-]>}
fs rm_data_pool <fs_name> <pool>
osd getcrushmap {<int[0-]>}
pg scrub <pgid>
Error EINVAL: invalid command
2020-01-17 09:04:44.645269 I | cephclient: command failed. trying again...
2020-01-17 09:04:47.128521 I | exec: Running command: ceph dashboard create-self-signed-cert --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/072617462
2020-01-17 09:04:47.431911 I | exec: no valid command found; 10 closest matches:
mon feature ls {--with-value}
mon add <name> <IPaddr[:port]>
mon rm <name>
mon stat
mon getmap {<int[0-]>}
fs set-default <fs_name>
mon dump {<int[0-]>}
fs rm_data_pool <fs_name> <pool>
osd getcrushmap {<int[0-]>}
pg scrub <pgid>
Error EINVAL: invalid command
2020-01-17 09:04:47.431970 I | op-mgr: dashboard module is not ready yet. trying again...
2020-01-17 09:04:49.645484 E | op-mgr: failed modules: "orchestrator modules". failed to set rook orchestrator backend: failed to set rook as the orchestrator backend: max command retries exceeded
2020-01-17 09:04:52.432228 I | op-mgr: Running command: ceph dashboard set-login-credentials admin *******
2020-01-17 09:04:52.741288 I | exec: no valid command found; 10 closest matches:
mon feature ls {--with-value}
mon add <name> <IPaddr[:port]>
mon rm <name>
mon stat
mon getmap {<int[0-]>}
fs set-default <fs_name>
mon dump {<int[0-]>}
fs rm_data_pool <fs_name> <pool>
osd getcrushmap {<int[0-]>}
pg scrub <pgid>
Error EINVAL: invalid command
2020-01-17 09:04:52.741362 I | cephclient: command failed. trying again...
2020-01-17 09:04:58.049436 I | exec: no valid command found; 10 closest matches:
mon feature ls {--with-value}
mon add <name> <IPaddr[:port]>
mon rm <name>
mon stat
mon getmap {<int[0-]>}
fs set-default <fs_name>
mon dump {<int[0-]>}
fs rm_data_pool <fs_name> <pool>
osd getcrushmap {<int[0-]>}
pg scrub <pgid>
Error EINVAL: invalid command
2020-01-17 09:04:58.049521 I | cephclient: command failed. trying again...
2020-01-17 09:05:03.457996 I | exec: no valid command found; 10 closest matches:
mon feature ls {--with-value}
mon add <name> <IPaddr[:port]>
mon rm <name>
mon stat
mon getmap {<int[0-]>}
fs set-default <fs_name>
mon dump {<int[0-]>}
fs rm_data_pool <fs_name> <pool>
osd getcrushmap {<int[0-]>}
pg scrub <pgid>
Error EINVAL: invalid command
2020-01-17 09:05:03.458075 I | cephclient: command failed. trying again...
2020-01-17 09:05:08.752864 I | exec: no valid command found; 10 closest matches:
mon feature ls {--with-value}
mon add <name> <IPaddr[:port]>
mon rm <name>
mon stat
mon getmap {<int[0-]>}
fs set-default <fs_name>
mon dump {<int[0-]>}
fs rm_data_pool <fs_name> <pool>
osd getcrushmap {<int[0-]>}
pg scrub <pgid>
Error EINVAL: invalid command
2020-01-17 09:05:08.752923 I | cephclient: command failed. trying again...
2020-01-17 09:05:14.099747 I | exec: no valid command found; 10 closest matches:
mon feature ls {--with-value}
mon add <name> <IPaddr[:port]>
mon rm <name>
mon stat
mon getmap {<int[0-]>}
fs set-default <fs_name>
mon dump {<int[0-]>}
fs rm_data_pool <fs_name> <pool>
osd getcrushmap {<int[0-]>}
pg scrub <pgid>
Error EINVAL: invalid command
2020-01-17 09:05:14.099816 I | cephclient: command failed. trying again...
2020-01-17 09:05:19.100060 E | op-mgr: failed modules: "dashboard". failed to initialize dashboard: failed to set login credentials for the ceph dashboard: failed to set login creds on mgr: max command retries exceeded
2020-01-17 09:05:19.116275 I | op-mgr: mgr metrics service started
2020-01-17 09:05:19.116344 I | op-osd: start running osds in namespace rook-ceph
2020-01-17 09:05:19.116357 I | op-osd: start provisioning the osds on pvcs, if needed
2020-01-17 09:05:19.116366 I | op-osd: no volume sources defined to configure OSDs on PVCs.
2020-01-17 09:05:19.116375 I | op-osd: start provisioning the osds on nodes, if needed
2020-01-17 09:05:19.124392 I | op-osd: 3 of the 3 storage nodes are valid
2020-01-17 09:05:19.130301 I | op-osd: osd provision job started for node benni
2020-01-17 09:05:19.142994 I | op-osd: osd provision job started for node hans
2020-01-17 09:05:19.316185 I | op-osd: osd provision job started for node ludwig
2020-01-17 09:05:19.316225 I | op-osd: start osds after provisioning is completed, if needed
2020-01-17 09:05:19.505941 I | op-osd: osd orchestration status for node benni is starting
2020-01-17 09:05:19.505995 I | op-osd: osd orchestration status for node hans is starting
2020-01-17 09:05:19.506020 I | op-osd: osd orchestration status for node ludwig is starting
2020-01-17 09:05:19.506032 I | op-osd: 0/3 node(s) completed osd provisioning, resource version 13504
2020-01-17 09:05:21.525087 I | op-osd: osd orchestration status for node hans is computingDiff
2020-01-17 09:05:21.742484 I | op-osd: osd orchestration status for node hans is orchestrating
2020-01-17 09:05:21.889676 I | op-osd: osd orchestration status for node ludwig is computingDiff
2020-01-17 09:05:21.898492 I | op-osd: osd orchestration status for node benni is computingDiff
2020-01-17 09:05:21.947180 I | op-osd: osd orchestration status for node ludwig is orchestrating
2020-01-17 09:05:21.962030 I | op-osd: osd orchestration status for node benni is orchestrating
2020-01-17 09:05:22.751013 I | op-osd: osd orchestration status for node hans is failed
2020-01-17 09:05:22.751073 E | op-osd: orchestration for node hans failed: &{OSDs:[] Status:failed PvcBackedOSD:false Message:failed to configure devices: failed to configure devices with ceph-volume: failed to initialize devices: failed ceph-volume: Failed to complete '': exit status 1. }
2020-01-17 09:05:23.638804 I | op-osd: skipping event from node hans status update since it is already completed
2020-01-17 09:05:23.850577 I | op-osd: skipping event from node hans status update since it is already completed
2020-01-17 09:05:30.011652 I | op-osd: skipping event from node hans status update since it is already completed
2020-01-17 09:05:42.979085 I | op-osd: skipping event from node hans status update since it is already completed
2020-01-17 09:05:43.039176 I | op-osd: skipping event from node hans status update since it is already completed
2020-01-17 09:06:08.976368 I | op-osd: skipping event from node hans status update since it is already completed
2020-01-17 09:06:09.124034 I | op-osd: skipping event from node hans status update since it is already completed
2020-01-17 09:06:51.974548 I | op-osd: skipping event from node hans status update since it is already completed
2020-01-17 09:06:52.202083 I | op-osd: skipping event from node hans status update since it is already completed
2020-01-17 09:07:52.202312 I | op-osd: waiting on orchestration status update from 2 remaining nodes
2020-01-17 09:08:20.018054 I | op-osd: skipping event from node hans status update since it is already completed
2020-01-17 09:08:20.088909 I | op-osd: skipping event from node hans status update since it is already completed
2020-01-17 09:09:20.089103 I | op-osd: waiting on orchestration status update from 2 remaining nodes
2020-01-17 09:10:20.089333 I | op-osd: waiting on orchestration status update from 2 remaining nodes
2020-01-17 09:11:07.921380 I | op-osd: skipping event from node hans status update since it is already completed
2020-01-17 09:11:08.126356 I | op-osd: skipping event from node hans status update since it is already completed
2020-01-17 09:12:08.126613 I | op-osd: waiting on orchestration status update from 2 remaining nodes
2020-01-17 09:13:08.126841 I | op-osd: waiting on orchestration status update from 2 remaining nodes
2020-01-17 09:14:08.127129 I | op-osd: waiting on orchestration status update from 2 remaining nodes
2020-01-17 09:15:08.127351 I | op-osd: waiting on orchestration status update from 2 remaining nodes
2020-01-17 09:16:08.127585 I | op-osd: waiting on orchestration status update from 2 remaining nodes
2020-01-17 09:17:08.127838 I | op-osd: waiting on orchestration status update from 2 remaining nodes
2020-01-17 09:18:08.128086 I | op-osd: waiting on orchestration status update from 2 remaining nodes
2020-01-17 09:19:08.128319 I | op-osd: waiting on orchestration status update from 2 remaining nodes
2020-01-17 09:20:08.128636 I | op-osd: waiting on orchestration status update from 2 remaining nodes
2020-01-17 09:21:08.128937 I | op-osd: waiting on orchestration status update from 2 remaining nodes
2020-01-17 09:22:08.129191 I | op-osd: waiting on orchestration status update from 2 remaining nodes
2020-01-17 09:23:08.129445 I | op-osd: waiting on orchestration status update from 2 remaining nodes
2020-01-17 09:24:08.129689 I | op-osd: waiting on orchestration status update from 2 remaining nodes
```

**Logs of second monitor (first one that does not join)**
```
root@ludwig ~ # kubectl -n rook-ceph logs rook-ceph-mon-b-6c9c68c9ff-4xjpm
debug 2020-01-17T08:58:31.107+0000 7fd2a63f76c0  0 setuser_match_path /var/lib/ceph/mon/ceph-b/store.db owned by 167:167. set uid:gid to 167:167 (ceph:ceph)
debug 2020-01-17T08:58:31.107+0000 7fd2a63f76c0  0 ceph version 15.0.0-9337-g4b7f5de (4b7f5de2353cc9e6d81c778d8e4c688de66cd6fd) octopus (dev), process ceph-mon, pid 1
debug 2020-01-17T08:58:31.107+0000 7fd2a63f76c0  0 pidfile_write: ignore empty --pid-file
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  0 load: jerasure load: lrc load: isa
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  0  set rocksdb option compression = kNoCompression
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  0  set rocksdb option level_compaction_dynamic_level_bytes = true
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  0  set rocksdb option write_buffer_size = 33554432
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  0  set rocksdb option compression = kNoCompression
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  0  set rocksdb option level_compaction_dynamic_level_bytes = true
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  0  set rocksdb option write_buffer_size = 33554432
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  1 rocksdb: do_open column families: [default]
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: RocksDB version: 6.1.2

debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: Git sha rocksdb_build_git_sha:@0@
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: Compile date Jan 15 2020
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: DB SUMMARY

debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: CURRENT file:  CURRENT

debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: IDENTITY file:  IDENTITY

debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: MANIFEST file:  MANIFEST-000001 size: 13 Bytes

debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: SST files in /var/lib/ceph/mon/ceph-b/store.db dir, Total Num: 0, files:

debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: Write Ahead Log file in /var/lib/ceph/mon/ceph-b/store.db: 000003.log size: 741 ;

debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                         Options.error_if_exists: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                       Options.create_if_missing: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                         Options.paranoid_checks: 1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                                     Options.env: 0x55b816991160
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                                Options.info_log: 0x55b817cc63e0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                Options.max_file_opening_threads: 16
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                              Options.statistics: (nil)
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                               Options.use_fsync: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                       Options.max_log_file_size: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                  Options.max_manifest_file_size: 1073741824
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                   Options.log_file_time_to_roll: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                       Options.keep_log_file_num: 1000
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                    Options.recycle_log_file_num: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                         Options.allow_fallocate: 1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                        Options.allow_mmap_reads: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                       Options.allow_mmap_writes: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                        Options.use_direct_reads: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                        Options.use_direct_io_for_flush_and_compaction: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:          Options.create_missing_column_families: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                              Options.db_log_dir:
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                                 Options.wal_dir: /var/lib/ceph/mon/ceph-b/store.db
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                Options.table_cache_numshardbits: 6
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                      Options.max_subcompactions: 1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                  Options.max_background_flushes: -1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                         Options.WAL_ttl_seconds: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                       Options.WAL_size_limit_MB: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:             Options.manifest_preallocation_size: 4194304
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                     Options.is_fd_close_on_exec: 1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                   Options.advise_random_on_open: 1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                    Options.db_write_buffer_size: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                    Options.write_buffer_manager: 0x55b81885eab0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:         Options.access_hint_on_compaction_start: 1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:  Options.new_table_reader_for_compaction_inputs: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:           Options.random_access_max_buffer_size: 1048576
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                      Options.use_adaptive_mutex: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                            Options.rate_limiter: (nil)
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:     Options.sst_file_manager.rate_bytes_per_sec: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                       Options.wal_recovery_mode: 2
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                  Options.enable_thread_tracking: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                  Options.enable_pipelined_write: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:         Options.allow_concurrent_memtable_write: 1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:      Options.enable_write_thread_adaptive_yield: 1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:             Options.write_thread_max_yield_usec: 100
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:            Options.write_thread_slow_yield_usec: 3
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                               Options.row_cache: None
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                              Options.wal_filter: None
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:             Options.avoid_flush_during_recovery: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:             Options.allow_ingest_behind: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:             Options.preserve_deletes: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:             Options.two_write_queues: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:             Options.manual_wal_flush: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:             Options.atomic_flush: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:             Options.avoid_unnecessary_blocking_io: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:             Options.max_background_jobs: 2
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:             Options.max_background_compactions: -1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:             Options.avoid_flush_during_shutdown: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:           Options.writable_file_max_buffer_size: 1048576
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:             Options.delayed_write_rate : 16777216
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:             Options.max_total_wal_size: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:             Options.delete_obsolete_files_period_micros: 21600000000
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                   Options.stats_dump_period_sec: 600
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                 Options.stats_persist_period_sec: 600
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                 Options.stats_history_buffer_size: 1048576
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                          Options.max_open_files: -1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                          Options.bytes_per_sync: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                      Options.wal_bytes_per_sync: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:       Options.compaction_readahead_size: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: Compression algorithms supported:
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:     kZSTDNotFinalCompression supported: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:     kZSTD supported: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:     kXpressCompression supported: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:     kLZ4HCCompression supported: 1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:     kLZ4Compression supported: 1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:     kBZip2Compression supported: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:     kZlibCompression supported: 1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:     kSnappyCompression supported: 1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: Fast CRC32 supported: Supported on x86
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: [db/version_set.cc:3543] Recovering from manifest file: /var/lib/ceph/mon/ceph-b/store.db/MANIFEST-000001

debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: [db/column_family.cc:477] --------------- Options for column family [default]:

debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:               Options.comparator: leveldb.BytewiseComparator
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:           Options.merge_operator:
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:        Options.compaction_filter: None
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:        Options.compaction_filter_factory: None
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:         Options.memtable_factory: SkipListFactory
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:            Options.table_factory: BlockBasedTable
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:            table_factory options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory (0x55b817be40e8)
  cache_index_and_filter_blocks: 1
  cache_index_and_filter_blocks_with_high_priority: 1
  pin_l0_filter_and_index_blocks_in_cache: 0
  pin_top_level_index_and_filter: 1
  index_type: 0
  data_block_index_type: 0
  data_block_hash_table_util_ratio: 0.750000
  hash_index_allow_collision: 1
  checksum: 1
  no_block_cache: 0
  block_cache: 0x55b817c1c210
  block_cache_name: BinnedLRUCache
  block_cache_options:
    capacity : 536870912
    num_shard_bits : 4
    strict_capacity_limit : 0
    high_pri_pool_ratio: 0.000
  block_cache_compressed: (nil)
  persistent_cache: (nil)
  block_size: 4096
  block_size_deviation: 10
  block_restart_interval: 16
  index_block_restart_interval: 1
  metadata_block_size: 4096
  partition_filters: 0
  use_delta_encoding: 1
  filter_policy: rocksdb.BuiltinBloomFilter
  whole_key_filtering: 1
  verify_compression: 0
  read_amp_bytes_per_bit: 0
  format_version: 2
  enable_index_compression: 1
  block_align: 0

debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:        Options.write_buffer_size: 33554432
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:  Options.max_write_buffer_number: 2
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:          Options.compression: NoCompression
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                  Options.bottommost_compression: Disabled
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:       Options.prefix_extractor: nullptr
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:   Options.memtable_insert_with_hint_prefix_extractor: nullptr
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:             Options.num_levels: 7
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:        Options.min_write_buffer_number_to_merge: 1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:     Options.max_write_buffer_number_to_maintain: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:            Options.bottommost_compression_opts.window_bits: -14
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                  Options.bottommost_compression_opts.level: 32767
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:               Options.bottommost_compression_opts.strategy: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:         Options.bottommost_compression_opts.max_dict_bytes: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:         Options.bottommost_compression_opts.zstd_max_train_bytes: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                  Options.bottommost_compression_opts.enabled: false
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:            Options.compression_opts.window_bits: -14
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                  Options.compression_opts.level: 32767
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:               Options.compression_opts.strategy: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:         Options.compression_opts.max_dict_bytes: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:         Options.compression_opts.zstd_max_train_bytes: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                  Options.compression_opts.enabled: false
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:      Options.level0_file_num_compaction_trigger: 4
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:          Options.level0_slowdown_writes_trigger: 20
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:              Options.level0_stop_writes_trigger: 36
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                   Options.target_file_size_base: 67108864
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:             Options.target_file_size_multiplier: 1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                Options.max_bytes_for_level_base: 268435456
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: Options.level_compaction_dynamic_level_bytes: 1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:          Options.max_bytes_for_level_multiplier: 10.000000
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[0]: 1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[1]: 1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[2]: 1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[3]: 1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[4]: 1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[5]: 1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: Options.max_bytes_for_level_multiplier_addtl[6]: 1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:       Options.max_sequential_skip_in_iterations: 8
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                    Options.max_compaction_bytes: 1677721600
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                        Options.arena_block_size: 4194304
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:   Options.soft_pending_compaction_bytes_limit: 68719476736
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:   Options.hard_pending_compaction_bytes_limit: 274877906944
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:       Options.rate_limit_delay_max_milliseconds: 100
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                Options.disable_auto_compactions: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                        Options.compaction_style: kCompactionStyleLevel
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                          Options.compaction_pri: kMinOverlappingRatio
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: Options.compaction_options_universal.size_ratio: 1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: Options.compaction_options_universal.min_merge_width: 2
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: Options.compaction_options_universal.max_merge_width: 4294967295
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: Options.compaction_options_universal.max_size_amplification_percent: 200
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: Options.compaction_options_universal.compression_size_percent: -1
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: Options.compaction_options_universal.stop_style: kCompactionStopStyleTotalSize
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: Options.compaction_options_fifo.max_table_files_size: 1073741824
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: Options.compaction_options_fifo.allow_compaction: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                   Options.table_properties_collectors:
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                   Options.inplace_update_support: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                 Options.inplace_update_num_locks: 10000
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:               Options.memtable_prefix_bloom_size_ratio: 0.000000
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:               Options.memtable_whole_key_filtering: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:   Options.memtable_huge_page_size: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                           Options.bloom_locality: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                    Options.max_successive_merges: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                Options.optimize_filters_for_hits: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                Options.paranoid_file_checks: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                Options.force_consistency_checks: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                Options.report_bg_io_stats: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb:                               Options.ttl: 0
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: [db/version_set.cc:3757] Recovered from manifest file:/var/lib/ceph/mon/ceph-b/store.db/MANIFEST-000001 succeeded,manifest_file_number is 1, next_file_number is 3, last_sequence is 0, log_number is 0,prev_log_number is 0,max_column_family is 0,min_log_number_to_keep is 0

debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: [db/version_set.cc:3766] Column family [default] (ID 0), log number is 0

debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: EVENT_LOG_v1 {"time_micros": 1579251511118040, "job": 1, "event": "recovery_started", "log_files": [3]}
debug 2020-01-17T08:58:31.111+0000 7fd2a63f76c0  4 rocksdb: [db/db_impl_open.cc:583] Recovering log #3 mode 2
debug 2020-01-17T08:58:31.115+0000 7fd2a63f76c0  4 rocksdb: EVENT_LOG_v1 {"time_micros": 1579251511121779, "cf_name": "default", "job": 1, "event": "table_file_creation", "file_number": 4, "file_size": 1655, "table_properties": {"data_size": 753, "index_size": 28, "filter_size": 69, "raw_key_size": 115, "raw_average_key_size": 23, "raw_value_size": 630, "raw_average_value_size": 126, "num_data_blocks": 1, "num_entries": 5, "filter_policy_name": "rocksdb.BuiltinBloomFilter"}}
debug 2020-01-17T08:58:31.115+0000 7fd2a63f76c0  4 rocksdb: [db/version_set.cc:3036] Creating manifest 5

debug 2020-01-17T08:58:31.123+0000 7fd2a63f76c0  4 rocksdb: EVENT_LOG_v1 {"time_micros": 1579251511128655, "job": 1, "event": "recovery_finished"}
debug 2020-01-17T08:58:31.131+0000 7fd2a63f76c0  4 rocksdb: DB pointer 0x55b81883f200
debug 2020-01-17T08:58:31.135+0000 7fd28d539700  4 rocksdb: [db/db_impl.cc:777] ------- DUMPING STATS -------
debug 2020-01-17T08:58:31.135+0000 7fd28d539700  4 rocksdb: [db/db_impl.cc:778]
** DB Stats **
Uptime(secs): 0.0 total, 0.0 interval
Cumulative writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 GB, 0.00 MB/s
Cumulative WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s
Cumulative stall: 00:00:0.000 H:M:S, 0.0 percent
Interval writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 MB, 0.00 MB/s
Interval WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 MB, 0.00 MB/s
Interval stall: 00:00:0.000 H:M:S, 0.0 percent

** Compaction Stats [default] **
Level    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  L0      1/0    1.62 KB   0.2      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.4      0.00              0.00         1    0.004       0      0
 Sum      1/0    1.62 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.4      0.00              0.00         1    0.004       0      0
 Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.4      0.00              0.00         1    0.004       0      0

** Compaction Stats [default] **
Priority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
User      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.4      0.00              0.00         1    0.004       0      0
Uptime(secs): 0.0 total, 0.0 interval
Flush(GB): cumulative 0.000, interval 0.000
AddFile(GB): cumulative 0.000, interval 0.000
AddFile(Total Files): cumulative 0, interval 0
AddFile(L0 Files): cumulative 0, interval 0
AddFile(Keys): cumulative 0, interval 0
Cumulative compaction: 0.00 GB write, 0.07 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Interval compaction: 0.00 GB write, 0.07 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Stalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count

** File Read Latency Histogram By Level [default] **

** Compaction Stats [default] **
Level    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  L0      1/0    1.62 KB   0.2      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.4      0.00              0.00         1    0.004       0      0
 Sum      1/0    1.62 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.4      0.00              0.00         1    0.004       0      0
 Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0

** Compaction Stats [default] **
Priority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
User      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.4      0.00              0.00         1    0.004       0      0
Uptime(secs): 0.0 total, 0.0 interval
Flush(GB): cumulative 0.000, interval 0.000
AddFile(GB): cumulative 0.000, interval 0.000
AddFile(Total Files): cumulative 0, interval 0
AddFile(L0 Files): cumulative 0, interval 0
AddFile(Keys): cumulative 0, interval 0
Cumulative compaction: 0.00 GB write, 0.07 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Interval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Stalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count

** File Read Latency Histogram By Level [default] **

debug 2020-01-17T08:58:31.135+0000 7fd2a63f76c0  0 starting mon.b rank 0 at public addrs [v2:10.233.39.112:3300/0,v1:10.233.39.112:6789/0] at bind addrs [v2:10.233.114.16:3300/0,v1:10.233.114.16:6789/0] mon_data /var/lib/ceph/mon/ceph-b fsid 026382b9-c699-4f64-b14d-0109e6db6676
debug 2020-01-17T08:58:31.139+0000 7fd2a63f76c0  1 mon.b@-1(???) e0 preinit fsid 026382b9-c699-4f64-b14d-0109e6db6676
debug 2020-01-17T08:58:31.139+0000 7fd2a63f76c0  1 mon.b@-1(???) e0  initial_members a,b, filtering seed monmap
debug 2020-01-17T08:58:31.143+0000 7fd2a63f76c0  0 mon.b@-1(probing) e0  my rank is now 0 (was -1)
debug 2020-01-17T08:58:31.151+0000 7fd28fd3e700  0 mon.b@0(probing) e1  my rank is now -1 (was 0)
debug 2020-01-17T09:01:25.260+0000 7fd28fd3e700  1 mon.b@-1(probing) e1  adding peer [v2:10.233.17.5:3300/0,v1:10.233.17.5:6789/0] to list of hints
=== repeats a few times
debug 2020-01-17T09:08:29.305+0000 7fd28fd3e700  1 mon.b@-1(probing) e1  adding peer [v2:10.233.17.5:3300/0,v1:10.233.17.5:6789/0] to list of hints
debug 2020-01-17T09:08:31.137+0000 7fd28d539700  4 rocksdb: [db/db_impl.cc:777] ------- DUMPING STATS -------
debug 2020-01-17T09:08:31.137+0000 7fd28d539700  4 rocksdb: [db/db_impl.cc:778]
** DB Stats **
Uptime(secs): 600.0 total, 600.0 interval
Cumulative writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 GB, 0.00 MB/s
Cumulative WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s
Cumulative stall: 00:00:0.000 H:M:S, 0.0 percent
Interval writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 MB, 0.00 MB/s
Interval WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 MB, 0.00 MB/s
Interval stall: 00:00:0.000 H:M:S, 0.0 percent

** Compaction Stats [default] **
Level    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  L0      1/0    1.62 KB   0.2      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.4      0.00              0.00         1    0.004       0      0
 Sum      1/0    1.62 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.4      0.00              0.00         1    0.004       0      0
 Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0

** Compaction Stats [default] **
Priority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
User      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.4      0.00              0.00         1    0.004       0      0
Uptime(secs): 600.0 total, 600.0 interval
Flush(GB): cumulative 0.000, interval 0.000
AddFile(GB): cumulative 0.000, interval 0.000
AddFile(Total Files): cumulative 0, interval 0
AddFile(L0 Files): cumulative 0, interval 0
AddFile(Keys): cumulative 0, interval 0
Cumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Interval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Stalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count

** File Read Latency Histogram By Level [default] **

** Compaction Stats [default] **
Level    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  L0      1/0    1.62 KB   0.2      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.4      0.00              0.00         1    0.004       0      0
 Sum      1/0    1.62 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.4      0.00              0.00         1    0.004       0      0
 Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0

** Compaction Stats [default] **
Priority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
User      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.4      0.00              0.00         1    0.004       0      0
Uptime(secs): 600.0 total, 0.0 interval
Flush(GB): cumulative 0.000, interval 0.000
AddFile(GB): cumulative 0.000, interval 0.000
AddFile(Total Files): cumulative 0, interval 0
AddFile(L0 Files): cumulative 0, interval 0
AddFile(Keys): cumulative 0, interval 0
Cumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Interval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Stalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count

** File Read Latency Histogram By Level [default] **

debug 2020-01-17T09:08:31.305+0000 7fd28fd3e700  1 mon.b@-1(probing) e1  adding peer [v2:10.233.17.5:3300/0,v1:10.233.17.5:6789/0] to list of hints
===== repeats a few times
debug 2020-01-17T09:14:13.338+0000 7fd28fd3e700  1 mon.b@-1(probing) e1  adding peer [v2:10.233.17.5:3300/0,v1:10.233.17.5:6789/0] to list of hints
debug 2020-01-17T09:14:15.338+0000 7fd28fd3e700  1 mon.b@-1(synchronizing) e1  adding peer [v2:10.233.17.5:3300/0,v1:10.233.17.5:6789/0] to list of hints
===== repeats a few times
debug 2020-01-17T09:15:13.346+0000 7fd28fd3e700  1 mon.b@-1(synchronizing) e1  adding peer [v2:10.233.17.5:3300/0,v1:10.233.17.5:6789/0] to list of hints
===== repeats a few times
debug 2020-01-17T09:18:29.362+0000 7fd28fd3e700  1 mon.b@-1(probing) e1  adding peer [v2:10.233.17.5:3300/0,v1:10.233.17.5:6789/0] to list of hints
debug 2020-01-17T09:18:31.134+0000 7fd28d539700  4 rocksdb: [db/db_impl.cc:777] ------- DUMPING STATS -------
debug 2020-01-17T09:18:31.134+0000 7fd28d539700  4 rocksdb: [db/db_impl.cc:778]
** DB Stats **
Uptime(secs): 1200.0 total, 600.0 interval
Cumulative writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 GB, 0.00 MB/s
Cumulative WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s
Cumulative stall: 00:00:0.000 H:M:S, 0.0 percent
Interval writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 MB, 0.00 MB/s
Interval WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 MB, 0.00 MB/s
Interval stall: 00:00:0.000 H:M:S, 0.0 percent

** Compaction Stats [default] **
Level    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  L0      1/0    1.62 KB   0.2      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.4      0.00              0.00         1    0.004       0      0
 Sum      1/0    1.62 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.4      0.00              0.00         1    0.004       0      0
 Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0

** Compaction Stats [default] **
Priority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
User      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.4      0.00              0.00         1    0.004       0      0
Uptime(secs): 1200.0 total, 600.0 interval
Flush(GB): cumulative 0.000, interval 0.000
AddFile(GB): cumulative 0.000, interval 0.000
AddFile(Total Files): cumulative 0, interval 0
AddFile(L0 Files): cumulative 0, interval 0
AddFile(Keys): cumulative 0, interval 0
Cumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Interval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Stalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count

** File Read Latency Histogram By Level [default] **

** Compaction Stats [default] **
Level    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  L0      1/0    1.62 KB   0.2      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.4      0.00              0.00         1    0.004       0      0
 Sum      1/0    1.62 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.4      0.00              0.00         1    0.004       0      0
 Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0

** Compaction Stats [default] **
Priority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
User      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.4      0.00              0.00         1    0.004       0      0
Uptime(secs): 1200.0 total, 0.0 interval
Flush(GB): cumulative 0.000, interval 0.000
AddFile(GB): cumulative 0.000, interval 0.000
AddFile(Total Files): cumulative 0, interval 0
AddFile(L0 Files): cumulative 0, interval 0
AddFile(Keys): cumulative 0, interval 0
Cumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Interval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Stalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count

** File Read Latency Histogram By Level [default] **

debug 2020-01-17T09:18:31.362+0000 7fd28fd3e700  1 mon.b@-1(probing) e1  adding peer [v2:10.233.17.5:3300/0,v1:10.233.17.5:6789/0] to list of hints
===== repeats a few times
debug 2020-01-17T09:28:29.419+0000 7fd28fd3e700  1 mon.b@-1(probing) e1  adding peer [v2:10.233.17.5:3300/0,v1:10.233.17.5:6789/0] to list of hints
debug 2020-01-17T09:28:31.135+0000 7fd28d539700  4 rocksdb: [db/db_impl.cc:777] ------- DUMPING STATS -------
debug 2020-01-17T09:28:31.135+0000 7fd28d539700  4 rocksdb: [db/db_impl.cc:778]
** DB Stats **
Uptime(secs): 1800.0 total, 600.0 interval
Cumulative writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 GB, 0.00 MB/s
Cumulative WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s
Cumulative stall: 00:00:0.000 H:M:S, 0.0 percent
Interval writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 MB, 0.00 MB/s
Interval WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 MB, 0.00 MB/s
Interval stall: 00:00:0.000 H:M:S, 0.0 percent

** Compaction Stats [default] **
Level    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  L0      1/0    1.62 KB   0.2      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.4      0.00              0.00         1    0.004       0      0
 Sum      1/0    1.62 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.4      0.00              0.00         1    0.004       0      0
 Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0

** Compaction Stats [default] **
Priority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
User      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.4      0.00              0.00         1    0.004       0      0
Uptime(secs): 1800.0 total, 600.0 interval
Flush(GB): cumulative 0.000, interval 0.000
AddFile(GB): cumulative 0.000, interval 0.000
AddFile(Total Files): cumulative 0, interval 0
AddFile(L0 Files): cumulative 0, interval 0
AddFile(Keys): cumulative 0, interval 0
Cumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Interval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Stalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count

** File Read Latency Histogram By Level [default] **

** Compaction Stats [default] **
Level    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  L0      1/0    1.62 KB   0.2      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.4      0.00              0.00         1    0.004       0      0
 Sum      1/0    1.62 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.4      0.00              0.00         1    0.004       0      0
 Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0

** Compaction Stats [default] **
Priority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
User      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.4      0.00              0.00         1    0.004       0      0
Uptime(secs): 1800.0 total, 0.0 interval
Flush(GB): cumulative 0.000, interval 0.000
AddFile(GB): cumulative 0.000, interval 0.000
AddFile(Total Files): cumulative 0, interval 0
AddFile(L0 Files): cumulative 0, interval 0
AddFile(Keys): cumulative 0, interval 0
Cumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Interval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds
Stalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count

** File Read Latency Histogram By Level [default] **

debug 2020-01-17T09:28:31.415+0000 7fd28fd3e700  1 mon.b@-1(probing) e1  adding peer [v2:10.233.17.5:3300/0,v1:10.233.17.5:6789/0] to list of hints
===== repeats a few times
debug 2020-01-17T09:31:31.436+0000 7fd28fd3e700  1 mon.b@-1(probing) e1  adding peer [v2:10.233.17.5:3300/0,v1:10.233.17.5:6789/0] to list of hints
```

**Environment**:
* OS (e.g. from /etc/os-release):
```
NAME="Ubuntu"
VERSION="18.04.3 LTS (Bionic Beaver)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 18.04.3 LTS"
VERSION_ID="18.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic
```
* Kernel (e.g. `uname -a`):
`Linux ludwig 4.15.0-72-generic #81-Ubuntu SMP Tue Nov 26 12:20:02 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux`
* Cloud provider or hardware configuration:
Hetzner; Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz CPUs
* Rook version (use `rook version` inside of a Rook Pod):
```
rook: v1.1.0-beta.0.769.ga42d822
go: go1.11
```
* Storage backend version (e.g. for ceph do `ceph -v`):
`ceph version 15.0.0-9337-g4b7f5de (4b7f5de2353cc9e6d81c778d8e4c688de66cd6fd) octopus (dev)`
* Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:"1", Minor:"16", GitVersion:"v1.16.3", GitCommit:"b3cbbae08ec52a7fc73d334838e18d17e8512749", GitTreeState:"clean", BuildDate:"2019-11-13T11:23:11Z", GoVersion:"go1.12.12", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"16", GitVersion:"v1.16.3", GitCommit:"b3cbbae08ec52a7fc73d334838e18d17e8512749", GitTreeState:"clean", BuildDate:"2019-11-13T11:13:49Z", GoVersion:"go1.12.12", Compiler:"gc", Platform:"linux/amd64"}
```
* Kubernetes cluster type (e.g. Tectonic, GKE, OpenShift):
Bare, deployed using kubespray
* Storage backend status (e.g. for Ceph use `ceph health` in the [Rook Ceph toolbox](https://rook.io/docs/rook/master/ceph-toolbox.html)):
I don't get to that

<!-- Please take a look at our [Contributing](https://rook.io/docs/rook/master/development-flow.html)
documentation before submitting a Pull Request!
Thank you for contributing to Rook! -->

**Description of your changes:**
Currently, there is no option to turn on and off the log level verbosity in csi containers, This PR adds the functionality to select logging levels for csi containers.

Signed-off-by: Madhu Rajanna <madhupr007@gmail.com>

**Which issue is resolved by this Pull Request:**
Resolves #4690

**Checklist:**

- [ ] Reviewed the developer guide on [Submitting a Pull Request](https://rook.io/docs/rook/master/development-flow.html#submitting-a-pull-request)
- [ ] Documentation has been updated, if necessary.
- [ ] Unit tests have been added, if necessary.
- [ ] Integration tests have been added, if necessary.
- [ ] Pending release notes updated with breaking and/or notable changes, if necessary.
- [ ] Upgrade from previous release is tested and upgrade user guide is updated, if necessary.
- [ ] Code generation (`make codegen`) has been run to update object specifications, if necessary.
- [ ] Comments have been added or updated based on the standards set in [CONTRIBUTING.md](https://github.com/rook/rook/blob/master/CONTRIBUTING.md#comments)
- [ ] Add the flag for skipping the CI if this PR does not require a build. See [here](https://github.com/rook/rook/blob/master/INSTALL.md#skip-ci) for more details.
[test ceph]
So not sure if doing something wrong but have been trying to set higher cpu for prepareosd since node im trying to set osd has so many lvm devices . Want to provide more cpu . 

cephcluster crd for resources was set like this 
```
  resources:
    mgr:
      limits:
        cpu: 500m
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 2Gi
    mon:
      limits:
        cpu: 500m
        memory: 1Gi
      requests:
        cpu: 500m
        memory: 1Gi
    osd:
      limits:
        cpu: 500m
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 2Gi
    prepareosd:
      limits:
        cpu: 500m
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 2Gi
```

the job was created with correct memory however only have 1m of cpu

```
        resources:
          limits:
            cpu: 1m
            memory: 2Gi
          requests:
            cpu: 1m
            memory: 2Gi
```

when resources were set to empty limit/requests for cpu and memory was set to 0 

Not sure if anyone else has seen this. Will try to change things around to see if it still occurs
Signed-off-by: Dustin Guerrero <dguerrero@twilio.com>

**Description of your changes:**
Changes device filter log messages to only print when a match actually occurs.

**Which issue is resolved by this Pull Request:**
Resolves #4711 

**Checklist:**

- [ ] Reviewed the developer guide on [Submitting a Pull Request](https://rook.io/docs/rook/master/development-flow.html#submitting-a-pull-request)
- [x] Documentation has been updated, if necessary.
- [x] Unit tests have been added, if necessary.
- [x] Integration tests have been added, if necessary.
- [x] Pending release notes updated with breaking and/or notable changes, if necessary.
- [x] Upgrade from previous release is tested and upgrade user guide is updated, if necessary.
- [x] Code generation (`make codegen`) has been run to update object specifications, if necessary.
- [x] Comments have been added or updated based on the standards set in [CONTRIBUTING.md](https://github.com/rook/rook/blob/master/CONTRIBUTING.md#comments)
- [ ] Add the flag for skipping the CI if this PR does not require a build. See [here](https://github.com/rook/rook/blob/master/INSTALL.md#skip-ci) for more details.

**Is this a bug report or feature request?**
* Bug Report

**Deviation from expected behavior:**
The following log message prints out when scanning devices
```
2020-01-16 23:01:37.564669 I | cephosd: device "sdc" (aliases: "/dev/disk/by-id/ata-SSDSC2KB960G8R_BTYF930201Z2960CGN /dev/disk/by-id/wwn-0x55cd2e41514802d9 /dev/disk/by-path/pci-0000:18:00.0-scsi-0:0:4:0") matches device path filter "/dev/disk/by-path/pci-0000:18:00.0-scsi-0:0:2:0": false
2020-01-16 23:01:37.564682 I | cephosd: skipping device "sdc" that does not match the device filter/list ([{/dev/disk/by-path/pci-0000:18:00.0-scsi-0:0:2:0 1  0  false true}]). <nil>
```

This is misleading because it looks like the device path filter successfully matches the device, when it actually doesn't. In this example it looks like `/dev/disk/by-path/pci-0000:18:00.0-scsi-0:0:4:0` is matched by the filter `/dev/disk/by-path/pci-0000:18:00.0-scsi-0:0:2:0 1`. The log message should only print if the device path filter actually matches.

https://github.com/rook/rook/blob/a42d8222761544bc103f9bdcbff54592ca98dddf/pkg/daemon/ceph/osd/daemon.go#L339

**Expected behavior:**

**How to reproduce it (minimal and precise):**
Setup a devicePathFilter (search on https://rook.io/docs/rook/v1.2/ceph-cluster-crd.html)
Tail the logs of the node prepare job (e.g. `rook-ceph-osd-prepare-kubeminionrook0016`)


**Environment**:
* Rook version (use `rook version` inside of a Rook Pod):
rook: v1.2.1

<!-- Please take a look at our [Contributing](https://rook.io/docs/rook/master/development-flow.html)
documentation before submitting a Pull Request!
Thank you for contributing to Rook! -->

**Description of your changes:**
The integration tests only have a single node so the upgrade checks do not provide any real safety for the cluster during the upgrade. We will be able to improve the reliability and speed of the tests by disabling these checks. In particular, the filesystem update spends a lot of time waiting for the mds daemons.

**Checklist:**

- [ ] Reviewed the developer guide on [Submitting a Pull Request](https://rook.io/docs/rook/master/development-flow.html#submitting-a-pull-request)
- [ ] Documentation has been updated, if necessary.
- [ ] Unit tests have been added, if necessary.
- [ ] Integration tests have been added, if necessary.
- [ ] Pending release notes updated with breaking and/or notable changes, if necessary.
- [ ] Upgrade from previous release is tested and upgrade user guide is updated, if necessary.
- [ ] Code generation (`make codegen`) has been run to update object specifications, if necessary.
- [ ] Comments have been added or updated based on the standards set in [CONTRIBUTING.md](https://github.com/rook/rook/blob/master/CONTRIBUTING.md#comments)
- [ ] Add the flag for skipping the CI if this PR does not require a build. See [here](https://github.com/rook/rook/blob/master/INSTALL.md#skip-ci) for more details.

[test ceph]
[test full]