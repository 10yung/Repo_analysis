nFeatures is currently a scalar, but if its a vector, we should search over the vector
Currently, if you build a prtFeatSelSfs with nFeatures = 10, and the performance goes : [1 2 3 4 5 4 3 2 1 0], dsOut = featSel.run(ds) will always give you the dsOut with 10 features despite 5 features being better.  

Propose: add parameter "selectBestNumberFeatures = false".  During RUNif the best performance was achieved with fewer than nFeatures, then just select that # features.

<pre>

Undefined variable "inputStructure" or class "inputStructure.handleEmptyClusters".

Error in prtClusterKmodes/trainAction (line 135)
                        switch lower(inputStructure.handleEmptyClusters)
</pre>


Checkout this revision: 85b6087b2917014530096c4f65c78c15189f966a

then run:
<pre>
ds = prtDataGenUnimodal;
class = prtClassFld;
ds = class.kfolds(ds,10);
disp(ds)
</pre>

You will see ds has userData is a [10 x 1] struct.    This bit me in the ass when I was running some cross-validation experiments, because:
<pre>
ds = class.kfolds(ds,100);
ds = class.kfolds(ds,100);
ds = class.kfolds(ds,100);
</pre>
Will make a very large userData structure.  Because in the first instance we make a ds with a 1x100 userData, and then we make a 1 x (100*100) userData, then (1 x (100*100*100)) etc.  That's... no good.

Note that, as far as I can tell, this does NOT happen in the parent commit: f441f1ce2b9ad1e763a1efebffb9686c158fc074

The problem is that userData was never intended to be used to hold information from different cross-validation folds - userData was always intended to be a single, global set of information for the dataSet, and mucking with it inside kfolds breaks that assumption.  


The bug above only really matters if you run cross-validaton on the same data set a bunch of times - it is admittedly not the worst problem in the world, but it can be weird.   If userData was actually BIG this problem can be significant - I don't necessarily want 1000 copies of userData for my 1000 folds.  

I don't know what the right answer is.  Usually I'd suggest putting whatever this information that you want to store in userData on every fold in the ACTION (and use the second output of kfolds or crossValidate) but that's clearly sub-optimal and confusing.  

We could ADD a new field to dataSets - kfoldsUserData, which is an nFolds x 1 array of userData?  Then in cross-Validate, we can change  prtDataSetBase.crossValidateCombineFoldResults to do this:

<pre>
    for i = 1:length(dsTestCell)
         dsOut.kfoldsUserData(i) = dsTestCell(i).userData;
    end
</pre>

Thoughts?
We can have multiple "Targets" - it would be nice to enforce a hierarchy on the targets - so that plots, displays, etc., knew to group by T1, then group by T2, for, e.g., plots.

In landmines you might have two labels you're interested in exploring - "Lane" and "Mine/No-Mine".  It would be nice to say:

<pre>
ds.hierarchicalLabeling = {'Lane','isTarget'};
ds.plot(); 
</pre>

and have the resulting data plot use, e.g., color for lanes, and within a color, use shape for isTarget.

The same could be done in imagesc, etc.  

The exact implementation details are unclear.  Use multiple target columns?  Multiple target columns probably breaks a ton of classifier code.  Use fields in the observationInfo?  Other?
The help for prtDataSetClass.bootstrap says it uses sampling with replacement. I would like to randomly subsample my training data without getting redundant observations. Does that functionality exist in the PRT?
I wrote a new prtClass in which metadata is written out to DataSet.userData in the runAction() method.

However, when I call crossValidate() on that prtClass or use it as a base classifier in prtClassBinaryToMaryOneVsAll, the userData is not saved to the output.

I would like it such that userData was saved to the DataSet that comes out of either prtClass.crossValidate() or prtClassBinaryToMaryOneVsAll.runAction() Is this an easy internal PRT fix?



I am trying to implement prtRegressLibSvm, but LibSVM is seg-faulting on me.  Is this our bad?

Here's the simplest version I can make:


ds = prtDataGenNoisySinc;
model = prtExternal.libsvm.svmtrain(ds.Y, ds.X,['-s 3']);
[dontNeed, dontNeed, decision_values] = prtExternal.libsvm.svmpredict(ds.Y, ds.X, model);

Yikes?

This is preventing me from making prtRegressLibSvm...

A bunch of objects (e.g. prtRv.plotPdf, plotLogPdf, classifier plots, etc.) all call

prtPlotUtilPlotGriddedEvaledFunction

or some equivalent.  It would be nice if we could pass input arguments into that method cleanly.  So you could specify, e.g., parameters to slice, or imagesc.

I did this in prtRv.plot so you can do:

close all; 
plot(dsPrtPreProc); 
hold on; 
g = prtRvGmm
g.plotPdf(axis,'slicerLocations',{4,4,[0 20]},'FaceAlpha',0.5);

But it's not ideal - e.g., the first input to plotPdf is the axis limits for some reason.  That should be:

g,plotPdf('axisLimits',axis,...)

The generalization of this is not straightforward to me though.  Should there be:

prtUtilDefaultImagescVarargin?

Or did we miss the boat here, and we need a 

prtPlotUtilPlotGriddedEvaled OBJECT?
