This ports the neutra example from numpyro to test the NeutraReparam reparametrizer. I am not seeing good results with the IAFNormal autoguide and specially with the default setting where it uses a single flow.

TODOs:
 - [ ] Try on another dataset - maybe high dimensional Neal's funnel.
 - [ ] Have a more general flow based autoguide to wrap over BNAF which is not currently available as an autoguide.
 - [ ] The samples from the warped posterior look far from gaussian, and this probably needs to be investigated. 

[neutra.pdf](https://github.com/pyro-ppl/pyro/files/4078250/neutra.pdf)

This PR fixes a logic gap for assign `self.transforms` in the `MCMC` api.
It can be the case that both `MCMC.transforms is None` and `MCMC.kernel.transforms is None`
in which case MCMC.transforms should be set to an empty dict.
This bug appears when using HMC/NUTS with a potential_fn.

Note sure on the style guide for inline comments so will leave them as is.
Tasks:
- [ ] Reuse traces across wake-theta and wake-phi loss computations
- [ ] Attempt to remove zero-expectation terms, similar to `pyro.infer.Dice.compute_expectation`
- [ ] Add stronger tests and tests for new `ops.packed` utilities
- [x] Add example
pyro 1.1.0 `sv-dkl.py` running error when enable trace.  Was exactly running `https://github.com/pyro-ppl/pyro/blob/dev/examples/contrib/gp/sv-dkl.py` with only `--jit` set to `True` on line https://github.com/pyro-ppl/pyro/blob/7692a503b64dc8200049e1c24a775d2170ba8039/examples/contrib/gp/sv-dkl.py#L186 , default setting can successfully be running. 

Running in conda (4.7.12) environment with `py36-pyro-tutorial.yml` set as:


```
name: py36-pyro-tutorial
dependencies:
  # Core
  - python = 3.6
  - numpy
  - pandas
  - seaborn
  - jupyter
  - torchvision
  - pillow = 6.2.1

  # Language tools
  - pip

  - pip:
    - pyro-ppl == 1.1.0 # 1.0 requires pytorch and torch to be higher version
    - pyro-ppl[extras]
```

### Guidelines

**NOTE:** Issues are for bugs and feature requests only. If you have a question about using Pyro or general modeling questions, please post it on the [forum](https://forum.pyro.ai/).

If you would like to address any minor bugs in the documentation or source, please feel free to contribute a Pull Request without creating an issue first. 

Please tag the issue appropriately in the title e.g. [bug], [feature request], [discussion], etc.

Please provide the following details:
--------------------------------------------------------------------------------------------------
### Issue Description
Provide a brief description of the issue.

### Environment
For any bugs, please provide the following:
 - OS and python version.  
 - PyTorch version, or if relevant, output of `pip freeze`.
 - Pyro version: output of `python -c 'import pyro; print pyro.__version__'`


python 3.6, pyro 1.1.0, conda 4.7.12

### Code Snippet
Provide any relevant code snippets and commands run to replicate the issue.

```
/home/{user}/miniconda3/envs/py36-pyro-tutorial/lib/python3.6/site-packages/pyro/contrib/gp/kernels/isotropic.py:45: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if X.size(1) != Z.size(1):
/home/{user}/miniconda3/envs/py36-pyro-tutorial/lib/python3.6/site-packages/pyro/contrib/gp/models/vsgp.py:106: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  Kuu.view(-1)[::M + 1] += self.jitter  # add jitter to the diagonal
/home/{user}/miniconda3/envs/py36-pyro-tutorial/lib/python3.6/site-packages/pyro/distributions/util.py:239: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  eye.view(-1)[:min(m, n) * n:n + 1] = 1
/home/{user}/miniconda3/envs/py36-pyro-tutorial/lib/python3.6/site-packages/pyro/distributions/util.py:239: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  eye.view(-1)[:min(m, n) * n:n + 1] = 1
/home/{user}/miniconda3/envs/py36-pyro-tutorial/lib/python3.6/site-packages/pyro/contrib/gp/likelihoods/multi_class.py:56: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if f_swap.size(-1) != self.num_classes:
Traceback (most recent call last):
  File "sv-dkl.py", line 195, in <module>
    main(args)
  File "sv-dkl.py", line 157, in main
    train(args, train_loader, gpmodule, optimizer, loss_fn, epoch)
  File "sv-dkl.py", line 68, in train
    loss = loss_fn(gpmodule.model, gpmodule.guide)
  File "/home/{user}/miniconda3/envs/py36-pyro-tutorial/lib/python3.6/site-packages/pyro/infer/trace_mean_field_elbo.py", line 160, in differentiable_loss
    return self._differentiable_loss(*args, **kwargs)
  File "/home/{user}/miniconda3/envs/py36-pyro-tutorial/lib/python3.6/site-packages/pyro/ops/jit.py", line 92, in __call__
    self.compiled[key] = torch.jit.trace(compiled, params_and_args, **self.jit_options)
  File "/home/{user}/miniconda3/envs/py36-pyro-tutorial/lib/python3.6/site-packages/torch/jit/__init__.py", line 883, in trace
    _force_outplace)
  File "/home/{user}/miniconda3/envs/py36-pyro-tutorial/lib/python3.6/site-packages/pyro/ops/jit.py", line 85, in compiled
    return poutine.replay(self.fn, params=constrained_params)(*args, **kwargs)
  File "/home/{user}/miniconda3/envs/py36-pyro-tutorial/lib/python3.6/site-packages/pyro/poutine/messenger.py", line 8, in _context_wrap
    return fn(*args, **kwargs)
  File "/home/{user}/miniconda3/envs/py36-pyro-tutorial/lib/python3.6/site-packages/pyro/infer/trace_mean_field_elbo.py", line 154, in differentiable_loss
    _, loss_particle = self._differentiable_loss_particle(model_trace, guide_trace)
  File "/home/{user}/miniconda3/envs/py36-pyro-tutorial/lib/python3.6/site-packages/pyro/infer/trace_mean_field_elbo.py", line 99, in _differentiable_loss_particle
    kl_qp = kl_divergence(guide_site["fn"], model_site["fn"])
  File "/home/{user}/miniconda3/envs/py36-pyro-tutorial/lib/python3.6/site-packages/torch/distributions/kl.py", line 165, in kl_divergence
    return fun(p, q)
  File "/home/{user}/miniconda3/envs/py36-pyro-tutorial/lib/python3.6/site-packages/pyro/distributions/kl.py", line 23, in _kl_independent_independent
    kl = kl_divergence(p, q)
  File "/home/{user}/miniconda3/envs/py36-pyro-tutorial/lib/python3.6/site-packages/torch/distributions/kl.py", line 165, in kl_divergence
    return fun(p, q)
  File "/home/{user}/miniconda3/envs/py36-pyro-tutorial/lib/python3.6/site-packages/torch/distributions/kl.py", line 385, in _kl_multivariatenormal_multivariatenormal
    p._unbroadcasted_scale_tril.shape[:-2])
```

This issue proposes a new statement in Pyro: a `pyro.barrier` statement to allow interaction between an inference algorithm and a model's local state. The interface is
```py
thing = pyro.barrier(thing)
```
where `thing` is a recursive data structure whose elements may be
1.  Python atoms, which will be unaffected
2.  Python collections, which will be recursed, roughly
    ```py
    barrier([x,y,z]) = [barrier(x), barrier(y), barrier(z)]
    ```
3. PyTorch tensors with named dimensions (or Funsors with named free variables). Note this feature crucially relies on PyTorch named dimensions to pass event_dim metadata through arithmetic computations in user code.

Using `pyro.barrier` we can re-implement SMC in a more Pyronic way, avoding restrictions on an `init`/`step` interface. For example we could implement SMC via effects acting on a `pyro.barrier` statement in the following annotated model:
```py
def model(data):
    z = pyro.sample("z_init", Normal(0,1))   # latent state
    k = 0 * z                                # control
    cost = 0 * z                             # cumulative cost of controller
    for t, x in enumerate(data):
        z,k,cost = pyro.barrier((z,k,cost))  # triggers resampling
        k = torch.where(z > 10, k + 1, k)
        k = torch.where(z < 10, k + 1, k)
        k = torch.where(-10 <= z & z <= 10, 0 * k, k)
        z = pyro.sample(f"z_{t}", Normal(z+k,1))
        pyro.sample(f"x_{t}", Normal(z,1), obs=x)
    return cost
```
While in Pyro this statement has limited power, supporting only a few features like SMC resampling and enumeration-delayed sampling, it becomes more powerful in a PPL with better support for lazy computation such as `funsor.minipyro`. I'd like to add this to the `pyroapi` so we can experiment with more inference algorithms in a generic way.

For details see draft [design doc](https://github.com/fritzo/barrier-design-doc/blob/master/main.pdf) and draft Funsor implementation https://github.com/pyro-ppl/funsor/pull/295 
I've continued the feature request of @bmazoure in #2066 / #2068, changing a number of the implementation details, adding tests, docs, etc.

This should be good to go for implementing Glow normalizing flow, and Neural Spline FLow
This is a new transform for reshaping the event shape of a random variable akin to TensorFlow:

[https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/Reshape](https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/Reshape)

I had to make some changes to `TransformedDistribution` to make it reshape-aware, and created a modified version in `pyro.distributions`.
The implementation will assume that `df > 2` throughout.

Right now, there is nothing implemented yet. I'll add more details below. The math is mostly corresponding to GammaGaussian op.

#### Reference

+ Robust Bayesian Filtering and Smoothing Using Student’s t Distribution https://arxiv.org/pdf/1703.02428.pdf

#### Tasks

- [x] Implement StudentT op, except the sum method. 
- [x] Consider to implement the "approximated" sum method <-  I'm not sure if this is necessary. **Edit:** this is necessary, and is the most tricky part.
- [x] Implement the "approximated" tensordot
- [x] Add test to verify that this match the sequential approximation mechanism in the above reference
- [x] Add tests similar to Gaussian/GammaGaussian ops.
co-authored with @iffsid 

Statistically, batching allows us to trade off the speed of taking gradient steps for lower variance gradient estimators. This can be done in a for loop. However, batching is most useful for parallelizing the computation of the log probs of the model and the guide by using low level numerical optimizations.

Plating is a useful construct for defining conditionally independent but identical random variables. While this is useful for defining hierarchical models, it is unsuited for batching since it **changes the definition of the model**. This leads to issues like IWAE-based algorithms failing silently, additional multiplicative factors in ELBO-based gradients, and being able to define nonsensical guides.

We propose defining a special keyword for plates that are supposed to indicate batching which circumvents the above problems.

### More details

Consider a model `p(z)p(x|z)` where `z` and `x` can have multiple sites and each site can have different shapes, depending on the distribution batch shapes but also the plates internal to the model. Given a dataset of training `xs`, we often want to take the gradient of `log p(x)`, averaged over a batch (subset of `xs`), as an estimator of `E_{xs}[grad log p(x)]`. In [practice](https://pyro.ai/examples/vae.html), it is [suggested](https://pyro.ai/examples/svi_part_ii.html#Subsampling-when-there-are-only-local-random-variables) that we **change** the model by adding a batching plate so that the model becomes `p(z_{1:B}, x_{1:B}) = prod_b p(z_b)p(x_b | z_b)`.

#### Problem 1: IWAE-based objectives fail silently

In IWAE-based objectives like [RenyiELBO](https://github.com/pyro-ppl/pyro/blob/dev/pyro/infer/renyi_elbo.py) and [RWS](https://github.com/pyro-ppl/pyro/blob/dev/pyro/infer/rws.py), we want to compute the model objective as `logsumexp` over the particle dimension and then average over the batch dimension. However, since there is no distinction between a batch dimension and a dimension of an actual plate, IWAE-based models are forced to sum over the batch dimension before taking the `logsumexp` over the particle dimension ([here](https://github.com/pyro-ppl/pyro/blob/dev/pyro/infer/renyi_elbo.py#L99) and [here](https://github.com/pyro-ppl/pyro/blob/dev/pyro/infer/rws.py#L130)). This is wrong since the order of these operation matters.

#### Problem 2: ELBO-based gradients have wrong multiplicative factors

We’re interested in estimating `E_{xs}[grad log p(x)]` using a Monte Carlo estimator, which boils down to taking an **average** of `log p(x)` over `xs`. However, if we change the model from `p(z, x)` to `p(z_{1:B}, x_{1:B})` using a plate in order to accomplish this, the resulting estimator will instead be a **sum** of `log p(x)` over `xs` which is off by a multiplicative factor of `B`. While this is fine if we use adaptive optimizers like Adam, it is wrong for non-adaptive ones like SGD.

#### Problem 3: Ability to define nonsensical guides

Consider the case of VAEs where latent vectors have dimension `D_z` and data has dimension `D_x`. If we change the model to `p(z_{1:B}, x_{1:B})`, we have the ability to write the guide to take in `[B, D_x]` observations and output a distribution over `[B, D_z]` which is not necessarily independent in the batch dimension, e.g. a multivariate normal distribution over `B * D_z` dimensions. This is clearly nonsensical for the VAE model where we just want the guide to map from data of shape `[D_x]` to distributions on vectors of shape `[D_z]`. While this is not as big as a problem as the previous ones, it is an indication that changing the model definition in order to do batching is unnatural.

#### Proposed solution: Use a plate with a special name for batching

Users will still deal with batching like before---by adding a plate around the body of model manually---but they are forced to name this plate using a string with a special keyword value. This keyword is then used internally by IWAE-based and ELBO-based objectives to compute the correct gradient estimators. This doesn’t solve the third problem.

Mentioned by @fritzo in https://github.com/pyro-ppl/pyro/pull/2196#issuecomment-560525534. Also related - https://github.com/pyro-ppl/pyro/pull/237.

Here is one reason to change it (which might not have been explicitly mentioned in that PR). I wanted to change our tutorials / examples to start using `pyro.deterministic` but the following behavior (under Predictive) will be unintuitive for users. It is obvious what is happening once you realize deterministic is an alias for a particular sample statement, and how broadcasting works (in the presence of event dims etc. which is probably too much to ask).

```python
    def forward(self, x, y=None):
        sigma = pyro.sample("sigma", dist.Uniform(0., 10.))
        mean = pyro.deterministic("mean", self.linear(x).squeeze(-1))
        with pyro.plate("data", x.shape[0]):
            obs = pyro.sample("obs", dist.Normal(mean, sigma), obs=y)
```

Here, the size of the mean variable will be `[1, len(x)]` under Predictive and not `[len(x)]` which one might expect. The reason is that `mean` is just a sample site under the hood and will prepend `1` to the left based on `max_plate_nesting` (in this case `1`, but since the remaining dims of mean are considered event dims we prepend 1).

