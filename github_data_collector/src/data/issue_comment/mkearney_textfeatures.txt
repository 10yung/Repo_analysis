Thank you so much for your excellent work on this package! ðŸ™Œ 

I have some questions about the way the `w` results are presented/documented. The [function documentation](https://github.com/mkearney/textfeatures/blob/a960fb243f56848a675c2acaa13c39ad4400a9b6/R/word2vec.R#L19) uses some language about [word2vec](https://en.wikipedia.org/wiki/Word2vec), but in the functions that create these results, only functions from text2vec for topic modeling are called, not any vector embedding functions.

The text2vec package has functions for [LDA topic modeling](http://text2vec.org/topic_modeling.html) and also for [word embeddings](http://text2vec.org/glove.html), specifically an implementation of [GloVe](https://nlp.stanford.edu/projects/glove/). If I am understanding correctly, it looks like only the first are used in this package, but the language describing the results refers to the second.

What are your thoughts on this? Do you think the language and naming should be updated?
Provides a concrete example for #6 within documentation. I would be happy to work on a PR with a more traditional interface (modeling function + predict method) if that is something you would like. 
sorry for such a lame pull request!
Hi, Could you explain/clarify how to apply train preprocessing to new data?


**newdata - If a textfeatures_model is supplied to text, supply this with new data to which you would like ???to apply the textfeatures_model.???**

Cause after ```textfeatures function``` I get a data frame.


I'm missing some information about where the `politeness_dict` is coming from. If you supply the source I don't mind adding the documentation! 