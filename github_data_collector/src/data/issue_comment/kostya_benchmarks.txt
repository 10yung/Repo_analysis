This is a wonderful find and Halvak is the longest code and I don't understand what it does.

Was the code in:
https://github.com/kostya/benchmarks/blob/master/havlak/havlak.go

Based on this with modified code?
https://github.com/chandradeepak/benchgraffiti/blob/master/havlak/havlak3.go

I can still compile original code from 2011 on Golang 1.13.5 without any modifications.
**rsc's Havlak (havlak6.go) with pool took:** 2 seconds (87MB memory)
**Kostya's Havlak in this repo took:** 26 seconds (350MB memory)

Russ Cox works on Go language.
https://github.com/chandradeepak/benchgraffiti/tree/master/havlak

Have a great holiday!
I compared Rust and Cpp versions and I found out that most of performance difference is branch misprediction. It also drastically changes from CPU to CPU becuase it relies on CPU internals.

I found that changing `match *op` on `if let` chain fixes this problem. Although I'm notsure if it follows your "Used data structures are idiomatic.". I mean it's more idiomatic to use `match` in these cases but what makes Rust itself is you could rewrite your code a bit to get better performance.

What do you think? On my machine I've got 1.45 rust vs 1.388 cpp
Today I came across [mimalloc](https://github.com/microsoft/mimalloc ), probably the most "stable" `malloc()` replacement and think it's wise to use it for all languages to avoid sensitivity to unstable performance behavior in different workloads (benchmarks).

Feel free to use the LD_PRELOAD hack from mimalloc's README for all tests in the Docker image.
Adding the size in bytes for the executable binaries (or VM bytecode) generated by the compiler would be very interesting.  Bonus points for also mentioning size sum of ``ldd`` library dependencies.

There's the [dom96/binary_size](https://github.com/dom96/binary_size) repo showing that executables produced by Nim are much leaner than Crystal, Rust, D, Haskell, and Go for a "Hello World" example.  Adding this column to this benchmark would show the difference for non-trivial examples and many more languages, with very little extra effort if automated.
https://ziglang.org
https://github.com/ziglang/zig

Thanks.
I made it faster and memory efficient by using ets tables.
Below is a statistical report generated by Benchee

Name               ips        average  deviation         median         99th %
ets            0.00689       2.42 min     ±0.00%       2.42 min       2.42 min
original       0.00624       2.67 min     ±0.00%       2.67 min       2.67 min

Comparison:
ets            0.00689
original       0.00624 - 1.10x slower +0.25 min

Memory usage statistics:

Name        Memory usage
ets             13.12 GB
original       100.76 GB - 7.68x memory usage +87.65 GB

# system
    available_memory: "3.85 GB",
    cpu_speed: "Intel(R) Xeon(R) Platinum 8168 CPU @ 2.70GHz",
    elixir: "1.9.1",
    erlang: "22.0.7",
    num_cores: 2,
    os: :Linux

Vala language: https://wiki.gnome.org/Projects/Vala
Better code style (use Kotlin stdlib rather than Java's where possible).
Avoid unnecessary allocations.
Removed warming.

On my machine, with Oracle's JDK 8, I observed an improvement when running the warming program from 1.3 seconds to 1.2 seconds.

This is extremely fast, much faster than Go for example, even including the JVM startup time!

Even the Java optimised implementation I submitted on https://github.com/kostya/benchmarks/pull/173, running with the GraalVM JIT, is not as fast!
This PR implements brainfuck2 in Java that runs much faster than the current implementation.

The code is still pretty clean and very readable, besides being [more testable](https://github.com/renatoathaydes/bf-jvm/blob/master/src/test/kotlin/BfTest.kt).

On my system, here are the figures I am getting comparing the new implementation with the current one, as well as with the Kotlin (current fastest in the project README) implementation:

> * in seconds

| Java (current) | Java (new) | Kotlin |
|------------------|-----------------|--------|
| 2.6 | 1.4 | 2.0 |

The Kotlin implementation is also not optimal, I will make a PR offering a faster implementation for it as well.
Like perl I would add a base64 module for encode/decode function to get better performance for python. Factor10 for python is available.