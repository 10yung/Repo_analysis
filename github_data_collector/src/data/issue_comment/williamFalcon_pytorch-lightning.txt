 üêõ Bug

<!-- A clear and concise description of what the bug is. -->
`imagenet_example` cannot be executed

### To Reproduce

Steps to reproduce the behavior:

1. Download imagenet
2. `cd pl_examples/full_examples/imagenet`
3. `python imagenet_example.py --data-path imagenet_path`

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->
Error:

```
Traceback (most recent call last):
  File "imagenet_example.py", line 248, in <module>
    main(get_args())
  File "imagenet_example.py", line 229, in main
    model = ImageNetLightningModel(hparams)
TypeError: Can't instantiate abstract class ImageNetLightningModel with abstract methods forward
```

### Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

The example should run.

### Environment

PyTorch version: 1.3.1
Is debug build: No
CUDA used to build PyTorch: 10.1.243

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: Could not collect

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: GeForce RTX 2080 Ti
Nvidia driver version: 440.44
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.18.1
[pip3] pytorch-lightning==0.6.0
[pip3] torch==1.3.1
[pip3] torchvision==0.4.2
[conda] blas                      1.0                         mkl  
[conda] mkl                       2019.4                      243  
[conda] mkl-service               2.3.0            py37he904b0f_0  
[conda] mkl_fft                   1.0.14           py37ha843d7b_0  
[conda] mkl_random                1.1.0            py37hd6b4f25_0


### Additional context

<!-- Add any other context about the problem here. -->
I suspect that there was breaking changes in the API and that the example is obsolete as the method `forward` is now expected.
## üêõ Bug

PyTorch LR schedulers now shouldn't get any arguments in `step` function, see [here](https://github.com/pytorch/pytorch/blob/9e9bfbfd8d893cdf8205db67f220551214787d7f/torch/optim/lr_scheduler.py#L13-L20) and [here](https://github.com/pytorch/pytorch/issues/31828).

Looks like the calls in PytorchLightning are not in line with the new interface, see [here](https://github.com/PyTorchLightning/pytorch-lightning/blob/dac59bb8d354f28b919d08aa369a7db6ab31bfa6/pytorch_lightning/trainer/training_loop.py#L334-L337).

This results in unexpected LR changes. Removing the `epoch` argument from `step` call solves the issue for me.

## Environment
PyTorch 1.4
PyTorchLightning 0.5.3.2
## üöÄ Feature
Experiment with novel Github CI - action

### Motivation
having builds for all main os in single CI, so we can drop Travis and Appveyo 
We would keep CircleCI for future GPU testing... 
## Bug

Fitting with `log_gpu_memory=True` in the Trainer fails in python3.6 version.

### To Reproduce

1. Use python3.6 version
2. Create any trainer with `log_gpu_memory=True` option.
3. Then fit it.
3. See error:

```
/a/pytorch-lightning/pytorch_lightning/core/memory.py in get_gpu_memory_map()
    237         encoding='utf-8',
    238         capture_output=True,
--> 239         check=True)
    240     # Convert lines into a dictionary
    241     gpu_memory = [int(x) for x in result.stdout.strip().split(os.linesep)]

/usr/lib/python3.6/subprocess.py in run(input, timeout, check, *popenargs, **kwargs)
    421         kwargs['stdin'] = PIPE
    422 
--> 423     with Popen(*popenargs, **kwargs) as process:
    424         try:
    425             stdout, stderr = process.communicate(input, timeout=timeout)

TypeError: __init__() got an unexpected keyword argument 'capture_output'

```


#### Code sample

```
trainer = Trainer(
    log_gpu_memory=True,
   # ....
)
trainer.fit()
```

### Expected behavior

For the same code there is no errors for python3.7

### Environment

pytorch:          1.2.0
Ubuntu 18.04
pytorch-lightning: 
    - installed to pip environment
    - commit 7a1df80f4e98fca
    - python setup.py develop
    - version 0.6.0 
python:         3.6.8
cuda: 10.0, V10.0.130 
cudnn:          7.6.2
GPU: RTX 2080 TI

### Additional context

In the `setup.py`
 python_requires='>=3.6',

But  `capture_output` is used in  `subprocess.run` calling, which is valid only for python3.7
See also workaround to maintain python3.6:
https://stackoverflow.com/questions/53209127/


I'd like to understand how to use ddp properly with multiple GPUs on a single machine as I'm unsure of how to bring results together using this method.

I'm using TensorBoard for logging

The problem seems to be that my code (below) runs on each of the three GPUs (with a third of the data each), but the variables like  "overall_correct" only exist for each of the three processes so only a third of the data gets logged. For example, my overall performance on a single GPU is 82% but with the above process on 3 GPUs it is a third of that. I know this is a kindof silly thing but can someone explain how I should bring together the required validation/training statistics from the sub-processes using pytorch lightning?

My process is roughly:

```
model = MyModel(hparams)
tt_logger = TestTubeLogger(save_dir="path",name=expname)
trainer = Trainer(logger = tt_logger , gpus=3, distributed_backend='ddp' )
trainer.fit(model)

class MyModel(LightningModule):

     def __init__(self, hparams):
          super(MyModel, self).__init__() 
          self.hparams = hparams
          self.resnet = ResNetEncoder(self.hparams)
          self.loss_meter_training = averageMeter()
          self.overall_correct = 0.0

    def training_step(self, batch, batch_i):   
        ...
        self.loss_meter_training.update(float(total_loss))
        return {'loss': total_loss}

    def validation_step(self, batch, batch_nb):
        ...
        if something:
            self.overall_correct += 0.0
        return {'val_loss': total_loss}

    def validation_end(self, outputs):

        self.logger.experiment.add_scalar('epoch losses/training/total', self.loss_meter_training.avg, self.epoch_nb)
        self.logger.experiment.add_scalar('metrics/validation performance', self.overall_correct/20000, self.epoch_nb)
        self.loss_meter_validation.reset() 
        self.overall_correct = 0.0

    def configure_optimizers(self):
        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr)
        return optimizer 

    @pl.data_loader
    def tng_dataloader(self):

        t_loader = PairLoader('dummy', self.hparams , split='training')
        dist_sampler = torch.utils.data.distributed.DistributedSampler(t_loader)
        trainloader = data.DataLoader(t_loader,batch_size=self.hparams.batch_size, sampler=dist_sampler, num_workers = 12)  
    
        return trainloader
    
    @pl.data_loader
    def val_dataloader(self):
        
        v_loader = PairLoader('dummy', self.hparams , split='validation')
        dist_sampler = torch.utils.data.distributed.DistributedSampler(v_loader)
        trainloader = data.DataLoader(v_loader,batch_size=self.hparams.batch_size, sampler=dist_sampler, num_workers = 12) 
        
        return trainloader
```



 
## üêõ Bug
Currently, the `num_training_batches` is set to `inf` when the dataset is in iterable-style, which may lead to this error:
```
Traceback (most recent call last):
  File "scripts/msmacro.py", line 119, in <module>
    main()
  File "scripts/msmacro.py", line 115, in main
    trainer.fit(model)
  File "/home/zhaohao/.anaconda3/envs/pytorch/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 417, in fit
    self.run_pretrain_routine(model)
  File "/home/zhaohao/.anaconda3/envs/pytorch/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 481, in run_pretrain_routine
    self.get_dataloaders(ref_model)
  File "/home/zhaohao/.anaconda3/envs/pytorch/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py", line 199, in get_dataloaders
    self.init_train_dataloader(model)
  File "/home/zhaohao/.anaconda3/envs/pytorch/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py", line 78, in init_train_dataloader
    self.val_check_batch = int(self.num_training_batches * self.val_check_interval)
OverflowError: cannot convert float infinity to integer
```
 
workaround: set `val_check_interval` to an integer.


However, if the validation dataset is also in iterable style, then the following error will be raised, as there is no dataset type check in loading validation dataset

```
Traceback (most recent call last):
  File "scripts/msmacro.py", line 119, in <module>
    main()
  File "scripts/msmacro.py", line 115, in main
    trainer.fit(model)
  File "/home/zhaohao/Documents/pytorch-lightning/pytorch_lightning/trainer/trainer.py", line 417, in fit
    self.run_pretrain_routine(model)
  File "/home/zhaohao/Documents/pytorch-lightning/pytorch_lightning/trainer/trainer.py", line 481, in run_pretrain_routine
    self.get_dataloaders(ref_model)
  File "/home/zhaohao/Documents/pytorch-lightning/pytorch_lightning/trainer/data_loading.py", line 201, in get_dataloaders
    self.init_val_dataloader(model)
  File "/home/zhaohao/Documents/pytorch-lightning/pytorch_lightning/trainer/data_loading.py", line 117, in init_val_dataloader
    self.num_val_batches = sum(len(dataloader) for dataloader in self.get_val_dataloaders())
  File "/home/zhaohao/Documents/pytorch-lightning/pytorch_lightning/trainer/data_loading.py", line 117, in <genexpr>
    self.num_val_batches = sum(len(dataloader) for dataloader in self.get_val_dataloaders())
  File "/home/zhaohao/.anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 297, in __len__
    return len(self._index_sampler)  # with iterable-style dataset, this will error
  File "/home/zhaohao/.anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/sampler.py", line 212, in __len__
    return (len(self.sampler) + self.batch_size - 1) // self.batch_size
  File "/home/zhaohao/.anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 57, in __len__
    raise TypeError('Cannot determine the DataLoader length of a IterableDataset')
TypeError: Cannot determine the DataLoader length of a IterableDataset
```


I am running on a 14 core, 7 gpu machine. Ubuntu 18.04.2LTS, python 3.6.8, lightning 0.5.3.2, no virtual environment, no SLURM.

I have moved a tried and true model to ddp. It works great in all scenarios, including ddp as a single invocation.

I cannot succesfully start a second one, unfortunately. I get the following failure:
```
  File "/home/seth/.local/lib/python3.6/site-packages/torch/distributed/rendezvous.py", line 143, in _env_rendezvous_handler
    store = TCPStore(master_addr, master_port, world_size, start_daemon)
RuntimeError: Address already in use
```

This second job is running on different gpus and has a different log path.
After a brief investigation, it seems to me that the second job is trying to use the same master address as the first. I did not see any way to alter this with pytorch-lightning, though it seems straightforward in pytorch.

My questions are:
Can I run multiple simultaneous ddp jobs on the same node with different GPUs?
If so, how?

Thanks
## üêõ Bug

There are some JIT problems with newly released `torchvision` 0.5
in #687 we freeze version to <0.5 but in future, we want to support all `torchvision`s
Maybe it is just a temporal bug in `torchvision` and they will handle it...

### To Reproduce

https://github.com/PyTorchLightning/pytorch-lightning/pull/687#issuecomment-574913237

### Environment

https://app.circleci.com/jobs/github/Borda/pytorch-lightning/839

## üöÄ Feature
replace the few Pandas usage by native CSV package

### Motivation

https://github.com/PyTorchLightning/pytorch-lightning/pull/687#discussion_r366989578

I have searched through the docs / Google as well as looked through the source code.

It seems like test_end() returns nothing (it has no `return` in the function). I was wondering if I was missing something really obvious. 

I would simply like to return the metrics of the test end.