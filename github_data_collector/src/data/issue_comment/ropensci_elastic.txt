related to #87 
related to #87 

I can see a use case for a config file with connection details for 1 or more elasticsearch instances, and then `connect()` pulls in that info. e.g., 

```r
connect(file = "es_setup.yml")
```

where

```yaml
host: some.ip.address
path: /search
transport_schema: https
user: foo
pwd: bar
errors: complete
```

or to support many, do 

```yaml
setup_one:
  host: some.ip.address
  path: /search
  transport_schema: https
  user: foo
  pwd: bar
  errors: complete
setup_two:
  host: some.other.ip.address
  path: /query
  transport_schema: http
  user: hello
  pwd: world
  errors: simple
```

and then call in the details you want like 


```r
connect(file = list(setup_two = "es_setup.yml"))
```

or some other thing, not sure ðŸ¤” 
via https://github.com/metacran/seer/issues/12

ES version information is essentially required to use this pkg. 

I think the solution might be to have a separate config option to either 

- pass the version info in directly/manually, e.,g. `connect(version = '1.3.9')`
- give a path that the version can be retrived from, has to be similar to what ES gives at the root path, but at a different path, e.g. `connect(ping_path = "/foo/bar")`
- let elastic pkg get the version as it does now from root path `/`
**Is there a way to  write a sparkR dataframe or RDD to ElasticSearch with multiple nodes?**

This elastic package for R is great for normal interactions with ElasticSearch but says nothing about hadoop, distributed dataframes, or RDDs in SparkR 2.0+. When I try to use it I get the following errors:

```
install.packages("elastic", repos = "http://cran.us.r-project.org")
library(elastic)
sparkR.session(enableHiveSupport = TRUE)
df <- read.json('/hadoop/file/location')
connect(es_port = 9200, es_host = 'https://hostname.dev.company.com', es_user = 'username', es_pwd = 'password')
docs_bulk(df)
```

> Error: no 'docs_bulk' method for class SparkDataFrame

If this were PySpark, I would use the [`rdd.saveAsNewAPIHadoopFile()` function as shown here](https://stackoverflow.com/questions/46762678/how-to-push-a-spark-dataframe-to-elastic-search-pyspark), but I can't find any information about it in SparkR from googling. ElasticSearch also has [good documentation, but only for Scala and Java](https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html).

Note that my elastic cluster has multiple nodes and, in Zeppelin, I am using the %spark2.r interpreter. This is a [re-post of a SO question](https://stackoverflow.com/questions/49141042/how-to-read-and-write-to-elasticsearch-with-sparkr).

via #185 

using nginx/reverse proxy - and/or caddy w/ equiv
`elasticseearch-py` does this. 

``` python
In [13]: es.search()
#> DEBUG:urllib3.util.retry:Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)
#> INFO:urllib3.connectionpool:Starting new HTTP connection (1): localhost
#> WARNING:elasticsearch:GET http://localhost:9200/_search [status:N/A request:0.004s]
#> 
#> DEBUG:urllib3.util.retry:Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)
#> INFO:urllib3.connectionpool:Starting new HTTP connection (2): localhost
#> WARNING:elasticsearch:GET http://localhost:9200/_search [status:N/A request:0.002s]
#> 
#> DEBUG:urllib3.util.retry:Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)
#> INFO:urllib3.connectionpool:Starting new HTTP connection (3): localhost
#> WARNING:elasticsearch:GET http://localhost:9200/_search [status:N/A request:0.002s]
#> 
#> DEBUG:urllib3.util.retry:Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)
#> INFO:urllib3.connectionpool:Starting new HTTP connection (4): localhost
#> WARNING:elasticsearch:GET http://localhost:9200/_search [status:N/A request:0.001s]
#> 
#> ConnectionError: ConnectionError(<urllib3.connection.HTTPConnection object at 0x1043dd3d0>: Failed to establish a new connection: [Errno 61] Connection refused) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x1043dd3d0>: Failed to establish a new connection: [Errno 61] Connection refused)
```

can't remember why, maybe there was a good reason not to have them
