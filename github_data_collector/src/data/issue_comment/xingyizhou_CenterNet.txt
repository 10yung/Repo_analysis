when I run python main.py ctdet --exp_id food --batch_size 32 --lr 1.25e-4  --gpus 0, a problem occurs:
RuntimeError: The size of tensor a (38) must match the size of tensor b (37) at non-singleton dimension 3, which is related to CenterNet/src/lib/models/networks/pose_dla_dcn.py", line 56, in forward
out += residual.



In focal loss paper, the author mentioned that the cls bias is computed as follows for cls: 

> -log((1-PI)/PI) 

and PI in that paper is set to 0.01. I'm very interesting that if -2.19 is computed as the above formula, it presents that the confidence of a point in feat map which is a positive sample is 0.1. Is it calculated according to the above formula, or is it some other way? Does this mean that you think that for centernet, each point is more likely to be a positive sample? Many thanks for your answer!
@xingyizhou ,have u train PE from strach but not finetune on OD? I did this ,and get a better result，i want to know,why u train PE finetuned on OD?
I am Always getting square bounding boxes, I am working on a different dataset. For training, I used resnet50 as the backbone. Well, heatmaps are good/exact but bounding boxes are always predicted almost square for all products. What may be the issue? 
  After reading and doing experiments on your CenterNet, I think CenterNet is quite different from other anchor-based methods, can we refer it as real anchor-free?
So the network produced (64, 64, C) heatmap for object centers of each class, then (64, 64, 2) for `wh` of the object, and also (64, 64, 2) for offset. 

it seems like ground truth `wh` is in shape of (max_obj, 2)
https://github.com/xingyizhou/CenterNet/blob/master/src/lib/datasets/sample/ddd.py#L70
similarly, 
https://github.com/xingyizhou/CenterNet/blob/master/src/lib/datasets/sample/ctdet.py#L87

there's also a `dense_wh` which seems to be in the right shape
However, it's not quite clear which one is the right one to use

```
wh = np.zeros((self.max_objs, 2), dtype=np.float32)
dense_wh = np.zeros((2, output_h, output_w), dtype=np.float32)
```

 in the ctdet trainer, it has option to use `dense_wh`, but also `cat_spec_wh` and `wh`

https://github.com/xingyizhou/CenterNet/blob/master/src/lib/trains/ctdet.py#L51
```
if opt.wh_weight > 0:
        if opt.dense_wh:
          mask_weight = batch['dense_wh_mask'].sum() + 1e-4
          wh_loss += (
            self.crit_wh(output['wh'] * batch['dense_wh_mask'],
            batch['dense_wh'] * batch['dense_wh_mask']) / 
            mask_weight) / opt.num_stacks
        elif opt.cat_spec_wh:
          wh_loss += self.crit_wh(
            output['wh'], batch['cat_spec_mask'],
            batch['ind'], batch['cat_spec_wh']) / opt.num_stacks
        else:
          wh_loss += self.crit_reg(
            output['wh'], batch['reg_mask'],
            batch['ind'], batch['wh']) / opt.num_stacks
```

In your paper, it didn't say how to put bboxes `wh` into the this (64, 64, 2). Does it also follow 2d gaussian? or shall we just set `wh` for the exact cell of the box center? Thanks
First of all, thanks again for sharing the code.

I got a couple more questions while reading it though. Please bear with for my ignorance.

Why do we need -2.19 for the bias of the last conv layer here?
https://github.com/xingyizhou/CenterNet/blob/master/src/lib/models/networks/large_hourglass.py#L242

is `kp_module `  keypoint module?

~~and what do these stand for? `exkp`, `exdet`, `ctdet`, `ddd`~~

Found answer for the above one here: https://github.com/xingyizhou/CenterNet/issues/478

Thanks.



Hi Xingyi,

Thanks for contributing such great code. 

As we can see from the initial implementation of Hourglass-104, they use a pre-activation version of residual module and put BN and ReLU before Conv layer:
https://github.com/princeton-vl/pose-hg-train/blob/master/src/models/layers/Residual.lua#L8

And in your version, it seems going back to the vanilla version where Conv layer followed by ReLU and BN: 
https://github.com/xingyizhou/CenterNet/blob/master/src/lib/models/networks/large_hourglass.py#L65

Besides, your Hourglass residual module has one less conv layer compared with the original implementation

Could you share some thoughts around this change? Thanks!
请问一下，在关节点检测问题中，预测的offset_map是n*H*W（n是关节点的个数）的，但一个关键点只会产生一个offset值，那么在构建offset_map的target的时候，是整张map都是这一个值吗？还是怎样构成的呢？
Hello, when I changed dla34 to dla60, the training was very normal.But when I tried to inference,I found the result is absolutely wrong. Have you ever met?