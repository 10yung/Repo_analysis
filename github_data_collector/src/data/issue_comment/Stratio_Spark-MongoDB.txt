I'm working on a Spark ML sentiment analysis platform using a mobile feedback APP as a data source. The Data (Feedback) re stored on MongoDB and i wanna read them with spark. I tried to connect spark to MongoDB but not lucky i keep getting this error : 
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.NoSuchMethodError: org.apache.spark.sql.catalyst.analysis.TypeCoercion$.findTightestCommonTypeOfTwo()Lscala/Function2;
	at com.stratio.datasource.mongodb.schema.MongodbSchema.com$stratio$datasource$mongodb$schema$MongodbSchema$$compatibleType(MongodbSchema.scala:85)
	at com.stratio.datasource.mongodb.schema.MongodbSchema$$anonfun$3.apply(MongodbSchema.scala:46)
	at com.stratio.datasource.mongodb.schema.MongodbSchema$$anonfun$3.apply(MongodbSchema.scala:46)
	at org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:189)
	at org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:188)
	at org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:150)
	at org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:194)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2124)
	at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1118)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1111)
	at com.stratio.datasource.mongodb.schema.MongodbSchema.schema(MongodbSchema.scala:46)
	at com.stratio.datasource.mongodb.MongodbRelation.com$stratio$datasource$mongodb$MongodbRelation$$lazySchema$lzycompute(MongodbRelation.scala:63)
	at com.stratio.datasource.mongodb.MongodbRelation.com$stratio$datasource$mongodb$MongodbRelation$$lazySchema(MongodbRelation.scala:60)
	at com.stratio.datasource.mongodb.MongodbRelation$$anonfun$1.apply(MongodbRelation.scala:65)
	at com.stratio.datasource.mongodb.MongodbRelation$$anonfun$1.apply(MongodbRelation.scala:65)
	at scala.Option.getOrElse(Option.scala:121)
	at com.stratio.datasource.mongodb.MongodbRelation.<init>(MongodbRelation.scala:65)
	at com.stratio.datasource.mongodb.DefaultSource.createRelation(DefaultSource.scala:36)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:340)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:164)
	at com.spml.MongoDbSparkCon.main(MongoDbSparkCon.java:36)
Caused by: java.lang.NoSuchMethodError: org.apache.spark.sql.catalyst.analysis.TypeCoercion$.findTightestCommonTypeOfTwo()Lscala/Function2;
	at com.stratio.datasource.mongodb.schema.MongodbSchema.com$stratio$datasource$mongodb$schema$MongodbSchema$$compatibleType(MongodbSchema.scala:85)
	at com.stratio.datasource.mongodb.schema.MongodbSchema$$anonfun$3.apply(MongodbSchema.scala:46)
	at com.stratio.datasource.mongodb.schema.MongodbSchema$$anonfun$3.apply(MongodbSchema.scala:46)
	at org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:189)
	at org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:188)
	at org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:150)
	at org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:194)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2018-05-28 18:54:24 INFO  DAGScheduler:54 - Job 0 failed: aggregate at MongodbSchema.scala:46, took 0,855790 s
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)



My spark code : Im using Java 

public class MongoDbSparkCon  {

    public static void main(String[] args) {


        JavaSparkContext sc = new JavaSparkContext("local[*]", "test spark-mongodb java");
        SQLContext sqlContext = new org.apache.spark.sql.SQLContext(sc);

        Map options = new HashMap();
        options.put("host", "localhost:27017");
        options.put("database", "feedbackuib");
        options.put("collection", "Feedback");

        Dataset<Row> df = sqlContext.read().format("com.stratio.datasource.mongodb").options(options).load();
        df.registerTempTable("Feedback");
        sqlContext.sql("SELECT * FROM Feedback");
        df.show();
}}


Pom.xml : 


   <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core -->
<dependencies>
    <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core -->
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-core_2.11</artifactId>
        <version>2.2.0</version>
    </dependency>
    <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-mllib -->
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-mllib_2.11</artifactId>
        <version>2.3.0</version>
    </dependency>
    <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-sql -->
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-sql_2.11</artifactId>
        <version>2.2.0</version>
    </dependency>
    <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-streaming -->
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-streaming_2.11</artifactId>
        <version>2.3.0</version>
        <scope>provided</scope>
    </dependency>
    <dependency>
        <groupId>org.mongodb.spark</groupId>
        <artifactId>mongo-spark-connector_2.10</artifactId>
        <version>2.2.2</version>

    </dependency>

    <!-- https://mvnrepository.com/artifact/com.stratio.datasource/spark-mongodb -->
    <dependency>
        <groupId>com.stratio.datasource</groupId>
        <artifactId>spark-mongodb_2.11</artifactId>
        <version>0.12.0</version>
    </dependency>

    <!-- https://mvnrepository.com/artifact/org.mongodb/mongo-java-driver -->
    <dependency>
        <groupId>org.mongodb</groupId>
        <artifactId>mongo-java-driver</artifactId>
        <version>3.6.3</version>
    </dependency>
    <!-- https://mvnrepository.com/artifact/org.mongodb/casbah-core -->
    <dependency>
        <groupId>org.mongodb</groupId>
        <artifactId>casbah-core_2.11</artifactId>
        <version>3.1.1</version>
    </dependency>
    <!-- https://mvnrepository.com/artifact/org.mongodb/casbah-commons -->
    <dependency>
        <groupId>org.mongodb</groupId>
        <artifactId>casbah-commons_2.11</artifactId>
        <version>3.1.1</version>
    </dependency>
    <!-- https://mvnrepository.com/artifact/org.mongodb/casbah-query -->
    <dependency>
        <groupId>org.mongodb</groupId>
        <artifactId>casbah-query_2.11</artifactId>
        <version>3.1.1</version>
    </dependency>
    <!-- https://mvnrepository.com/artifact/org.scala-lang/scala-library -->
    <dependency>
        <groupId>org.scala-lang</groupId>
        <artifactId>scala-library</artifactId>
        <version>2.11.0</version>
    </dependency>
    <!-- https://mvnrepository.com/artifact/com.typesafe.akka/akka-actor -->
    <dependency>
        <groupId>com.typesafe.akka</groupId>
        <artifactId>akka-actor_2.11</artifactId>
        <version>2.5.12</version>
    </dependency>




</dependencies>

Please anny help !!

`MongodbRelation` extends `PrunedFilteredScan`
it can not push down nested struct filter
I think it can use `CatalystScan` to push down nested struct query
Hello everyone
   last ,I use mongodb spark ,but when writen data to mongodb db ，the connection socket error. error tips come out com.mongodb.MongoTimeoutException: Timed out after 10000 ms while waiting to connect
   I find the source but not find how to make the connectTimeout  to take effect
                             
                                                                               thank you


hello 
    how to make index use column to read data faster


                                                                    thank you
My cloud server is using SCRAM-SHA-1 authMechanism, I don't see how to config this authMechanism in the docs. Can I change the authMechanism in the python version?
The latest compatible version does not have SPARK 1.6, do not support it?
I want to read collection from mongodb which is secured. I am writing the following query:

df_raw_eventdata = sqlContext.read.format("com.stratio.datasource.mongodb").\
    options(host='server_ip:port', database='db1', collection='collec_1', credentials = 'username,db1,password').load()

But I am getting this error. 

`py4j.protocol.Py4JJavaError: An error occurred while calling o33.load.
: com.mongodb.CommandFailureException: { "serverUsed" : "myserver:27017" , "ok" : 0.0 , "errmsg" : "Authentication failed." , "code" : 18 , "codeName" : "AuthenticationFailed"}
	at com.mongodb.CommandResult.getException(CommandResult.java:76)
	at com.mongodb.CommandResult.throwOnError(CommandResult.java:140)
	at com.mongodb.DBPort$SaslAuthenticator.authenticate(DBPort.java:899)
	at com.mongodb.DBPort.authenticate(DBPort.java:432)
	at com.mongodb.DBPort.checkAuth(DBPort.java:443)
	at com.mongodb.DBTCPConnector.innerCall(DBTCPConnector.java:289)
	at com.mongodb.DBTCPConnector.call(DBTCPConnector.java:269)
	at com.mongodb.DBCollectionImpl.find(DBCollectionImpl.java:84)
	at com.mongodb.DB.command(DB.java:320)
	at com.mongodb.DB.command(DB.java:299)
	at com.mongodb.DBCollection.getStats(DBCollection.java:1808)
	at com.mongodb.casbah.MongoCollectionBase$class.getStats(MongoCollection.scala:608)
	at com.mongodb.casbah.MongoCollection.getStats(MongoCollection.scala:1162)
	at com.mongodb.casbah.MongoCollectionBase$class.stats(MongoCollection.scala:606)
	at com.mongodb.casbah.MongoCollection.stats(MongoCollection.scala:1162)
	at com.stratio.datasource.mongodb.partitioner.MongodbPartitioner.isShardedCollection(MongodbPartitioner.scala:79)
	at com.stratio.datasource.mongodb.partitioner.MongodbPartitioner$$anonfun$computePartitions$1.apply(MongodbPartitioner.scala:67)
	at com.stratio.datasource.mongodb.partitioner.MongodbPartitioner$$anonfun$computePartitions$1.apply(MongodbPartitioner.scala:66)
	at com.stratio.datasource.mongodb.util.usingMongoClient$.apply(usingMongoClient.scala:27)
	at com.stratio.datasource.mongodb.partitioner.MongodbPartitioner.computePartitions(MongodbPartitioner.scala:66)
	at com.stratio.datasource.mongodb.rdd.MongodbRDD.getPartitions(MongodbRDD.scala:42)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:246)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:246)
	at org.apache.spark.Partitioner$.defaultPartitioner(Partitioner.scala:65)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:328)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:328)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)
	at org.apache.spark.rdd.PairRDDFunctions.reduceByKey(PairRDDFunctions.scala:327)
	at com.stratio.datasource.mongodb.schema.MongodbSchema.schema(MongodbSchema.scala:46)
	at com.stratio.datasource.mongodb.MongodbRelation.com$stratio$datasource$mongodb$MongodbRelation$$lazySchema$lzycompute(MongodbRelation.scala:63)
	at com.stratio.datasource.mongodb.MongodbRelation.com$stratio$datasource$mongodb$MongodbRelation$$lazySchema(MongodbRelation.scala:60)
	at com.stratio.datasource.mongodb.MongodbRelation$$anonfun$1.apply(MongodbRelation.scala:65)
	at com.stratio.datasource.mongodb.MongodbRelation$$anonfun$1.apply(MongodbRelation.scala:65)
	at scala.Option.getOrElse(Option.scala:121)
	at com.stratio.datasource.mongodb.MongodbRelation.<init>(MongodbRelation.scala:65)
	at com.stratio.datasource.mongodb.DefaultSource.createRelation(DefaultSource.scala:36)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:345)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:149)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:122)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)`



Any help is appreciated.

Despite the multiple improvements to prevent connection leakage, I have observed a sharp spike in leaked connections moving to 0.12.0 from 0.11.2 (the migration is required by our move to Spark 2.x). 

Here is what happened after the upgrade.

![0.12.0](https://content.screencast.com/users/ssimeonov/folders/Snagit/media/209f0e72-2fe2-475a-93d6-7a35ed9c557a/2017-01-26_22-33-18.png)

The steps happen when we run a scheduled Spark job that pulls 15+ collections from Mongo. The drops are cluster restarts for library upgrades. 

Zooming out, here what the situation looked like under 0.11.2 before the upgrade to 0.12.0.

![0.11.2](https://content.screencast.com/users/ssimeonov/folders/Snagit/media/fa1231ea-1773-4757-8d56-5d24a6d9d685/2017-01-26_22-37-21.png)

Since 0.12.0 is the only release compatible with Spark 2.x, there is nothing to downgrade to.


Hi ,
I installed spark 2.0.2 , and run the spark shell.

 bin]$ ./spark-shell --jars ~/spark-mongodb_2.10-0.11.2.jar  --packages org.mongodb:casbah-core_2.10:3.0.0
Ivy Default Cache set to: /home/centos/.ivy2/cache
The jars for the packages stored in: /home/centos/.ivy2/jars
:: loading settings :: url = jar:file:/home/centos/spark/spark-2.0.2-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
org.mongodb#casbah-core_2.10 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0
        confs: [default]
        found org.mongodb#casbah-core_2.10;3.0.0 in central
        found org.mongodb#casbah-commons_2.10;3.0.0 in central
        found com.github.nscala-time#nscala-time_2.10;1.0.0 in central
        found joda-time#joda-time;2.3 in central
        found org.joda#joda-convert;1.2 in central
        found org.mongodb#mongo-java-driver;3.0.4 in central
        found org.slf4j#slf4j-api;1.6.0 in central
        found org.mongodb#casbah-query_2.10;3.0.0 in central
:: resolution report :: resolve 377ms :: artifacts dl 13ms
        :: modules in use:
        com.github.nscala-time#nscala-time_2.10;1.0.0 from central in [default]
        joda-time#joda-time;2.3 from central in [default]
        org.joda#joda-convert;1.2 from central in [default]
        org.mongodb#casbah-commons_2.10;3.0.0 from central in [default]
        org.mongodb#casbah-core_2.10;3.0.0 from central in [default]
        org.mongodb#casbah-query_2.10;3.0.0 from central in [default]
        org.mongodb#mongo-java-driver;3.0.4 from central in [default]
        org.slf4j#slf4j-api;1.6.0 from central in [default]
        ---------------------------------------------------------------------
        |                  |            modules            ||   artifacts   |
        |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
        ---------------------------------------------------------------------
        |      default     |   8   |   0   |   0   |   0   ||   8   |   0   |
        ---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent
        confs: [default]
        0 artifacts copied, 8 already retrieved (0kB/8ms)
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel).
17/01/23 14:17:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/01/23 14:17:55 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.
Spark context Web UI available at http://172.31.19.188:4040
Spark context available as 'sc' (master = local[*], app id = local-1485181074885).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.0.2
      /_/

Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_111)
Type in expressions to have them evaluated.
Type :help for more information.

scala> import org.apache.spark.sql._
import org.apache.spark.sql._

scala> import com.mongodb.casbah.{WriteConcern => MongodbWriteConcern}
import com.mongodb.casbah.{WriteConcern=>MongodbWriteConcern}

scala> import com.stratio.datasource.mongodb._
import com.stratio.datasource.mongodb._

scala> import com.stratio.datasource.mongodb.config._
import com.stratio.datasource.mongodb.config._

scala> import com.stratio.datasource.mongodb.config.MongodbConfig._
import com.stratio.datasource.mongodb.config.MongodbConfig._

scala> val builder = MongodbConfigBuilder(Map(Host -> List("localhost:27017"), Database -> "db1", Collection ->"coll1", SamplingRatio -> 0.001, WriteConcern -> "normal"))
builder: com.stratio.datasource.mongodb.config.MongodbConfigBuilder = MongodbConfigBuilder(Map(database -> db1, writeConcern -> normal, schema_samplingRatio -> 0.001, collection -> coll1, host -> List(localhost:27017)))

scala>

scala> val readConfig = builder.build()
readConfig: com.stratio.datasource.util.Config = com.stratio.datasource.util.ConfigBuilder$$anon$1@f3cee0fa

scala> val mongoRDD = spark.sqlContext.fromMongoDB(readConfig)
java.lang.NoSuchMethodError: com.stratio.datasource.mongodb.MongodbContext.fromMongoDB(Lcom/stratio/datasource/util/Config;Lscala/Option;)Lorg/apache/spark/sql/Dataset;
  ... 56 elided
