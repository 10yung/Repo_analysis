Dear All, 

I have successfully installed the RHive package. 

> rhive.env()
	hadoop home: /usr/hdp/2.5.3.0-37/hadoop
	hadoop conf: /usr/hdp/2.5.3.0-37/hadoop/conf
	fs: hdfs://bdc-master.novalocal:8020
	hive home: /usr/hdp/2.5.3.0-37/hive2
	user name: shivanand
	user home: /home/shivanand
	temp dir: /tmp/shivanand

> rhive.connect()
Error: org.apache.hadoop.security.AccessControlException: Permission denied: user=shivanand, access=WRITE, inode="/rhive/lib/2.0-0.10/rhive_udf.jar":hdfs:hdfs:drwxr-xr-x

I can understand the error, as it says permission denied. However I am not able to find the "/rhive/lib/2.0-0.10/rhive_udf.jar" file.

it is located at : /usr/lib64/R/library/RHive/java/rhive_udf.jar

So my question is from where does it get the location of rhive_udf.jar file '?



Hello,

I'm having an issue with the function rhive.load.table.

I managed to connect to Hive and both functions rhive.query and rhive.write.table work.

But whenever I try to call rhive.load.table or rhive.load.table2, it returns the following error:
Error: java.sql.SQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask

Do you have any idea how to resolve this problem?

Thanks in advance

Hi,

 I have an HDP2.3 cluster with Tez version 0.7 and Hive 1.2 . I'm using the below configurations to connect to Hive from R(3.3).
 >Sys.setenv(HIVE_HOME="/usr/hdp/2.3.4.7-4/hive/")
> Sys.setenv(RHIVE_DATA="/rhive_data")
> Sys.setenv(HADOOP_HOME="/usr/hdp/2.3.4.7-4/hadoop")
> Sys.setenv(RHIVE_FS_HOME="/RHive")
> library(RHive)
> rhive.init()
> rhive.connect(host="xx.xxx.xx.xx", port=10000, hiveServer2=TRUE,defaultFS="hdfs://mycluster1")

I'm able to get the results of aggregate query correctly, but the MR jobs are not listed in the Resource Manager Web UI. Any suggestions are greatly appreciated.

Thanks

 install.packages("RHive")
Warning in install.packages :
  package ‘RHive’ is not available (for R version 3.3.2)
# Dependency rJava is already there because I am using rhdfs as well and it works just fine. I have installed Rserve too and that works fine too.

These packages are installed in 

`/root/R/x86_64-redhat-linux-gnu-library/3.2/`

In my R working directory I have cloned RHive repo , used ant build and R CMD build RHive. Works fine until here but it just does not install RHive.

It keeps saying 

dependencies ‘rJava’, ‘Rserve’ are not available for package ‘RHive’

How to fix this?

I am on a centos system. Hadoop_Home and Hive_home have been set correctly. 

Is NexR going to maintain this package, or should it be forked/replaced by an independant effort?

I am trying to use RHive package to connect to Hive on Hortonworks Data Platform 2.4. When I execute:

`rhive.connect(host="192.168.56.101",port=10000, hiveServer2=TRUE)`

I received the following error message: 

> Cannot modify mapred.child.env at runtime. It is not in list of params that are allowed to be modified at runtime.

I inserted the following property and value in the hive-site.xml file, as suggested by some other posts:

>   <property>
>       <name>hive.security.authorization.sqlstd.confwhitelist.append</name>
>       <value>mapred.child.env</value>
>    </property>

But i am still getting the same error. Any suggestion? Thanks in advance!

I've created a new UDAF as per below commands 

hsum <- function(prev, sal) {
  if(is.null(prev))
    sal
  else
    prev + sal
}

hsum.partial <- function(agg_sal) {
  agg_sal
}

hsum.merge <- function(prev, agg_sal) {
  if(is.null(prev))
    agg_sal
  else
    prev + agg_sal
}

hsum.terminate <- function(agg_sal) {
  agg_sal
}

rhive.assign('hsum', hsum)
rhive.assign('hsum.partial', hsum.partial)
rhive.assign('hsum.merge', hsum.merge)
rhive.assign('hsum.terminate', hsum.terminate)
rhive.exportAll('hsum')

Everything was fine till then and also file "hsum.RData" has been created in "/rhive/udf/user_name" directory.
But then when I'm using the UDAF to run query on one of my tables as:

rhive.query("select RA('hsum', prev, sal) from employee")

It gives me error:
"Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask"  

and when I checked the log, I found below exception:

org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"emp_id":667266,"prev":6000,"sal":8000} at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:562) at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:163) ... 8 more Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.FileNotFoundException: File /rhive/udf/user_name/hsum.RData does not exist at com.nexr.rhive.hive.udf.RUDAF$GenericRUDAF.loadRObjects(RUDAF.java:525) at com.nexr.rhive.hive.udf.RUDAF$GenericRUDAF.iterate(RUDAF.java:244) at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(GenericUDAFEvaluator.java:185) at org.apache.hadoop.hive.ql.exec.GroupByOperator.updateAggregations(GroupByOperator.java:612) at org.apache.hadoop.hive.ql.exec.GroupByOperator.processHashAggr(GroupByOperator.java:787) at org.apache.hadoop.hive.ql.exec.GroupByOperator.processKey(GroupByOperator.java:693) at org.apache.hadoop.hive.ql.exec.GroupByOperator.process(GroupByOperator.java:761) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:838) at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:88) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:838) at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:117) at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:167) at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:552) ... 9 more Caused by: java.io.FileNotFoundException: File /rhive/udf/user_name/hsum.RData does not exist at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609) at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822) at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599) at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421) at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337) at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289) at org.apache.hadoop.fs.LocalFileSystem.copyToLocalFile(LocalFileSystem.java:88) at com.nexr.rhive.hive.udf.RUDAF$GenericRUDAF.loadRObjects(RUDAF.java:517) ... 21 more
attempt_1466028339333_0021_m_000000_1

As I mentioned above,"hsum.RData" has already been created in "/rhive/udf/user_name"  directory when I ran the export command, but I still get the exception:
"java.io.FileNotFoundException: File /rhive/udf/user_name/hsum.RData does not exist at com.nexr.rhive.hive.udf.RUDAF$GenericRUDAF.loadRObjects"

I am using the following version of Hive 1.1.0-cdh5.4.3 with R 3.2.2. My typical jdbc string would look something like below if I were connecting via SQL Squirrel for instance:

jdbc:hive2://hive.server.com:10000/default;AuthMech=1;principal=hive/_HOST@SOME.DOMAIN.COM

See the following error after the connect, note I have a valid credential prior to invoking R repl:

> rhive.connect(host = "hive.server.com", port = "10001",  db = "default", user = "bayroot", password = "XXXXX", defaultFS="hdfs://nameservice1/rhive", properties="hive.principal=hive/_HOST@SOME.DOMAIN.COM")
> Warning:
>         +----------------------------------------------------------+
>         + / hiveServer2 argument has not been provided correctly.  +
>         + / RHive will use a default value: hiveServer2=TRUE.      +
>         +----------------------------------------------------------+
> 15/10/21 11:17:03 INFO jdbc.Utils: Supplied authorities: hive.server.com:10001
> 15/10/21 11:17:03 INFO jdbc.Utils: Resolved authority: hive.server.com:10001
> 15/10/21 11:17:03 INFO jdbc.HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://hive.server.com:10001/default;principal=hive/_HOST@SOME.DOMAIN.COM
> 15/10/21 11:20:05 ERROR transport.TSaslTransport: SASL negotiation failure
> javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Fail to create credential. (63) - No service creds)]

Please note this code was working fine in CDH 5.3.2 which was hive .13 I believe:

Package: RHive
Type: Package
Title: R and Hive
Version: 2.0-1.0

# Fixes issue 84

This patch is not mine, its from winghc, but I also need the fix. I've tested it locally, it works perfectly:
#84
