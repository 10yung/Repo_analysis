Hello,
    ZheDong, thanks for you sharing such a good work. I want to reproduce it in `Pytorch`,but I'm sorry that I encountered the overfitting problem.
   To get the results quickly, I randomly choose 10,000 samples as traindata and 1,000 as valdata, 1,000 as testdata separately. Finally I got about 100% recall@5 on the training set while only half of it on the val data.
   And I'm a fresh man to ImageTextEmbedding,could you share some solutions to that. I guess there are relevant reasons:

1. **Data normalization**. I don't compute the mean and var of train_data explicitly, and just divide it by 255, subtract 0.5, and thendivide it by 0.5

2. **L2 regularization**. I just use the **1e-5** regularization intensity
3. **The complexity of classifier**. After generator, I add a classifier with a softmax layer directly. Whether more fully connection layers can slow down the fitting of the training set

Finally, I want to ask how to mine the hard triplet online in `Pytorch` efficiently.

Thanks.
How to use the trained model
txt-image rank-1:0.384178 mAP:0.354052 Medr:3.000000
txt-image rank-5:0.610136 rank-10:0.703866

which is normally 5-6% lower than reported. I made no change to the parameters, any ideas what could possibly be the reason for the performance drop?

trained on Ubuntu 16.04 LTS Matlab 2015b (preprocessing data on another machine with a higher version for jsondecode support) + cudnn 5.0 with a Titan Xp.


I am trying to train the model on MSCOCO and run into the following issues:

**1-** When running 'train_coco_word2_1_pool.m' as you suggest, I get the error that the function 'coco_word2_pool_no_w2v' does not exist.

**2-** I therefore changed it for 'coco_word2_pool' since this function is indeed in the directory (is this what you meant?). Then I get the following error:

_Error using reshape
To RESHAPE the number of elements must not change.

Error in coco_word2_pool (line 273)
net.params(first).value = reshape(single(subset.features'),1,1,29972,300);

Error in train_coco_word2_1_pool (line 17)
net = coco_word2_pool();_

**3-** I experience the same issue when running 'train_coco_word2_1_pool_vgg19.m'

**4-** The reason that I am training is that I want to reproduce your results but the 20 epochs in your pretrained model don't seem to be enough. Are these the parameters that you used to report test results?

I am running the code on a MacBook Pro, on Matlab R2018b.
Thank you in advance.
Recently read your thesis, there are problems in the process of running the code, ask you.
The following problem occurs when split data set in Image-Text-Embedding:
Index exceeds matrix dimensions.

Error in prepare_imdb (line 24)
Images = fullfile(imdb.images.data(train(1:end))) ;

Going beyond the border, I don’t know what’s wrong.
Hey layumi, I am trying to replicate your results for mscoco in tensorflow I had some questions about processing data and loss:

1. At the end of Stage 1 my text CNN ('objective_txt')  loss is high around 5.5, what was the loss you got at the end of Stage 1?

2. in dataset/MSCOCO-prepare/prepare_wordcnn_feature2.m you create 
wordcnn = zeros(32,611765,'int16')
then loop over all the captions in MSCOCO, but there is 616,767 captions in MSCOCO, so what's the reason of this 5002 difference? it throws an out of range error when I implemented it in tensorflow because there is more captions than the rows/columns in the matrix wordcnn created

3. coco_dictionary.mat dimensions is 29972 in your code but my dimensions are different? I wonder if this is the reason why the loss is high or it might be because tensorflow uses a different random generator than matlab, if you have any suggestion on this that would be great

Thank you!
Hi, I run the train_coco_word2_1_pool.m, but after more than 10 epochs, I found  the train result is still bad(as below). I didn't change any hyper parameters but I don't know why it doesn't work.
![qq 20180515115715](https://user-images.githubusercontent.com/28893002/40035968-3af3de2e-5837-11e8-9f66-24f1cd071c1a.png)
Can you tell me what make this result happen? And I found the learning rate in your code is 0.1, but it's reported as 0.001 in the paper. Which lr is correct and better in this task? 

is there any way to get text or image embedding if given text or image as input?
thanks