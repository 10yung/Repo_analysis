In makefile traefik version is not specified, and the syntax is incompatible with the current version (2)
Problem - see stacktrace below

My solution for now: Changed the docker-compose.yml file and added "priviledged: true" setting to the nodes configured there 

namenode          | org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /hadoop/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
namenode          | 	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:322)
namenode          | 	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:210)
namenode          | 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1012)
namenode          | 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:691)
namenode          | 	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:634)
namenode          | 	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:695)
namenode          | 	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:898)
namenode          | 	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:877)
namenode          | 	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1603)
namenode          | 	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1671)



I copied files from local filesystem to namenode container and then copy it to hdfs on "/user/root/data" path. Now, i have a problem to load data from hdfs into local spark application
`spark.read.format("json").load("hdfs://127.0.1.1:50070/user/root/data/file_name.json")`.
Problem is url to data i tried hdfs://127.0.1.1:50070, hdfs://localhost:50070, hdfs://namenode:50070, hdfs://namenode:8020 and none of this is valid. Is someone having similar problem ?
when i cloned this project , i run ' start-hadoop-spark-workbench-with-Hive.sh' ,
then there are some CONTAINERs running, but when i  'docker exec -it  spark-master /bin/bash' ,
and cd spark/bin , then run spark-shell , some thing happened:
this is my cmd:


scala> val textFile = sc.textFile("/spark/bin/words.txt")
textFile: org.apache.spark.rdd.RDD[String] = /spark/bin/words.txt MapPartitionsRDD[3] at textFile at <console>:20

scala> text
text   textFile
scala> textFile.count()
java.lang.IllegalArgumentException: java.net.UnknownHostException: namenode
  at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:378)
  at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:310)
  at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:176)
  at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:678)
  at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:619)
  at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:149)
  at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669)
  at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)
  at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)
  at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)
  at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)
  at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:172)
  at org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:656)
  at org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:438)
  at org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:411)
  at org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$29.apply(SparkContext.scala:1013)
  at org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$29.apply(SparkContext.scala:1013)
  at org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:179)
  at org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:179)
  at scala.Option.foreach(Option.scala:257)
  at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:179)
  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:198)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)
  at org.apache.spark.rdd.RDD.count(RDD.scala:1158)
  ... 46 elided
Caused by: java.net.UnknownHostException: namenode
  ... 79 more

i  want to use spark-sql or hive to exec some sql and connect to hive use python  or use pyspark,  can i achieve my aim ?
Added kafka image (+ zookeeper) to compose with web console UI over REST interface. 

i have 
namenode(10.0.5.x) in machine-1
spark master(10.0.5.x) in machine-1
network-endpoint(10.0.5.3) in machine-2
spark worker(10.0.5.x) in machine-2
datanode(10.0.5.x) in machine-2

my code run in spark master(use pyspark)
`text = sc.textFile("hdfs://namenode:9000/path/file")`
`text.collect()`

I create swarm with spark(gettyimage) with your hadoop and I cannot read data in hadoop, I read log in worker and it say `Failed to connect to /10.0.5.3:50010 for block BP-1439091006-10.0.5.76-1536712157279:blk_1073741825_1001, add to deadNodes and continue.`

10.0.5.3 is network endpoint 
**why namenode connect with endpoint ip?**

but I can access data in namenode and why ? it endpoint ip.

docker-compose-hadoop.yml
```
version: '3'
services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
    ports:
      - 50070:50070
    volumes:
      - ./namenode:/hadoop/dfs/name
      - ./hadoop-data:/hadoop-data
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints: [node.role == manager]
      restart_policy:
        condition: on-failure

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
    volumes:
      - ./datanode:/hadoop/dfs/data
    env_file:
      - ./hadoop.env
    environment:
      SERVICE_PRECONDITION: "namenode:50070"
    deploy:
      mode: global
      placement:
        constraints: [node.role == worker]
      restart_policy:
        condition: on-failure

networks:
    default:
        external:
            name: hadoop-spark-swarm-network
```

docker-compose-spark.yml
```
version: '3'
services:
  master:
    image: gettyimages/spark
    command: bin/spark-class org.apache.spark.deploy.master.Master
    hostname: master
    environment:
      MASTER: spark://master:7077
      SPARK_CONF_DIR: /conf
      SPARK_PUBLIC_DNS: localhost
      SPARK_MASTER_HOST: 0.0.0.0
    env_file:
      - ./hadoop.env
    ports:
      - 4040:4040
      - 6066:6066
      - 7077:7077
      - 8001:8080
      - 8888:8888
    volumes:
      - ./data:/tmp/data
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      placement:
        constraints: [node.role == manager]

  worker:
    image: gettyimages/spark
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://master:7077
    hostname: worker
    environment:
      SPARK_CONF_DIR: /conf
      SPARK_WORKER_CORES: 4
      SPARK_WORKER_MEMORY: 6g
      SPARK_WORKER_PORT: 8881
      SPARK_WORKER_WEBUI_PORT: 8081
      SPARK_PUBLIC_DNS: localhost
    env_file:
      - ./hadoop.env
    depends_on:
      - master
    links:
      - master
    ports:
      - 8081:8081
    volumes:
      - ./data:/tmp/data
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      placement:
        constraints: [node.role == worker]

networks:
    default:
        external:
            name: hadoop-spark-swarm-network
```
I create my own network with `docker network create -d overlay hadoop-spark-swarm-network`
`
print ("user")
`

I got the error below
`
<console>:12: error: not found: value globalScope
       globalScope.sparkContext.setJobGroup("cell-A0F42CF740B04DD287456255B6E4528E", "run-1534702622531: print ('user')")
`

incompatible clusterID Hadoop

Hi,
anytime I rebooted the swarm I have this problem

> java.io.IOException: Incompatible clusterIDs in /hadoop/dfs/data: namenode clusterID = CID-b25a0845-5c64-4603-a2cb-d7878c265f44; datanode clusterID = CID-f90183ca-4d87-4b49-8fb2-ca642d46016c
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:777)

> FATAL datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to namenode/10.0.0.7:8020. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:574)

I solved this problem deleting this docker volume

> sudo docker volume inspect hadoop_datanode

`[
    {
        "CreatedAt": "2018-05-10T19:35:31Z",
        "Driver": "local",
        "Labels": {
            "com.docker.stack.namespace": "hadoop"
        },
        "Mountpoint": "/data0/docker_var/volumes/hadoop_datanode/_data",
        "Name": "hadoop_datanode",
        "Options": {},
        "Scope": "local"
    }
]
`
but in this volume are present the files which I put in hdfs, so in this way I have to to put again the files into hdfs when I deploy the swarm. I'm not  sure this is the right way to solve this problem. 
Googling I found one solution but I dont know how to applicate it before the swarm reboot, this is the solution:
_The problem is with the property name dfs.datanode.data.dir, it is misspelt as dfs.dataode.data.dir. This invalidates the property from being recognised and as a result, the default location of ${hadoop.tmp.dir}/hadoop-${USER}/dfs/data is used as data directory.
hadoop.tmp.dir is /tmp by default, on every reboot the contents of this directory will be deleted and forces datanode to recreate the folder on startup. And thus Incompatible clusterIDs.
Edit this property name in hdfs-site.xml before formatting the namenode and starting the services._

thanks.

I tried
docker exec -it namenode hadoop fs -put README.md /tmp/README.md
and got 
put: `README.md': No such file or directory