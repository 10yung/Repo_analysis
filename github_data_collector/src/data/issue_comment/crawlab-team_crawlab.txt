当我再可配置爬虫的页面 修改和增加相关自己的内容后，点击保存，但过段时间服务器上的相应目录下的内容会被清空并复原到没有被修改的状态

 crawlab-sdk 版本: v0.0.7
报错信息如下：
返回值为 data Skip value must be non-negative, but received: -99999999
crawlab spiders
Traceback (most recent call last):
  File "/Users/fantasy/anaconda3/bin/crawlab", line 10, in <module>
    sys.exit(main())
  File "/Users/fantasy/anaconda3/lib/python3.6/site-packages/crawlab/cli/__init__.py", line 116, in main
    cli()
  File "/Users/fantasy/anaconda3/lib/python3.6/site-packages/click/core.py", line 764, in __call__
    return self.main(*args, **kwargs)
  File "/Users/fantasy/anaconda3/lib/python3.6/site-packages/click/core.py", line 717, in main
    rv = self.invoke(ctx)
  File "/Users/fantasy/anaconda3/lib/python3.6/site-packages/click/core.py", line 1137, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/Users/fantasy/anaconda3/lib/python3.6/site-packages/click/core.py", line 956, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/Users/fantasy/anaconda3/lib/python3.6/site-packages/click/core.py", line 555, in invoke
    return callback(*args, **kwargs)
  File "/Users/fantasy/anaconda3/lib/python3.6/site-packages/crawlab/cli/__init__.py", line 67, in spiders
    client.list_spiders()
  File "/Users/fantasy/anaconda3/lib/python3.6/site-packages/crawlab/core/client.py", line 89, in list_spiders
    if data.get('error'):
AttributeError: 'str' object has no attribute 'get'

我把超级用户删除了，然后自己的普通用户无法创建新用户了。。。。
我只能在数据库修改，把普通用户的权限normal修改为admin。
是否加个验证机制，必须存在一个超级用户。避免没有超级用户的尴尬

**Is your feature request related to a problem? Please describe.**

Yes, I'd like my crawler restart when it crashed, no matter what the reason is.My Crawler is deployed at an aws machine out of china, it will retrive url from redis which is deployed in the mainland of china. sometimes my crawler will crash because of the network. in this case, I just want to restart my crawler.

**Describe the solution you'd like**

 I want to restart my crawler, just like the `--restart unless-stopped` in `docker run` command.

**Describe alternatives you've considered**

check the crawler every 5min, if it is crashed, restart a new job.



~/crawlab/backend# go install ./...
main.go:12:2: cannot find package "github.com/apex/log" in any of:
        /usr/lib/go-1.10/src/github.com/apex/log (from $GOROOT)
        /root/go/src/github.com/apex/log (from $GOPATH)
main.go:13:2: cannot find package "github.com/gin-gonic/gin" in any of:
        /usr/lib/go-1.10/src/github.com/gin-gonic/gin (from $GOROOT)
        /root/go/src/github.com/gin-gonic/gin (from $GOPATH)
main.go:14:2: cannot find package "github.com/gin-gonic/gin/binding" in any of:
        /usr/lib/go-1.10/src/github.com/gin-gonic/gin/binding (from $GOROOT)
        /root/go/src/github.com/gin-gonic/gin/binding (from $GOPATH)
main.go:15:2: cannot find package "github.com/spf13/viper" in any of:
        /usr/lib/go-1.10/src/github.com/spf13/viper (from $GOROOT)
        /root/go/src/github.com/spf13/viper (from $GOPATH)
config/config.go:4:2: cannot find package "github.com/fsnotify/fsnotify" in any of:
        /usr/lib/go-1.10/src/github.com/fsnotify/fsnotify (from $GOROOT)
        /root/go/src/github.com/fsnotify/fsnotify (from $GOPATH)
database/mongo.go:4:2: cannot find package "github.com/globalsign/mgo" in any of:
        /usr/lib/go-1.10/src/github.com/globalsign/mgo (from $GOROOT)
        /root/go/src/github.com/globalsign/mgo (from $GOPATH)
database/pubsub.go:8:2: cannot find package "github.com/gomodule/redigo/redis" in any of:
        /usr/lib/go-1.10/src/github.com/gomodule/redigo/redis (from $GOROOT)
        /root/go/src/github.com/gomodule/redigo/redis (from $GOPATH)
database/pubsub.go:9:2: cannot find package "github.com/pkg/errors" in any of:
        /usr/lib/go-1.10/src/github.com/pkg/errors (from $GOROOT)
        /root/go/src/github.com/pkg/errors (from $GOPATH)
lib/validate_bridge/validator.go:8:2: cannot find package "gopkg.in/go-playground/validator.v9" in any of:
        /usr/lib/go-1.10/src/gopkg.in/go-playground/validator.v9 (from $GOROOT)
        /root/go/src/gopkg.in/go-playground/validator.v9 (from $GOPATH)
mock/node.go:9:2: cannot find package "github.com/globalsign/mgo/bson" in any of:
        /usr/lib/go-1.10/src/github.com/globalsign/mgo/bson (from $GOROOT)
        /root/go/src/github.com/globalsign/mgo/bson (from $GOPATH)
mock/task.go:12:2: cannot find package "github.com/satori/go.uuid" in any of:
        /usr/lib/go-1.10/src/github.com/satori/go.uuid (from $GOROOT)
        /root/go/src/github.com/satori/go.uuid (from $GOPATH)
model/spider.go:12:2: cannot find package "gopkg.in/yaml.v2" in any of:
        /usr/lib/go-1.10/src/gopkg.in/yaml.v2 (from $GOROOT)
        /root/go/src/gopkg.in/yaml.v2 (from $GOPATH)
services/user.go:8:2: cannot find package "github.com/dgrijalva/jwt-go" in any of:
        /usr/lib/go-1.10/src/github.com/dgrijalva/jwt-go (from $GOROOT)
        /root/go/src/github.com/dgrijalva/jwt-go (from $GOPATH)
希望对于列表页和详情页能提供通过浏览器滑动选择，来进行字段提取的配置，简化通过人工总结xpath或css的方式进行字段提取
希望能够在可配置爬虫中，在生成scrapy代码时 增加是否需要分布式抓取的配置。并结合选择部署节点和任务ID，在生成代码时增加分布式调度队列相关的内容。
希望用「可配置爬虫」完成大部分简单的需求，生成好一个 Scrapy 工程后，通过修改代码做一些定制修改，然后传到 crawlab 上运行。
目前 crawlab 已有这个逻辑，希望能独立出来一个工具或者是后台的某个功能。
节点的_id随机时间会自动变更，导致历史任务节点全部消失，定时执行的任务显示待定不能执行任务，目前解决方案是写一个脚本先查询当前可用节点，再依次替换定时任务和任务的节点id信息
**Describe the bug**
Task interface updated too quickly, less than 2 seconds. 
It causes checkbox target lost. 
Please remove the auto-refresh feature for specific page. 

**To Reproduce**
Steps to reproduce the behavior:
Null. 

**Expected behavior**
Nil.

**Screenshots**
None.

**Desktop:**
 - Device: [any]
 - OS: [any]
 - Browser [any]
 - Version [v0.4.3]

**Additional context**
Zero.
