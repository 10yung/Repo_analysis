Hi, 
I'm trying to understand if I could use brigade to implement the following workflow:

- one job is already running, triggered by a push to a GitHub repository
- another commit is pushed, triggering a new event
- before starting the job for a new event, the previous running job is terminated 
- once the old job is terminated, the new one is started, allowing for only one job (the latest) at any given time.

Not knowing brigade really well, I'm not sure of which part should be modified to allow that. Would a brigade.js file be able to do those actions? Or would it make more sense to write a custom gateway and possibly use some external state or just the kubernetes API? 

Thank you in advance for taking a look at this ðŸ˜„ 
This `vcs-sidecar` failure has happened 3 times in recent memory, but very intermittently. 
Has anyone experienced this or knows of steps to prevent this failure? Thanks in advance.

Output of `kubectl logs <BRIGADE_CONTAINER_NAME> -c vcs-sidecar`:
```+ : 
+ : refs/heads/master
+ : /vcs
+ refspec=refs/heads/master
+ git ls-remote --exit-code <COMPANY_NAME>@vs-ssh.visualstudio.com:v3/<COMPANY_NAME>/<TEAM_PROJECT_NAME>/Kube-Brigade-Script refs/heads/master
+ cut -f2
Warning: Permanently added 'vs-ssh.visualstudio.com,20.44.80.98' (RSA) to the list of known hosts.
remote: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond 13.86.38.60:22
fatal: Could not read from remote repository.

Please make sure you have the correct access rights
and the repository exists.
+ full_ref=
+ git init -q /vcs
+ cd /vcs
+ retry git fetch -q --force --update-head-ok <COMPANY_NAME>@vs-ssh.visualstudio.com:v3/<COMPANY_NAME>/<TEAM_PROJECT_NAME>/Kube-Brigade-Script refs/heads/master
+ local 'n=1'
+ local 'max=5'
+ local 'delay=5'
+ true
+ git fetch -q --force --update-head-ok <COMPANY_NAME>@vs-ssh.visualstudio.com:v3/<COMPANY_NAME>/<TEAM_PROJECT_NAME>/Kube-Brigade-Script refs/heads/master
Warning: Permanently added 'vs-ssh.visualstudio.com,20.44.80.98' (RSA) to the list of known hosts.
+ break
+ retry git checkout -q --force refs/heads/master
+ local 'n=1'
+ local 'max=5'
+ local 'delay=5'
+ true
+ git checkout -q --force refs/heads/master
error: pathspec 'refs/heads/master' did not match any file(s) known to git.
+ test 1 -lt 5
+ echo 'Command failed. Attempt 1/5. Waiting for 5 seconds before retrying.'
+ sleep 5
Command failed. Attempt 1/5. Waiting for 5 seconds before retrying.
+ n=2
+ true
+ git checkout -q --force refs/heads/master
error: pathspec 'refs/heads/master' did not match any file(s) known to git.
+ test 2 -lt 5
+ echo 'Command failed. Attempt 2/5. Waiting for 10 seconds before retrying.'
+ sleep 10
Command failed. Attempt 2/5. Waiting for 10 seconds before retrying.
+ n=3
+ true
+ git checkout -q --force refs/heads/master
error: pathspec 'refs/heads/master' did not match any file(s) known to git.
Command failed. Attempt 3/5. Waiting for 15 seconds before retrying.
+ test 3 -lt 5
+ echo 'Command failed. Attempt 3/5. Waiting for 15 seconds before retrying.'
+ sleep 15
+ n=4
+ true
+ git checkout -q --force refs/heads/master
error: pathspec 'refs/heads/master' did not match any file(s) known to git.
+ test 4 -lt 5
+ echo 'Command failed. Attempt 4/5. Waiting for 20 seconds before retrying.'
+ sleep 20
Command failed. Attempt 4/5. Waiting for 20 seconds before retrying.
+ n=5
+ true
+ git checkout -q --force refs/heads/master
error: pathspec 'refs/heads/master' did not match any file(s) known to git.
+ test 5 -lt 5
+ fail 'The command has failed after 5 attempts.'
+ echo The command has failed after 5 attempts.
The command has failed after 5 attempts.
+ exit 1
```



Output of `brig version`: v1.0.0-23-g1b849be

Output of `kubectl version`: Client Version: version.Info{Major:"1", Minor:"14", GitVersion:"v1.14.0", GitCommit:"641856db18352033a0d96dbc99153fa3b27298e5", GitTreeState:"clean", BuildDate:"2019-03-25T15:53:57Z", GoVersion:"go1.12.1", Compiler:"gc", Platform:"darwin/amd64"}
Server Version: version.Info{Major:"1", Minor:"14", GitVersion:"v1.14.8", GitCommit:"1da9875156ba0ad48e7d09a5d00e41489507f592", GitTreeState:"clean", BuildDate:"2019-11-14T05:19:20Z", GoVersion:"go1.12.10", Compiler:"gc", Platform:"linux/amd64"}

Cloud Provider/Platform (AKS, GKE, Minikube etc.): AKS

If you use the `brig` CLI to create projects, you don't have many options for adding values (ex: sshKey) after initial creation.

The options as I currently see them are:
* using `brig` CLI:
  - `brig project get org/project-name > project.json`
  - edit the json to include your desired values
  - `brig project delete org/project-name`
  - `brig project create -f project.json -x`
* using `kubectl`: 
  - `kubectl edit secret brigade-projectid`
  - add/modify additional values in plaintext under the stringData top-level key, rather than data, allowing the API to base64 encode the values.
  - OR: base64 encode the values yourself, and add them to the data key.
* use the helm chart to create projects.
  - this chart has all of the hooks to get/set/modify values built in already
  - this method is considered deprecated, so it probably shouldn't be used/recommended


Proposals:
* create `brig project edit` verb that bring up the project in $EDITOR, similar to `kubectl edit`
* create `brig project get-value` and `brig project set-value` verbs
* create a helm-like `--set` flag on one or more of the existing verbs

This probably isn't a huge deal once you've got a project established and working, but I'm currently in the experimentation phase, and the effort required to add/modify a value seems higher than it should be.

I realize this may be nullified by whatever v2.0's configuration document/mechanism is, but project modification should still be a consideration going forward.
A manual input/configmation step would be useful in some types of workloads.

For example it could be implemented by adding a `service` (ingress+service+deployment) to `Brigade.js API` or by combination of dependecies in a project trigger (an event from another project + generic gateway with an extra input or one time secret).
It would be easier to manage secrets if project's configuration would only have references to particular secrets.

E.g. this could allow jobs running in different namespaces to directly access secrets deployed there.
Generic gateway secret should be stored securely as a [hash](https://cheatsheetseries.owasp.org/cheatsheets/Password_Storage_Cheat_Sheet.html#leverage-an-adaptive-one-way-function) (e.g. Argon2).
Right now Generic Gateway's secret is send in a [path](https://docs.brigade.sh/topics/genericgateway/#using-the-generic-gateway). It could be logged by proxies. Common approach is to use [Authorization HTTP Header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Authorization).
There are cases where a gateway, in its capacity as the "event broker" seems like the logical candidate to take responsibility for reporting build and/or job status upstream to the original source of the event.

To illustrate, let's consider a scenario we're familiar with today-- GitHub webhooks and statuses reported back to GH via its checks API-- and let's consider how our handling of that could be improved.

Currently:

1. GW receives a `check_suite:requested` or `check_suite:requested` event (for example) from GitHub.

1. Knowing that workers and jobs have no inherent permissions to report status back to GH using the checks API, the gateway "wraps" the original payload in a new object that also encapsulates a token. This new object becomes the payload that is added to the event that is emitted into Brigade.

1. The author of the `brigade.js` _needs to be aware_ that depending on the type of event they are receiving there may or may not be a wrapper around the original payload. (ðŸ˜ž) In the case of a `check_suite:requested` or `check_suite:requested` event (for example), they extract the original payload from the wrapper before proceeding.

1. The author of the `brigade.js` may optionally retrieve the token that the gateway added to the payload. This can be utilized in creating "job sandwiches" wherein every principal job in the pipeline is preceded and followed by additional jobs that use the token and the GH checks API to report job status.

This process is problematic mostly because `brigade.js` authors need to know too much about how the gateway has handled specific event payloads _and_ `brigade.js` authors are left responsible for something the gateway probably _could_ have handled itself.

_It also delegates the worker permissions that it doesn't otherwise need._

An improved process could look like this:

1. GW receives an event of any type from GH and treats them all the same. Every payload is passed on _as is_.

1. Knowing that workers and jobs have no inherent permissions to report status back to GH using the checks API, for applicable event types, the gateway adds a callback URL _to a new field on the event_ (not to a wrapper around the payload).

1. The author of the `brigade.js` can use the callback URL to report status _to the gateway_ as each job starts and again as each completes. Alternatively, brigadier could be updated to do this automatically on job start and completion if/when such a callback URL is defined.

1. The GW receives the callback and uses the GH checks API to report status back to GH.

Please keep in mind that GH and the GH gateway are used above only for illustration. This proposal could quite easily streamline the process of status reporting for quite a wide variety of events from quite a wide variety of sources.
Currently, the controller passes most configuration to a worker through environment variables set on the worker's container. _Some_ configuration, however, is obtained by the worker through an alternative channel-- _it must use the k8s API to locate and read the project secret._

At minimum, the worker could be spared some effort by the controller mounting the project secret to the file system. This lowers the bar for what is expected of custom workers _and_ possibly moves us toward further restricting permissions on workers to make Brigade more secure on the whole.

Better still, we could consider consolidating the two methods through which the controller passes configuration to the worker so that we don't have _some_ set as environment variables and other configuration passed in via a secret.
Related to #1034 

Currently, the only record of a build is its underlying Kubernetes secret (see #1036). Related-- the ability to inspect a build's results (view logs, etc.) depends entirely on the continued existence of the pods that implemented that build's jobs. This effectively presents users with a choice between accepting a cluster littered with completed pods or cleaning up periodically and losing a potentially valuable papertrail of build results-- which for some use cases in some industries may not be an option.

It seems that there would be a lot to be gained by streaming worker and job logs somewhere where they can be persisted. Doing so would enable us to clean up completed job pods more aggressively-- perhaps even having the worker do this immediately upon completion.