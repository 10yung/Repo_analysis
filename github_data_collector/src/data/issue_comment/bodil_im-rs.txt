**Short Version:**
I was expecting the data structure used in im to be provide a large performance boost when "merging" to similar HashMap (coming from the same "ancestors" HashMap). It does not. I wonder is there are low hanging fruit to perform such optimisation with the data structure used.

**Long Version:**

We are writing Rust for the Mercurial DVCS. We started investigate the use of IM for an algorithm that travel a DAG, agregating information along the way. (For the record, The information is copies/renames data, They exist as a source/destination mapping.).
Not all DAG nodes carries information, and the information is usually (but not always small). The agregation of these small piece can become larges (or we sometime encounter node with a lot of renames).

The DAG can contains a lots of branch-point | merge point (up to 30% merge point for some users…). Each time there is a branch-point we basically have to duplicated the map to update it independently, and each time there is a merge, we have to merge the two maps together again (one of the side has priority). In many case the two maps are initially copied from the same one, and only have a small faction of changes (or None at all) on each side. This looks like a job for persistent data structure

Switching to `rust + im` had gave the intended result in regards with copies. Branch point is mostly cheap now. However, the extend/unions call does not benefit from the data structure. It seems like the implementation is mostly calling `insert` in a loop

Do you think there would be a reasonably easy way to leverage the underlying data structure to quickly recognise similar data section during unions/extends.

**Examples**

Here is a basic rust source showing a simplied example of the pattern we have to deal with. 
[main.rs.txt](https://github.com/bodil/im-rs/files/3859819/main.rs.txt)

And here is the rust code we used to try this out on real data: https://dev.heptapod.net/octobus/mercurial-devel/merge_requests/4/diffs#24a275295c8f5ee4028a08d6da11d9f65cb5871c



In particular provided implementations of `len`, `count`, `nth`, and `nth_back` for `Iter`, `IterMut`, `Chunks`, `ChunksMut` that make use of `front_index` and `back_index` to improve performance.

Similar work might also be useful for `ConsumingIter`, but I couldn't easily trace that code to see where/if such improvements would make sense.
Hi, I have a (hopefully small) feature request: Could `OrdSet<T>` expose a method `find_lesser` that takes a `&T` and returns a reference to the greatest value in the set that is less than or equal to `T`, or `None` if no such element exists? Analogously `find_greater` (returns least value that is greater or equal to the argument), and then the same for `OrdMap` (operating on the keys).

From a quick look at the code, it seems like the lookup implementation could support this, but it would take me a while to fully understand what exactly is going on in order to implement this myself.
It might be interesting to have some benchmark results in readme, comparing `im`, `im-rc` and `std::collections`, to get a rough estimate of the overhead of immutability and atomic reference counting.  Context: I actually want to know about `rc` vs `Arc` specifically, and though that `im` might be the ideal test-case!
Hello

From the docs in 12.3:

> O(1)* means 'amortised O(1),' which means that an operation usually runs in constant time but will occasionally be more expensive: for instance, Vector::push_back, if called in sequence, will be O(1) most of the time but every 64th time it will be O(log n), as it fills up its tail chunk and needs to insert it into the tree. Please note that the O(1) with the asterisk attached is not a common notation; it's just a convention I've used in these docs to save myself from having to type 'amortised' everywhere.

This is AFAIK wrong. Amortized means that while *one* operation may be higher than the advertised complexity, n operations in a sequence must be bounded by n* the advertised complexity. For example, the classical std::vector::Vec has O(1) amortized push ‒ while a single push can take O(n) to reallocate and grow to twice the size, it happens on every (1/2)^n pushes so it sums to something like ~2 ‒ the trick there is that the bigger the reallocations, the rarer they happen.

On the other hand, O(log n) every *constant* number of elements is still amortized to O(log n).
In the standard library, the only collection that allocates when you call `new()` is `VecDeque`, which will allocate space for 7 elements.

By comparison, the collections in this library allocate a lot more. Using an element type of `u64` for testing:
* `HashSet` (and `HashMap`) will allocate a small amount for the hasher (which is typically a 0-size or Copy type) and 536 bytes of space
* `OrdSet` allocates 1080 bytes
* `Vector` allocates 544 bytes

I'm not sure if this is easily fixable without a performance penalty but it would be nice to not allocate at all.
I haven't seen too much prior art in the persistent sphere, but finger trees seem to be the way they go for it in haskell.

Loving your work here so far btw, thanks ever so much :)
Hi!

First of all: this is amazing work. I was long waiting for someone to go ahead and do this. The code looks very good and you also seem to be very fast in making progress! :smile: 

I myself have been doing some work on bringing Clojure-like data-structures to system languages, but in the ugly land of C++, which resulted in the [Immer library](https://github.com/arximboldi/immer) and [this paper](https://public.sinusoid.es/misc/immer/immer-icfp17.pdf). One of the things that took most effort there (and brought most complexity) is to try achieve the most cache locality possible. In particular, vector elements are not boxed individually but live in the same array block. Studying your code, it seems to me that you are boxing the elements individually, and it seems to me that it would be impossible to do otherwise without adding ridiculous amounts of `unsafe {}` code. Do you have some thoughts on that?

Also, do you plan to implement catenable vectors (Relaxed Radix Balanced trees)?

Once again, congrats! I would be interested also in trying to do some benchmarking between the libraries. Let me know eventually if you would like to work together on that to try to find useful and fair benchmarks!

Cheers!
[This](https://doc.rust-lang.org/std/collections/#when-should-you-use-which-collection) and [this](https://doc.rust-lang.org/std/collections/#performance) is what I mean.

Maybe also performance comparisons of data structures with std::collections equivalents.

If you're interested I'll write something up in the next couple of days :)