## Description
<!-- Brief description. Refer to the related issues if existed.
It'll be great if relevant reviewers can be assigned as well.-->

## Checklist
Please feel free to remove inapplicable items for your PR.
- [ ] The PR title starts with [$CATEGORY] (such as [NN], [Model], [Doc], [Feature]])
- [ ] Changes are complete (i.e. I finished coding on this PR)
- [ ] All changes have test coverage
- [ ] Code is well-documented
- [ ] To the my best knowledge, examples are either not affected by this change,
      or have been fixed to be compatible with this change
- [ ] Related issue is referred in this PR

## Changes
<!-- You could use following template
- [ ] Feature1, tests, (and when applicable, API doc)
- [ ] Feature2, tests, (and when applicable, API doc)
-->

## Description
<!-- Brief description. Refer to the related issues if existed.
It'll be great if relevant reviewers can be assigned as well.-->

## Checklist
Please feel free to remove inapplicable items for your PR.
- [ ] The PR title starts with [$CATEGORY] (such as [NN], [Model], [Doc], [Feature]])
- [ ] Changes are complete (i.e. I finished coding on this PR)
- [ ] All changes have test coverage
- [ ] Code is well-documented
- [ ] To the my best knowledge, examples are either not affected by this change,
      or have been fixed to be compatible with this change
- [ ] Related issue is referred in this PR

## Changes
<!-- You could use following template
- [ ] Feature1, tests, (and when applicable, API doc)
- [ ] Feature2, tests, (and when applicable, API doc)
-->

## Description
This implements fixed-length random walk operator described in #1199 .

Although the branch name is called `pinsage-refactor`, this is only the very first step towards actually refactoring the PinSAGE implementation.

## Checklist
Please feel free to remove inapplicable items for your PR.
- [x] The PR title starts with [$CATEGORY] (such as [NN], [Model], [Doc], [Feature]])
- [ ] Changes are complete (i.e. I finished coding on this PR)
  - [x] API
  - [ ] Migrate examples from existing `random_walk` calls to the new one.
- [x] All changes have test coverage
- [x] Code is well-documented
- [x] To the my best knowledge, examples are either not affected by this change,
      or have been fixed to be compatible with this change
- [x] Related issue is referred in this PR

## Misc Changes
* New C++ NDArray operator `Assign(array, index)` and `IsEmpty(array)`.
* New NDArray constructor `VecToNDArray` that works with int32/64 and float32/64.
* `EXPECT` macros to perform array type checks.
* `HeteroGraph::GetEndpointTypes(dgl_type_t etype)` to get node types of both endpoints given the edge type.
* `RandomEngine::Choice(FloatArray array)` that does non-uniform sampling using `TreeSampler`.
## Description
Fix for https://github.com/dmlc/dgl/issues/1186

## Checklist
Please feel free to remove inapplicable items for your PR.
- [x] The PR title starts with [$CATEGORY] (such as [NN], [Model], [Doc], [Feature]])
- [x] Changes are complete (i.e. I finished coding on this PR)
- [ ] All changes have test coverage
- [ ] Code is well-documented
- [x] To the my best knowledge, examples are either not affected by this change,
      or have been fixed to be compatible with this change
- [x] Related issue is referred in this PR

## üêõ Bug

In the RGCN [entity classification example](https://github.com/dmlc/dgl/blob/master/examples/pytorch/rgcn/entity_classify.py), the model has a softmax activation, but the loss function, [F.cross_entropy](https://pytorch.org/docs/stable/nn.functional.html#cross-entropy), expects raw logits.

Unless I'm mistaken, either the last layer should have no activation, or the loss should be changed to F.nll_loss.

## To Reproduce

Running the example. Setting the activation to None has no effect on the score for AIFB. I can't easily test larger data like AM atm. If I add a double softmax to my own implementation of RGCN the performance on AM completely collapses (but then, I'm still debugging it).

## üìö Documentation
It's very memory consuming to generate one-hot encoding with eye(). We should replace it with an embedding layer.

## üöÄ Feature
We have finally settled down (in principle) the design of the long-awaited sampling on heterogeneous graphs, and we have started working on it.

This issue keeps track of the implementation progress.

**You're welcome to read the proposal publicly available on https://docs.google.com/document/d/1cVvbM_QtaIN0UsqrgB0FhlYti1PjXua2QbSAoMaE2ng/edit**

The document is not fixed and it is subject to changes: you're welcome to provide feedback on either the proposal itself or this issue thread.

## Task tracker

- [ ] C APIs
  - [ ] Neighbor sampling
    - [ ] sample_neighbors
    - [ ] sample_neighbors_topk [@BarclayII ]
    - [ ] in_subgraph
    - [ ] out_subgraph
    - [ ] sample_layerwise
  - [ ] Random walks
    - [ ] random_walk [@BarclayII ]
    - [ ] random_walk_with_restart [@BarclayII ]
    - [ ] node2vec_random_walk
  - [ ] Misc
    - [ ] `dgl::aten` refactor to cover float types as well (only necessary ones)
    - [ ] create_graph_from_paths [@BarclayII ]
    - [ ] compact_graphs [@BarclayII ]
    - [ ] to_simple_graph [@BarclayII ]
    - [ ] Graph.unique_src()
    - [ ] Graph.unique_dst()
    - [x] dgl.choice(prob_array, num_samples) (faster numpy.random.choice) [@yzh119, already done in #1142 , may a python interface is required? ]
    - [ ] Graph.remove_edges() [@BarclayII ]
- [ ] Models (rewrite)
  - [ ] GCN
  - [ ] GCN with Control Variate Sampling
  - [ ] GraphSAGE (with distributed)
  - [ ] GraphSAGE with Control Variate Sampling
  - [ ] PinSAGE (with distributed) [@BarclayII ]
  - [ ] GCMC
  - [ ] RGCN?
  - [ ] GAT?
  - [ ] FastGCN?
  - [ ] DGI?
  - [ ] Adaptive Layerwise Sampling?

Related:
* Unification of DGLGraph, DGLHeteroGraph, SubGraph, BatchedGraph, etc.
* Shared memory storage

## Description
<!-- Brief description. Refer to the related issues if existed.
It'll be great if relevant reviewers can be assigned as well.-->

## Checklist
Please feel free to remove inapplicable items for your PR.
- [x] The PR title starts with [$CATEGORY] (such as [NN], [Model], [Doc], [Feature]])
- [x] Changes are complete (i.e. I finished coding on this PR)
- [x] All changes have test coverage
- [x] Code is well-documented
- [x] To the my best knowledge, examples are either not affected by this change,
      or have been fixed to be compatible with this change
- [x] Related issue is referred in this PR

## Changes
<!-- You could use following template
- [ ] Feature1, tests, (and when applicable, API doc)
- [ ] Feature2, tests, (and when applicable, API doc)
-->

## Description
Balanced relation partition, when using gpu train relations are stored in cpu
Strict relation partition, when using gpu train relations are streod in gpu

For Balanced relation partition run in Freebase Ôºà32 partitions):
|Train|: 304727650
relation partition 304727650 edges into 32 parts
part 0 has 9522740 edges and 1 relations
part 1 has 9522740 edges and 1 relations
part 2 has 9522739 edges and 599 relations
part 3 has 9522740 edges and 1 relations
part 4 has 9522739 edges and 609 relations
part 5 has 9522740 edges and 1 relations
part 6 has 9522739 edges and 609 relations
part 7 has 9522740 edges and 1 relations
part 8 has 9522739 edges and 609 relations
part 9 has 9522740 edges and 1 relations
part 10 has 9522739 edges and 609 relations
part 11 has 9522740 edges and 1 relations
part 12 has 9522739 edges and 611 relations
part 13 has 9522740 edges and 1 relations
part 14 has 9522739 edges and 611 relations
part 15 has 9522739 edges and 600 relations
part 16 has 9522739 edges and 600 relations
part 17 has 9522739 edges and 603 relations
part 18 has 9522739 edges and 606 relations
part 19 has 9522739 edges and 609 relations
part 20 has 9522739 edges and 610 relations
part 21 has 9522739 edges and 610 relations
part 22 has 9522739 edges and 610 relations
part 23 has 9522739 edges and 610 relations
part 24 has 9522739 edges and 610 relations
part 25 has 9522739 edges and 611 relations
part 26 has 9522738 edges and 610 relations
part 27 has 9522738 edges and 610 relations
part 28 has 9522738 edges and 610 relations
part 29 has 9522738 edges and 610 relations
part 30 has 9522738 edges and 610 relations
part 31 has 9522738 edges and 610 relations

## Checklist
Please feel free to remove inapplicable items for your PR.
- [ ] The PR title starts with [$CATEGORY] (such as [NN], [Model], [Doc], [Feature]])
- [ ] Changes are complete (i.e. I finished coding on this PR)
- [ ] All changes have test coverage
- [ ] Code is well-documented
- [ ] To the my best knowledge, examples are either not affected by this change,
      or have been fixed to be compatible with this change
- [ ] Related issue is referred in this PR

## Changes
<!-- You could use following template
- [ ] Feature1, tests, (and when applicable, API doc)
- [ ] Feature2, tests, (and when applicable, API doc)
-->

In https://github.com/dmlc/dgl/blob/d30a69bf277effd9a013ddd81fa33661f2e31a01/python/dgl/backend/mxnet/tensor.py#L20, we have `mx.set_np_shape(True)`. Since at this moment MXNet and its community packages (GluonCV/GluonNLP etc) are not fully compatible with MXNet's np shape feature, users need to be able to turn it on/off, for example through an environmental variable.