*Preparing:*

Download Bistro from http://casual-effects.com/data/index.html

*Running:*

`lodviewer.exe bistro/Exterior/exterior.obj`

*Observing:*

Damaged texture coordinates. Multiple holes in the mesh.

![image](https://user-images.githubusercontent.com/2510143/71030055-b99e0600-2121-11ea-977d-25e4a3598588.png)

![image](https://user-images.githubusercontent.com/2510143/71030072-c3276e00-2121-11ea-84a3-81b65335bd67.png)

I am using meshoptimizer's simplification methods to simplify 3D scans w/ have been triangulated using marching cubes. The meshes are far more dense than they need to be, so I am attempting to simplify them down to 20% of their original density.

Unfortunately the result is unusable and full of holes and other defects. Is this normal?

Here is an example:

**Input Mesh**

![image](https://user-images.githubusercontent.com/19617165/70648446-0d3bbb80-1c19-11ea-8379-0a533e00a81a.png)


**Simplified Result**

![image](https://user-images.githubusercontent.com/19617165/70648475-19c01400-1c19-11ea-8b7e-4820d6bb6391.png)

I am using MeshLab to visualize the result. The meshes are exported to STL format.

The mesh format I am using internally in my software is that of the VCG library, but I think I am appropriately converting the data such that meshoptimizer's simplification methods can operate correctly.

Here is how I am converting the data:

```
const MeshVert* __restrict root = input.vert.data();
std::vector<unsigned> remap;
std::vector<vcg::Point3f> vertices;

for (const auto& v : input.vert) 
    vertices.emplace_back(v.P());

for (const auto& f : input.face) {
    remap.emplace_back(unsigned((MeshVert*)(f.cV(0)) - (MeshVert*)root));
    remap.emplace_back(unsigned((MeshVert*)(f.cV(1)) - (MeshVert*)root));
    remap.emplace_back(unsigned((MeshVert*)(f.cV(2)) - (MeshVert*)root));
}

const size_t index_count = remap.size();
std::vector<unsigned> simplified(index_count);
size_t target_count = simplification_factor * index_count;
float target_error = 1e-3f;

if (restricted)
    simplified.resize(meshopt_simplify(&simplified[0], remap.data(), index_count, (const float* __restrict)&vertices[0], vertices.size(), sizeof(vcg::Point3f), target_count, target_error));
else
    simplified.resize(meshopt_simplifySloppy(&simplified[0], remap.data(), index_count, (const float* __restrict)&vertices[0], vertices.size(), sizeof(vcg::Point3f), target_count));
```

I'm a little confused by the concept of "Index" in this mesh processing library. Are the indexes here referring to triplets of offsets from Vertex_Ptr* + 0 which represent each face/triangle?

The VCG mesh has a vector of vertices, each represented by a Point3f type (the internal data of which is 3 floats - X, Y, and Z). The faces contain 3 vertex pointers. I compute the offset/index from these pointers by subtracting the start of the vertex array (root).

Is this the correct approach? Am I doing something wrong? The result looks *reasonable*, but not entirely correct. Perhaps this is just the nature of decimation?
Hi, I integrated your optimizer for generating collision meshes by using `meshopt_simplify` followed by `meshopt_optimizeVertexFetch`. Unfortunately, on some meshes, the decimated result has some parts aggressively simplified, and some other parts have redundants triangles conserved; vertices have no attributes other than their position.
Does it come from the topology of the mesh ? My settings ?
The most noticeable come from an airport terminal, as you can see [here](https://imgur.com/a/FL26Tbp); some parts are really reduced while others seem to be ignored.
The target error were 0.001, then 0.1 and the vertices conserved were respectively 64.5% and 63%.

Models as used (only positions for vertex attribute), exported as .OBJ:
[Original](http://s000.tinyupload.com/download.php?file_id=99133989768181263339&t=9913398976818126333930085)    [Decimated](http://s000.tinyupload.com/download.php?file_id=89301722578383214643&t=8930172257838321464377590)
When I use the aggressive option in `gltfpack` I find that the model shading looks off. This appears to be caused by the normal being a bit messy. Attached is a screen shot from the babylon.js viewer with normal preview turned on for one of the meshes (hence the beautiful rainbow).

<img width="1029" alt="Screen Shot 2019-10-24 at 3 00 30 PM" src="https://user-images.githubusercontent.com/247092/67528892-79fdf500-f66f-11e9-8246-ef4db493273d.png">

I'm not sure if this is an unavoidable artifact, or something that can be improved. Also, I realize the input model is already pretty sparse. Nonetheless, the vertex removals seem reasonable, if only the normals were better preserved.

Here's an example of the command I'm using. I've also attached the input and output GLBs I'm using.

`gltfpack -i /tmp/input.glb -o /tmp/output.glb -si 0.8 -sa -v`

[Archive.zip](https://github.com/zeux/meshoptimizer/files/3769644/Archive.zip)


I kinda calculate an approximate error metric roughly based on hausdorff distance between the 2  mesh after generating the simplified one. Is there any chance to avoid this calculation and use some consistent object space error metric used internally by meshopt_simplify()?

Thanks in advance.
BTW very handy library. much appreciated.