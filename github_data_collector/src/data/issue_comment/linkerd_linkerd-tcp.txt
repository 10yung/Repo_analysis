Hello-

I've deployed linkerd as well as a couple sample applications with linkerd-tcp containers. We've successfully applied cfssl and openssl certificates and keys, but are having trouble with keys generated with Vault. Our pk8-encoded key will throw the error "WrongNumberOfKeysInPrivateKeyFile" and our regular .key file will throw "FailedToConstructPrivateKey" errors. There does not appear to be any errors with the certificates on the client side. We have similar vault-generated certs that work with our main linkerd service mesh.

client configuration:
```
    routers:
...
      client:
        kind: io.l5d.static
        configs:
        - prefix: /svc/server
          connectTimeoutMs: 400
          tls:
            dnsName: "server.default.svc.cluster.local"
            trustCerts:
            - /io.buoyant/linkerd/certs/tls.chain
```

server configuration:
```
    routers:
...
      servers:
      - ip: 0.0.0.0
        port: 7474
        dstName: /$/inet/127.1/80
        tls:
          defaultIdentity:
            privateKey: /io.buoyant/linkerd/certs/tls.key
            certs:
            - /io.buoyant/linkerd/certs/tls.crt
``` 

Is there any reason that this may be happening to Vault keys and not other private keys?
Is there any timeline for Alpha release? Or this beta version is ready for production build?
I've been trying to build linkerd-tcp from source (since the docker image doesn't include the latest version, see #82), but I had to make the some changes in order to make it work.

- Changing the rustls commit ID (as instructed in #79)
- Changing the rust image from `jimmycuadra/rust:1.16.0` to `rust:1.22.1` in the `dockerize` script, otherwise I get Rust compilations errors.

Would you like a PR to fix that?
This change hides Namerd behind a Namer interface so that the Namer can be dynamically configurable.  I'm looking for feedback on the general approach taken here, as well as code style and idioms.  

The main challenge I ran into was making the builder pattern polymorphic.  An executor contains _something_ on which I can call `with_handle` which returns _something_ on which I can call `resolve` which returns _something_ that implements Stream.  Furthermore, I think dynamic dispatch is necessary since the actual choice of implementation will be dynamically determined by a config file.

I do not plan to merge this as is.  After getting feedback on this I will remove the namerd implementation and replace it with a static namer.
Currently k8s ingress only supports HTTPS SNI.

I need k8s support for TLS SNI such that I can dynamically create TCP services with virtual server names and have a dynamically created TCP SNI reverse proxy dispatch connections to the correct k8s service.

I see that the linked-tcp beta is available and supports SNI. I see that linked-tcp integrates with the k8s API via namerd. I see some info on configuring namerd for k8s. 

Since I’m hosting k8s on AWS, I’m assuming that the I would be using a loadbalancer service (that creates an ELB instance) as the internet entry point for TCP connections. This would load balance connections across instances of linked-tcp (that have been plumbed-into k8s via namerd).

What I don’t see is the full set of k8s resources that are required to get this to work.

Has anyone done this? What is the best way to get this configured?

```
$ curl -X POST -vvvv  http://127.0.0.1:9989/shutdown
*   Trying 127.0.0.1...
* TCP_NODELAY set
* Connected to 127.0.0.1 (127.0.0.1) port 9989 (#0)
> POST /shutdown HTTP/1.1
> Host: 127.0.0.1:9989
> User-Agent: curl/7.52.1
> Accept: */*
>
< HTTP/1.1 200 OK
< Content-Length: 0
< Date: Tue, 11 Jul 2017 22:50:51 GMT
<
* Curl_http_done: called premature == 0
* Connection #0 to host 127.0.0.1 left intact
```
A few minutes later, the process is still running.
testing linkerd-tcp with the stock example.yml and slow_cooker at 20k qps is resulting in large waves of EOFs.

```
Get http://proxy-test-4d:7474: EOF                                                                                                                              [96/25466]
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
2017-06-28T18:53:27Z  19829/0/181 20000  99% 10s   0 [  2   4   5   18 ]   18      0
...
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: read tcp 10.240.0.4:47703->10.240.0.21:7474: read: connection reset by peer
2017-06-28T18:54:27Z  19819/0/181 20000  99% 10s   0 [  2   4   6   14 ]   14      0
...
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
2017-06-28T18:55:27Z  19817/0/182 20000  99% 10s   0 [  1   4   5  206 ]  206      0
...
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
2017-06-28T18:56:27Z  19828/0/172 20000  99% 10s   0 [  2   4   6    9 ]    9      0
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
Get http://proxy-test-4d:7474: EOF
2017-06-28T18:57:27Z  19839/0/161 20000  99% 10s   0 [  3   5   7    9 ]    9      0
```

Interestingly, they seem to come in groups of 170-200 at a time about every 60 seconds.
It would be nice to have linkerd-tcp run fully standalone and load all of the configuration settings / routing / etc.. from files.
It would be usefull in environments where TLS is terminated on different machienes than the loadbalancer and client information (like the source IP) needs to be preserved.

See https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt for a description of the protocol.

I can try to implement it as I have some rust experience but I never worked with tokio directly so I have to see.
## Prototype

The initial implementation is basically a prototype. It proves the concept, but it has
severe deficiencies that cause performance (and probably correctness) problems.
Specifically, it implements its own polling... poorly.

At startup, the configuration is parsed. For each **proxy**, the namerd and serving
configurations are split and connectd by an async channel so that namerd updates are
processed outside of the serving thread. All of the namerd watchers are collected to be
run together with the admin server. Once all of the proxy configurations are processed,
the application is run.

The admin thread is started, initiating all namerd polling and starting the admin server.

Simultaneously, all of the proxies are run in the main thread. For each of these, a
**connector** is created to determine how all downstream connections are established for
the proxy. A **balancer** is created with the connector and a stream of namerd updates. An
**acceptor** is created for each listening interface, which manifests as a stream of
connections, connections. The balancer is made shareable across servers by creating an
async channel and each server's connections are streamed into a sink clone. The balancer
is driven to process all of these connections.

The balancer implements a Sink that manages _all_ I/O and connection management. Each
time `Balancer::start_send` or `Balancer::poll_complete` is called, the following work is
done:
- _all_ conneciton streams are checked for I/O and data is transfered;
- closed connections are reaped;
- service discovery is checked for updates;
- new connections are established;
- stats are recorded;

## Lessons/Problems

### Inflexible

This model doesn't really reflect that of linkerd. We have no mechanism to _route_
connections. All connections are simply forwarded. We cannot, for instance, route based on
client credentials or SNI destination.

### Inefficient

Currently, each balancer is effectively a scheduler, and a pretty poor one at that. I/O
processing should be far more granular and we shouldn't update load balancer endpoints in
the I/O path (unless absolutely necessary).

### Timeouts

We need several types of timeouts that are not currently implemented:
- Connection timeout: time from incoming connection to outbound established.
- Stream lifetime: maximum time a stream may stay open.
- Idle timeout: maximum time a connection may stay open without transmitting data.

## Proposal

linkerd-tcp should become a _stream router_. In the same way that linkerd routes requests,
linkerd-tcp should route connections. The following is a rough, evolving sketch of how
linkerd-tcp should be refactored to accomodate this:

The linkerd-tcp configuration should support one or more **routers**. Each router is
configured with one or more **servers**. A server, which may or may not terminate TLS,
produces a stream of incoming connections comprising an envelope--a source identity (an
address, but maybe more) and a destination name--and a bidirectional data stream. The
server may choose the destination by static configuration or as some function of the
connection (e.g. client credentials, SNI, etc). Each connection envelope may be annotated
with a standard set of metadata including, for example, an optional connect deadline,
stream deadline, etc.

The streams of all incoming connections for a router are merged into a single stream of
enveloped connections. This stream is forwarded to a **binder**. A binder is responsible
for maintaining a cache of balancers by destination name. When a balancer does not exist
in the cache, a new namerd lookup is initiated and its result stream (and value) is cached
so that future connections may resolve quickly. The binder obtains a **balancer** for each
destination name that maintains a list of endpoints and their load (in terms of
connections, throughput, etc).

If the inbound connection has not expired (i.e. due to a timeout), it is dispatched to the
balancer for processing. The balancer maintains a reactor handle and initiates I/O and
balancer state management on the reactor.

```
 ------       ------
| srv0 | ... | srvN |
 ------   |   ------
          |
          | (Envelope, IoStream)
          V
 -------------------      -------------
| binder            |----| interpreter |
 -------------------      -------------
  |
  V
 ----------
| balancer |
 ----------
  |
  V
 ----------
| endpoint |
 ----------
  |
  V
 --------
| duplex |
 --------
```
