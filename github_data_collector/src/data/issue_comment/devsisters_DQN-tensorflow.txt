
I don't know why this error shows up. Please help me out here.

[*] GPU : 1.0000
2019-05-23 23:34:52.437286: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
{'_save_step': 500000,
 '_test_step': 50000,
 'action_repeat': 4,
 'backend': 'tf',
 'batch_size': 32,
 'cnn_format': 'NHWC',
 'discount': 0.99,
 'display': True,
 'double_q': False,
 'dueling': False,
 'env_name': 'Breakout-v0',
 'env_type': 'detail',
 'ep_end': 0.1,
 'ep_end_t': 100000,
 'ep_start': 1.0,
 'history_length': 4,
 'learn_start': 50000.0,
 'learning_rate': 0.00025,
 'learning_rate_decay': 0.96,
 'learning_rate_decay_step': 50000,
 'learning_rate_minimum': 0.00025,
 'max_delta': 1,
 'max_reward': 1.0,
 'max_step': 50000000,
 'memory_size': 100000,
 'min_delta': -1,
 'min_reward': -1.0,
 'model': 'm1',
 'random_start': 30,
 'scale': 10000,
 'screen_height': 84,
 'screen_width': 84,
 'target_q_update_step': 10000,
 'train_frequency': 4}
WARNING:tensorflow:From /home/tejask98/Desktop/DQN-tensorflow/dqn/agent.py:225: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.
Instructions for updating:
Use the `axis` argument instead
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
 [*] Loading checkpoints...
 [!] Load FAILED: checkpoints/Breakout-v0/min_delta--1/max_delta-1/history_length-4/train_frequency-4/target_q_update_step-10000/double_q-False/memory_size-100000/action_repeat-4/ep_end_t-100000/dueling-False/min_reward--1.0/backend-tf/random_start-30/scale-10000/env_type-detail/learning_rate_decay_step-50000/ep_start-1.0/screen_width-84/learn_start-50000.0/cnn_format-NHWC/learning_rate-0.00025/batch_size-32/discount-0.99/max_step-50000000/max_reward-1.0/learning_rate_decay-0.96/learning_rate_minimum-0.00025/env_name-Breakout-v0/ep_end-0.1/model-m1/screen_height-84/
Traceback (most recent call last):
  File "main.py", line 70, in <module>
    tf.app.run()
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py", line 125, in run
    _sys.exit(main(argv))
  File "main.py", line 65, in main
    agent.train()
  File "/home/tejask98/Desktop/DQN-tensorflow/dqn/agent.py", line 43, in train
    screen, reward, action, terminal = self.env.new_random_game()
  File "/home/tejask98/Desktop/DQN-tensorflow/dqn/environment.py", line 28, in new_random_game
    self.new_game(True)
  File "/home/tejask98/Desktop/DQN-tensorflow/dqn/environment.py", line 23, in new_game
    self._step(0)
  File "/home/tejask98/Desktop/DQN-tensorflow/dqn/environment.py", line 35, in _step
    self._screen, self.reward, self.terminal, _ = self.env.step(action)
  File "/usr/local/lib/python2.7/dist-packages/gym/wrappers/time_limit.py", line 30, in step
    assert self._episode_started_at is not None, "Cannot call env.step() before calling reset()"
AssertionError: Cannot call env.step() before calling reset()

class M1(DQNConfig):
    backend = 'tf'
    env_type = 'detail'
    action_repeat = 1

class M2(DQNConfig):
    backend = 'tf'
    env_type = 'detail'
    action_repeat = 4

I use 
python main.py --env_name=Breakout-v0 --is_train=True --display=False --use_gpu=True --model=m2
and
python main.py --env_name=Breakout-v0 --is_train=True --display=False --use_gpu=True --model=m1

The "avg_ep_r" in both models reaches 2.1 - 2.3  at around 5 million iterations. But when it comes to even 15 million iterations,  the "avg_ep_r" still fluctuates between 2.1 and 2.3.

Just like the result they have shown( I guess that is the result of Action-repeat (frame-skip) of 1, without learning rate decay). I didn't change any parameters.

![image](https://user-images.githubusercontent.com/12911394/46345648-eb8b8f80-c677-11e8-9f35-9d8b0c51cbc0.png)



The strange thing is, even when I use model m2(Action-repeat (frame-skip) of 4), my result is similar to model m1. 
The "avg_ep_r" fluctuates between 2.1 and 2.3 from around 5 million to 15 million iterations. 
The max_ep_r fluctuates between 10 and 18 from around 5 million to 15 million iterations.

class M2(DQNConfig):
    backend = 'tf'
    env_type = 'detail'
    action_repeat = 4


Do I need to change some parameters to reach the best result they have shown?

Thank you very much.
`
  
    for self.step in tqdm(range(start_step, self.max_step), ncols=70, initial=start_step):

    if self.step == self.learn_start:
        num_game, self.update_count, ep_reward = 0, 0, 0.
        total_reward, self.total_loss, self.total_q = 0., 0., 0.
        ep_rewards, actions = [], []

    # 1. predict
    action = self.predict(self.history.get())
    # 2. act
    screen, reward, terminal = self.env.act(action, is_training=True)
    # 3. observe

    self.observe(screen, reward, action, terminal)

    if terminal:
        screen, reward, action, terminal = self.env.new_random_game()
        num_game += 1
        ep_rewards.append(ep_reward)
        ep_reward = 0.
`
 Function train in agent.py may not handle properly when the game is terminated. As the game is terminated, the new screen didn't add into history and memory,  self.history isn't get updated. And in the next iteration, action = self.predict(self.history.get()) will be the same, i.e. terminated.
in dqn/agent.py line 59

      if terminal:
        screen, reward, action, terminal = self.env.new_random_game()

when starting a new game due to a terminal state.

why we don't need to reset the self.history?

because it would affect the next iteration.

      # 1. predict
      action = self.predict(self.history.get())
      # 2. act
      screen, reward, terminal = self.env.act(action, is_training=True)
      # 3. observe
      self.observe(screen, reward, action, terminal)

the predicted action for self.history.get() is not depending on the current game screens, it will predict action for the previous game screen, which is ended, instead.

Do I miss anything?

Thank you very much.
what's the difference between simple game and detail game
I tried to run from the windows command line using the command in Readme but it says "Import Error: No module named tensorflow." How do I run the program on Windows? I am using Anaconda.
where is the m2 and m3? 
Fixed minor screen size bug in environment.
[This](https://github.com/devsisters/DQN-tensorflow/blob/c7b1f1051dfa152530322445fc8febb9a2ea078b/dqn/agent.py#L297) suggests that the maximum of the minimum learning rate and exponentially decayed rate is calculated. But in the [configurations file](https://github.com/devsisters/DQN-tensorflow/blob/c7b1f1051dfa152530322445fc8febb9a2ea078b/config.py#L14), both the learning rate and the minimum learning rate are supplied the same values. This will result in no updates to the learning rate with more training steps. 
**OR** Is this specifically for the case with no updates in the learning rate?
Thanks.