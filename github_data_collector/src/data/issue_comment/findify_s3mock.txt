I upgrade all dependencies to Scala 2.13 and change the alpakka test for the new akka-stream-alpakka-s3 version.
I use the S3Mock for SpringBoot integration tests and noticed a remarkable performance difference when running the build on the local Windows machine compared to the build on a Linux machine (e.g. Jenkins or a local VM):

The Windows build takes about 3 minutes whereas the Linux build takes about 20 minutes.
The test Blobstore uses the in-memory backend and Maven as the build tool.

It seems like starting the S3Mock Blobstore is not the problem, but working with the objects.
Please see the following logs and have a look at the timestamps and the operations/methods.

## Windows

### PutObject
```
2019-10-22 09:04:40.393  INFO 15348 --- [lt-dispatcher-4] io.findify.s3mock.route.PutObject        : put object localbucket/test.txt (unsigned)
2019-10-22 09:04:40.407  INFO 15348 --- [t-dispatcher-15] io.findify.s3mock.route.PutObject        : put object localbucket/outer/test.txt (unsigned)
2019-10-22 09:04:40.420  INFO 15348 --- [t-dispatcher-11] io.findify.s3mock.route.PutObject        : put object localbucket/outer/inner/test.json (unsigned)
```

### DeleteObjects
```
2019-10-22 09:04:48.377  INFO 15348 --- [lt-dispatcher-2] io.findify.s3mock.route.DeleteObjects    : deleted object localbucket/test.txt
2019-10-22 09:04:48.377  INFO 15348 --- [lt-dispatcher-2] io.findify.s3mock.route.DeleteObjects    : deleted object localbucket/outer/test.txt
2019-10-22 09:04:48.377  INFO 15348 --- [lt-dispatcher-2] io.findify.s3mock.route.DeleteObjects    : deleted object localbucket/outer/inner/test.json
```

## Linux

### PutObject
```
2019-10-22 09:00:11.498  INFO 29088 --- [t-dispatcher-13] io.findify.s3mock.route.PutObject        : put object localbucket/outer/inner/test.json (unsigned)
2019-10-22 09:00:14.577  INFO 29088 --- [t-dispatcher-13] io.findify.s3mock.route.PutObject        : put object localbucket/outer/test.txt (unsigned)
2019-10-22 09:00:17.648  INFO 29088 --- [t-dispatcher-13] io.findify.s3mock.route.PutObject        : put object localbucket/test.txt (unsigned)
```

### DeleteObjects
```
2019-10-22 09:02:11.614  INFO 30315 --- [t-dispatcher-11] io.findify.s3mock.route.DeleteObjects    : deleted object localbucket/test.txt
2019-10-22 09:02:11.618  INFO 30315 --- [t-dispatcher-11] io.findify.s3mock.route.DeleteObjects    : deleted object localbucket/outer/test.txt
2019-10-22 09:02:11.618  INFO 30315 --- [t-dispatcher-11] io.findify.s3mock.route.DeleteObjects    : deleted object localbucket/outer/inner/test.json
```

The `PutObject` somehow only takes some milliseconds on Windows but about 3 seconds on Linux. Wherease `DeleteObjects` only takes some milliseconds on Linux as well.

Is there any idea how to speed things up on Linux?
Both the InMemoryProvider and FileProvider use `Int.MaxValue` as their default maxkeys value:

```scala
val count = maxkeys.getOrElse(Int.MaxValue)
```

This should be set to 1,000 as a default so it is similar to the real S3 API, otherwise you can run into situations when testing where the Mock API will return more items than a real API call would.

See documentation of AWS limit here: https://docs.aws.amazon.com/AmazonS3/latest/API/v2-RESTBucketGET.html


Would like to see versioning supported. Thank you.
My Requirement is bucket contains 'parentBucket/folder1/folder2/folder3/abc.txt'.

While running mock, I get below error:

java.lang.IllegalArgumentException: Bucket name should not contain '/'

	at com.amazonaws.services.s3.internal.BucketNameUtils.exception(BucketNameUtils.java:189)
	at com.amazonaws.services.s3.internal.BucketNameUtils.isValidV2BucketName(BucketNameUtils.java:160)
	at com.amazonaws.services.s3.internal.BucketNameUtils.validateBucketName(BucketNameUtils.java:47)
	at com.amazonaws.services.s3.AmazonS3Client.createBucket(AmazonS3Client.java:1042)
	at com.amazonaws.services.s3.AmazonS3Client.createBucket(AmazonS3Client.java:1011)

Can you please fix it asap.

The library itself works with Scala 2.13. But to migrate the tests i need to upgrade alpakka to 1.1 which have a different api than alpakka 0.2 :(
When try to use 

```
 S3.multipartCopy(
        sourceBucket,
        fileName,
        sourceBucket,
        destinationFileName))
```

```
got this error : 
There was an internal server error.
akka.stream.alpakka.s3.S3Exception: There was an internal server error. (Code: -, RequestID: -, HostID: -)
	at akka.stream.alpakka.s3.impl.S3Stream$.$anonfun$processUploadCopyPartRequests$5(S3Stream.scala:637)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at akka.http.scaladsl.util.FastFuture$FulfilledFuture.transform(FastFuture.scala:84)
	at scala.concurrent.Future.map(Future.scala:292)
	at scala.concurrent.Future.map$(Future.scala:292)
	at akka.http.scaladsl.util.FastFuture$FulfilledFuture.map(FastFuture.scala:77)
	at akka.stream.alpakka.s3.impl.S3Stream$.$anonfun$processUploadCopyPartRequests$3(S3Stream.scala:635)
	at akka.stream.impl.fusing.Map$$anon$1.onPush(Ops.scala:52)
	at akka.stream.impl.fusing.GraphInterpreter.processPush(GraphInterpreter.scala:523)
	at akka.stream.impl.fusing.GraphInterpreter.processEvent(GraphInterpreter.scala:480)
	at akka.stream.impl.fusing.GraphInterpreter.execute(GraphInterpreter.scala:376)
	at akka.stream.impl.fusing.GraphInterpreterShell.runBatch(ActorGraphInterpreter.scala:599)
	at akka.stream.impl.fusing.GraphInterpreterShell$AsyncInput.execute(ActorGraphInterpreter.scala:480)
	at akka.stream.impl.fusing.GraphInterpreterShell.processEvent(ActorGraphInterpreter.scala:574)
	at akka.stream.impl.fusing.ActorGraphInterpreter.akka$stream$impl$fusing$ActorGraphInterpreter$$processEvent(ActorGraphInterpreter.scala:742)
	at akka.stream.impl.fusing.ActorGraphInterpreter$$anonfun$receive$1.applyOrElse(ActorGraphInterpreter.scala:757)
	at akka.actor.Actor.aroundReceive(Actor.scala:539)
	at akka.actor.Actor.aroundReceive$(Actor.scala:537)
	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundReceive(ActorGraphInterpreter.scala:664)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612)
	at akka.actor.ActorCell.invoke(ActorCell.scala:581)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268)
	at akka.dispatch.Mailbox.run(Mailbox.scala:229)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:241)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
```

Are there plans to upgrade to Scala 2.13?
When trying to perform any action using the S3AsyncClient I get the following error.  Works fine with real S3 server also, works fine when using S3Client and s3mock.

[WARN] [06/07/2019 17:02:57.707] [s3mock-akka.actor.default-dispatcher-4] [akka.actor.ActorSystemImpl(s3mock)] Illegal request, responding with status '400 Bad Request': Request is missing required `Host` header
- should ingest into solr from s3 based paths *** FAILED *** (1 second, 398 milliseconds)
  java.util.concurrent.CompletionException: software.amazon.awssdk.services.s3.model.S3Exception: null (Service: S3, Status Code: 400, Request ID: null)
  at software.amazon.awssdk.utils.CompletableFutureUtils.errorAsCompletionException(CompletableFutureUtils.java:61)
  at software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncExecutionFailureExceptionReportingStage.lambda$execute$0(AsyncExecutionFailureExceptionReportingStage.java:51)
  at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)
  at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)
  at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
  at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
  at software.amazon.awssdk.utils.CompletableFutureUtils.lambda$forwardExceptionTo$0(CompletableFutureUtils.java:75)
  at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
  at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
  at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
  ...
  Cause: software.amazon.awssdk.services.s3.model.S3Exception: null (Service: S3, Status Code: 400, Request ID: null)
  at software.amazon.awssdk.services.s3.model.S3Exception$BuilderImpl.build(S3Exception.java:95)
  at software.amazon.awssdk.services.s3.model.S3Exception$BuilderImpl.build(S3Exception.java:55)
  at software.amazon.awssdk.protocols.query.unmarshall.AwsXmlErrorProtocolUnmarshaller.handle(AwsXmlErrorProtocolUnmarshaller.java:127)
  at software.amazon.awssdk.protocols.query.unmarshall.AwsXmlErrorProtocolUnmarshaller.handle(AwsXmlErrorProtocolUnmarshaller.java:86)
  at software.amazon.awssdk.core.internal.http.async.AsyncResponseHandler.lambda$prepare$0(AsyncResponseHandler.java:88)
  at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:952)
  at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926)
  at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
  at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)
  at software.amazon.awssdk.core.internal.http.async.AsyncResponseHandler$BaosSubscriber.onComplete(AsyncResponseHandler.java:129)

