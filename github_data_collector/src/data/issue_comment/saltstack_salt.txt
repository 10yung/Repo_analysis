### Description of Issue
Today, stale bot closed two of my issues after just one month of inactivity. This is WAY to short! All in all, I needed to reopen ~10+ issues in the last few weeks because of stale bot going rampage on them. Please reconfigure that stupid bot so that it's less annoying or even disable it altogether. Thanks a lot.
I am ttrying to set up 3 VMs in Opennebula using salt-cloud. two of them are Windows 2016 servers and 1 is centos7. I am having issues while deploying salt-minion on one of the windows machine. I get this error :

```
+ sudo /usr/bin/salt-cloud -m /etc/salt/cloud.maps.d/12-3set.conf -P -y
[WARNING ] Failed auth for mech NEGOEX (1.3.6.1.4.1.311.2.2.30): Mech Type 1.3.6.1.4.1.311.2.2.30 is not yet supported
[WARNING ] Failed auth for mech NEGOEX (1.3.6.1.4.1.311.2.2.30): Mech Type 1.3.6.1.4.1.311.2.2.30 is not yet supported
[WARNING ] Failed auth for mech NEGOEX (1.3.6.1.4.1.311.2.2.30): Mech Type 1.3.6.1.4.1.311.2.2.30 is not yet supported
[ERROR   ] Port connection timed out: 900
[ERROR   ] Failed to start Salt on host exchange-vgautam-625
[WARNING ] Failed auth for mech NEGOEX (1.3.6.1.4.1.311.2.2.30): Mech Type 1.3.6.1.4.1.311.2.2.30 is not yet supported
[WARNING ] Failed auth for mech NEGOEX (1.3.6.1.4.1.311.2.2.30): Mech Type 1.3.6.1.4.1.311.2.2.30 is not yet supported
appliance-625:
    ----------
    deployed:
        True
    id:
        3444
    image:
        zzAppliance_win2k16_2020
    name:
        appliance-625
    private_ips:
        10.27.8.124
    public_ips:
    size:
        8192
    state:
        3
backend-625:
    ----------
    deployed:
        True
    id:
        3446
    image:
        zzBackend-CentOS7_20191120.raw
    name:
        backend-625
    private_ips:
        10.27.9.26
    public_ips:
    size:
        4096
    state:
        3
exchange-625:
    ----------
    Error:
        ----------
        Not Deployed:
            Failed to start Salt on host exchange-625
    id:
        3445
    image:
        zzExchange_win2k16_2020.raw
    name:
        exchange-625
    private_ips:
        10.27.9.10
    public_ips:
    size:
        16384
    state:
        3

```
This whole process takes almost 25 minutes which is very slow. Most of the time is spent while installing salt-minions on Windows VM and has a different behavior everytime. If i log on to the failed machine i dont see any salt folder or service.


```
Salt Version:
           Salt: 2019.2.2

Dependency Versions:
           cffi: 1.13.1
       cherrypy: unknown
       dateutil: Not Installed
      docker-py: Not Installed
          gitdb: 2.0.6
      gitpython: Not Installed
          ioflo: Not Installed
         Jinja2: 2.10.3
        libgit2: Not Installed
        libnacl: Not Installed
       M2Crypto: Not Installed
           Mako: 1.1.0
   msgpack-pure: Not Installed
 msgpack-python: 0.5.6
   mysql-python: Not Installed
      pycparser: Not Installed
       pycrypto: 2.6.1
   pycryptodome: Not Installed
         pygit2: Not Installed
         Python: 2.7.5 (default, Aug  7 2019, 00:51:29)
   python-gnupg: Not Installed
         PyYAML: 3.11
          PyZMQ: 15.3.0
           RAET: Not Installed
          smmap: 2.0.5
        timelib: Not Installed
        Tornado: 4.2.1
            ZMQ: 4.1.4

System Versions:
           dist: centos 7.6.1810 Core
         locale: UTF-8
        machine: x86_64
        release: 3.10.0-957.5.1.el7.x86_64
         system: Linux
        version: CentOS Linux 7.6.1810 Core
```


### Description of Issue
With the 3000rc dropping the deprecation of `json_decode_dict` in the next release was announced. The new way to do it is to use `tojson`.

In prepare my states and pillars before upgrading from 2018.2.3 to 3000rc2 I noticed issues when replacing `json_decode_dict` with `tojson`.

json_decode_dict would output an in the order it was received, tojson will output the keys sorted alphabetically!

### Setup
```
{% load_yaml as x %}
z: some
y: text
x: here
{% endload %}
/tmp/tojson.out:
  file.managed:
    - contents: |
        {{ x|tojson }}

/tmp/json_decode_dict.out:
  file.managed:
    - contents: |
        {{ x|json_decode_dict }}
```

### Steps to Reproduce Issue
I provided a basic test state to see it in action, notice the different order between tosjon and json_decode_dict

### Versions Report
```
Salt Version:
           Salt: 2019.2.3

Dependency Versions:
           cffi: 1.11.5
       cherrypy: 17.3.0
       dateutil: 2.7.3
      docker-py: Not Installed
          gitdb: 2.0.5
      gitpython: 2.1.11
          ioflo: Not Installed
         Jinja2: 2.10
        libgit2: Not Installed
        libnacl: 1.6.1
       M2Crypto: 0.30.1
           Mako: Not Installed
   msgpack-pure: Not Installed
 msgpack-python: 0.5.6
   mysql-python: Not Installed
      pycparser: 2.19
       pycrypto: 3.6.6
   pycryptodome: Not Installed
         pygit2: Not Installed
         Python: 2.7.15 (default, Oct 23 2018, 17:11:49)
   python-gnupg: 0.4.3
         PyYAML: 3.13
          PyZMQ: 17.1.2
           RAET: Not Installed
          smmap: 0.9.0
        timelib: Not Installed
        Tornado: 5.1.1
            ZMQ: 4.2.5

System Versions:
           dist:
         locale: UTF-8
        machine: i86pc
        release: 5.11
         system: SunOS
        version: Not Installed
```
### What does this PR do?

Enables dnf support for CentOS 8 systems.

### What issues does this PR fix or reference?

Fixes #55898 

### Previous Behavior

If you tried to use pkg.installed this error appears:
```
          ID: timezone
    Function: pkg.installed
      Result: False
     Comment: Error occurred installing package(s). Additional info follows:

              errors:
                  - Failed to find executable yum: No such file or directory
```

### Tests written?

No

### Commits signed with GPG?

Yes

### Description of Issue
in salt_return table in Postgresql the 'success' column is always false for runners
the full_ret structure is different for runners :
there is a "return" dict in "return" dict in full_ret
the real success status is inside that.
I think the outer "return" must be updated with inner "return" values and must not include it

### Versions Report
Salt Version:
           Salt: 2019.2.2
 
Dependency Versions:
           cffi: 0.8.6
       cherrypy: unknown
       dateutil: 2.2
      docker-py: Not Installed
          gitdb: 0.5.4
      gitpython: 0.3.2 RC1
          ioflo: Not Installed
         Jinja2: 2.10
        libgit2: Not Installed
        libnacl: Not Installed
       M2Crypto: Not Installed
           Mako: Not Installed
   msgpack-pure: Not Installed
 msgpack-python: 0.5.6
   mysql-python: 1.2.3
      pycparser: 2.10
       pycrypto: 2.6.1
   pycryptodome: Not Installed
         pygit2: Not Installed
         Python: 2.7.9 (default, Sep 14 2019, 20:00:08)
   python-gnupg: Not Installed
         PyYAML: 3.12
          PyZMQ: 14.4.0
           RAET: Not Installed
          smmap: 0.8.2
        timelib: Not Installed
        Tornado: 4.2.1
            ZMQ: 4.0.5
### What does this PR do?
Only return cached pillar data for minion that is passed with `tgt`.

### What issues does this PR fix or reference?
Fixes https://github.com/saltstack/salt/issues/53039

### Previous Behavior
If you run `salt-run --out=json cache.pillar tgt='doesnotexist'` with a target that does not exist it still returns all the cached pilllar data for ALL minions. 

### New Behavior
If you run `salt-run --out=json cache.pillar tgt='doesnotexist'` with a target that does not exist will not return anything.

Also as you can see I updated the docs to state: `If tgt is not set will return cached pillars for all minions.` The reason I did this is this is the current behavior that a user can run `cache.pillar` without a tgt set and all pillar data would be returned,and the other reason is `tgt` is an not required as its currently a kwarg. For these reasons I kept this behavior as its expected. If we want to eventually require the `tgt` argument for some reason then we would need to set it on a deprecation path, but I believe this change is best path for now.


### Tests written?
Yes

### Commits signed with GPG?

Yes
### What does this PR do?

This PR introduces a new minion configuration option, master_return_strategy. Can be source or any. The default of source preserves the current behavior, namely, the minion will only attempt to return the job results to the master that sent the job. If set to any, then the minion will first attempt to return the job results to the master that sent the job. If that fails then the minion will attempt to return the job results to the other configured masters, one by one, until successful or the master list has been exhausted. master_alive_interval must also be set. This is used to keep track of which masters are currently connected.

### What issues does this PR fix or reference?

### Previous Behavior

When a master becomes unavailable after sending a job but before receiving the return information, that return info will never make it back to the masters. If relying on master_job_cache or event_returns on the master side then the job return information is lost forever.

### New Behavior

When the initiating master becomes unavailable after firing a job to a minion then the minion will attempt to return the job information to another configured master.

### Tests written?

Not yet

### Commits signed with GPG?

No

Please review [Salt's Contributing Guide](https://docs.saltstack.com/en/latest/topics/development/contributing.html) for best practices.

See GitHub's [page on GPG signing](https://help.github.com/articles/signing-commits-using-gpg/) for more information about signing commits with GPG.

### What does this PR do?
SmartOS will automatically calculate (and override) the mac property of any nics that have `vrrp_vrid` property set.

This causes a lot of issues:
- we keep trying to add a new nic if the mac that was provided is incorrect
- we keep trying to delete the existing nic with the mac based on the vrid
- we keep trying to incorrect update a nic if the vrid was change (cannot be update, remove and add new nic)

### What issues does this PR fix or reference?
n/a

### Previous Behavior
We would try to add/remove/update the incorrect nic when `vrrp_vrid` property was present making the state module fail.

### New Behavior
We now account for this in `_parse_vmconfig()`, this function is used to map unique identifiers for various smartos resource types (nics, disks, ...)

We will use the provided mac address from the resource key **unless* the `vrrp_vrid` property is present, then generate the mac based on the provided vrid.

This results in the nic being correctly added, when the vrid is changes this results in a remove of the old nic and adding of the new nic as is expected by vmadm.

### Tests written?
Yes, I've added tests to test the mac handling of `_parse_vmconfig()`

### Commits signed with GPG?
No
### Description of Issue
Epic or container of issues related to Sodium release deprecation warnings.

### Description of Issue
Having defined a service in salt (that is, a configuration file that's a managed Jinja template and a package install that's sourced in a yum repository which installs an init script), I routinely get a "Service failed to die" message for the "service.dead" state. However, the command issued to stop the service appears to be completing successfully.

### Setup
I'm using Vagrant to test this. I stand up a host on AWS and run a highstate on the machine. Here are the relevant SLS files related to this particular state failing.

    pganalyze-collector:
      pkgrepo.managed:
        - name: pganalyze_collector
        - enabled: True
        - humanname: pganalyze_collector
        - baseurl: https://packages.pganalyze.com/el/6
        - gpgkey: https://packages.pganalyze.com/pganalyze_signing_key.asc
        - repo_gpgcheck: 1
        - sslverify: 1
        - sslcacert: /etc/pki/tls/certs/ca-bundle.crt
        - metadata_expire: 300
      pkg.installed:
        - require:
          - pkgrepo: pganalyze-collector
      service.{{ salt['pillar.get']('pganalyze:state') }}:
        - enable: {{ salt['pillar.get']('pganalyze:enable') }}
        - require:
          - pkg: pganalyze-collector
          - file: /etc/pganalyze-collector.conf
          - service: postgres
    
    /etc/pganalyze-collector.conf:
      file.managed:
        - source: salt://postboot/templates/pganalyze-collector.conf.j2
        - template: jinja
        - user: root
        - group: pganalyze
        - mode: 0644

Suffice it to say that for the environment in question, the pillars here are being rendered correctly; the service state is dead and the "enable" flag is False.

A little more context... This service is a metrics collector for a third party Postgres performance analyzer, and it depends upon a running postgres server to collect that data. This is why there is a requirement for the "service: postgres" state. We want that to be available before we decide to launch pganalyzer (which will only happen in higher environments, which is why we want it dead at the time salt is applied). That postgres service is just fine, as shown here:

             ID: postgres
        Function: service.running
            Name: postgresql-9.5
          Result: True
         Comment: Service reloaded
         Started: 17:22:20.726654
        Duration: 160.732 ms
         Changes:   
                  ----------
                  postgresql-9.5:
                      True

The managed template is also rendering correctly.

### Steps to Reproduce Issue
I can reproduce this by running a highstate:

    salt-call --local state.highstate

The output (excluding the dozens of other, unrelated states) looks like this:

              ID: pganalyze-collector
        Function: service.dead
           Result: False
         Comment: Service pganalyze-collector failed to die
         Started: 17:22:20.890966
        Duration: 374.818 ms
         Changes:

### Versions Report

    # salt-call --versions-report
    Salt Version:
               Salt: 2019.2.2
     
    Dependency Versions:
               cffi: Not Installed
           cherrypy: Not Installed
           dateutil: 2.8.1
          docker-py: Not Installed
              gitdb: Not Installed
          gitpython: Not Installed
              ioflo: Not Installed
             Jinja2: 2.7.2
            libgit2: Not Installed
            libnacl: Not Installed
           M2Crypto: Not Installed
               Mako: Not Installed
       msgpack-pure: Not Installed
     msgpack-python: 0.6.2
       mysql-python: Not Installed
          pycparser: Not Installed
           pycrypto: 2.6.1
       pycryptodome: Not Installed
             pygit2: Not Installed
             Python: 2.7.16 (default, Oct 14 2019, 21:26:56)
       python-gnupg: Not Installed
             PyYAML: 5.2
              PyZMQ: 14.5.0
               RAET: Not Installed
              smmap: Not Installed
            timelib: Not Installed
            Tornado: 4.2.1
                ZMQ: 4.0.5
     
    System Versions:
               dist:   
             locale: UTF-8
            machine: x86_64
            release: 4.14.154-99.181.amzn1.x86_64
             system: Linux
            version: Not Installed

### Some Other Details

This is an older InitV system, so I did some testing around various service stop commands. All of the following commands succeed, throw no errors, and return an exit code of 0:

    /etc/init.d/pganalyze-collector stop
    service pganalyze-collector stop
    salt-call --local service.stop pganalyze-collector

The last one actually outputs the following in addition to the zero exit code:

    local:
        True

I looked at the code as well. On my system, the service states are defined in `/usr/lib/python2.7/dist-packages/salt/states/service.py`. The logic goes that it will use the inbuilt salt "service.stop" function and respond to the return code. I added a print statement so that this segment of code looks like this:

    607     func_ret = __salt__['service.stop'](name, **stop_kwargs)
    608     print("stop returned: {}".format(func_ret))
    609     if not func_ret:
    610         ret['result'] = False
    611         ret['comment'] = 'Service {0} failed to die'.format(name)
    612         if enable is True:
    613             ret.update(_enable(name, True, result=False, **kwargs))
    614         elif enable is False:
    615             ret.update(_disable(name, True, result=False, **kwargs))
    616         return ret

And shortly beneath that, this code:

    634     print("after_toggle_status: {}".format(after_toggle_status))
    635     if after_toggle_status:
    636         ret['result'] = False
    637         ret['comment'] = 'Service {0} failed to die'.format(name)
    638     else:
    639         ret['comment'] = 'Service {0} was killed'.format(name)
    640 
    641     if enable is True:
    642         ret.update(_enable(name, after_toggle_status, result=not after_toggle_status, **kwargs))
    643     elif enable is False:
    644         ret.update(_disable(name, after_toggle_status, result=not after_toggle_status, **kwargs))

When I run the highstate, if I scroll up past the state output, I see this:

    stop returned: True                                                                                                                                                                                                                           
    after_toggle_status: True                                                                                                                                                                                                                     
    [ERROR   ] Service pganalyze-collector failed to die

`after_toggle_status` gets set to True based on the `service.status` function. I can run that manually:

    # salt-call --local service.status pganalyze-collector
    local:
        True

And I can confirm from the init script that the service is offline:

    # /etc/init.d/pganalyze-collector status
    pganalyze-collector is stopped
    # service pganalyze-collector status
    pganalyze-collector is stopped

So as far as I can tell, this service should be considered successfully "dead", and I can't figure out why salt disagrees with that assessment.