The if statement `if (word == -1) continue;` will never evaluate to `true` because when filling the `sen` array, words with id `-1` are already being discarded by [this](https://github.com/dav/word2vec/blob/0f29b188b17145d0c1d0953ba0bc80a2208dd8a0/src/word2vec.c#L413) check a few lines above.

I suggest to remove the if-statement because it is misleading and if it every evaluated to `true`, it would do so in the next iteration again (because `sentence_position` would not change) and the program would be in an infinite loop.
I thought this was the Mac-savvy version of the code... but it's failing right out of the box for me on the first allocation (around line 350 of word2vec.c), because the conditional compilation doesn't include a Mac case:

```
#ifdef _MSC_VER
  syn0 = _aligned_malloc((long long)vocab_size * layer1_size * sizeof(real), 128);
#elif defined  linux
  a = posix_memalign((void **)&syn0, 128, (long long)vocab_size * layer1_size * sizeof(real));
#endif
```

Neither _MSC_VER nor linux is defined, so we simply don't allocate anything.

Just wondering if I have missed some obvious solution, or whether I need to add a Mac case (or maybe a generic case, simply using calloc?) to each such block.
I have my own training set of 50k words(written in Devanagari script/Indic language), how should I approach?
Do I need to change the file path in 'demo-word.sh' or help me with the stepwise execution.



Memory leaks detected. I'm running program word2vec with command line like:
./word2vec -train ./questions-words.txt -output out.txt
```
=================================================================
==1469==ERROR: LeakSanitizer: detected memory leaks

Direct leak of 96 byte(s) in 1 object(s) allocated from:
    #0 0x7fdddc6f0602 in malloc (/usr/lib/x86_64-linux-gnu/libasan.so.2+0x98602)
    #1 0x411ff4 in TrainModel /home/mfc_fuzz/newprogram/word2vec/src/word2vec.c:600

SUMMARY: AddressSanitizer: 96 byte(s) leaked in 1 allocation(s).

```
In the skipgram part,  when computing propagate  hidden -> output , use this code :
` for (c = 0; c < layer1_size; c++) f += syn0[c + l1] * syn1[c + l2];` while  `l1  =  l1 = last_word * layer1_size;   l2 = vocab[word].point[d] * layer1_size; ` which means the syn0 is input word , syn1 is output word.    
the codes show . syn1 is the target word,  syn0 is  context(target word).
The skipgram  is using w to predict context(w), but this code is use  context(w) to predit w.  is that right ?  
If a `-read-vocab` file is specified, then all their counts are 0, it seems `SortVocab()` will remove all words in such case.

https://github.com/dav/word2vec/blob/5f2e9661025c4f6b496c6a6888be0d090b9c44b3/src/word2vec.c#L323

https://github.com/dav/word2vec/blob/5f2e9661025c4f6b496c6a6888be0d090b9c44b3/src/word2vec.c#L155
Hi, I'm working on a paper, after reading the word2vec.c code, it looks like the CBOW "gradient of loss" is calculated in:

f = expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))];          
g = (1 - vocab[word].code[d] - f) * alpha; // 'g' is the gradient multiplied by the learning rate

I want to be able to get the loss of an input and not the gradient of it, is there anyway to get that? is there any function that does so?

thanks for sharing
Could anyone please point me to where the embedding matrix is initialized in the code? I would like to know how it is initialized. If random, what random distribution are weights sampled from? Thank you!
Hi, 
We're using word2vec for hypernymy discovering. In order to design a more efficient version of word2vec, we need to know what is exactly the semantics of the variable "c" in function ReadVocab() within the file word2vec.c? Thanks in advance.
void ReadVocab() {
	long long a, i = 0;
	char c;
	char word[MAX_STRING];
	FILE *fin = fopen(read_vocab_file, "rb");
	if (fin == NULL) {
		printf("Vocabulary file not found\n");
		exit(1);
	}
	for (a = 0; a < vocab_hash_size; a++) vocab_hash[a] = -1;
	vocab_size = 0;
	while (1) {
		ReadWord(word, fin);
		if (feof(fin)) break;
		a = AddWordToVocab(word);
		fscanf(fin, "%lld%c", &vocab[a].cn, &c); // semantics of c?
		i++;
	}
	SortVocab();
	if (debug_mode > 0) {
		printf("Vocab size: %lld\n", vocab_size);
		printf("Words in train file: %lld\n", train_words);
	}
	fin = fopen(train_file, "rb");
	if (fin == NULL) {
		printf("ERROR: training data file not found!\n");
		exit(1);
	}
	fseek(fin, 0, SEEK_END);
	file_size = ftell(fin);
	fclose(fin);
}
Hi, I'm using word2vec for translations between languages. I know if you're using the same vector, it is possible to calculate a similarity score between words, but is the same true between words on different vectors? Thanks in advance. I love the package.