```
import org.neo4j.spark._
neo: org.neo4j.spark.Neo4j = org.neo4j.spark.Neo4j@5f492b15
beat: (String, Seq[String]) = (Beat,List(beat))
on_beat: (String, Seq[Nothing]) = (ON_BEAT,List())
crime: (String, Seq[String]) = (Crime,List(id, description, case_number, point))
crimeType: (String, Seq[String]) = (CrimeType,List(primary_type))
is_type: (String, Seq[Nothing]) = (IS_TYPE,List())
crimesDates: (String, Seq[String]) = (CrimeDate,List(parsed_date, date))
on_date: (String, Seq[Nothing]) = (ON_DATE,List())
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5, localhost, executor driver): org.neo4j.driver.v1.exceptions.ClientException: The server does not support any of the protocol versions supported by this driver. Ensure that you are using driver and server versions that are compatible with one another.
	at org.neo4j.driver.internal.async.HandshakeHandler.protocolNoSupportedByServerError(HandshakeHandler.java:147)
	at org.neo4j.driver.internal.async.HandshakeHandler.decode(HandshakeHandler.java:123)
	at org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:489)
	at org.neo4j.driver.internal.shaded.io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:367)
	at org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265)
	at org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.neo4j.driver.internal.shaded.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.neo4j.driver.internal.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1414)
	at org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at org.neo4j.driver.internal.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:945)
	at org.neo4j.driver.internal.shaded.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:146)
	at org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at org.neo4j.driver.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:886)
	at org.neo4j.driver.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)
  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
  at scala.Option.foreach(Option.scala:257)
  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)
  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
  at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:929)
  at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:927)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
  at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:927)
  at org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply$mcV$sp(Dataset.scala:2675)
  at org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply(Dataset.scala:2675)
  at org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply(Dataset.scala:2675)
  at org.apache.spark.sql.Dataset$$anonfun$withNewRDDExecutionId$1.apply(Dataset.scala:3239)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)
  at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:3235)
  at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:2674)
  at org.neo4j.spark.Neo4jDataFrame$.mergeEdgeList(Neo4jDataFrame.scala:27)
  ... 56 elided
Caused by: org.neo4j.driver.v1.exceptions.ClientException: The server does not support any of the protocol versions supported by this driver. Ensure that you are using driver and server versions that are compatible with one another.
  at org.neo4j.driver.internal.async.HandshakeHandler.protocolNoSupportedByServerError(HandshakeHandler.java:147)
  at org.neo4j.driver.internal.async.HandshakeHandler.decode(HandshakeHandler.java:123)
  at org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:489)
  at org.neo4j.driver.internal.shaded.io.netty.handler.codec.ReplayingDecoder.callDecode(ReplayingDecoder.java:367)
  at org.neo4j.driver.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265)
  at org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
  at org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
  at org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
  at org.neo4j.driver.internal.shaded.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
  at org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
  at org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
  at org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
  at org.neo4j.driver.internal.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1414)
  at org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
  at org.neo4j.driver.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
  at org.neo4j.driver.internal.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:945)
  at org.neo4j.driver.internal.shaded.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:146)
  at org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
  at org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
  at org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
  at org.neo4j.driver.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
  at org.neo4j.driver.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:886)
  at org.neo4j.driver.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)```
Hi,
I've been doing a lot of work on a fork of this library for use at my workplace.
It adds in support for a python interface, however this interface changes the scala library API significantly.
Would it be considered to merge this into a major release? Or are the changes to the interface too drastic?
When changing complex properties (such as arrays) on a node/property, Neo4j can't convert the Scala type (WrappedArray, etc.) to Neo4j Value.

To Reproduce:
```
import org.apache.spark.sql.types._
import org.neo4j.spark.dataframe._

val rows = sc.makeRDD(Seq(Row("M_CASHOUT_VERIFICATION_SUBMIT", "M_LOGIN_SUCCESS", 30, 0.2, Seq("M_CASHOUT_VERIFICATION_SUBMIT", "M_LOGIN_SUCCESS", "a"))))
val schema = StructType(Seq(
      StructField("src_name", StringType),
      StructField("dst_name", StringType),
      StructField("count", IntegerType),
      StructField("support", DoubleType),
      StructField("path", ArrayType(StringType))
    ))
val df = sqlContext.createDataFrame(rows, schema)
val rename = Map("src_name" -> "name", "dst_name" -> "name")
Neo4jDataFrame.mergeEdgeList(sc, df, ("ACTION", Seq("src_name")), ("Path", Seq("path", "count", "support")), ("ACTION", Seq("dst_name")), rename)
```
My neo4j conf have：

`dbms.connector.bolt.tls_level=REQUIRED`

but，when use

`Neo4j(sc)`

have error

> org.neo4j.driver.v1.exceptions.ServiceUnava
ilableException: Connection to the database terminated. This can happen due to n
etwork instabilities, or due to restarts of the database

How can I use encrypted client connections with neo4j-spark-connector?
Problem:

**I have an RDD/DataFrame/Dataset (representing a Graph) and I want to persist it in Neo4J (efficiently).** 

I read somewhere that this connector should suit my needs my I'm not sure that is actually the case. Or perhaps I'm missing something.

Anyway, this connector is based on the neo4j-java-driver. Following the examples, it all happens in the `Neo4jDataFrame` class if I'm not mistaken.

First cause of concern, in both `createNodes` and `mergeEdgeList` there is a `dataFrame.count() ` followed by a `dataFrame.foreachPartition`. Both are actions, and there is no cache of the dataFrame. As it is that should trigger the the computation of the same dataframe from the beginning **twice**. 

Second cause of concern: 
in the `dataFrame.foreachPartition` you basically create a Neo4J driver, get a session from it and then close both, for each record in the dataframe (if I read it correctly, that is what the `execute` method does). Hence the second cause of concern, accordingly to the Neo4J documentation a `driver` is a quite expensive resource to obtain as opposed to a `session` with is cheap and expendable. Also, I might be wrong, but I don't think a transaction can span over a session, so I can't see the benefit of running all that within a transactional context when you open and close the session for each record in the Dataframe.

Could you please throw some light on the above points? Am I missing something pretty obvious?




Move the current business logic into the Spark DataSource API

https://www.slideshare.net/databricks/extending-spark-sql-24-with-new-data-sources-live-coding-session

We should consider with this refactoring if we could start directly with DataSource API V2 (supported from Spark 2.3):

https://databricks.com/session/apache-spark-data-source-v2

https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataSourceV2.html

The old datasource api should be removed in spark 3 so the last version is future-proof


as an option to avoid the bolt-driver creation issue as a bolt driver (with pool) is not serializable.

might be much cheaper with http cypher endpoint

perhaps even utilize jdbc??
e.g. write and read 1bn nodes and rels to / from Neo4j
