Is there an option that we can exhaust and put the Precog databases fully In-Memory?

Cached queries are implemented in Precog master, but may have bugs, have not been tested extensively, and their APIs likely differ from the required format.

The following document contains the "analysis" API which is meant to replace the old query APIs, and form the basis of cached queries:

https://docs.google.com/document/d/1j43rvBNPvV7sDpO5l9vUXqtO9IPO-oWT2_tF8fMJEt0/edit?usp=sharing

This ticket will be considered complete when these APIs are implemented, tested, and thoroughly documented.

This works:

datam:=//statistics/jobs/months
datam1:=datam.Results where datam.JobGroup="Database Administrator"
finalmonth:=flatten(datam1.avgsalbyitskill) with {date:datam.QueryDate}
finalmonth

This doesnâ€™t and its essentially the same data, just in a different collection:

data:=//statistics/jobs/jobgroups
data1:=data.Results where data.JobGroup="Database Administrator" & data.QueryDate="9/31/2013"
finalday:=flatten(data1.avgsalbyitskill) with {date:data.QueryDate}
finalday

This is the error I am getting:

2013-11-01 12:46:51,582 [atcher-148] E c.p.s.s.SyncQueryServiceHandler {} - Error executing shard query:
java.lang.UnsupportedOperationException: empty.max
       at scala.collection.TraversableOnce$class.max(TraversableOnce.scala:201)
       at scala.collection.immutable.Set$EmptySet$.max(Set.scala:52)
       at com.precog.daze.ArrayLibModule$ArrayLib$Flatten$$anonfun$apply$1$$anonfun$2.apply(ArrayLib.scala:33)
       at com.precog.daze.ArrayLibModule$ArrayLib$Flatten$$anonfun$apply$1$$anonfun$2.apply(ArrayLib.scala:27)
       at scalaz.StreamT$$anonfun$map$1$$anonfun$apply$58.apply(StreamT.scala:82)
       at scalaz.StreamT$$anonfun$map$1$$anonfun$apply$58.apply(StreamT.scala:82)
       at scalaz.StreamT$Yield$$anon$7.apply(StreamT.scala:215)
       at scalaz.StreamT$$anonfun$map$1.apply(StreamT.scala:82)
       at scalaz.StreamT$$anonfun$map$1.apply(StreamT.scala:82)
       at scalaz.Monad$$anonfun$map$1$$anonfun$apply$1.apply(Monad.scala:14)
       at akka.dispatch.Future$$anon$3.liftedTree1$1(Future.scala:195)
       at akka.dispatch.Future$$anon$3.run(Future.scala:194)
       at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:94)
       at akka.jsr166y.ForkJoinTask$AdaptedRunnableAction.exec(ForkJoinTask.java:1381)
       at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259)
       at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)
       at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479)
       at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

`start-shard.sh` tries to download zookeeper-3.4.3 and kafka-0.7.5 from a private s3 repo. 

Zookeeper-3.4.3 can be obtained on the apache project website
Kafka-0.7.5 zip file seems problematic as explained [here](https://github.com/precog/platform/pull/538#issuecomment-27403351) : 0.7.5 is not an official release

Adding additional shard services to a current precog instance requires copying over the shard-data path. However, since the original pathname is stored in the cooked files, the new shard service will not be able to query the new (copied) shard-data path. Here is the config:

 queryExecutor {
        systemId = "dev2"
        precog {
          storage {
            root = /home/precog/work/shard2-data/
          }
        }

Here is the error log:

2013-10-17 12:05:15,061 [patcher-13] E c.p.s.s.SyncQueryServiceHandler {} - Error executing shard query:
java.io.FileNotFoundException: /home/precog/work/shard-data/data/0000000066/statistics/jobs/jobgroups/perAuthProjections/b35a170be4ef15e078a741233a0f4245854c9bb3/cooked_blocks/segment-0--580161124-Decimal6211499993968270611.cooked (No such file or directory)
    at java.io.FileInputStream.open(Native Method)
    at java.io.FileInputStream.<init>(FileInputStream.java:137)
    at com.precog.niflheim.CookedReader.com$precog$niflheim$CookedReader$$read(CookedReader.scala:32)
    at com.precog.niflheim.CookedReader$$anonfun$load$1$$anonfun$apply$15$$anonfun$9.apply(CookedReader.scala:119)
    at com.precog.niflheim.CookedReader$$anonfun$load$1$$anonfun$apply$15$$anonfun$9.apply(CookedReader.scala:118)
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
    at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59)
    at scala.collection.immutable.List.foreach(List.scala:76)
    at scala.collection.TraversableLike$class.map(TraversableLike.scala:233)
    at scala.collection.immutable.List.map(List.scala:76)
    at com.precog.niflheim.CookedReader$$anonfun$load$1$$anonfun$apply$15.apply(CookedReader.scala:118)
    at com.precog.niflheim.CookedReader$$anonfun$load$1$$anonfun$apply$15.apply(CookedReader.scala:117)
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
    at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59)
    at scala.collection.immutable.List.foreach(List.scala:76)
    at scala.collection.TraversableLike$class.map(TraversableLike.scala:233)
    at scala.collection.immutable.List.map(List.scala:76)
    at com.precog.niflheim.CookedReader$$anonfun$load$1.apply(CookedReader.scala:117)
    at com.precog.niflheim.CookedReader$$anonfun$load$1.apply(CookedReader.scala:116)
    at scalaz.Validation$class.flatMap(Validation.scala:141)
    at scalaz.Success.flatMap(Validation.scala:329)
    at com.precog.niflheim.CookedReader.load(CookedReader.scala:116)
    at com.precog.niflheim.CookedReader$$anonfun$6.apply(CookedReader.scala:75)
    at com.precog.niflheim.CookedReader$$anonfun$6.apply(CookedReader.scala:74)
    at scala.Option.map(Option.scala:133)
    at com.precog.niflheim.CookedReader.snapshotRef(CookedReader.scala:74)
    at com.precog.niflheim.NIHDBSnapshot$$anonfun$getBlockAfter$1.apply(NIHDBSnapshot.scala:54)
    at com.precog.niflheim.NIHDBSnapshot$$anonfun$getBlockAfter$1.apply(NIHDBSnapshot.scala:53)
    at scala.Option.map(Option.scala:133)
    at com.precog.niflheim.NIHDBSnapshot$class.getBlockAfter(NIHDBSnapshot.scala:53)
    at com.precog.niflheim.NIHDBSnapshot$$anon$1.getBlockAfter(NIHDBSnapshot.scala:18)
    at com.precog.niflheim.NIHDB$$anonfun$getBlockAfter$1.apply(NIHDBActor.scala:75)
    at com.precog.niflheim.NIHDB$$anonfun$getBlockAfter$1.apply(NIHDBActor.scala:75)
    at akka.dispatch.Future$$anonfun$map$1.liftedTree3$1(Future.scala:625)
    at akka.dispatch.Future$$anonfun$map$1.apply(Future.scala:624)
    at akka.dispatch.Future$$anonfun$map$1.apply(Future.scala:621)
    at akka.dispatch.DefaultPromise.akka$dispatch$DefaultPromise$$notifyCompleted(Future.scala:943)
    at akka.dispatch.DefaultPromise$$anonfun$tryComplete$1$$anonfun$apply$mcV$sp$4.apply(Future.scala:920)
    at akka.dispatch.DefaultPromise$$anonfun$tryComplete$1$$anonfun$apply$mcV$sp$4.apply(Future.scala:920)
    at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59)
    at scala.collection.immutable.List.foreach(List.scala:76)
    at akka.dispatch.DefaultPromise$$anonfun$tryComplete$1.apply$mcV$sp(Future.scala:920)
    at akka.dispatch.Future$$anon$4$$anonfun$run$1.apply$mcV$sp(Future.scala:386)
    at akka.dispatch.Future$$anon$4$$anonfun$run$1.apply(Future.scala:378)
    at akka.dispatch.Future$$anon$4$$anonfun$run$1.apply(Future.scala:378)
    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
    at akka.dispatch.Future$$anon$4.run(Future.scala:378)
    at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:94)
    at akka.jsr166y.ForkJoinTask$AdaptedRunnableAction.exec(ForkJoinTask.java:1381)
    at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259)
    at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)
    at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479)
    at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

currently we have shard server running. we can fetch data now.
is there any document on how to run labcoat against this?

Our data is getting around 200GB and our query times are going up quite substantially in terms of time to complete. It seems that with no indexing it is doing full data or full table scans of the data. This is a pretty high priority since our data will begin to grow at 5GB/day. Thanks.

[ERROR] [10/07/2013 18:20:47.688] [PrecogShard-akka.actor.default-dispatcher-310] [akka.dispatch.Dispatcher] Promise already completed: akka.dispatch.DefaultPromise@103dccd9 tried to complete with Right(HttpResponse(OK ,HttpHeaders(Map(Access-Control-Allow-Origin -> *, Access-Control-Allow-Headers -> Origin,X-Requested-With,Content-Type,X-File-Name,X-File-Size,X-File-Type,X-Precog-Path,X-Precog-Service,X-Precog-Token,X-Precog-Uuid,Accept,Authorization, Access-Control-Allow-Methods -> GET,POST,OPTIONS,DELETE,PUT, Allow -> GET,POST,OPTIONS,DELETE,PUT, Content-Type -> application/json)),Some(Right(scalaz.StreamT@2313e481)),HTTP/1.1))
java.lang.IllegalStateException: Promise already completed: akka.dispatch.DefaultPromise@103dccd9 tried to complete with Right(HttpResponse(OK ,HttpHeaders(Map(Access-Control-Allow-Origin -> *, Access-Control-Allow-Headers -> Origin,X-Requested-With,Content-Type,X-File-Name,X-File-Size,X-File-Type,X-Precog-Path,X-Precog-Service,X-Precog-Token,X-Precog-Uuid,Accept,Authorization, Access-Control-Allow-Methods -> GET,POST,OPTIONS,DELETE,PUT, Allow -> GET,POST,OPTIONS,DELETE,PUT, Content-Type -> application/json)),Some(Right(scalaz.StreamT@2313e481)),HTTP/1.1))
    at akka.dispatch.Promise$class.complete(Future.scala:782)
    at akka.dispatch.DefaultPromise.complete(Future.scala:847)
    at akka.dispatch.Future$$anonfun$recover$1.apply(Future.scala:548)
    at akka.dispatch.Future$$anonfun$recover$1.apply(Future.scala:546)
    at akka.dispatch.DefaultPromise.akka$dispatch$DefaultPromise$$notifyCompleted(Future.scala:943)
    at akka.dispatch.DefaultPromise$$anonfun$tryComplete$1$$anonfun$apply$mcV$sp$4.apply(Future.scala:920)
    at akka.dispatch.DefaultPromise$$anonfun$tryComplete$1$$anonfun$apply$mcV$sp$4.apply(Future.scala:920)
    at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59)
    at scala.collection.immutable.List.foreach(List.scala:76)
    at akka.dispatch.DefaultPromise$$anonfun$tryComplete$1.apply$mcV$sp(Future.scala:920)
    at akka.dispatch.Future$$anon$4$$anonfun$run$1.apply$mcV$sp(Future.scala:386)
    at akka.dispatch.Future$$anon$4$$anonfun$run$1.apply(Future.scala:378)
    at akka.dispatch.Future$$anon$4$$anonfun$run$1.apply(Future.scala:378)
    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
    at akka.dispatch.Future$$anon$4.run(Future.scala:378)
    at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:94)
    at akka.jsr166y.ForkJoinTask$AdaptedRunnableAction.exec(ForkJoinTask.java:1381)
    at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259)
    at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)
    at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479)
    at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

[ERROR] [10/07/2013 18:20:59.229] [PrecogShard-akka.actor.default-dispatcher-314] [akka.dispatch.Dispatcher] Promise already completed: akka.dispatch.DefaultPromise@41a0e9b5 tried to complete with Right(HttpResponse(OK ,HttpHeaders(Map()),Some(Left(java.nio.HeapByteBuffer[pos=0 lim=744 cap=744])),HTTP/1.1))
java.lang.IllegalStateException: Promise already completed: akka.dispatch.DefaultPromise@41a0e9b5 tried to complete with Right(HttpResponse(OK ,HttpHeaders(Map()),Some(Left(java.nio.HeapByteBuffer[pos=0 lim=744 cap=744])),HTTP/1.1))
    at akka.dispatch.Promise$class.complete(Future.scala:782)
    at akka.dispatch.DefaultPromise.complete(Future.scala:847)
    at akka.dispatch.Future$$anonfun$recover$1.apply(Future.scala:548)
    at akka.dispatch.Future$$anonfun$recover$1.apply(Future.scala:546)
    at akka.dispatch.DefaultPromise.akka$dispatch$DefaultPromise$$notifyCompleted(Future.scala:943)
    at akka.dispatch.DefaultPromise$$anonfun$tryComplete$1$$anonfun$apply$mcV$sp$4.apply(Future.scala:920)
    at akka.dispatch.DefaultPromise$$anonfun$tryComplete$1$$anonfun$apply$mcV$sp$4.apply(Future.scala:920)
    at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59)
    at scala.collection.immutable.List.foreach(List.scala:76)
    at akka.dispatch.DefaultPromise$$anonfun$tryComplete$1.apply$mcV$sp(Future.scala:920)
    at akka.dispatch.Future$$anon$4$$anonfun$run$1.apply$mcV$sp(Future.scala:386)
    at akka.dispatch.Future$$anon$4$$anonfun$run$1.apply(Future.scala:378)
    at akka.dispatch.Future$$anon$4$$anonfun$run$1.apply(Future.scala:378)
    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
    at akka.dispatch.Future$$anon$4.run(Future.scala:378)
    at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:94)
    at akka.jsr166y.ForkJoinTask$AdaptedRunnableAction.exec(ForkJoinTask.java:1381)
    at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259)
    at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975)
    at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479)
    at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

To make it as simple as possible to run Precog, we need to bundle Precog and Labcoat and all required dependencies into a fully self-contained package suitable for distribution, run all Precog services in a single process using a single port, and launch Precog and Labcoat with a single command.

Configuration options for this single process server should be kept extremely minimal, and every option must have a sensible default which works out of the box on all supported platforms (Mac, Linux, Windows).

Among the possible options:
- The port to run on. Could default to something like 7777.
- The home directory for the file system (if local file system is being used). Could default to something like ./data/.
- The home directory for temporary files. Could default to /tmp/ or ./tmp/.
- The directory for accounts/security/etc. metadata (location of H2 database?). Could default to something like ./meta/.

With no external dependencies and simple configuration options that all have sensible defaults, it will be possible for average and casual users to maintain Precog, and many more people to try Precog out without having to master a half dozen other technologies (kafka, zookeeper, haproxy, mongodb, httpd, etc.).

This ticket will be considered complete when the following is possible:
- Run an sbt task to build the new standalone release from scratch (both Precog and Labcoat)
- `cd` into the standalone release directory
- Run `precog` or `precog.bat` scripts depending on OS (Mac/Linux or Windows)
- If no command-line arguments are specified, the script launches Precog server in a single process and port (if it is not already running), and launches Labcoat configured to point to the newly-launched Precog server
- In addition to the default action of starting Precog and launching Labcoat, the scripts support the commands 'stop', 'start', and 'restart', which stop, start, and restart the Precog server (respectively), as well as a  'launch' command which launches Labcoat, and a 'status' command which shows whether or not Precog server is running.

The standalone release will be the release version that's pre-built and distributed online for users who don't want to build Precog / Labcoat from scratch. Therefore, it's essential that it be bullet-proof and "just work" out of the box with no tweaking, configuration, or additional external dependencies.

This ticket should not be completed most of the other tickets in the Simplified Precog milestone have been completed.

The auth and accounts services need to be merged (they are heavily dependent on each other), and their interfaces simplified.

Below is a brief account of the intended Precog security model resulting from this ticket.

Users have grants. Grants are the analogue of operations in an ACL security model.

All grants are bound to a particular file or directory; they confer permissions with respect to that resource.
- **Read** -- Read contents of file / read children of directory
- **Append** -- Append new contents to file / append new child in directory
- **Update** -- Change contents of file / rename children
- **Execute** -- Execute script / execute default script associated with directory
- **Delete** -- Delete file / delete directory
- **Mount** -- Mount a data source to the file / mount a data source in the directory
- **Unmount** -- Unmount a data source to the file / unmount a data source in the directory

Unlike the POSIX file security model, grants are hierarchical. Currently, they are always and only hierarchical.

Grants can be used to create additional grants that have the same or reduced permissions.

This ticket will be considered complete when the auth and accounts service have been unified into a single service, the internal logic simplified and refactored to match the above, and a clean, robust, and well-documented REST API exposed (the existing API is not unified, is inconsistent in places, is not robust, and is poorly documented).
## Minimal API

```
GET, POST /access/users/
GET, PUT /access/users/'userId
GET /access/users/'userId/grants/
GET /access/users/'userId/grants/'grantId
GET, POST, DELETE /access/users/'userId/shares/_byusers/'user
GET, POST, DELETE /access/users/'userId/shares/_bypaths/'path
GET, POST, DELETE /access/users/'userId/shares/_byperms/'perm
```
