### System information
Type | Version/Name
 --- | --- 
Distribution Name	| Ubuntu
Distribution Version	| 18.04
Linux Kernel	| 5.3.8
Architecture	| amd64
ZFS Version	| 0.8.2-1
SPL Version	| 0.8.2-1

The kernel is self compiled using GCC 8.3.0, based on Ubuntu's stock config (mostly unchanged except disabling RETPOLINE).

### Describe the problem you're observing
I was copying a bunch of media files (size varies from 30MB to 4GB) from a ZFS filesystem to a XFS filesystem using `cp -av`. Suddenly, it freezed and unable to be interrupted by `Ctrl-C`.

### Describe how to reproduce the problem
Not reproduced so far (rebooted and continue by `cp -auv`).

### Include any warning/errors/backtraces from the system logs
```
[150684.873375] INFO: task cp:29256 blocked for more than 120 seconds.
[150684.873468]       Tainted: P        W  OEL    5.3.8-reimu #1
[150684.873598] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[150684.873712] cp              D    0 29256   6441 0x00000000
[150684.873719] Call Trace:
[150684.873742]  ? __schedule+0x2b0/0x640
[150684.873749]  schedule+0x2f/0xa0
[150684.873754]  schedule_timeout+0x16a/0x320
[150684.873923]  ? dbuf_rele_and_unlock+0x610/0x610 [zfs]
[150684.873933]  ? __next_timer_interrupt+0xc0/0xc0
[150684.873940]  io_schedule_timeout+0x23/0x50
[150684.873959]  __cv_timedwait_common+0x14d/0x190 [spl]
[150684.873968]  ? wait_woken+0x80/0x80
[150684.874133]  zio_wait+0x134/0x280 [zfs]
[150684.874268]  dmu_buf_hold_array_by_dnode+0x153/0x480 [zfs]
[150684.874404]  dmu_read_uio_dnode+0x50/0x100 [zfs]
[150684.874572]  ? rangelock_enter+0x145/0x500 [zfs]
[150684.874705]  dmu_read_uio_dbuf+0x4f/0x80 [zfs]
[150684.874873]  zfs_read+0x128/0x490 [zfs]
[150684.875039]  zpl_read_common_iovec+0xaf/0x100 [zfs]
[150684.875205]  zpl_iter_read+0x118/0x1a0 [zfs]
[150684.875216]  new_sync_read+0x124/0x1b0
[150684.875222]  vfs_read+0xa8/0x170
[150684.875226]  ksys_read+0x6e/0x100
[150684.875234]  do_syscall_64+0x4f/0x130
[150684.875240]  entry_SYSCALL_64_after_hwframe+0x44/0xa9
[150684.875246] RIP: 0033:0x7fd8fb4d0081
[150684.875258] Code: Bad RIP value.
[150684.875261] RSP: 002b:00007fff0ef5f3a8 EFLAGS: 00000246 ORIG_RAX: 0000000000000000
[150684.875265] RAX: ffffffffffffffda RBX: 0000000000020000 RCX: 00007fd8fb4d0081
[150684.875268] RDX: 0000000000020000 RSI: 0000562cc4b5e000 RDI: 0000000000000003
[150684.875270] RBP: 0000000000000000 R08: 0000000000020000 R09: 0000000000000000
[150684.875272] R10: 0000000000020000 R11: 0000000000000246 R12: 000000000ef5f900
[150684.875274] R13: 0000000000000000 R14: 0000562cc4b7e000 R15: 000000000ef5f900
[150805.713292] INFO: task cp:29256 blocked for more than 241 seconds.
[150805.713376]       Tainted: P        W  OEL    5.3.8-reimu #1
[150805.713450] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[150805.713558] cp              D    0 29256   6441 0x00000004
[150805.713561] Call Trace:
[150805.713576]  ? __schedule+0x2b0/0x640
[150805.713578]  schedule+0x2f/0xa0
[150805.713580]  schedule_timeout+0x16a/0x320
[150805.713652]  ? dbuf_rele_and_unlock+0x610/0x610 [zfs]
[150805.713657]  ? __next_timer_interrupt+0xc0/0xc0
[150805.713660]  io_schedule_timeout+0x23/0x50
[150805.713668]  __cv_timedwait_common+0x14d/0x190 [spl]
[150805.713672]  ? wait_woken+0x80/0x80
[150805.713726]  zio_wait+0x134/0x280 [zfs]
[150805.713770]  dmu_buf_hold_array_by_dnode+0x153/0x480 [zfs]
[150805.713814]  dmu_read_uio_dnode+0x50/0x100 [zfs]
[150805.713868]  ? rangelock_enter+0x145/0x500 [zfs]
[150805.713911]  dmu_read_uio_dbuf+0x4f/0x80 [zfs]
[150805.713966]  zfs_read+0x128/0x490 [zfs]
[150805.714020]  zpl_read_common_iovec+0xaf/0x100 [zfs]
[150805.714074]  zpl_iter_read+0x118/0x1a0 [zfs]
[150805.714079]  new_sync_read+0x124/0x1b0
[150805.714082]  vfs_read+0xa8/0x170
[150805.714083]  ksys_read+0x6e/0x100
[150805.714087]  do_syscall_64+0x4f/0x130
[150805.714090]  entry_SYSCALL_64_after_hwframe+0x44/0xa9
[150805.714093] RIP: 0033:0x7fd8fb4d0081
[150805.714099] Code: Bad RIP value.
[150805.714100] RSP: 002b:00007fff0ef5f3a8 EFLAGS: 00000246 ORIG_RAX: 0000000000000000
[150805.714102] RAX: ffffffffffffffda RBX: 0000000000020000 RCX: 00007fd8fb4d0081
[150805.714103] RDX: 0000000000020000 RSI: 0000562cc4b5e000 RDI: 0000000000000003
[150805.714104] RBP: 0000000000000000 R08: 0000000000020000 R09: 0000000000000000
[150805.714104] R10: 0000000000020000 R11: 0000000000000246 R12: 000000000ef5f900
[150805.714105] R13: 0000000000000000 R14: 0000562cc4b7e000 R15: 000000000ef5f900
```

<!-- Please fill out the following template, which will help other contributors address your issue. -->

<!--
Thank you for reporting an issue.

*IMPORTANT* - Please search our issue tracker *before* making a new issue.
If you cannot find a similar issue, then create a new issue.
https://github.com/zfsonlinux/zfs/issues 

*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.
Please search the wiki and the mailing list archives before asking 
questions on the mailing list.
https://github.com/zfsonlinux/zfs/wiki/Mailing-Lists

Please fill in as much of the template as possible.
-->

### System information
<!--  add version after "|" character -->
Type | Version/Name
 --- | --- 
Distribution Name	|  CentOS
Distribution Version	| 8.1.1911
Linux Kernel	| 4.18.0
Architecture	| x86_6
ZFS Version	| 0.8.2
SPL Version	| 
<!-- 
Commands to find ZFS/SPL versions:
modinfo zfs | grep -iw version
modinfo spl | grep -iw version 
-->

### Describe the problem you're observing

Installing zfs-kmod package fails on CentOS 8.1.1911.
Furthermore working Centos8 zfs systems fail to work after update to 8.1.

### Describe how to reproduce the problem

- Install fresh Centos 8.1
- install zfs repo
- switch to zfs-kmod repo
- run "dnf install zfs"

or

- Update working CentOS 8 system with zfs (dnf update)
- reboot


### Include any warning/errors/backtraces from the system logs

Fresh install:

```
# dnf install zfs
Last metadata expiration check: 11:13:19 ago on Fri 17 Jan 2020 01:58:40 PM CET.
Error: 
 Problem: package zfs-0.8.2-1.el8.x86_64 requires zfs-kmod = 0.8.2, but none of the providers can be installed
  - conflicting requests
  - nothing provides kernel(__remove_inode_hash) = 0x8b7a4ec8 needed by kmod-zfs-0.8.2-1.el8.x86_64
  - nothing provides kernel(clear_inode) = 0x05ef95a0 needed by kmod-zfs-0.8.2-1.el8.x86_64
  - nothing provides kernel(clear_nlink) = 0x51b31641 needed by kmod-zfs-0.8.2-1.el8.x86_64
  - nothing provides kernel(current_time) = 0x79e296fb needed by kmod-zfs-0.8.2-1.el8.x86_64
  - nothing provides kernel(d_add_ci) = 0xee0c1e0d needed by kmod-zfs-0.8.2-1.el8.x86_64
  - nothing provides kernel(d_instantiate) = 0xa5b7f5db needed by kmod-zfs-0.8.2-1.el8.x86_64
  - nothing provides kernel(d_invalidate) = 0x84984326 needed by kmod-zfs-0.8.2-1.el8.x86_64
  - nothing provides kernel(d_make_root) = 0x0297e905 needed by kmod-zfs-0.8.2-1.el8.x86_64
  - nothing provides kernel(d_obtain_alias) = 0xa56bdac4 needed by kmod-zfs-0.8.2-1.el8.x86_64
  - nothing provides kernel(d_prune_aliases) = 0x91edc70b needed by kmod-zfs-0.8.2-1.el8.x86_64
  - nothing provides kernel(d_set_d_op) = 0xd41942ad needed by kmod-zfs-0.8.2-1.el8.x86_64

[...]
```  

Update from CentOS8:

```
# /sbin/modprobe zfs
modprobe: FATAL: Module zfs not found in directory /lib/modules/4.18.0-147.3.1.el8_1.x86_64

```



<!-- 
*IMPORTANT* - Please mark logs and text output from terminal commands 
or else Github will not display them correctly. 
An example is provided below.

Example:
```
this is an example how log text should be marked (wrap it with ```)
```
-->


[zdb -b filesystem022-OST21.txt](https://github.com/zfsonlinux/zfs/files/4075758/zdb.-b.filesystem022-OST21.txt)
Problem description:
File Size < on disk size - currently unexplained, size on disk is 2-3 x file size.

Observations:
1.	A potential ZFS filesystem022 corruption across RaidInc Storage in London?
2.	zdb check for leaks, it walks the entire block tree constructing the space maps in memory and then compares them to the ones stored on disk. If they differ it reports the leak. 
a.	Presuming from the below investigation that the “space leaks” mean the pool is corrupted somehow. zdb (ZFS debug) has detected tons of corruptions. 
3.	zdb did not report space leaks on ZFS Houston SI’s.   
4.	Does zdb leaked space means trouble with the pool and could explain the file size < disk size discrepancy?  
5.	Is it possible that errors got injected due to failover or hardware errors?
6.	It seems to be at least inconsistent which is supposed to never happen with ZFS. Is this indicative of a larger problem? Numerous lockups, etc.?

Investigation:
For the troubleshooting, the following file located in WEY, was selected. There are no snapshots/reservations/quotas involved here.

server01]</users/user001>$ du -h --apparent-size /lus/filesystem022/project/file_name/*
33K    /lus/filesystem022/project/file_name/aux_data
19K    /lus/filesystem022/project/file_name/descriptor.yaml
104G   /lus/filesystem022/project/file_name/trace_data.bin
14G    /lus/filesystem022/project/file_name/trace_header.bin

[server01]</users/user001>$ du -h /lus/filesystem022/project/file_name/*
33K    /lus/filesystem022/project/file_name/aux_data
56K    /lus/filesystem022/project/file_name/descriptor.yaml
237G   /lus/filesystem022/project/file_name/trace_data.bin
31G    /lus/filesystem022/project/file_name/trace_header.bin

1.	Copy of the dataset onto the same storage. 
o	Disk size is different. 
o	Checksum matches.

[server01]</users/user001>$ cp -rp /lus/filesystem022/project/file_name /lus/filesystem022/project/p005j02_2010_SRME_1238A018_JC

[server01]</users/user001>$ md5sum  /lus/filesystem022/project/file_name/*
md5sum: /lus/filesystem022/project/file_name/aux_data: Is a directory
f861b60d2b1b844e5ae252345aa20497  /lus/filesystem022/project/file_name/descriptor.yaml
e8ac57c241e52b38b60907e4e767b451  /lus/filesystem022/project/file_name/trace_data.bin
0826bc74e525697d769248aabcb195cd  /lus/filesystem022/project/file_name/trace_header.bin

[server01]</users/user001>$  md5sum  /lus/filesystem022/project/p005j02_2010_SRME_1238A018_JC/*
md5sum: /lus/filesystem022/project/p005j02_2010_SRME_1238A018_JC/aux_data: Is a directory
f861b60d2b1b844e5ae252345aa20497  /lus/filesystem022/project/p005j02_2010_SRME_1238A018_JC/descriptor.yaml
e8ac57c241e52b38b60907e4e767b451  /lus/filesystem022/project/p005j02_2010_SRME_1238A018_JC/trace_data.bin
0826bc74e525697d769248aabcb195cd  /lus/filesystem022/project/p005j02_2010_SRME_1238A018_JC/trace_header.bin

[server01]</users/user001>$ du -h  /lus/filesystem022/project/p005j02_2010_SRME_1238A018_JC/*
33K    /lus/filesystem022/project/p005j02_2010_SRME_1238A018_JC/aux_data
56K    /lus/filesystem022/project/p005j02_2010_SRME_1238A018_JC/descriptor.yaml
99G    /lus/filesystem022/project/p005j02_2010_SRME_1238A018_JC/trace_data.bin
13G    /lus/filesystem022/project/p005j02_2010_SRME_1238A018_JC/trace_header.bin

[server01]</users/user001>$ du -h --apparent-size /lus/filesystem022/project/p005j02_2010_SRME_1238A018_JC/*
33K    /lus/filesystem022/project/p005j02_2010_SRME_1238A018_JC/aux_data
19K    /lus/filesystem022/project/p005j02_2010_SRME_1238A018_JC/descriptor.yaml
104G   /lus/filesystem022/project/p005j02_2010_SRME_1238A018_JC/trace_data.bin
14G    /lus/filesystem022/project/p005j02_2010_SRME_1238A018_JC/trace_header.bin


2.	Printing the OST name hosting the given file.
[server01]</users/user001>$ ./lustre-find-ost-for-file /lus/filesystem022/project/file_name/trace_data.bin
15
/lus/filesystem022/project/file_name/trace_data.bin: ['filesystem022-OST000f'] (filesystem022-oss6)

3.	Run zdb to check for leaks
[root@filesystem022-oss6 ~]# zfs list
NAME                          USED  AVAIL  REFER  MOUNTPOINT
filesystem022-OST17                 48.3T  18.5T   219K  none
filesystem022-OST17/filesystem022-OST0005  48.3T  18.5T  48.3T  none
filesystem022-OST19                 49.5T  17.3T   219K  none
filesystem022-OST19/filesystem022-OST0009  49.5T  17.3T  49.5T  none
filesystem022-OST21                 47.3T  19.5T   219K  none
filesystem022-OST21/filesystem022-OST000f  47.3T  19.5T  47.3T  none
filesystem022-OST23                 51.1T  15.7T   219K  none
filesystem022-OST23/filesystem022-OST0013  51.1T  15.7T  51.1T  none

[root@filesystem022-oss6 ~]# zdb -b filesystem022-OST21
Traversing all blocks to verify nothing leaked ...

loading space map for vdev 0 of 1, metaslab 180 of 181 ...
62.0T completed (12801MB/s) estimated time remaining: 0hr 00min 07sec        
leaked space: vdev 0, offset 0x1d80003de000, size 1081344
[…]
See attachment.

Please would someone be able to advise.

Thanks
Nick

<!--- Please fill out the following template, which will help other contributors review your Pull Request. -->

<!--- Provide a general summary of your changes in the Title above -->

<!---
Documentation on ZFS Buildbot options can be found at
https://github.com/zfsonlinux/zfs/wiki/Buildbot-Options
-->

### Motivation and Context
<!--- Why is this change required? What problem does it solve? -->
<!--- If it fixes an open issue, please link to the issue here. -->
Presently, native encryption of a ZFS root filesystem requires a passphrase or keyfile to be available every boot.  A TPM 2.0 chip has the ability to store and release data, if certain criteria are met.  This PR provides an optional method, which allows TPM 2.0 to be used to automatically unlock the filesystem, without user intervention.

This is a DRAFT PR to foster discussion surrounding the method.  THIS MAY NOT CONFORM TO STYLE GUIDES OR EVEN GOOD CODING PRACTICES.  Code nit comments are not unwelcome, but methodology is the initial goal.

### Description
<!--- Describe your changes in detail -->
This PR does the following:
 - Within zfs's initramfs hooks, detects and copies needed tpm2-tools 4.x+ to the initrd
 - Provides zfs initramfs's script a method of detecting and unlocking using TPM2 tools
 - If TPM2 unlock is not successful, allows Initramfs to fallback to other manual passphrase methods 
 - Stores required (and user definable) unlocking variables in the encryption root using `org.openzfs.tpm2:index` and `org.openzfs.tpm2:pcrs` 
 - Provides a utility that maintains the TPM and the above zfs properties
 - The zfs passphrase is locked within the TPM's nvram, and only releasable with an expected PCR state and password (GUID used to confirm encrypted filesystem match)
 - The nvram region of the TPM is locked from further reading following the retrieval of the password within the initramfs script, preventing even root users access to the password post-boot.

Quick (and probably correctish) PCR usage:
~~~~~
PCR - Use
0 - Bios/Platform Code
1 - Bios/Platform Config
2 - Option Rom / UEFI Driver Code
3 - Option Rom / UEFI Config Data
4 - Bootloader code (GPT), boot attempts?
5 - Bootloader data
6 - Manufacturer use / wake events?
7 - Manufacturer use / Secureboot policy

8 - Grub 2.04 - Grub, Kernel, and Module Commands
9 - Grub 2.04 - ANY file read by Grub (includes kernel and initrd)
10 - Linux Integrity Measurement Architecture (IMA)

1-7 Hardware
8+ Operating System
~~~~~

### How Has This Been Tested?
<!--- Please describe in detail how you tested your changes. -->
<!--- Include details of your testing environment, and the tests you ran to -->
<!--- see how your change affects other areas of the code, etc. -->
<!--- If your change is a performance enhancement, please provide benchmarks here. -->
<!--- Please think about using the draft PR feature if appropriate -->
Tpm-tools 4.x revamped its arguments.  Grub 2.04 is secure boot compatible and now reports additional boot time measurements that could harden the boot protections.  Therefore, this has been developed and tested solely on Ubuntu Focal pre-release (to be 20.04 LTS) on Dell TPM 2.0 hardware using UEFI boot.  Testing different distros, kernels, and hardware TPM devices need to be conducted. 


### Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [ ] Bug fix (non-breaking change which fixes an issue)
- [X] New feature (non-breaking change which adds functionality)
- [ ] Performance enhancement (non-breaking change which improves efficiency)
- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)
- [ ] Documentation (a change to man pages or other documentation)

### Checklist:
<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->
- [ ] My code follows the ZFS on Linux [code style requirements](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md#coding-conventions).
- [ ] I have updated the documentation accordingly.
- [X] I have read the [**contributing** document](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md).
- [ ] I have added [tests](https://github.com/zfsonlinux/zfs/tree/master/tests) to cover my changes.
- [ ] I have run the ZFS Test Suite with this change applied.
- [ ] All commit messages are properly formatted and contain [`Signed-off-by`](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md#signed-off-by).

### Things to Explore / Current Status (will be edited over time)
- Dracut integration
- Include kernel commandline `panic=5` monitoring/injection to prevent snooping from the initrd prompt
- Consider updating TPM within the initrd, following autounlock failure, but successful manual entry (ie. after kernel/initrd updates)
- Documentation of TPM PCRs  
- Consider the ability to use keyfiles  (recovery method must match what is in the TPM)
- Prove functionality on other platforms

### Resources
[Tpm2-tools man pages](https://github.com/tpm2-software/tpm2-tools/tree/master/man)
[Grub manual (specifically Measured Boot Section)](https://www.gnu.org/software/grub/manual/grub/grub.html#Measured-Boot)
[UEFI PCR explanation on Pg. 2](https://media.defense.gov/2019/Jul/16/2002158058/-1/-1/0/CSI-BOOT-SECURITY-MODES-AND-RECOMMENDATIONS.PDF)
[Another PCR description on 4th page](https://www.usenix.org/system/files/conference/usenixsecurity18/sec18-han.pdf)

This adds a lazytime dataset property that controls the lazytime mount
option. The handling of the relatime property was used as a template for
how to implement this. This addresses #9843.

Signed-off-by: Clint Armstrong <clint@clintarmstrong.net>

<!--- Please fill out the following template, which will help other contributors review your Pull Request. -->

<!--- Provide a general summary of your changes in the Title above -->

<!---
Documentation on ZFS Buildbot options can be found at
https://github.com/zfsonlinux/zfs/wiki/Buildbot-Options
-->

### Motivation and Context
Fixes #9843
<!--- Why is this change required? What problem does it solve? -->
<!--- If it fixes an open issue, please link to the issue here. -->

### Description
Adds a lazytime property to influence lazytime on mount, basically done by copying how relatime is handled.
<!--- Describe your changes in detail -->

### How Has This Been Tested?
Built and manually tested on Ubuntu 18.04. Table below shows the mount options when creating a dataset with the options listed:

| atime  | relatime | lazytime | mount options |
| ------------- | ------------- | ----------|----|
| off | off | off | noatime |
| off | on | off | noatime |
| off | off | on | noatime,lazytime |
| off | on | on | noatime,lazytime |
| on | off | off | strictatime |
| on | on | off | relatime |
| on | off | on | lazytime |
| on | on | on | relatime,lazytime |

Tests have been written using relatime tests as a guide, but none of the atime tests pass, and did not pass on the last tagged release, zfs-0.8.2.

<!--- Please describe in detail how you tested your changes. -->
<!--- Include details of your testing environment, and the tests you ran to -->
<!--- see how your change affects other areas of the code, etc. -->
<!--- If your change is a performance enhancement, please provide benchmarks here. -->
<!--- Please think about using the draft PR feature if appropriate -->

### Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [ ] Bug fix (non-breaking change which fixes an issue)
- [x] New feature (non-breaking change which adds functionality)
- [ ] Performance enhancement (non-breaking change which improves efficiency)
- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)
- [ ] Documentation (a change to man pages or other documentation)

### Checklist:
<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->
- [x] My code follows the ZFS on Linux [code style requirements](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md#coding-conventions).
- [x] I have updated the documentation accordingly.
- [x] I have read the [**contributing** document](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md).
- [x] I have added [tests](https://github.com/zfsonlinux/zfs/tree/master/tests) to cover my changes.
- [ ] I have run the ZFS Test Suite with this change applied.
- [x] All commit messages are properly formatted and contain [`Signed-off-by`](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md#signed-off-by).

<!-- Please fill out the following template, which will help other contributors address your issue. -->

<!--
Thank you for reporting an issue.

*IMPORTANT* - Please search our issue tracker *before* making a new issue.
If you cannot find a similar issue, then create a new issue.
https://github.com/zfsonlinux/zfs/issues 

*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.
Please search the wiki and the mailing list archives before asking 
questions on the mailing list.
https://github.com/zfsonlinux/zfs/wiki/Mailing-Lists

Please fill in as much of the template as possible.
-->

### System information
<!--  add version after "|" character -->
Type | Version/Name
 --- | --- 
Distribution Name	|  Proxmox 
Distribution Version	|  6.1
Linux Kernel	| Linux longmox 5.3.13-1-pve #1 SMP PVE 5.3.13-1 (Thu, 05 Dec 2019 07:18:14 +0100) x86_64 GNU/Linux
ZFS Version	|  0.8.2-pve2
SPL Version	|  0.8.2-pve2
<!-- 
Commands to find ZFS/SPL versions:
modinfo zfs | grep -iw version
modinfo spl | grep -iw version 
-->

### Describe the problem you're observing

So I have a zpool that's degraded. I then tried to create snapshots to do a backup. I snapshotted the whole pool `sudo zfs snapshot -r rpool@dump`

I now have a pool that's wedged, and I cannot do *anything*.

I'm not sure how to reproduce this, and I currently have a server that's basically totally hosed. 

```NAME    SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT
rpool   464G   237G   227G        -         -    67%    51%  1.00x  DEGRADED  -
root@longmox:/# sudo zfs list
NAME                       USED  AVAIL     REFER  MOUNTPOINT
rpool                      501G     0B      168K  /rpool
rpool/ROOT                13.3G     0B       96K  /rpool/ROOT
rpool/ROOT/pve-1          13.3G     0B     13.3G  /
rpool/base-127-disk-0     18.0G  15.5G     2.57G  -
rpool/basevol-119-disk-0   765M     0B      765M  /rpool/basevol-119-disk-0
rpool/data                  96K     0B       96K  /rpool/data
rpool/subvol-101-disk-0   61.5G     0B     61.5G  /rpool/subvol-101-disk-0
rpool/subvol-102-disk-0    921M     0B      921M  /rpool/subvol-102-disk-0
rpool/subvol-108-disk-0   7.84G     0B     7.84G  /rpool/subvol-108-disk-0
rpool/subvol-120-disk-0   2.12G     0B     2.12G  /rpool/subvol-120-disk-0
rpool/subvol-121-disk-0   3.53G     0B     3.53G  /rpool/subvol-121-disk-0
rpool/subvol-122-disk-0   1.47G     0B     1.47G  /rpool/subvol-122-disk-0
rpool/subvol-123-disk-0   1.80G     0B     1.80G  /rpool/subvol-123-disk-0
rpool/subvol-124-disk-0   1.66G     0B     1.66G  /rpool/subvol-124-disk-0
rpool/subvol-125-disk-0   2.07G     0B     2.07G  /rpool/subvol-125-disk-0
rpool/subvol-126-disk-0   1.43G     0B     1.43G  /rpool/subvol-126-disk-0
rpool/subvol-130-disk-0   1.53G     0B     1.53G  /rpool/subvol-130-disk-0
rpool/subvol-131-disk-0   1.12G     0B     1.12G  /rpool/subvol-131-disk-0
rpool/subvol-135-disk-0    919M     0B      919M  /rpool/subvol-135-disk-0
rpool/subvol-138-disk-0   3.10G     0B     3.10G  /rpool/subvol-138-disk-0
rpool/subvol-141-disk-0    832M     0B      832M  /rpool/subvol-141-disk-0
rpool/vm-103-disk-0       56.1G  33.0G     23.1G  -
rpool/vm-104-disk-0       17.0G  10.3G     6.71G  -
rpool/vm-104-disk-1       6.42G     0B     6.42G  none
rpool/vm-104-disk-2        137G   102G     34.1G  -
rpool/vm-105-disk-0       27.0G  15.4G     11.5G  -
rpool/vm-106-disk-0       25.4G  15.5G     9.94G  -
rpool/vm-128-disk-0       18.4G  15.5G     2.90G  -
rpool/vm-129-disk-0       22.0G  15.5G     6.55G  -
rpool/vm-133-disk-0       46.4G  25.8G     20.6G  -
rpool/vm-134-disk-0       21.3G  15.5G     5.84G  -
root@longmox:/# sudo zfs list -t snapshot
NAME                                USED  AVAIL     REFER  MOUNTPOINT
rpool@dump                            0B      -      168K  -
rpool/ROOT@dump                       0B      -       96K  -
rpool/ROOT/pve-1@dump                 0B      -     13.3G  -
rpool/base-127-disk-0@__base__        8K      -     2.57G  -
rpool/base-127-disk-0@dump            0B      -     2.57G  -
rpool/basevol-119-disk-0@__base__     8K      -      765M  -
rpool/basevol-119-disk-0@dump         0B      -      765M  -
rpool/data@dump                       0B      -       96K  -
rpool/subvol-101-disk-0@dump          0B      -     61.5G  -
rpool/subvol-102-disk-0@dump          0B      -      921M  -
rpool/subvol-108-disk-0@dump          0B      -     7.84G  -
rpool/subvol-120-disk-0@dump          0B      -     2.12G  -
rpool/subvol-121-disk-0@dump          0B      -     3.53G  -
rpool/subvol-122-disk-0@dump          0B      -     1.47G  -
rpool/subvol-123-disk-0@dump          0B      -     1.80G  -
rpool/subvol-124-disk-0@dump          0B      -     1.66G  -
rpool/subvol-125-disk-0@dump          0B      -     2.07G  -
rpool/subvol-126-disk-0@dump          0B      -     1.43G  -
rpool/subvol-130-disk-0@dump          0B      -     1.53G  -
rpool/subvol-131-disk-0@dump          0B      -     1.12G  -
rpool/subvol-135-disk-0@dump          0B      -      919M  -
rpool/subvol-138-disk-0@dump          0B      -     3.10G  -
rpool/subvol-141-disk-0@dump          0B      -      832M  -
rpool/vm-103-disk-0@dump              0B      -     23.1G  -
rpool/vm-104-disk-0@dump             11M      -     6.71G  -
rpool/vm-104-disk-1@snap              0B      -     6.42G  -
rpool/vm-104-disk-1@dump              0B      -     6.42G  -
rpool/vm-104-disk-2@dump            487M      -     33.6G  -
rpool/vm-105-disk-0@dump           26.5M      -     11.5G  -
rpool/vm-106-disk-0@dump           7.99M      -     9.94G  -
rpool/vm-128-disk-0@dump              0B      -     2.90G  -
rpool/vm-129-disk-0@dump              0B      -     6.55G  -
rpool/vm-133-disk-0@dump           10.4M      -     20.6G  -
rpool/vm-134-disk-0@dump              0B      -     5.84G  -

```


I cannot do *anything*.

```
root@longmox:/# zfs destroy rpool/vm-104-disk-2@dump
internal error: Channel number out of range
Aborted
root@longmox:/# zfs destroy rpool/data@dump
internal error: Channel number out of range
Aborted
root@longmox:~# rm vzdump-qemu-104-2019_04_22-23_31_57.vma
rm: cannot remove 'vzdump-qemu-104-2019_04_22-23_31_57.vma': No space left on device
root@longmox:~# echo > vzdump-qemu-104-2019_04_22-23_31_57.vma
bash: vzdump-qemu-104-2019_04_22-23_31_57.vma: No space left on device
root@longmox:~# truncate --size=0 vzdump-qemu-104-2019_04_22-23_31_57.vma
truncate: failed to truncate 'vzdump-qemu-104-2019_04_22-23_31_57.vma' at 0 bytes: No space left on device

```

(rm vzdump-qemu-104-2019_04_22-23_31_57.vma is a large file I was hoping to delete to clear up space)

I'm not sure what's going on. The underlying zpool apparently has `51%` utilization, but all the zvols are 100% full. Additionally, there are no quotas that seem to be causing this:

```
root@longmox:/# zfs get quota
NAME                               PROPERTY  VALUE  SOURCE
rpool                              quota     none   default
rpool@dump                         quota     -      -
rpool/ROOT                         quota     none   default
rpool/ROOT@dump                    quota     -      -
rpool/ROOT/pve-1                   quota     none   default
rpool/ROOT/pve-1@dump              quota     -      -
rpool/base-127-disk-0              quota     -      -
rpool/base-127-disk-0@__base__     quota     -      -
rpool/base-127-disk-0@dump         quota     -      -
rpool/basevol-119-disk-0           quota     none   default
rpool/basevol-119-disk-0@__base__  quota     -      -
rpool/basevol-119-disk-0@dump      quota     -      -
rpool/data                         quota     none   default
rpool/data@dump                    quota     -      -
rpool/subvol-101-disk-0            quota     none   default
rpool/subvol-101-disk-0@dump       quota     -      -
rpool/subvol-102-disk-0            quota     none   default
rpool/subvol-102-disk-0@dump       quota     -      -
rpool/subvol-108-disk-0            quota     none   default
rpool/subvol-108-disk-0@dump       quota     -      -
rpool/subvol-120-disk-0            quota     none   default
rpool/subvol-120-disk-0@dump       quota     -      -
rpool/subvol-121-disk-0            quota     none   default
rpool/subvol-121-disk-0@dump       quota     -      -
rpool/subvol-122-disk-0            quota     none   default
rpool/subvol-122-disk-0@dump       quota     -      -
rpool/subvol-123-disk-0            quota     none   default
rpool/subvol-123-disk-0@dump       quota     -      -
rpool/subvol-124-disk-0            quota     none   default
rpool/subvol-124-disk-0@dump       quota     -      -
rpool/subvol-125-disk-0            quota     none   default
rpool/subvol-125-disk-0@dump       quota     -      -
rpool/subvol-126-disk-0            quota     none   default
rpool/subvol-126-disk-0@dump       quota     -      -
rpool/subvol-130-disk-0            quota     none   default
rpool/subvol-130-disk-0@dump       quota     -      -
rpool/subvol-131-disk-0            quota     none   default
rpool/subvol-131-disk-0@dump       quota     -      -
rpool/subvol-135-disk-0            quota     none   default
rpool/subvol-135-disk-0@dump       quota     -      -
rpool/subvol-138-disk-0            quota     none   default
rpool/subvol-138-disk-0@dump       quota     -      -
rpool/subvol-141-disk-0            quota     none   default
rpool/subvol-141-disk-0@dump       quota     -      -
rpool/vm-103-disk-0                quota     -      -
rpool/vm-103-disk-0@dump           quota     -      -
rpool/vm-104-disk-0                quota     -      -
rpool/vm-104-disk-0@dump           quota     -      -
rpool/vm-104-disk-1                quota     none   default
rpool/vm-104-disk-1@snap           quota     -      -
rpool/vm-104-disk-1@dump           quota     -      -
rpool/vm-104-disk-2                quota     -      -
rpool/vm-104-disk-2@dump           quota     -      -
rpool/vm-105-disk-0                quota     -      -
rpool/vm-105-disk-0@dump           quota     -      -
rpool/vm-106-disk-0                quota     -      -
rpool/vm-106-disk-0@dump           quota     -      -
rpool/vm-128-disk-0                quota     -      -
rpool/vm-128-disk-0@dump           quota     -      -
rpool/vm-129-disk-0                quota     -      -
rpool/vm-129-disk-0@dump           quota     -      -
rpool/vm-133-disk-0                quota     -      -
rpool/vm-133-disk-0@dump           quota     -      -
rpool/vm-134-disk-0                quota     -      -
rpool/vm-134-disk-0@dump           quota     -      -
root@longmox:/# zfs get refquota
NAME                               PROPERTY  VALUE     SOURCE
rpool                              refquota  none      default
rpool@dump                         refquota  -         -
rpool/ROOT                         refquota  none      default
rpool/ROOT@dump                    refquota  -         -
rpool/ROOT/pve-1                   refquota  none      default
rpool/ROOT/pve-1@dump              refquota  -         -
rpool/base-127-disk-0              refquota  -         -
rpool/base-127-disk-0@__base__     refquota  -         -
rpool/base-127-disk-0@dump         refquota  -         -
rpool/basevol-119-disk-0           refquota  10G       local
rpool/basevol-119-disk-0@__base__  refquota  -         -
rpool/basevol-119-disk-0@dump      refquota  -         -
rpool/data                         refquota  none      default
rpool/data@dump                    refquota  -         -
rpool/subvol-101-disk-0            refquota  100G      local
rpool/subvol-101-disk-0@dump       refquota  -         -
rpool/subvol-102-disk-0            refquota  8G        local
rpool/subvol-102-disk-0@dump       refquota  -         -
rpool/subvol-108-disk-0            refquota  50G       local
rpool/subvol-108-disk-0@dump       refquota  -         -
rpool/subvol-120-disk-0            refquota  10G       local
rpool/subvol-120-disk-0@dump       refquota  -         -
rpool/subvol-121-disk-0            refquota  10G       local
rpool/subvol-121-disk-0@dump       refquota  -         -
rpool/subvol-122-disk-0            refquota  10G       local
rpool/subvol-122-disk-0@dump       refquota  -         -
rpool/subvol-123-disk-0            refquota  8G        local
rpool/subvol-123-disk-0@dump       refquota  -         -
rpool/subvol-124-disk-0            refquota  8G        local
rpool/subvol-124-disk-0@dump       refquota  -         -
rpool/subvol-125-disk-0            refquota  10G       local
rpool/subvol-125-disk-0@dump       refquota  -         -
rpool/subvol-126-disk-0            refquota  10G       local
rpool/subvol-126-disk-0@dump       refquota  -         -
rpool/subvol-130-disk-0            refquota  30G       local
rpool/subvol-130-disk-0@dump       refquota  -         -
rpool/subvol-131-disk-0            refquota  10G       local
rpool/subvol-131-disk-0@dump       refquota  -         -
rpool/subvol-135-disk-0            refquota  10G       local
rpool/subvol-135-disk-0@dump       refquota  -         -
rpool/subvol-138-disk-0            refquota  50G       local
rpool/subvol-138-disk-0@dump       refquota  -         -
rpool/subvol-141-disk-0            refquota  15G       local
rpool/subvol-141-disk-0@dump       refquota  -         -
rpool/vm-103-disk-0                refquota  -         -
rpool/vm-103-disk-0@dump           refquota  -         -
rpool/vm-104-disk-0                refquota  -         -
rpool/vm-104-disk-0@dump           refquota  -         -
rpool/vm-104-disk-1                refquota  none      default
rpool/vm-104-disk-1@snap           refquota  -         -
rpool/vm-104-disk-1@dump           refquota  -         -
rpool/vm-104-disk-2                refquota  -         -
rpool/vm-104-disk-2@dump           refquota  -         -
rpool/vm-105-disk-0                refquota  -         -
rpool/vm-105-disk-0@dump           refquota  -         -
rpool/vm-106-disk-0                refquota  -         -
rpool/vm-106-disk-0@dump           refquota  -         -
rpool/vm-128-disk-0                refquota  -         -
rpool/vm-128-disk-0@dump           refquota  -         -
rpool/vm-129-disk-0                refquota  -         -
rpool/vm-129-disk-0@dump           refquota  -         -
rpool/vm-133-disk-0                refquota  -         -
rpool/vm-133-disk-0@dump           refquota  -         -
rpool/vm-134-disk-0                refquota  -         -
rpool/vm-134-disk-0@dump           refquota  -         -

```

Additionally, I have no idea where `internal error: Channel number out of range` is coming from. It doesn't appear to exist in the current source.

### System information
<!--  add version after "|" character -->
Type | Linux x86-64 CentOS 7.7
 --- | --- 
Distribution Name	| CentOS
Distribution Version	| 7.7.1908
Linux Kernel	| 3.10.0-1062.9.1.el7.x86_64
Architecture	| x86_64
ZFS Version	| 0.8.2-1
SPL Version	| 0.8.2-1
<!-- 
Commands to find ZFS/SPL versions:
modinfo zfs | grep -iw version
modinfo spl | grep -iw version 
-->

### Describe the problem you're observing

Over the recent vacations, I decided to replace three drives in our ZFS pool which had been showing occasional read errors. This should be a fairly fast procedure, because the pool would be pretty much completely idle. The pool consists of 6 raidz2 vdevs with 12 10TB SAS drives in each. Coincidentally, the three drives I wanted to replace were all in different vdevs.

I manually faulted all three drives with ```zpool offline -f <olddrive>```, and then replaced each drive physically in the enclosure, and did a ```zpool replace <olddrive> <newdrive>```. Time between the three replace commands was in the range of 5-8 minutes, basically the time it took to remove the drive, get the new one in the sled, and replace it.

After I had done this, ```zpool status``` showed the pool as resilvering, and each of the three devices had ```(resilvering)``` after them in the list of devices. After a while, the time estimate stabilized at about 60 hours total, which seemed fine.

I let this run until completion, logging in remotely about when it was scheduled to finish to check that everything was ok. To my surprise, when the resilver finished, it immediately started again, but now ```zpool status``` showed the first drive I'd replaced as ```ONLINE```, while the other two were again marked as ```(resilvering)```. The time estimate again was about 60 hours after stabilizing. At this point, I prepared myself for this resilver to finish and then to start again with only the last drive, but surprisingly, after another two and a half days, the second resilver finished, and all drives show up as ```ONLINE```.

There seems to be several suboptimal behaviors here. One is that, obviously ZFS is capable of concurrently resilvering several drives in a pool (at least when they're in different vdevs, as was the case here, I've never tried doing it in the same vdev, and probably wouldn't unless the drives were totally dead), but it sometimes chooses to do it and other times not.

The other problem is that, if ZFS is really just resilvering one drive out of several, it should probably say so somehow. There's no way I could find out to tell if one or several drives were being resilvered, they all showed up as ```(resilvering)``` in ```zpool status```.


<!-- Please fill out the following template, which will help other contributors address your issue. -->

<!--
Thank you for reporting an issue.

*IMPORTANT* - Please search our issue tracker *before* making a new issue.
If you cannot find a similar issue, then create a new issue.
https://github.com/zfsonlinux/zfs/issues 

*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.
Please search the wiki and the mailing list archives before asking 
questions on the mailing list.
https://github.com/zfsonlinux/zfs/wiki/Mailing-Lists

Please fill in as much of the template as possible.
-->

### System information
<!--  add version after "|" character -->
Type | Version/Name
 --- | --- 
Distribution Name	| Arch Linux
Linux Kernel	| 5.4.10
Architecture	| x86_64
ZFS Version	| 0.8.2-1
SPL Version	| 0.8.2-1
<!-- 
Commands to find ZFS/SPL versions:
modinfo zfs | grep -iw version
modinfo spl | grep -iw version 
-->

ZFS supports lazytime, and datasets can be mounted with lazytime by running `mount -o remount,lazytime tank/foo`, but there is no property in the dataset to influence lazytime so that `zfs mount tank/foo` will set lazytime on.

<!-- Please fill out the following template, which will help other contributors address your issue. -->

<!--
Thank you for reporting an issue.

*IMPORTANT* - Please search our issue tracker *before* making a new issue.
If you cannot find a similar issue, then create a new issue.
https://github.com/zfsonlinux/zfs/issues 

*IMPORTANT* - This issue tracker is for *bugs* and *issues* only.
Please search the wiki and the mailing list archives before asking 
questions on the mailing list.
https://github.com/zfsonlinux/zfs/wiki/Mailing-Lists

Please fill in as much of the template as possible.
-->

### System information
<!--  add version after "|" character -->
Type | Version/Name
 --- | --- 
Distribution Name	|  Ubuntu Eoan 
Distribution Version	|  19.10
Linux Kernel	|  5.4.11
Architecture	|  aarch64
ZFS Version	|  0.8.1 
SPL Version	| 
<!-- 
Commands to find ZFS/SPL versions:
modinfo zfs | grep -iw version
modinfo spl | grep -iw version 
-->

### Describe the problem you're observing
zfs_dkms package fails to build due to kernel changes introduced in v5.4-rc1.
```
commit ce6595a28a15c874aee374757dcd08f537d7b24d
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Jul 14 16:42:44 2019 -0400

    kill the last users of user_{path,lpath,path_dir}()

    old wrappers with few callers remaining; put them out of their misery...

    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
```
Removed `user_path_dir` from include/linux/namei.h
This breaks building of spl, as spl-vnode.c uses this function.

### Describe how to reproduce the problem

Attempt to install zfs_dkms on Ubuntu Eoan running a v5.4 or above kernel.

### Include any warning/errors/backtraces from the system logs
<!-- 
*IMPORTANT* - Please mark logs and text output from terminal commands 
or else Github will not display them correctly. 
An example is provided below.

Example:
```
this is an example how log text should be marked (wrap it with ```)
```
-->
```
Unpacking zfs-dkms (0.8.1-1ubuntu14.3) ...
Setting up zfs-dkms (0.8.1-1ubuntu14.3) ...
Loading new zfs-0.8.1 DKMS files...
Building for 5.4.11-00006-g887426577d1c
Building initial module for 5.4.11-00006-g887426577d1c
Error! Bad return status for module build on kernel: 5.4.11-00006-g887426577d1c (aarch64)
Consult /var/lib/dkms/zfs/0.8.1/build/make.log for more information.
dpkg: error processing package zfs-dkms (--configure):
 installed zfs-dkms package post-installation script subprocess returned error exit status 10
Errors were encountered while processing:
 zfs-dkms
E: Sub-process /usr/bin/dpkg returned an error code (1)
```
```
CC [M]  /var/lib/dkms/zfs/0.8.1/build/module/zfs/cityhash.o
/var/lib/dkms/zfs/0.8.1/build/module/spl/spl-vnode.c: In function ‘vn_set_pwd’:
/var/lib/dkms/zfs/0.8.1/build/module/spl/spl-vnode.c:683:7: error: implicit declaration of function ‘user_path_dir’; did you mean ‘user_path_at’? [-Werror=implicit-function-declaration]
  683 |  rc = user_path_dir(filename, &path);
      |       ^~~~~~~~~~~~~
      |       user_path_at
  CC [M]  /var/lib/dkms/zfs/0.8.1/build/module/zfs/dbuf.o
cc1: some warnings being treated as errors
  CC [M]  /var/lib/dkms/zfs/0.8.1/build/module/zfs/dbuf_stats.o
make[5]: *** [scripts/Makefile.build:265: /var/lib/dkms/zfs/0.8.1/build/module/spl/spl-vnode.o] Error 1
make[4]: *** [scripts/Makefile.build:509: /var/lib/dkms/zfs/0.8.1/build/module/spl] Error 2
make[4]: *** Waiting for unfinished jobs....
```

Reference: https://github.com/zfsonlinux/spl/issues/719
<!--- Please fill out the following template, which will help other contributors review your Pull Request. -->

<!--- Provide a general summary of your changes in the Title above -->
dmu_buf_hold_array_by_dnode() can create zio on demand saving on few allocations.

<!---
Documentation on ZFS Buildbot options can be found at
https://github.com/zfsonlinux/zfs/wiki/Buildbot-Options
-->

### Motivation and Context
<!--- Why is this change required? What problem does it solve? -->
<!--- If it fixes an open issue, please link to the issue here. -->

### Description
<!--- Describe your changes in detail -->

### How Has This Been Tested?
<!--- Please describe in detail how you tested your changes. -->
tested with Lustre using number of different tests (including well-known like dbench, iozone)

<!--- Include details of your testing environment, and the tests you ran to -->
mostly in KVM

<!--- see how your change affects other areas of the code, etc. -->
<!--- If your change is a performance enhancement, please provide benchmarks here. -->
Lustre's sanity/60a takes 81s intead of 90s w/o the patch.

<!--- Please think about using the draft PR feature if appropriate -->

### Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [X ] Performance enhancement (non-breaking change which improves efficiency)
- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)
- [ ] Documentation (a change to man pages or other documentation)

### Checklist:
<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->
- [ ] My code follows the ZFS on Linux [code style requirements](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md#coding-conventions).
- [ ] I have updated the documentation accordingly.
- [ ] I have read the [**contributing** document](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md).
- [ ] I have added [tests](https://github.com/zfsonlinux/zfs/tree/master/tests) to cover my changes.
- [ ] I have run the ZFS Test Suite with this change applied.
- [ ] All commit messages are properly formatted and contain [`Signed-off-by`](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md#signed-off-by).
