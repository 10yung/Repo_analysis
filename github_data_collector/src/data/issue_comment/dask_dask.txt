I am creating an issue on behalf of Apache Airflow community and it's about Apache Airflow integration with Dask.

We are working on Apache Airflow 2.0 (still at least 3-4 months to release it) but we are doing various cleanup tasks. One of the things we noticed is that we have a DaskExecutor https://github.com/apache/airflow/blob/master/airflow/executors/dask_executor.py . It uses Dask to run the tasks. The executor is very little used. It's used so little that we even have all the Dask Executor tests failing for months - without anyone noticing. We understand that Dask is important for a number of people, but also in order to keep it in Airflow we need someone who actually uses Dask (and Dask Executor in Airflow) to keep it in a healthy state. We are querying at our devlist and checking if someone is willing to maintain it but we also think it might be a good idea to ask here. 

We are happy as Apache Airflow committers to review and accept all the related code but we would love someone more familiar and using Dask to maintain it :).

Note that the executor is rather simple and the tests https://github.com/apache/airflow/blob/master/tests/executors/test_dask_executor.py are just a few and not very complex.

It's literally 200 lines of code in total. We could fix it ourselves, but maybe having someone who actually uses Dask being just a bit more active in the Apache Airflow community is a good idea :)
```
>>> import dask.dataframe as dd
>>> import pandas as pd
>>> pdf = pd.DataFrame()
>>> ddf = dd.from_pandas(pdf, npartitions=1)
>>> pdf.sum()
Series([], dtype: float64)
>>> ddf.sum().compute()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/ec2-user/.local/lib/python3.6/site-packages/dask/dataframe/core.py", line 1522, in sum
    "sum", axis=axis, skipna=skipna, split_every=split_every, out=out
  File "/home/ec2-user/.local/lib/python3.6/site-packages/dask/dataframe/core.py", line 1490, in _reduction_agg
    result.divisions = (min(self.columns), max(self.columns))
ValueError: min() arg is an empty sequence
>>> pdf.count()
Series([], dtype: int64)
>>> ddf.count().compute()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/ec2-user/.local/lib/python3.6/site-packages/dask/dataframe/core.py", line 1648, in count
    result.divisions = (min(self.columns), max(self.columns))
ValueError: min() arg is an empty sequence
```
Same story for `mean`, `var` and `sem`.
The issue stems from the following line in `dask/dataframe/core.py`:
```
result.divisions = (min(self.columns), max(self.columns))
```

This may address #5441 and [cudf#3319](https://github.com/rapidsai/cudf/issues/3319), and may be a reasonable alternative to #5450 

The idea here is to accept a `sort=` argument for groupby operations, which can be passed along the final apply phase of apply-concat-apply groupby aggregations.  Currently, aggregations triggered by `_groupby_aggregate` are hard-coded to use `sort=False` (I assume for performance reasons), while others us the backend's default behavior.  Ideally, the sorting behavior should always depend on the argument added here.

**Notes**:
- #5450 achieves something similar by exposing `sort=` for aggregations themselves.  The approach used here seems easier to implement/maintain, but I am open to feedback.
- [**TODO**] Perhaps we should also set `sort=False` for the first *apply* phase of aggregation (for performace reasons).
- [**TODO**] The changes in this PR currently address cudf#3319, but thorough testing still needs to be added.

cc @beckernick (please feel free to advise on downstream needs here)

- [ ] Tests added / passed
- [ ] Passes `black dask` / `flake8 dask`

- [x] Tests added / passed
- [x] Passes `black dask` / `flake8 dask`

Hello, 
Our pipeline a has a place where we pass in a 4D array and a 3D array into `dask.array.map_blocks`. Each array has chunks along the left-most axis, as a 'batch size' and the batch sizes are equal. For example
```
arr4d.chunks = ((16,16,16), (100,), (100,), (3,))
arr3d.chunks = ((16,16,16), (4,), (2,))
```

To Clarify, that results in: 
```
arr4d.shape = (48, 100, 100, 3) 
arr4d.chunksize = (16, 100, 100, 3) 

arr3d.shape = (48, 4, 2) 
arr3d.chunksize = (16, 4, 2) 
```

when we called `da.map_blocks(fn, arr4d, arr3d)` (or vice-versa) we expected the function to be called once for each 'batch' (in this example, 3 times). 
However, the resulting behavior was it was called 9 times. The graph reflects this, it has a product type of effect where each combination is called. 

We were eventually able to get the expected behavior by swapping the axis so our arrays look like this: 
```
arr4d.chunks = ((100,) (16,16,16), (100,), (3,))
arr3d.chunks = ((16,16,16), (4,), (2,))
```
Then, we have the correct graph and results. 

I can understand the reason for the behavior, but for our pipeline it causes a major inconvenience, since the left axis for us almost always the batch size. So we may pass 16 images in, and get 16 lower dimension feature sets. That 16 will always sit on the left side. This is causing us to make a hacky solution to add dimensions to the lower dimension array so, our arrays look like this: 
```
arr4d.chunks = ((16,16,16), (100,), (100,), (3,))
arr3d.chunks = ((16,16,16), (4,), (2,), (1,))
```

This works, but for our application is quite hacky, since we have to do this dynamically to not change a bunch of code. Our current solution detects the highest number of dimensions, adds extra dimensions, then wraps the supplied function to removed the extra dimensions before continuing. 

It seems `map_blocks` is aligning it arguments' axes based on a _right-justified_ alignment of their axes. For our application it makes sense to operate on a _left-justified_ alignment, so our arrays will always line up on the lowest-indexed axis. so if we pass in a `NxWxH` array, a `Nx1` array and a `NxWxHxC` array all into a `map_blocks` call, we can do so without extensive reshaping and axis work.

Couple Questions: 
1. Is this all correct? Am I misunderstanding?
2. Is there an easy solution I am missing?
3. Is this noted in the documentation somewhere? If not, let's add it!
 
Thanks!


Fixes #5051 

- modifies `mean` to handle calculation for datetime series

- adds test to check result is equivalent to pandas

- [ ] Tests added / passed
- [ ] Passes `black dask` / `flake8 dask`

On https://docs.dask.org/en/latest/why.html the images for `Growth of major programming languages` and `Stack overflow traffic to various packages` are no longer rendering correctly
test-upstream
When running this script, the following errors represented.
```javascript
import cupy as cp
import dask.array as da
A = cp.asarray(np.ones((10, 10))*5)
B = cp.asarray(np.ones((10, 10))*6)
A = da.from_array(A, chunks=A.shape)
B = da.from_array(B, chunks=B.shape)
C = A + 1j * B
C.conj().compute()
```

errors

> TypeError: operand type(s) all returned NotImplemented from __array_ufunc__(<ufunc 'conjugate'>, '__call__', array([[5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j,
        5.+6.j, 5.+6.j],
       [5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j,
        5.+6.j, 5.+6.j],
       [5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j,
        5.+6.j, 5.+6.j],
       [5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j,
        5.+6.j, 5.+6.j],
       [5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j,
        5.+6.j, 5.+6.j],
       [5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j,
        5.+6.j, 5.+6.j],
       [5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j,
        5.+6.j, 5.+6.j],
       [5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j,
        5.+6.j, 5.+6.j],
       [5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j,
        5.+6.j, 5.+6.j],
       [5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j, 5.+6.j,
        5.+6.j, 5.+6.j]])): 'ndarray'

If I use Numpy, the errors do not appear. 
How can I use `dask.linalg.svd` with cupy array?
Thanks!

Pandas 1.0 changed the repr for DataFrame.info(): https://pandas.pydata.org/pandas-docs/version/1.0.0/whatsnew/v1.0.0.html#extended-verbose-info-output-for-dataframe

This is causing a couple failures

```
$ pytest dask/dataframe/tests/test_dataframe.py::test_info
================================================================================ test session starts ================================================================================
platform darwin -- Python 3.7.6, pytest-5.2.1, py-1.8.0, pluggy-0.13.0
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/Users/taugspurger/sandbox/dask/.hypothesis/examples')
rootdir: /Users/taugspurger/sandbox/dask, inifile: setup.cfg
plugins: xdist-1.29.0, hypothesis-4.36.2, forked-1.0.2, repeat-0.8.0, cov-2.8.1
collected 1 item

dask/dataframe/tests/test_dataframe.py F                                                                                                                                      [100%]

===================================================================================== FAILURES ======================================================================================
_____________________________________________________________________________________ test_info _____________________________________________________________________________________

    def test_info():
        from io import StringIO

        pandas_format._put_lines = put_lines

        test_frames = [
            pd.DataFrame(
                {"x": [1, 2, 3, 4], "y": [1, 0, 1, 0]}, index=pd.Int64Index(range(4))
            ),  # No RangeIndex in dask
            pd.DataFrame(),
        ]

        for df in test_frames:
            ddf = dd.from_pandas(df, npartitions=4)
>           _assert_info(df, ddf)

dask/dataframe/tests/test_dataframe.py:3054:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

df =    x  y
0  1  1
1  2  0
2  3  1
3  4  0
ddf = Dask DataFrame Structure:
                   x      y
npartitions=3
0              int64  int64
1                ...    ...
2                ...    ...
3                ...    ...
Dask Name: from_pandas, 3 tasks
memory_usage = True

    def _assert_info(df, ddf, memory_usage=True):
        from io import StringIO

        assert isinstance(df, pd.DataFrame)
        assert isinstance(ddf, dd.DataFrame)

        buf_pd, buf_da = StringIO(), StringIO()

        df.info(buf=buf_pd, memory_usage=memory_usage)
        ddf.info(buf=buf_da, verbose=True, memory_usage=memory_usage)

        stdout_pd = buf_pd.getvalue()
        stdout_da = buf_da.getvalue()
        stdout_da = stdout_da.replace(str(type(ddf)), str(type(df)))
        # TODO
>       assert stdout_pd == stdout_da
E       assert "<class 'pand... 96.0 bytes\n" == "<class 'pand... 96.0 bytes\n"
E         Skipping 90 identical leading characters in diff, use -v to show
E           columns):
E         -  #   Column  Non-Null Count  Dtype
E         - ---  ------  --------------  -----
E         -  0   x       4 non-null      int64
E         ? -----     ---           -----
E         + x    4 non-null int64...
E
E         ...Full output truncated (6 lines hidden), use '-vv' to show

dask/dataframe/tests/test_dataframe.py:3037: AssertionError
============================================================================= slowest 10 test durations =============================================================================
0.02s call     dask/dataframe/tests/test_dataframe.py::test_info

(0.00 durations hidden.  Use -vv to show these durations.)
================================================================================= 1 failed in 1.05s =================================================================================

```

Also `dask/dataframe/tests/test_dataframe.py::test_groupby_multilevel_info`.

We can either adjust the test or (more preferably) update our info repr to match pandas' new one.