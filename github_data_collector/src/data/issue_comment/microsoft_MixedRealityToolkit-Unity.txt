## Overview

Brings over a few helper classes for XR SDK input on Windows Mixed Reality, preparing for the full device manager.

1. Add an extension for the Windows `SpatialInteractionSourceHandedness` enum to allow for easy conversion the the MRTK's enum.
1. Add the XR SDK implementation from https://github.com/microsoft/MixedRealityToolkit-Unity/pull/6989
1. Rename the XR SDK configuration profile to better fit our name scheme

## Changes
- Part of #7003

## Describe the bug

I would like to use any pointer (head gaze, eye gaze, hand rays) to target a hologram on HoloLens 2 and then when tapping down, I would like to start moving it. Ideally I want to reuse the same ManipulationHandler script, but while the movement behavior seems to be working well when using hand rays, it shows some weird depth movement when having hand rays turned off. 

## To reproduce
Steps to reproduce the behavior:
1. If you use the HandInteractionAllExample.unity scene and start it with the DefaultHololens2ConfigurationProfile, holograms can be targeted and moved using hand rays.

2. In comparison, if you switch the profile to EyeTrackingDemoConfigurationProfile, you can look and air-tap to select a hologram and then while pressing your index finger and thumb together move the hologram. The left/right/up/down movement seems ok, but when I move my hand to and away from my body, the grabbed target moves on an unexpected trajectory.

## Expected behavior
I would expect that when I move my hand toward my body that the currently targeted (no matter how I targeted it) is coming toward me. 

## Your setup (please complete the following information)
- Unity Version 2018.4.14f1
- MRTK Version  v2.2.0

## Target platform (please complete the following information)
- HoloLens 2

In reviewing the Boundary System code, i noticed the following in Initialize

``` c#
            if (ShowBoundaryWalls)
            {
                GetBoundaryWallVisualization();
            }
            if (ShowBoundaryWalls)
            {
                GetBoundaryCeilingVisualization();
            }
```

The second if is checking the wrong flag.

The boundary system reads the profile during initialization. If the BoundaryHeight property is changed, it should be resizing and repositioning walls and ceiling objects correctly.

Repro

- Configure the boundary profile to show walls and the ceiling
- Enter play mode
- From code, raise the BoundaryHeight from 3 (the default) to 10
- Notice the walls and ceiling are not resized/repositioned.

The solution to this is to

- Not use an autoproperty for BoundaryHeight
- On set, if the height has been changed
    - Reposition the ceiling object
    - Resize the wall objects
    - Reposition the wall objects so they remain grounded on the floor

## Overview
For custom applications, it would be very useful to define custom gestures to perform certain actions. Being able to add recognized gestures would open up a lot more interactivity in custom applications. 

## Expected behavior
I expect to be able to add a custom gesture and invoke events when it is recognized. I would also expect some tutorials or documentation on how to create custom gestures

## Actual behavior
There is a predefined list of hand gestures that cannot be added upon. The recognizer of gestures is blocked by dlls and inaccessible. 

## Unity editor version
2018.4.11f1+

## Mixed Reality Toolkit release version
MRTKv2.0


## Overview
This PR is following the issue I posted about the the InteractableRotationTheme behaviour. It replaces the usage of world rotation for the start value of the interpolation by local rotation, which seems to be more adapted.  

## Changes
- Fixes: #7080 


## Verification
> As a reviewer, it is possible to check out this change locally by using the following
> commands (substituting {PR_ID} with the ID of this pull request):
>
> git fetch origin pull/{PR_ID}/head:name_of_local_branch
>
> git checkout name_of_local_branch

## Describe the bug

_InteractableRotationTheme_ is using a world rotation for the interpolation's start value : 
```
/// <inheritdoc />
public override ThemePropertyValue GetProperty(ThemeStateProperty property)
{
    ThemePropertyValue start = new ThemePropertyValue();
    start.Vector3 = hostTransform.eulerAngles;
    return start;
}

/// <inheritdoc />
public override void SetValue(ThemeStateProperty property, int index, float percentage)
{
    Vector3 lerpTarget = property.Values[index].Vector3;

    bool relative = Properties[0].Value.Bool;
    if (relative)
    {
        lerpTarget = originalRotation + lerpTarget;
    }

    hostTransform.localRotation = Quaternion.Euler(Vector3.Lerp(property.StartValue.Vector3, lerpTarget, percentage));
}
```
Since the interpolation is setting the local rotation in the _SetValue_ function, this induces weird behaviour in some cases, for exemple when the parent transform  of the rotated object has a rotation different from Vector3.zero

By using `hostTransform.localEulerAngles` in the _GetProperty_ function, the issue seems to be fixed.

## To reproduce

Steps to reproduce the behavior:

1. Open the InteractableExamples scene
2. Set a random rotation to the object Model_Bucky (for example I used 12,80,67)
3. Press play and hover the object with your cursor to reach the focus state.

## Your setup (please complete the following information)

- Unity Version 2019.2.6f1
- MRTK Version 2.2

## Target platform (please complete the following information)

- HoloLens
- HoloLens 2


## Describe the problem
Design & layout is a big challenge in the editor since there is no way to figure out if an element is in a certain range around FoV until build & deploy to the device. It would be great to have a visual indicator that can communicate the FoV of devices such as HoloLens 2. 

## Describe the solution you'd like
Visual outline box or actual viewport size in the editor that matches the actual FoV of the target device.

Hi,

I want to have access to the depth camera streams with the HoloLens 2 as I was able to do with the HoloLens 1. With the HoloLens 1, I was able to call that method while with the HoloLens 2 I have the exception "no capture device available":

`            IReadOnlyList<MediaFrameSourceGroup> allFrameSourceGroups = await MediaFrameSourceGroup.FindAllAsync();`

I paid attention to activate both the Camera and the perceptionSensorsExperimental Capabilities: 

```
<?xml version="1.0" encoding="utf-8"?>
<Package xmlns:mp="http://schemas.microsoft.com/appx/2014/phone/manifest" xmlns:uap="http://schemas.microsoft.com/appx/manifest/uap/windows10" xmlns:uap2="http://schemas.microsoft.com/appx/manifest/uap/windows10/2" xmlns:uap3="http://schemas.microsoft.com/appx/manifest/uap/windows10/3" xmlns:uap4="http://schemas.microsoft.com/appx/manifest/uap/windows10/4" xmlns:iot="http://schemas.microsoft.com/appx/manifest/iot/windows10" xmlns:mobile="http://schemas.microsoft.com/appx/manifest/mobile/windows10" xmlns:rescap="http://schemas.microsoft.com/appx/manifest/foundation/windows10/restrictedcapabilities" IgnorableNamespaces="uap uap2 uap3 uap4 mp mobile iot rescap" xmlns="http://schemas.microsoft.com/appx/manifest/foundation/windows10">
  <Identity Name="SciVisHololens" Publisher="CN=DefaultCompany" Version="1.0.0.0" />
  <mp:PhoneIdentity PhoneProductId="4e761bd6-9819-4b93-943b-0d9037e8077d" PhonePublisherId="00000000-0000-0000-0000-000000000000" />
  <Properties>
    <DisplayName>SciVis_Hololens</DisplayName>
    <PublisherDisplayName>DefaultCompany</PublisherDisplayName>
    <Logo>Assets\StoreLogo.png</Logo>
  </Properties>
  <Dependencies>
    <TargetDeviceFamily Name="Windows.Holographic" MinVersion="10.0.17763.0" MaxVersionTested="10.0.18362.0" />
  </Dependencies>
  <Resources>
    <Resource Language="x-generate" />
  </Resources>
  <Applications>
    <Application Id="App" Executable="$targetnametoken$.exe" EntryPoint="SciVisHololens.App">
      <uap:VisualElements DisplayName="SciVis_Hololens" Square150x150Logo="Assets\Square150x150Logo.png" Square44x44Logo="Assets\Square44x44Logo.png" Description="SciVis_Hololens" BackgroundColor="transparent">
        <uap:DefaultTile ShortName="SciVis_Hololens" Wide310x150Logo="Assets\Wide310x150Logo.png" />
        <uap:SplashScreen Image="Assets\SplashScreen.png" BackgroundColor="#FFFFFF" />
        <uap:InitialRotationPreference>
          <uap:Rotation Preference="landscape" />
          <uap:Rotation Preference="landscapeFlipped" />
          <uap:Rotation Preference="portrait" />
          <uap:Rotation Preference="portraitFlipped" />
        </uap:InitialRotationPreference>
      </uap:VisualElements>
    </Application>
  </Applications>
  <Capabilities>
    <Capability Name="internetClient" />
    <uap2:Capability Name="spatialPerception" />
    <iot:Capability Name="lowLevelDevices" />
    <rescap:Capability Name="perceptionSensorsExperimental" />
    <DeviceCapability Name="webcam" />
    <DeviceCapability Name="microphone" />
    <DeviceCapability Name="humaninterfacedevice">
      <Device Id="any">
        <Function Type="usage:0001 0004" />
      </Device>
      <Device Id="any">
        <Function Type="usage:0001 0005" />
      </Device>
    </DeviceCapability>
  </Capabilities>
</Package>
```
My HoloLens 2 is up-to-date and I should at least be able to access the default RGB camera.
I compiled a managed assembly OUTSIDE Unity (for the piece of code handling cameras) in ARM 32, minimal version 15063. I wanted to compile with the 17134 version as the doc advices (regarding that function), but then I have a conflict about system.runtime versions between my managed assembly and Unityâ€¦

I both tried using Unity 2018 LTS and Unity 2019 (last version) to generate the IL2CPP code compiled using Unity. target: HoloLens ARM. MRTK version: v2.2.0
thank you!
## Describe the issue
It seems that ARFoundation is compatible with MRTK. So I wanted to use the [Object Tracker Subsystem](https://docs.unity3d.com/Packages/com.unity.xr.arsubsystems@3.1/manual/object-tracking.html) to recognize a real object and displays virtual info around it.

In the documentation of the [Object Tracker Susbsystem](https://docs.unity3d.com/Packages/com.unity.xr.arsubsystems@3.1/manual/object-tracking.html) it is written

> You will need to then populate the reference object entries with provider-specific assets. Refer to the provider's documentation for instructions on how to do this.

So I looked into the MRTK Documentation and found nothing.


## Feature area
The documentation is missing information about "How to Create a Reference Object Entries in UWP for using ARFoundation Object Tracker Subsystem"

## Existing doc link
The only things I found [are about iOS and Android](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/CrossPlatform/UsingARFoundation.html?q=arfoundation). But I would like to use the ARFoundation with UWP and HoloLens2.

## Additional context
Maybe I'm wrong and I can't use ARFoundation with HoloLens2 ? I did not even try to use the image tracker. Maybe this one does not even work. If ARFoundation doesn't work with HL2, sorry to believe that in the first place.
