item 38522 : token "0.065581" ohly has 24 dimensions in 'glove.twitter.27B.25d.txt'
It appears that in the vocab_count.c, the authors malloced some memory to store the word read from the corpus when they build the vocabulary, but they didn't free these memories in the end. I think this will lead to a memory leak issue right? Can anyone tell me that if I am right, and how to fix this issue? 
I encounter cost: -nan after a few iterations (from 5 to 7) on my own dataset of 39 Gb. The vectors are also all nans.

I have read [this issue](https://github.com/stanfordnlp/GloVe/issues/99), but decreasing learning rate doesn't solve my problem (I tried 0.05, 0.005, 0.0005 and 0.0001, the same results).

The cost is decreasing for a few steps, but then goes to NaN. Like this:
![glove](https://user-images.githubusercontent.com/17129987/67583699-ca9b4e00-f754-11e9-970c-946c7f58d3ee.png)

I have a really small vocabulary size of less than 5000 (that's intentional, I pretokenized my corpus this way) and a large vector size (tried 500 and 1000).

Cooccurrence matrix seems to be constructed fine and weights about 396 Mb. vocab file also looks good. 
And even resulted vectors after a few iterations (before -nan appears) seem good and do not completely fail on lexical similarity task.

However, I would like to continue training, since I am not sure that the model has converged. 

Please, give some advice about how to avoid -nan in cost and vectors. 

I installed glove with 
$ git clone http://github.com/stanfordnlp/glove
$ cd glove && make
if it matters.

`$ build/cooccur -memory 4.0 -vocab-file vocab.txt -verbose 2 -window-size 15 < ./train_data/data.txt > cooccurrence.bin
COUNTING COOCCURRENCES
window size: 15
context: symmetric
max product: 13752509
overflow length: 38028356
Reading vocab from file "vocab.txt"...loaded 7459765 words.
Building lookup table...table contains 215584655 elements.
Processed 7713292825 tokens.
Writing cooccurrences to disk............1141 files in total.
Merging cooccurrence files: processed 0 lines.Unable to open file overflow_1021.bin.`

when I train glove with big dataset (46G) , I met the problem, anyone knows why?
Hi, 
I just want some help in getting the **training vocabulary** & **word frequency** from the binary file (.bin) of embedding trained by word2vec or GloVe. I know that fastText offers such API like [`getwords()`](https://github.com/facebookresearch/fastText/blob/9479e4ecd565aa8f1e0da52303c876b5fb6fa0d1/python/fasttext_module/fasttext/FastText.py#L187), but I just don't know how to do the same thing on pretrained GloVe models. 
Thanks in advance!

I am not able to run the "vocab_count.c" file on codeblocks IDE ...which IDE I need to use to generate vectors for my own corpus?
please help 
This PR fixes two issues with NaNs during training.
First, the checks for NaNs in glove.c did not work with the compiler options from Makefile.
-Ofast disables checks for NaNs and +-Infs. Changed to -O3 by default, so that users at least see the error messages during training.
Second, default learning rate causes on some data exploding gradients, and, as a result, +-Infs and NaNs. People on the internet recommend lowering learning rate in such a case, but this solution is not perfect (users have to get NaNs in their embeddings, then to google for a solution, then train again).
Now, there is a gradient components clipping parameter with a reasonable default which allows training with arbitrary initial learning rates (including default) on arbitrary data out of the box, and no fiddling is required.
(base) tw-mbp-mjalal:glove mjalal$ cd GloVe-1.2 && make
mkdir -p build
gcc src/glove.c -o build/glove -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result
gcc src/shuffle.c -o build/shuffle -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result
gcc src/cooccur.c -o build/cooccur -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result
gcc src/vocab_count.c -o build/vocab_count -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result
(base) tw-mbp-mjalal:GloVe-1.2 mjalal$ ./demo.sh
mkdir -p build
gcc src/glove.c -o build/glove -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result
gcc src/shuffle.c -o build/shuffle -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result
gcc src/cooccur.c -o build/cooccur -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result
gcc src/vocab_count.c -o build/vocab_count -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result
--2019-07-03 15:03:15--  http://mattmahoney.net/dc/text8.zip
Resolving mattmahoney.net (mattmahoney.net)... 67.195.197.75
Connecting to mattmahoney.net (mattmahoney.net)|67.195.197.75|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 31344016 (30M) [application/zip]
Saving to: ‘text8.zip’

text8.zip                             100%[========================================================================>]  29.89M  1.25MB/s    in 24s     

2019-07-03 15:03:40 (1.23 MB/s) - ‘text8.zip’ saved [31344016/31344016]

Archive:  text8.zip
  inflating: text8                   
BUILDING VOCABULARY
Processed 17005207 tokens.
Counted 253854 unique words.
Truncating vocabulary at min count 5.
Using vocabulary of size 71290.

COUNTING COOCCURRENCES
window size: 15
context: symmetric
max product: 13752509
overflow length: 38028356
Reading vocab from file "vocab.txt"...loaded 71290 words.
Building lookup table...table contains 94990279 elements.
Processed 17005206 tokens.
Writing cooccurrences to disk.........2 files in total.
Merging cooccurrence files: processed 60666466 lines.

SHUFFLING COOCCURRENCES
array size: 255013683
Shuffling by chunks: processed 60666466 lines.
Wrote 1 temporary file(s).
Merging temp files: processed 60666466 lines.

TRAINING MODEL
Read 60666466 lines.
Initializing parameters...done.
vector size: 50
vocab size: 71290
x_max: 10.000000
alpha: 0.750000
iter: 001, cost: 0.068991
iter: 002, cost: 0.051679
iter: 003, cost: 0.046140
iter: 004, cost: 0.043019
iter: 005, cost: 0.041187
iter: 006, cost: 0.039974
iter: 007, cost: 0.039112
iter: 008, cost: 0.038458
iter: 009, cost: 0.037941
iter: 010, cost: 0.037520
iter: 011, cost: 0.037170
iter: 012, cost: 0.036879
iter: 013, cost: 0.036616
iter: 014, cost: 0.036395
iter: 015, cost: 0.036201
Traceback (most recent call last):
  File "eval/python/evaluate.py", line 110, in <module>
    main()
  File "eval/python/evaluate.py", line 22, in main
    vector_dim = len(vectors[ivocab[0]])
TypeError: object of type 'map' has no len()
(base) tw-mbp-mjalal:GloVe-1.2 mjalal$ 

In the original paper, best window size is 8, why the default window size here is 15?