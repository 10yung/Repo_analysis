I am loading my local kubeconfig which contains multiple clusters and contexts. The default context is "prod" and one of the config values I want to override is the `CurrentContext` 

```
	clientConfig := clientcmd.NewNonInteractiveDeferredLoadingClientConfig(
		&clientcmd.ClientConfigLoadingRules{ExplicitPath: "/Users/me/.kube/config"},
		&clientcmd.ConfigOverrides{
			CurrentContext: "stage",
		})

	rawConfig, _ := clientConfig.RawConfig()
	log.Printf(rawConfig.CurrentContext) // outputs "prod" instead of "stage"
```
When I inspect `RawConfig()` the current context is still "prod" instead of "stage".
Why does the config override not work?

Also how does the override for AuthInfo etc. work? The override accepts only a single `AuthInfo` whereas the configuration contains a map of `AuthInfo` etc.
**What happened**:
I wanted to test my method to scale deployments. But after changing the replicas by using `updateScale` I was not able to list deployments anymore.

**What you expected to happen**:
- create fake api
- create mock deployment
- scale it
- list it

**How to reproduce it (as minimally and precisely as possible)**:
- create fake api
- create a mock deployment
- scale the deployment
- list the deployment

**Anything else we need to know?**:
```
package scaling

import (
    "testing"

    v1api "k8s.io/api/apps/v1"
    autoscalingv1 "k8s.io/api/autoscaling/v1"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/client-go/kubernetes/fake"

    "github.com/stretchr/testify/require"
)

func TestScaling(t *testing.T) {
    initialReplicas := int32(3)
    cs := fake.NewSimpleClientset()
    _, err := cs.AppsV1().Deployments("test-namespace").Create(&v1api.Deployment{
        ObjectMeta: metav1.ObjectMeta{
            Name: "test-deployment",
            Labels: map[string]string{
                "app": "test-app",
            },
        },
        Spec: v1api.DeploymentSpec{
            Replicas: &initialReplicas,
        },
    })
    require.NoError(t, err)
    scale := &autoscalingv1.Scale{
        ObjectMeta: metav1.ObjectMeta{
            Name:      "test-deployment",
            Namespace: "test-namespace",
        },
        Spec: autoscalingv1.ScaleSpec{
            Replicas: 5,
        },
    }
    _, err = cs.AppsV1().Deployments("test-namespace").UpdateScale("test-deployment", scale)
    require.NoError(t, err)
    _, err = cs.AppsV1().Deployments("test-namespace").List(metav1.ListOptions{FieldSelector: "app=test-app"})
    require.NoError(t, err)
}
```
Run like this:
```
go test -run TestScaling ./scaling
```

This will lead to an error like the following:
```
item[0]: can't assign or convert v1.Scale into v1.Deployment
```

**Environment**:
- go version: `go1.13.5 linux/amd64`
- client-go version: `k8s.io/client-go@v0.16.4`
- related issue(s)/pr(s):
  - [80354](https://github.com/kubernetes/kubernetes/issues/80354) 
  - [410](https://github.com/kubernetes-sigs/controller-runtime/pull/410)
  - [579](https://github.com/kubernetes/client-go/issues/579)

[Istio](https://github.com/istio/istio) uses `SharedInformer` to set up watchers for resources and process them as events. [See here](https://github.com/istio/istio/blob/7f503255fa479f33c9bab9afc851716002610809/galley/pkg/config/source/kube/apiserver/watcher.go#L53-L94) for some of the core code involved. 

One problem we've run into is that if there is a permission error, the `SharedInformer` apparently does nothing to tell us. We get a log message, and that's it. We're [adding a timeout](https://github.com/istio/istio/pull/19890) as a workaround for our immediate issue, but it feels like a crude option.

Is there a proper way to handle errors (such as permission errors) that happen in the course of using a `SharedInformer`? It would be nice to be able to directly respond to specific kinds of errors (e.g. on permissions error, choose to fail immediately, on network error, choose to retry with timeout). 

If not... consider this a feature request. Thank you!
How to query pods that are not associated with a replica controller ? who can help me?
https://github.com/kubernetes/client-go/blob/1b1a35e41a57053c50fd15d9743d2ed2cc144e4b/tools/leaderelection/leaderelection.go#L153

The description of this field says: "If you set this to true...you may have two processes simultaneously acting on the critical path". However, my understanding is the opposite - if we set this field to true, then when the context is cancelled, the current leading process will update its own observedRecord so it won't consider itself as leader. Therefore we won't have two processes acting on the same critical path.

Conversely, if we set this field to false, then the current leading process won't update its observedRecord. Its leader elector will exit, and it will always think that itself is the leader. It won't ever check the leader election record again unless it restarts its leader elector. Another process will take over the lease. Now we have two processes both believing themselves as the leader.

Did I understand this correctly? 
*Issue*:

When running a remotecommand and the connection dies or is killed (e.g. by using `tcpkill`) the returned values [here](https://github.com/kubernetes/client-go/blob/20e59c6/tools/remotecommand/errorstream.go#L42) are `0` and `nil` since [Buffer.ReadFrom](https://github.com/golang/go/blob/0f897f9/src/bytes/buffer.go#L211) handles the `EOF` case and returns `nil`. These values do not meet any of the conditions so it ends up sending a `nil` error into the channel.

*Desired result*:

Maybe we should return an error. What happens now is that the stream dies but instead it appears that everything is OK.
I try to run the example codes out-of-configure with client-go@kubernetes-1.14.3，I try to use the command go build . to generate the app, but I find an error:
../../go/pkg/mod/k8s.io/client-go@v11.0.1-0.20190606204521-b8faab9c5193+incompatible/rest/request.go:598:31: 
 not enough arguments in call to watch.NewStreamWatcher
        have (*versioned.Decoder)
        want (watch.Decoder, watch.Reporter)
![2019-12-08 16-32-00 的屏幕截图](https://user-images.githubusercontent.com/34976119/70386818-7b457000-19d8-11ea-9778-7af2f9124697.png)


We have a custom controller for a CRD.  The controller updates status sub-resource in each sync loop. In real world, it works fine. But when unit test with fake clientset, the update causes informer to trigger resource updated function and lead to another sync loop. As result, the sync never stops in unit test. 

How real k8s client break the loop in such scenario?  Anything we can do to solve the problem?
I have been through several examples of connection (e.g. [this](https://github.com/kubernetes/client-go/blob/master/examples/out-of-cluster-client-configuration/main.go)) and all of them seem to include usage of the config file. 
If I am trying to connect to a Kubernetes cluster from OUTSIDE of the cluster, how to proceed? I was looking into having service accounts and roles (e.g. [here](https://kubernetes.io/docs/tasks/administer-cluster/access-cluster-api/#without-using-a-proxy)). But is there any example where one can do it using the go client? 
Thanks.
This is related to this issue described here: https://github.com/terraform-providers/terraform-provider-helm/issues/299#issuecomment-560153039

Helm [initializes](https://github.com/helm/helm/blob/865c46c014cdb7622f97ae287ee92fb8a280f3a9/pkg/action/action.go#L227) a ClientSet using the RESTClientGetter (in order to manage its storage driver functionality.
This same implementation is used in the new version of the [helm terraform provider](https://github.com/aaronmell/terraform-provider-helm/blob/dev-v3/helm/provider.go#L382) where there's a flag controlling if it should or should not load the local kubeconfig file. The problem is that even if we set the KubeConfig field to `nil` on the ConfigFlags instance it still loads the default config file right here: https://github.com/kubernetes/client-go/blob/8d0e6f1b7b780ef8c8f81800d504509cd1f68e9f/tools/clientcmd/client_config.go#L447 and there seems to be no way to prevent that from happening at the moment, at least at the level of the ConfigFlags struct.

I'm just wondering if this is a bug or if it was intentionally implemented this way and if that's the case, what would be the ideal solution for instructing the client to not load the local configuration. The only thing I was able to come up was to set the KubeConfig to an empty file's path.

Hoping we can figure something out here. Thank you!