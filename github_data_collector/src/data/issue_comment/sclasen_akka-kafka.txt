We get an occasional NullPointerException in the BatchConnectorFSM, and then all consuming halts. There is no obvious way to setup a supervisor to restart?

```
[ERROR] [2017-01-11 04:51:32,383] [OneForOneStrategy] [loader-akka.actor.default-dispatcher-90] [] null
java.lang.NullPointerException: null
        at org.I0Itec.zkclient.ZkConnection.writeDataReturnStat(ZkConnection.java:138)
        at org.I0Itec.zkclient.ZkClient$13.call(ZkClient.java:1151)
        at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:990)
        at org.I0Itec.zkclient.ZkClient.writeDataReturnStat(ZkClient.java:1147)
        at org.I0Itec.zkclient.ZkClient.writeData(ZkClient.java:1142)
        at org.I0Itec.zkclient.ZkClient.writeData(ZkClient.java:1110)
        at kafka.utils.ZkUtils$.updatePersistentPath(ZkUtils.scala:326)
        at kafka.consumer.ZookeeperConsumerConnector.commitOffsetToZooKeeper(ZookeeperConsumerConnector.scala:283)
        at kafka.consumer.ZookeeperConsumerConnector$$anonfun$5.apply(ZookeeperConsumerConnector.scala:304)
        at kafka.consumer.ZookeeperConsumerConnector$$anonfun$5.apply(ZookeeperConsumerConnector.scala:303)
        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
        at kafka.consumer.ZookeeperConsumerConnector.commitOffsets(ZookeeperConsumerConnector.scala:303)
        at kafka.consumer.ZookeeperConsumerConnector.commitOffsets(ZookeeperConsumerConnector.scala:380)
        at com.sclasen.akka.kafka.BatchConnectorFSM$$anonfun$4.applyOrElse(BatchActors.scala:142)
        at com.sclasen.akka.kafka.BatchConnectorFSM$$anonfun$4.applyOrElse(BatchActors.scala:137)
        at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
        at akka.actor.FSM$class.processEvent(FSM.scala:663)
        at com.sclasen.akka.kafka.BatchConnectorFSM.processEvent(BatchActors.scala:62)
        at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657)
        at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651)
        at akka.actor.Actor$class.aroundReceive(Actor.scala:484)
        at com.sclasen.akka.kafka.BatchConnectorFSM.aroundReceive(BatchActors.scala:62)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
        at akka.actor.ActorCell.invoke(ActorCell.scala:495)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
        at akka.dispatch.Mailbox.run(Mailbox.scala:224)
        at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)```
Pardon me for English is not my primary language. 

We have size limitation on the messages so that StreamFSM.hasNext() will receive a MessageSizeTooLargeException when encountering a too large message. The default SupervisorStrategy behavior is simply restart the process. Since nothing changed the process gets stuck with this ill message forever.

We'd like to recover this error by moving offset to next so that we can skip this message and restart the process. However we could not find a way to set SupervisorStrategy for the framework and change the default behavior. I am wondering if there is a way for us to do so?
Simple fix of a typo (`at=recieve-timeout`) in batch actor.

I'm using the 0.2.1 version from Sonatype (resolvers += "Sonatype OSS" at "https://oss.sonatype.org/content/repositories/releases"), and getting some weird errors. 

However when I build my own 0.2.2-SNAPSHOT version everything works find. Looking the at the commit history, nothing has changed between the two versions. 

Any idea what's wrong with the Sonatype build? 

Would it make sense to allow passing a Map of <Topic, Stream #> to create a consumer for multiple topics? 
It's currently possible to do that by passing a TopicFilter, but it would be nice to have controlle over the number of streams for each topic

We're seeing the following message in our logs:

`[WARN] [2016-05-05 17:40:35,793] [EventService-akka.actor.default-dispatcher-12] c.s.a.k.StreamFSM akka://EventService/user/$b/stream0: unhandled event StartProcessing in state Draining`

Is this an error we should be concerned about?  How should we go about debugging?

Thanks in advance!

There are many stream consumer actors created (FSMStreams), but only one worker actor - this essentially turns any non-trivial worker into a bottle neck - would it make sense to have a consumer actor per stream? 

Is there any plan to make this work with the .9 consumer? I know the architecture of that consumer is much different. 

I would like to have a common base for akka consumers... to allow for a generic way to process clean-up 

ex:
 `private[this] val consumerRegistry = new ConcurrentHashMap[String, AkkaConsumerBase[String, _]]`
 `

``` scala
def shutdown() = {
    logger.info("Shutting down Event Bus")
    producer.close()
    Await.ready(Future.sequence(consumerRegistry.map(e => e._2.stop())), 5 minute)
  }
```

Thoughts ?

@coreyauger check out the last commit here
