Hello, I am currently trying to run custom models on the Nvidia Xavier using this jetson-inference repo. I have gone through the tutorials using the prebuilt networks just fine but when I try to use custom models I run into problems.

### Pretrained MobileNetV2 model from Torchvision

I converted it to ONNX format using [this](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html)

`Exported model has been tested with ONNXRuntime, and the result looks good!
`

but fails the TensorRT onnx test:
```
/usr/src/tensorrt/bin/trtexec --onnx=onnx_models/MobileNetV2.onnx 
&&&& RUNNING TensorRT.trtexec # /usr/src/tensorrt/bin/trtexec --onnx=onnx_models/MobileNetV2.onnx
[01/17/2020-11:34:07] [I] === Model Options ===
[01/17/2020-11:34:07] [I] Format: ONNX
[01/17/2020-11:34:07] [I] Model: onnx_models/MobileNetV2.onnx
[01/17/2020-11:34:07] [I] Output:
[01/17/2020-11:34:07] [I] === Build Options ===
[01/17/2020-11:34:07] [I] Max batch: 1
[01/17/2020-11:34:07] [I] Workspace: 16 MB
[01/17/2020-11:34:07] [I] minTiming: 1
[01/17/2020-11:34:07] [I] avgTiming: 8
[01/17/2020-11:34:07] [I] Precision: FP32
[01/17/2020-11:34:07] [I] Calibration: 
[01/17/2020-11:34:07] [I] Safe mode: Disabled
[01/17/2020-11:34:07] [I] Save engine: 
[01/17/2020-11:34:07] [I] Load engine: 
[01/17/2020-11:34:07] [I] Inputs format: fp32:CHW
[01/17/2020-11:34:07] [I] Outputs format: fp32:CHW
[01/17/2020-11:34:07] [I] Input build shapes: model
[01/17/2020-11:34:07] [I] === System Options ===
[01/17/2020-11:34:07] [I] Device: 0
[01/17/2020-11:34:07] [I] DLACore: 
[01/17/2020-11:34:07] [I] Plugins:
[01/17/2020-11:34:07] [I] === Inference Options ===
[01/17/2020-11:34:07] [I] Batch: 1
[01/17/2020-11:34:07] [I] Iterations: 10
[01/17/2020-11:34:07] [I] Duration: 3s (+ 200ms warm up)
[01/17/2020-11:34:07] [I] Sleep time: 0ms
[01/17/2020-11:34:07] [I] Streams: 1
[01/17/2020-11:34:07] [I] ExposeDMA: Disabled
[01/17/2020-11:34:07] [I] Spin-wait: Disabled
[01/17/2020-11:34:07] [I] Multithreading: Disabled
[01/17/2020-11:34:07] [I] CUDA Graph: Disabled
[01/17/2020-11:34:07] [I] Skip inference: Disabled
[01/17/2020-11:34:07] [I] Input inference shapes: model
[01/17/2020-11:34:07] [I] Inputs:
[01/17/2020-11:34:07] [I] === Reporting Options ===
[01/17/2020-11:34:07] [I] Verbose: Disabled
[01/17/2020-11:34:07] [I] Averages: 10 inferences
[01/17/2020-11:34:07] [I] Percentile: 99
[01/17/2020-11:34:07] [I] Dump output: Disabled
[01/17/2020-11:34:07] [I] Profile: Disabled
[01/17/2020-11:34:07] [I] Export timing to JSON file: 
[01/17/2020-11:34:07] [I] Export output to JSON file: 
[01/17/2020-11:34:07] [I] Export profile to JSON file: 
[01/17/2020-11:34:07] [I] 
----------------------------------------------------------------
Input filename:   onnx_models/MobileNetV2.onnx
ONNX IR version:  0.0.4
Opset version:    10
Producer name:    pytorch
Producer version: 1.2
Domain:           
Model version:    0
Doc string:       
----------------------------------------------------------------
[01/17/2020-11:34:08] [W] [TRT] Calling isShapeTensor before the entire network is constructed may result in an inaccurate result.
[01/17/2020-11:34:08] [E] [TRT] Network has dynamic or shape inputs, but no optimization profile has been defined.
[01/17/2020-11:34:08] [E] [TRT] Network validation failed.
[01/17/2020-11:34:08] [E] Engine creation failed
[01/17/2020-11:34:08] [E] Engine set up failed
&&&& FAILED TensorRT.trtexec # /usr/src/tensorrt/bin/trtexec --onnx=onnx_models/MobileNetV2.onnx
```

I have tried PyTorch 1.1 (which fails at converting model to ONNX due to dynamic_axes label), PyTorch 1.2 and 1.3


### Pre-trained MobileNetV2 

model found [here](https://github.com/onnx/models/tree/master/vision/classification/mobilenet)

Passes the trtexec:

`&&&& PASSED TensorRT.trtexec # /usr/src/tensorrt/bin/trtexec --onnx=onnx_models/mobilenetv2-1.0.onnx
`

but runs into an error when loading model with jerson-inference:
```
./imagenet-console.py airplane_0.jpg output_0.jpg --model=mobilenetv2-1.0.onnx 
jetson.inference.__init__.py
jetson.inference -- initializing Python 2.7 bindings...
jetson.inference -- registering module types...
jetson.inference -- done registering module types
jetson.inference -- done Python 2.7 binding initialization
jetson.utils.__init__.py
jetson.utils -- initializing Python 2.7 bindings...
jetson.utils -- registering module functions...
jetson.utils -- done registering module functions
jetson.utils -- registering module types...
jetson.utils -- done registering module types
jetson.utils -- done Python 2.7 binding initialization
[image] loaded 'airplane_0.jpg'  (500 x 375, 3 channels)
jetson.inference -- PyTensorNet_New()
jetson.inference -- PyImageNet_Init()
jetson.inference -- imageNet loading network using argv command line params
jetson.inference -- imageNet.__init__() argv[0] = './imagenet-console.py'
jetson.inference -- imageNet.__init__() argv[1] = 'airplane_0.jpg'
jetson.inference -- imageNet.__init__() argv[2] = 'output_0.jpg'
jetson.inference -- imageNet.__init__() argv[3] = '--model=mobilenetv2-1.0.onnx'
[TRT]   imageNet -- failed to initialize.
jetson.inference -- imageNet failed to load built-in network 'googlenet'
PyTensorNet_Dealloc()
Traceback (most recent call last):
  File "./imagenet-console.py", line 49, in <module>
    net = jetson.inference.imageNet(opt.network, sys.argv)
Exception: jetson.inference -- imageNet failed to load network
jetson.utils -- freeing CUDA mapped memory

```

### Pretrained MobileNetV2 Caffe Model
model found [here](https://github.com/zhanghanbin3159/MobileNetV2-SSD)

```
./imagenet-console.py airplane_0.jpg output_0.jpg --prototxt= MobileNet_deploy.prototxt \ 
--model=MobileNetSSD_deploy.caffemodel \ 
--labels=labels_map.txt \
--input_blob=data \
--output_blob=softmax
jetson.inference.__init__.py
jetson.inference -- initializing Python 2.7 bindings...
jetson.inference -- registering module types...
jetson.inference -- done registering module types
jetson.inference -- done Python 2.7 binding initialization
jetson.utils.__init__.py
jetson.utils -- initializing Python 2.7 bindings...
jetson.utils -- registering module functions...
jetson.utils -- done registering module functions
jetson.utils -- registering module types...
jetson.utils -- done registering module types
jetson.utils -- done Python 2.7 binding initialization
[image] loaded 'airplane_0.jpg'  (500 x 375, 3 channels)
jetson.inference -- PyTensorNet_New()
jetson.inference -- PyImageNet_Init()
jetson.inference -- imageNet loading network using argv command line params
jetson.inference -- imageNet.__init__() argv[0] = './imagenet-console.py'
jetson.inference -- imageNet.__init__() argv[1] = 'airplane_0.jpg'
jetson.inference -- imageNet.__init__() argv[2] = 'output_0.jpg'
jetson.inference -- imageNet.__init__() argv[3] = '--prototxt='
jetson.inference -- imageNet.__init__() argv[4] = 'MobileNet_deploy.prototxt'
jetson.inference -- imageNet.__init__() argv[5] = '--model=MobileNetSSD_deploy.caffemodel'
jetson.inference -- imageNet.__init__() argv[6] = '--labels=labels_map.txt'
jetson.inference -- imageNet.__init__() argv[7] = '--input_blob=data'
jetson.inference -- imageNet.__init__() argv[8] = '--output_blob=softmax'

imageNet -- loading classification network model from:
         -- prototxt     
         -- model        MobileNetSSD_deploy.caffemodel
         -- class_labels labels_map.txt
         -- input_blob   'data'
         -- output_blob  'softmax'
         -- batch_size   1

[TRT]   TensorRT version 6.0.1
[TRT]   loading NVIDIA plugins...
[TRT]   Plugin Creator registration succeeded - GridAnchor_TRT
[TRT]   Plugin Creator registration succeeded - GridAnchorRect_TRT
[TRT]   Plugin Creator registration succeeded - NMS_TRT
[TRT]   Plugin Creator registration succeeded - Reorg_TRT
[TRT]   Plugin Creator registration succeeded - Region_TRT
[TRT]   Plugin Creator registration succeeded - Clip_TRT
[TRT]   Plugin Creator registration succeeded - LReLU_TRT
[TRT]   Plugin Creator registration succeeded - PriorBox_TRT
[TRT]   Plugin Creator registration succeeded - Normalize_TRT
[TRT]   Plugin Creator registration succeeded - RPROI_TRT
[TRT]   Plugin Creator registration succeeded - BatchedNMS_TRT
[TRT]   Could not register plugin creator:  FlattenConcat_TRT in namespace: 
[TRT]   completed loading NVIDIA plugins.
[TRT]   detected model format - caffe  (extension '.caffemodel')
[TRT]   desired precision specified for GPU: FASTEST
[TRT]   requested fasted precision for device GPU without providing valid calibrator, disabling INT8
[TRT]   native precisions detected for GPU:  FP32, FP16, INT8
[TRT]   selecting fastest native precision for GPU:  FP16
[TRT]   attempting to open engine cache file MobileNetSSD_deploy.caffemodel.1.1.GPU.FP16.engine
[TRT]   cache file not found, profiling network model on device GPU
[TRT]   device GPU, loading /usr/bin/ MobileNetSSD_deploy.caffemodel
[TRT]   failed to retrieve tensor for Output "softmax"
Segmentation fault (core dumped)
```
### Another Caffe Model
found [here](https://github.com/zhanghanbin3159/MobileNetV2-SSD)

```
./imagenet-console.py airplane_0.jpg output_0.jpg --prototxt=MobileNet-Caffe/mobilenet_v2_deploy.prototxt --model=MobileNet-Caffe/mobilenet_v2.caffemodel --labels=labels_map.txt --input_blob=data --output_blob=softmax
jetson.inference.__init__.py
jetson.inference -- initializing Python 2.7 bindings...
jetson.inference -- registering module types...
jetson.inference -- done registering module types
jetson.inference -- done Python 2.7 binding initialization
jetson.utils.__init__.py
jetson.utils -- initializing Python 2.7 bindings...
jetson.utils -- registering module functions...
jetson.utils -- done registering module functions
jetson.utils -- registering module types...
jetson.utils -- done registering module types
jetson.utils -- done Python 2.7 binding initialization
[image] loaded 'airplane_0.jpg'  (500 x 375, 3 channels)
jetson.inference -- PyTensorNet_New()
jetson.inference -- PyImageNet_Init()
jetson.inference -- imageNet loading network using argv command line params
jetson.inference -- imageNet.__init__() argv[0] = './imagenet-console.py'
jetson.inference -- imageNet.__init__() argv[1] = 'airplane_0.jpg'
jetson.inference -- imageNet.__init__() argv[2] = 'output_0.jpg'
jetson.inference -- imageNet.__init__() argv[3] = '--prototxt=MobileNet-Caffe/mobilenet_v2_deploy.prototxt'
jetson.inference -- imageNet.__init__() argv[4] = '--model=MobileNet-Caffe/mobilenet_v2.caffemodel'
jetson.inference -- imageNet.__init__() argv[5] = '--labels=labels_map.txt'
jetson.inference -- imageNet.__init__() argv[6] = '--input_blob=data'
jetson.inference -- imageNet.__init__() argv[7] = '--output_blob=softmax'

imageNet -- loading classification network model from:
         -- prototxt     MobileNet-Caffe/mobilenet_v2_deploy.prototxt
         -- model        MobileNet-Caffe/mobilenet_v2.caffemodel
         -- class_labels labels_map.txt
         -- input_blob   'data'
         -- output_blob  'softmax'
         -- batch_size   1

[TRT]   TensorRT version 6.0.1
[TRT]   loading NVIDIA plugins...
[TRT]   Plugin Creator registration succeeded - GridAnchor_TRT
[TRT]   Plugin Creator registration succeeded - GridAnchorRect_TRT
[TRT]   Plugin Creator registration succeeded - NMS_TRT
[TRT]   Plugin Creator registration succeeded - Reorg_TRT
[TRT]   Plugin Creator registration succeeded - Region_TRT
[TRT]   Plugin Creator registration succeeded - Clip_TRT
[TRT]   Plugin Creator registration succeeded - LReLU_TRT
[TRT]   Plugin Creator registration succeeded - PriorBox_TRT
[TRT]   Plugin Creator registration succeeded - Normalize_TRT
[TRT]   Plugin Creator registration succeeded - RPROI_TRT
[TRT]   Plugin Creator registration succeeded - BatchedNMS_TRT
[TRT]   Could not register plugin creator:  FlattenConcat_TRT in namespace: 
[TRT]   completed loading NVIDIA plugins.
[TRT]   detected model format - caffe  (extension '.caffemodel')
[TRT]   desired precision specified for GPU: FASTEST
[TRT]   requested fasted precision for device GPU without providing valid calibrator, disabling INT8
[TRT]   native precisions detected for GPU:  FP32, FP16, INT8
[TRT]   selecting fastest native precision for GPU:  FP16
[TRT]   attempting to open engine cache file MobileNet-Caffe/mobilenet_v2.caffemodel.1.1.GPU.FP16.engine
[TRT]   cache file not found, profiling network model on device GPU
[TRT]   device GPU, loading MobileNet-Caffe/mobilenet_v2_deploy.prototxt MobileNet-Caffe/mobilenet_v2.caffemodel
Error: Weight 99977.8 is outside of [65504, -65504].
Error: Weight 99977.8 is outside of [65504, -65504].
Error: Weight 98641.7 is outside of [65504, -65504].
Error: Weight 68991.2 is outside of [65504, -65504].
Error: Weight 99977.8 is outside of [65504, -65504].
Error: Weight 99977.8 is outside of [65504, -65504].
Error: Weight 99977.8 is outside of [65504, -65504].
Error: Weight 99977.8 is outside of [65504, -65504].
Error: Weight 99977.8 is outside of [65504, -65504].
Error: Weight 99977.8 is outside of [65504, -65504].
Error: Weight 99977.8 is outside of [65504, -65504].
Error: Weight 99977.8 is outside of [65504, -65504].
Error: Weight 99977.8 is outside of [65504, -65504].
Error: Weight 99977.8 is outside of [65504, -65504].
Error: Weight 99977.8 is outside of [65504, -65504].
Error: Weight 99977.8 is outside of [65504, -65504].
Error: Weight 99977.8 is outside of [65504, -65504].
[TRT]   device GPU, failed to parse caffe network
[TRT]   device GPU, failed to load MobileNet-Caffe/mobilenet_v2.caffemodel
[TRT]   failed to load MobileNet-Caffe/mobilenet_v2.caffemodel
[TRT]   imageNet -- failed to initialize.
jetson.inference -- imageNet failed to load built-in network 'googlenet'
PyTensorNet_Dealloc()
Traceback (most recent call last):
  File "./imagenet-console.py", line 49, in <module>
    net = jetson.inference.imageNet(opt.network, sys.argv)
Exception: jetson.inference -- imageNet failed to load network
jetson.utils -- freeing CUDA mapped memory

```

<hr>

I know jetson-inference supplies a prebuilt MobileNetV2 + SSD but I am trying to get a custom mobilenetV2 model (personally trained) to work but decided to see if I could get the original to work first (above models).

Ideally I would like to be able to get my custom PyTorch models converted to ONNX (or native PyTorch support?) and run them on the Xavier.

Thank you for any and all help.
@dusty-nv ,

I bought my jetson nano board.

Could you please explain what is the difference between Jetson inference SDK object detection code vs Deepstream SDk4.0.2 object detection code?

when should we go for what?


highly appreciated your explanation.
I followd [this](https://github.com/dusty-nv/jetson-inference/blob/master/docs/imagenet-console-2.md) repo and run the demo of imageNet successfully. But I'm wondering how speed-up does TensorRT do. Is there any way to DISABLE tensorRT on this example, if not, how to run without tensorRT and compare the speed-up of tensorRT on image classification. thanks.
Addresses issue #473
- Add binding to another overload of segNet::Mask
- Use integer division in segnet-camera.py

Usage of the new python binding:

```python
# classes is an uint8 array
classes = jetson.utils.cudaAllocMapped(width * height)
net.MaskClass(classes, width, height)
# cudaToNumpy assumes float32. Need to dividie output length by 4.
mask = jetson.utils.cudaToNumpy(classes, 1, width * height // 4, 1)[:, 0, 0]
# convert resulting float32 view to uint8 view
mask_np = mask.view('uint8').reshape(width, height)
```
Look like the demo cannot be run with tensorRT 6.0.1. Tried detectnet-camera not working.
Hey there,

during inference stdout is flooded with thouds of messages like this:

```
jetson.utils -- cudaFromNumpy()  ndarray dim 0 = 480                                     
jetson.utils -- cudaFromNumpy()  ndarray dim 1 = 640
jetson.utils -- cudaFromNumpy()  ndarray dim 2 = 4
jetson.utils -- freeing CUDA mapped memory                                         
jetson.inference -- PyDetection_Dealloc()
jetson.inference -- PyDetection_Dealloc()                        
jetson.utils -- cudaFromNumpy()  ndarray dim 0 = 480
jetson.utils -- cudaFromNumpy()  ndarray dim 1 = 640        
jetson.utils -- cudaFromNumpy()  ndarray dim 2 = 4  
jetson.utils -- freeing CUDA mapped memory                                   
jetson.inference -- PyDetection_Dealloc() 
jetson.inference -- PyDetection_Dealloc()
jetson.inference -- PyDetection_Dealloc()
jetson.utils -- cudaFromNumpy()  ndarray dim 0 = 480
jetson.utils -- cudaFromNumpy()  ndarray dim 1 = 640   
```


I tried 

```python
import logging
logging.basicConfig(level=logging.ERROR)
```

But this did not help ..
There are 2 methods in segNet.hï¼Œshow below:
![image](https://user-images.githubusercontent.com/27957744/72057988-519aa400-330a-11ea-9a5d-7c2e5acfe498.png)
I try to used the first method in python,  but failed. I want to know how to input the first parameter to distinguish the two methods in python?
When extracting the model :
`tar -zxvf ResNet-18.tar.gz`
There comes an error
```
ResNet-18
ResNet-18/train. prototxt 
ResNet-18/ResNet mean. binaryproto 
ResNet-18/solver. prototxt 
ResNet-18/deploy. prototxt 
ResNet-18/ResNet-18. caffemodel 
tar:Skipping to next header 

gzip:stdin:unexpected end of file 
tar:child returned status 1
tar:Error is not recoverable:exiting now
```
I was successfully able to build and run to install tensorflow after following this guide https://medium.com/repro-repo/speed-up-learning-by-building-tensorflow-gpu-from-source-on-ubuntu-d03bb4e06b23 .

Target //tensorflow/tools/pip_package:build_pip_package up-to-date:
bazel-bin/tensorflow/tools/pip_package/build_pip_package

INFO: Elapsed time: 3517.322s, Critical Path: 250.75s
INFO: 9346 processes: 9346 local.
INFO: Build completed successfully, 9500 total actions

I ran these 2 commands to verify the installation
**python -c "import tensorflow as tf; print(tf.version)"**
/home/path/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
_np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/path/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
_np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/path/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
_np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/path/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
_np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/path/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
_np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/path/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
np_resource = np.dtype([("resource", np.ubyte, 1)])
1.12.0

**python -c "import tensorflow as tf; print(tf.contrib.eager.num_gpus())"**
/home/path/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
_np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/path/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
_np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/path/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
_np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/path/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
_np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/path/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
_np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/path/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
np_resource = np.dtype([("resource", np.ubyte, 1)])
2019-12-25 18:15:33.357674: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-12-25 18:15:33.416894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-25 18:15:33.417115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:
name: GeForce GTX 1050 major: 6 minor: 1 memoryClockRate(GHz): 1.455
pciBusID: 0000:01:00.0
totalMemory: 1.95GiB freeMemory: 1.65GiB
2019-12-25 18:15:33.417129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-12-25 18:15:33.634565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-25 18:15:33.634593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988] 0
2019-12-25 18:15:33.634599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0: N
2019-12-25 18:15:33.634702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1397 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)
1

But still my digits says Tensorflow support disabled. Can anyone please guide me through this?

Thanks
Getting this error since latest Jetson Nano SD image came out. It matters not which network you attempt to use. Maybe something changed the directory structure?