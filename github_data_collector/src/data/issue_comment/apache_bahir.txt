
As discussed in the JIRA ticket (https://issues.apache.org/jira/browse/BAHIR-183), revisited version of HDFS persistence of MQTT messages, which should ease recovery in case of device failure. The design discussion may be continued in JIRA, but I decided to open WIP PR for @yanlin-Lynn to review and comment.
JIRA ticket: https://issues.apache.org/jira/browse/BAHIR-60

Sample output:
```
$ ./merge-pr.sh 3
Commits to be merged:
   8b307bd [BAHIR-1] Corner fix
   540f478 [BAHIR-1] Important feature
Maybe ask contributor to squash commits.
Continue "as is"? [y/n] y
Close pull request #3? [y/n] y
Commits pushed to origin/master.
Approver JIRA user: root
Approver JIRA password:
Specify assignee of JIRA ticket BAHIR-1 (current: root):
   0: root
   1: other
Chosen option: 1
Enter JIRA account name: lukasz
Specify fixed release version:
   0: 1.0.1
Chosen option: 0
JIRA ticket successfully updated.
```
If you comment out lines pushing changes to remote repository (line 125), clean-up of working directory (line 64), and two REST requests updating JIRA ticket (lines 182-205); you can perform a dry run to make sure the script works well.

Comments welcome. Improvement ideas?
This component implements Hadoop File System (org.apache,hadoop.fs.FileSystem) to provide an alternate mechanism (instead of using 'webhdfs or swebhdfs' file uri) for Spark to access (read/write) files from/to a remote Hadoop cluster using webhdfs protocol. 

This component takes care of the following requirements related to accessing files (read/write) from/to a remote enterprise Hadoop cluster from a remote Spark cluster-

1. Support for Apache Knox
2. Support for passing user id/password different from the user who has started the spark-shell/spark-submit process.
3. Support for SSL in three modes - Ignoring certificate validation, certificate validation through user supplied trust store path and password, and automatic creation of certificate using openssl and keytool.
4. Optimized way of getting data from remote HDFS where each connection will get only its part of data.


This component is not a full fledged implementation of Hadoop File System. It implements only those interfaces those are needed by Spark for reading data form remote HDFS and writing back the data to remote HDFS.

**Example Usage -**

Step 1: Set Hadoop configuration to define a custom uri of your choice and specify the class name BahirWebHdfsFileSystem. For example - 
`sc.hadoopConfiguration.set("fs.remoteHdfs.impl","org.apache.bahir.datasource.webhdfs.BahirWebHdfsFileSystem")`.
You can use any name (apart form the standard uris like hdfs, webhdfs, file etc. already used by Spark) instead of 'remoteHdfs'. However subsequently while loading the file (or writing a file) the same should be used.

Step 2: Set the user name and password as below -

`val userid = "biadmin"`
`val password = "password"`
`val userCred = userid + ":" + password`
`sc.hadoopConfiguration.set("usrCredStr",userCred)`

Step 3 : Now you are ready to load any file from the remote Hadoop cluster using Spark's standard Dataframe/DataSet APIs. For example -

`val filePath = "biginsights/spark-enablement/datasets/NewYorkCity311Service/311_Service_Requests_from_2010_to_Present.csv"`
`val srvr = "ehaasp-577-mastermanager.bi.services.bluemix.net:8443/gateway/default/webhdfs/v1"`
`val knoxPath = "gateway/default"`
`val webhdfsPath = "webhdfs/v1"`
`val prtcl = "remoteHdfs"`
`val fullPath = s"$prtcl://$srvr/$knoxPath/$webhdfsPath/$filePath"`

`val df = spark.read.format("csv").option("header", "true").load(fullPath)`

Please not the use of 'gateway/default' and 'webhdfs/v1' used for specifying the server specific information in the path. The first one is specific to Apache Knox and the second one is specific for  webhdfs protocol.

Step 4; To write data back to remote HDFS following steps can be used (using standard Dataframe writer of spark)

`val filePathWrite = "biginsights/spark-enablement/datasets/NewYorkCity311Service/Result.csv"`
`val srvr = "ehaasp-577-mastermanager.bi.services.bluemix.net:8443"`
`val knoxPath = "gateway/default"`
`val webhdfsPath = "webhdfs/v1"`
`val prtcl = "remoteHdfs"`
`val fullPath = s"$prtcl://$srvr/$knoxPath/$webhdfsPath/$filePathWrite"`

`df.write.format("csv").option("header", "true").save(filePathw)`

**We are still working on followings -**

- Unit Testing
- Code cleanup
- Examples showcasing various configuration parameters
- API documentation

