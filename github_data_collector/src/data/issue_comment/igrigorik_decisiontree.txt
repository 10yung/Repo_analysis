There is already one similar open [issue](https://github.com/igrigorik/decisiontree/issues/14).
Consider a training set as"
```
[
        ["myself", 0, "#{my_name}"],
        ["friend", 1, "#{friend_name}"],
        ["family", 1, "#{member_name}"],
        ["other", 1, "#{other_name}"]
]
```
If all methods(my_name, friend_name etc) return same name, decision tree is breaking.
In your file,
the first line is "Age,Education,Income,Marital Status". You should change it to  "Age,Education,Income,Marital,Status"
Add MIT license
notice you forgot to made a word adverb and suggested a better word than "look at"
I'm try run examples/simple.rb in master but throw this error:

```
entropy': undefined method `sum' for [2, 3, 1]:Array (NoMethodError)
```
Try gem version 0.5.0 and work find, maybe it introduced in pull request [#32](https://github.com/igrigorik/decisiontree/commit/846448acd635e192828da64eb0d17ab7435c45a2)
I am using the graphr Ruby gem and GraphViz on Ubuntu and all I get is 
![discrete](https://cloud.githubusercontent.com/assets/1646184/16079683/1e43b4c4-3305-11e6-8aec-c6fbc28eabb2.png)

Btw, why didnâ€™t you put the graphr gem in the dependencies?

I need the train data out of the box, so added an attributes accessor to access the train data in RuleSet Class, So that I can get the accuracy directly. For better understanding I can show u the code snippet 

``` ruby
class Simple
  def initialize(attributes=nil, training=nil)
    @attributes = attributes || ['Temperature']
    @training = training ||[
        [36.6, 'healthy'],
        [37, 'sick'],
        [38, 'sick'],
        [36.7, 'healthy'],
        [40, 'sick'],
        [50, 'really sick']
    ]
  end

  def tree
    dec_tree = DecisionTree::ID3Tree.new(@attributes, @training, 'sick', :continuous)
    dec_tree.train
    rule_set = dec_tree.rule_set

    result = rule_set.rules.each { |r| r.accuracy(rule_set.train_data) }
    result.collect { |r| ap [r.conclusion, r.accuracy] }
  end
end
```

Thank you Ilya for your great job. I work with @nicomahler  and we look forward contributing to this project.

As I worked on #19, I remarked that the last line of the code snippet below, extracted from `ID3Tree#id3_continuous` ([line 117 in `id3_tree.rb`](https://github.com/igrigorik/decisiontree/blob/master/lib/decisiontree/id3_tree.rb#L117)) seems to have no effect at all:

``` ruby
    gain = thresholds.collect { |threshold|
      (...)
      [data.classification.entropy - pos*sp[0].classification.entropy - neg*sp[1].classification.entropy, threshold]
    }.max { |a,b| a[0] <=> b[0] }

    return [-1, -1] if gain.size == 0  # gain.size is never 0, so this line of code has no effect (but will raise exception if gain is nil).
```

`gain` is a result of `Enumerable#max` method applied on an array of 2-elements arrays. Its value is either `nil`, if the array is empty (case where `thresholds` is empty - I don't know if it can happen, we never met that case), or a 2-elements array. It can never be an empty array. So `gain.size` is never 0.

On a real production dataset with 5 explanatory variables and ~1000 lines, I received a `SystemStackError: stack level too deep` when calling `DecisionTree::ID3Tree#train`.

Trying to figure out what was happening, I built the following simple dataset, which allows to reveal the bug:

``` ruby
attributes = ["X0", "X1", "X2", "X3"]
data = [["a", 0, 1, 1, 1], ["a", 0, 1, 0, 0], ["a", 0, 0, 1, 0], ["a", 0, 0, 0, 1]]
data_type = {X0: :discrete, X1: :continuous, X2: :continuous, X3: :continuous}
tree = DecisionTree::ID3Tree.new(attributes, data, 1, data_type)
tree.train  # SystemStackError is raised here!
```

The reason of this bug seems to lie in the specific output (`[-1, -1]`) of `DecisionTree::ID3Tree#id3_continuous` in the case `if values.size == 1` (see [this line](https://github.com/igrigorik/decisiontree/blob/master/lib/decisiontree/id3_tree.rb#L104)). 

Returning `[0, -1]` instead of `[-1, -1]` in the cases `if values.size == 1` and `if gain.size == 1` in the method `#id3_continuous` solves the problem.

It would also be relevant to stop the recursion in the case where the selection of each variable leads to a zero gain. That can be done adding in `#id3_train` the following line:

``` ruby
return data.first.last if performance.all? { |a, b| a <= 0 }
```

after [this line](https://github.com/igrigorik/decisiontree/blob/master/lib/decisiontree/id3_tree.rb#L75):

``` ruby
performance = attributes.collect { |attribute| fitness_for(attribute).call(data, attributes, attribute) }
```

What do you think?

Do you want me to make a pull request with these changes?

I've encountered a problem where if you have a training set like:

``` ruby
[[5, 1, "9.990941"], [5, 1, "9.990926"], [5, 1, "9.991411"], [5, 1, "9.991286"], [5, 1, "9.9916579615681"], [5, 1, "9.9917500513096"], [5, 1, "9.991682"], [5, 1, "9.991675"], [5, 1, "9.990981"], [5, 1, "9.990918"], [5, 1, "9.990918"], [5, 1, "9.990926"], [5, 1, "9.990934"], [5, 1, "9.9907993677691"], [5, 1, "9.9907108548716"], [5, 1, "9.9907190386056"], [5, 1, "9.9907190386056"]]
```

Where the attributes do not vary, only the target feature does, I get an error:

```
undefined method `to_a' for 9.990941:String
```

I encounter the same issue if the attributes vary but the target feature does not.
