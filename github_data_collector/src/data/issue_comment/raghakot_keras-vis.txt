
I tried installing the keras-vis package both the pip and python install way. However , after installing, it shows "no module found " when trying to import in spyder
Version 0.5.0 also introduces `keras.backend.identity` via [this PR](https://github.com/raghakot/keras-vis/pull/120)

But this isn't introduced in Keras until [version 2.0.4](https://github.com/keras-team/keras/releases/tag/2.0.4).

So the current `install_requires=['keras>=2.0', 'six', 'scikit-image', 'matplotlib', 'h5py']` needs to be updated to `install_requires=['keras>=2.0.4', 'six', 'scikit-image', 'matplotlib', 'h5py']`

_Originally posted by @thommiano in https://github.com/raghakot/keras-vis/issues/198#issuecomment-535039953_
I have an issue with visualizing my conv layers in the following model architecture, because keras-vis seems not to be able to handle model in models architecture. I want to visualize a conv layer within my vgg19 transfered model. But i can't acces them. Does anyone know how to visualize the conv layers in a model within a model architecture or even how to "flatten out" the vgg19 model so i can access all layers. It seems to be an open issue of keras-vis and i need the visualization urgently for my bachelor thesis. Any help or recommendation of another visualization tool is greatly appreciated! Thank you!

_________________________________________________________________
Layer (type)                 Output Shape              Param #  
========================================
input_1 (InputLayer)         (None, 120, 160, 1)       0        
_________________________________________________________________
lambda_1 (Lambda)            (None, 120, 160, 3)       0        
_________________________________________________________________
batch_normalization_1 (Batch (None, 120, 160, 3)       12       
_________________________________________________________________
vgg19 (Model)                multiple                  20024384 
_________________________________________________________________
flatten_1 (Flatten)          (None, 7680)              0        
_________________________________________________________________
dense_1 (Dense)              (None, 80)                614480   
_________________________________________________________________
dropout_1 (Dropout)          (None, 80)                0        
_________________________________________________________________
dense_2 (Dense)              (None, 4800)              388800   
_________________________________________________________________
reshape_1 (Reshape)          (None, 60, 80)            0        
========================================
Total params: 21,027,676
Trainable params: 1,003,286
Non-trainable params: 20,024,390
_________________________________________________________________
None


The VGG19 model consists of the following layers, that i can't access!

['input_2', 'block1_conv1', 'block1_conv2', 'block1_pool', 'block2_conv1', 'block2_conv2', 'block2_pool', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block3_conv4', 'block3_pool', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block4_conv4', 'block4_pool', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'block5_conv4', 'block5_pool']

model = load_model('/home/marco/HyperOptBachelor/vgg19_final100epochs/weights.hdf5', custom_objects={'motion_metric': motion_metric})
print(model.summary())
print([layer.name for layer in model.get_layer('vgg19').layers])
layer_idx = utils.find_layer_idx(model, 'block5_conv4')
# This is just a teacher frame from my test dataset, but it can to test it any image
img = utils.load_img('/home/marco/HyperOptBachelor/vgg19_final100epochs/out/test-96-teacher.png')
f, ax = plt.subplots(1,1)
grads = visualize_saliency(model,-5,filter_indices=10,seed_input=img)
ax[1].imshow(grads, cmap='jet')

When i try to get the index of a specific layer in that vgg19 model it comes up with the following error.

Traceback (most recent call last):
  File "/home/marco/PycharmProjects/ideal-system/helper_scripts/visualization.py", line 11, in <module>
    layer_idx = utils.find_layer_idx(model, 'block5_conv4')
  File "/home/marco/bachelor/lib/python3.5/site-packages/vis/utils/utils.py", line 164, in find_layer_idx
    raise ValueError("No layer with name '{}' within the model".format(layer_name))
ValueError: No layer with name 'block5_conv4' within the model

so it doesnt find that layer because theres just the model not all the layers listed i guess. And if i just call by index the vgg19 model it throws an error about that the model has mutliple output nodes. I searched the internet and it seems to be an open issue. but now i dont know how to keep progressing!


Kind Regards,

Marco Lengua
- [ ] Check that you are up-to-date with the master branch of keras-vis. You can update with:
pip install git+git://github.com/raghakot/keras-vis.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

Hello I also have a similar issue, i am using my own image and i get the error below:
"tensorflow.python.framework.errors_impl.InvalidArgumentError: The first dimension of paddings must be the rank of inputs[4,2] [1,512,512]
[[{{node zero_padding2d_1_1/Pad}}]]"

I am giving the same size with which i trained the model for my image.

there is an example of gradcam with guided backprop modifier. but in the literature guided gradcam means something else: guided backprop multiplied by gradcam
not sure why but the combination of guided backprop + filter_indices=None (all filters) + linear final activation gives me zero gradients for VGG16. you can see this in the demo notebook.
Is it possible to use visualize_cam with a network having multiple inputs? I have a modified version of resnet50, with 3 inputs (2 are images and 1 is audio). I want to visualize activations of input1 or input2 (which are images) as in this example: https://github.com/raghakot/keras-vis/blob/master/examples/resnet/attention.ipynb
However, if I give as input to visualize_cam "seed_input=input_1" or "seed_input=[input_1, input_2, input_3]", I get the error. Any help is more than appreciated. Thanks

  grads = visualize_cam(model, layer_idx_m1, filter_indices=None,seed_input=scene, penultimate_layer_idx=penultimate_layer_m1)
  File "/usr/local/lib/python2.7/dist-packages/vis/visualization/saliency.py", line 239, in visualize_cam
    return visualize_cam_with_losses(model.input, losses, seed_input, penultimate_layer, grad_modifier)
  File "/usr/local/lib/python2.7/dist-packages/vis/visualization/saliency.py", line 160, in visualize_cam_with_losses
    _, grads, penultimate_output_value = opt.minimize(seed_input, max_iter=1, grad_modifier=grad_modifier, verbose=False)
  File "/usr/local/lib/python2.7/dist-packages/vis/optimizer.py", line 122, in minimize
    seed_input = self._get_seed_input(seed_input)
  File "/usr/local/lib/python2.7/dist-packages/vis/optimizer.py", line 85, in _get_seed_input
    desired_shape = (1, ) + K.int_shape(self.input_tensor)[1:]
  File "/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py", line 584, in int_shape
    return tuple(x.get_shape().as_list())
AttributeError: 'list' object has no attribute 'get_shape'
Version 0.5.0 fixes a number of bugs for me, and the current PyPI version is 0.4.1!
- [ ] Check that you are up-to-date with the master branch of keras-vis. You can update with:
pip install git+git://github.com/raghakot/keras-vis.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

### Hello, I am trying to run through the Attention on MNIST (Saliency and grad-CAM) example and I am running into the following issue. 


---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-22-faedabb421eb> in <module>
     11 model = utils.apply_modifications(model)
     12 
---> 13 grads = visualize_saliency(model, layer_idx, filter_indices=class_idx, seed_input=x_test[idx])
     14 # Plot with 'jet' colormap to visualize as a heatmap.
     15 plt.imshow(grads, cmap='jet')

~/jessica/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/vis/visualization/saliency.py in visualize_saliency(model, layer_idx, filter_indices, seed_input, wrt_tensor, backprop_modifier, grad_modifier, keepdims)
    132         (ActivationMaximization(model.layers[layer_idx], filter_indices), -1)
    133     ]
--> 134     return visualize_saliency_with_losses(model.input, losses, seed_input, wrt_tensor, grad_modifier, keepdims)
    135 
    136 

~/jessica/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/vis/visualization/saliency.py in visualize_saliency_with_losses(input_tensor, losses, seed_input, wrt_tensor, grad_modifier, keepdims)
     77     """
     78     opt = Optimizer(input_tensor, losses, wrt_tensor=wrt_tensor, norm_grads=False)
---> 79     grads = opt.minimize(seed_input=seed_input, max_iter=1, grad_modifier=grad_modifier, verbose=False)[1]
     80 
     81     if not keepdims:

~/jessica/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/vis/optimizer.py in minimize(self, seed_input, max_iter, input_modifiers, grad_modifier, callbacks, verbose)
    151 
    152             # 0 learning phase for 'test'
--> 153             computed_values = self.compute_fn([seed_input, 0])
    154             losses = computed_values[:len(self.loss_names)]
    155             named_losses = list(zip(self.loss_names, losses))

~/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py in __call__(self, inputs)
   2713                 return self._legacy_call(inputs)
   2714 
-> 2715             return self._call(inputs)
   2716         else:
   2717             if py_any(is_tensor(x) for x in inputs):

~/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py in _call(self, inputs)
   2653                 array_vals.append(
   2654                     np.asarray(value,
-> 2655                                dtype=tf.as_dtype(tensor.dtype).as_numpy_dtype))
   2656         if self.feed_dict:
   2657             for key in sorted(self.feed_dict.keys()):

AttributeError: 'int' object has no attribute 'dtype'


If we could reload the MNIST model trained in the example and draw all the saliency charts for for **one digit** (e.g. '7') using **filter index from zero to ten**, we can see that it's very difficult to observe the contrast using the linear activation. Yet, the contrast can indeed be seen using softmax activation. Moreover, we need to manually set a consistent scale to see the contrast.

It may be important, at least for defect detection topics, to know how and why an image is classified as one thing but not another by checking the significant contrasts in saliency visualizations.

```
from __future__ import print_function
from keras.datasets import mnist
import numpy as np
from matplotlib import pyplot as plt
from vis.visualization import visualize_saliency
from vis.utils import utils
from keras import activations
from keras.models import load_model

(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Load once for a model with softmax for last dense layer, and load again for one with linear swap

MODEL_PATH = "model.h5"

model = load_model(MODEL_PATH)
raw_model = load_model(MODEL_PATH)

# check layers in the model
NAMES = []
for index, layer in enumerate(model.layers):
    NAMES.append(layer.name)
    print(index, layer.name)
print('====================================================\n\n\n')

# swap softmax
layer_idx = utils.find_layer_idx(model, 'dense_2')
model.layers[layer_idx].activation = activations.linear
model = utils.apply_modifications(model)


# prepare a sample image '7'
img = x_test[0]/255

seed = img.copy()
seed = np.expand_dims(seed, 2)
seed = np.expand_dims(seed, 0)

# use absolute scale for the 
MAX_PIXEL_softmax = 0.01
MAX_PIXEL_linear = 1


for index in range(10):
    print('----------------------------------------------')
    print('Digit: ', index)
    f, ax = plt.subplots(1, 3)

    grads_softmax = visualize_saliency(raw_model, layer_idx, filter_indices=index,
                               seed_input=seed, backprop_modifier="guided")
    print('total:', round(grads_softmax.sum()*10000), '  max:', round(grads_softmax.max(),5), '  min:', round(grads_softmax.min(),5))
    grads_softmax[0,0] = MAX_PIXEL_softmax
    ax[0].set_title('Softmax ' + str(index))
    ax[0].imshow(grads_softmax, cmap = 'jet')
    
    grads_linear = visualize_saliency(model, layer_idx, filter_indices=index,
                               seed_input=seed, backprop_modifier="guided")
    print('total:', round(grads_linear.sum()), '  max:', round(grads_linear.max(),5), '  min:', round(grads_linear.min(),5))
    grads_linear[0,0] = MAX_PIXEL_linear
    ax[1].set_title('Linear ' + str(index))
    ax[1].imshow(grads_linear, cmap = 'jet')

    ax[2].set_title('Raw image')
    ax[2].imshow(img)
    
```
