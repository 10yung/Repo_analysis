weighs的梯度推导过程应该是根据最大似然函数求极值来的吧；要求的是最大似然函数的极大值，所以称为梯度上升；如果给极大似然函数加负号就是梯度下降了。所求的梯度不是z=w0*x0+w1*x1...的梯度。这是我对《机器学习实战》的理解。
大佬，怎么加入群聊学习呢？为啥子加不上呢？是不是需要有某些要求呢？
1.  词向量
2.  文本分类
3.  文本标注：分词/词性标注/NER
4.  文本生成
    1.  机器翻译
    2.  对话系统
    3.  文本摘要
> [GBDT算法代码实现 - 代码地址](https://github.com/echohandsome/Machine_Learning_in_Action_for_smallwhite/tree/master/GBDT/GBDT_python3_code)

需要了解GBDT算法原理的朋友，sklearn源代码过于复杂，现在整理分享一个代码实现的简单版本，经过调试可以下载以后直接在jupyter notebook中运行，如果没有面向对象编程基础建议不要下载学习。
 AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py 第111行
 gini += float(size)/D * (proportion * (1.0 - proportion)) # 个人理解：计算代价，分类越准确，则 gini 越小
是不是少了一个2，见下：
 gini += float(size)/D * (2*proportion * (1.0 - proportion)) # 个人理解：计算代价，分类越准确，则 gini 越小
如题
### What is the problem with Chinese README's

Firstly, we congratulate you for getting so much star by sharing this repository with humanity.

But it is very disappointing for non-Chinese speakers when one couldn't understand what a trending repository is about.

When we see such a repo on trending, our minds are blurring like Gollum's.

![Gollum Image](https://media.giphy.com/media/V4uGHRgz0zi6Y/giphy-downsized-large.gif)

There is a way **you can help** to solve this disappointment which I believe is experienced by many people who want to know more about your valuable work and appreciate it.

### What we want:
 - Please add English translation of your README so you are sharing your work and knowledge with more people.

### How this will help you:
 - More feedback to fix and improve your project.
 - New ideas about your project.
 - Greater fame.
 - ![SungerBob Image](https://media.giphy.com/media/3o7absbD7PbTFQa0c8/source.gif)

---

_“Sharing knowledge is the most fundamental act of friendship. Because it is a way you can give something without loosing something.”_

_**— Richard Stallman**_

**Thank you!**	

 > This issue created by `us/english-please` script. Please report on any error. Thank you!

半监督学习(Semi-Supervised Learning,SSL)类属于机器学习(Machine Learning,ML)。

## 一 ML有两种基本类型的学习任务：

> 1.监督学习(Supervised Learning,SL)

    根据输入-输出样本对L={(x1,y1),···,(xl,yl)}学习输入到输出的映射f:X->Y,来预测测试样例的输出值。SL包括分类(Classification)和回归(Regression)两类任务，分类中的样例xi∈Rm(输入空间)，类标签yi∈{c1,c2,···,cc},cj∈N;回归中的输入xi∈Rm，输出yi∈R(输出空间)。

> 2. 无监督学习(Unsupervised Learning,UL)

    利用无类标签的样例U={x1,···,xn}所包含的信息学习其对应的类标签Yu=[y1···yn]T,由学习到的类标签信息把样例划分到不同的簇(Clustering)或找到高维输入数据的低维结构。UL包括聚类(Clistering)和降维(Dimensionality Reduction)两类任务。

## 二 半监督学习(Semi-Supervised Learning,UL)

    在许多ML的实际应用中，很容易找到海量的无类标签的样例，但需要使用特殊设备或经过昂贵且用时非常长的实验过程进行人工标记才能得到有类标签的样本，由此产生了极少量的有类标签的样本和过剩的无类标签的样例。因此，人们尝试将大量的无类标签的样例加入到有限的有类标签的样本中一起训练来进行学习，期望能对学习性能起到改进的作用，由此产生了SSL，如如图１所示。SSL避免了数据和资源的浪费，同时解决了SL的 模型泛化能力不强和UL的模型不精确等问题。

![image](https://user-images.githubusercontent.com/9199175/53324336-803ea480-391b-11e9-9887-3cceb9b92959.png)

> 1.半监督学习依赖的假设

SSL的成立依赖于模型假设，当模型假设正确时，无类标签的样例能够帮助改进学习性能。SSL依赖的假设有以下３个：

（１）平滑假设（Smoothness Assumption）

    位于稠密数据区域的两个距离很近的样例的类标签相似，也就是说，当两个样例被稠密数据区域中的边连接时，它们在很大的概率下有相同的类标签；相反地，当两个样例被稀疏数据区域分开时，它们的类标签趋于不同． 

（２）聚类假设（Cluster Assumption）

    当两个样例位于同一聚类簇时，它们在很大的概率下有相同的类标签．这个假设的等价定义为低密度分离假设（Low Sensity Separation Assumption），即分类 决策边界应该穿过稀疏数据区域，而避免将稠密数 据区域的样例分到决策边界两侧．

 （３）流形假设（Manifold Assumption）

    将高维数据嵌入到低维流形中，当两个样例位于低维流形中的一个小局部邻域内时，它们具有相似的类标签。许多实验研究表明当SSL不满足这些假设或模型假设不正确时，无类标签的样例不仅不能对学习性能起到改进作用，反而会恶化学习性能，导致 SSL的性能下降．但是还有一些实验表明，在一些特殊的情况下即使模型假设正确，无类标签的样例也有可能损害学习性能。

> 2.半监督学习的分类 

    SSL按照统计学习理论的角度包括直推 （Transductive ）SSL和归纳（Inductive）SSL两类模式。直推 SSL只处理样本空间内给定的训练数据，利用训练数据中有类标签的样本和无类标签的样例进行训练，预测训练数据中无类标签的样例的类标签；归纳SSL处理整个样本空间中所有给定和未知的样例，同时利用训练数据中有类标签的样本和无类标签的样例，以及未知的测试样例一起进行训练，不仅预测训练数据中无类标签的样例的类标签，更主要的是预测未知的测试样例的类标签。从不同的学习场景看，SSL可分为４大类： 

（１）半监督分类 （Semi-Supervised Classification）

    在无类标签的样例的帮助下训练有类标 签的样本，获得比只用有类标签的样本训练得到的分类器性能更优的分类器，弥补有类标签的样本不足的缺陷，其中类标签yi取有限离散值yi∈{c1,c2,···,cc},cj∈N。

（２）半监督回归（Semi-Supervised Regression）

    在无输出的输入的帮助下训练有输出的输入，获得比只用有输出的输入训练得到的回归器性能更好的回归器，其中输出yi 取连续值 yi∈Ｒ。 

（３）半监督聚类（Semi-Supervised Clustering）

    在有类标签的样本的信息帮助下获得比只用无类标 签的样例得到的结果更好的簇，提高聚类方法的精度。

（４）半监督降维（Semi-Supervised Dimensionality Reduction）

    在有类标签的样本的信息帮助下找到高维输入数据的低维结构，同时保持原始高维数据和成对约束（Pair-Wise Constraints）的结构不变，即在高维空间中满足正约束（Must-Link Constraints）的样例在低维空间中相距很近，在高维空间中满足负约束（Cannot-Link Constraints）的样例在低维空间中距离很远。

![image](https://user-images.githubusercontent.com/9199175/53324326-79b02d00-391b-11e9-8e4d-87c1e59562d2.png)

---

原文地址：https://blog.csdn.net/jiusake/article/details/80016171
如题，希望有相关教程
http://ailearning.apachecn.org/join-us/ 

 ApacheCN 专注于优秀项目维护的开源组织