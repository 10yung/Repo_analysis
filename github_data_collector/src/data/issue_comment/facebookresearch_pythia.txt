I am training the pythia vqa model on multiple datasets. Say, I have 2 datasets for the vqa task (say vqa2 and some new vqax). I noticed that while training a dataset is chosen at random (based on its length) at the beginning, and the model is trained with just that dataset, and never on the other dataset. Is this the expected behaviour? See https://github.com/facebookresearch/pythia/blob/12f67cd4f67499814bb0b3665ff14dd635800f63/pythia/tasks/base_task.py#L137 , the change_dataset() function is called at the time of initialization, and never later, preventing the dataset that is not chosen at first from being used. Could you please let me know if I am missing something?
Merge the M4C model (https://arxiv.org/pdf/1911.06258.pdf) for TextVQA into Pythia.

Summary of changes:
* Adding `README.md` under `projects/M4C`
* Adding new models: M4C under `pythia/models/m4c.py`
* Adding new dataset classes: `m4c_textvqa`, `m4c_stvqa`, and `m4c_ocrvqa` under `pythia/datasets/vqa/`
* Adding new config files under `configs/vqa`
* Adding new processors, metrics and losses for M4C training and evaluation.
* Adding other utilities (such as PHOC feature extraction).

Introducing new dependencies (added to `requirements.txt`):
* `pytorch-transformers`  
* `editdistance`

## M4C for the TextVQA Task
* R. Hu, A. Singh, T. Darrell, M. Rohrbach, *Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA*. arXiv preprint arXiv:1911.06258, 2019 ([PDF](https://arxiv.org/pdf/1911.06258.pdf))
```
@article{hu2019iterative,
  title={Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA},
  author={Hu, Ronghang and Singh, Amanpreet and Darrell, Trevor and Rohrbach, Marcus},
  journal={arXiv preprint arXiv:1911.06258},
  year={2019}
}
```

## Vocabs, ImDBs and Features:

| Datasets      | M4C Vocabs | M4C ImDBs | Object Faster R-CNN Features | OCR Faster R-CNN Features |
|--------------|-----|-----|-----------------------------------------------------------------------------------|---------------------------------------------------------------------------------|
| TextVQA      | [All Vocabs](https://dl.fbaipublicfiles.com/pythia_m4c/data/m4c_vocabs.tar.gz) | [TextVQA ImDB](https://dl.fbaipublicfiles.com/pythia_m4c/data/imdb/m4c_textvqa.tar.gz) | [OpenImages](https://dl.fbaipublicfiles.com/pythia/features/open_images.tar.gz) | [TextVQA Rosetta-en OCRs](https://dl.fbaipublicfiles.com/pythia_m4c/data/m4c_textvqa_ocr_en_frcn_features.tar.gz), [TextVQA Rosetta-ml OCRs](https://dl.fbaipublicfiles.com/pythia_m4c/data/m4c_textvqa_ocr_ml_frcn_features.tar.gz) |
| ST-VQA      | [All Vocabs](https://dl.fbaipublicfiles.com/pythia_m4c/data/m4c_vocabs.tar.gz) | [ST-VQA ImDB](https://dl.fbaipublicfiles.com/pythia_m4c/data/imdb/m4c_stvqa.tar.gz) | [ST-VQA Objects](https://dl.fbaipublicfiles.com/pythia_m4c/data/m4c_stvqa_obj_frcn_features.tar.gz) | [ST-VQA Rosetta-en OCRs](https://dl.fbaipublicfiles.com/pythia_m4c/data/m4c_stvqa_ocr_en_frcn_features.tar.gz) |
| OCR-VQA      | [All Vocabs](https://dl.fbaipublicfiles.com/pythia_m4c/data/m4c_vocabs.tar.gz) | [OCR-VQA ImDB](https://dl.fbaipublicfiles.com/pythia_m4c/data/imdb/m4c_ocrvqa.tar.gz) | [OCR-VQA Objects](https://dl.fbaipublicfiles.com/pythia_m4c/data/m4c_ocrvqa_obj_frcn_features.tar.gz) | [OCR-VQA Rosetta-en OCRs](https://dl.fbaipublicfiles.com/pythia_m4c/data/m4c_ocrvqa_ocr_en_frcn_features.tar.gz) |

## Pretrained models:

| Datasets  | Configs (under `configs/vqa/`)         | Pretrained Models | Metrics                     | Notes                         |
|--------|------------------|----------------------------|-------------------------------|-------------------------------|
| TextVQA (`m4c_textvqa`) | `m4c_textvqa/m4c_with_stvqa.yml` | [`download`](https://dl.fbaipublicfiles.com/pythia_m4c/m4c_release_models/m4c_textvqa/m4c_textvqa_m4c_with_stvqa.ckpt) | val accuracy - 40.55%; test accuracy - 40.46% | Rosetta-en OCRs; ST-VQA as additional data |
| TextVQA (`m4c_textvqa`) | `m4c_textvqa/m4c.yml` | [`download`](https://dl.fbaipublicfiles.com/pythia_m4c/m4c_release_models/m4c_textvqa/m4c_textvqa_m4c.ckpt) | val accuracy - 39.40%; test accuracy - 39.01% | Rosetta-en OCRs |
| TextVQA (`m4c_textvqa`) | `m4c_textvqa/m4c_ocr_ml.yml` | [`download`](https://dl.fbaipublicfiles.com/pythia_m4c/m4c_release_models/m4c_textvqa/m4c_textvqa_m4c_ocr_ml.ckpt) | val accuracy - 37.06% | Rosetta-ml OCRs |
| ST-VQA (`m4c_stvqa`)  | `m4c_stvqa/m4c.yml` | [`download`](https://dl.fbaipublicfiles.com/pythia_m4c/m4c_release_models/m4c_stvqa/m4c_stvqa_m4c.ckpt) | val ANLS - 0.472 (accuracy - 38.05%); test ANLS - 0.462 | Rosetta-en OCRs |
| OCR-VQA (`m4c_ocrvqa`) | `m4c_ocrvqa/m4c.yml` | [`download`](https://dl.fbaipublicfiles.com/pythia_m4c/m4c_release_models/m4c_ocrvqa/m4c_ocrvqa_m4c.ckpt) | val accuracy - 63.52%; test accuracy - 63.87% | Rosetta-en OCRs |
## â“ Questions and Help
I have 3 RTX 2080s on my ubuntu 18.04.3 machine and I'm running the pythia vqa demo on images. Right now it takes around 11-12 seconds for it to process 1 image (it's running on 1 gpu). Is there a way to accelerate the running time or even run it on multiple gpus simultaneously?

Edit: Here is the nvidia-smi output when running pythia
|   0  GeForce RTX 2080    Off  | 00000000:09:00.0 Off |                  N/A |
|  0%   38C    P2    61W / 215W |   2160MiB /  7982MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce RTX 2080    Off  | 00000000:0A:00.0 Off |                  N/A |
|  0%   48C    P0    52W / 215W |     11MiB /  7982MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GeForce RTX 2080    Off  | 00000000:41:00.0 Off |                  N/A |
|  5%   49C    P0    44W / 215W |     11MiB /  7974MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
## ðŸ› Bug

Without the evalai_inference argument 
AttributeError: 'dict' object has no attribute 'dataset_type'

This error occurs.
![image](https://user-images.githubusercontent.com/8081512/71147204-61731b00-226b-11ea-863c-5fc9ebad5598.png)


You can run the script with:
```
python tools/run.py --tasks vqa --datasets vqa2 --model pythia --config configs/vqa/vqa2/pythia_train_and_val.yml --resume_file data/models/pythia_train_val.pth --run_type val
```

## Additional context

It seems that in the batch, it should refer 'dataset_type' as a dictionary key, not an attribute.

Either

1. in pythia/common/task_loader.py the prepare_batch function should change to
batch.dataset_type to batch['dataset_type'] or add dataset_type as an attribute to the dictionary
![image](https://user-images.githubusercontent.com/8081512/71147328-d7778200-226b-11ea-9d0f-851e61bd51d3.png)


## ðŸš€ Feature

Based on #193, we definitely need to move to Detectron2 as soon as possible for feature extraction. This would also allow faster e2e training. 

## Motivation

Detectron2 https://github.com/facebookresearch/detectron2 is going to maintained in future rather than maskrcnn-benchmark and detectron and thus we need to move to it. We need to implement similar features in our fork of detectron like we did for vqa-maskrcnn-benchmark.

## Pitch

- Look into the changes that were made to original maskrcnn-benchmark to create vqa-maskrcnn-benchmark. See the [diff](https://gitlab.com/meetshah1995/vqa-maskrcnn-benchmark/compare/0dfac376668a6618ed17c2d890c92728bb2926b6...master).
- Fork [detectron2](https://github.com/facebookresearch/detectron2)
- First read the breaking changes in detectron2 compared to detectron/maskrcnn-benchmark at this [link](https://detectron2.readthedocs.io/notes/compatibility.html#compatibility-with-detectron).
- Make changes according to the diff above to your fork and try training a model.


## â“ Questions and Help

Hi while trying to reproduce LoRRA model on Text VQA, my computer freezes indefinitely while loading `fasttext model from .vector_cache/wiki.en.bin`. 
I have 16GB of RAM and I followed the instructions on the README homepage of Pythia. 
Running the following command
```
python tools/run.py --tasks vqa --datasets textvqa --model lorra --config configs/vqa/textvqa/lorra.yml \
--run_type inference --evalai_inference 1 --resume_file data/models/lorra_best.pth
```
any suggestions if i can make it work on my computer, or is 16GB memory insufficient to run LoRRA?
## â“ Questions and Help
Hi,
I have a custom dataset of annotated images. So far I have managed to generate the detectron features for the images, the vocabulary for the captions and set up the config files correctly. When I try to train on using a butd model I encounter an error:
```
AttributeError: Key answers not found in the SampleList. Valid choices are ['text', 'caption_len', 'image_id', 'image_feature_0', 'dataset_type', 'dataset_name']
```
which I think is because my dataset only contains captions and there are no question-answer segments associated with it. 
My question is that can Pythia be setup for an image captioning / image retrieval task? What would be the steps required to train it on this custom dataset?
Thanks!
## â“ Questions and Help

I tried to reproduce the validation result of LoRRA on TextVQA. After finishing training, I evaluated the LoRRA model offline by setting the batch_size = 1 and using the command â€œpython tools/run.py --tasks vqa --datasets textvqa --model lorra --config configs/vqa/textvqa/lorra.yml --run_type val --resume_file lorra_best.pthâ€ and I got a result of 27.12%. When I generated the json file by using the command â€œpython tools/run.py --tasks vqa --datasets textvqa --model lorra --config configs/vqa/textvqa/lorra.yml --run_type val --evalai_inference 1 --resume_file lorra_best.pthâ€ and evaluated it via EvalAI, I got a result of 27.40%.

Could you please explain the difference?

## â“ Questions and Help
Hello, ask a question. What is the role of text_processor and answer_processor in vqa2.yml, and the corresponding vocab_file: vocabs/vocabulary_100k.txt and vocab_file: vocabs/answers_vqa.txt What is the use?
## â“ Questions and Help
I met an error when training after the max iterationsï¼š

> 2019-10-12T20:02:44 INFO: textvqa: full val:, 26001/26000, val/total_loss: 9.5553, val/logit_bce: 9.5553, val/vqa_accuracy: 0.3055, validation time: 01m 25s 349ms, best iteration: 17000, best val/vqa_accuracy: 0.309980
2019-10-12T20:02:45 INFO: Restoring checkpoint
2019-10-12T20:02:49 ERROR: CUDA out of memory. Tried to allocate 39.12 MiB (GPU 0; 10.76 GiB total capacity; 9.71 GiB already allocated; 5.56 MiB free; 10.13 MiB cached)
Traceback (most recent call last):
  File "tools/run.py", line 94, in <module>
    run()
  File "tools/run.py", line 83, in run
    trainer.train()
  File "/home/B/gcy/code/pythia-master/pythia/trainers/base_trainer.py", line 237, in train
    self.finalize()
  File "/home/B/gcy/code/pythia-master/pythia/trainers/base_trainer.py", line 272, in finalize
    self.checkpoint.restore()
  File "/home/B/gcy/code/pythia-master/pythia/utils/checkpoint.py", line 242, in restore
    self.trainer.optimizer.load_state_dict(ckpt["optimizer"])
  File "/home/gcy/anaconda3/envs/ye/lib/python3.6/site-packages/torch/optim/optimizer.py", line 103, in load_state_dict
    state_dict = deepcopy(state_dict)
  File "/home/gcy/anaconda3/envs/ye/lib/python3.6/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/home/gcy/anaconda3/envs/ye/lib/python3.6/copy.py", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/home/gcy/anaconda3/envs/ye/lib/python3.6/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/home/gcy/anaconda3/envs/ye/lib/python3.6/copy.py", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/home/gcy/anaconda3/envs/ye/lib/python3.6/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/home/gcy/anaconda3/envs/ye/lib/python3.6/copy.py", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/home/gcy/anaconda3/envs/ye/lib/python3.6/copy.py", line 161, in deepcopy
    y = copier(memo)
  File "/home/gcy/anaconda3/envs/ye/lib/python3.6/site-packages/torch/tensor.py", line 26, in __deepcopy__
    new_storage = self.storage().__deepcopy__(memo)
  File "/home/gcy/anaconda3/envs/ye/lib/python3.6/site-packages/torch/storage.py", line 28, in __deepcopy__
    new_storage = self.clone()
  File "/home/gcy/anaconda3/envs/ye/lib/python3.6/site-packages/torch/storage.py", line 44, in clone
    return type(self)(self.size()).copy_(self)
RuntimeError: CUDA out of memory. Tried to allocate 39.12 MiB (GPU 0; 10.76 GiB total capacity; 9.71 GiB already allocated; 5.56 MiB free; 10.13 MiB cached)

How can I do with this error without setting my batch-size smaller?