Muffet complains `id #mozTocId468501 not found` when it sees my site link to:

http://www.clearskyinstitute.com/xephem/help/xephem.html#mozTocId468501

Yet the link works, jumping to the correct section of the page. It appears that muffet might not yet know about link targets created with `<a href="#name">`? Adding that knowledge would help reduce its noise when run against links whose sites use them. Thanks!
I'm enjoying the performance of this tool! But, to improve user experience of the web and lower server costs, I like linking directly to the current URL for a page instead of linking to an old URL that is simply a redirect.

Could an option be added that would report all URLs leading to redirects as broken instead of valid? I tried `-l 0` to turn redirection-following off, but muffet still seems to follow at least a single initial redirection.
Hello.

Would it be possible to implement parameter which will disable/enable colors in muffet ?

Something like:

```
-C, --color <auto | always | never>    Use color [default: auto]
```

It will help on some "dumb" terminals to "force" color or just disable it completely.
Hi Yota,

just a question. Is it possible to limit the check to a given Tag like <img> ?

In my special case I just want to check some special links to images on a sigle URL

Something like --include "page-0-cover-big.jpg"

Joerg

I just tried:

muffet --timeout 60 https://istio.io 

The result is 8000 lines of output indicating that a great many pages experienced timeout errors. Any given page is shown a great many times in the output, which seems to indicate it is being processed multiple times (I would expect any given link to be visited only once during a given scan).

When I use the -v option, The output grows to at least 50000 lines, and this time both page that time out and pages that don't time out appear multiple times in the output.

So the first problem is the fact the same pages seem to be visited multiple times.

The second problem are these timeouts. I gave it 60 second timeout intervals, and I don't believe that our CDN is unable to deliver the desired content in 60 seconds for so many pages. When accessing the links from a browser, they appear instantaneously.

I change my command-line to:

muffet -c 32 --timeout 10 https://istio.io 

This eliminated all the timeout errors. So the problem is on my side, and not on the CDN. I'm running this on a Mac, perhaps it has some limits to the number of opened sockets (or something similar) which is being interpreted as timeouts by the link checker. So it might be good to dynamically discover the problem and tune down the concurrency accordingly.

When producing a web site from a static site generator like Hugo, it'd be highly desirable to be able to verify the links in a statically generated site on disk. So basically:

- Instead of specifying a URL to fetch from, let me specify a file system directory

- Iterate through all the HTML files and analyze them.

- Provide a mappings layer to map the public location of the web site to the directory. So basically make it so https://foo/bar gets translated to mysite/bar.

I could then check the site's link before publishing.

I currently use htmlproofer for this task and it is awfully slow and it requires Ruby. Muffet seems like a much superior solution.

Thanks.
As the title states, would it be possible to get the resultset as JSON?
Not sure if it's platform specific or not, but when were running a scan for https://msdn.microsoft.com for fun as mentioned in #27 just noticed muffet.exe using ~7Gb of memory and kept going...
Current version doesn't work behind proxy