The list folder was not being written to dindex files when performing a
compact operation. After the data blocks have been moved to a new
volume the blocklists are now read from the database and written to
the new dindex file.

Fixes #4048 
This adds a simple test for recovering files using the `RecoveryTool`.  We verify that the backend files can be downloaded, decrypted, and used to restore the source files to a different location.

In doing so, we discovered an instance of `HashLookupHelper` that wasn't disposed that kept an open `FileStream` to the `index.txt` file.
- [x] I have searched open and closed issues for duplicates.

----------------------------------------

## Environment info

- **Duplicati version**:  2.0.4.23_beta_2019-07-14
- **Operating system**: Windows 10
- **Backend**: local

## Description

When restoring files, if the drive containing the backup is not connected, the restore finishes with a success message, even tho no files were actually restored. A separate warning pop up appears that says the backup doesn't exist, but since errors like that happen from time to time, I associate them with backup failures, and it wasn't clear that it was related to the restore failure.

## Steps to reproduce

1. Backup something to a drive
2. Disconnect that drive
3. Attempt to restore from that backup

- **Actual result**:

Duplicati fails to restore any files and yet gives a success  message.

- **Expected result**:

No success message should appear, and instead a failure message should appear.
<!-- Please search to see if an issue has already been created for your report. -->
<!-- Replace the empty checkbox [ ] below with a checked one [x] if you already searched for duplicate bugs. -->
- [x ] I have searched open and closed issues for duplicates.

----------------------------------------

## Environment info
- **Duplicati version**:  2.0.4.38_canary_2019-12-29
- **Operating system**: Archlinux 5.4.10-arch1-1 
- **Backend**: local mount point of a smb share through systemd automount
//192.168.2.1/backup on /mnt/speedportBackup type cifs (rw,relatime,vers=1.0,cache=strict,username=XXX,domain=XXX,uid=1000,forceuid,gid=1000,forcegid,addr=192.168.2.1,soft,unix,posixpaths,serverino,mapposix,acl,rsize=61440,wsize=65536,bsize=1048576,echo_interval=60,actimeo=1)


## Description
I ran a backup sync and duplicati failed somehow to finish uploading the file. 

## Steps to reproduce
unknown

- **Actual result**:
 - The file is not saved correctly
- **Expected result**:
 - Try to redo the specific file handling
 - Give an opportunity to the user to fix this by rerun the failed task again
 - Give an opportunity to the user to fix this by rollback the failed task

## Screenshots

## Debug log
[bugreport.zip](https://github.com/duplicati/duplicati/files/4049032/bugreport.zip)


While testing starting and stopping a backup the "Unexpected difference
in fileset" exception would sometimes occur.

The exception could occur due to the existing delete for FilesetEntry
deleting based on volume ids and the `bsIdsSubQuery` in LocalDatabase
`RemoveRemoteVolumes()` returning other BlocksetIds that should
also be deleted. FilesetEntry would then have a higher count of rows
than what would be returned by the LEFT OUTER JOINS on all of the
other tables in `VerifyConsistency()` since more rows had been deleted
from FileLookup.
<!-- Thank you for taking the time to submit an issue using this template. By following the instructions and filling out the sections below, you will help the developers get the necessary information to fix your issue. You may remove sections that aren't relevant to your particular case. You can also preview your report before submitting it. -->
<!-- Love Duplicati? Please consider supporting our collective at https://opencollective.com/duplicati/donate. -->

<!-- Please search to see if an issue has already been created for your report. -->
<!-- Replace the empty checkbox [ ] below with a checked one [x] if you already searched for duplicate bugs. -->
- [x] I have searched open and closed issues for duplicates.

----------------------------------------

## Environment info
<!-- Please include some relevant information about your environment. -->
<!-- For "Backend", please indicate the backup destination (e.g. Amazon S3, OneDrive, FTP, WebDAV, local). -->
- **Duplicati version**: 2.0.3.5 to 2.0.5.0 were the ends of the tested range. Looks like a very old bug.
- **Operating system**: Windows 10 Version 1909
- **Backend**: Local folder

## Description
<!-- Describe the issue that you are experiencing below. -->
Compact may fail to write the list folder into the newly created dindex file created by a dblock merge. 
I think the files in this folder are performance-enhancers, controlled by [--index-file-policy](https://duplicati.readthedocs.io/en/latest/06-advanced-options/#index-file-policy), so the dlist file `blocklists` reference can read the blocklist from the dindex file without downloading its dblock. 

Downloading dblock files for DB rebuild in Recreate or direct restore seems to go against the design intent, slows things down, and might get questions or complaints from people who observe carefully:

[Repair is downloading dblock files](https://forum.duplicati.com/t/repair-is-downloading-dblock-files/2780)

Note that this is a far more subtle performance reduction than the all-dblocks-downloaded situation.

[Database recreate desperately needs improvement #4041](https://github.com/duplicati/duplicati/issues/4041) has one person who might be heading for downloading all dblocks, and one who might be hitting bad dindex files. See technical remarks there.

## Steps to reproduce
<!-- List the steps to reproduce the bug if possible. -->
1. Prepare four test files, which are variations of a 1024 text character file, intended to force all files to get a blocklist, and all blocks to be different. Line endings shouldn't matter. My notepad had none.

```
1more.txt
1 1234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234

2more.txt
12 1234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234

3more.txt
123 1234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234

4more.txt
1234 1234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234

```
2. Backup 1more.txt and 2more.txt, using --blocksize=1 KByte and --keep-versions=1, also without encryption and with --no-auto-compact true  for step-by-step view and ease of watching dindex.
3. Add 3more.txt and 4more.txt into backup via checkbox editing or move into folder, then backup.
4. Remove 1more.txt and 3more.txt from the backup, then backup. Uncheck is slightly tidier than a delete-from-folder because it won't even need to output a dblock to contain folder info update.
5. Compact. This merges the two half-empty dblock files into a new dblock, and its new dindex file.

<!-- For "Actual result", describe what happens after you run the steps above (i.e. buggy behavior). -->
<!-- For "Expected result", describe what should happen after you run the steps above (i.e. corrected behavior). -->
- **Actual result**:
New dindex file has no list folder.
- **Expected result**:
New dindex file has list folder with two files containing the blocklists for 2more.txt and 4more.txt.

I'm not sure if you'll notice, but if you're looking very carefully in the DB and filenames, you'll see that the list folder filenames are not exactly the Base64 encoding of the hash, but a character-safe variant:

https://github.com/duplicati/duplicati/blob/52929e9eb2d88fbf48fb7f235976830b81c6cb34/Duplicati/Library/Main/Volumes/IndexVolumeWriter.cs#L75-L77

One way to try to spot this problem "in the wild" is to look for compact in logs, then look at the dindex from that time. Or look for dindex files with no list folder, then see if it was by compact. Without known sources though, it's hard to know whether no list folder is from all small files thus no blocklists needed.

## Screenshots
<!-- Make it easier to get your point across with screenshots. -->
<!-- You can drag & drop or paste your images below. -->

## Debug log
<!-- Posting a debug log helps the developers find and fix your particular issue more easily. -->
<!-- Please wrap your code in code blocks with triple back-ticks to increase readability. -->

This fixes a bug introduced in pull request #3930 that broke the repair operation when `dindex` files were missing and no encryption was used.  Since we are keeping around a reference to the `IndexVolumeWriter` and `BlockVolumeWriter` instances, we cannot wrap their usages in `using` statements.


This also adds a test to verify the behavior.

This fixes issue #4046.
<!-- Thank you for taking the time to submit an issue using this template. By following the instructions and filling out the sections below, you will help the developers get the necessary information to fix your issue. You may remove sections that aren't relevant to your particular case. You can also preview your report before submitting it. -->
<!-- Love Duplicati? Please consider supporting our collective at https://opencollective.com/duplicati/donate. -->

<!-- Please search to see if an issue has already been created for your report. -->
<!-- Replace the empty checkbox [ ] below with a checked one [x] if you already searched for duplicate bugs. -->
- [x] I have searched open and closed issues for duplicates.

----------------------------------------

## Environment info
<!-- Please include some relevant information about your environment. -->
<!-- For "Backend", please indicate the backup destination (e.g. Amazon S3, OneDrive, FTP, WebDAV, local). -->
- **Duplicati version**: 2.0.4.31 Canary
- **Operating system**: Windows 10 Version 1909
- **Backend**: Local folder

## Description
<!-- Describe the issue that you are experiencing below. -->
Missing dindex file Repair fails.

## Steps to reproduce
<!-- List the steps to reproduce the bug if possible. -->
1. Make a backup of a short text file, using no encryption. --tempdir="C:\tmp" optional for privacy.
2. Delete the dindex file.
3. Do database Repair. This worked on 2.0.4.30 Canary and earlier and was a rare recovery method.

<!-- For "Actual result", describe what happens after you run the steps above (i.e. buggy behavior). -->
<!-- For "Expected result", describe what should happen after you run the steps above (i.e. corrected behavior). -->
- **Actual result**:
Missing dindex file is not replaced. After about a minute of failed uploads to folder, popup error:
`Path cannot be null. Parameter name: path`

There is no job log made, but If you look in About --> Show log --> Stored you can see a stack trace:
```
System.ArgumentNullException: Path cannot be null.
Parameter name: path
   at Duplicati.Library.Main.BackendManager.WaitForComplete(LocalDatabase db, IDbTransaction transation)
   at Duplicati.Library.Main.Operation.RepairHandler.RunRepairRemote()
   at Duplicati.Library.Main.Operation.RepairHandler.Run(IFilter filter)
   at Duplicati.Library.Main.Controller.RunAction[T](T result, String[]& paths, IFilter& filter, Action`1 method)
   at Duplicati.Library.Main.Controller.Repair(IFilter filter)
   at Duplicati.Server.Runner.Run(IRunnerData data, Boolean fromQueue)
```
There's a side oddity on repeated Repair after the initial failure. Sometimes it believes all is fine even though it's still missing a dindex. Sometimes it actually replaces dindex. If in doubt, restart Duplicati. 

This has also at least once (maybe on different source data) errored about a Temp file it can't locate.
[Ensure that dindex references correct dblock file after renames #3938](https://github.com/duplicati/duplicati/pull/3938/files) added TemporaryIndexVolume and might be relevant. 2.0.4.31 is one of the few places where dindex file changes made changelog:

> Fixed a retry error where uploaded dindex-files would reference non-existing dblock files, thanks @warwickmm

This was first seen on 2.0.5.0 Experimental while wondering whether the dindex file loss of list folder mentioned in [Repair is downloading dblock files](https://forum.duplicati.com/t/repair-is-downloading-dblock-files/2780/17?u=ts678) can be fixed by delete and rebuild. 2.0.4.5 can do it.

- **Expected result**:
Missing dindex file replaced with one built from DB. You can keep on deleting it and rebuilding. 

## Screenshots
<!-- Make it easier to get your point across with screenshots. -->
<!-- You can drag & drop or paste your images below. -->

## Debug log
<!-- Posting a debug log helps the developers find and fix your particular issue more easily. -->
<!-- Please wrap your code in code blocks with triple back-ticks to increase readability. -->
Repair profiling log with --profile-all-database-queries=true:
```
2020-01-09 20:45:01 -05 - [Information-Duplicati.Library.Main.Controller-StartingOperation]: The operation Repair has started
2020-01-09 20:45:01 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Controller-RunRepair]: Starting - Running Repair
2020-01-09 20:45:01 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalarInt64]: Starting - ExecuteScalarInt64: INSERT INTO "Operation" ("Description", "Timestamp") VALUES ("Repair", 1578620701); SELECT last_insert_rowid();
2020-01-09 20:45:01 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalarInt64]: ExecuteScalarInt64: INSERT INTO "Operation" ("Description", "Timestamp") VALUES ("Repair", 1578620701); SELECT last_insert_rowid(); took 0:00:00:00.143
2020-01-09 20:45:01 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteNonQuery]: Starting - ExecuteNonQuery: PRAGMA optimize
2020-01-09 20:45:01 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteNonQuery]: ExecuteNonQuery: PRAGMA optimize took 0:00:00:00.000
2020-01-09 20:45:01 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalarInt64]: Starting - ExecuteScalarInt64: INSERT INTO "Operation" ("Description", "Timestamp") VALUES ("Repair", 1578620701); SELECT last_insert_rowid();
2020-01-09 20:45:01 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalarInt64]: ExecuteScalarInt64: INSERT INTO "Operation" ("Description", "Timestamp") VALUES ("Repair", 1578620701); SELECT last_insert_rowid(); took 0:00:00:00.088
2020-01-09 20:45:01 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteReader]: Starting - ExecuteReader: SELECT "Key", "Value" FROM "Configuration" 
2020-01-09 20:45:01 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteReader]: ExecuteReader: SELECT "Key", "Value" FROM "Configuration"  took 0:00:00:00.001
2020-01-09 20:45:01 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteReader]: Starting - ExecuteReader: SELECT "Key", "Value" FROM "Configuration" 
2020-01-09 20:45:01 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteReader]: ExecuteReader: SELECT "Key", "Value" FROM "Configuration"  took 0:00:00:00.001
2020-01-09 20:45:01 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteReader]: Starting - ExecuteReader: SELECT "Key", "Value" FROM "Configuration" 
2020-01-09 20:45:01 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteReader]: ExecuteReader: SELECT "Key", "Value" FROM "Configuration"  took 0:00:00:00.005
2020-01-09 20:45:01 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalarInt64]: Starting - ExecuteScalarInt64: SELECT COUNT(*) FROM ( SELECT DISTINCT c1 FROM (SELECT COUNT(*) AS "C1" FROM (SELECT DISTINCT "BlocksetID" FROM "Metadataset") UNION SELECT COUNT(*) AS "C1" FROM "Metadataset" ))
2020-01-09 20:45:01 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalarInt64]: ExecuteScalarInt64: SELECT COUNT(*) FROM ( SELECT DISTINCT c1 FROM (SELECT COUNT(*) AS "C1" FROM (SELECT DISTINCT "BlocksetID" FROM "Metadataset") UNION SELECT COUNT(*) AS "C1" FROM "Metadataset" )) took 0:00:00:00.001
2020-01-09 20:45:01 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalarInt64]: Starting - ExecuteScalarInt64: SELECT COUNT(*) FROM (SELECT "PrefixID", "Path", "BlocksetID", "MetadataID", COUNT(*) as "Duplicates" FROM "FileLookup" GROUP BY "PrefixID", "Path", "BlocksetID", "MetadataID") WHERE "Duplicates" > 1
2020-01-09 20:45:01 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalarInt64]: ExecuteScalarInt64: SELECT COUNT(*) FROM (SELECT "PrefixID", "Path", "BlocksetID", "MetadataID", COUNT(*) as "Duplicates" FROM "FileLookup" GROUP BY "PrefixID", "Path", "BlocksetID", "MetadataID") WHERE "Duplicates" > 1 took 0:00:00:00.001
2020-01-09 20:45:01 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalarInt64]: Starting - ExecuteScalarInt64: SELECT COUNT(*) FROM (SELECT * FROM (SELECT "BlocksetID", "Index", COUNT(*) AS "EC" FROM "BlocklistHash" GROUP BY "BlocksetID", "Index") WHERE "EC" > 1)
2020-01-09 20:45:01 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalarInt64]: ExecuteScalarInt64: SELECT COUNT(*) FROM (SELECT * FROM (SELECT "BlocksetID", "Index", COUNT(*) AS "EC" FROM "BlocklistHash" GROUP BY "BlocksetID", "Index") WHERE "EC" > 1) took 0:00:00:00.000
2020-01-09 20:45:01 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalarInt64]: Starting - ExecuteScalarInt64: SELECT COUNT(*) FROM (SELECT * FROM (SELECT "N"."BlocksetID", (("N"."BlockCount" + 3200 - 1) / 3200) AS "BlocklistHashCountExpected", CASE WHEN "G"."BlocklistHashCount" IS NULL THEN 0 ELSE "G"."BlocklistHashCount" END AS "BlocklistHashCountActual" FROM (SELECT "BlocksetID", COUNT(*) AS "BlockCount" FROM "BlocksetEntry" GROUP BY "BlocksetID") "N" LEFT OUTER JOIN (SELECT "BlocksetID", COUNT(*) AS "BlocklistHashCount" FROM "BlocklistHash" GROUP BY "BlocksetID") "G" ON "N"."BlocksetID" = "G"."BlocksetID" WHERE "N"."BlockCount" > 1) WHERE "BlocklistHashCountExpected" != "BlocklistHashCountActual")
2020-01-09 20:45:01 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalarInt64]: ExecuteScalarInt64: SELECT COUNT(*) FROM (SELECT * FROM (SELECT "N"."BlocksetID", (("N"."BlockCount" + 3200 - 1) / 3200) AS "BlocklistHashCountExpected", CASE WHEN "G"."BlocklistHashCount" IS NULL THEN 0 ELSE "G"."BlocklistHashCount" END AS "BlocklistHashCountActual" FROM (SELECT "BlocksetID", COUNT(*) AS "BlockCount" FROM "BlocksetEntry" GROUP BY "BlocksetID") "N" LEFT OUTER JOIN (SELECT "BlocksetID", COUNT(*) AS "BlocklistHashCount" FROM "BlocklistHash" GROUP BY "BlocksetID") "G" ON "N"."BlocksetID" = "G"."BlocksetID" WHERE "N"."BlockCount" > 1) WHERE "BlocklistHashCountExpected" != "BlocklistHashCountActual") took 0:00:00:00.000
2020-01-09 20:45:01 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteNonQuery]: Starting - ExecuteNonQuery: PRAGMA optimize
2020-01-09 20:45:01 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteNonQuery]: ExecuteNonQuery: PRAGMA optimize took 0:00:00:00.000
2020-01-09 20:45:01 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalarInt64]: Starting - ExecuteScalarInt64: INSERT INTO "Operation" ("Description", "Timestamp") VALUES ("Repair", 1578620701); SELECT last_insert_rowid();
2020-01-09 20:45:01 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalarInt64]: ExecuteScalarInt64: INSERT INTO "Operation" ("Description", "Timestamp") VALUES ("Repair", 1578620701); SELECT last_insert_rowid(); took 0:00:00:00.104
2020-01-09 20:45:01 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteReader]: Starting - ExecuteReader: SELECT "Key", "Value" FROM "Configuration" 
2020-01-09 20:45:01 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteReader]: ExecuteReader: SELECT "Key", "Value" FROM "Configuration"  took 0:00:00:00.000
2020-01-09 20:45:01 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteReader]: Starting - ExecuteReader: SELECT "Key", "Value" FROM "Configuration" 
2020-01-09 20:45:01 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteReader]: ExecuteReader: SELECT "Key", "Value" FROM "Configuration"  took 0:00:00:00.001
2020-01-09 20:45:01 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalarInt64]: Starting - ExecuteScalarInt64: SELECT COUNT(*) FROM "Block" WHERE "Size" > 102400
2020-01-09 20:45:01 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalarInt64]: ExecuteScalarInt64: SELECT COUNT(*) FROM "Block" WHERE "Size" > 102400 took 0:00:00:00.000
2020-01-09 20:45:01 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteReader]: Starting - ExecuteReader: SELECT "Key", "Value" FROM "Configuration" 
2020-01-09 20:45:01 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteReader]: ExecuteReader: SELECT "Key", "Value" FROM "Configuration"  took 0:00:00:00.000
2020-01-09 20:45:01 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteReader]: Starting - ExecuteReader: SELECT "Key", "Value" FROM "Configuration" 
2020-01-09 20:45:01 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteReader]: ExecuteReader: SELECT "Key", "Value" FROM "Configuration"  took 0:00:00:00.000
2020-01-09 20:45:01 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.BackendManager-RemoteOperationList]: Starting - RemoteOperationList
2020-01-09 20:45:01 -05 - [Information-Duplicati.Library.Main.BasicResults-BackendEvent]: Backend event: List - Started:  ()
2020-01-09 20:45:01 -05 - [Information-Duplicati.Library.Main.BasicResults-BackendEvent]: Backend event: List - Completed:  (2 bytes)
2020-01-09 20:45:01 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.BackendManager-RemoteOperationList]: RemoteOperationList took 0:00:00:00.014
2020-01-09 20:45:01 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteReader]: Starting - ExecuteReader: SELECT "ID", "IsFullBackup", "Timestamp" FROM "Fileset" ORDER BY "Timestamp" DESC
2020-01-09 20:45:01 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteReader]: ExecuteReader: SELECT "ID", "IsFullBackup", "Timestamp" FROM "Fileset" ORDER BY "Timestamp" DESC took 0:00:00:00.000
2020-01-09 20:45:01 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteReader]: Starting - ExecuteReader: SELECT DISTINCT "Name", "State" FROM "Remotevolume" WHERE "Name" IN (SELECT "Name" FROM "Remotevolume" WHERE "State" IN ("Deleted", "Deleting")) AND NOT "State" IN ("Deleted", "Deleting")
2020-01-09 20:45:01 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteReader]: ExecuteReader: SELECT DISTINCT "Name", "State" FROM "Remotevolume" WHERE "Name" IN (SELECT "Name" FROM "Remotevolume" WHERE "State" IN ("Deleted", "Deleting")) AND NOT "State" IN ("Deleted", "Deleting") took 0:00:00:00.000
2020-01-09 20:45:01 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteReader]: Starting - ExecuteReader: SELECT "Name", "Hash", "Size" FROM "RemoteVolume" WHERE "ID" IN (SELECT "BlockVolumeID" FROM "IndexBlockLink" WHERE "IndexVolumeID" IN (SELECT "ID" FROM "RemoteVolume" WHERE "Name" = "duplicati-iba0b55f4455141b8a4f9bd0c37533437.dindex.zip"))
2020-01-09 20:45:01 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteReader]: ExecuteReader: SELECT "Name", "Hash", "Size" FROM "RemoteVolume" WHERE "ID" IN (SELECT "BlockVolumeID" FROM "IndexBlockLink" WHERE "IndexVolumeID" IN (SELECT "ID" FROM "RemoteVolume" WHERE "Name" = "duplicati-iba0b55f4455141b8a4f9bd0c37533437.dindex.zip")) took 0:00:00:00.001
2020-01-09 20:45:01 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalarInt64]: Starting - ExecuteScalarInt64: SELECT "ID" FROM "Remotevolume" WHERE "Name" = "duplicati-b8120a8f475f1466aa9b218c74f2c688e.dblock.zip"
2020-01-09 20:45:01 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalarInt64]: ExecuteScalarInt64: SELECT "ID" FROM "Remotevolume" WHERE "Name" = "duplicati-b8120a8f475f1466aa9b218c74f2c688e.dblock.zip" took 0:00:00:00.000
2020-01-09 20:45:01 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteReader]: Starting - ExecuteReader: SELECT DISTINCT "Hash", "Size" FROM "Block" WHERE "VolumeID" = 2
2020-01-09 20:45:01 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteReader]: ExecuteReader: SELECT DISTINCT "Hash", "Size" FROM "Block" WHERE "VolumeID" = 2 took 0:00:00:00.000
2020-01-09 20:45:01 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteReader]: Starting - ExecuteReader: SELECT "A"."Hash", "C"."Hash" FROM (SELECT "BlocklistHash"."BlocksetID", "Block"."Hash", * FROM  "BlocklistHash","Block" WHERE  "BlocklistHash"."Hash" = "Block"."Hash" AND "Block"."VolumeID" = 2) A,  "BlocksetEntry" B, "Block" C WHERE "B"."BlocksetID" = "A"."BlocksetID" AND  "B"."Index" >= ("A"."Index" * 3200) AND "B"."Index" < (("A"."Index" + 1) * 3200) AND "C"."ID" = "B"."BlockID"  ORDER BY "A"."BlocksetID", "B"."Index"
2020-01-09 20:45:01 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteReader]: ExecuteReader: SELECT "A"."Hash", "C"."Hash" FROM (SELECT "BlocklistHash"."BlocksetID", "Block"."Hash", * FROM  "BlocklistHash","Block" WHERE  "BlocklistHash"."Hash" = "Block"."Hash" AND "Block"."VolumeID" = 2) A,  "BlocksetEntry" B, "Block" C WHERE "B"."BlocksetID" = "A"."BlocksetID" AND  "B"."Index" >= ("A"."Index" * 3200) AND "B"."Index" < (("A"."Index" + 1) * 3200) AND "C"."ID" = "B"."BlockID"  ORDER BY "A"."BlocksetID", "B"."Index" took 0:00:00:00.000
2020-01-09 20:45:02 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.BackendManager-RemoteOperationPut]: Starting - RemoteOperationPut
2020-01-09 20:45:02 -05 - [Information-Duplicati.Library.Main.BasicResults-BackendEvent]: Backend event: Put - Started: duplicati-iba0b55f4455141b8a4f9bd0c37533437.dindex.zip (610 bytes)
2020-01-09 20:45:02 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.BackendManager-RemoteOperationPut]: RemoteOperationPut took 0:00:00:00.004
2020-01-09 20:45:02 -05 - [Retry-Duplicati.Library.Main.BackendManager-RetryPut]: Operation Put with file duplicati-iba0b55f4455141b8a4f9bd0c37533437.dindex.zip attempt 1 of 5 failed with message: Path cannot be null.
Parameter name: path
System.ArgumentNullException: Path cannot be null.
Parameter name: path
   at System.IO.FileStream.Init(String path, FileMode mode, FileAccess access, Int32 rights, Boolean useRights, FileShare share, Int32 bufferSize, FileOptions options, SECURITY_ATTRIBUTES secAttrs, String msgPath, Boolean bFromProxy, Boolean useLongPath, Boolean checkHost)
   at System.IO.FileStream..ctor(String path, FileMode mode, FileAccess access, FileShare share)
   at Duplicati.Library.Main.BackendManager.DoPut(FileEntryItem item)
   at Duplicati.Library.Main.BackendManager.ThreadRun()
2020-01-09 20:45:02 -05 - [Information-Duplicati.Library.Main.BasicResults-BackendEvent]: Backend event: Put - Retrying: duplicati-iba0b55f4455141b8a4f9bd0c37533437.dindex.zip (610 bytes)
2020-01-09 20:45:02 -05 - [Information-Duplicati.Library.Main.BasicResults-BackendEvent]: Backend event: Put - Rename: duplicati-iba0b55f4455141b8a4f9bd0c37533437.dindex.zip (610 bytes)
2020-01-09 20:45:02 -05 - [Information-Duplicati.Library.Main.BasicResults-BackendEvent]: Backend event: Put - Rename: duplicati-i7665e4507c1f4cd79369ae5f3cf9ebfe.dindex.zip (610 bytes)
2020-01-09 20:45:02 -05 - [Information-Duplicati.Library.Main.BackendManager-RenameRemoteTargetFile]: Renaming "duplicati-iba0b55f4455141b8a4f9bd0c37533437.dindex.zip" to "duplicati-i7665e4507c1f4cd79369ae5f3cf9ebfe.dindex.zip"
2020-01-09 20:45:12 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.BackendManager-RemoteOperationPut]: Starting - RemoteOperationPut
2020-01-09 20:45:12 -05 - [Information-Duplicati.Library.Main.BasicResults-BackendEvent]: Backend event: Put - Started: duplicati-i7665e4507c1f4cd79369ae5f3cf9ebfe.dindex.zip (610 bytes)
2020-01-09 20:45:12 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.BackendManager-RemoteOperationPut]: RemoteOperationPut took 0:00:00:00.000
2020-01-09 20:45:12 -05 - [Retry-Duplicati.Library.Main.BackendManager-RetryPut]: Operation Put with file duplicati-i7665e4507c1f4cd79369ae5f3cf9ebfe.dindex.zip attempt 2 of 5 failed with message: Path cannot be null.
Parameter name: path
System.ArgumentNullException: Path cannot be null.
Parameter name: path
   at System.IO.FileStream.Init(String path, FileMode mode, FileAccess access, Int32 rights, Boolean useRights, FileShare share, Int32 bufferSize, FileOptions options, SECURITY_ATTRIBUTES secAttrs, String msgPath, Boolean bFromProxy, Boolean useLongPath, Boolean checkHost)
   at System.IO.FileStream..ctor(String path, FileMode mode, FileAccess access, FileShare share)
   at Duplicati.Library.Main.BackendManager.DoPut(FileEntryItem item)
   at Duplicati.Library.Main.BackendManager.ThreadRun()
2020-01-09 20:45:12 -05 - [Information-Duplicati.Library.Main.BasicResults-BackendEvent]: Backend event: Put - Retrying: duplicati-i7665e4507c1f4cd79369ae5f3cf9ebfe.dindex.zip (610 bytes)
2020-01-09 20:45:12 -05 - [Information-Duplicati.Library.Main.BasicResults-BackendEvent]: Backend event: Put - Rename: duplicati-i7665e4507c1f4cd79369ae5f3cf9ebfe.dindex.zip (610 bytes)
2020-01-09 20:45:12 -05 - [Information-Duplicati.Library.Main.BasicResults-BackendEvent]: Backend event: Put - Rename: duplicati-i500877fbf8ed474c86e1a4fa4956706d.dindex.zip (610 bytes)
2020-01-09 20:45:12 -05 - [Information-Duplicati.Library.Main.BackendManager-RenameRemoteTargetFile]: Renaming "duplicati-i7665e4507c1f4cd79369ae5f3cf9ebfe.dindex.zip" to "duplicati-i500877fbf8ed474c86e1a4fa4956706d.dindex.zip"
2020-01-09 20:45:22 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.BackendManager-RemoteOperationPut]: Starting - RemoteOperationPut
2020-01-09 20:45:22 -05 - [Information-Duplicati.Library.Main.BasicResults-BackendEvent]: Backend event: Put - Started: duplicati-i500877fbf8ed474c86e1a4fa4956706d.dindex.zip (610 bytes)
2020-01-09 20:45:22 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.BackendManager-RemoteOperationPut]: RemoteOperationPut took 0:00:00:00.000
2020-01-09 20:45:22 -05 - [Retry-Duplicati.Library.Main.BackendManager-RetryPut]: Operation Put with file duplicati-i500877fbf8ed474c86e1a4fa4956706d.dindex.zip attempt 3 of 5 failed with message: Path cannot be null.
Parameter name: path
System.ArgumentNullException: Path cannot be null.
Parameter name: path
   at System.IO.FileStream.Init(String path, FileMode mode, FileAccess access, Int32 rights, Boolean useRights, FileShare share, Int32 bufferSize, FileOptions options, SECURITY_ATTRIBUTES secAttrs, String msgPath, Boolean bFromProxy, Boolean useLongPath, Boolean checkHost)
   at System.IO.FileStream..ctor(String path, FileMode mode, FileAccess access, FileShare share)
   at Duplicati.Library.Main.BackendManager.DoPut(FileEntryItem item)
   at Duplicati.Library.Main.BackendManager.ThreadRun()
2020-01-09 20:45:22 -05 - [Information-Duplicati.Library.Main.BasicResults-BackendEvent]: Backend event: Put - Retrying: duplicati-i500877fbf8ed474c86e1a4fa4956706d.dindex.zip (610 bytes)
2020-01-09 20:45:22 -05 - [Information-Duplicati.Library.Main.BasicResults-BackendEvent]: Backend event: Put - Rename: duplicati-i500877fbf8ed474c86e1a4fa4956706d.dindex.zip (610 bytes)
2020-01-09 20:45:22 -05 - [Information-Duplicati.Library.Main.BasicResults-BackendEvent]: Backend event: Put - Rename: duplicati-iba13c08243704e77b9f3dd1f8d0e0421.dindex.zip (610 bytes)
2020-01-09 20:45:22 -05 - [Information-Duplicati.Library.Main.BackendManager-RenameRemoteTargetFile]: Renaming "duplicati-i500877fbf8ed474c86e1a4fa4956706d.dindex.zip" to "duplicati-iba13c08243704e77b9f3dd1f8d0e0421.dindex.zip"
2020-01-09 20:45:32 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.BackendManager-RemoteOperationPut]: Starting - RemoteOperationPut
2020-01-09 20:45:32 -05 - [Information-Duplicati.Library.Main.BasicResults-BackendEvent]: Backend event: Put - Started: duplicati-iba13c08243704e77b9f3dd1f8d0e0421.dindex.zip (610 bytes)
2020-01-09 20:45:32 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.BackendManager-RemoteOperationPut]: RemoteOperationPut took 0:00:00:00.000
2020-01-09 20:45:32 -05 - [Retry-Duplicati.Library.Main.BackendManager-RetryPut]: Operation Put with file duplicati-iba13c08243704e77b9f3dd1f8d0e0421.dindex.zip attempt 4 of 5 failed with message: Path cannot be null.
Parameter name: path
System.ArgumentNullException: Path cannot be null.
Parameter name: path
   at System.IO.FileStream.Init(String path, FileMode mode, FileAccess access, Int32 rights, Boolean useRights, FileShare share, Int32 bufferSize, FileOptions options, SECURITY_ATTRIBUTES secAttrs, String msgPath, Boolean bFromProxy, Boolean useLongPath, Boolean checkHost)
   at System.IO.FileStream..ctor(String path, FileMode mode, FileAccess access, FileShare share)
   at Duplicati.Library.Main.BackendManager.DoPut(FileEntryItem item)
   at Duplicati.Library.Main.BackendManager.ThreadRun()
2020-01-09 20:45:32 -05 - [Information-Duplicati.Library.Main.BasicResults-BackendEvent]: Backend event: Put - Retrying: duplicati-iba13c08243704e77b9f3dd1f8d0e0421.dindex.zip (610 bytes)
2020-01-09 20:45:32 -05 - [Information-Duplicati.Library.Main.BasicResults-BackendEvent]: Backend event: Put - Rename: duplicati-iba13c08243704e77b9f3dd1f8d0e0421.dindex.zip (610 bytes)
2020-01-09 20:45:32 -05 - [Information-Duplicati.Library.Main.BasicResults-BackendEvent]: Backend event: Put - Rename: duplicati-idc39ea83786a43cc87a151452d3a1d91.dindex.zip (610 bytes)
2020-01-09 20:45:32 -05 - [Information-Duplicati.Library.Main.BackendManager-RenameRemoteTargetFile]: Renaming "duplicati-iba13c08243704e77b9f3dd1f8d0e0421.dindex.zip" to "duplicati-idc39ea83786a43cc87a151452d3a1d91.dindex.zip"
2020-01-09 20:45:42 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.BackendManager-RemoteOperationPut]: Starting - RemoteOperationPut
2020-01-09 20:45:42 -05 - [Information-Duplicati.Library.Main.BasicResults-BackendEvent]: Backend event: Put - Started: duplicati-idc39ea83786a43cc87a151452d3a1d91.dindex.zip (610 bytes)
2020-01-09 20:45:42 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.BackendManager-RemoteOperationPut]: RemoteOperationPut took 0:00:00:00.000
2020-01-09 20:45:42 -05 - [Retry-Duplicati.Library.Main.BackendManager-RetryPut]: Operation Put with file duplicati-idc39ea83786a43cc87a151452d3a1d91.dindex.zip attempt 5 of 5 failed with message: Path cannot be null.
Parameter name: path
System.ArgumentNullException: Path cannot be null.
Parameter name: path
   at System.IO.FileStream.Init(String path, FileMode mode, FileAccess access, Int32 rights, Boolean useRights, FileShare share, Int32 bufferSize, FileOptions options, SECURITY_ATTRIBUTES secAttrs, String msgPath, Boolean bFromProxy, Boolean useLongPath, Boolean checkHost)
   at System.IO.FileStream..ctor(String path, FileMode mode, FileAccess access, FileShare share)
   at Duplicati.Library.Main.BackendManager.DoPut(FileEntryItem item)
   at Duplicati.Library.Main.BackendManager.ThreadRun()
2020-01-09 20:45:42 -05 - [Information-Duplicati.Library.Main.BasicResults-BackendEvent]: Backend event: Put - Failed: duplicati-idc39ea83786a43cc87a151452d3a1d91.dindex.zip (610 bytes)
2020-01-09 20:45:42 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteNonQuery]: Starting - ExecuteNonQuery: UPDATE "Remotevolume" SET "Name" = "duplicati-i7665e4507c1f4cd79369ae5f3cf9ebfe.dindex.zip" WHERE "Name" = "duplicati-iba0b55f4455141b8a4f9bd0c37533437.dindex.zip"
2020-01-09 20:45:42 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteNonQuery]: ExecuteNonQuery: UPDATE "Remotevolume" SET "Name" = "duplicati-i7665e4507c1f4cd79369ae5f3cf9ebfe.dindex.zip" WHERE "Name" = "duplicati-iba0b55f4455141b8a4f9bd0c37533437.dindex.zip" took 0:00:00:00.000
2020-01-09 20:45:42 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalar]: Starting - ExecuteScalar: SELECT "Type" FROM "Remotevolume" WHERE "Name" = "duplicati-i7665e4507c1f4cd79369ae5f3cf9ebfe.dindex.zip"
2020-01-09 20:45:42 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalar]: ExecuteScalar: SELECT "Type" FROM "Remotevolume" WHERE "Name" = "duplicati-i7665e4507c1f4cd79369ae5f3cf9ebfe.dindex.zip" took 0:00:00:00.000
2020-01-09 20:45:42 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalarInt64]: Starting - ExecuteScalarInt64: INSERT INTO "Remotevolume" ("OperationID", "Name", "Type", "State", "Size", "VerificationCount", "DeleteGraceTime") VALUES (10, "duplicati-iba0b55f4455141b8a4f9bd0c37533437.dindex.zip", "Index", "Deleting", -1, 0, 0); SELECT last_insert_rowid();
2020-01-09 20:45:42 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalarInt64]: ExecuteScalarInt64: INSERT INTO "Remotevolume" ("OperationID", "Name", "Type", "State", "Size", "VerificationCount", "DeleteGraceTime") VALUES (10, "duplicati-iba0b55f4455141b8a4f9bd0c37533437.dindex.zip", "Index", "Deleting", -1, 0, 0); SELECT last_insert_rowid(); took 0:00:00:00.000
2020-01-09 20:45:42 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteNonQuery]: Starting - ExecuteNonQuery: UPDATE "Remotevolume" SET "Name" = "duplicati-i500877fbf8ed474c86e1a4fa4956706d.dindex.zip" WHERE "Name" = "duplicati-i7665e4507c1f4cd79369ae5f3cf9ebfe.dindex.zip"
2020-01-09 20:45:42 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteNonQuery]: ExecuteNonQuery: UPDATE "Remotevolume" SET "Name" = "duplicati-i500877fbf8ed474c86e1a4fa4956706d.dindex.zip" WHERE "Name" = "duplicati-i7665e4507c1f4cd79369ae5f3cf9ebfe.dindex.zip" took 0:00:00:00.018
2020-01-09 20:45:42 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalar]: Starting - ExecuteScalar: SELECT "Type" FROM "Remotevolume" WHERE "Name" = "duplicati-i500877fbf8ed474c86e1a4fa4956706d.dindex.zip"
2020-01-09 20:45:42 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalar]: ExecuteScalar: SELECT "Type" FROM "Remotevolume" WHERE "Name" = "duplicati-i500877fbf8ed474c86e1a4fa4956706d.dindex.zip" took 0:00:00:00.000
2020-01-09 20:45:42 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalarInt64]: Starting - ExecuteScalarInt64: INSERT INTO "Remotevolume" ("OperationID", "Name", "Type", "State", "Size", "VerificationCount", "DeleteGraceTime") VALUES (10, "duplicati-i7665e4507c1f4cd79369ae5f3cf9ebfe.dindex.zip", "Index", "Deleting", -1, 0, 0); SELECT last_insert_rowid();
2020-01-09 20:45:42 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalarInt64]: ExecuteScalarInt64: INSERT INTO "Remotevolume" ("OperationID", "Name", "Type", "State", "Size", "VerificationCount", "DeleteGraceTime") VALUES (10, "duplicati-i7665e4507c1f4cd79369ae5f3cf9ebfe.dindex.zip", "Index", "Deleting", -1, 0, 0); SELECT last_insert_rowid(); took 0:00:00:00.000
2020-01-09 20:45:43 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteNonQuery]: Starting - ExecuteNonQuery: UPDATE "Remotevolume" SET "Name" = "duplicati-iba13c08243704e77b9f3dd1f8d0e0421.dindex.zip" WHERE "Name" = "duplicati-i500877fbf8ed474c86e1a4fa4956706d.dindex.zip"
2020-01-09 20:45:43 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteNonQuery]: ExecuteNonQuery: UPDATE "Remotevolume" SET "Name" = "duplicati-iba13c08243704e77b9f3dd1f8d0e0421.dindex.zip" WHERE "Name" = "duplicati-i500877fbf8ed474c86e1a4fa4956706d.dindex.zip" took 0:00:00:00.000
2020-01-09 20:45:43 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalar]: Starting - ExecuteScalar: SELECT "Type" FROM "Remotevolume" WHERE "Name" = "duplicati-iba13c08243704e77b9f3dd1f8d0e0421.dindex.zip"
2020-01-09 20:45:43 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalar]: ExecuteScalar: SELECT "Type" FROM "Remotevolume" WHERE "Name" = "duplicati-iba13c08243704e77b9f3dd1f8d0e0421.dindex.zip" took 0:00:00:00.000
2020-01-09 20:45:43 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalarInt64]: Starting - ExecuteScalarInt64: INSERT INTO "Remotevolume" ("OperationID", "Name", "Type", "State", "Size", "VerificationCount", "DeleteGraceTime") VALUES (10, "duplicati-i500877fbf8ed474c86e1a4fa4956706d.dindex.zip", "Index", "Deleting", -1, 0, 0); SELECT last_insert_rowid();
2020-01-09 20:45:43 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalarInt64]: ExecuteScalarInt64: INSERT INTO "Remotevolume" ("OperationID", "Name", "Type", "State", "Size", "VerificationCount", "DeleteGraceTime") VALUES (10, "duplicati-i500877fbf8ed474c86e1a4fa4956706d.dindex.zip", "Index", "Deleting", -1, 0, 0); SELECT last_insert_rowid(); took 0:00:00:00.000
2020-01-09 20:45:43 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteNonQuery]: Starting - ExecuteNonQuery: UPDATE "Remotevolume" SET "Name" = "duplicati-idc39ea83786a43cc87a151452d3a1d91.dindex.zip" WHERE "Name" = "duplicati-iba13c08243704e77b9f3dd1f8d0e0421.dindex.zip"
2020-01-09 20:45:43 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteNonQuery]: ExecuteNonQuery: UPDATE "Remotevolume" SET "Name" = "duplicati-idc39ea83786a43cc87a151452d3a1d91.dindex.zip" WHERE "Name" = "duplicati-iba13c08243704e77b9f3dd1f8d0e0421.dindex.zip" took 0:00:00:00.001
2020-01-09 20:45:43 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalar]: Starting - ExecuteScalar: SELECT "Type" FROM "Remotevolume" WHERE "Name" = "duplicati-idc39ea83786a43cc87a151452d3a1d91.dindex.zip"
2020-01-09 20:45:43 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalar]: ExecuteScalar: SELECT "Type" FROM "Remotevolume" WHERE "Name" = "duplicati-idc39ea83786a43cc87a151452d3a1d91.dindex.zip" took 0:00:00:00.000
2020-01-09 20:45:43 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalarInt64]: Starting - ExecuteScalarInt64: INSERT INTO "Remotevolume" ("OperationID", "Name", "Type", "State", "Size", "VerificationCount", "DeleteGraceTime") VALUES (10, "duplicati-iba13c08243704e77b9f3dd1f8d0e0421.dindex.zip", "Index", "Deleting", -1, 0, 0); SELECT last_insert_rowid();
2020-01-09 20:45:43 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteScalarInt64]: ExecuteScalarInt64: INSERT INTO "Remotevolume" ("OperationID", "Name", "Type", "State", "Size", "VerificationCount", "DeleteGraceTime") VALUES (10, "duplicati-iba13c08243704e77b9f3dd1f8d0e0421.dindex.zip", "Index", "Deleting", -1, 0, 0); SELECT last_insert_rowid(); took 0:00:00:00.000
2020-01-09 20:45:43 -05 - [Profiling-Timer.Begin-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteNonQuery]: Starting - ExecuteNonQuery: PRAGMA optimize
2020-01-09 20:45:43 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Database.ExtensionMethods-ExecuteNonQuery]: ExecuteNonQuery: PRAGMA optimize took 0:00:00:00.000
2020-01-09 20:45:43 -05 - [Profiling-Timer.Finished-Duplicati.Library.Main.Controller-RunRepair]: Running Repair took 0:00:00:42.347
```

<!-- Please search to see if an issue has already been created for your report. -->
<!-- Replace the empty checkbox [ ] below with a checked one [x] if you already searched for duplicate bugs. -->
- [x ] I have searched open and closed issues for duplicates.

----------------------------------------

## Environment info
- **Duplicati version**: 2.0.5.0_experimental
- **Operating system**: Windows 10 Pro
- **Backend**: Local Minio (version RELEASE.2020-01-03T19-12-21Z).

## Description
<!-- Describe the issue that you are experiencing below. -->
Denied access to System Volume Information is logged as a Warning. 

Related to the old [VSS: access denied for "System Volume Information" #3015](https://github.com/duplicati/duplicati/issues/3015), mentioned in the merged [Fix default filters in server / UI #3023](https://github.com/duplicati/duplicati/pull/3023), 


## Steps to reproduce
<!-- List the steps to reproduce the bug if possible. -->
1. Create or edit a backup in WebGUI.
2. Select a entire partion to backup - eg. D:\
3. Start backup.

<!-- For "Actual result", describe what happens after you run the steps above (i.e. buggy behavior). -->
<!-- For "Expected result", describe what should happen after you run the steps above (i.e. corrected behavior). -->
- **Actual result**:
Got a Warning.
- **Expected result**:
No warning, since this system folder is expected to be excluded.

## Debug log
<!-- Posting a debug log helps the developers find and fix your particular issue more easily. -->
<!-- Please wrap your code in code blocks with triple back-ticks to increase readability. -->
`2020-01-09 15.17.34 +01 - [Warning-Duplicati.Library.Main.Operation.Backup.FileEnumerationProcess-FileAccessError]: Error reported while accessing file: D:\System Volume Information\`


- [x] I have searched open and closed issues for duplicates.

----------------------------------------

## Environment info
<!-- Please include some relevant information about your environment. -->
<!-- For "Backend", please indicate the backup destination (e.g. Amazon S3, OneDrive, FTP, WebDAV, local). -->
- **Duplicati version**: v2.0.4.38-2.0.4.38_canary_2019-12-29 
- **Operating system**: Ubuntu 18.04
- **Backend**: local

## Description
I was running RecoveryTool restore and after it extracted about 460 gb of data split accross 8 files over a couple of days it stopped working with lots of error messages like this:

```
Failed to read Blocklist hash: dyJVHkdVVfs+RKNXzvCkVwi8ZbFGMjoLDYq5OjguwIY=                                                                                                                                                                        
System.IO.IOException: Too many open files                                                                                                                                                                                                            
  at System.IO.FileStream..ctor (System.String path, System.IO.FileMode mode, System.IO.FileAccess access, System.IO.FileShare share, System.Int32 bufferSize, System.Boolean anonymous, System.IO.FileOptions options) [0x0019e] in <d2ec5c92492f4d6b
a8c422bdf574b786>:0                                                                                                                                                                                                                                   
  at System.IO.FileStream..ctor (System.String path, System.IO.FileMode mode, System.IO.FileAccess access, System.IO.FileShare share) [0x00000] in <d2ec5c92492f4d6ba8c422bdf574b786>:0                                                               
  at (wrapper remoting-invoke-with-check) System.IO.FileStream..ctor(string,System.IO.FileMode,System.IO.FileAccess,System.IO.FileShare)                                                                                                              
  at Duplicati.CommandLine.RecoveryTool.Restore+CompressedFileMRUCache.ReadBlock (System.String filename, System.String hash) [0x00020] in <816ceacbda9946529228fd14428c2fcd>:0                                                                       
  at Duplicati.CommandLine.RecoveryTool.Restore+HashLookupHelper.ReadHash (System.String hash) [0x00068] in <816ceacbda9946529228fd14428c2fcd>:0                                                                                                      
  at Duplicati.CommandLine.RecoveryTool.Restore+HashLookupHelper+<ReadBlocklistHashes>d__12.MoveNext () [0x0001e] in <816ceacbda9946529228fd14428c2fcd>:0                                                                                             
  at Duplicati.CommandLine.RecoveryTool.Restore.Run (System.Collections.Generic.List`1[T] args, System.Collections.Generic.Dictionary`2[TKey,TValue] options, Duplicati.Library.Utility.IFilter filter) [0x004c2] in <816ceacbda9946529228fd14428c2fcd
>:0       
```
