I am current ly implementing the filter in capture Photo delegate as below 


When it comes to the implementation, the filter is not working. 
    
```
    @IBAction func capturePhoto(_ sender: Any) {
        
        // stop text recognition
        cameraSession.stopRunning()
        
        //start filter
        videoFilter = CoreImageVideoFilter(superview: view, applyFilterCallback: nil)
        // Simulate a tap on the mode selector to start the process

        if let videoFilter = videoFilter {
            videoFilter.stopFiltering()
            detector = prepareRectangleDetector()
            videoFilter.applyFilter = {
                image in
                self.resultCIIMage = self.performRectangleDetection(image)!
                return self.performRectangleDetection(image)
            }
            
            videoFilter.startFiltering(currentSession: cameraSession)
        }
        
        
        
        cameraSession.beginConfiguration()
        cameraSession.sessionPreset = AVCaptureSessionPresetPhoto
        let device : AVCaptureDevice = AVCaptureDevice.defaultDevice(withMediaType: AVMediaTypeVideo)
        do {
            let captureDeviceInput = try AVCaptureDeviceInput(device: device)
            if cameraSession.canAddInput(captureDeviceInput) {
                cameraSession.addInput(captureDeviceInput)
            }
        }
        catch {
            print("Error occured \(error)")
            return
        }
        if(device.isFocusModeSupported(.continuousAutoFocus)) {
            try! device.lockForConfiguration()
            device.focusMode = .continuousAutoFocus
            device.unlockForConfiguration()
        }
        runStillImageCaptureAnimation()
        cameraSession.addOutput(cameraOutput)
        cameraSession.commitConfiguration()
        
        let photoSettings = AVCapturePhotoSettings()
        photoSettings.flashMode = .on
        photoSettings.isHighResolutionPhotoEnabled = true
        
        if photoSettings.__availablePreviewPhotoPixelFormatTypes.count > 0 {
            photoSettings.previewPhotoFormat = [ kCVPixelBufferPixelFormatTypeKey as String : photoSettings.__availablePreviewPhotoPixelFormatTypes.first!]
            
        }
        
        cameraSession.startRunning()
        print("go this")
        cameraOutput.isHighResolutionCaptureEnabled = true
        cameraOutput.capturePhoto(with: photoSettings, delegate: self)
        //  print("go that")
        //   cameraSession.stopRunning()
    }
    
    
    func runStillImageCaptureAnimation(){
        DispatchQueue.main.async {
            self.preview.layer.opacity = 0.0
            print("opacity 0")
            UIView.animate(withDuration: 1.0) {
                self.preview.layer.opacity = 1.0
                print("opacity 1")
            }
        }
    }
    
    func capture(_ captureOutput: AVCapturePhotoOutput, didFinishProcessingPhotoSampleBuffer photoSampleBuffer: CMSampleBuffer?, previewPhotoSampleBuffer: CMSampleBuffer?, resolvedSettings: AVCaptureResolvedPhotoSettings, bracketSettings: AVCaptureBracketedStillImageSettings?, error: Error?) {
        
        print("go threee ")
        
        if let error = error {
            print("error occure : \(error.localizedDescription)")
        }
        
        if  let sampleBuffer = photoSampleBuffer,
            let previewBuffer = previewPhotoSampleBuffer,
            let dataImage =  AVCapturePhotoOutput.jpegPhotoDataRepresentation(forJPEGSampleBuffer:  sampleBuffer, previewPhotoSampleBuffer: previewBuffer) {
            
            let dataProvider = CGDataProvider(data: dataImage as CFData)
            let cgImageRef: CGImage! = CGImage(jpegDataProviderSource: dataProvider!, decode: nil, shouldInterpolate: true, intent: .defaultIntent)
            //let imageX = UIImage(cgImage: cgImageRef, scale: 1.0, orientation: UIImageOrientation.right)
            let image = UIImage(cgImage: cgImageRef, scale: 1.0, orientation: UIImageOrientation.right)
            
            capturedImage = videoFilter?.resuItImage == nil ? image : convert(cmage: resultCIIMage)
            user?.setScannedlist(list: self.scannerdText)
            user?.capImage(captured :  capturedImage!  )
            tesseract?.delegate = nil
            tesseract = nil
            
            
            self.dismiss(animated: false, completion: { Void in self.cameraSession.stopRunning()})
            
        } else {
            print("some errorX  here")
        }
    }
```
In Day 13 CoreImage Detectors, when debug on iPhone iOS 11 always crashes with bad access in videoDisplayView.display().
Without debug mode all works correct, and on iOS 8, 10 too.
I'm trying to use your chroma key filter and right now I have to run two pass on my image, first getting the average color of a small area of the image, then extract the color from the returned image, then pass that color to your chroma filter, then run the chroma filter.

It would be great to be able to chain those two filters. Any idea how i could do that?
Hi, Can I Take image and crop when detect rectangle? Can u help me
/iOS8-day-by-day-master/13-coreimage-detectors/LiveDetection/LiveDetection/CoreImageVideoFilter.swift:89:69: Value of type 'OSType' (aka 'UInt32') does not conform to expected dictionary value type 'AnyObject'

having this problem. does it work for image sharing from photo library

Is there any way to instead of bringing up the menu to click on the elements, to open a Safari web view and have the JS communicate with that window? I have a little bookmarklet that works wonders with your code, but it overrides the parent window. I'd either like to launch the full iOS app to process the URL, or run the JS on that popover dialog in safari.

Is this possible?

Hello!
I've noticed that this example doesn't work on iOS 8.3. See an attachment below. 
![bug](https://cloud.githubusercontent.com/assets/4773042/8129078/d14a2576-111e-11e5-86e5-889b74d6e82d.png)
I checked the issue in emulator and on my iPod, with the same result. On iOS 8.1 and 8.2 everything works as expected.
Any ideas how to fix it?

I'm learning the sample code about how split view works. The code runs fine in iPad, but in iPhone8.1/8.3 the split view doesn't collapse(left screen is part of the master control). I'm using the iOS simulator.

First, I'd like to thank you for the great example. It was very helpful. I have implemented TouchId in my app per your example and it works as expected as long as the deployment is done via cable (local). 

However, it stops working if one deploys via iTunesConnect. In that case it does not seem to retrieve anything anymore?

Can somebody please provide some help or insight? Thoughts?  Thanks in advance.
