How can I generate an ROC curve using your code? I see that there is a ROC function in metrics.py, but not sure how to extract raw values that'll give me the curve. 
Hi, the official master program has been downloaded and I tried to evaluate the performance of SIFT feature on MATLAB. When typed into the command "hb retrieval sift", an error threw out:
Evaluating "retrieval" for sift[                    ]   0% --:--:--Error using sub2ind (line 52)
Out of range subscript.

Error in desc.memdesc>getdesc (line 98)
sel = sub2ind([size(obj.data, 2), size(obj.data, 3)], idxs_o, gni);

Error in desc.memdesc>@(obj,varargin)getdesc(obj,varargin{:}) (line 48)
obj.getdesc = @(obj, varargin) getdesc(obj, varargin{:});

Error in bench.retrieval.eval (line 24)
[query_desc, query_si] = descs.getdesc(descs, queries.s, geom_noise, ...

Error in bench.retrieval (line 84)
      res_s = bench.retrieval.eval(des, gnoise, queries, distractors, ...

Error in hb (line 61)
  cmds.(cmd).fun(varargin{:});

Such kind of error also occurs when making verification and matching. Could anybody help me with this problem? BTW, I used the original codes and default settings as provided in this website.

Thanks!
 
Hello,

I have tested Hardnet's pretrained model of liberty with augmentation (HardNet+) with python implementation of HPatches. I was able to reproduce results of paper but when I tested with matlab implementation of HPatches to generate graphs I wasn't able to reproduce retrieval results (patch verification and matching results were correct) Can you please guide what I possibly be doing incorrect. Split is "full", I have checked. Does it has something to do with distractors?

Hardnetlibertywithaug (mAP with python implementation) = 69.66 (same as reported on paper)
Hardnetlibertywithaug (mAP with matlab implementation) = 60.64

Thanks in advance
Hi, I have an issue evaluating the descriptors using --descr-dir argument.

I've downloaded the sift descriptors and saved in **_root/data/descriptors/sift/_**. And when I'm in the folder of _**python/**_, I run the following taught by the website: 
_**python hpatch_eval.py --descr-dir=descrs/sift/ --task=verification --task=matching --delimiter=";"**_
It gives me the Usage instruction rather than executing the command all the time instead.

I' don't know where is problem, thank you!
Hi, 
When I run the script "run_article" with matlab.
It successfully run and show logs as following
> run_article
28 tasks.
Loading cached descriptor from /media/user/70303F89D6F80E5A/code/matching/hpatches-benchmark/data/descriptors/sift/desc.mat.
Results loaded from /media/user/70303F89D6F80E5A/code/matching/hpatches-benchmark/matlab/scores/scores_all/sift/verification.csv.
Results loaded from /media/user/70303F89D6F80E5A/code/matching/hpatches-benchmark/matlab/scores/scores_all/sift/matching.csv.
Results loaded from /media/user/70303F89D6F80E5A/code/matching/hpatches-benchmark/matlab/scores/scores_all/sift/retrieval.csv.
Loading cached descriptor from /media/user/70303F89D6F80E5A/code/matching/hpatches-benchmark/data/descriptors/sift/desc.mat.
Normalising the sift features (_nsplit_c_wzca_ceig0.25_pl0.50_l2n).
Clipping 103/128 eigen values.
Results loaded from /media/user/70303F89D6F80E5A/code/matching/hpatches-benchmark/matlab/scores/scores_all/sift_nsplit_c_wzca_ceig0.25_pl0.50_l2n/verification.csv.
Results loaded from /media/user/70303F89D6F80E5A/code/matching/hpatches-benchmark/matlab/scores/scores_all/sift_nsplit_c_wzca_ceig0.25_pl0.50_l2n/matching.csv.
Results loaded from /media/user/70303F89D6F80E5A/code/matching/hpatches-benchmark/matlab/scores/scores_all/sift_nsplit_c_wzca_ceig0.25_pl0.50_l2n/retrieval.csv.

When it hit the function in 
hb.m (line 61) : cmds.(cmd).fun(varargin{:});
-> all.m (line 12) : [des, varargin] = desc.memdesc(varargin{:});
->memdesc.m (line 66) : status = utls.textprogressbar(numDescs, 'startmsg', sprintf('Loading %s CSVs', descname));
-> textprogressbar.m (line 103) :  parse(p, n, varargin{:});


The error occur because n(numDescs) = 0

How can I fix this error?
Thanks for your reply!
If we turn off the scale/rotation invariance functionalities of ORB and BRIEF, these two descriptors should be the same. However, it shows different performance in the paper. Why?
Hi folks, 

I downloaded HPatches package, as well as sift and orb descriptors (e.g. `sh download.sh descr sift`). It turned out that for each patch per sequence(image), there exists one and only one descriptor (e.g. for `i_ajuntament`, there are 853 patches and 853 sift descriptors and the same for orb's). How could that be possible? What if my new key point detector finds no interest point no that specific patch? I had run through default opencv detectors as follows,

    import cv2
    print cv2.__version__ #3.4.0 
    import matplotlib.pyplot as plt
    %matplotlib inline

    IMG = "/home/mi/Downloads/hpatches-release/i_ajuntament/e2.png"

    img_ori = cv2.imread(IMG, cv2.IMREAD_GRAYSCALE)

    h,w = img_ori.shape
    print h/w

    for i in range(h/w):
        img = img_ori[i*w:(i+1)*w,:]

        sift = cv2.xfeatures2d.SIFT_create()
        kp, des = sift.detectAndCompute(img, None)
        print des

        img2 = cv2.drawKeypoints(img, kp, None, color=(0,255,0), flags=0)
        plt.imshow(img2)
        plt.show()

        if i>5:
            break

Some detected (sometimes more that one) and other did not. How could we run evaluation on it?

Any clarification is appreciated.
https://github.com/hpatches/hpatches-benchmark/blob/000984ee493dd5d0f3d1d8140d8d3d7a84e9848b/matlab/res_article.m#L97

It looks like texname is not used anywhere, and I guess that's the name that should be displayed in the figure...
Thanks for providing this huge dataset.

I want to download hbench-v85-bin.tar.gz to run the matlab code, but failed.


Please update the website: [www.robot.ox.ac.uk/~karel/blobs/${PACKAGE_BIN}]

Hi,

Thanks for collecting the huge dataset and providing an evaluation benchmark!

There seem to be some duplications of patch combinations in the definition files. For example, [verif_pos_split-illum](https://raw.githubusercontent.com/hpatches/hpatches-benchmark/master/tasks/verif_pos_split-illum.csv)  definition file contains ~0.36 M duplicated entries, [verif_neg_intra_split-illum](https://github.com/hpatches/hpatches-benchmark/blob/master/tasks/verif_neg_intra_split-illum.csv) definition file contains 763 duplicated entries and [verif_neg_inter_split-illum](https://github.com/hpatches/hpatches-benchmark/blob/master/tasks/verif_neg_inter_split-illum.csv)  definition file contains 9 duplicated entries.

If we consider this evaluation with & without duplications, the resulting numbers (FPR95, PR curve) would differ.
Are such settings intentional? If not, is it possible to correct the definition files?

Thanks!