@ali-raza-tariq commented on [Fri Jan 10 2020](https://github.com/apache/openwhisk-deploy-kube/issues/573)

The existing implementation shows the following error message when inflight limit is exceeded
```
<Response [429]> b'{\n  "code": "BWm0N3Qemn4uMAcZWY0JCy8PjTAQ191A",\n  "error": "Too many concurrent requests in flight (count: 240, allowed: 240)."\n}'
```
Few suggestions:

The message is misleading because the `allowed` should show the existing `actionInvokesConcurrent` limit set by the user (in this case 1000- which got diluted to 1200-`20% increase`, because of more than 1 controller in deployment)

So either it should mention `allowed per controller`, or simply show the total allowed over all the controllers (in this case, 5 controllers (5*240=1200)

Also, It think it would be better to change the `count` to something more meaningful like `currently running` for easy debugging. (good first issue label)




## Environment details:

* local deployment Mac OS Catalina (v10.15.2)
* Docker desktop v 2.1.0.5 stable (Engine: 19.03.5, Compose: 1.24.1, kubernates: v1.14.8, Notary: 0.6.1, Credential Helper: 0.6.3, Machine 0.16.2)

## Steps to reproduce the issue:

```
% git clone https://github.com/apache/openwhisk-devtools.git
% cd openwhisk-devtools/docker-compose
% make quick-start
```

## Provide the actual results and outputs:

```
Unpacking tarball.
downloading the CLI tool ... 
downloading cli for mac
Archive:  wsk.zip
  inflating: wsk                     
  inflating: LICENSE.txt             
  inflating: README.md               
  inflating: NOTICE.txt              
nightly: Pulling from openwhisk/controller
4fe2ade4980c: Pull complete 
932858db6d53: Pull complete 
67726ed84b62: Pull complete 
ff6bedb90346: Pull complete 
165d3dddf8b0: Pull complete 
e18bb0ce432b: Pull complete 
ef2a6847c1c5: Pull complete 
31dd388abe18: Pull complete 
0274340f13b2: Pull complete 
1ea79539b25b: Pull complete 
4668fd60f1ff: Pull complete 
faadab5ae708: Pull complete 
e6ce86e32ca7: Pull complete 
d08bc929e4df: Pull complete 
a16af49d18e6: Pull complete 
f970f34e90df: Pull complete 
Digest: sha256:46dea18ba7d34c3547aa8e333588f0e68f1a2583565072c9228115be71614c30
Status: Downloaded newer image for openwhisk/controller:nightly
docker.io/openwhisk/controller:nightly
nightly: Pulling from openwhisk/nodejs6action
a7344f52cb74: Pull complete 
515c9bb51536: Pull complete 
e1eabe0537eb: Pull complete 
4701f1215c13: Pull complete 
0e1adb90c373: Pull complete 
3bda49dd3517: Pull complete 
d9025659f192: Pull complete 
a9bd0d2e47ea: Pull complete 
4fb33b35e43d: Pull complete 
4479be1dbc1d: Pull complete 
79e64e555c1a: Pull complete 
436d1134ccda: Pull complete 
2e3b497b7a75: Pull complete 
Digest: sha256:3b1d3f096bc8bb2f65f61e3a929c86e2d93a660fdd123e296ec3823d63b009d6
Status: Downloaded newer image for openwhisk/nodejs6action:nightly
docker.io/openwhisk/nodejs6action:nightly
nightly: Pulling from openwhisk/dockerskeleton
e6b0cf9c0882: Pull complete 
da0e9bf0cc60: Pull complete 
ed6c52b90ff7: Pull complete 
89ff3fe31269: Pull complete 
97f442458f88: Pull complete 
010627f3c06b: Pull complete 
52638ea8d1a7: Pull complete 
c182ec37a960: Pull complete 
cf3366b43d73: Pull complete 
fddec4aeb473: Pull complete 
d99e07a4a50a: Pull complete 
Digest: sha256:55ba5064cac239395584e468449432189b32805c100e50f2ccfa69f9d533099f
Status: Downloaded newer image for openwhisk/dockerskeleton:nightly
docker.io/openwhisk/dockerskeleton:nightly
pulling the docker images short list... 
nightly: Pulling from openwhisk/invoker
4fe2ade4980c: Already exists 
932858db6d53: Already exists 
67726ed84b62: Already exists 
ff6bedb90346: Already exists 
165d3dddf8b0: Already exists 
e18bb0ce432b: Already exists 
ef2a6847c1c5: Already exists 
31dd388abe18: Already exists 
0274340f13b2: Already exists 
5077a5ce0afc: Pull complete 
da013b368680: Pull complete 
326b2e3b11f6: Pull complete 
e70df907a6c1: Pull complete 
242d934dc560: Pull complete 
0588de99685f: Pull complete 
Digest: sha256:6b0030bb1fc0f52414fe40374b111a7a7da788a8eba2751b60970aae007f9cff
Status: Downloaded newer image for openwhisk/invoker:nightly
docker.io/openwhisk/invoker:nightly
host ip address: 10.0.0.102
checking required ports ... 
 ... OK
cat: /Users/luizlima/tmp/openwhisk/local.env: No such file or directory
cat: /Users/luizlima/tmp/openwhisk/local.env: No such file or directory
cat: /Users/luizlima/tmp/openwhisk/local.env: No such file or directory
cat: /Users/luizlima/tmp/openwhisk/local.env: No such file or directory
cat: /Users/luizlima/tmp/openwhisk/local.env: No such file or directory
  ... preparing api-gateway configuration
pinging minio...
..WARNING: The DOCKER_COMPOSE_HOST variable is not set. Defaulting to a blank string.
Creating network "openwhisk_default" with the default driver
Pulling minio (minio/minio:RELEASE.2018-07-13T00-09-07Z)...
Creating openwhisk_minio_1 ... done
 ... OK
WARNING: The DOCKER_COMPOSE_HOST variable is not set. Defaulting to a blank string.
Pulling apigateway (openwhisk/apigateway:nightly)...
nightly: Pulling from openwhisk/apigateway
e7c96db7181b: Pull complete
f41be7f7d868: Pull complete
f4642b36ba98: Pull complete
f817911ac992: Pull complete
639073524d34: Pull complete
7ce1dc3f7bfb: Pull complete
41eb79dc8276: Pull complete
16f59ddd93e3: Pull complete
8b0df1464fa7: Pull complete
023847b19259: Pull complete
3df7dd1f4eac: Pull complete
258879ac9426: Pull complete
Digest: sha256:8f09e8d0cd9edc5588cbc366c80aaaf5aa7b1100bdb471df4b77f124c69606e0
Status: Downloaded newer image for openwhisk/apigateway:nightly
[dumb-init] rclone: No such file or directory
make: *** [setup] Error 2

```


The unicode tests are currently being skipped for the dotnet runtimes.  There appears to be some sources for dotnet 2.2 (openwhisk/tests/dat/actions/unicode.tests/src/dotnet2.2), but the test harness isn't finding them.

```
system.basic.WskUnicodeTests STANDARD_OUT

    test router? false

    Kinds to test: ballerina:0.990, dotnet:2.2, dotnet:3.1, go:1.11, java:8, nodejs:8, nodejs:10, nodejs:12, php:7.3, php:7.4, python:2, python:3, ruby:2.5, swift:4.2

    JUnit version is: 4.12

    WARNING: did not find text or binary action for kind ballerina:0.990, skipping it

    WARNING: did not find text or binary action for kind dotnet:2.2, skipping it

    WARNING: did not find text or binary action for kind dotnet:3.1, skipping it
```
Sometimes, admin may want to reinitalize the runtime config, e.g. nodejs:10 prewarm container number is less, lead to `cold start`, in order to handle user's request as soon as possible, admin may want to reinitalize the runtime configuration to increase the nodejs:10 prewarm containers.
And admin may want to reinitalize the runtime config on some limited invokers, this patch includes below changes
- [x] config runtime via controller to all managed invokers(or some limited inovkers which included in managed inovkers)
- [x] config runtime via invoker directly
- [x] get runtime info via invoker directly
- [x] add documents
- [ ] test cases


<!--- Provide a concise summary of your changes in the Title -->

## Description
<!--- Provide a detailed description of your changes. -->
<!--- Include details of what problem you are solving and how your changes are tested. -->

## Related issue and scope
<!--- Please include a link to a related issue if there is one. -->
- [ ] I opened an issue to propose and discuss this change (#????)

## My changes affect the following components
<!--- Select below all system components are affected by your change. -->
<!--- Enter an `x` in all applicable boxes. -->
- [ ] API
- [x] Controller
- [ ] Message Bus (e.g., Kafka)
- [x] Loadbalancer
- [x] Invoker
- [ ] Intrinsic actions (e.g., sequences, conductors)
- [ ] Data stores (e.g., CouchDB)
- [x] Tests
- [ ] Deployment
- [ ] CLI
- [ ] General tooling
- [ ] Documentation

## Types of changes
<!--- What types of changes does your code introduce? Use `x` in all the boxes that apply: -->
- [ ] Bug fix (generally a non-breaking change which closes an issue).
- [x] Enhancement or new feature (adds new functionality).
- [ ] Breaking change (a bug fix or enhancement which changes existing behavior).

## Checklist:
<!--- Please review the points below which help you make sure you've covered all aspects of the change you're making. -->

- [x] I signed an [Apache CLA](https://github.com/apache/openwhisk/blob/master/CONTRIBUTING.md).
- [ ] I reviewed the [style guides](https://github.com/apache/openwhisk/wiki/Contributing:-Git-guidelines#code-readiness) and followed the recommendations (Travis CI will check :).
- [ ] I added tests to cover my changes.
- [ ] My changes require further changes to the documentation.
- [x] I updated the documentation where necessary.


When I run  ansible-playbook -i environments/local setup.yml

I got the error about:

fatal: [localhost -> localhost]: FAILED! => {"changed": true, "cmd": "\"/home/zl/project/Noah/openwhisk/ansible/files/genssl.sh\" \"*.localhost\" \"server\" \"/home/zl/project/Noah/openwhisk/ansible/roles/nginx/files\"", "delta": "0:00:00.005662", "end": "2020-01-06 15:08:22.220630", "msg": "non-zero return code", "rc": 126, "start": "2020-01-06 15:08:22.214968", "stderr": "/bin/sh: 1: /home/zl/project/Noah/openwhisk/ansible/files/genssl.sh: Permission denied", "stderr_lines": ["/bin/sh: 1: /home/zl/project/Noah/openwhisk/ansible/files/genssl.sh: Permission denied"], "stdout": "", "stdout_lines": []}

[FAILED]
> "/home/zl/project/Noah/openwhisk/ansible/files/genssl.sh" "*.localhost" "server" "/home/zl/project/Noah/openwhisk/ansible/roles/nginx/files"
non-zero return code
/bin/sh: 1: /home/zl/project/Noah/openwhisk/ansible/files/genssl.sh:
Permission denied

How can I fix them

This PR is based on work done as part of #4509 to figure out approaches to reduce compilation times. It uses a [new proposed approach](pureconfig/pureconfig#673) to switch to [non Shapeless](https://github.com/milessabin/shapeless) based config derivation similar to what we do with spary-json

## Description

This PR is a work in progress till the new support in Pureconfig is completed. With new approach the compilation time for `common/scala` module reduced from ~ 60 sec to ~40 sec on my laptop

## Related issue and scope
<!--- Please include a link to a related issue if there is one. -->
- [ ] I opened an issue to propose and discuss this change (#4509)

## My changes affect the following components
<!--- Select below all system components are affected by your change. -->
<!--- Enter an `x` in all applicable boxes. -->
- [ ] API
- [ ] Controller
- [ ] Message Bus (e.g., Kafka)
- [ ] Loadbalancer
- [ ] Invoker
- [ ] Intrinsic actions (e.g., sequences, conductors)
- [ ] Data stores (e.g., CouchDB)
- [ ] Tests
- [ ] Deployment
- [ ] CLI
- [ ] General tooling
- [ ] Documentation

## Types of changes
<!--- What types of changes does your code introduce? Use `x` in all the boxes that apply: -->
- [ ] Bug fix (generally a non-breaking change which closes an issue).
- [ ] Enhancement or new feature (adds new functionality).
- [ ] Breaking change (a bug fix or enhancement which changes existing behavior).

## Checklist:
<!--- Please review the points below which help you make sure you've covered all aspects of the change you're making. -->

- [ ] I signed an [Apache CLA](https://github.com/apache/openwhisk/blob/master/CONTRIBUTING.md).
- [ ] I reviewed the [style guides](https://github.com/apache/openwhisk/wiki/Contributing:-Git-guidelines#code-readiness) and followed the recommendations (Travis CI will check :).
- [ ] I added tests to cover my changes.
- [ ] My changes require further changes to the documentation.
- [ ] I updated the documentation where necessary.


Error: whiskclient.go [271]: [ERROR_WHISK_CLIENT_INVALID_CONFIG]: 
  >> whiskclient.go [254]: The authentication key is not configured.
  >> whiskclient.go [259]: The API host is not configured.
## Environment details:

* Mac OS Majove

* 
```bash
java --version
openjdk 13.0.1 2019-10-15
OpenJDK Runtime Environment AdoptOpenJDK (build 13.0.1+9)
OpenJDK 64-Bit Server VM AdoptOpenJDK (build 13.0.1+9, mixed mode, sharing)
```
* 
```bash
gradle --version

------------------------------------------------------------
Gradle 6.0.1
------------------------------------------------------------

Build time:   2019-11-18 20:25:01 UTC
Revision:     fad121066a68c4701acd362daf4287a7c309a0f5

Kotlin:       1.3.50
Groovy:       2.5.8
Ant:          Apache Ant(TM) version 1.10.7 compiled on September 1 2019
JVM:          13.0.1 (AdoptOpenJDK 13.0.1+9)
OS:           Mac OS X 10.14.5 x86_64

```
## Steps to reproduce the issue:

1.  `git clone https://github.com/apache/openwhisk.git`
2.  Go to `/openwhisk/core/standalone/`
2.   `gradle init`


## Provide the actual results and outputs:

```
FAILURE: Build failed with an exception.

* Where:
Build file '~$HOME/openwhisk/core/standalone/build.gradle' line: 22

* What went wrong:
Plugin [id: 'org.scoverage'] was not found in any of the following sources:

- Gradle Core Plugins (plugin is not in 'org.gradle' namespace)
- Plugin Repositories (plugin dependency must include a version number for this source)

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.

* Get more help at https://help.gradle.org

BUILD FAILED in 664ms
```
As @rabbah mentioned [here](https://cwiki.apache.org/confluence/display/OPENWHISK/2019-12-04+OW+Tech+Interchange+Meeting+Notes), we have many proposals in [OpenWhisk Wiki](https://cwiki.apache.org/confluence/display/OPENWHISK/Proposals).

But we lack the official procedures for them.
While it is ok for small changes, lifecycle management is necessary for large changes.

We need procedures to discuss proposals, evaluate them, make a consensus for the direction, and finally include or drop some of them, etc.

Many renowned projects have their own procedures:

* K8S(KEP): https://github.com/kubernetes/enhancements/tree/master/keps
* Kafka(KIP): https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals 

It's clear for users to figure out which proposals are active and will be included in and when.

I will look into their approaches and define our own.
<!--
We use the issue tracker for bugs and feature requests. For general questions and discussion please use http://slack.openwhisk.org/ or https://openwhisk.apache.org/contact.html instead.

Do NOT share passwords, credentials or other confidential information.

Before creating a new issue, please check if there is one already open that
fits the defect you are reporting.
If you open an issue and realize later it is a duplicate of a pre-existing
open issue, please close yours and add a comment to the other.

Issues can be created for either defects or enhancement requests. If you are a committer than please add the labels "bug" or "feature". If you are not a committer please make clear in the comments which one it is, so that committers can add these labels later.

If you are reporting a defect, please edit the issue description to include the
information shown below.

If you are reporting an enhancement request, please include information on what you are trying to achieve and why that enhancement would help you.

For more information about reporting issues, see
https://github.com/apache/openwhisk/blob/master/CONTRIBUTING.md#raising-issues

Use the commands below to provide key information from your environment:
You do not have to include this information if this is a feature request.
-->

## Environment details:

* mesos (but likely happens in all other multi-controller envs)  
* actions that support concurrency > 1

## Steps to reproduce the issue:

1.   run a load test - not sure how much load is required, but this test ran ~6000 users, ~1k rps
2.   while test is running, restart a single controller
3.   


## Provide the expected results and outputs:

Some errors are expected related to in-flight requests at that controller that was killed.


## Provide the actual results and outputs:

There is possibly some issue with graceful shutdown of `NestedSemaphore` or related code. See logged errors below.

Separately, when a replacement controller comes back online, it becomes used immediately despite having only set invoker states to Offline or Unhealthy, resulting in extra errors. 


```
[2019-12-04T17:40:55.689Z] [INFO] [#tid_oXbBU0OYYp1bB5CyEhGe7jUb0KrRpivc] [BasicHttpService] [marker:http_get.200_counter:5010:5010]
[2019-12-04T17:40:55.694Z] [INFO] [#tid_YaUx7tDZOJY5aubZq5lC4GfZQo7TvQzw] [ShardingContainerPoolBalancer] received completion ack for '836225a9509e42e1a225a9509e02e136', system error=false
[2019-12-04T17:40:55.694Z] [INFO] [#tid_YaUx7tDZOJY5aubZq5lC4GfZQo7TvQzw] [ShardingContainerPoolBalancer] received completion ack for '836225a9509e42e1a225a9509e02e136'
[2019-12-04T17:40:55.694Z] [INFO] [#tid_YaUx7tDZOJY5aubZq5lC4GfZQo7TvQzw] [ShardingContainerPoolBalancer] received result ack for '836225a9509e42e1a225a9509e02e136'
[2019-12-04T17:40:55.694Z] [INFO] [#tid_YaUx7tDZOJY5aubZq5lC4GfZQo7TvQzw] [WebActionsApi]  [marker:controller_blockingActivation_finish:5009:5009]
[2019-12-04T17:40:55.694Z] [INFO] [#tid_YaUx7tDZOJY5aubZq5lC4GfZQo7TvQzw] [BasicHttpService] [marker:http_get.200_counter:5009:5009]
[2019-12-04T17:40:55.704Z] [INFO] [#tid_XKy3TepK8GP7tggjfBgpzdjex9yahxwE] [ShardingContainerPoolBalancer] received completion ack for '539e835782fb4da49e835782fb6da4c9', system error=false
[2019-12-04T17:40:55.704Z] [INFO] [#tid_XKy3TepK8GP7tggjfBgpzdjex9yahxwE] [ShardingContainerPoolBalancer] received completion ack for '539e835782fb4da49e835782fb6da4c9'
[2019-12-04T17:40:55.704Z] [INFO] [#tid_XKy3TepK8GP7tggjfBgpzdjex9yahxwE] [ShardingContainerPoolBalancer] received result ack for '539e835782fb4da49e835782fb6da4c9'
[2019-12-04T17:40:55.704Z] [INFO] [#tid_XKy3TepK8GP7tggjfBgpzdjex9yahxwE] [WebActionsApi]  [marker:controller_blockingActivation_finish:5012:5012]
[2019-12-04T17:40:55.704Z] [INFO] [#tid_XKy3TepK8GP7tggjfBgpzdjex9yahxwE] [BasicHttpService] [marker:http_get.200_counter:5012:5012]
[2019-12-04T17:40:55.704Z] [INFO] Cluster Node [akka.tcp://controller-actor-system@10.66.22.81:1105] - Exiting completed
[2019-12-04T17:40:55.708Z] [INFO] Cluster Node [akka.tcp://controller-actor-system@10.66.22.81:1105] - Shutting down...
[2019-12-04T17:40:55.709Z] [INFO] Cluster Node [akka.tcp://controller-actor-system@10.66.22.81:1105] - Successfully shut down
[2019-12-04T17:40:55.710Z] [INFO] [#tid_sid_loadbalancer] [ShardingContainerPoolBalancerState] loadbalancer cluster size changed from 6 to 5 active nodes. 12 invokers with 30000000 MB average memory size - total invoker memory 360000000 MB.
[2019-12-04T17:40:55.710Z] [INFO] [#tid_sid_loadbalancer] [ShardingContainerPoolBalancerState] loadbalancer cluster size changed from 5 to 4 active nodes. 12 invokers with 37500000 MB average memory size - total invoker memory 450000000 MB.
[2019-12-04T17:40:55.710Z] [INFO] [#tid_sid_loadbalancer] [ShardingContainerPoolBalancerState] loadbalancer cluster size changed from 4 to 3 active nodes. 12 invokers with 50000000 MB average memory size - total invoker memory 600000000 MB.
[2019-12-04T17:40:55.710Z] [INFO] [#tid_sid_loadbalancer] [ShardingContainerPoolBalancerState] loadbalancer cluster size changed from 3 to 2 active nodes. 12 invokers with 75000000 MB average memory size - total invoker memory 900000000 MB.
[2019-12-04T17:40:55.710Z] [INFO] Message [akka.cluster.GossipEnvelope] from Actor[akka.tcp://controller-actor-system@10.66.22.93:30204/system/cluster/core/daemon#-960490949] to Actor[akka://controller-actor-system/system/cluster/core/daemon#-221480326] was not delivered. [2] dead letters encountered. If this is not an expected behavior, then [Actor[akka://controller-actor-system/system/cluster/core/daemon#-221480326]] may have terminated unexpectedly, This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
[2019-12-04T17:40:55.710Z] [INFO] [#tid_sid_loadbalancer] [ShardingContainerPoolBalancerState] loadbalancer cluster size changed from 2 to 1 active nodes. 12 invokers with 150000000 MB average memory size - total invoker memory 1800000000 MB.
[2019-12-04T17:40:55.711Z] [INFO] Message [akka.cluster.GossipEnvelope] from Actor[akka.tcp://controller-actor-system@10.66.22.159:11643/system/cluster/core/daemon#-790933116] to Actor[akka://controller-actor-system/system/cluster/core/daemon#-221480326] was not delivered. [3] dead letters encountered. If this is not an expected behavior, then [Actor[akka://controller-actor-system/system/cluster/core/daemon#-221480326]] may have terminated unexpectedly, This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
[2019-12-04T17:40:55.714Z] [INFO] [#tid_sid_unknown] [Controller] Shutting down Kamon with coordinated shutdown
[2019-12-04T17:40:55.725Z] [INFO] Message [org.apache.openwhisk.core.connector.MessageFeed$FillCompleted] from Actor[akka://controller-actor-system/user/$e#-1558842808] to Actor[akka://controller-actor-system/user/$e#-1558842808] was not delivered. [4] dead letters encountered. If this is not an expected behavior, then [Actor[akka://controller-actor-system/user/$e#-1558842808]] may have terminated unexpectedly, This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
[2019-12-04T17:40:55.727Z] [INFO] Shutting down remote daemon.
[2019-12-04T17:40:55.728Z] [INFO] Remote daemon shut down; proceeding with flushing remote transports.
[2019-12-04T17:40:55.745Z] [INFO] Remoting shut down
[2019-12-04T17:40:55.745Z] [INFO] Remoting shut down.
[ERROR] [12/04/2019 17:40:55.768] [controller-actor-system-akka.actor.default-dispatcher-96] [akka.actor.ActorSystemImpl(controller-actor-system)] exception while executing timer task
java.util.NoSuchElementException
	at scala.collection.concurrent.TrieMap.apply(TrieMap.scala:833)
	at org.apache.openwhisk.common.NestedSemaphore.releaseConcurrent(NestedSemaphore.scala:103)
	at org.apache.openwhisk.core.loadBalancer.ShardingContainerPoolBalancer.$anonfun$releaseInvoker$1(ShardingContainerPoolBalancer.scala:329)
	at org.apache.openwhisk.core.loadBalancer.ShardingContainerPoolBalancer.$anonfun$releaseInvoker$1$adapted(ShardingContainerPoolBalancer.scala:329)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.openwhisk.core.loadBalancer.ShardingContainerPoolBalancer.releaseInvoker(ShardingContainerPoolBalancer.scala:329)
	at org.apache.openwhisk.core.loadBalancer.CommonLoadBalancer.processCompletion(CommonLoadBalancer.scala:285)
	at org.apache.openwhisk.core.loadBalancer.CommonLoadBalancer.$anonfun$setupActivation$4(CommonLoadBalancer.scala:150)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:202)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.run(LightArrayRevolverScheduler.scala:343)
	at akka.actor.LightArrayRevolverScheduler.$anonfun$close$1(LightArrayRevolverScheduler.scala:145)
	at akka.actor.LightArrayRevolverScheduler.$anonfun$close$1$adapted(LightArrayRevolverScheduler.scala:144)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at akka.actor.LightArrayRevolverScheduler.close(LightArrayRevolverScheduler.scala:144)
	at akka.actor.ActorSystemImpl.stopScheduler(ActorSystem.scala:980)
	at akka.actor.ActorSystemImpl.$anonfun$_start$1(ActorSystem.scala:910)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at akka.actor.ActorSystemImpl$$anon$2.run(ActorSystem.scala:931)
	at akka.actor.ActorSystemImpl$TerminationCallbacks$$anonfun$addRec$1$1.applyOrElse(ActorSystem.scala:1118)
	at akka.actor.ActorSystemImpl$TerminationCallbacks$$anonfun$addRec$1$1.applyOrElse(ActorSystem.scala:1118)
	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:536)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[ERROR] [12/04/2019 17:40:55.775] [controller-actor-system-akka.actor.default-dispatcher-96] [akka.actor.ActorSystemImpl(controller-actor-system)] exception while executing timer task
java.util.NoSuchElementException
	at scala.collection.concurrent.TrieMap.apply(TrieMap.scala:833)
	at org.apache.openwhisk.common.NestedSemaphore.releaseConcurrent(NestedSemaphore.scala:103)
	at org.apache.openwhisk.core.loadBalancer.ShardingContainerPoolBalancer.$anonfun$releaseInvoker$1(ShardingContainerPoolBalancer.scala:329)
	at org.apache.openwhisk.core.loadBalancer.ShardingContainerPoolBalancer.$anonfun$releaseInvoker$1$adapted(ShardingContainerPoolBalancer.scala:329)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.openwhisk.core.loadBalancer.ShardingContainerPoolBalancer.releaseInvoker(ShardingContainerPoolBalancer.scala:329)
	at org.apache.openwhisk.core.loadBalancer.CommonLoadBalancer.processCompletion(CommonLoadBalancer.scala:285)
	at org.apache.openwhisk.core.loadBalancer.CommonLoadBalancer.$anonfun$setupActivation$4(CommonLoadBalancer.scala:150)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:202)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.run(LightArrayRevolverScheduler.scala:343)
	at akka.actor.LightArrayRevolverScheduler.$anonfun$close$1(LightArrayRevolverScheduler.scala:145)
	at akka.actor.LightArrayRevolverScheduler.$anonfun$close$1$adapted(LightArrayRevolverScheduler.scala:144)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at akka.actor.LightArrayRevolverScheduler.close(LightArrayRevolverScheduler.scala:144)
	at akka.actor.ActorSystemImpl.stopScheduler(ActorSystem.scala:980)
	at akka.actor.ActorSystemImpl.$anonfun$_start$1(ActorSystem.scala:910)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at akka.actor.ActorSystemImpl$$anon$2.run(ActorSystem.scala:931)
	at akka.actor.ActorSystemImpl$TerminationCallbacks$$anonfun$addRec$1$1.applyOrElse(ActorSystem.scala:1118)
	at akka.actor.ActorSystemImpl$TerminationCallbacks$$anonfun$addRec$1$1.applyOrElse(ActorSystem.scala:1118)
	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:536)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
```



## Additional information you deem important:
* Primary issue here is prematurely making use of controller before some number of invokers are Healthy. This may be accomplished via deployment healthchecks, but ideally we could expose a configurable `/health` endpoint to indicate that a number of invokers are healthy. I know this is available at `/invokers` endpoint, but it employs no logic for assigning healthiness in current form.
