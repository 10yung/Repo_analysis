These requirements makes ticket #22 irrelevant.
This project would not work with cifar

New problems can be implemented very easily. You can see in train.py that
the meta_minimize method from the MetaOptimizer class is given a function
that returns the TensorFlow operation that generates the loss function we want
to minimize (see problems.py for an example).

It's important that all operations with Python side effects (e.g. queue
creation) must be done outside of the function passed to meta_minimize. The
cifar10 function in problems.py is a good example of a loss function that
uses TensorFlow queues.
I want to add accuracy is like loss,but if i add accuracy in build(),accuracy will like loss doing gradient to weights?So could you help me!Thank you very much for your help.
when i try quadratic and minist expriment produce the follow error
TypeError: The two structures don't have the same nested structure.
Entire first structure:
[., ., [.], [[((., .), (., .))]]]
Entire second structure:
[., ., [.], [[(LSTMState(hidden=., cell=.), LSTMState(hidden=., cell=.))]]]

please help me how can i address this problem。
thank you very much！
This PR solves some issue with running MNIST that can be found here: https://github.com/deepmind/learning-to-learn/issues/22.

I also added a Python .gitignore and the google .pylintrc.

I'm trying to run w/ the following versions in python==3.6.5
```
>>> sonnet.__version__
'1.27'
>>> tensorflow.__version__
'1.12.0'
```
and getting the following error:
```
$ python train.py --problem=mnist --save_path=./mnist
WARNING:tensorflow:From /root/.anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
Optimizee variables
['mlp/linear_0/w:0', 'mlp/linear_0/b:0', 'mlp/linear_1/w:0', 'mlp/linear_1/b:0']
Problem variables
[]
Traceback (most recent call last):
  File "/root/.anaconda/lib/python3.6/site-packages/tensorflow/python/util/nest.py", line 179, in assert_same_structure
    _pywrap_tensorflow.AssertSameStructure(nest1, nest2, check_types)
TypeError: The two structures don't have the same nested structure.
```

Presumably this is a version issue -- does anyone know tensorflow/sonnet versions where this repo will run?

Thanks!
~ Ben

When I try to run problem cifar and cifar-multi experiments, I run into an error that the boolean variable **is_training** is not specified as follows:

ValueError: Boolean is_training flag must be explicitly specified when using batch normalization.
originally defined at:
  File "train.py", line 61, in main
    problem, net_config, net_assignments = util.get_config(FLAGS.problem)
  File "/qydata/wwangbc/code/learning_to_optimize/l2l/util.py", line 113, in get_config
    mode=mode)
  File "/qydata/wwangbc/code/learning_to_optimize/l2l/problems.py", line 258, in cifar10
    use_batch_norm=batch_norm)
  File "/qydata/wwangbc/bin/anaconda/lib/python2.7/site-packages/sonnet/python/modules/nets/convnet.py", line 142, in __init__
    super(ConvNet2D, self).__init__(name=name)
  File "/qydata/wwangbc/bin/anaconda/lib/python2.7/site-packages/sonnet/python/modules/base.py", line 124, in __init__
    custom_getter_=self._custom_getter)
originally defined at:
  File "train.py", line 61, in main
    problem, net_config, net_assignments = util.get_config(FLAGS.problem)
  File "/qydata/wwangbc/code/learning_to_optimize/l2l/util.py", line 113, in get_config
    mode=mode)
  File "/qydata/wwangbc/code/learning_to_optimize/l2l/problems.py", line 268, in cifar10
    network = snt.Sequential([conv, snt.BatchFlatten(), mlp])
  File "/qydata/wwangbc/bin/anaconda/lib/python2.7/site-packages/sonnet/python/modules/sequential.py", line 65, in __init__
    super(Sequential, self).__init__(name=name)
  File "/qydata/wwangbc/bin/anaconda/lib/python2.7/site-packages/sonnet/python/modules/base.py", line 124, in __init__
    custom_getter_=self._custom_getter)

I think the is_training should be passed for the BN of both Conv2d and MLP and snt.Sequential function seems to be misused since we need to pass extra build arguments.

What's more, the code "network = snt.Sequential([conv, snt.BatchFlatten(), mlp])" shows that there is only one convolution layer in the network while there should be 3 in the paper. 

Could you please fix the bug and implement the complete 3-layer CNN network?

Thanks a lot!


Using Adam optimizer, not L2L for the CIFAR problem: If I print the cost after each epoch, it doesn't decrease over time running with learning rate .001, num_steps 100 num_epochs 100. However, printing the cost for each num_step, it decreases within the epoch. Why does it seem like the weights are being reset each epoch? 

I've also added code to check the training and validation accuracy after each epoch. These are also not decreasing with each epoch. 


Hi,

This is, of course, a minor issue, but managed to make it run (Ubuntu 16.04) after installing "dill."

Best,

Pedro
 
