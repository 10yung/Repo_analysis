New syntax in clojure 1.9:

https://clojure.org/reference/reader#_maps
Also remove a println (leftover from debug?)

This makes life easier when dealing with query results, as it allows them to be extracted without knowing the types in advance. This makes use from clojure easier.

Returning standard data structures and types instead (commented on elsewhere) is something that I think is also worth a look.

I would be interested in helping out with this project so I would be happy to chat sometime.
Needs a review regarding safety/correctness. I am pretty new to rust / JNA.
* Move from using `ghp-import` to using a travis deploy stage to commit documentation updates.
* Add Jazzy installation and swiftdoc generation to document building script.
* Commit generated documentation to a new branch `gh-pages-api-docs`. The existing `gh-pages` branch was incorrectly formatted and it was easier to just create a new branch.
* Remove existing `cargo-doc` script.
* Ensure build phase only executes on the master branch.

TODO: Point GithubPages to `gh-pages-api-docs` branch rather than master. This is done directly in settings and so will not form part of this PR. It needs to be done once this patch has landed.

Additionally, there's no way to execute a transaction (or query, I think, although I haven't looked as hard) directly from EDN.
This doesn't ever unescape `string_special_char`s, and just assumes they're the same as without the backslash, which is only true for \\ and \" escapes: https://github.com/mozilla/mentat/blob/e3113783aef99916812ae543dbe76f4451e440fd/edn/src/edn.rustpeg#L83
This document highlights many of the issues and concerns https://docs.google.com/document/d/14ywV4PlBAdOsJrxd7QhMcxbo7pw8cdc1kLAb7I4QhFY/edit?ts=5b7b7f87. 

Some work to measure the size of the database when storing history is in https://github.com/mozilla/application-services/pull/191. For my places.sqlite (100k places, 150k visits), it gets around 200MB larger.

It's worth noting that 'disk usage' was one of the primary concerns reported by user research for Fenix (although it's not clear if this size increase (relative to places) is the kind of thing that would make a dent relative to stuff like caches and the like -- A very informal poll of some friends of mine found that Fennec typically uses around 500MB of space (app + data), another 200MB isn't a trivial increase, but doesn't substantially change where we are in terms of app size).

Some bugs which may help (suggested by @rnewman):

- https://github.com/mozilla/mentat/issues/29: Pack the various flag columns into one TINYINT -- appears to give a 1%-2% benefit, which is probably not substantial enough to justify the effort IMO.
- https://github.com/mozilla/mentat/issues/69: implement something like place's url_hash automatically. If this could keep strings values out of the aevt/eavt indices it could have a huge benefit.
- https://github.com/mozilla/mentat/issues/32: Interning keywords. Our test schema didn't use these and I'm not sure what the actual use case for them is over `:db.type/ref` to a `{ :db/ident :my/keyword }`, so this doesn't seem like a high priority.
- https://github.com/mozilla/mentat/issues/33: Store the data canonically in a sql table instead of in datoms. This is interesting but seems like a lot of work.

I think something like [sqlite's zipvfs extension](https://www.sqlite.org/zipvfs/doc/trunk/www/index.wiki) would likely help (as the databases compress well), but have not tried it. Implementing it ourselves is likely beyond the scope of this effort (I took a look at the effort required and it wasn't exactly trivial). Additionally, whatever we do would need to somehow integrate with sqlcipher (I also took a look at bolting compression into sqlcipher before the encryption, but the fact that this makes the block output a variable size seemed to make this problematic).

Other notes:

1. Storing strings as fulltext and using the `compress`/`uncompress` options of FTS4 did not help, since the strings in each column are relatively small. Additionally, the performance overhead here was substantial even for a very fast compressor (LZ4).
2. Most string data seems to be duplicated ~4 times, in datoms, timelined_transactions, and in the indices idx_datoms_eavt, idx_datoms_aevt.
3. During RustConf, @rnewman suggested that ultimately mentat will likely not want to use sqlite, and instead want to read datoms chunks directly out of something like RKV. These chunks could be compressed more easily. This seems out of scope, as it would be a massive change to mentat, but is worth writing down.

Additional concerns exist around the fact that this problem may be exacerbated by materialized views (perhaps #33 will help or prevent this?)

Figured I'd throw up a PR for this old branch. It's almost certainly bitrotted, but better in a PR than in a branch.
This adds a lot of boilerplate which could be simplified by macros.

I was planning on cleaning this up before pushing it but here it is!

Caveat: I haven't tested this in versions of rust other than whatever i happen to be using by default right now! I did try to make the changes compatible with 1.25.0 (e.g. `&`/`ref` noise in matches) but that was just to minimize future work and I might not have been thorough here