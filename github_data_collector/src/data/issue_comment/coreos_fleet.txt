
Use [CodeLingo](https://codelingo.io) to automatically fix function comments following the
[Code Review Comments guidelines](https://github.com/golang/go/wiki/CodeReviewComments#error-strings).

This patch was generated by running the CodeLingo Rewrite Flow over the "[go-error-fmt](https://github.com/codelingo/codelingo/blob/master/tenets/codelingo/code-review-comments/go-error-fmt/codelingo.yaml)" Tenet. Note: the same Tenet can be used to automate PR reviews and generate contributor docs.

[Install CodeLingo to drive Continuous Higher Standards](https://github.com/apps/codelingo)

[Learn about CodeLingo](https://codelingo.io)

None
[Unit]
Description=SSH Per-Connection Server
After=syslog.target

[Service]
EnvironmentFile=-/etc/default/dropbear
#ExecStartPre=/ect/scripts/ssh_timeout.sh
ExecStartPre=/bin/sh -c 'SSHTIMEOUT=$(/bin/cat /etc/atom/defaults/syscfg_baseline.db | /bin/grep ssh_timeout | /bin/cut -d "=" -f2)'
ExecStart=/usr/sbin/dropbear -i -I $SSHTIMEOUT -r /var/tmp/dropbear_rsa_host_key -p 22 $DROPBEAR_EXTRA_ARGS
ExecReload=/bin/kill -HUP $MAINPID
StandardInput=socket

Handle hashed host names and breaking out of loops as soon as a match is found

fixes: #1727

Also, the loops in GetHostKeyAlgorithms, and Check should exit as soon as a match is found.

Pull request inbound :)
Hello
  We just saw a pretty server issue on our production CoreOs setup. Details are:

- 3 CoreOs nodes running in AWS EC2 Us East1
- m3.2xlarge instance types
- CoreOS nodes - 2 are DISTRIB_RELEASE=1068.2.0 and 1 is at DISTRIB_RELEASE=1081.5.0
- etcd version 0.4.9
- we have auto update disabled on CoreOs
- Around 21:56 UTC on Jan 17 we saw all our containers go down and the logs seemed to suggest an issue with etcd

Jan 17 21:56:22 ip-10-26-31-100.ec2.internal fleetd[999]: ERROR server.go:189: Server monitor triggered: Monitor timed out before successful heartbeat
Jan 17 21:56:22 ip-10-26-31-100.ec2.internal fleetd[999]: INFO server.go:157: Establishing etcd connectivity
Jan 17 21:56:22 ip-10-26-31-100.ec2.internal fleetd[999]: ERROR engine.go:179: Engine leadership acquisition failed: context deadline exceeded
Jan 17 21:59:41 ip-10-26-31-100.ec2.internal fleetd[999]: INFO server.go:168: Starting server components
Jan 17 21:59:42 ip-10-26-31-100.ec2.internal fleetd[999]: INFO engine.go:185: Engine leadership acquired
Jan 17 21:59:43 ip-10-26-31-100.ec2.internal fleetd[999]: ERROR engine.go:254: Failed unscheduling Unit(kafka-broker-1.service) from Machine(6ca65ead2f164b2682c0d941c8a75d9b): context deadline exceeded
Jan 17 21:59:43 ip-10-26-31-100.ec2.internal fleetd[999]: ERROR reconciler.go:62: Failed resolving task: task={Type: UnscheduleUnit, JobName: kafka-broker-1.service, MachineID: 6ca65ead2f164b2682c0d941c
Jan 17 21:59:44 ip-10-26-31-100.ec2.internal fleetd[999]: ERROR engine.go:254: Failed unscheduling Unit(newNewApps.service) from Machine(6ca65ead2f164b2682c0d941c8a75d9b): context deadline exceeded

- We checked the CPU and disk IO for all 3 instances, there is NO indication of any CPU spike per AWS Cloudwatch
- ETCD config is as below
core@ip-10-26-33-251 ~ $ sudo systemctl cat etcd
# /usr/lib64/systemd/system/etcd.service
[Unit]
Description=etcd
Conflicts=etcd2.service

[Service]
User=etcd
PermissionsStartOnly=true
Environment=ETCD_DATA_DIR=/var/lib/etcd
Environment=ETCD_NAME=%m
ExecStart=/usr/bin/etcd
Restart=always
RestartSec=10s
LimitNOFILE=40000

# /run/systemd/system/etcd.service.d/10-oem.conf
[Service]
Environment=ETCD_PEER_ELECTION_TIMEOUT=1200
# /run/systemd/system/etcd.service.d/20-cloudinit.conf
[Service]
Environment="ETCD_ADDR=10.26.33.251:4001"
Environment="ETCD_CERT_FILE=/home/etcd/certs/cert.crt"
Environment="ETCD_DISCOVERY=https://discovery.etcd.io/<BLAH>"
Environment="ETCD_KEY_FILE=/home/etcd/certs/key.pem"
Environment="ETCD_PEER_ADDR=10.26.33.251:7001"
- Attached the fleet & etcd logs from all nodes

[etcd-10-26-31-100.txt](https://github.com/coreos/etcd/files/712606/etcd-10-26-31-100.txt)
[etcd-10-26-32-94.txt](https://github.com/coreos/etcd/files/712607/etcd-10-26-32-94.txt)
[etcd-10-26-33-251.txt](https://github.com/coreos/etcd/files/712604/etcd-10-26-33-251.txt)
[fleet-10-26-31-100.txt](https://github.com/coreos/etcd/files/712603/fleet-10-26-31-100.txt)
[fleet-10-26-32-94.txt](https://github.com/coreos/etcd/files/712605/fleet-10-26-32-94.txt)
[fleet-10-26-33-251.txt](https://github.com/coreos/etcd/files/712602/fleet-10-26-33-251.txt)
- AWS status dashboard does not show any errors or issues on their end

Appreciate if someone can take a look at the above and give us any pointers on what to look at and what we can do to mitigate this.

I opened a fleet ticket - https://github.com/coreos/etcd/issues/7177 and was redirected to here

Thx
Maulik
[etcd-10-26-31-100.txt](https://github.com/coreos/fleet/files/712820/etcd-10-26-31-100.txt)
[etcd-10-26-32-94.txt](https://github.com/coreos/fleet/files/712821/etcd-10-26-32-94.txt)
[etcd-10-26-33-251.txt](https://github.com/coreos/fleet/files/712825/etcd-10-26-33-251.txt)
[fleet-10-26-31-100.txt](https://github.com/coreos/fleet/files/712822/fleet-10-26-31-100.txt)
[fleet-10-26-32-94.txt](https://github.com/coreos/fleet/files/712823/fleet-10-26-32-94.txt)
[fleet-10-26-33-251.txt](https://github.com/coreos/fleet/files/712824/fleet-10-26-33-251.txt)






I'm running CoreOS Stable, 1122.3.0 on Google Compute Engine. (Thus: fleet 0.11.7.)

Sometimes, after a reboot, fleet-controlled timers try to start before their associated fleet-controlled associated services have been loaded, resulting in timer failures. I'd expect that the fleet launcher would wait until all the parts of a timer are loaded before starting. (Or maybe just load everything on a rebooted node before starting anything.)

A stripped log shows the sequence:

```
-- Reboot --
systemd[1]: Started fleet daemon.
fleetd[1221]: INFO fleetd.go:64: Starting fleetd version 0.11.7
fleetd[1221]: INFO manager.go:246: Writing systemd unit cd-pipeline-run.timer (118b)
fleetd[1221]: INFO manager.go:182: Instructing systemd to reload units
systemd[1]: cd-pipeline-run.timer: Refusing to start, unit to trigger not loaded.
systemd[1]: Failed to start Run the Classifier Data Pipeline.
fleetd[1221]: INFO manager.go:127: Triggered systemd unit cd-pipeline-run.timer start: job=1432
fleetd[1221]: INFO reconcile.go:330: AgentReconciler completed task: type=LoadUnit job=cd-pipeline-run.timer reason="unit scheduled here but not loaded"
fleetd[1221]: INFO reconcile.go:330: AgentReconciler completed task: type=ReloadUnitFiles job=N/A reason="always reload unit files"
fleetd[1221]: INFO reconcile.go:330: AgentReconciler completed task: type=StartUnit job=cd-pipeline-run.timer reason="unit currently loaded but desired state is launched"
fleetd[1221]: INFO manager.go:246: Writing systemd unit cd-pipeline-run.service (2267b)
fleetd[1221]: INFO manager.go:182: Instructing systemd to reload units
fleetd[1221]: INFO reconcile.go:330: AgentReconciler completed task: type=LoadUnit job=cd-pipeline-run.service reason="unit scheduled here but not loaded"
```

(The more complete log is in a Gist, [here](https://gist.github.com/scole-scea/b5341370f3e9e958c53b2e42117f2056).)

I've yet to find the fleet option that tells it that a pair (or more) of unit files need to be handled together...

So far `systemd.LinkUnitFiles()` has been called with the force option turned on. That was one of the reasons of performance issues.

Let's introduce an option `systemd_link_unit_force`, false by default. Users can set the option in `fleetd.conf`. That option will be passed directly to `systemd.LinkUnitFiles()`.

/cc @hectorj2f 
Fixes https://github.com/coreos/fleet/issues/1694

At the moment `systemdUnitManager.writeUnit()` [calls `systemd.LinkUnitFiles()` with the `force` flag turned on](https://github.com/coreos/fleet/blob/master/systemd/manager.go#L268). This brings negative impact on performance.

So `force` should be set to `false` by default. It might be also a good idea to introduce a new option in `fleetd.conf`, so that users are able to set the flag.

/cc @hectorj2f 
See also https://github.com/coreos/go-systemd/issues/189

- Introduce an option UseLeaseTTL. Default is false.
- If UseLeaseTTL turned on, create lease manager with the TTL value.
- When gRPC turned on, remove previous workaround of setting leaseTTL
  to a huge value.
- Increase engine reconcile interval from 2 to 5 sec.

Originally written by @hectorj2f 
Taken from https://github.com/giantswarm/fleet/tree/patch_lease_ttl
