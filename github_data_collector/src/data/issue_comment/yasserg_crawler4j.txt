It is possible to configure the database names from the CrawlControler constructor.

It allowed me to create multiple CrawlerControllers on the same working directory without having the URLs mixed between them.

Substituting #420 because I broke the branch
Hey,
I am currently facing problem with servers which have regular expressions within their robots.txt which require an unreasonable amount of computing time to match for using RobotstxtServer.java ([https://www.regular-expressions.info/catastrophic.html](https://www.regular-expressions.info/catastrophic.html)). Eg:
```
User-agent: *
Disallow: /********************/
Disallow: /*******************
```
Matching for the url: "/asdjdsfsdfjkhejrhwjerhjkfdhksdjfhksjdfhjksdfhjksfdhjksdfasdasdd/js/jquery/jquery-migrate.min.js" completly freezes the thread. The console just shows:
```
10:53:34.438 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing connections idle longer than 30 SECONDS
10:53:39.438 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing expired connections
10:53:39.438 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing connections idle longer than 30 SECONDS
10:53:44.438 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing expired connections
10:53:44.438 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing connections idle longer than 30 SECONDS
10:53:49.439 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing expired connections
10:53:49.439 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing connections idle longer than 30 SECONDS
10:53:54.439 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing expired connections
10:53:54.439 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing connections idle longer than 30 SECONDS
10:53:59.439 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing expired connections
10:53:59.439 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing connections idle longer than 30 SECONDS
10:54:04.439 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing expired connections
10:54:04.439 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing connections idle longer than 30 SECONDS
10:54:09.440 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing expired connections
10:54:09.440 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing connections idle longer than 30 SECONDS
10:54:14.440 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing expired connections
10:54:14.440 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing connections idle longer than 30 SECONDS
10:54:19.440 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing expired connections
10:54:19.440 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing connections idle longer than 30 SECONDS
10:54:24.440 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing expired connections
10:54:24.440 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing connections idle longer than 30 SECONDS
10:54:29.441 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing expired connections
10:54:29.441 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing connections idle longer than 30 SECONDS
10:54:34.441 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing expired connections
10:54:34.441 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing connections idle longer than 30 SECONDS
10:54:39.442 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing expired connections
10:54:39.442 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing connections idle longer than 30 SECONDS
10:54:44.442 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing expired connections
10:54:44.442 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing connections idle longer than 30 SECONDS
10:54:49.442 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing expired connections
10:54:49.442 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing connections idle longer than 30 SECONDS
10:54:54.442 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing expired connections
10:54:54.442 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing connections idle longer than 30 SECONDS
10:54:59.443 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing expired connections
10:54:59.443 [Connection Manager] DEBUG e.u.i.c.f.SniPoolingHttpClientConnectionManager - Closing connections idle longer than 30 SECONDS
```

Is there a way around this?
**HELLO All**

i have multiple working crawlers run together 
eg. 
-crawler 1
-crawler 2
-crawler 3

> my question is ..

what if i want to shut down crawler number 2 only ?

_i imagine that every crawler in crawler4j has a session ID and i can shut it off while requesting its ID_

**HOW CAN I IMPLEMENT THAT ?**
More details: https://travis-ci.community/t/error-installing-oraclejdk8-expected-feature-release-number-in-range-of-9-to-14-but-got-8/3766
The image size condition will never take effectï¼Œchange the parentheses position
Extracted interfaces from Parser and PageFetcher in order to make it easier to create totally custom classes
Support for POST requests by using isPost() method in WebURL. 

This commit allows users to add WebURLs and mark them as POST URLs. Current parsers do not get PostURLs, however, users can add their custom post URLs either as seeds or using the visit() method.

It is based on the authentication methods already provided by Crawler4j.

May need improvement in DocIDServer because it is using an encoded version of the URL with all the POST data. Encoding / decoding of WebURLs to string for DBs could also be improved (maybe using a hash?)

**Modifications**

- WebURLs have a post flag and can store post parameters as BasicNameValuePair (inspired in post form auth)
- WebURLTupleBinding can serialize/deserialize the new WebURL.
- PageFetcher can now create POST request.
- PageFetchResult can hold now a complete WebURL.
- DocIDServer uses an encoded version of the URL which contains the post data. It allows you to visit the same URL multiple times if the post parameters are different. 
- You can add WebURLs directly as seeds. Depth will be reset to zero and POST parameters will be taken into consideration.
- Some minor internal deprecation suggestions, given the new features. They'll only affect custom complex overriden crawlers.
- Everything is still backwards compatible.


**I don't know gradle, so I'm not sure why this is giving compile errors on travis, but it seems that there's a problem in the config or in the master, rather than in my commit. Could someone please take a look?**
It is not working on Android, I tested on a new project, when I put the dependency on gradle and run, console return this error:

```
FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':app:checkDebugDuplicateClasses'.
> 1 exception was raised by workers:
  java.lang.RuntimeException: Duplicate class org.apache.commons.logging.Log found in modules commons-logging-1.2.jar (commons-logging:commons-logging:1.2) and jcl-over-slf4j-1.7.24.jar (org.slf4j:jcl-over-slf4j:1.7.24)
  Duplicate class org.apache.commons.logging.LogConfigurationException found in modules commons-logging-1.2.jar (commons-logging:commons-logging:1.2) and jcl-over-slf4j-1.7.24.jar (org.slf4j:jcl-over-slf4j:1.7.24)
  Duplicate class org.apache.commons.logging.LogFactory found in modules commons-logging-1.2.jar (commons-logging:commons-logging:1.2) and jcl-over-slf4j-1.7.24.jar (org.slf4j:jcl-over-slf4j:1.7.24)
  Duplicate class org.apache.commons.logging.impl.NoOpLog found in modules commons-logging-1.2.jar (commons-logging:commons-logging:1.2) and jcl-over-slf4j-1.7.24.jar (org.slf4j:jcl-over-slf4j:1.7.24)
  Duplicate class org.apache.commons.logging.impl.SimpleLog found in modules commons-logging-1.2.jar (commons-logging:commons-logging:1.2) and jcl-over-slf4j-1.7.24.jar (org.slf4j:jcl-over-slf4j:1.7.24)
  Duplicate class org.apache.commons.logging.impl.SimpleLog$1 found in modules commons-logging-1.2.jar (commons-logging:commons-logging:1.2) and jcl-over-slf4j-1.7.24.jar (org.slf4j:jcl-over-slf4j:1.7.24)
  
  Go to the documentation to learn how to <a href="d.android.com/r/tools/classpath-sync-errors">Fix dependency resolution errors</a>.


* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.

* Get more help at https://help.gradle.org

BUILD FAILED in 2s
```
I can sync and import classes, but can't build
https://github.com/yasserg/crawler4j/blob/4fcddc86414d1831973aff94050af55c7aeff3bc/crawler4j/src/main/java/edu/uci/ics/crawler4j/frontier/WorkQueues.java#L125

This method's documentation says that -> Lower key values results in earlier crawling.
But if this byte array is converted into Long or Double then this particular line should always result in positive value because it'll be the most significant byte. If it contains negative values then the most significant bit will be set to "1". Then if the conversion happens in the Java environment, the absolute value will be negative and thus signifies a very small number. So the logic will not be true according to documentation. Please change in documentation that URL priority should always be a positive value for the logic to be true.
usage of apache commons lang for better performance