If you pass in the classOf the log and continue exception handler, the mocked streams fails because it can't cast this to a string. KafkaStreams takes care of this internally. 

```
p.put(DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_CONFIG,
      classOf[LogAndContinueExceptionHandler])
```

KafkaStream's Properties is a Hashtable<Object, Object>

This is with version 3.4.0
Kafka itself now supports 2.13,
Can this be cross compiled with 2.13?

Because we aren't considering that the topology could fail, we're not cleaning up if there is a failure in running the topology. 

Consider closing even if there was an exception to clean up the directory: 

https://github.com/jpzk/mockedstreams/blob/929849f5b60af61660816e218b4ca2fdd8cb32c5/src/main/scala/com/madewithtea/mockedstreams/MockedStreams.scala#L143
When Kafka builds a local table, it often takes some time to stream all of the records in. This means that doing a join against this table may fail due to the matching record not yet appearing. 

I am trying to write a test to detect this issue, with topology and inputs as below;
 ```
val mstreams = MockedStreams()
      .topology(b => {
        ...some join logic...
        })
        .input(StreamforTable,
        someserde,
        someserde,
        irrelevantInputForTable) //A seq with 100 000 elements.
       .input(StreamForTable,
        someserde,
        someserde,
        relevantInputForTable) // A seq with one element that matches the third input() call.
      .input(StreamToJoin,
        someserde,
        someserde,
        singleRecordShouldMatch)
```

The intention is that the irrelevant input (in the first call to `input()`) should overwhelm the app, causing a delay in adding the relevant input (in the second call to `input()`) to the table. The join should then fail. But it appears that the `input()` calls are processed sequentially, which means that this isn't working. 

Does anyone have any ideas on how to write this test?
I have two streams for inner join:

Stream1 : Seq(("key1", 1), ("key3", 1), ("key2", 2))
Stream2: Seq(("key1", 3), ("key2", 1), ("key2", 2))

Expected Stream after inner-join (with addition as result) between Stream1 and Stream2: Seq(("key1", 4), ("key2", 3), ("key2", 4)) but I get the actual output as Seq(("key1", 4), ("key2", 3))
Hello there. I write an kStream consumer (connector). So I don't need the builder's output here. Just run it like `KafkaStreams.start()` does. Can it be added? Something simple, just like `def start() = withProcessedDriver { _ => () }`.

Hi, thanks for the great library!

One problem I encountered was when I tried to test a TumblingWindow aggregation.
Let's say I create a 2 second tumbling window like so:
`val w = TimeWindows
            .of(TimeUnit.SECONDS.toMillis(2)) 
            .advanceBy(TimeUnit.SECONDS.toMillis(2))
            .until(TimeUnit.SECONDS.toMillis(2)) `

Now, I use the CustomTimestampExtractor and send in events with value "number@timestamp", e.g.
("a", "1@1000"), ("a", "1@1500"), ("a", "1@2000"), ("a", "1@10000"), ("a", "1@100")
I run a simple count aggregation and would expect count to be (0->2), (2000->1), (10000->1), because the last event arrives when the time window [0,2000) is already closed because of until(2000). However, the last event is accounted for.
Could this be an interpretation, that the until duration is understood as a lower bound?

Thanks
`MockedStreams.output` doesn't allow you to specify zero for the `size` of the expected output.  That means you can't set an expectation that certain inputs should not have any outputs in your topology.

Instead of raising an exception when given a `size` of zero, `output`  should verify that `driver.readOutput` returns `null` and return an empty set.