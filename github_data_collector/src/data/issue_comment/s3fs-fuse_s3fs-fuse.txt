### Additional Information
_The following information is very important in order to help us to help you. Omission of the following details may delay your support request or receive no attention at all._
_Keep in mind that the commands we provide to retrieve information are oriented to GNU/Linux Distributions, so you could need to use others if you use s3fs on macOS or BSD_

#### Version of s3fs being used (s3fs --version)
_example: 1.00_

#### Version of fuse being used (pkg-config --modversion fuse, rpm -qi fuse, dpkg -s fuse)
_example: 2.9.4_

#### Kernel information (uname -r)
_command result: uname -r_

#### GNU/Linux Distribution, if applicable (cat /etc/os-release)
_command result: cat /etc/os-release_

#### s3fs command line used, if applicable
```
```
#### /etc/fstab entry, if applicable
```
```
#### s3fs syslog messages (grep s3fs /var/log/syslog, journalctl | grep s3fs, or s3fs outputs)
_if you execute s3fs with dbglevel, curldbg option, you can get detail debug messages_
```
```
### Details about issue
So when trying to mount and debug it getting an ' Transport endpoint not connected' error
![trasport endpoint](https://user-images.githubusercontent.com/49943773/72111267-5ef97200-338e-11ea-8e9f-33c3fbb672f2.png)


Hello, I wonder can s3fs use write cache only, not for read? 

A write cache is needed for a FUSE-mount device to do non-sequential write; on the other hand, a non-sequential read seem works w/o a cache anyway(?). So, to suppot write-only cache may reduce the local disk IO when reading. Is it true?
I'm running large scale tasks (over 2000 EC2 instances) referencing the same S3 source mounted on local disk.
And I randomly produced `file not exist` error in `boost::filesystem::exists` in very few instances, but actually those files do exist, and I was able to somehow hotfix it by retrying a few times......

It's very hard to reproduce this issue, it happened at a very low chance randomly, and on different files each time.

I'm using Ubuntu 16.04 with g++ 5.4.0,  C++14 and  boost 1.59

Just curious if anyone has met this issue before. Though retry is a workaround for it but that make my code very verbose and I'm not sure what kind of `boost::filesystem` operation might fail unexpectedly.
`src/s3fs_util.cpp` and `doc/man/s3fs.1` become desynchronized when contributors edit one but not the other.  Can we unify these somehow?  [ronn](https://github.com/rtomayko/ronn) generates man pages from Markdown; perhaps we can generate plain text from the Markdown and include it in `src/s3fs_util.cpp`?
I'm looking for a way to throttle the bandwidth usage or alternatively the fuse I/O writes. Is there any combination of options that may help achieving such purpose?  


#### s3fs command line used, if applicable
```
complement_stat
or:
-o complement_stat

```

### Details about issue
Hi, I tried using s3fs for the first time today and ran into the known issue of not being able to see folder permissions once mounted. Following your FAQ I tried complement_stat
or: -o complement_stat as part of the s3fs command line . The error informed me that the complement_stat is unknown. 

It would be awesome if you could add syntax examples of how handle the missing permissions to your FAQ (or fix the issue with complement_stat not being a known option if this was an issue). 

thank you, Mark

Kubernetes supports extending volume support via CSI. It would be great if s3fs would be wrapped up in a CSI driver allowing Kubernetes workload to easily utilize s3fs.

There is an example here:
https://github.com/kubernetes-csi/csi-driver-image-populator
`[ERR] curl.cpp:RequestPerform(2423): HTTP response code 400, returning EIO. Body Text: <Error><Code>EntityTooSmall</Code><Message>Your proposed upload is smaller than the minimum allowed size</Message><ProposedSize>528384</ProposedSize><$inSizeAllowed>5242880</MinSizeAllowed><PartNumber>1</PartNumber><ETag>480294879a56052a5937e78cb358580c</ETag><RequestId>AA1936A5569E43CF</RequestId><HostId>1pnskFIU9nckO2A1FqBhqCAV/92ZJaMbSixIEy+6HgS1BwyUN5PDhu4tLn81hxx229H+iSK2yzjN</Hos$Id></Error>`

The size of the file is 1.6GiB and the multipart_size is 16MiB. Everything else is default.

The part size is set as:

`-o multipart_size=16
`

Amazon Simple Storage Service File System V1.85 (commit:cc4a307) with OpenSSL
Copyright (C) 2010 Randy Rizun <rrizun@gmail.com>
License GPL2: GNU GPL version 2 <https://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.
### Additional Information
Currently only standard, one zone standard, one zone infrequent access and reduced redundancy storage classes are supported for adding objects to S3. S3 has added a new storage class for storing objects with unknown data patterns, called the intelligent tiering. This behavior is currently not supported by s3fs and the closest implementation is to use life cycle rules. Unfortunately, they do not provide the same level of granularity and benefits of intelligent tiering.

#### s3fs command line used, if applicable
```
s3fs -o storage_class=intelligent_tiering
```


Hi, I've mounted my s3 bucket with s3fs, it works well, but I use it to stream files in asterisk, and it takes about 15 seconds before starting every file…
I gave it permissions, also the option “allow_other” on mounting, nothing helped.

I appreciate your kindly help, thanks.