**Describe the bug**
I try the code in the section _Sending files and form data_ but it fail with ``Error: Unable to call `file`, which is undefined or falsey``

**To Reproduce**
1.  Copy the sample code in _Sending files and form data_ to a file ``sendfiles.yml``
2. Create a dummy file and update the respective path in ``sendfiles.yml``:   ``echo 'test content' > test.txt``
3. Execute Strest:  ``strest sendfiles.yml``

**Return**
```
[ Strest ] Found 1 test file(s)
[ Strest ] Schema validation: 1 of 1 file(s) passed

✖ Testing postwithfile failed (0.007s)
[ Validation ] Failed to process postwithfile request line using nunjucks:
 Template render error: (unknown path) [Line 9, Column 26]
  Error: Unable to call `file`, which is undefined or falsey

Response:
null
[ Strest ] Failed before finishing all requests
```

**Additional context**
* Strest installation:   ``npm i -g @strest/cli``
* node --version:  ``v8.10.0``


**The Json output generated do not have status on Test**
Thanks for this awesome tool . 
When performing a test it is always nice to have a place where the test results , are stored 
It would be nice have a section in json where status of test as is PASS , FAIL , SKIPPED along with reason is maintained  ! 

```yml
version: 2

requests:
    getDummyData:
      request:
        url: https://reqres.in/api/users/2
        method: GET
      validate:
        - jsonpath: status
          expect: 200  
        - jsonpath: content.data.email
          expect: janet.weaver@reqres               
      log: true   
```
Command used to execute the test 
`strest 0-sampleTest.strest.yml  -n -s dummy_test.json`

**Output Json**

```json
{
  "getDummyData": {
    "status": 200,
    "statusText": "OK",
    "headers": {
      "date": "Wed, 26 Jun 2019 15:37:00 GMT",
      "content-type": "application/json; charset=utf-8",
      "content-length": "170",
      "connection": "close",
      "set-cookie": [
        "__cfduid=d7b5472470e700bae60fa035ac616659f1561563420; expires=Thu, 25-Jun-20 15:37:00 GMT; path=/; domain=.reqres.in; HttpOnly; Secure"
      ],
      "x-powered-by": "Express",
      "access-control-allow-origin": "*",
      "etag": "W/\"aa-yZW/45DWGt/1ri05OLnMt/FJ3RY\"",
      "via": "1.1 vegur",
      "cf-cache-status": "HIT",
      "age": "6363",
      "expires": "Wed, 26 Jun 2019 19:37:00 GMT",
      "cache-control": "public, max-age=14400",
      "accept-ranges": "bytes",
      "expect-ct": "max-age=604800, report-uri=\"https://report-uri.cloudflare.com/cdn-cgi/beacon/expect-ct\"",
      "server": "cloudflare",
      "cf-ray": "4ed042942f5f3498-LHR"
    },
    "content": {
      "data": {
        "id": 2,
        "email": "janet.weaver@reqres.in",
        "first_name": "Janet",
        "last_name": "Weaver",
        "avatar": "https://s3.amazonaws.com/uifaces/faces/twitter/josephstein/128.jpg"
      }
    }
  }
}
```
**Solution I'd like**
The above test is actually a failed test . when run from the console we can see that it has thrown a red text as below . 
``` shell
✖ Testing getDummyData failed (1.234s)
[ Validation ] The JSON response value should have been janet.weaver@reqres but instead it was janet.weaver@reqres.in
```
But in the future when we go back and take a look at the json it does not provide an insight on if its a success / failed test case ! 

**Example Output:**
Showcasing only the desired section which can be appended to the existing json 
**Test result for  each test**
```json
{
"content": {
      "data": {
        "id": 2,
        "email": "janet.weaver@reqres.in",
        "first_name": "Janet",
        "last_name": "Weaver",
        "avatar": "https://s3.amazonaws.com/uifaces/faces/twitter/josephstein/128.jpg"
      }
    },
"result":{
   "status": "failed",
"reason": "The JSON response value should have been janet.weaver@reqres but instead it was janet.weaver@reqres.in"
    }
}
```
**Test summary at the end of a suite**
below data is fictional ! 
```json
{
"summary":{
   "total": 10,
   "pass": 5,
   "fail": 3,
  "skipped":2,
 "time": "1.144s"
     }
}
```


For some requests it's useful to validate that a value was not returned, e.g. when creating a user and ensuring the response doesn't have the password.

At the moment https://github.com/eykrehbein/strest/blob/master/src/test.ts#L426 prevents one from testing non-existence of properties.

```yml
version: 2
requests:
  create_user:
    request:
      url: example.com/user
      method: POST
        mimeType: 'application/json'
        text:
           email: 'test@example.com'
           password: 'password'
      validate:
        - jsonpath: content.email
          expect: 'test@example.com'
        - jsonpath: content.password
          expect: undefined
```

**Describe the solution you'd like**
Adding `undefined` as a 'type' or value to validate against.

**Describe alternatives you've considered**
Adding `null` to responses is a work-around, but isn't ideal.
It'd be great to be able to consume the results of tests, as [Postman's Newman CLI tool allows](https://github.com/postmanlabs/newman#using-newman-as-a-library).

Jest allows for custom test runners to be configured; one doesn't need to use Jest's methods to run tests, but one can still benefit from Jest's other features.

e.g. there is a [Mocha runner for Jest](https://github.com/rogeliog/jest-runner-mocha), as well as an [ESLint runner](https://github.com/jest-community/jest-runner-eslint).

By allowing for a custom Jest runner to consume results of tests we get the following from Jest:

- watching files for changes to run tests
- running specific tests via Jest's CLI
- running projects simultaneously (e.g. API tests, unit tests, integration tests)
- running specific tests using [typeahead](https://github.com/jest-community/jest-watch-typeahead)
- running specific projects using [jest-watch-select-projects](https://github.com/rogeliog/jest-watch-select-projects)

**Describe the solution you'd like**

A module that passes the results of a test run to a callback.

Example:

```javascript
// my-api-endpoint-1.test.js
const strest = require('strest')

const strestConfig = {
	// references to yaml files and configs for this test
};

strest.run(strestConfig, (err, result) => {
	// handle err / result of test run
});
```

**Additional context**

For reference on what can be done with Jest...

Jest Mocha runner:
![Gif of jest-runner-mocha](https://user-images.githubusercontent.com/574806/30088955-728bf97e-925e-11e7-9b25-6aac237085ca.gif)

---

Watch typeahead:
![Gif of jest-watch-typeahead](https://user-images.githubusercontent.com/574806/40672937-25dab91a-6325-11e8-965d-4e55ef23e135.gif)

---

Watch select projects:
![Gif of jest-watch-select-projects](https://user-images.githubusercontent.com/574806/40741798-3ca7c250-6401-11e8-8398-d39ab4eea011.gif)

Running all of the requests synchronously takes a lot of time. Running requests asynchronously would be much faster.
I know there's a lot to consider here with what is already in place especially when considering dependent variables from other requests.

I was thinking of these changes to the yaml to accomplish this:


```yml
version: 2 # might require version 3 for this
async: true

requests:
  healthCheck:
    request:
      url: https://foobar.com/health-check
      method: GET
  longRunningRequest:
    request:
      url: https://foobar.com/lots-of-work
      method: GET
  login: # will return { authenticated: true }
    ...
  authNeeded:
    requires: # list of all dependent requests that need to run first (e.g. for variables to be available)
      - login
    request:
    ...
      headers:
      - name: Authorization
        value: Bearer <$ login.content.authenticated $>  # It's possible to use the status code, headers, and status text from previous calls.
  anotherAuthNeeded:
    requires: # list of all dependent requests that need to run first (e.g. for variables to be available)
      - login
    request:
    ...
      headers:
      - name: Authorization
        value: Bearer <$ login.content.authenticated $>  # It's possible to use the status code, headers, and status text from previous calls.
```

In this scenario: 
1. `healthCheck`, `longRunningRequest` and `login` all run/resolve asynchronously.
2. Once `login` returns successfully, `authNeeded` and `anotherAuthNeeded` both run asynchronously. If there are more dependent requests, their names are added to the `requires` list and are only ran after the dependencies have returned successfully.

[listr](https://github.com/SamVerschueren/listr) seems like it would come in handy to accomplish this.

I installed via `npm i -D @strest/cli`, and installed the vscode plugin. When I run it I with <kbd>ctrl+alt+r</kbd> I get the error

> /bin/sh: 1: strest: not found

The docs say to install globally, presumably then it would work. Many people don't install anything globally, ever, other than npm itself.

Please tell me to how I can make this work?

If it's not currently possible, please add the ability to install locally. Or perhaps add a config file, so we can specify where to find the executable, or that it should run using `npx`.
Current behavior is not to print output if calls fails 

[ Strest ] Found 1 test file(s)
[ Strest ] Schema validation: 1 of 1 file(s) passed

[ Strest ] Failed before finishing all requests 

**Describe the solution you'd like**
Print the output whenever rest calls fails
**Is your feature request related to a problem? Please describe.**
I'd like to be able to see the contents of a set variable. e.g: In relation to #122 I'm pulling the token from the previous request and using it in the next as a header value.
```
      headers:
        - name: x-device-auth
          value: <$ login.content.Result.token | dump | safe $>
```
This should be working, but no. Being able to view the processed variable will help with debugging.

**Describe the solution you'd like**
When using the `strest -p` switch, can it output the request headers and body, not just the response?

Not sure if I'm missing something here

I am trying to dynamically produce the current day in a test. Faker will not allow me to format for the required post data that I need. I know of moment.js and day.js that will produce this and allow me to format the data. I have not been able to successfully import either into my test and was wondering if import was possible, if there was something else already built in, or if this is something that could possibly added as feature?
Possible implementation:

```
version: 2

requests:
  call1:
    request:
      url: https://postman-echo.com/time/now
      method: GET
  call2:
    request:
      url: https://jsonplaceholder.typicode.com/posts
      method: POST
      postData:
        mimeType: application/json
        text:
          foo: 2
  repeat_until:
    until:
      jsonpath: call1.content
      expect: <$ Env("STREST_GMT_DATE") $>
      else_goto: call1 # when false
      goto: call3 # when true
  call3:
    request:
      url: https://jsonplaceholder.typicode.com/posts
      method: POST
      postData:
        mimeType: application/json
        text:
          foo: 2
```