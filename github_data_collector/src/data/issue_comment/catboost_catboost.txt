Problem: Training slows down proportionally to the number of GPUs I use. E.g. on device 0 the first 100 iterations take 15 sec, with devices 0:1 about 30 seconds an with devices 0-7 about 120 seconds. When I switch to 'CPU', it's roughly 100x slower. So the fastest way was using a single GPU. Is it a bug?

Dataset: 768 dense features (BERT embeddings vectors), 100k samples

catboost version: 0.20.2
Operating System: Debian GNU/Linux 9
CPU: 96 vCPU
GPU: 8 x V100

Problem: Today we can load the model by passing its path to the `load_model` method
Catboost version: 0.20.2
Operating System: Ubuntu 19.10
CPU: Intel(R) Core(TM) i7-10510U CPU @ 1.80GHz
Language: Python 3.7

Hi,

Nowadays it's not very practical to load models from paths only. Especially when you work with cloud-based infrastructure where you can have your model stored in S3/GS/What-else buckets. Maybe something like `load_blob_model` could improve the situation or maybe make `load_model` capable of working with file instances (instead of paths). What do think about such features?

Many thanks.
**The problem:**
I'm looking to emit prediction intervals for each predicted value (the mean) in regression. I need that these intervals cover say 90% of true values and be as narrow as possible. In other words I want to learn and emit variance (or noise) which can't be explained by features of the model in each region of data - each sample would have different intervals determined by input vector.
**For example:**
Predicting income by number of education years. Given there is no additional data we have I would expect lower variance of income for lower education and higher variance for higher education. Another example - predicting how much years left to leave, by age and health data of a person. Young would have larger variance, while old lower, and old and unhealthy even more lower.

There are two main methods I'm aware of to do it:
 - non-parametric methods - using Quantile Loss,  where trying to get 90% of coverage we can train three models: Quantile:alpha=0.5 for the median and alpha=0.1 and 0.9 for the lower and higher bounds (quantiles) respectfully.
- parametric method - suppose some distribution of the noise, and learn it's parameters. For example to get 90% coverage suppose Normal distribution and train models for mean and stdev. Then calculate the interval using stdev.
- bayesian methods, essentially parametric as well, but the modeling is done using probabilistic methods. 

**The question:**
As mentioned I want the interval be as narrow as possible but still satisfy the needed coverage, which means learning separate models for quantiles or parameters for parametric methods wouldn't provide optimal solution in terms of coverage and width.
I'm looking for the loss function which can optimize two things simultaneously. Is there something builtin already in some library and if not what would be the simplest way to implement it? 
Both parametric and non parametric methods are accepted. 
Thanks in advance!
Alexander

**Example**
The data in example is simulated: 2 independent variables - stage (categorical) and age (axis X), axis Y is the predicted value. The bounds and mean created by 3 separate quantile models. But real data is much more complex, so separate models approach not creating nice results.
![image](https://user-images.githubusercontent.com/6525588/72510297-dd07be00-3851-11ea-8d2f-9a4f0897297d.png)

Problem: `get_feature_importance` with `fstr_type='ShapValues'` has poor performances
catboost version: _0.20.2_
Operating System: _Ubuntu 19.10_
CPU: _Intel(R) Core(TM) i7-10510U CPU @ 1.80GHz_
model type: Regression 
model format: cbm
model size: ~250mb

Greetings,

First of all, thanks a lot for such a great library. I'm facing some issues with explanation of predicted values. While the prediction works blazingly fast the explanation of predictions through shap-values shows poor performances.

In order to understand my issue here are few figures:
Prediction time: ~ _0.0099 sec_
Shap values time: ~ _0.7799 sec_

Here is a portion of my code:
```python
features = MODEL.get_feature_importance(data=pool, fstr_type='ShapValues')
shap_values, base_value = np.split(features[0], [-1])
```

My primary goal is to explain the prediction for a given item at the same time when prediction is made. Ideally I would like to launch everything inside of a GCP cloud-function.

Many thanks.
Problem:

Hi,
it is straightforward to verify that the C++ api gives different prediction results to the python api when the CatboostRegressor model is trained in python, on any m float features and n>0 categorical features, saved in cpp format and executed. This is not the case with n=0 categorical features. Can you please investigate?


```
params = {"boosting_type":"Ordered", 
"num_boost_round":500,
"loss_function":"RMSE",
"eval_metric":"RMSE"}

model = CatBoostRegressor(**params)
model.fit(Pool(X,label=y, cat_features=["A","B"]))
model.save_model(filename, format="cpp")

ModelCalcerWrapper calcer("model.cbm");
std::vector<float> floatFeatures(100);
std::vector<std::string> catFeatures = {"a", "b"};
std::cout << calcer.Calc(floatFeatures, catFeatures) << std::endl;


```
catboost version: 0.17.5
Operating System: Unix


Problem: When fitting a catboostclassifier model i get with evaluation set (x_test,y_test) I get max accuracy 0.899 - i  have also set accuracy as eval_metric -. However when I use model.predict at x_test afterwards I find that 21% of samples are misclassified. Am I missing something? 
( p.s. no categorical features in this test)
catboost version:catboost==0.18.1
Operating System:mac os
<img width="1440" alt="catboost_mismatch" src="https://user-images.githubusercontent.com/38654121/72382388-d24e0b80-3721-11ea-9527-94e8f3bae51b.png">
Thanks in advance
CPU:
 GPU:
Hello,

Does anybody knows how really works Dum Models and the 3 different strategies? 

At documentation there is no especific  information and no sources of how it works. 

Thanks.

Problem: Testing binary classification using python and c tester programs. The results are different. Is it normal?
catboost version: v0.9.1.1
Operating System: Ubuntu 16.04
CPU: Intel(R) Core(TM)2 Duo CPU     T9400  @ 2.53GHz
# GPU:

Hello! I've just set up a simple example in which CatBoost behaves in a strange way. The training set consists in 300 data points, and the model should learn this simple function:
"A" -> 10
"B" -> 20
"C" -> 30

The model gives the correct output for `"A"` (according to training examples) and `"D"` (handling an unseen level, I get approximately the average label on the training set)

However, for `"B"` and `"C"` the model fails to learn the simple function.

```
from catboost import CatBoostRegressor

# Initialize data
train_data = [["A"],
              ["B"],
              ["C"]] * 100

train_labels = [10, 20, 30] * 100

# Initialize CatBoostRegressor
model = CatBoostRegressor(cat_features=[0])

# Fit model
model.fit(train_data, train_labels, verbose=False)

print("{:.2f}".format(model.predict(['A'])))
print("{:.2f}".format(model.predict(['B'])))
print("{:.2f}".format(model.predict(['C'])))
print("{:.2f}".format(model.predict(['D'])))
```

Output:
```
10.00
25.00
25.00
19.99
```


Expected output:
```
10.00
20.00
30.00
20.00
```

`(catboost==0.20.1)`
In the code below, when using `get_feature_importance` to get ShapValues and the expected value, I am getting a strangely different "behavior" from what is described in this tutorial [notebook](https://github.com/catboost/catboost/blob/master/catboost/tutorials/model_analysis/shap_values_tutorial.ipynb).

The mentioned above code:
```
import catboost
from catboost import CatBoostClassifier, Pool
from catboost.datasets import titanic
from sklearn.model_selection import train_test_split
import numpy as np

train_df, test_df = catboost.datasets.titanic()
train_df.fillna(-999, inplace=True)
test_df.fillna(-999, inplace=True)
X = train_df.drop('Survived', axis=1)
y = train_df.Survived

cat_ix = np.where(X.dtypes != np.float)[0]
X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42)
X_test = test_df
params = {
    'iterations': 500,
    'learning_rate': 0.1,
    'eval_metric': 'Accuracy',
    'random_seed': 42,
    'logging_level': 'Silent',
}
train_pool = Pool(X_train, y_train, cat_features=cat_ix)
validate_pool = Pool(X_val, y_val, cat_features=cat_ix)
model = CatBoostClassifier(**params)
model.fit(train_pool, eval_set=validate_pool)

print(model.predict(train_pool, prediction_type='RawFormulaVal').mean().round(4))
# prints 0.7318

shap_values = model.get_feature_importance(train_pool, type="ShapValues")
expected_value = shap_values[:,-1][0]
print(expected_value.round(4))
# prints -0.5393
```
In the code above we obtain different values given by `model.predict` and by `shap_values[0,-1]`, whereas if you follow the [shap_values_tutorial.ipynb](https://github.com/catboost/catboost/blob/master/catboost/tutorials/model_analysis/shap_values_tutorial.ipynb) you would obtain same value:
```
shap_values = model.get_feature_importance(Pool(X, y), type='ShapValues')
expected_value = shap_values[0,-1]
print(expected_value.round(4), model.predict(X).mean().round(4))
# prints 22.5318 22.5318
```

After some exploration, I have found the workaround which gives the expected value for the first code (with titanic dataset):
```
shap_values.sum(axis=1).mean().round(4) # -0.7318
```
If possible, could you please explain this inconsistency and what causes it.

Thank you for a great tool that you provided the community with!

---
catboost version: 0.20.2
Operating System: Ubuntu 19.04
CPU: Intel® Core™ i5-7400 CPU


