1) Create valid vs with existing upstream
2) Create second vs with a non-existent upstream

#2 vs goes to the expected status:

```
+-----------------+--------------+-----------------+------+--------------------------------------+-----------------+------------------------------------+
| VIRTUAL SERVICE | DISPLAY NAME |     DOMAINS     | SSL  |                STATUS                | LISTENERPLUGINS |               ROUTES               |
+-----------------+--------------+-----------------+------+--------------------------------------+-----------------+------------------------------------+
| default         |              | *               | none | *v1.Proxy.gloo-system.gateway-proxy |                 | /api ->                            |
|                 |              |                 |      | Warning: warning:    Route Warning:  |                 | gloo-system.static-pet            |
|                 |              |                 |      | InvalidDestinationWarning. Reason:   |                 | (upstream)                         |
|                 |              |                 |      | *v1.Upstream {missing gloo-system}  |                 |                                    |
|                 |              |                 |      | not found                            |                 |                                    |

```

However, #1 also goes to Warning state for missing upstream:

```
status:
  reason: "warning: \n  Route Warning: InvalidDestinationWarning. Reason: *v1.Upstream
    {missing gloo-system} not found"
  reported_by: gateway
  state: 3
  subresource_statuses:
    '*v1.Proxy.gloo-system.gateway-proxy': {}
```
To recreate:
1) Deploy petstore
2) Deploy a vs pointing to petstore upstream 
3) Edit the petstore upstream and add upstream.timeout: 10s (NOTE: this is invalid)
4) glooctl get upstream

4 will return` Error: converting output crd: reading crd spec on resource default-petstore-8080 in namespace apicor-istio into v1Upstream: unknown field "timeout" in v1.Upstream` 

And the console will show empty upstreams
The problem with the previous version of this check was that in CI we check out a commit by its SHA (not by branch name) which puts the repo in a detached HEAD state, which in turn causes 

```make
CURRENT_BRANCH := $(shell git rev-parse --abbrev-ref HEAD)
```

to evaluate to `HEAD` instead of the branch name.

This caused `ASSETS_ONLY` to always be true:

```make
ASSETS_ONLY := true
# This would be: if "master" == "HEAD"
ifeq ($(DEFAULT_BRANCH), $(CURRENT_BRANCH))
    ASSETS_ONLY = false
endif
```




BOT NOTES: 
resolves https://github.com/solo-io/gloo/issues/2220

[This change](https://github.com/solo-io/gloo/commit/73b56cdf72616fc8ecd50b555aa3de545e94bdc9#diff-e5fc65747fb9417968a3295dc1ac5111) reordered some of our cloud build steps. As a result, the `upload-github-release-assets` make target - which uploads github release assets and updates our `glooctl` formulas - is now executed **before** the regression tests, which is wrong.

These actions should be performed **after** the regressions tests and only if the tests completed successfully.


https://github.com/solo-io/solo-kit/blob/master/pkg/api/v1/clients/kube/resource_client.go#L418

If this line returns `false` due to an error OTHER than a does-not-exist error, it will cause this line:
https://github.com/solo-io/solo-kit/blob/master/pkg/api/v1/clients/kube/resource_client.go#L214

to evaluate to false
which means that we hit this line:

https://github.com/solo-io/solo-kit/blob/master/pkg/api/v1/clients/kube/resource_client.go#L238

that will attempt to CREATE a Gateway object, causing this error seen by a customer: https://solo-io.slack.com/archives/GLXP5T8GY/p1579221016040100

```
{"level":"warn","ts":"2020-01-16T20:57:49.319Z","logger":"gateway.v1.event_loop.gateway.v1.event_loop.translatorSyncer.reporter","caller":"reporter/reporter.go:166","msg":"failed to write status {Accepted  gateway map[*v1.Proxy.gloo-system.gateway-proxy:state:Accepted reported_by:\"gloo\" ] nil {} [] 0} for resource [REDACTED]: creating kube resource [REDACTED]: resourceVersion should not be set on objects to be created","version":"1.2.12"}
```

The [1.3.2](https://github.com/solo-io/gloo/releases/tag/v1.3.2) release of Gloo was assets-only, i.e. no PR for homebrew formulas or fish food were created by our build bot.

From the [release build logs](https://console.cloud.google.com/cloud-build/builds/6b78c3f2-5de3-468e-a24e-7f65ad16c0dc?project=solo-public&organizationId=325453704078#step_17):
> GO111MODULE=on go run ci/upload_github_release_assets.go true

This means that the `ASSETS_ONLY` variable [here](https://github.com/solo-io/gloo/blob/07a7568a5a028a3b4fd5561f06d5bba1cc31ac6f/Makefile#L465) evaluates to `true`, meaning that this does not work as intended:

```make
ASSETS_ONLY := true
ifeq ($(DEFAULT_BRANCH), $(CURRENT_BRANCH))
    ASSETS_ONLY = false
endif
```


We've discussed this before and agreed that `glooctl uninstall --all` would remove *everything*. It currently does NOT remove CRDs.

https://github.com/solo-io/gloo/issues/433
I've seen both in the docs (and in my scripts) using a shortcut of https://$(glooctl proxy url)/foo as a quick way to access the Gloo Gateway endpoint. 

When you have a version mismatch, it spits out the version information too. This blows up the scripts. 

I like that it warns about version mismatches, but it should do them once or at least with an option to hide that warning with --ignore-version-mismatch or something.


We need a regression test that ensures that the `helm upgrade` command works, even if we just change helm values (e.g. "upgrade" from 1.2.15 -> 1.2.15)

Currently this doesn't work in the 1.2.15 -> 1.2.15 case, and we get:

```
helm install gloo/gloo -n gloo --namespace gloo-system --version 1.2.15 --set "crds.create=true" --set "global.glooRbac.create=true"
helm upgrade gloo gloo/gloo --force --namespace gloo-system --version 1.2.15 --set "crds.create=false" --set "global.glooRbac.create=false" --set "settings.watchNamespaces[0]=foobar" --set "settings.watchNamespaces[1]=gloo-system"
UPGRADE FAILED
ROLLING BACK
Error: validatingwebhookconfigurations.admissionregistration.k8s.io "gloo-gateway-validation-webhook-gloo-system" already exists
Error: UPGRADE FAILED: validatingwebhookconfigurations.admissionregistration.k8s.io "gloo-gateway-validation-webhook-gloo-system" already exists
```