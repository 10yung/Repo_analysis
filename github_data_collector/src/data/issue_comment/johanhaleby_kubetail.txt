With that change and a following kube config
```
- context
    namespace: dev
  name: dev
- context
    namespace: prod
  name: prod
```
all calls to kubetail below will work and yield the same result
```
kubetail --context prod
kubetail -n prod --context prod
kubetail --context prod -n prod
```

Hi there, 

Not sure, if that's the intended behaviour but presently if I pass `--context` and that context has a namespace attached to it in my kube config file, I won't be able to tail anything as it won't get the specific namespace of that context, it will set the namespace of the `current-context` no matter what.

My configuration is something like that:
```
- context:
    cluster: cluster-1.com
    namespace: dev
    user: user-cluster-1.com
  name: dev
- context:
    cluster: cluster-2.com
    namespace: prod
    user: user-cluster-2.com
  name: prod
current-context: dev
```
When running the following with kubetail version 1.6.10
```
$ kubetail oauth --context prod
```
returns
```
No pod exists that matches oauth
```
However, running
```
kubetail oauth --context prod -n prod
```
returns all the logs  expected.

I've looked into the script and while I'm not an expert in bash I think it's due to the method `calculate_default_namespace()` which doesn't get passed the `--context` option if one is given to the `kubetail` command, thus, grabbing the namespace of the `current-context` every time that function is ran.

Happy to help if needed.
If you call kubetail from a script, it will kill the script when it's done.
```
% sh -c 'echo shell pid $$; kubetail -k false -f false -s 1s >/dev/null; echo foo'
shell pid 31337
[1]    31337 terminated  sh -c 
%
```
This forces the user to trap TERM, to avoid failure during successful execution and consequently messes up signal handling in that script, which won't die when sent TERM from elsewhere.

See: https://github.com/johanhaleby/kubetail/blob/master/kubetail#L277

we tried 

`kubetail -j  <pod> -n <namespace>`

we get: 

```
backend/0 is not defined at <top-level>, line 1:
. as $line | try (fromjson | <pod name> ) catch $line
```

Am i calling this correctly? Our logs for a particular service output as json
Hi
Thanks for your great product! But it has some misses which seem really useful. For example grep option which there is a pull request which adds this feature. And another essential feature is some option that helps me to get all log of the pods not from now on, I mean not applying tail option. Can you add this feature too?
Hello,

I have multiple cluster and for safety reason I use a different .kube/config file in order to avoid doing harm to my production cluster.

I would love to see some support for  `--kubeconfig=` instead to realy on the default `.kube/config`

Thanks for this !
**Search Pod in all namespaces**
**Search Pod in all namespaces and ignore some**
**Search Pod in defined namespaces**
**Search Logs or Exclude Logs**

TODO : Adding GREP options, for search or exclude some logs
( state POC )
TODO: export to a file
TODO: export to rsyslog
TODO: export as json
https://github.com/kubernetes-sigs/krew/
https://github.com/kubernetes-sigs/krew/blob/master/docs/DEVELOPER_GUIDE.md
https://github.com/kubernetes-sigs/krew/blob/master/docs/NAMING_GUIDE.md
Plz review and feedback.
A subshell now checks for matching pods every second and pipes a new list of pods if changes.
Pipe is being listened on by main, every time a new list of pods comes through tails get remade

issues:
- new pods appearing are still in initializing state, and should not be picked up by subshell.
- pipe /tmp/kubetail-matching_pods_pipe.$$ can be used for arbitrary code execution
- don't like the "kill %t"
- colours of existing pods shouldn't change when the list of pods changes


