**Pull Request checklist**

- [x] The commit(s) message(s) follows the contribution [guidelines](CONTRIBUTING.md) ?
- [ ] Tests for the changes have been added (for bug fixes / features) ?
- [ ] Docs have been added / updated (for bug fixes / features) ?

**Current behavior :** (link exiting issues here : https://help.github.com/articles/basic-writing-and-formatting-syntax/#referencing-issues-and-pull-requests)

An NPE is thrown if a query happens to hit a partition/time series with a schema ID that is not known to FiloDB.  In theory this should not be possible, as both ingestion and the index/partkey recovery process skips partitions with unknown schema IDs. 

**New behavior :**

Unknown schemas will result in a new, specific `UnknownSchemaQueryErr`.  Upon seeing this error, operators need to truncate the data tables as this should only occur when an existing schema is changed, which is extremely rare.

**Pull Request checklist**

- [x] The commit(s) message(s) follows the contribution [guidelines](CONTRIBUTING.md) ?
- [x] Tests for the changes have been added (for bug fixes / features) ?
- [ ] Docs have been added / updated (for bug fixes / features) ?

This PR revolves around refresh of Downsampled Lucene Index using part key data start/end time  mutations from raw dataset. Once the downsample cluster loads the initial lucene index, it needs to do a periodic fetch of all recent mutations that have occurred.

We keep track of mutations in new Cass table partitionkeys_by_update_time that will allow for spark migration job and downsample filo cluster to query index data by update time.

Downsample Filo Cluster now refreshes index using this table

E2E Unit test that performs query on Downsample Filo Cluster.

Caveat:
* When index refresh is done, the reverse lookup from partKey to partId on the lucene index can be slow. If this gets out of hand, we may have to figure out other ways to achieve the lookup faster.
**Pull Request checklist**

- [x] The commit(s) message(s) follows the contribution [guidelines](CONTRIBUTING.md) ?
- [x] Tests for the changes have been added (for bug fixes / features) ?
- [ ] Docs have been added / updated (for bug fixes / features) ?

Support for Prometheus timestamp function
**Pull Request checklist**

- [x] The commit(s) message(s) follows the contribution [guidelines](CONTRIBUTING.md) ?
- [x] Tests for the changes have been added (for bug fixes / features) ?
- [ ] Docs have been added / updated (for bug fixes / features) ?

This PR is still WIP.

Sending it out to get some feedback from team on the rough direction for cardinality control via rate limiting.
**Pull Request checklist**

- [x] The commit(s) message(s) follows the contribution [guidelines](CONTRIBUTING.md) ?
- [x] Tests for the changes have been added (for bug fixes / features) ?
- [x] Docs have been added / updated (for bug fixes / features) ?

**Related Issue:**
https://github.com/filodb/FiloDB/issues/458

**Changes:**
1. - [x] Introducing build.sbt:
Since FlioBuild is defining the project, it should be organized in build.sbt (according to the new sbt reqs). FiloSettings can be kept in the .scala files because it contains objects that FiloBuild needs.
2. - [x] Some of the common setting things like scalaVersion and organization can move to ThisBuild / x. see https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html#step+4%3A+Configure+build.sbt
3. - [x] Tracking all the dependencies in one file - https://www.scala-sbt.org/1.x/docs/Organizing-Build.html#Tracking+dependencies+in+one+place

**Challenges:**
1.  Got this error `not found: value scalastyleFailOnError; scalastyleFailOnError := true,` in project/FiloSettings.scala at line 61, using sbt 1.3. I went through scalastyle doc http://www.scalastyle.org/sbt.html. Seems like scalastyleFailOnError filed still exists. Currently trying to find out what is wrong.

2. Also in scalastyle doc (linked above): It says that scalastyle >= 0.9.0, should use `testScalastyle := scalastyle.in(Test).toTask("").value`. compare to scalastyle <= 0.8.0, `testScalastyle := org.scalastyle.sbt.ScalastylePlugin.scalastyle.in(Test).toTask("").value`.
I changed the code accordingly. But still got `not found: value scalastyle`.
This might be nontrivial due to our use of project/Filo{Build,Settings}.scala, but will be worth it for the new features It brings.  Perfect for a Scala dev who's new to the project.

**Branch, version, commit**

**OS and Environment**

**JVM version**

**Scala version**

**Kafka and Cassandra versions and setup**

**Spark version if used**

**Deployed mode**
(client/cluster on Spark Standalone/YARN/Mesos/EMR or default)

**Actual (wrong) behavior**

**Steps to reproduce**

**Logs**

```
some log
```

or as attached file (see below)


Unused parts of this template should be removed (including this line).
**Branch, version, commit**

**OS and Environment**

**JVM version**

**Scala version**

**Kafka and Cassandra versions and setup**

**Spark version if used**

**Deployed mode**
(client/cluster on Spark Standalone/YARN/Mesos/EMR or default)

**Actual (wrong) behavior**

**Steps to reproduce**

**Logs**

```
some log
```

or as attached file (see below)


Unused parts of this template should be removed (including this line).

**Branch, version, commit**

**OS and Environment**
CentOS Linux 7.3

**JVM version**
JDK 1.8

**Scala version**
Scala version 2.11

**Kafka and Cassandra versions and setup**
Cassandra version 3.2

**Spark version if used**
Spark 2.0

**Deployed mode**
(client/cluster on Spark Standalone/YARN/Mesos/EMR or default)
Standalone

**Actual (wrong) behavior**
scala> dfResult.write.format("filodb.spark").option("dataset", "flow_raw").option("row_keys", "exportMs").option("partition_keys", "protocolId").mode(SaveMode.Append).save()
[INFO] [05/10/2018 23:15:15.943] [main] [StatsDExtension(akka://kamon)] Starting the Kamon(StatsD) extension
[Stage 8:====>                                                    (1 + 11) / 12]#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f5204acf802, pid=3245, tid=0x00007f5041dca700
#
# JRE version: OpenJDK Runtime Environment (8.0_131-b12) (build 1.8.0_131-b12)
# Java VM: OpenJDK 64-Bit Server VM (25.131-b12 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# J 22484 C1 org.velvia.filo.ZeroCopyBinary$class.cachedHash64(Lorg/velvia/filo/ZeroCopyBinary;)J (140 bytes) @ 0x00007f5204acf802 [0x00007f5204acf760+0xa2]
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /root/spark-2.0.0-bin-hadoop2.6/hs_err_pid3245.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
#
bin/spark-shell: line 44:  3245 Aborted                 (core dumped) "${SPARK_HOME}"/bin/spark-submit --class org.apache.spark.repl.Main --name "Spark shell" "$@"



scala> dfResult1.write.format("filodb.spark").option("dataset", "flow_raw").option("row_keys", "exportMs").option("partition_keys", "protocolId").mode(SaveMode.Append).save()
18/05/10 23:09:35 ERROR OneForOneStrategy:
java.lang.NullPointerException
        at org.velvia.filo.ZeroCopyBinary$class.cachedHash64(ZeroCopyBinary.scala:77)
        at org.velvia.filo.ZeroCopyUTF8String.cachedHash64(ZeroCopyBinary.scala:101)
        at org.velvia.filo.ZeroCopyBinary$class.hashCode(ZeroCopyBinary.scala:85)
        at org.velvia.filo.ZeroCopyUTF8String.hashCode(ZeroCopyBinary.scala:101)
        at java.util.HashMap.hash(HashMap.java:338)
        at java.util.HashMap.putIfAbsent(HashMap.java:1061)
        at org.velvia.filo.vectors.DictUTF8Vector$.shouldMakeDict(DictUTF8Vector.scala:49)
        at org.velvia.filo.vectors.UTF8PtrAppendable.optimizedVector(UTF8Vector.scala:304)
        at org.velvia.filo.vectors.UTF8PtrAppendable.suboptimize(UTF8Vector.scala:294)
        at org.velvia.filo.vectors.ObjectVector.optimize(ObjectVector.scala:59)




**Steps to reproduce**
scala> val files=Seq("file:/root/parquetfiles/fragment1523366588968000000.dat","file:/root/parquetfiles/fragment1523350809009000000.dat")
scala> val sqlContext = new org.apache.spark.sql.SQLContext(sc)
scala> val parDF=sqlContext.read.parquet(files:_*)

scala> parDF1.count
res2: Long = 1168530

val dfResult = parDF.withColumn("exporterIp", inetToString(parDF("exporterIp"))).withColumn("srcIp", inetToString(parDF("srcIp"))).withColumn("dstIp", inetToString(parDF("dstIp"))).withColumn("nextHopIp", inetToString(parDF("nextHopIp"))).withColumn("bgpNextHopIp", inetToString(parDF("bgpNextHopIp"))).withColumn("appId", inetToString(parDF("appId"))).withColumn("policyQosClassificationHierarchy", inetToString(parDF("policyQosClassificationHierarchy"))).withColumn("protocolId", parDF("protocolId").cast(IntegerType)).withColumn("srcTos", parDF("srcTos").cast(IntegerType)).withColumn("dstTos", parDF("dstTos").cast(IntegerType)).withColumn("srcMask", parDF("srcMask").cast(StringType)).withColumn("dstMask", parDF("dstMask").cast(StringType)).withColumn("direction", parDF("direction").cast(StringType))

dfResult.write.format("filodb.spark").option("dataset", "flow_metric_raw_new").option("row_keys", "exportMs").option("partition_keys", "protocolId").mode(SaveMode.Append).save()


It throws either JVM errors or null pointer exceptions





**Logs**

```
some log
```

or as attached file (see below)


Unused parts of this template should be removed (including this line).

**Branch, version, commit**

**OS and Environment**
Red Hat Enterprise Linux Server release 7.2 (Maipo)


**JVM version**
 java -version
java version "1.8.0_161"
Java(TM) SE Runtime Environment (build 1.8.0_161-b12)
Java HotSpot(TM) 64-Bit Server VM (build 25.161-b12, mixed mode)


**Scala version**
Scala version 2.11.8

**Kafka and Cassandra versions and setup**
apache-cassandra-3.11.2

**Spark version if used**
2.0.0

**Deployed mode**
(client/cluster on Spark Standalone/YARN/Mesos/EMR or default)
Spark Standalone

**Actual (wrong) behavior**
parDFnew.write.format("filodb.spark").
             option("dataset", "parDFNF").
             option("row_keys", "appId").
             option("partition_keys", "exportMs").
			 mode(SaveMode.Overwrite).save()

[Stage 2:>                                                          (0 + 2) / 2]18/04/10 02:40:47 ERROR DatasetCoordinatorActor: Error in reprojection task (filodb.parDFNF/0)
java.lang.NullPointerException
        at org.velvia.filo.vectors.UTF8PtrAppendable.addData(UTF8Vector.scala:270)
        at org.velvia.filo.BinaryAppendableVector$class.addVector(BinaryVector.scala:154)
        at org.velvia.filo.vectors.ObjectVector.addVector(ObjectVector.scala:16)
        at org.velvia.filo.GrowableVector.addData(BinaryVector.scala:251)
        at org.velvia.filo.BinaryVectorBuilder.addData(BinaryVector.scala:308)
        at org.velvia.filo.VectorBuilderBase$class.add(VectorBuilder.scala:36)
        at org.velvia.filo.BinaryVectorBuilder.add(BinaryVector.scala:303)
        at org.velvia.filo.RowToVectorBuilder.addRow(RowToVectorBuilder.scala:70)
        at filodb.core.store.ChunkSet$$anonfun$2.apply(ChunkSetInfo.scala:53)
        at filodb.core.store.ChunkSet$$anonfun$2.apply(ChunkSetInfo.scala:52)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
        at scala.collection.Iterator$class.foreach(Iterator.scala:893)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
        at scala.collection.AbstractIterator.to(Iterator.scala:1336)
        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
        at filodb.core.store.ChunkSet$.apply(ChunkSetInfo.scala:55)
        at filodb.core.store.ChunkSet$.withSkips(ChunkSetInfo.scala:70)
        at filodb.core.store.ChunkSetSegment.addChunkSet(Segment.scala:193)
        at filodb.core.reprojector.DefaultReprojector$$anonfun$toSegments$1$$anonfun$apply$2$$anonfun$apply$1.apply$mcV$sp(Reprojector.scala:94)
        at filodb.core.reprojector.DefaultReprojector$$anonfun$toSegments$1$$anonfun$apply$2$$anonfun$apply$1.apply(Reprojector.scala:93)
        at filodb.core.reprojector.DefaultReprojector$$anonfun$toSegments$1$$anonfun$apply$2$$anonfun$apply$1.apply(Reprojector.scala:93)
        at kamon.trace.TraceContext$class.withNewSegment(TraceContext.scala:53)
        at kamon.trace.MetricsOnlyContext.withNewSegment(MetricsOnlyContext.scala:28)
        at filodb.core.Perftools$.subtrace(Perftools.scala:26)
        at filodb.core.reprojector.DefaultReprojector$$anonfun$toSegments$1$$anonfun$apply$2.apply(Reprojector.scala:92)
        at filodb.core.reprojector.DefaultReprojector$$anonfun$toSegments$1$$anonfun$apply$2.apply(Reprojector.scala:79)
        at kamon.trace.Tracer$$anonfun$withNewContext$1.apply(TracerModule.scala:62)
        at kamon.trace.Tracer$.withContext(TracerModule.scala:53)
        at kamon.trace.Tracer$.withNewContext(TracerModule.scala:61)
        at kamon.trace.Tracer$.withNewContext(TracerModule.scala:77)
        at filodb.core.reprojector.DefaultReprojector$$anonfun$toSegments$1.apply(Reprojector.scala:79)
        at filodb.core.reprojector.DefaultReprojector$$anonfun$toSegments$1.apply(Reprojector.scala:78)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
        at scala.collection.immutable.List.map(List.scala:285)
        at filodb.core.reprojector.DefaultReprojector.toSegments(Reprojector.scala:78)
        at filodb.core.reprojector.DefaultReprojector$$anonfun$reproject$1$$anonfun$apply$3.apply(Reprojector.scala:118)
        at filodb.core.reprojector.DefaultReprojector$$anonfun$reproject$1$$anonfun$apply$3.apply(Reprojector.scala:117)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
        at scala.collection.AbstractTraversable.map(Traversable.scala:104)
        at filodb.core.reprojector.DefaultReprojector$$anonfun$reproject$1.apply(Reprojector.scala:117)
        at filodb.core.reprojector.DefaultReprojector$$anonfun$reproject$1.apply(Reprojector.scala:117)
        at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
        at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
18/04/10 02:40:47 ERROR OneForOneStrategy:
java.lang.NullPointerException
        at org.velvia.filo.vectors.UTF8PtrAppendable.addData(UTF8Vector.scala:270)
        at org.velvia.filo.BinaryAppendableVector$class.addVector(BinaryVector.scala:154)
        at org.velvia.filo.vectors.ObjectVector.addVector(ObjectVector.scala:16)
        at org.velvia.filo.GrowableVector.addData(BinaryVector.scala:251)
        at org.velvia.filo.BinaryVectorBuilder.addData(BinaryVector.scala:308)
        at org.velvia.filo.VectorBuilderBase$class.add(VectorBuilder.scala:36)
        at org.velvia.filo.BinaryVectorBuilder.add(BinaryVector.scala:303)
        at org.velvia.filo.RowToVectorBuilder.addRow(RowToVectorBuilder.scala:70)
        at filodb.core.store.ChunkSet$$anonfun$2.apply(ChunkSetInfo.scala:53)
        at filodb.core.store.ChunkSet$$anonfun$2.apply(ChunkSetInfo.scala:52)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
        at scala.collection.Iterator$class.foreach(Iterator.scala:893)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
        at scala.collection.AbstractIterator.to(Iterator.scala:1336)
        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
        at filodb.core.store.ChunkSet$.apply(ChunkSetInfo.scala:55)
        at filodb.core.store.ChunkSet$.withSkips(ChunkSetInfo.scala:70)
        at filodb.core.store.ChunkSetSegment.addChunkSet(Segment.scala:193)
        at filodb.core.reprojector.DefaultReprojector$$anonfun$toSegments$1$$anonfun$apply$2$$anonfun$apply$1.apply$mcV$sp(Reprojector.scala:94)
        at filodb.core.reprojector.DefaultReprojector$$anonfun$toSegments$1$$anonfun$apply$2$$anonfun$apply$1.apply(Reprojector.scala:93)
        at filodb.core.reprojector.DefaultReprojector$$anonfun$toSegments$1$$anonfun$apply$2$$anonfun$apply$1.apply(Reprojector.scala:93)
        at kamon.trace.TraceContext$class.withNewSegment(TraceContext.scala:53)
        at kamon.trace.MetricsOnlyContext.withNewSegment(MetricsOnlyContext.scala:28)
        at filodb.core.Perftools$.subtrace(Perftools.scala:26)
        at filodb.core.reprojector.DefaultReprojector$$anonfun$toSegments$1$$anonfun$apply$2.apply(Reprojector.scala:92)
        at filodb.core.reprojector.DefaultReprojector$$anonfun$toSegments$1$$anonfun$apply$2.apply(Reprojector.scala:79)
        at kamon.trace.Tracer$$anonfun$withNewContext$1.apply(TracerModule.scala:62)
        at kamon.trace.Tracer$.withContext(TracerModule.scala:53)
        at kamon.trace.Tracer$.withNewContext(TracerModule.scala:61)
        at kamon.trace.Tracer$.withNewContext(TracerModule.scala:77)
        at filodb.core.reprojector.DefaultReprojector$$anonfun$toSegments$1.apply(Reprojector.scala:79)
        at filodb.core.reprojector.DefaultReprojector$$anonfun$toSegments$1.apply(Reprojector.scala:78)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
        at scala.collection.immutable.List.map(List.scala:285)
        at filodb.core.reprojector.DefaultReprojector.toSegments(Reprojector.scala:78)
        at filodb.core.reprojector.DefaultReprojector$$anonfun$reproject$1$$anonfun$apply$3.apply(Reprojector.scala:118)
        at filodb.core.reprojector.DefaultReprojector$$anonfun$reproject$1$$anonfun$apply$3.apply(Reprojector.scala:117)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
        at scala.collection.AbstractTraversable.map(Traversable.scala:104)
        at filodb.core.reprojector.DefaultReprojector$$anonfun$reproject$1.apply(Reprojector.scala:117)
        at filodb.core.reprojector.DefaultReprojector$$anonfun$reproject$1.apply(Reprojector.scala:117)
        at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
        at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
18/04/10 02:40:47 WARN NodeCoordinatorActor: Actor Actor[akka://filo-spark/user/coordinator/ds-coord-parDFNF-0#-1168010970] has terminated!  Ingestion for (filodb.parDFNF,0) will stop.
18/04/10 02:41:07 WARN RddRowSourceActor:  ==> (filodb.parDFNF_0_0_1) No Acks received for last 20 seconds
18/04/10 02:41:07 WARN RddRowSourceActor:  ==> (filodb.parDFNF_0_1_0) No Acks received for last 20 seconds


**Steps to reproduce**
scala> val files=Seq("/root/FiloDB/FiloDB/parquetfile/fragment1522917434336000000.dat","/root/FiloDB/FiloDB/parquetfile/fragment1522917494312000000.dat")
files: Seq[String] = List(/root/FiloDB/FiloDB/parquetfile/fragment1522917434336000000.dat, /root/FiloDB/FiloDB/parquetfile/fragment1522917494312000000.dat)

scala>  val parDF=sqlContext.read.parquet(files:_*)
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
18/04/10 02:39:20 WARN General: Plugin (Bundle) "org.datanucleus" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL "file:/root/FiloDB/spark-2.0.0-bin-hadoop2.7/jars/datanucleus-core-3.2.10.jar" is already registered, and you are trying to register an identical plugin located at URL "file:/root/FiloDB/spark/jars/datanucleus-core-3.2.10.jar."
18/04/10 02:39:20 WARN General: Plugin (Bundle) "org.datanucleus.store.rdbms" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL "file:/root/FiloDB/spark-2.0.0-bin-hadoop2.7/jars/datanucleus-rdbms-3.2.9.jar" is already registered, and you are trying to register an identical plugin located at URL "file:/root/FiloDB/spark/jars/datanucleus-rdbms-3.2.9.jar."
18/04/10 02:39:20 WARN General: Plugin (Bundle) "org.datanucleus.api.jdo" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL "file:/root/FiloDB/spark-2.0.0-bin-hadoop2.7/jars/datanucleus-api-jdo-3.2.6.jar" is already registered, and you are trying to register an identical plugin located at URL "file:/root/FiloDB/spark/jars/datanucleus-api-jdo-3.2.6.jar."
parDF: org.apache.spark.sql.DataFrame = [epoch: bigint, rowNum: bigint ... 38 more fields]

scala> val parDFnew=parDF.withColumn("exporterIp", 'exporterIp.cast("String")).withColumn("srcIp", 'srcIp.cast("String")).withColumn("dstIp", 'dstIp.cast("String")).withColumn("nextHopIp", 'nextHopIp.cast("String")).withColumn("bgpNextHopIp", 'bgpNextHopIp.cast("String")).withColumn("appId", 'appId.cast("String")).withColumn("policyQosClassificationHierarchy", 'policyQosClassificationHierarchy.cast("String")).withColumn("protocolId", 'protocolId.cast("Int")).withColumn("srcTos", 'srcTos.cast("Int")).withColumn("dstTos", 'dstTos.cast("Int")).withColumn("srcMask", 'srcMask.cast("Int")).withColumn("dstMask", 'dstMask.cast("Int")).withColumn("direction", 'direction.cast("String")).select('epoch, 'srcIp, 'dstIp, 'exporterIp, 'rowNum, 'exportMs, 'pktSeqNum, 'flowSeqNum, 'protocolId, 'srcTos, 'dstTos, 'srcMask, 'dstMask, 'tcpBits, 'srcPort, 'inIfId, 'inIfEntityId, 'inIfEnabled, 'dstPort, 'outIfId, 'outIfEntityId, 'outIfEnabled, 'direction, 'inOctets, 'outOctets, 'inPackets, 'outPackets, 'nextHopIp, 'bgpSrcAsNum, 'bgpDstAsNum, 'bgpNextHopIp, 'endMs, 'startMs, 'appId, 'appName, 'srcIpGroup, 'dstIpGroup, 'policyQosClassificationHierarchy, 'policyQosQueueId, 'workerId)
parDFnew: org.apache.spark.sql.DataFrame = [epoch: bigint, srcIp: string ... 38 more fields]

parDFnew.write.format("filodb.spark").
             option("dataset", "parDFNF").
             option("row_keys", "appId").
             option("partition_keys", "exportMs").
			 mode(SaveMode.Overwrite).save()

**Logs**

```
some log
```

or as attached file (see below)


Unused parts of this template should be removed (including this line).

dse-spark1.4.8 branch

For example => 
select * from loadtest.fdb_partition_test_chunks where "partition"=0x016101620163016401650166 and version=0; This query gives me data when i run it against C*.

But when i translate this query as a thrift query then no data returned.
select * from fdb_loadtest_partitiontest where field1='a' and field2='b' and field3='c' and field4='d'
and field5='e' and field6='f'