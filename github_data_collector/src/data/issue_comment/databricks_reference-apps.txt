That's kinda *really* important no?

https://databricks.gitbooks.io/databricks-spark-reference-applications/content/LICENSE/

Using jdk8, Maven 3.3.3, Eclipse Mars Release (4.5.0), Log-Analyzer application

Trying to import a Maven project into eclipse (Import 'existing Maven project'). Getting error: CoreException: Could not calculate build plan: Plugin org.apache.maven.plugins:maven-compiler-plugin:2.3.2 or one of its dependencies could not be resolved: Failed to read artifact descriptor for org.apache.maven.plugins:maven-compiler-plugin:jar:2.3.2: ArtifactResolutionException: Failure to transfer org.apache.maven.plugins:maven-compiler-plugin:pom:2.3.2 from https://repo.maven.apache.org/maven2

Have tried to install this plugin- whatever I tried 'Install New Software"- did not work. Tried adding a couple of M2E connectors also. I now have: 
m2e connector for maven-remote-resources-plugin
m2e connector for the Maven Dependency Plugin

How to solve this? Thanks
While running the sample WeatherApp I get the following exception while (embedded) Kafka is trying to connect to the ZooKeeper instance. Pls advice me to resolve this issue.

[INFO] [2016-11-28 17:53:49,727] [org.apache.zookeeper.ClientCnxn]: Opening socket connection to server 192.168.0.8/192.168.0.8:2181. Will not attempt to authenticate using SASL (unknown error)
[INFO] [2016-11-28 17:53:55,834] [org.apache.zookeeper.ZooKeeper]: Session: 0x0 closed
[INFO] [2016-11-28 17:53:55,834] [org.apache.zookeeper.ClientCnxn]: EventThread shut down
[ERROR] [2016-11-28 17:53:55,838] [org.apache.zookeeper.server.NIOServerCnxnFactory]: Thread Thread[main,5,main] died
org.I0Itec.zkclient.exception.ZkTimeoutException: Unable to connect to zookeeper server within timeout: 6000
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:880) ~[zkclient-0.3.jar:0.3]
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98) ~[zkclient-0.3.jar:0.3]
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84) ~[zkclient-0.3.jar:0.3]
	at com.datastax.spark.connector.embedded.EmbeddedKafka.<init>(EmbeddedKafka.scala:29) ~[spark-cassandra-connector-embedded_2.10-1.1.0.jar:1.1.0]
	at com.datastax.spark.connector.embedded.EmbeddedKafka.<init>(EmbeddedKafka.scala:18) ~[spark-cassandra-connector-embedded_2.10-1.1.0.jar:1.1.0]
	at com.datastax.spark.connector.embedded.EmbeddedKafka.<init>(EmbeddedKafka.scala:23) ~[spark-cassandra-connector-embedded_2.10-1.1.0.jar:1.1.0]
	at com.databricks.apps.WeatherApp$delayedInit$body.apply(WeatherApp.scala:46) ~[classes/:na]
	at scala.Function0$class.apply$mcV$sp(Function0.scala:40) ~[scala-library.jar:na]
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12) ~[scala-library.jar:na]
	at scala.App$$anonfun$main$1.apply(App.scala:71) ~[scala-library.jar:na]
	at scala.App$$anonfun$main$1.apply(App.scala:71) ~[scala-library.jar:na]
	at scala.collection.immutable.List.foreach(List.scala:318) ~[scala-library.jar:na]
	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:32) ~[scala-library.jar:na]
	at scala.App$class.main(App.scala:71) ~[scala-library.jar:na]
	at com.databricks.apps.WeatherApp$.main(WeatherApp.scala:40) ~[classes/:na]
	at com.databricks.apps.WeatherApp.main(WeatherApp.scala) ~[classes/:na]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_45]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_45]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_45]
	at java.lang.reflect.Method.invoke(Method.java:497) ~[na:1.8.0_45]
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147) ~[idea_rt.jar:na]

There are examples illustrating Streaming with SQL processing. I suppose, in Spark 2 the preferred way of processing streaming data with SQL queries is [Spark Structured Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html). Makes sense to rework at least one example to illustrate this technique. Candidates are:
- LogAnalyzerStreamingSQL
- LogAnalyzerStreamingImportDirectory

Guidelines to run examples force user to run Spark local master with 4 cores:
`--master local[4]`
Real number of logical cores may differ from user to user. It would be better to let Spark decide how many cores it needs:
`--master local[*]`

As Spark 2 is not a breaking news anymore, it's time to make reference apps working with it.
I can see at least the following things to do:
- Use SparkSession instead of SQLContext
- Use Dataset API
- Update JavaDStream.foreachRDD() invocations to new contract (returns nothing)
- Update JavaPairDStream.updateStateByKey() to new contract (uses Spark implementation of Optional)

Hi, Guys!

I tried the Log Analyzer examples. In particular the LogAnalyzerSql one. I include in my classpath spark-core_2.10.1.6.1 as well as spark-sql_2.10.1.6.2 but I keep getting compilation errors: cannot Access Row class... if I use other jar versions (1.1.0, etc.) I keep getting missing classes (DataFrames etc.) Could any one tell me which versions of these jars should I include to run  the LogAnalyzerSql examples? I have google this but nobody else seems to have any similar issue...

thanks!

 Aim - API call from any machine that submits a Spark job to Spark EC2 cluster Job runs perfectly well - Python file running on Localhost- Apache Spark However, unable to run it on Apache Spark EC2.

API call

```
 curl -X POST http://ec2-54-209-108-127.compute-1.amazonaws.com:6066/v1/submissions/create --header "Content-Type:application/json;charset=UTF-8" --data '{
  "action" : "CreateSubmissionRequest",
  "appArgs" : [ "" ],
  "appResource" : "wordcount.py",
  "clientSparkVersion" : "1.5.0",
  "environmentVariables" : {
    "SPARK_ENV_LOADED" : "1"
  },
  "mainClass" : "",
  "sparkProperties" : {
    "spark.jars" : "wordcount.py",
    "spark.driver.supervise" : "true",
    "spark.app.name" : "MyJob",
    "spark.eventLog.enabled": "true",
    "spark.submit.deployMode" : "cluster",
    "spark.master" : "spark://ec2-54-209-108-127.compute-1.amazonaws.com:6066"
  }}'

{
  "action" : "CreateSubmissionResponse",
  "message" : "Driver successfully submitted as driver-20160712145703-0003",
  "serverSparkVersion" : "1.6.1",
  "submissionId" : "driver-20160712145703-0003",
  "success" : true
}
```

To get the response, following API returns error - File not found

```
curl  http://ec2-54-209-108-127.compute-1.amazonaws.com:6066/v1/submissions/status/driver-20160712145703-0003
{
  "action" : "SubmissionStatusResponse",
  "driverState" : "ERROR",
  "message" : "Exception from the cluster:\njava.io.FileNotFoundException: wordcount.py (No such file or directory)\n\tjava.io.FileInputStream.open(Native Method)\n\tjava.io.FileInputStream.<init>(FileInputStream.java:146)\n\torg.spark-project.guava.io.Files$FileByteSource.openStream(Files.java:124)\n\torg.spark-project.guava.io.Files$FileByteSource.openStream(Files.java:114)\n\torg.spark-project.guava.io.ByteSource.copyTo(ByteSource.java:202)\n\torg.spark-project.guava.io.Files.copy(Files.java:436)\n\torg.apache.spark.util.Utils$.org$apache$spark$util$Utils$$copyRecursive(Utils.scala:539)\n\torg.apache.spark.util.Utils$.copyFile(Utils.scala:510)\n\torg.apache.spark.util.Utils$.doFetchFile(Utils.scala:595)\n\torg.apache.spark.util.Utils$.fetchFile(Utils.scala:394)\n\torg.apache.spark.deploy.worker.DriverRunner.org$apache$spark$deploy$worker$DriverRunner$$downloadUserJar(DriverRunner.scala:150)\n\torg.apache.spark.deploy.worker.DriverRunner$$anon$1.run(DriverRunner.scala:79)",
  "serverSparkVersion" : "1.6.1",
  "submissionId" : "driver-20160712145703-0003",
  "success" : true,
  "workerHostPort" : "172.31.17.189:59433",
  "workerId" : "worker-20160712083825-172.31.17.189-59433"
}
```

Awaiting suggestions and improvements. p.s. - newbie in Apache Spark..

Update API call (Set the main class, appArgs, appResource, clientSparkVersion to updated value) ->

```
curl -X POST http://ec2-54-209-108-127.compute-1.amazonaws.com:6066/v1/submissions/create{
"action" : "CreateSubmissionRequest",
"appArgs" : [ "/wordcount.py" ],
"appResource" : "file:/wordcount.py",
"clientSparkVersion" : "1.6.1",
"environmentVariables" : {
"SPARK_ENV_LOADED" : "1"
},
"mainClass" : "org.apache.spark.deploy.SparkSubmit",
"sparkProperties" : {
"spark.driver.supervise" : "false",
"spark.app.name" : "Simple App",
"spark.eventLog.enabled": "true",
"spark.submit.deployMode" : "cluster",
"spark.master" : "spark://ec2-54-209-108-127.compute-1.amazonaws.com:6066"
}
}
```

Hi, 
I have successfully compiled the Twitter classifier sample and I am trying to run the first program to collect the tweets. When I run the example I am running into this issue: 

`16/06/13 21:52:43 ERROR scheduler.ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error receiving tweets - sun.security.validator.ValidatorException: PKIX path validation failed: java.security.cert.CertPathValidatorException: Algorithm constraints check failed: SHA1withRSA
Relevant discussions can be found on the Internet at:
        http://www.google.co.jp/search?q=d0031b0b or
        http://www.google.co.jp/search?q=1db75522
TwitterException{exceptionCode=[d0031b0b-1db75522 db667dea-99334ae4 db667dea-99334ae4 db667dea-99334ae4], statusCode=-1, message=null, code=-1, retryAfter=-1, rateLimitStatus=null, version=3.0.3}
        at twitter4j.internal.http.HttpClientImpl.request(HttpClientImpl.java:192)
        at twitter4j.internal.http.HttpClientWrapper.request(HttpClientWrapper.java:61)
        at twitter4j.internal.http.HttpClientWrapper.get(HttpClientWrapper.java:89)
        at twitter4j.TwitterStreamImpl.getSampleStream(TwitterStreamImpl.java:176)
        at twitter4j.TwitterStreamImpl$4.getStream(TwitterStreamImpl.java:164)
        at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:462)
Caused by: javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path validation failed: java.security.cert.CertPathValidatorException: Algorithm constraints check failed: SHA1withRSA`

My java is 
/usr/jdk64/java-1.8.0-openjdk-1.8.0.45-28.b13.el6_6.x86_64/jre/bin/java
