```
// The Schlick Fresnel approximation is:
//
// R = R(0) + (1 - R(0)) (1 - cos theta)^5,
//
// where R(0) is the reflectance at normal indicence.
inline Float SchlickWeight(Float cosTheta) {
    Float m = Clamp(1 - cosTheta, 0, 1);
    return (m * m) * (m * m) * m;
}

inline Float FrSchlick(Float R0, Float cosTheta) {
    return Lerp(SchlickWeight(cosTheta), R0, 1);
}

inline Spectrum FrSchlick(const Spectrum &R0, Float cosTheta) {
    return Lerp(SchlickWeight(cosTheta), R0, Spectrum(1.));
}
```
Lerp here is (1 - SchlickWeight(cosTheta)) * R0 + SchlickWeight(cosTheta)
not R0 + (1 - R0) * SchlickWeight(cosTheta)

Hi, I am learning the bssrdf code of PBRT, and possibly found a problem.

The problem occurs when I was trying to render a sqare with BSSRDF material. The original scene file is from the material test ball scene from Tungsten rendering resource page. And I remove the all of objects except a floor.

The strange thing is when I set subsurface material to the floor, the rendering result is totaly black. The original material for the floor is a lambert, which, of course, show something in the result.

Then, I tried to look into the shape of the floor, the floor actually has a normal facing downwards , i.e. the normal is [0 0 -1] .  The rendering result looks OK when material is Lamber and floor facing downwards, which means pbrt works with "two sided shading", right? But, it seems that bssdf has a problem here. (bssrdf works, if I revert the floor's normal manually, i.e. make the normal [0 0 1])

I guess, the reason for this "black floor" is as following:
The bssrdf has a bsdf to sample reflection or transmission at the boudary at po, i.e. bssrdf will only happen when a transmission is sampled. The reason for being black is that transmission actually can NOT be sampled at boundary.

I guess, there might be an inconsistency regarding the direction defined for wo in the following code.

    Spectrum FresnelSpecular::Sample_f(
                  const Vector3f &wo, 
                  Vector3f *const Point2f &u, 
                  Float *BxDFType *sampledType) const {

    Float F = FrDielectric(CosTheta(wo), etaA, etaB);        <---- here F is always 1

    if (u[0] < F) {
       ……

when the floor is facing downwards, wo here has z value that is less than zero; but here, FrDielectric() use the sign of wo.z to judge if the ray is entering or exiting the boundary.

The "black floor" is due to that for the case wo.z < 0, and the program thought the ray is exiting the boudary, and then total reflection happens here (F =1), and no transmission sampled for bssrdf! so the image is "all black".

the following the scene file that trigger this problem, uncomment the mesh to see the problem

--------------- scene file ------------

Integrator "path" "integer maxdepth" [ 10 ] 
Transform [ 0.721367 -0.373123 -0.583445 -0 -0 0.842456 -0.538765 -0 -0.692553 -0.388647 -0.60772 -0 0.0258668 -0.29189 5.43024 1]
Sampler "random" "integer pixelsamples" [ 16 ] 
PixelFilter "triangle" "float xwidth" [ 1.000000 ] "float ywidth" [ 1.000000 ] 
Film "image" "integer xresolution" [ 512 ] "integer yresolution" [ 512 ] "string filename" [ "sss_problem.exr" ] 
Camera "perspective" "float fov" [ 20.114292 ] 

WorldBegin
    
        MakeNamedMaterial "sss_1" 
        "string type" [ "subsurface" ]  
        "float g" [0.0]
        "float eta" [1.5]
        "string name" ["Skin1"]
        "float scale" [1]
        "rgb Kr" [0 0 0]

    MakeNamedMaterial "sss_10" 
        "string type" [ "subsurface" ]  
        "float g" [0.0]
        "float eta" [1.5]
        "string name" ["Skin1"]
        "float scale" [10]
   

    LightSource "point" "point from" [0 5 0] "rgb I" [100 100 100]
    
    NamedMaterial "sss_1" 

    # bssrdf does NOT works
    #Shape "trianglemesh" "integer indices" [ 0 1 2 0 2 3 ] "point P" [ -0.785994 0 3.11108 -4.55196 -4.75246e-007 -0.80933 -0.63155 0 -4.57529 3.13441 4.75246e-007 -0.654886 ] "normal N" [ 0 -1 0 0 -1 0 0 -1 0 0 -1 0 ] "float uv" [ 0 0 1 0 1 1 0 1 ] 

    # bssrdf works, if I revert the floor's normal manually
    Shape "trianglemesh" "integer indices" [ 0 1 2 0 2 3 ] "point P" [ -0.785994 0 3.11108 -4.55196 -4.75246e-007 -0.80933 -0.63155 0 -4.57529 3.13441 4.75246e-007 -0.654886 ] "normal N" [ 0 1 0 0 1 0 0 1 0 0 1 0 ] "float uv" [ 0 0 1 0 1 1 0 1 ] 

WorldEnd










One of the GitHub features I'm not comfortable with is embedding external dependencies. It can be convenient for quickly trying out a project (you download everything recursively, build everything together, and there you go), but when you finish your first glance and you move into seriously using a project, then you need to install it, and you realize that you are duplicating code because you are using the external dependencies for more than one project (and each of them embeds and builds its own copy --although quite often the "install" target is broken in that workflow).

I want to use pbrt tightly integrated in another application, so the first thing is to get a tidy and clean source tree and build workflow, where dependencies are parallel to pbrt (at the same hierarchy level in the source tree and not inside it).

Is it currently possible to invoke Cmake in some way to achieve this?

Of course I could fork pbrt and tune it to my needs, but I prefer to avoid doing that, because when you do your custom modified fork, applying commits from the official fork becomes harder and harder with time...

So, if I could achieve this with the official source, unmodified, it would be great!
"volpath" is not included in the list of available integrators in https://pbrt.org/fileformat-v3.html 
According to the formula, dpdu is not a normalized vector, but si->shading.dpdu is normalized.
```
// Compute bump-mapped differential geometry
    Vector3f dpdu = si->shading.dpdu +
                    (uDisplace - displace) / du * Vector3f(si->shading.n) +
                    displace * Vector3f(si->shading.dndu);


    Vector3f dpdv = si->shading.dpdv +
                    (vDisplace - displace) / dv * Vector3f(si->shading.n) +
                    displace * Vector3f(si->shading.dndv);
```
This is a part of code in "Triangle::Intersect", ts and ss are normalized, the values are assigned to shading.dpdu and shading.dpdv. 
```
        Vector3f ts = Cross(ss, ns);
        if (ts.LengthSquared() > 0.f) {
            ts = Normalize(ts);
            ss = Cross(ts, ns);
        } else
            CoordinateSystem((Vector3f)ns, &ss, &ts);
        isect->SetShadingGeometry(ss, ts, dndu, dndv, true);
```
Is the code wrong?
FourierBSDFTable's fields don't get deleted when the struct is being destroyed.
in file "src/shapes/sphere.h", line 55 to line 58
since z = r * cos(theta) according to the book. Theta is in [0, pi]. Obviously, z is a monotone decreasing function to theta in [0, pi]. 
Is it should be 
          thetaMin(std::acos(Clamp(std::**max**(zMin, zMax) / radius, -1, 1))),
          thetaMax(std::acos(Clamp(std::**min**(zMin, zMax) / radius, -1, 1))),
since if zMax equals 1 ( let's say it's a unit sphere), the theta should be 0, which is the minimum value in its range.
I know it wont make such a difference but shouldn't you stop recursion when the number of primitives in current node is equal or bellow maxPrimsInNode ?

In the current version, `end == start + 1` then the for loop is useless.

I suppose you wanted the if condition to be `if (nPrimitives <= maxPrimsInNode) { ... }`

https://github.com/mmp/pbrt-v3/blob/3f94503ae1777cd6d67a7788e06d67224a525ff4/src/accelerators/bvh.cpp#L248
So, I was using my own scene and rendering it from different view points by tweaking the lookat values in the pbrt file. I have the 3D coordinates of the model that I am rendering so I would like to do a 3D to 2D projection to find out where some specific point lies on a rendered image. I am using the openCV projectPoints function however it requires to know the camera intrinsics and extrinsics.  I know the extrinsics using the lookat matrix however i dont know the focal length / focal distance for the default camera in PBRT. As far as I am concerned the default camera in PBRT uses a Pinhole model where by default the focal length is 0. But using a value of 0 in the camera matrix does not make sense and also the values that I am getting for the 3D points are very close to the principal point. The principal point is half_width, half_height of my rendered images. So basically I am looking for a value for my focal lenght. 

Can anyone help me with this? Or am I missing something? 

Thanks for reading, any sort of help will be appreciated!
I build the pbrt-v3 on Linux, Windows, MacOS. Every rendering on these OS is normal and can be generate the exr results (for a CG scene by myself). But I encounter an issue that the linux rendering exr result is "Decode error" when trying to open it while the result of same behavior on Windows and MacOS is normal.
What is wrong about the Linux version pbrt rendering result? And how to solve this issue?

Looking forward to get some reply and thanks in advance.