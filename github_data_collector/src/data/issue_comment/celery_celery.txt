Hello!

I have a lot of http requests to make in my django project that uses celery for task processing.

I tried using eventlet and gevent as a pool executor and I immediately ran into problems when using eventlet. Gevent seems to work, I will give it more data to work on in the near future, to see how it behaves.

I've read in the source code that they both should patch themselves on the celery __init__.

The error that I got with eventlet is:

``` python
DatabaseWrapper objects created in a thread can only be used in that same thread. The object with alias 'default' was created in thread id x and this is thread id y.
```

these are the most important requirements of the project

```
aioredis==1.3.1
amqp==2.5.2
asgiref==3.2.3
Babel==2.8.0
billiard==3.6.1.0
celery==4.4.0
channels==2.4.0
channels-redis==2.4.1
daphne==2.4.1
decorator==4.4.1
Django==3.0.2
django-redis==4.11.0
dnspython==1.16.0
eventlet==0.25.1
flower==0.9.3
gevent==1.4.0
greenlet==0.4.15
hiredis==1.0.1
kombu==4.6.7
psycopg2-binary==2.8.4
redis==3.3.11
vine==1.3.0
```

These are my celery settings

```
CELERY_BROKER_URL = 'redis://localhost:6379/1'
CELERY_RESULT_BACKEND = 'redis://localhost:6379/1'
CELERY_RESULT_EXPIRES = timedelta(hours=6)
CELERY_ACCEPT_CONTENT = ['application/json']
CELERY_TASK_SERIALIZER = 'json'
CELERY_RESULT_SERIALIZER = 'json'
CELERY_TIMEZONE = TIME_ZONE
CELERY_WORKER_PREFETCH_MULTIPLIER = 1
```

I've read on stack overflow and such that it is mandatory to monkey_patch eventlet before anything else, but that seem like it's taken care of here https://github.com/celery/celery/blob/d0563058f8f47f347ac1b56c44f833f569764482/celery/__init__.py#L103


If you need further information just let me know.

I'm open on helping in coding and testing eventlet pool worker.
![微信图片_20200116211115](https://user-images.githubusercontent.com/47964538/72527912-c5a5f080-38a4-11ea-87ba-a8f53e20f83f.jpg)
Hello, I met a problem in the use of celery, need help, the problem is as follows, our celery after receiving the task have been waiting for, but does not perform task, and we are executing task is empty, this should not have to do?Above, I use a flower monitoring data, waiting for more tasks, our celery is blocked, I restart the service, these services are performed again, and normal
*Note*: Before submitting this pull request, please review our [contributing
guidelines](https://docs.celeryproject.org/en/master/contributing.html).

## Description
This fixes #5919 by adding reconnection logic to the Redis result backend which restores the pubsub context and deals with tasks that changed state while the connection was down.
<!-- Please describe your pull request.

NOTE: All patches should be made against master, not a maintenance branch like
3.1, 2.5, etc.  That is unless the bug is already fixed in master, but not in
that version series.

If it fixes a bug or resolves a feature request,
be sure to link to that issue via (Fixes #4412) for example.
-->

<!--
Please fill this template entirely and do not erase parts of it.
We reserve the right to close without a response
bug reports which are incomplete.
-->
# Checklist
<!--
To check an item on the list replace [ ] with [x].
-->
- [ ] I have verified that the issue exists against the `master` branch of Celery.
- [ ] This has already been asked to the [discussion group](https://groups.google.com/forum/#!forum/celery-users) first.
- [x] I have read the relevant section in the
  [contribution guide](http://docs.celeryproject.org/en/latest/contributing.html#other-bugs)
  on reporting bugs.
- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)
  for similar or identical bug reports.
- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)
  for existing proposed fixes.
- [x] I have checked the [commit log](https://github.com/celery/celery/commits/master)
  to find out if the bug was already fixed in the master branch.
- [x] I have included all related issues and possible duplicate issues
  in this issue (If there are none, check this box anyway).

## Mandatory Debugging Information

- [x] I have included the output of ``celery -A proj report`` in the issue.
    (if you are not able to do this, then at least specify the Celery
     version affected).
- [ ] I have verified that the issue exists against the `master` branch of Celery. 
      XXX: I've checked the commits since 4.4.0 and I couldn't see anything related.
- [x] I have included the contents of ``pip freeze`` in the issue.
- [x] I have included all the versions of all the external dependencies required
  to reproduce this bug.

## Optional Debugging Information
<!--
Try some of the below if you think they are relevant.
It will help us figure out the scope of the bug and how many users it affects.
-->
- [ ] I have tried reproducing the issue on more than one Python version
  and/or implementation.
- [ ] I have tried reproducing the issue on more than one message broker and/or
  result backend.
- [ ] I have tried reproducing the issue on more than one version of the message
  broker and/or result backend.
- [ ] I have tried reproducing the issue on more than one operating system.
- [ ] I have tried reproducing the issue on more than one workers pool.
- [ ] I have tried reproducing the issue with autoscaling, retries,
  ETA/Countdown & rate limits disabled.
- [ ] I have tried reproducing the issue after downgrading
  and/or upgrading Celery and its dependencies.

## Related Issues and Possible Duplicates
<!--
Please make sure to search and mention any related issues
or possible duplicates to this issue as requested by the checklist above.

This may or may not include issues in other repositories that the Celery project
maintains or other repositories that are dependencies of Celery.

If you don't know how to mention issues, please refer to Github's documentation
on the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests
-->

#### Related Issues

- Couldn't find any. This might very well be a kombu or py-amqp bug, but its hard to tell from here.

#### Possible Duplicates

- None

## Environment & Settings
<!-- Include the contents of celery --version below -->
**Celery version**: 4.4.0 (cliffs)
<!-- Include the output of celery -A proj report below -->
<details>
<summary><b><code>celery report</code> Output:</b></summary>
<p>

```
software -> celery:4.4.0 (cliffs) kombu:4.6.7 py:3.5.9
            billiard:3.6.1.0 py-amqp:2.5.2
platform -> system:Linux arch:64bit
            kernel version:4.9.0-11-amd64 imp:CPython
loader   -> celery.loaders.app.AppLoader
settings -> transport:amqp results:rpc:///

task_default_exchange: 'konditorei.queued.lifecycle'
worker_pool_restarts: True
task_default_queue: 'konditorei.queued.lifecycle'
accept_content: ['json']
timezone: 'Europe/Vienna'
result_serializer: 'json'
result_backend: 'rpc:///'
broker_url: 'amqp://celery:********@db02:5672/celery'
worker_redirect_stdouts_level: 'info'
task_serializer: 'json'
task_queues: [<unbound Queue konditorei.queued.lifecycle -> <unbound Exchange konditorei.queued.lifecycle(direct)> -> konditorei.queued.lifecycle>]
task_default_routing_key: '********'
worker_log_format: ('%(asctime)s PID:%(process)d '
 '%(processName)s/%(name)s %(levelname)s '
 '%(message)s')
result_expires: datetime.timedelta(0, 3600)
```

</p>
</details>

# Steps to Reproduce

## Required Dependencies
<!-- Please fill the required dependencies to reproduce this issue -->
* **Minimal Python Version**: Unknown, we use 3.5.9
* **Minimal Celery Version**: Unknown, we use 4.4.0
* **Minimal Kombu Version**: Unknown, we use 4.6.7
* **Minimal Broker Version**: Unknown, we use RabbitMQ 3.6.6-1
* **Minimal Result Backend Version**: N/A or Unknown
* **Minimal OS and/or Kernel Version**: We use kernel 4.9.0-11-amd64 on Debian stretch 9.11
* **Minimal Broker Client Version**: Unknown, we use amqp 2.5.2
* **Minimal Result Backend Client Version**: N/A or Unknown

### Python Packages
<!-- Please fill the contents of pip freeze below -->
<details>
<summary><b><code>pip freeze</code> Output:</b></summary>
<p>

```
amqp==2.5.2
appdirs==1.4.3
attrs==19.1.0
Babel==2.8.0
backcall==0.1.0
bcrypt==3.1.7
billiard==3.6.1.0
cached-property==1.5.1
celery==4.4.0
certifi==2019.11.28
cffi==1.13.2
chardet==3.0.4
cryptography==2.8
debtcollector==1.22.0
decorator==4.4.1
defusedxml==0.6.0
et-xmlfile==1.0.1
idna==2.8
importlib-metadata==1.3.0
ipython==7.9.0
ipython-genutils==0.2.0
iso8601==0.1.12
isodate==0.6.0
jdcal==1.4.1
jedi==0.15.2
Jinja2==2.10.3
jsonschema==2.5.1
keystoneauth1==3.18.0
kombu==4.6.7
lxml==4.4.2
MarkupSafe==1.1.1
more-itertools==8.0.2
msgpack==0.6.2
munch==2.5.0
netaddr==0.7.19
netifaces==0.10.9
openpyxl==2.6.4
os-service-types==1.7.0
oslo.config==7.0.0
oslo.i18n==3.25.1
oslo.serialization==2.29.2
oslo.utils==3.42.1
paramiko==2.7.1
parso==0.5.2
pbr==5.4.4
pexpect==4.7.0
pickleshare==0.7.5
prompt-toolkit==2.0.10
psycopg2==2.8.4
ptyprocess==0.6.0
pycparser==2.19
Pygments==2.5.2
PyNaCl==1.3.0
pyparsing==2.4.6
python-dateutil==2.8.1
python-keystoneclient==3.22.0
python-redis-lock==3.4.0
python-stdnum==1.12
python-swiftclient==3.7.1
pytz==2019.3
PyYAML==5.3
redis==3.3.11
requests==2.22.0
requests-toolbelt==0.9.1
rfc3986==1.3.2
six==1.13.0
SQLAlchemy==1.3.12
stevedore==1.31.0
traitlets==4.3.3
urllib3==1.25.7
vine==1.3.0
wcwidth==0.1.8
Werkzeug==0.11.15
wrapt==1.11.2
xmltodict==0.12.0
zeep==3.4.0
zipp==0.6.0
```

(local packages omitted)

</p>
</details>

### Other Dependencies
<!--
Please provide system dependencies, configuration files
and other dependency information if applicable
-->
<details>
<p>
N/A
</p>
</details>

## Minimally Reproducible Test Case
<!--
Please provide a reproducible test case.
Refer to the Reporting Bugs section in our contribution guide.

We prefer submitting test cases in the form of a PR to our integration test suite.
If you can provide one, please mention the PR number below.
If not, please attach the most minimal code example required to reproduce the issue below.
If the test case is too large, please include a link to a gist or a repository below.
-->

<details>
Unclear. Might not be related to our code in the first place.
</details>

# Expected Behavior
<!-- Describe in detail what you expect to happen -->

Start/stop should not show any WARNINGs.

# Actual Behavior
<!--
Describe in detail what actually happened.
Please include a backtrace and surround it with triple backticks (```).
In addition, include the Celery daemon logs, the broker logs,
the result backend logs and system logs below if they will help us debug
the issue.
-->

We automatically reload our workers on deploys, and this tends to happen in parallel (`systemctl reload celery*`, which in turn calls `/bin/kill -TERM $MAINPID`).

We also run multiple workers (for different queues) on the same machine; they all have a dedicated `-n` set.

This all works pretty nicely, but 1 out of 20 reloads it logs something like this:

```
.../amqp WARNING Received method (60, 31) during closing channel 1. This method will be ignored
```
(`(60, 31)` appears to be `CancelOk`)

In the log below you can also see weird `Closed channel #None` stuff happening.

For this log, I've hacked in logging of the Channel and Connection object IDs, so it's a bit clearer which objects were involved.

Workers are started like this:

```
python -m celery worker -A konditorei.queued.lifecycle -Q konditorei.queued.lifecycle -n konditorei.queued.lifecycle.devworkers01 --autoscale=5,1 -f /srv/logs/celery/celery-konditorei.queued.lifecycle.log -O fair --loglevel=INFO
```


```
2020-01-14 22:46:45,326 PID:27417 MainProcess/celery.bootsteps DEBUG | Worker: Preparing bootsteps.
2020-01-14 22:46:45,327 PID:27417 MainProcess/celery.bootsteps DEBUG | Worker: Building graph...
2020-01-14 22:46:45,328 PID:27417 MainProcess/celery.bootsteps DEBUG | Worker: New boot order: {Timer, Hub, Pool, Autoscaler, StateDB, Beat, Consumer}
2020-01-14 22:46:45,336 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Preparing bootsteps.
2020-01-14 22:46:45,337 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Building graph...
2020-01-14 22:46:45,427 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: New boot order: {Connection, Events, Mingle, Tasks, Control, Agent, Gossip, Heart, event loop}
2020-01-14 22:46:45,432 PID:27417 MainProcess/celery.bootsteps DEBUG | Worker: Starting Hub
2020-01-14 22:46:45,433 PID:27417 MainProcess/celery.bootsteps DEBUG ^-- substep ok
2020-01-14 22:46:45,433 PID:27417 MainProcess/celery.bootsteps DEBUG | Worker: Starting Pool
2020-01-14 22:46:45,478 PID:27417 MainProcess/celery.bootsteps DEBUG ^-- substep ok
2020-01-14 22:46:45,480 PID:27417 MainProcess/celery.bootsteps DEBUG | Worker: Starting Autoscaler
2020-01-14 22:46:45,480 PID:27417 MainProcess/celery.bootsteps DEBUG ^-- substep ok
2020-01-14 22:46:45,481 PID:27417 MainProcess/celery.bootsteps DEBUG | Worker: Starting Consumer
2020-01-14 22:46:45,482 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Starting Connection
2020-01-14 22:46:45,492 PID:27417 MainProcess/amqp DEBUG Start from server, version: 0.9, properties: {'cluster_name': 'node01@db02', 'version': '3.6.6', 'copyright': 'Copyright (C) 2007-2016 Pivotal Software, Inc.', 'informat
ion': 'Licensed under the MPL.  See http://www.rabbitmq.com/', 'platform': 'Erlang/OTP', 'product': 'RabbitMQ', 'capabilities': {'connection.blocked': True, 'exchange_exchange_bindings': True, 'consumer_priorities': True, 'per_consumer_qos': True, 'publi
sher_confirms': True, 'direct_reply_to': True, 'basic.nack': True, 'authentication_failure_close': True, 'consumer_cancel_notify': True}}, mechanisms: [b'PLAIN', b'AMQPLAIN'], locales: ['en_US']
2020-01-14 22:46:45,494 PID:27417 MainProcess/celery.worker.consumer.connection INFO Connected to amqp://celery:**@db02:5672/celery
2020-01-14 22:46:45,494 PID:27417 MainProcess/celery.bootsteps DEBUG ^-- substep ok
2020-01-14 22:46:45,495 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Starting Events
2020-01-14 22:46:45,503 PID:27417 MainProcess/amqp DEBUG Start from server, version: 0.9, properties: {'cluster_name': 'node01@db02', 'version': '3.6.6', 'copyright': 'Copyright (C) 2007-2016 Pivotal Software, Inc.', 'informat
ion': 'Licensed under the MPL.  See http://www.rabbitmq.com/', 'platform': 'Erlang/OTP', 'product': 'RabbitMQ', 'capabilities': {'connection.blocked': True, 'exchange_exchange_bindings': True, 'consumer_priorities': True, 'per_consumer_qos': True, 'publi
sher_confirms': True, 'direct_reply_to': True, 'basic.nack': True, 'authentication_failure_close': True, 'consumer_cancel_notify': True}}, mechanisms: [b'PLAIN', b'AMQPLAIN'], locales: ['en_US']
2020-01-14 22:46:45,505 PID:27417 MainProcess/celery.bootsteps DEBUG ^-- substep ok
2020-01-14 22:46:45,506 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Starting Mingle
2020-01-14 22:46:45,507 PID:27417 MainProcess/celery.worker.consumer.mingle INFO mingle: searching for neighbors
2020-01-14 22:46:45,508 PID:27417 MainProcess/amqp DEBUG <kombu.transport.pyamqp.Channel object at 0x7fea1a45cac8> using channel_id: 1 of connection: <kombu.transport.pyamqp.Connection object at 0x7fea1a45c0f0>
2020-01-14 22:46:45,510 PID:27417 MainProcess/amqp DEBUG Channel open
2020-01-14 22:46:45,522 PID:27417 MainProcess/amqp DEBUG Start from server, version: 0.9, properties: {'cluster_name': 'node01@db02', 'version': '3.6.6', 'copyright': 'Copyright (C) 2007-2016 Pivotal Software, Inc.', 'informat
ion': 'Licensed under the MPL.  See http://www.rabbitmq.com/', 'platform': 'Erlang/OTP', 'product': 'RabbitMQ', 'capabilities': {'connection.blocked': True, 'exchange_exchange_bindings': True, 'consumer_priorities': True, 'per_consumer_qos': True, 'publi
sher_confirms': True, 'direct_reply_to': True, 'basic.nack': True, 'authentication_failure_close': True, 'consumer_cancel_notify': True}}, mechanisms: [b'PLAIN', b'AMQPLAIN'], locales: ['en_US']
2020-01-14 22:46:45,524 PID:27417 MainProcess/amqp DEBUG <kombu.transport.pyamqp.Channel object at 0x7fea1a4729e8> using channel_id: 1 of connection: <kombu.transport.pyamqp.Connection object at 0x7fea1a472a20>
2020-01-14 22:46:45,525 PID:27417 MainProcess/amqp DEBUG Channel open
2020-01-14 22:46:46,532 PID:27417 MainProcess/celery.worker.consumer.mingle INFO mingle: all alone
2020-01-14 22:46:46,533 PID:27417 MainProcess/celery.bootsteps DEBUG ^-- substep ok
2020-01-14 22:46:46,534 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Starting Tasks
2020-01-14 22:46:46,541 PID:27417 MainProcess/celery.bootsteps DEBUG ^-- substep ok
2020-01-14 22:46:46,541 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Starting Control
2020-01-14 22:46:46,542 PID:27417 MainProcess/amqp DEBUG <kombu.transport.pyamqp.Channel object at 0x7fea1a481ac8> using channel_id: 2 of connection: <kombu.transport.pyamqp.Connection object at 0x7fea1a45c0f0>
2020-01-14 22:46:46,544 PID:27417 MainProcess/amqp DEBUG Channel open
2020-01-14 22:46:46,548 PID:27417 MainProcess/celery.bootsteps DEBUG ^-- substep ok
2020-01-14 22:46:46,549 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Starting Gossip
2020-01-14 22:46:46,549 PID:27417 MainProcess/amqp DEBUG <kombu.transport.pyamqp.Channel object at 0x7fea1a481d30> using channel_id: 3 of connection: <kombu.transport.pyamqp.Connection object at 0x7fea1a45c0f0>
2020-01-14 22:46:46,551 PID:27417 MainProcess/amqp DEBUG Channel open
2020-01-14 22:46:46,558 PID:27417 MainProcess/celery.bootsteps DEBUG ^-- substep ok
2020-01-14 22:46:46,559 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Starting Heart
2020-01-14 22:46:46,560 PID:27417 MainProcess/amqp DEBUG <kombu.transport.pyamqp.Channel object at 0x7fea1a4232e8> using channel_id: 1 of connection: <kombu.transport.pyamqp.Connection object at 0x7fea1a45c6d8>
2020-01-14 22:46:46,561 PID:27417 MainProcess/amqp DEBUG Channel open
2020-01-14 22:46:46,563 PID:27417 MainProcess/celery.bootsteps DEBUG ^-- substep ok
2020-01-14 22:46:46,564 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Starting event loop
2020-01-14 22:46:46,565 PID:27417 MainProcess/celery.bootsteps DEBUG | Worker: Hub.register Autoscaler...
2020-01-14 22:46:46,566 PID:27417 MainProcess/celery.bootsteps DEBUG | Worker: Hub.register Pool...

... ready, heartbeat (but often no tasks on these development workers) ...

2020-01-14 22:52:34,516 PID:27417 MainProcess/celery.bootsteps DEBUG | Worker: Closing Hub...
2020-01-14 22:52:34,516 PID:27417 MainProcess/celery.bootsteps DEBUG | Worker: Closing Pool...
2020-01-14 22:52:34,517 PID:27417 MainProcess/celery.bootsteps DEBUG | Worker: Closing Autoscaler...
2020-01-14 22:52:34,518 PID:27417 MainProcess/celery.bootsteps DEBUG | Worker: Closing Consumer...
2020-01-14 22:52:34,519 PID:27417 MainProcess/celery.bootsteps DEBUG | Worker: Stopping Consumer...
2020-01-14 22:52:34,519 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Closing Connection...
2020-01-14 22:52:34,520 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Closing Events...
2020-01-14 22:52:34,521 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Closing Mingle...
2020-01-14 22:52:34,522 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Closing Tasks...
2020-01-14 22:52:34,523 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Closing Control...
2020-01-14 22:52:34,524 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Closing Gossip...
2020-01-14 22:52:34,525 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Closing Heart...
2020-01-14 22:52:34,525 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Closing event loop...
2020-01-14 22:52:34,526 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Stopping event loop...
2020-01-14 22:52:34,527 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Stopping Heart...
2020-01-14 22:52:34,528 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Stopping Gossip...
2020-01-14 22:52:34,531 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Stopping Control...
2020-01-14 22:52:34,531 PID:27417 MainProcess/amqp DEBUG Closed channel #3 obj <kombu.transport.pyamqp.Channel object at 0x7fea1a481d30>
2020-01-14 22:52:34,532 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Stopping Tasks...
2020-01-14 22:52:34,533 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Stopping Mingle...
2020-01-14 22:52:34,534 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Stopping Events...
2020-01-14 22:52:34,534 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Stopping Connection...
2020-01-14 22:52:34,535 PID:27417 MainProcess/celery.bootsteps DEBUG | Worker: Stopping Autoscaler...
2020-01-14 22:52:34,535 PID:27417 MainProcess/celery.bootsteps DEBUG | Worker: Stopping Pool...
2020-01-14 22:52:35,542 PID:27417 MainProcess/celery.bootsteps DEBUG | Worker: Stopping Hub...
2020-01-14 22:52:35,543 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Shutdown Heart...
2020-01-14 22:52:35,543 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Shutdown Gossip...
2020-01-14 22:52:35,544 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Shutdown Control...
2020-01-14 22:52:35,545 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Shutdown Tasks...
2020-01-14 22:52:35,545 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Shutdown Events...
2020-01-14 22:52:35,546 PID:27417 MainProcess/amqp DEBUG Closed channel #1 obj <kombu.transport.pyamqp.Channel object at 0x7fea1a4232e8>
2020-01-14 22:52:35,548 PID:27417 MainProcess/celery.bootsteps DEBUG | Consumer: Shutdown Connection...
2020-01-14 22:52:35,548 PID:27417 MainProcess/amqp DEBUG Closed channel #2 obj <kombu.transport.pyamqp.Channel object at 0x7fea1a481ac8>
2020-01-14 22:52:35,549 PID:27417 MainProcess/amqp WARNING Received method (60, 31) during closing channel 1. This method will be ignored
2020-01-14 22:52:35,550 PID:27417 MainProcess/amqp DEBUG Closed channel #1 obj <kombu.transport.pyamqp.Channel object at 0x7fea1a45cac8>
2020-01-14 22:52:35,550 PID:27417 MainProcess/amqp DEBUG Closed channel #None obj <kombu.transport.pyamqp.Channel object at 0x7fea1a481ac8>
2020-01-14 22:52:35,551 PID:27417 MainProcess/amqp DEBUG Closed channel #None obj <kombu.transport.pyamqp.Channel object at 0x7fea1a481d30>
```

<!--
Please fill this template entirely and do not erase parts of it.
We reserve the right to close without a response
enhancement requests which are incomplete.
-->
# Checklist
<!--
To check an item on the list replace [ ] with [x].
-->

- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Enhancement%22+-label%3A%22Category%3A+Documentation%22)
  for similar or identical enhancement to an existing feature.
- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22Issue+Type%3A+Enhancement%22+-label%3A%22Category%3A+Documentation%22)
  for existing proposed enhancements.
- [x] I have checked the [commit log](https://github.com/celery/celery/commits/master)
  to find out if the if the same enhancement was already implemented in the
  master branch.
- [x] I have included all related issues and possible duplicate issues in this issue
      (If there are none, check this box anyway).

## Related Issues and Possible Duplicates
<!--
Please make sure to search and mention any related issues
or possible duplicates to this issue as requested by the checklist above.

This may or may not include issues in other repositories that the Celery project
maintains or other repositories that are dependencies of Celery.

If you don't know how to mention issues, please refer to Github's documentation
on the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests
-->

#### Related Issues

- None

#### Possible Duplicates

- None

# Brief Summary
<!--
Please include a brief summary of what the enhancement is
and why it is needed.
-->
When there is a connection error with Redis while executing a command, in most cases, the redis client will [discard the connection](https://github.com/andymccurdy/redis-py/blob/272d3139e1c82c2d89551f87d12df7c18d938ea2/redis/client.py#L885-L886), causing the next command sent to Redis to open a new connection. This allows applications to recover from connection errors by simply retrying, a property that is used in Celery, for example when setting keys in the Redis result backend: https://github.com/celery/celery/blob/d0563058f8f47f347ac1b56c44f833f569764482/celery/backends/redis.py#L324-L325

This is not the case however when the [connection to Redis is in a pubsub state](https://github.com/andymccurdy/redis-py/blob/272d3139e1c82c2d89551f87d12df7c18d938ea2/redis/client.py#L3397-L3414). The reason for that is that some state associated with the connection (namely the list of keys subscibed to). The Redis client doesn't keep track of this state, so it can't possibly restore it when creating a new connection and leaves the connection handling to the application code.

The Celery Redis result consumer uses pubsub in order to be notified when results are available, but doesn't handle connection errors at all, causing a result consumer to end up in a state where it can't connect to the result backend any more after a single connection error, as any further attempt will reuse the same faulty connection.

The solution would be to add error handling logic to the result consumer, so it will recreate the connection on connection errors and initialize it to the proper state.

# Design

## Architectural Considerations
<!--
If more components other than Celery are involved,
describe them here and the effect it would have on Celery.
-->
None

## Proposed Behavior
<!--
Please describe in detail how this enhancement is going to change the behavior
of an existing feature.
Describe what happens in case of failures as well if applicable.
-->
Add error handling in all places the Redis result consumer sends a Redis command in a pubsub context:
* https://github.com/celery/celery/blob/d0563058f8f47f347ac1b56c44f833f569764482/celery/backends/redis.py#L127
* https://github.com/celery/celery/blob/d0563058f8f47f347ac1b56c44f833f569764482/celery/backends/redis.py#L142
* https://github.com/celery/celery/blob/d0563058f8f47f347ac1b56c44f833f569764482/celery/backends/redis.py#L148

We should catch all Redis connection errors, and call a new method that will reinitialize a pubsub connection in the proper state (discard the current connection from the pool, start the pubsub context, subscribe to all keys in `ResultConsumer.subscribed_to`) using the retry policy. If in `drain_events`, we should try to get new messages again. 

This will take care of most issues with connection errors. I see two remaining issues:
1.Some message might have been lost (sent between losing the connection and reconnecting). We could read all keys subscribed to right after reconnecting and before starting the pubsub context and call `on_state_change` for each existing key, but this might cause some messages to be delivered twice and I don't know how Celery will react to that.
2. If the connection can't be re-established despite the retries and reaches max-retries, the result consumer will end up with a faulty connection that can't be recovered from. This should be communicated somehow to the user (documentation, logging an explicit error message, custom exception).

## Proposed UI/UX
<!--
Please provide your ideas for the API, CLI options,
configuration key names etc. that will be adjusted for this enhancement.
-->
None

## Diagrams
<!--
Please include any diagrams that might be relevant
to the implementation of this enhancement such as:
* Class Diagrams
* Sequence Diagrams
* Activity Diagrams
You can drag and drop images into the text box to attach them to this issue.
-->
N/A

## Alternatives
<!--
If you have considered any alternative implementations
describe them in detail below.
-->
None

#5917
*Note*: Before submitting this pull request, please review our [contributing
guidelines](https://docs.celeryproject.org/en/master/contributing.html).

## Description

<!-- Please describe your pull request.

NOTE: All patches should be made against master, not a maintenance branch like
3.1, 2.5, etc.  That is unless the bug is already fixed in master, but not in
that version series.

If it fixes a bug or resolves a feature request,
be sure to link to that issue via (Fixes #4412) for example.
-->

<!--
Please fill this template entirely and do not erase parts of it.
We reserve the right to close without a response
bug reports which are incomplete.
-->
# Checklist
<!--
To check an item on the list replace [ ] with [x].
-->
- [x] I have verified that the issue exists against the `master` branch of Celery.
- [ ] This has already been asked to the [discussion group](https://groups.google.com/forum/#!forum/celery-users) first.
- [x] I have read the relevant section in the
  [contribution guide](http://docs.celeryproject.org/en/latest/contributing.html#other-bugs)
  on reporting bugs.
- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)
  for similar or identical bug reports.
- [x] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)
  for existing proposed fixes.
- [ ] I have checked the [commit log](https://github.com/celery/celery/commits/master)
  to find out if the bug was already fixed in the master branch.
- [ ] I have included all related issues and possible duplicate issues
  in this issue (If there are none, check this box anyway).

## Mandatory Debugging Information

- [ ] I have included the output of ``celery -A proj report`` in the issue.
    (if you are not able to do this, then at least specify the Celery
     version affected).
- [x] I have verified that the issue exists against the `master` branch of Celery.
- [ ] I have included the contents of ``pip freeze`` in the issue.
- [ ] I have included all the versions of all the external dependencies required
  to reproduce this bug.

## Optional Debugging Information
<!--
Try some of the below if you think they are relevant.
It will help us figure out the scope of the bug and how many users it affects.
-->
- [x] I have tried reproducing the issue on more than one Python version
  and/or implementation.
- [x] I have tried reproducing the issue on more than one message broker and/or
  result backend.
- [x] I have tried reproducing the issue on more than one version of the message
  broker and/or result backend.
- [x] I have tried reproducing the issue on more than one operating system.
- [x] I have tried reproducing the issue on more than one workers pool.
- [x] I have tried reproducing the issue with autoscaling, retries,
  ETA/Countdown & rate limits disabled.
- [ ] I have tried reproducing the issue after downgrading
  and/or upgrading Celery and its dependencies.

## Related Issues and Possible Duplicates
<!--
Please make sure to search and mention any related issues
or possible duplicates to this issue as requested by the checklist above.

This may or may not include issues in other repositories that the Celery project
maintains or other repositories that are dependencies of Celery.

If you don't know how to mention issues, please refer to Github's documentation
on the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests
-->

#### Related Issues

I use mongo broker and mongo backend.
There is a decoding error while try to get task result with AsyncResult

`File "task_result.py", line 9, in <module>
    print(res.result)
......
\python37-32\Lib\json\__init__.py", line 341, in loads
    raise TypeError(f'the JSON object must be str, bytes or bytearray, '
kombu.exceptions.DecodeError: the JSON object must be str, bytes or bytearray, not dict`

tasks.py


```
> @celery.task(name='web.add', bind=True)
> def add_test(self, x):
>     time.sleep(6)
>     message = 'IN WORKER'
>     self.update_state(state='PROGRESS', meta={ 'current': 50, 'total': 100, 'status': message})
>     time.sleep(10)
>     message = 'END'
>     return { 'current': 100, 'total': 100, 'status': message, 'result': { 'video_url': 42, 'video_player_url' : 'https://invidza.com' } }
```

task_result.py
```
>res = add.AsyncResult('d6605146-9296-463f-9463-9795d6b87f37')
>print(res)
>print(res.result)
```


#### Possible Duplicates

- None

## Environment & Settings
<!-- Include the contents of celery --version below -->
**Celery version**:
<!-- Include the output of celery -A proj report below -->
<details>
<summary><b><code>celery report</code> Output:</b></summary>
<p>

```
```

</p>
</details>

# Steps to Reproduce

## Required Dependencies
<!-- Please fill the required dependencies to reproduce this issue -->
* **Minimal Python Version**: N/A or Unknown
* **Minimal Celery Version**: N/A or Unknown
* **Minimal Kombu Version**: N/A or Unknown
* **Minimal Broker Version**: N/A or Unknown
* **Minimal Result Backend Version**: N/A or Unknown
* **Minimal OS and/or Kernel Version**: N/A or Unknown
* **Minimal Broker Client Version**: N/A or Unknown
* **Minimal Result Backend Client Version**: N/A or Unknown

### Python Packages
<!-- Please fill the contents of pip freeze below -->
<details>
<summary><b><code>pip freeze</code> Output:</b></summary>
<p>

```
```

</p>
</details>

### Other Dependencies
<!--
Please provide system dependencies, configuration files
and other dependency information if applicable
-->
<details>
<p>
N/A
</p>
</details>

## Minimally Reproducible Test Case
<!--
Please provide a reproducible test case.
Refer to the Reporting Bugs section in our contribution guide.

We prefer submitting test cases in the form of a PR to our integration test suite.
If you can provide one, please mention the PR number below.
If not, please attach the most minimal code example required to reproduce the issue below.
If the test case is too large, please include a link to a gist or a repository below.
-->

<details>
<p>

```python
```

</p>
</details>

# Expected Behavior
result = {'current': 50, 'total': 100, 'status': 'IN WORKER'}

# Actual Behavior
<!--
Describe in detail what actually happened.
Please include a backtrace and surround it with triple backticks (```).
In addition, include the Celery daemon logs, the broker logs,
the result backend logs and system logs below if they will help us debug
the issue.
-->

<!--
Please fill this template entirely and do not erase parts of it.
We reserve the right to close without a response
bug reports which are incomplete.
-->
# Checklist
<!--
To check an item on the list replace [ ] with [x].
-->
- [x] I have verified that the issue exists against the `master` branch of Celery.
- [ ] This has already been asked to the [discussion group](https://groups.google.com/forum/#!forum/celery-users) first.
- [x] I have read the relevant section in the
  [contribution guide](http://docs.celeryproject.org/en/latest/contributing.html#other-bugs)
  on reporting bugs.
- [x] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)
  for similar or identical bug reports.
- [ ] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)
  for existing proposed fixes.
- [ ] I have checked the [commit log](https://github.com/celery/celery/commits/master)
  to find out if the bug was already fixed in the master branch.
- [ ] I have included all related issues and possible duplicate issues
  in this issue (If there are none, check this box anyway).

## Mandatory Debugging Information

- [ ] I have included the output of ``celery -A proj report`` in the issue.
    (if you are not able to do this, then at least specify the Celery
     version affected).
- [x] I have verified that the issue exists against the `master` branch of Celery.
- [ ] I have included the contents of ``pip freeze`` in the issue.
- [ ] I have included all the versions of all the external dependencies required
  to reproduce this bug.

## Optional Debugging Information
<!--
Try some of the below if you think they are relevant.
It will help us figure out the scope of the bug and how many users it affects.
-->
- [ ] I have tried reproducing the issue on more than one Python version
  and/or implementation.
- [ ] I have tried reproducing the issue on more than one message broker and/or
  result backend.
- [ ] I have tried reproducing the issue on more than one version of the message
  broker and/or result backend.
- [ ] I have tried reproducing the issue on more than one operating system.
- [ ] I have tried reproducing the issue on more than one workers pool.
- [ ] I have tried reproducing the issue with autoscaling, retries,
  ETA/Countdown & rate limits disabled.
- [ ] I have tried reproducing the issue after downgrading
  and/or upgrading Celery and its dependencies.

## Related Issues and Possible Duplicates
<!--
Please make sure to search and mention any related issues
or possible duplicates to this issue as requested by the checklist above.

This may or may not include issues in other repositories that the Celery project
maintains or other repositories that are dependencies of Celery.

If you don't know how to mention issues, please refer to Github's documentation
on the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests
-->

#### Related Issues

- None

#### Possible Duplicates

- None

## Environment & Settings
<!-- Include the contents of celery --version below -->
**Celery version**:
<!-- Include the output of celery -A proj report below -->
<details>
<summary><b><code>celery report</code> Output:</b></summary>
<p>

```
```

</p>
</details>

# Steps to Reproduce

## Required Dependencies
<!-- Please fill the required dependencies to reproduce this issue -->
* **Minimal Python Version**: 3.7
* **Minimal Celery Version**: 4.3.0
* **Minimal Kombu Version**: N/A or Unknown
* **Minimal Broker Version**: N/A or Unknown
* **Minimal Result Backend Version**: N/A or Unknown
* **Minimal OS and/or Kernel Version**: N/A or Unknown
* **Minimal Broker Client Version**: N/A or Unknown
* **Minimal Result Backend Client Version**: N/A or Unknown

### Python Packages
<!-- Please fill the contents of pip freeze below -->
<details>
<summary><b><code>pip freeze</code> Output:</b></summary>
<p>

```
```

</p>
</details>

### Other Dependencies
<!--
Please provide system dependencies, configuration files
and other dependency information if applicable
-->
<details>
<p>
N/A
</p>
</details>

## Minimally Reproducible Test Case
<!--
Please provide a reproducible test case.
Refer to the Reporting Bugs section in our contribution guide.

We prefer submitting test cases in the form of a PR to our integration test suite.
If you can provide one, please mention the PR number below.
If not, please attach the most minimal code example required to reproduce the issue below.
If the test case is too large, please include a link to a gist or a repository below.
-->

<details>
<p>

```python
start = time.time()
print(celery.control.ping(timeout=5))
end = time.time()
print(end - start)
```

</p>
</details>

# Expected Behavior
I would expect .ping() to return as soon as it gets a response from the celery workers.

# Actual Behavior
Whatever value the timeout is set to, it seems to add that value in time to the check, running the above is returning values such as `5.477962970733643`, if the timeout is set to 0.3 I get values such as `0.49860501289367676`. 

The docs state:
`timeout (float): Timeout in seconds to wait for the reply.`

It appears to be waiting for at least the timeout amount + the wait for the reply. Is my interpretation of "timeout" here wrong? 
This will show current retry progress so it will clear confusion about how many retries will be tried for connecting to broker.
I still leave the default value in 100, because If I change it, I'm afraid it will affect many celery apps that was okay or used to with the current default value 100 in `broker_connection_max_retries`.
May fixes #4556 
python version: 3.7.5

package version:
  celery==4.4.0
  flower==0.9.3
  kombu==4.6.7
  redis==3.3.11
  tornado==5.1.1

when I started flower wirh celery like this: celery flower --broker=redis://10.200.161.50:6301/7

[I 200111 14:42:07 command:136] Visit me at http://localhost:5555
[I 200111 14:42:07 command:141] Broker: redis://10.200.161.50:6301/7
[I 200111 14:42:07 command:144] Registered tasks:
['celery.accumulate',
'celery.backend_cleanup',
'celery.chain',
'celery.chord',
'celery.chord_unlock',
'celery.chunks',
'celery.group',
'celery.map',
'celery.starmap']
Fatal Python error: Cannot recover from stack overflow.

Thread 0x00007000070ce000 (most recent call first):
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/lib/python3.7/socket.py", line 414 in _real_close
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/lib/python3.7/socket.py", line 420 in close
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 635 in disconnect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/kombu/transport/redis.py", line 953 in disconnect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 711 in read_response
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 645 in check_health
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 661 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 623 in on_connect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 548 in connect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 658 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 650 in check_health
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 661 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 623 in on_connect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 548 in connect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 658 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 650 in check_health
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 661 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 623 in on_connect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 548 in connect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 658 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 650 in check_health
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 661 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 623 in on_connect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 548 in connect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 658 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 650 in check_health
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 661 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 623 in on_connect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 548 in connect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 658 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 650 in check_health
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 661 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 623 in on_connect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 548 in connect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 658 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 650 in check_health
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 661 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 623 in on_connect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 548 in connect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 658 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 650 in check_health
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 661 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 623 in on_connect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 548 in connect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 658 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 650 in check_health
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 661 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 623 in on_connect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 548 in connect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 658 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 650 in check_health
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 661 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 623 in on_connect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 548 in connect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 658 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 650 in check_health
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 661 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 623 in on_connect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 548 in connect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 658 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 650 in check_health
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 661 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 623 in on_connect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 548 in connect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 658 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 650 in check_health
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 661 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 623 in on_connect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 548 in connect
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 658 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 650 in check_health
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 661 in send_packed_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 687 in send_command
File "/Users/yeyuqiu/.pyenv/versions/3.7.5/envs/schedule/lib/python3.7/site-packages/redis/connection.py", line 623 in on_connect
...