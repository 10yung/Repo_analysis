title says it all
Hello, 
I really like Figaro, thanks for it.
I use Figaro for several internal projects, I even got practical-probabilistic-programming from Manning :), but there are a few questions:
* Is this project dead? 
* Are there any plans for a future release?

Dear Figaro Team,

No luck when installing Figaro 5 on Mac OS X / both with Scala 2.12 and 2.11. No installation is successful.
Error message: The application "figaro-5.0.0.0-2.12-osx-installer.app" can not be opened; It may be damaged or incomplete.

Tried a few times on different macs and no success. Please check if anyone can install these on their computers.
Thanks !
Best,
I was poking around but could not find anything: does Figaro include any test for comparing whether two elements are generating from the same distribution? (could be KL divergence, but pretty much anything will do for me that reasonably reliably detects if we are dealing with the same distribution)
Hi team,

I just wrote [this wiki entry](https://github.com/p2t2/figaro/wiki/Discuss-Demonstrations-and-Examples), but I'm not sure whether that results in a notification to any committers, so I wanted to bring it to your attention more explicitly. My apologies if this isn't the appropriate process!
Downloading the binary figaro-5.0.0.0-2.12-linux-x64-installer.run gives a corrupted file
I'm a beginer in  figaro and my IDE is Intellij. when I run a simple code without println(VariableElimination.probability()), I dont have any error. but when I use it .......
this is my program for testing:

import com.cra.figaro.language._
import com.cra.figaro.algorithm.factored.VariableElimination

object testFigaro {
  def main(args: Array[String]){ 
    val sunnyToday = Flip(0.2)
    println(VariableElimination.probability(sunnyToday, true))
  }
}

And this is error:

Exception in thread "main" java.lang.NoClassDefFoundError: scala/reflect/runtime/package$
	at com.cra.figaro.algorithm.factored.ProbQueryVariableElimination.makeResultFactor(VariableElimination.scala:238)
	at com.cra.figaro.algorithm.factored.ProbQueryVariableElimination.finish(VariableElimination.scala:242)
	at com.cra.figaro.algorithm.factored.VariableElimination.$anonfun$doElimination$5(VariableElimination.scala:175)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at com.cra.figaro.algorithm.factored.VariableElimination.optionallyShowTiming(VariableElimination.scala:63)
	at com.cra.figaro.algorithm.factored.VariableElimination.doElimination(VariableElimination.scala:175)
	at com.cra.figaro.algorithm.factored.VariableElimination.doElimination$(VariableElimination.scala:163)
	at com.cra.figaro.algorithm.factored.ProbQueryVariableElimination.doElimination(VariableElimination.scala:213)
	at com.cra.figaro.algorithm.factored.VariableElimination.ve(VariableElimination.scala:160)
	at com.cra.figaro.algorithm.factored.VariableElimination.ve$(VariableElimination.scala:155)
	at com.cra.figaro.algorithm.factored.ProbQueryVariableElimination.ve(VariableElimination.scala:213)
	at com.cra.figaro.algorithm.factored.VariableElimination.run(VariableElimination.scala:186)
	at com.cra.figaro.algorithm.factored.VariableElimination.run$(VariableElimination.scala:186)
	at com.cra.figaro.algorithm.factored.ProbQueryVariableElimination.run(VariableElimination.scala:213)
	at com.cra.figaro.algorithm.OneTime.doStart(OneTime.scala:28)
	at com.cra.figaro.algorithm.OneTime.doStart$(OneTime.scala:26)
	at com.cra.figaro.algorithm.factored.ProbQueryVariableElimination.doStart(VariableElimination.scala:213)
	at com.cra.figaro.algorithm.Algorithm.start(Algorithm.scala:83)
	at com.cra.figaro.algorithm.Algorithm.start$(Algorithm.scala:80)
	at com.cra.figaro.algorithm.factored.ProbQueryVariableElimination.start(VariableElimination.scala:213)
	at com.cra.figaro.algorithm.factored.VariableElimination$.probability(VariableElimination.scala:359)
	at com.cra.figaro.algorithm.factored.VariableElimination$.probability(VariableElimination.scala:369)
	at testFigaro$.main(testFigaro.scala:7)
	at testFigaro.main(testFigaro.scala)
Caused by: java.lang.ClassNotFoundException: scala.reflect.runtime.package$
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 24 more

Please help me 
What should I do to deal with?

This issue is partly trying to raise a red flag, and partly trying to grasp why Figaro's learning logic works the way it does. The way it works appears to hinder our efforts at machine learning by introducing statistical insignificance for observations regarding less than oftenly "relevant" or "active" cases in our model.
We'll try to provide a specific, boiled down, example:

Consider the following Figaro model:

    class Model(parameters: ParameterCollection, modelUniverse: Universe) {
        val flip = Flip(0.1)("myFlip", modelUniverse)

        val weightTrue = Flip(parameters.get("weightTrue"))("elem_weightTrue", modelUniverse)
        val weightFalse = Flip(parameters.get("weightFalse"))("elem_weightFalse", modelUniverse)

        val cpd = RichCPD(flip,
          (OneOf(true)) -> weightTrue,
          (*) -> weightFalse)("elem_cpd", modelUniverse)
    }

Note that the model contains only two regular elements, "cpd" and "flip" and the CPD is constructed using two different weights, one for the case when "flip" is true, and one for when it is false.

The parameters are both initialized as Beta(1,1) elements, postulating the fact that we have no prior beliefs regarding the parameters, and we place these Beta elements inside the ModelParameters construct.

Next, consider the following observations taking place:

    for (i <- 1 to 100) {
      val model = new Model(modelParameters.priorParameters, universe)
      model.flip.observe(true)
      model.cpd.observe(true)
    }

At this point, we've learned that the "cpd" element very oftenly turns out true, given that "flip" is also true. But what about "cpd"'s probability of being true, when "flip" is false? Indeed, we have had no observations that signify this case for us.
Accordingly, we get the following outcomes for the parameters' MAP values:

weightTrue -> 1.0 (101.0, 1.0)
weightFalse -> 0.5 (51.0, 51.0)

**First question:** What is up with the counting of the alpha and beta values of "weightFalse" here? As mentioned, we've not seen a single sample of what probability "cpd" should have when "flip" is false. Even so, it would appear that Figaro records 50 cases on either side. This gives us a probability of 50 %, which is actually fine - for now - but what happens when we also start observing things about the case where "flip" is observed to be false? Let's take a look:

We add a single additional Model-instance with observations:

    val model = new Model(modelParameters.priorParameters, universe)
    model.flip.observe(false)
    model.cpd.observe(true)

After this additional evidence is supplied, we _expect_ the following:
- We will maintain the same probability of "cpd" being true when "flip" is known to be true, namely 100 %. This is what we expect since the new observation does not fit into the bucket of _cases where "flip" is true_.
- We will expect that the probability of "cpd" being true when "flip" is known to be false to change from 50 % (unknown, Beta(1,1), no prior beliefs) to 100 % (having seen exactly one case to support the claim, and none for the opposite case ("cpd" being false when "flip" is false)).

However, here are the actual results:

weightTrue -> 0.9999999951950983 (101.99999951470492, 1.000000485295074)
weightFalse -> 0.5195098277585943 (53.470492603618055, 49.529507396382)

**Second question:** Why is it that our belief of weightFalse is not "1.0 (2.0, 1.0)" here? We've seen exactly one instance where "flip" was false, thus "activating" or "making relevant" this weight. Why is all this noise of the other "irrelevant" 100 instances present ?

**Third question:** Similarly for weightTrue: Why does this latest case affect weightTrue. Even in such an odd manner, to add "almost 1" to its learnedAlpha and nearly nothing to its learnedBeta.

**Notes:**
We used EMWithVE regular training for this test setup. Can provide the specific source-code if desired.
Using Online EM makes no difference.
Building the model without RichCPD (e.g. with If-constructs) makes no difference. Regardless of whether elements are declared before the If or inside of it (when testing whether the laziness of If mattered)

**Taking a broader perspective:**
In our actual model, we are dealing with a larger model, which contains more CPDs as in this fashion. Essentially, the effect of this odd behaviour has various effects on our training and thus our evaluations:

- All of our learned parameters are "flooded" by the statistical insignificance introduced in the same way as in the example above.
- All of our parameters are **heavily** "guided" by whatever prior beliefs we supply to the Beta-elements when initializing them.

Basically, Beta(1,1) isn't a special case where a problem exists. Say we initialize a parameter with Beta(2,5) (namely 1 positive case and 4 negative), then with no relevant cases observed, we get 20 % as expected, but not with learnedAlpha and learnedBeta values of 2 and 5 as expected, but instead (with 100 model-instances with observations) learnedAlpha = _22.2_ and learnedBeta = _85.8_).
This is concerning, since this reproduces the problem examplified above no matter what prior beliefs are passed on, since the significance of **one** actually relevant data instance wth observations is naturaly extremely low when compared to all data instances with observations in total.

_____________________

In general, we fear that this voids our usage of the built-in machine learning setup via the EM-algorithm in Figaro.

In case our concerns are actually what is void, we would very much like an explanation that either (a) defends the results observed, with backing in some theoretical foundation that we appear to be missing, or (b) suggests an error in our usage of Figaro, which makes it act strangely, along with a concrete modification to the given example, which will make Figaro behave as we expect.

We highly appreciate any input anyone may have toward a resolution for this problem.


Thank you very much,
Best regards,
Hopeful students of the IT University of Copenhagen,
Christian and Kenneth,
chnr@itu.dk and kulr@itu.dk

Since you helped us before, I am going to go ahead and add a tag for you, @apfeffer :)
Hi :)


We are doing our master's thesis at the IT University of Copenhagen, and we have a series of questions, that we hope there exists some useful answers for :)

We are working with a setup very similar to the spam-filter application case from chapter 3 in the "Practical Probabilistic Programming" book, and our questions regard the efficiency of learning for such a model. In essence, we have several model instances, which all reference some "shared" Beta-elements for learning, which in effect results in quite a large net of connected elements. We are looking to be able to perform the learning of our Beta-elements, but without having to evaluate the entire network of connected elements at once, but instead train and learn from each individual model instance one at a time instead.

Here are some more specific questions:

- Why does EMWithVE use a completely different setup (ExpectationMaximizationWithFactors) compared to the other inference algorithms when used with EM? What are the optimizations / differences that apply here - and is there some litterature that you could point us to that would help us understand some of the differences?

- If we attempt use GeneralizedEM with VE, it seems that that all active elements in the universe (thereby all our connected model instances) are passed as inputs to the inference algorithm. As the amount of model instances increases, this quickly becomes infeasible for an algorithm such as VE.
If we consider the spam filter case from Chapter 3, would it not be possible to use the inference algorithm on each sample separately and then combine their results during the expectation step, rather than attempting to calculate the sufficient statistics for all model instances' elements all at once?
We figured that this splitting-approach might be feasible with VE (if each individual model instance is not very complex), and also have the added benefit of being parallelizable (since each sample can be reasoned about separately) if we can use StructuredVE for the task. 
Is there a reason why this approach is not used? Is it not feasible? If it is possible, could you provide some pointers for how we can achieve this goal?


To bring about our perspective, we are trying to optimize our training-setup for our thesis work, such that an alteration to the probabilistic model will take a little time as possible to see the effect of - both in regards to training and of course evaluation. The setup with our model instances getting tangled into each other due to the shared Beta-elements seems to meddle with the efficiency of most inference algorithms in Figaro that are usable with EM. Is there some other approach that we could go with as an alternate setup?

As another note, we believe that we are able to build our model in such a fashion that we should have little to no hidden variables (namely 0 in the learning-case, and only a single one in the evaluation-phase), which should help the efficiency of whatever inference algorithm we end up with.
Also, according to litterature (https://ai.stanford.edu/~chuongdo/papers/em_tutorial.pdf), if one has no hidden variables, then you are in fact in the "complete data case", meaning that Maximum Likelihood Estimation should be feasible for the problem, namely the simple learning of the frequencies of one's dataset, rather than requiring the use of EM. Is there some way to access the MLE logic that is used as part of the EM-algorithm from somewhere in the source code?


Thanks a lot,
Hoping the best,
Best regards,
Christian and Kenneth,
Students at the IT University of Copenhagen, Denmark
Hi,

I am new to Figaro and I would like to create a model that will be used in more than one actor. In fact I have created a router with workers. They execute in different threads as you know.

Each worker actor creates new instance of a class that creates the model,  all variables, then make evidences and Inferences with VariableElimination. When I create only one worker, that is there is no multithreading, it works perfectly. Bu with more than one I got this error:
key not found: Select(0.01 -> 'visit, 0.99 -> 'novisit)

I have implemented the well known ChestClinic model like this:

 ```
 val universe = Universe.createNew()

  val worldTravel = Select(0.01 -> 'visit, 0.99 -> 'novisit) (s"worldTravel$msgIndex", universe)
  val smoking = Select(0.5 -> 'smoker, 0.5 -> 'nosmoker) (s"smoking$msgIndex", universe)

  val tuberculosis = CPD(worldTravel,
    'visit -> Select(0.05 -> 'present, 0.95 -> 'absent),
    'novisit -> Select(0.01 -> 'present, 0.99 -> 'absent)) (s"tuberculosis$msgIndex", universe)

  val lungCancer = CPD(smoking,
    'smoker -> Select(0.1 -> 'present, 0.9 -> 'absent),
    'nosmoker -> Select(0.01 -> 'present, 0.99 -> 'absent)) (s"lungCancer$msgIndex", universe)

  val bronChitis = CPD(smoking,
    'smoker -> Select(0.6 -> 'present, 0.4 -> 'absent),
    'nosmoker -> Select(0.3 -> 'present, 0.7 -> 'absent)) (s"bronChitis$msgIndex", universe)

  def tbOrCFcn(tuberculosis: Symbol, lungCancer: Symbol): Boolean = {
    (tuberculosis, lungCancer) match {
      case ('present, 'present) => true
      case ('present, 'absent) => true
      case ('absent, 'present)=> true
      case ('absent, 'absent) => false
    }
  }

  val tuberculosisOrCancer = Apply(tuberculosis, lungCancer, tbOrCFcn) (s"tuberculosisOrCancer$msgIndex", universe)

  val xRayPrior = Chain(tuberculosisOrCancer, (tb: Boolean) => if(tb) Constant(0.982) else Constant(0.02)) (s"xRayPrior$msgIndex", universe)
  val xRay = CPD(tuberculosisOrCancer,
    true -> Select(0.98 -> 'abnormal, 0.02 -> 'normal),
    false -> Select(0.05 -> 'abnormal, 0.95 -> 'normal)) (s"xRay$msgIndex", universe)

  val dyspnea = CPD(tuberculosisOrCancer, bronChitis,
    (true, 'present) -> Select(0.9 -> 'present, 0.1 -> 'absent),
    (true, 'absent) -> Select(0.7 -> 'present, 0.3 -> 'absent),
    (false, 'present) -> Select(0.8 -> 'present, 0.2 -> 'absent),
    (false, 'absent) -> Select(0.1 -> 'present, 0.9 -> 'absent)) (s"dyspnea$msgIndex", universe)
```
As you can see I have create a new Universe and created the variables in it. The msgIndex is an Int that indexes each worker actor created. Before that, using the default universe, it never worked but after that sometimes I can create the model, but the error appears in some executions.

The VariableElimination comes just after the model creation: 

```
val tbPriorAlg = VariableElimination(tuberculosisOrCancer) (universe)
  tbPriorAlg.start()
  val tbPrior = tbPriorAlg.probability(tuberculosisOrCancer, true)
  if(isDebug) println("Prior probability the Tuberculosis Or Cancer on = " + tbPrior.toString)
```

But even if I remove the VariableElimination, the error occurs. 

Can you help me ?

Thanks in advance