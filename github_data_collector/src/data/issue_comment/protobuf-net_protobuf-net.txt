We have a number of flat structures defined in data storage (i.e. a database) which are defined through a user interface.

These are essentially "types", but we do not have a CLR type that represents them.

We retrieve them as property bags and then generate JSON documents on the fly using JSON.NET to feed the client (a JS app).  This is not much of an issue, as the results stream from the data source (usually SQL Server or RavenDB) and we don't incur the cost of materializing everything in memory.

Additionally, we incur the overhead of serializing the data from the providers to an intermediate CLR type instance and then serialize *that* to the response stream (standard HTTP response from ASP.NET Core).

However, we have some very large payloads, with many rows (a few hundred K) which is sometimes wide (maybe 50 columns).

When this happens, the problem we have on the client is that we have to get the entire JSON string representing the response and deserialize that, which presents some resource constraint issues.  It would be preferable if we were able to use a serializer/serialization format will be able to yield/stream objects (or at least construct arrays) without needing to store the entire response up front.

One solution we have is to page the data and make multiple calls with smaller payloads; this has a number of problems. mainly:

- We'd have to run the query multiple times, which could lead to potentially inconsistent results
- We could store the results of the query up front on the server which we explicitly avoided up to now by streaming from the data provider to the response stream.

We also have thought about using [line-delimited JSON](https://en.wikipedia.org/wiki/JSON_streaming) but that presents it's own set of problems.

With that in mind, we are searching for other serialization format's that support the ðŸ‘† scenario.

Protobuf allows for this on the writer side, as [mentioned by @mgravell](https://stackoverflow.com/a/8272947/50776).

However, I'd still like to avoid needing to materialize to an intermediate CLR type if possible.  So the question becomes, can I define a schema at runtime without a CLR type and populate instances of that schema when streaming results?

On the JavaScript side, [protobufjs allows for defining a schema purely through reflection](https://github.com/protobufjs/protobuf.js#using-reflection-only), so we're covered there.

The idea is that we can define the same schema on the server and client side through the type definitions we have stored in our providers.

Thanks in advance.
We have data coming in to our eventhubs which are encoded using protobuf.net, the read however is from a spark pipeline which uses google's protbuf, I see that while this works fine for other fields, there are issues with Timestamp column. 
In the C# gen, the column used is DateTime? and in scala  it is google.protobuf.Timestamp .

While this doesn't fail during run time, the values produces are incorrect.
Below is one such value ( it was encoded for value in a recent date)
101926-07-17T13:39:34.000-07:00
I'm generating a list of grpc proto rpc method signatures and I need the Type Name that protobuf-net uses in its `RuntimeTypeModel.GetSchema()`. At the moment I'm using reflection to call the internal method which works as expected, but would obv. prefer to call a public API.
It would be great to add a differencer that can be used to determine the differences between two specified protocol messages. Similar to https://developers.google.com/protocol-buffers/docs/reference/cpp/google.protobuf.util.message_differencer#MessageDifferencer

This would allow notifying users of changes prior to processing them when using protocol buffers for serializing.
I'm trying to wrap my head around the following issue but I apologize if I misunderstand how things like this supposed to be implemented.

I have a custom type in our system that I would like to serialize using Protobuf. The type has implicit conversion to Int64, so I want to send it on the wire as if it would have been an Int64. On the receiving end (different types of consumers), this type does not exist, and I want it to look as if it was an Int64 in the first place.

This is a small repro example on what I'm trying to do:

    public struct CustomType {
        public static implicit operator long(CustomType d) => d.value;
        public static implicit operator CustomType(long d) => new CustomType(d);
        private long value;
        public CustomType(long value) {
            this.value = value;
        }
    }

    public class Item
    {
        [ProtoMember(1)]
        public CustomType CustomId { get; set; }

        [ProtoMember(2)]
        public string Description { get; set; }
    }

    static void Main(string[] args) {
            var item = new Item {
                CustomId = 12,
                Description = "Test"
            };

            var stream = new MemoryStream();
            Serializer.Serialize(stream, item); // This throws: Unhandled Exception: System.InvalidOperationException: Type is not expected, and no contract can be inferred: proto.Item
            ...
            ...

I first tried with declaring a surrogate, but then I couldn't deserialize it in the other end as an Int64, so then I tried using the 3.0.0 alpha version and writing a custom serializer for the "CustomType" above, that I couldn't make work (if that's even supposed to be possible to do?) since it complained about invalid WireType being string when trying to write it as a In64 (probably me implementing it wrong)
After upgrading (accidentally) to the latest version of protobuf-net, I can see a lot of errors with AsReference obsoletion.

I chose Protobuf-net to implement serialization as opposed to say json solely because of this feature - if a complex graph of objects with child and parent relationships and differenct branches connected through relationships needs to be serialized, there need to be some kind of identifiers added to identify relationships after deserialization to correctly rebuild object structure.

Obviously there is wire size benefit of serializing unique instances of objects only once, not to mention easy handling of cycles.

Additionally, worrying about re-connecting all relationships is obviously error prone and if serialization library (and protocol) supports such feature, it is a life saver.

It seems AsReference was chosen to be removed leaving protobuf-net in the same ball park as json serialization for objects with interconnected nodes (graphs).

Is there a way to implement AsReference feature as an add-on logic, which will hook into global process of property serialization with the purpose of identifying unique objects and enabling (de)serialization of them in one place and referencing them in other places?
it`s so hard for me about reading it.So many class that i cant remember anyone when i see it again.
Shall we make a graph about the Class Diagram...
Protogen does not compile `.proto` file with reserved keyword in enum.

Protogen version: 2.3.17+g4ec48abff8
Protoc version: libprotoc 3.10.1

Consider following `.proto` file:
``` protobuf
syntax = "proto3";
/**
 * Enumeration
 */
enum SomeEnum {
  /**
   * Reserve values
   */
  reserved 2, 4 to 9, 12 to max;
  UNKNOWN = 0;
  ENUM_01 = 1;
  ENUM_03 = 3;
  ENUM_10 = 10;
  ENUM_11 = 11;
}
```

`protogen` is not able to compile it:
```
>protogen --csharp_out=. test-reserved.proto
test-reserved.proto(10,12,10,13): error: expected Symbol '='
```

while `protoc` compiles it without issues:
```
>protoc --csharp_out=. test-reserved.proto
```
``` CSharp
// <auto-generated>
//     Generated by the protocol buffer compiler.  DO NOT EDIT!
//     source: test-reserved.proto
// </auto-generated>
#pragma warning disable 1591, 0612, 3021
#region Designer generated code

using pb = global::Google.Protobuf;
using pbc = global::Google.Protobuf.Collections;
using pbr = global::Google.Protobuf.Reflection;
using scg = global::System.Collections.Generic;
/// <summary>Holder for reflection information generated from test-reserved.proto</summary>
public static partial class TestReservedReflection {

  #region Descriptor
  /// <summary>File descriptor for test-reserved.proto</summary>
  public static pbr::FileDescriptor Descriptor {
    get { return descriptor; }
  }
  private static pbr::FileDescriptor descriptor;

  static TestReservedReflection() {
    byte[] descriptorData = global::System.Convert.FromBase64String(
        string.Concat(
          "ChN0ZXN0LXJlc2VydmVkLnByb3RvKmEKCFNvbWVFbnVtEgsKB1VOS05PV04Q",
          "ABILCgdFTlVNXzAxEAESCwoHRU5VTV8wMxADEgsKB0VOVU1fMTAQChILCgdF",
          "TlVNXzExEAsiBAgCEAIiBAgEEAkiCAgMEP////8HYgZwcm90bzM="));
    descriptor = pbr::FileDescriptor.FromGeneratedCode(descriptorData,
        new pbr::FileDescriptor[] { },
        new pbr::GeneratedClrTypeInfo(new[] {typeof(global::SomeEnum), }, null));
  }
  #endregion

}
#region Enums
/// <summary>
///*
/// Enumeration for lock type.
/// </summary>
public enum SomeEnum {
  [pbr::OriginalName("UNKNOWN")] Unknown = 0,
  [pbr::OriginalName("ENUM_01")] Enum01 = 1,
  [pbr::OriginalName("ENUM_03")] Enum03 = 3,
  [pbr::OriginalName("ENUM_10")] Enum10 = 10,
  [pbr::OriginalName("ENUM_11")] Enum11 = 11,
}

#endregion

#endregion Designer generated code 
```


Noticed an overload we need was missing, adding it here

hello,

I am wondering if it is anyhow possible to create a duplex communication with low level TCP Socket. Something like nettcpbinding in wcf.
Maybe also with help of Tcplistener and TCPClient. 