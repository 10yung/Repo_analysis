<!--- Provide a general summary of your changes in the Title above -->

## Description
<!--- Describe your changes in detail -->
Improved LRU by switching order and storing disk reads, increased default cache sizes

Add developers section to the documentation

## Description
Add IDEA set up guide and tests run instructions
We are getting "java.io.UTFDataFormatException: encoded string too long" while running "model.transform()" when the job is submitted as --deploy-mode cluster


org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:688) Caused by: java.io.UTFDataFormatException: encoded string too long: 76663 bytes at java.io.DataOutputStream.writeUTF(DataOutputStream.java:364) at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323) at

Spark version: 2.3.0
Spark nlp: 2.2.1
Scla: 2.11

When we run the spark job as "--deploy-mode client" it works fine.



Detailed stack trace:

User class threw exception: org.apache.spark.SparkException: Task not serializable at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:345) at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:335) at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:159) at org.apache.spark.SparkContext.clean(SparkContext.scala:2299) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:844) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:843) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) at org.apache.spark.rdd.RDD.withScope(RDD.scala:363) at org.apache.spark.rdd.RDD.mapPartitionsWithIndex(RDD.scala:843) at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:608) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127) at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247) at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:337) at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38) at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3273) at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484) at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484) at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77) at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253) at org.apache.spark.sql.Dataset.head(Dataset.scala:2484) at org.apache.spark.sql.Dataset.head(Dataset.scala:2491) at org.apache.spark.sql.Dataset.first(Dataset.scala:2498) at org.apache.spark.ml.feature.VectorAssembler.first$lzycompute$1(VectorAssembler.scala:57) at org.apache.spark.ml.feature.VectorAssembler.org$apache$spark$ml$feature$VectorAssembler$$first$1(VectorAssembler.scala:57) at org.apache.spark.ml.feature.VectorAssembler$$anonfun$2$$anonfun$1.apply$mcI$sp(VectorAssembler.scala:88) at org.apache.spark.ml.feature.VectorAssembler$$anonfun$2$$anonfun$1.apply(VectorAssembler.scala:88) at org.apache.spark.ml.feature.VectorAssembler$$anonfun$2$$anonfun$1.apply(VectorAssembler.scala:88) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.ml.feature.VectorAssembler$$anonfun$2.apply(VectorAssembler.scala:88) at org.apache.spark.ml.feature.VectorAssembler$$anonfun$2.apply(VectorAssembler.scala:58) at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241) at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241) at scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:186) at org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:58) at org.apache.spark.ml.PipelineModel$$anonfun$transform$1.apply(Pipeline.scala:306) at org.apache.spark.ml.PipelineModel$$anonfun$transform$1.apply(Pipeline.scala:306) at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57) at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66) at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:186) at org.apache.spark.ml.PipelineModel.transform(Pipeline.scala:306) at com.tr.ccc.la.dockets.de.inference.functions.EntryPipelineFunctions.prep_and_discover_motion_orders(PipelineFunctions.scala:36) at com.tr.ccc.la.dockets.de.inference.SparkApp$.main(SparkApp.scala:45) at com.tr.ccc.la.dockets.de.inference.SparkApp.main(SparkApp.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:688) Caused by: java.io.UTFDataFormatException: encoded string too long: 76663 bytes at java.io.DataOutputStream.writeUTF(DataOutputStream.java:364) at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323) at com.typesafe.config.impl.SerializedConfigValue.writeValueData(SerializedConfigValue.java:312) at com.typesafe.config.impl.SerializedConfigValue.writeValue(SerializedConfigValue.java:386) at com.typesafe.config.impl.SerializedConfigValue.writeValueData(SerializedConfigValue.java:326) at com.typesafe.config.impl.SerializedConfigValue.writeValue(SerializedConfigValue.java:386) at com.typesafe.config.impl.SerializedConfigValue.writeValueData(SerializedConfigValue.java:326) at com.typesafe.config.impl.SerializedConfigValue.writeValue(SerializedConfigValue.java:386) at com.typesafe.config.impl.SerializedConfigValue.writeValueData(SerializedConfigValue.java:326) at com.typesafe.config.impl.SerializedConfigValue.writeValue(SerializedConfigValue.java:386) at com.typesafe.config.impl.SerializedConfigValue.writeExternal(SerializedConfigValue.java:452) at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348) at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43) at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100) at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:342) ... 
Cannot import `spark-nlp-ocr`, sbt compile is failing because of a third party library that cannot find on maven `javax.media.jai#com.springsource.javax.media.jai.core;1.1.3`

## Description
I'm tryin to use  `spark-nlp-ocr` so I've added it to my project `build.sbt` but the compile is failing as follows:

```
[info] Resolving javax.media.jai#com.springsource.javax.media.jai.core;1.1.3 ...
[warn] 	module not found: javax.media.jai#com.springsource.javax.media.jai.core;1.1.3
[warn] ==== local: tried
[warn]   /Users/dzlab/.ivy2/local/javax.media.jai/com.springsource.javax.media.jai.core/1.1.3/ivys/ivy.xml
[warn] ==== public: tried
[warn]   https://repo1.maven.org/maven2/javax/media/jai/com.springsource.javax.media.jai.core/1.1.3/com.springsource.javax.media.jai.core-1.1.3.pom
[warn] ==== local-preloaded-ivy: tried
[warn]   /Users/dzlab/.sbt/preloaded/javax.media.jai/com.springsource.javax.media.jai.core/1.1.3/ivys/ivy.xml
[warn] ==== local-preloaded: tried
[warn]   file:////Users/dzlab/.sbt/preloaded/javax/media/jai/com.springsource.javax.media.jai.core/1.1.3/com.springsource.javax.media.jai.core-1.1.3.pom
[warn] ==== Maven Central: tried
[warn]   http://repo1.maven.org/maven2/javax/media/jai/com.springsource.javax.media.jai.core/1.1.3/com.springsource.javax.media.jai.core-1.1.3.pom
[warn] ==== osgeo: tried
[warn]   http://download.osgeo.org/webdav/geotools/javax/media/jai/com.springsource.javax.media.jai.core/1.1.3/com.springsource.javax.media.jai.core-1.1.3.pom
[warn] ==== geotoolkit: tried
[warn]   http://maven.geotoolkit.org/javax/media/jai/com.springsource.javax.media.jai.core/1.1.3/com.springsource.javax.media.jai.core-1.1.3.pom
[warn] ==== Akka repository: tried
[warn]   http://repo.akka.io/releases/javax/media/jai/com.springsource.javax.media.jai.core/1.1.3/com.springsource.javax.media.jai.core-1.1.3.pom
[warn] ==== Typesafe repository: tried
[warn]   http://repo.typesafe.com/typesafe/releases/javax/media/jai/com.springsource.javax.media.jai.core/1.1.3/com.springsource.javax.media.jai.core-1.1.3.pom
[warn] ==== Spark Packages Repo: tried
[warn]   http://dl.bintray.com/spark-packages/maven/javax/media/jai/com.springsource.javax.media.jai.core/1.1.3/com.springsource.javax.media.jai.core-1.1.3.pom
[info] Resolving jline#jline;2.14.3 ...
[warn] 	::::::::::::::::::::::::::::::::::::::::::::::
[warn] 	::          UNRESOLVED DEPENDENCIES         ::
[warn] 	::::::::::::::::::::::::::::::::::::::::::::::
[warn] 	:: javax.media.jai#com.springsource.javax.media.jai.core;1.1.3: not found
[warn] 	::::::::::::::::::::::::::::::::::::::::::::::
[warn] 
[warn] 	Note: Unresolved dependencies path:
[warn] 		javax.media.jai:com.springsource.javax.media.jai.core:1.1.3
[warn] 		  +- com.johnsnowlabs.nlp:spark-nlp-ocr_2.11:2.2.2 (/Users/dzlab/workspace/testnlp/build.sbt#L339)
[warn] 		  +- com.dzlab:testnlp_2.11:2.0.0-beta.1+353.g38d84c0.dirty.SNAPSHOT
sbt.ResolveException: unresolved dependency: javax.media.jai#com.springsource.javax.media.jai.core;1.1.3: not found
```
## Expected Behavior
I expect the project to compile when adding a dependency to `spark-nlp-ocr`

## Current Behavior
It seems that one of `spark-nlp-ocr` dependencies `javax.media.jai:com.springsource.javax.media.jai.core:1.1.3` cannot be found on Maven.

## Possible Solution
I tried to add other repos to my resolvers `osgeo` and `geotoolkit` like the following but still the issue persist.
```
lazy val commonSettings = Seq(
  resolvers ++= Seq(
    "Maven Central" at "http://repo1.maven.org/maven2",
    "osgeo" at "http://download.osgeo.org/webdav/geotools/",
    "geotoolkit" at "http://maven.geotoolkit.org",
    "Akka repository" at "http://repo.akka.io/releases/",
    "Typesafe repository" at "http://repo.typesafe.com/typesafe/releases/",
    "Spark Packages Repo" at "http://dl.bintray.com/spark-packages/maven"
  ),
 ...
}
```

Here is how I'm trying to import my `spark-nlp` in my build.sbt
```
val sparknlpVersion = "2.2.2"
lazy val nlpLibs = Seq(
  "com.johnsnowlabs.nlp" %% "spark-nlp" % sparknlpVersion,
  "com.johnsnowlabs.nlp" %% "spark-nlp-ocr" % sparknlpVersion,
  "com.johnsnowlabs.nlp" %% "spark-nlp-eval" % sparknlpVersion
)
```
But the issue is not solved, I keep getting the same problem.

## Steps to Reproduce
1. Start an empty scala project
2. Add `spark-nlp-ocr` as dependcy in `build.sbt`
3. Compile project with `sbt compile`

## Context
I'm trying to use Spark NLP OCR feature to parse text from PDF files, if I cannot get this done I will swicth to use Apache TIKA which worked for me like a charm.

## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in -->
* Spark NLP version: 2.2.2
* Apache NLP version: (not sure about this one) 
* Setup and installation (Pypi, Conda, Maven, etc.): build.sbt (scala project)
* Operating System and version: OSX 10.15.2 (19C57)
* Link to your project (if any):

Using offline pretrained pipelines or models on EMR cluster results in Wrong FS error. 

## Description
Offline pretrained pipeline doesn't get loaded successfully. The pretrained pipeline was manually downloaded and then uploaded to a s3 bucket (xxx). Here's a code that was tested: 
```
import org.apache.spark.SparkConf
import org.apache.spark.ml.{PipelineModel}
import org.apache.spark.sql.SparkSession

object SimpleApp {
  def main(args: Array[String]) = {

    // initialise spark context
    val conf = new SparkConf().setAppName(SimpleApp.getClass.getName)
    val spark: SparkSession = SparkSession.builder.config(conf).getOrCreate()


    println("loading pretrained pipeline offline from s3")
    PipelineModel.load(
      "s3a://xxx/pipelines/onto_recognize_entities_sm_en_2.1.0_2.4_1564330931782/"
    )
    println("loaded pretrained pipeline offline from s3")

    spark.stop()
  }
}
```

## Expected Behavior
The pipeline should be loaded successfully

## Current Behavior
An exception is thrown:
```
Exception in thread "main" java.lang.IllegalArgumentException: Wrong FS: hdfs://ip-xx-xx-xx-xx.us-west-2.compute.internal:8020/user/hadoop/cache_pretrained/embeddings_tmp, expected: file:///
	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:669)
	at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:86)
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:635)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:866)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:630)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:452)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1440)
	at org.apache.hadoop.fs.FileUtil.checkDest(FileUtil.java:500)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:351)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:292)
	at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:2114)
	at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:2083)
	at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:2059)
	at com.johnsnowlabs.nlp.embeddings.EmbeddingsHelper$.load(EmbeddingsHelper.scala:70)
```

## Possible Solution
If embeddings are on S3, they are currently copied to the local file system, even when code is running on an EMR cluster. The code should be modified to handle copying to HDFS. 

## Steps to Reproduce

1. Create an EMR cluster (version 5.28.0)
2. Run SimpleApp (see above) using client mode on the EMR cluster

## Context
Load pretrained pipelines and models to perform NER and sentiment analysis for huge volume of data on a distributed cluster (EMR)

## Your Environment

* EMR version: 5.28.0 (Spark version: 2.4.4)
* Spark NLP version: 2.3.5


<!--- Provide a general summary of the issue in the Title above -->

## Description
<!--- Provide a more detailed introduction to the issue itself, and why you consider it to be a bug -->
A Zeppelin or Jupyter notebook connected to a Spark cluster (Spark on YARN in client mode) was created to train a model for NER.  The Spark on YARN cluster is a service provided by Qubole (https://www.qubole.com) that we are evaluating.  We also created a simple Spark on YARN cluster with one node ourselves solely for the purpose of writing this ticket.

Our simple pipeline consists of only `WordEmbeddings`, `NerDLApproach`, `NerConverter`, and `Finisher`.  For `WordEmbeddings`, we use the public GloVe file `glove.6B.100d.txt`.  For `NerDLApproach`, we defined our custom tags, so we have a combination of tag count, embedding dimension, character count, and LSTM size different from that of the graphs included in the package.  We followed the instruction (https://nlp.johnsnowlabs.com/docs/en/graph) to generate a graph with our customized sizes.

In order for any executor to be able to access the embeddings file for `WordEmbeddings` and the Tensorflow graph folder for `NerDLApproach` at training time, we put both the embeddings file and the graph on Hadoop.  When `fit` is called by the pipeline, `WordEmbeddings` has no problem with reading the embeddings file on HDFS.  `NerDLApproach` finds the graph file on HDFS but throws an exception, asking for a local path instead.  Naively, I would assume both of them would accept HDFS paths or both of them would reject HDFS paths.  When one accepts HDFS and the other rejects HDFS, either there is a bug or we missed something in our pipeline definition.

## Expected Behavior
<!--- Tell us what should happen -->
When a Jupyter notebook is connected to Spark cluster on YARN, `NerDLApproach().setGraphFolder(…)` should accept an HDFS path as the location of the graph directory.

## Current Behavior
<!--- Tell us what happens instead of the expected behavior -->
The Jupyter notebook shows an error that looks like the following:
```
Py4JJavaError: An error occurred while calling o38.fit.
: java.lang.IllegalArgumentException: Wrong FS: hdfs://localhost:9000/user/hadoop/app_supplement_data/tf_graphs/blstm-noncontrib_13_100_128_100.pb, expected: file:///
```
We see the exact same behavior both with Qubole’s Spark cluster and with our own Spark cluster.

## Possible Solution
<!--- Not obligatory, but suggest a fix/reason for the bug, -->

## Steps to Reproduce
<!--- Provide a link to a live example, or an unambiguous set of steps to -->
<!--- reproduce this bug. Include code to reproduce, if relevant -->
1. As the user `hadoop`, activate Python 3 virtualenv
    ```
    source ~/.jupyter_enterprise_gateway_env/bin/activate
    ```
2. Start Hadoop cluster
    ```
    start-dfs.sh
    start-yarn.sh
    ```
3.  Start Jupyter Enterprise Gateway using the script from the attachment
    ```
    ./start_jupyter_enterprise_gateway.sh
    ```
4.  Start Jupyter Notebook
    ```
    jupyter notebook --gateway-url=http://127.0.0.1:8888 --GatewayClient.http_user=guest --GatewayClient.http_pwd=guest-password
    ```
5. Establish an SSH tunnel from the local machine to the server running Jupyter Notebook.  In our case, we do something like `gcloud compute --project "<GCP project ID>" ssh --zone "us-east1-b" --ssh-flag="-L 8889:localhost:8889 -C -N" "hadoop@<GCP VM name>"`.  TCP ports `8888` and/or `8889` may also need to be open for Jupyter Enterprise Gateway.

6. Open the Jupyter notebook from the attachment in a web browser.

## Context
<!--- How has this bug affected you? What were you trying to accomplish? -->
We need to establish an NER system in our production in Q1 2020.  We have been evaluating Qubole’s Spark cluster service for the purpose.  However, we are blocked from being able to make a decision because of this issue.  Qubole’s engineers have been very generously trying to figure out if there is a workaround for us even though we’re past our free-trial period already.  We should not keep dragging this on.

## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in -->
This describes our own Spark cluster, not Qubole’s Spark cluster.  We are able to reproduce the same issue using our own Spark cluster.
* Spark NLP version: 2.3.4
* Apache Spark version: 2.4.0.  We downloaded `http://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz` and unzipped it to `/opt/spark`.
    ```
    export SPARK_HOME=/opt/spark
    export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
    ```
* Hadoop version: 2.7.3.  We downloaded `https://archive.apache.org/dist/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz` and unzipped it to `/home/hadoop/hadoop-2.7.3`.  We followed this instruction (https://tecadmin.net/setup-hadoop-on-ubuntu/) to configure Hadoop.
    ```
    export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/
    export HADOOP_HOME=/home/hadoop/hadoop-2.7.3
    export HADOOP_INSTALL=$HADOOP_HOME
    export HADOOP_MAPRED_HOME=$HADOOP_HOME
    export HADOOP_COMMON_HOME=$HADOOP_HOME
    export HADOOP_HDFS_HOME=$HADOOP_HOME
    export YARN_HOME=$HADOOP_HOME
    export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
    export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
    export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
    export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
    ```
* Setup and installation (Pypi, Conda, Maven, etc.):
    * `hadoop` user account and `/home/hadoop` need to exist.
    * Python 3.6.6, because it is the highest version of Python 3 that still allows `pip install tensorflow==1.12.0`.  We followed this instruction (https://tecadmin.net/install-python-3-6-ubuntu-linuxmint/) to install Python 3.6.6 instead of 3.6.9.
    * Virtualenv, e.g., `python3.6 -m venv .jupyter_enterprise_gateway_env`
    * `pip install -r requirements.txt`. `requirements.txt` is attached to the ticket.
    * Copy the `spark_python_yarn_client` folder from the attachment to `.jupyter_enterprise_gateway_env/share/jupyter/kernels/`.  Our version is a modified copy of the example from `https://github.com/jupyter/enterprise_gateway/releases/download/v2.0.0/jupyter_enterprise_gateway_kernelspecs-2.0.0.tar.gz`
    * Copy the data files from the attachment to Hadoop.
        ```
        hdfs dfs -copyFromLocal sample_training_data.parquet /user/hadoop/
        hdfs dfs -mkdir /user/hadoop/app_supplement_data
        hdfs dfs -copyFromLocal tf_graphs /user/hadoop/app_supplement_data
        ```
    * Get a copy of `glove.6B.100d.txt` from somewhere and copy it to Hadoop.  We cannot attach it to the ticket because it is too large for Github.
        ```
        hdfs dfs -mkdir /user/hadoop/app_supplement_data/word_embeddings
        hdfs dfs -copyFromLocal <path to glove.6B.100d.txt> /user/hadoop/app_supplement_data/word_embeddings/
        ```
* Operating System and version: Ubuntu 18.04.3 LTS (GNU/Linux 5.0.0-1026-gcp x86_64)
* Link to your project (if any):


[requirements.txt](https://github.com/JohnSnowLabs/spark-nlp/files/4037948/requirements.txt)
[jupyter_enterprise_gateway_customization.tar.gz](https://github.com/JohnSnowLabs/spark-nlp/files/4037950/jupyter_enterprise_gateway_customization.tar.gz)
[supplement_data.tar.gz](https://github.com/JohnSnowLabs/spark-nlp/files/4037951/supplement_data.tar.gz)
[Example_Notebook_with_Spark_on_YARN_Client_Mode.ipynb.zip](https://github.com/JohnSnowLabs/spark-nlp/files/4037953/Example_Notebook_with_Spark_on_YARN_Client_Mode.ipynb.zip)

<!--- Provide a general summary of the issue in the Title above -->
CoNLL produce bad sentences annotations

## Description
<!--- Provide a more detailed introduction to the issue itself, and why you consider it to be a bug -->
I train a NerDlApproach with CoNLL files, using the "sentences" column in the inputcols. After the fit, when i transform a test file in a pipeline with a NerConverter Anntoator, this one produces only results for the first sentence. Trying to evaluate with NerHelper.measureExact, the results are bad . If you make the same using the document column in NerDlApproach it works well. 

## Expected Behavior
<!--- Tell us what should happen -->

## Current Behavior
<!--- Tell us what happens instead of the expected behavior -->
NER_SPAN (after finisher, extracting from metadata):
Any = WrappedArray([entity,MUT_DNA], [sentence,0], [chunk,0])
LABEL_SPAN (after finisher, extracting from metadata):
Any = WrappedArray([entity,MUT_DNA], [sentence,0], [chunk,0], [entity,MUT_PRO], [sentence,0], [chunk,1])

## Possible Solution
<!--- Not obligatory, but suggest a fix/reason for the bug, -->

## Steps to Reproduce
<!--- Provide a link to a live example, or an unambiguous set of steps to -->
<!--- reproduce this bug. Include code to reproduce, if relevant -->
1.
2.
3.
4.

## Context
<!--- How has this bug affected you? What were you trying to accomplish? -->

## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in -->
* Spark NLP version: 2.3.4
* Apache NLP version:
* Setup and installation (Pypi, Conda, Maven, etc.):
* Operating System and version:
* Link to your project (if any):

Added to documentation section related OCR 2.0



<!--- Provide a general summary of the issue in the Title above -->

## Description
<!--- Provide a more detailed introduction to the issue itself, and why you consider it to be a bug -->
While some of the issue is specific to my environment, I think it could occur in multiple contexts where the user does not have write access for temporary directories. I am in an environment where I do not have write access to most folders and need to use sudo to accomplish most file-system tasks. My understanding is that spark nlp creates a temporary directory /tmp/sparknlp_contrib_spar for which in this case, it doesn't have permissions. 
## Expected Behavior
<!--- Tell us what should happen -->
I should be able to load the embeddings and fit a model using them.
## Current Behavior
<!--- Tell us what happens instead of the expected behavior -->
Py4JJavaError: An error occurred while calling o150.load.
: java.io.FileNotFoundException: /tmp/sparknlp_contrib_spar (Permission denied)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:162)
	at com.johnsnowlabs.nlp.annotators.ner.dl.LoadsContrib$.copyResourceToTmp(LoadsContrib.scala:39)
	at com.johnsnowlabs.nlp.annotators.ner.dl.LoadsContrib$.loadContribToCluster(LoadsContrib.scala:57)
	at com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel$class.readTensorflowModel(TensorflowSerializeModel.scala:60)
## Possible Solution
<!--- Not obligatory, but suggest a fix/reason for the bug, -->
One possible solution would be to check if the directory is writable and if not allow the user to specify their own temporary directory.
## Steps to Reproduce
<!--- Provide a link to a live example, or an unambiguous set of steps to -->
<!--- reproduce this bug. Include code to reproduce, if relevant -->
bert = BertEmbeddings.load(bert_path)\
    .setInputCols(["token","sentence"])\
    .setOutputCol("bert")

## Context
<!--- How has this bug affected you? What were you trying to accomplish? -->
Use pretrained BertEmbeddings for classification and NER.
## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in -->
* Version used: 2.3.4
* Operating System and version (desktop or mobile): CentOS 7.7.1908
