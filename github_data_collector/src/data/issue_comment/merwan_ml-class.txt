Your code for gradient descent for multivariate regression is not vectorized. 
Here's a vectorized implementation

```
function [theta, J_history] = gradientDescentMulti(X, y, theta, alpha, num_iters)
%GRADIENTDESCENTMULTI Performs gradient descent to learn theta
%   theta = GRADIENTDESCENTMULTI(x, y, theta, alpha, num_iters) updates theta by
%   taking num_iters gradient steps with learning rate alpha

% Initialize some useful values
m = length(y); % number of training examples
J_history = zeros(num_iters, 1);
sum1=zeros(size(X,2),1)
for iter = 1:num_iters

    % ====================== YOUR CODE HERE ======================
    % Instructions: Perform a single gradient step on the parameter vector
    %               theta. 
    %
    % Hint: While debugging, it can be useful to print out the values
    %       of the cost function (computeCostMulti) and gradient here.
    %

notsum=((X*theta)-y)
sum1=(alpha/m)*(sum(notsum.*X))
theta=theta-sum1'









    % ============================================================

    % Save the cost J in every iteration    
    J_history(iter) = computeCostMulti(X, y, theta);

end

end

```
https://github.com/merwan/ml-class/blob/0b06b73d4aac0b8d7c726325200c3781b5c9d3b0/mlclass-ex1/ex1_multi.m#L153

The size and number of bedrooms should be scaled with the previously computed mean `mu` and standard deviation `sigma`.

From Andrew Ng's exercise sheet:

> Implementation Note: When normalizing the features, it is important
> to store the values used for normalization - the mean value and the stan-
> dard deviation used for the computations. After learning the parameters
> from the model, we often want to predict the prices of houses we have not
> seen before. Given a new x value (living room area and number of bed-
> rooms), we must rst normalize x using the mean and standard deviation
> that we had previously computed from the training set.
In week 6 - exercise 5 - [validationCurve.m](https://github.com/merwan/ml-class/blob/master/mlclass-ex5/validationCurve.m) could you explain why your code works and this doesn't:

```
for i=1:length(lambda_vec)
	lambda = lambda_vec(i);
	theta = trainLinearReg(X, y, lambda);
	error_train(i) = linearRegCostFunction(X, y, theta, lambda);
	error_val(i) = linearRegCostFunction(Xval, yval, theta, lambda);
end
```

The above snippet makes perfect sense to me, but it isn't correct.
  
sir i'd like to know the paper you have been following which refers to the equations in the comments in svm_traim.m  file...

could you pls send the link for it....
email: mail2rohilla@gmail.com

