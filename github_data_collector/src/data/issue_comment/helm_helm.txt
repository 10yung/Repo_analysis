Use the same install order for hooks as for normal resources (non-hooks) for hooks with equal weight.
This makes resource handling more consistent and helps, when there are hook consisting of several resources like e.g. a service account and a job using this service account.

The sort functions are changed from an in place search to an out of place sort to avoid inout parameters.

Closes #7416.

Signed-off-by: Daniel Strobusch <1847260+dastrobu@users.noreply.github.com>
Draft implementation of the `update --recreate` strategy from #7082. 

The feature is implemented and ready to play around with. 

TODOs
 * write (and fix) tests
 * test that shell completion works correctly
 * Check if the flag can be added in backwards compatible way: [interface.go#L48](https://github.com/helm/helm/blob/5e3bf420c7becc7f7cbed3bfe2352f9751bbf572/pkg/kube/interface.go#L48)
It seems to me that Helm 3 does not provide a way to create an `action.Configuration` structure if the code is running from within the cluster.

Here is a code sample that I have tried, building my own generic flags:

```
config, err := rest.InClusterConfig()
if err != nil {
    panic(err)
}
insecure := false

genericConfigFlag := &genericclioptions.ConfigFlags{
    Timeout: stringptr("0"),
    Insecure: &insecure,
    APIServer: stringptr(config.Host),
    CAFile: stringptr(config.CAFile),
    BearerToken: stringptr(config.BearerToken),
    ImpersonateGroup: &[]string{},
    Namespace: stringptr(namespace),
}

actionConfig := &action.Configuration{
    RESTClientGetter: genericConfigFlag,
    KubeClient: kube.New(genericConfigFlag),
    Log: log.Infof,
}

```

Unfortunately, this result in a `SIGSEGV` error later when running `action.NewList(actionConfig).Run()`.

Is it the right way to define an action config for Helm 3 from within a Kubernetes cluster, without altering the current library of course?

In the event of a CrashLoopBackOff scenario, helm upgrade simply hangs and either times out or in my case at least, a context deadline exceeded error will occur. It would be more useful for in this scenario to have helm upgrade to fail loudly and to output error logs. 
<!-- If you need help or think you have found a bug, please help us with your issue by entering the following information (otherwise you can delete this text): -->

Output of `helm version`:
```
Client: &version.Version{SemVer:"v2.12.3", GitCommit:"eecf22f77df5f65c823aacd2dbd30ae6c65f186e", GitTreeState:"clean"}
Server: &version.Version{SemVer:"v2.14.2", GitCommit:"a8b13cc5ab6a7dbef0a58f5061bcc7c0c61598e7", GitTreeState:"clean"}
```

Output of `kubectl version`:
```
Client Version: version.Info{Major:"1", Minor:"13", GitVersion:"v1.13.4", GitCommit:"c27b913fddd1a6c480c229191a087698aa92f0b1", GitTreeState:"clean", BuildDate:"2019-03-01T23:34:27Z", GoVersion:"go1.12", Compiler:"gc", Platform:"darwin/amd64"}
Server Version: version.Info{Major:"1", Minor:"13", GitVersion:"v1.13.12", GitCommit:"a8b52209ee172232b6db7a6e0ce2adc77458829f", GitTreeState:"clean", BuildDate:"2019-10-15T12:04:30Z", GoVersion:"go1.11.13", Compiler:"gc", Platform:"linux/amd64"}
```

Cloud Provider/Platform (AKS, GKE, Minikube etc.): N/A, locally


I used the [`required` function](https://v2.helm.sh/docs/charts_tips_and_tricks/#using-the-required-function) from within a `with` block. If I didn't provide the value it threw an error as expected. If I *did* provide the value, it evaluated to true, which gets included in the output. The docs don't specify it should only be declared in the global scope and it was reasonable to assume it would work in a local scope because you may have values that are only required based on another condition.

stripped example:
```yaml
{{- with .Values.favorite }}
{{ required "Value flavor is required!" .Values.flavor }}
drink: {{ .drink | default "tea" | quote }}
flavor: {{ .Values.flavor }}
{{- end }}
```
->
```
truedrink: "tea"
flavor: "green"
```





After this change, make works on nixos.
<!-- If you need help or think you have found a bug, please help us with your issue by entering the following information (otherwise you can delete this text): -->

Hi,

I am trying to install a release using the `--atomic` flag but it seems that it hangs forever:

```
▶ helm3 install bar stable/mariadb -n default --atomic
Error: release bar failed, and has been uninstalled due to atomic being set: timed out waiting for the condition
```

The chart resources get actually installed in the cluster but the command eventually times out and the release gets deleted:

```
▶ helm3 ls
NAME           	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART               	APP VERSION            
```

Is this a bug or am I missing something?

Output of `helm version`:
```
▶ helm3 version
version.BuildInfo{Version:"v3.0.2", GitCommit:"19e47ee3283ae98139d98460de796c1be1e3975f", GitTreeState:"clean", GoVersion:"go1.13.5"}
```
Output of `kubectl version`:
```
▶ kubectl version
Client Version: version.Info{Major:"1", Minor:"17", GitVersion:"v1.17.1", GitCommit:"d224476cd0730baca2b6e357d144171ed74192d6", GitTreeState:"clean", BuildDate:"2020-01-14T21:04:32Z", GoVersion:"go1.13.5", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"16", GitVersion:"v1.16.3", GitCommit:"b3cbbae08ec52a7fc73d334838e18d17e8512749", GitTreeState:"clean", BuildDate:"2019-11-16T01:01:59Z", GoVersion:"go1.12.12", Compiler:"gc", Platform:"linux/amd64"}
```
Cloud Provider/Platform (AKS, GKE, Minikube etc.): `kind`

Helm --wait --install does not wait for Pods to be ready in a statefulset when setting updateStrategy: OnDelete

Output of `helm version`:
version.BuildInfo{Version:"v3.0.2", GitCommit:"19e47ee3283ae98139d98460de796c1be1e3975f", GitTreeState:"clean", GoVersion:"go1.13.5"}
Output of `kubectl version`:
Client Version: version.Info{Major:"1", Minor:"16", GitVersion:"v1.16.2", GitCommit:"c97fe5036ef3df2967d086711e6c0c405941e14b", GitTreeState:"clean", BuildDate:"2019-10-15T19:18:23Z", GoVersion:"go1.12.10", Compiler:"gc", Platform:"darwin/amd64"}
Server Version: version.Info{Major:"1", Minor:"16", GitVersion:"v1.16.4", GitCommit:"224be7bdce5a9dd0c2fd0d46b83865648e2fe0ba", GitTreeState:"clean", BuildDate:"2019-12-11T12:37:43Z", GoVersion:"go1.12.12", Compiler:"gc", Platform:"linux/amd64"}
Cloud Provider/Platform (AKS, GKE, Minikube etc.): 
AKS


fix #7422 
<!-- If you need help or think you have found a bug, please help us with your issue by entering the following information (otherwise you can delete this text): -->

Output of `helm version`:

`
version.BuildInfo{Version:"v3.0.2", GitCommit:"19e47ee3283ae98139d98460de796c1be1e3975f", GitTreeState:"clean", GoVersion:"go1.13.5"}
`

Output of `kubectl version`:

```
Client Version: version.Info{Major:"1", Minor:"14", GitVersion:"v1.14.2", GitCommit:"66049e3b21efe110454d67df4fa62b08ea79a19b", GitTreeState:"clean", BuildDate:"2019-05-16T16:23:09Z", GoVersion:"go1.12.5", Compiler:"gc", Platform:"darwin/amd64"}
Server Version: version.Info{Major:"1", Minor:"10+", GitVersion:"v1.10.5-tke.3", GitCommit:"53e244be925234190938376fe8637189b6caf125", GitTreeState:"clean", BuildDate:"2018-12-04T04:10:15Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}
```

Cloud Provider/Platform (AKS, GKE, Minikube etc.): 
TKE [https://intl.cloud.tencent.com/product/tke](https://intl.cloud.tencent.com/product/tke)


## How to reproduce

```
helm --debug upgrade -i redis -n default --wait stable/redis
```

## Logs

history.go:52: [debug] getting history for release redis
Release "redis" does not exist. Installing it now.
install.go:149: [debug] Original chart version: ""
install.go:166: [debug] CHART PATH: /Users/hoozecn/Library/Caches/helm/repository/redis-10.3.1.tgz

```
client.go:89: [debug] creating 8 resource(s)
wait.go:51: [debug] beginning wait for 8 resources with timeout of 5m0s
wait.go:280: [debug] StatefulSet is not ready: default/redis-master. 0 out of 1 expected pods have been scheduled
wait.go:280: [debug] StatefulSet is not ready: default/redis-master. 0 out of 1 expected pods have been scheduled
wait.go:280: [debug] StatefulSet is not ready: default/redis-master. 0 out of 1 expected pods have been scheduled
wait.go:280: [debug] StatefulSet is not ready: default/redis-master. 0 out of 1 expected pods have been scheduled
wait.go:280: [debug] StatefulSet is not ready: default/redis-master. 0 out of 1 expected pods have been scheduled
wait.go:280: [debug] StatefulSet is not ready: default/redis-master. 0 out of 1 expected pods have been scheduled
wait.go:280: [debug] StatefulSet is not ready: default/redis-master. 0 out of 1 expected pods have been scheduled
wait.go:280: [debug] StatefulSet is not ready: default/redis-master. 0 out of 1 expected pods have been scheduled
wait.go:280: [debug] StatefulSet is not ready: default/redis-master. 0 out of 1 expected pods have been scheduled
wait.go:280: [debug] StatefulSet is not ready: default/redis-master. 0 out of 1 expected pods have been scheduled
wait.go:280: [debug] StatefulSet is not ready: default/redis-master. 0 out of 1 expected pods have been scheduled
wait.go:280: [debug] StatefulSet is not ready: default/redis-master. 0 out of 1 expected pods have been scheduled
wait.go:280: [debug] StatefulSet is not ready: default/redis-master. 0 out of 1 expected pods have been scheduled
wait.go:280: [debug] StatefulSet is not ready: default/redis-master. 0 out of 1 expected pods have been scheduled
```

## actual statefulset status

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  creationTimestamp: "2020-01-17T02:17:26Z"
  generation: 1
  labels:
    app: redis
    chart: redis-10.3.1
    heritage: Helm
    release: redis
  name: redis-master
  namespace: default
  resourceVersion: "6443000542"
  selfLink: /apis/apps/v1/namespaces/default/statefulsets/redis-master
  uid: 8163c0d5-38cf-11ea-ae57-be7eeeb98d2b
spec:
  podManagementPolicy: OrderedReady
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: redis
      release: redis
      role: master
  serviceName: redis-headless
  template:
    metadata:
      annotations:
        checksum/configmap: 45ff1febab3e528682d90a0fd55d8cce0d7d63fd7ee9f378cf3349effc2f3ddf
        checksum/health: f8b96e603e91fdcb5367b65c16c641e3c425598fd3e1947cd02dfb575f5ad8bd
        checksum/secret: 93ebb64794354e08f5ab9a40e27bda27e3ac444eabc9fa5132a55bb0ce042b52
      creationTimestamp: null
      labels:
        app: redis
        chart: redis-10.3.1
        release: redis
        role: master
    spec:
      containers:
      - command:
        - /bin/bash
        - -c
        - |
          if [[ -n $REDIS_PASSWORD_FILE ]]; then
            password_aux=`cat ${REDIS_PASSWORD_FILE}`
            export REDIS_PASSWORD=$password_aux
          fi
          if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then
            cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
          fi
          if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then
            cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
          fi
          ARGS=("--port" "${REDIS_PORT}")
          ARGS+=("--requirepass" "${REDIS_PASSWORD}")
          ARGS+=("--masterauth" "${REDIS_PASSWORD}")
          ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
          ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
          /run.sh ${ARGS[@]}
        env:
        - name: REDIS_REPLICATION_MODE
          value: master
        - name: REDIS_PASSWORD
          valueFrom:
            secretKeyRef:
              key: redis-password
              name: redis
        - name: REDIS_PORT
          value: "6379"
        image: docker.io/bitnami/redis:5.0.7-debian-9-r12
        imagePullPolicy: IfNotPresent
        livenessProbe:
          exec:
            command:
            - sh
            - -c
            - /health/ping_liveness_local.sh 5
          failureThreshold: 5
          initialDelaySeconds: 5
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 5
        name: redis
        ports:
        - containerPort: 6379
          name: redis
          protocol: TCP
        readinessProbe:
          exec:
            command:
            - sh
            - -c
            - /health/ping_readiness_local.sh 5
          failureThreshold: 5
          initialDelaySeconds: 5
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 1
        resources: {}
        securityContext:
          runAsUser: 1001
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /health
          name: health
        - mountPath: /data
          name: redis-data
        - mountPath: /opt/bitnami/redis/mounted-etc
          name: config
        - mountPath: /opt/bitnami/redis/etc/
          name: redis-tmp-conf
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext:
        fsGroup: 1001
      serviceAccount: default
      serviceAccountName: default
      terminationGracePeriodSeconds: 30
      volumes:
      - configMap:
          defaultMode: 493
          name: redis-health
        name: health
      - configMap:
          defaultMode: 420
          name: redis
        name: config
      - emptyDir: {}
        name: redis-tmp-conf
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
  - metadata:
      creationTimestamp: null
      labels:
        app: redis
        component: master
        heritage: Helm
        release: redis
      name: redis-data
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 8Gi
    status:
      phase: Pending
status:
  collisionCount: 0
  currentReplicas: 1
  currentRevision: redis-master-76bdc76d8
  observedGeneration: 1
  readyReplicas: 1
  replicas: 1
  updateRevision: redis-master-76bdc76d8
```

## probable cause

https://github.com/kubernetes/kubernetes/issues/52653

The  `UpdatedReplicas` is missing in status of statefulset, bug is fixed after kubernetes version v1.12.0-alpha.1
