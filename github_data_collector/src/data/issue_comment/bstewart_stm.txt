Many package developers now include `tidy` and `glance` methods to improve user experience. This allows users to interact with standardized model summaries in a programmatic way. 

The `broom` package supplies `tidy` and `glance` functions for a lot of model types:

https://github.com/tidymodels/broom

The `estimatr` package (from `DeclareDesign`) is an example of a package that supplies its own `glance` and `tidy` methods:

https://github.com/DeclareDesign/estimatr/blob/master/R/S3_tidy.R

This PR adds `tidy` and `glance` methods for the `estimateEffect` object type. It works as follows:

```
> library(stm)
> data(gadarian)
> fx <- estimateEffect(1:3 ~ treatment, gadarianFit, gadarian)
>
> tidy(fx)
  topic        term    estimate  std.error statistic      p.value
1     1 (Intercept)  0.44036752 0.02038472 21.602819 1.067327e-65
2     1   treatment -0.15985146 0.02843963 -5.620729 3.974606e-08
3     2 (Intercept)  0.20957780 0.02197562  9.536831 2.975169e-19
4     2   treatment  0.24664633 0.03210726  7.681950 1.699603e-13
5     3 (Intercept)  0.35005179 0.01992782 17.565984 1.420879e-49
6     3   treatment -0.08683705 0.02896184 -2.998327 2.915232e-03
>
> glance(fx)
  uncertainty nobs ntopics
1      Global  341       3
```

There is a package called `tidystm` on github, but it has not been updated for 3 years, and it seems to calculate quantities differently:

- https://github.com/mikajoh/tidystm/blob/master/R/extractEffect.R#L69
- https://github.com/bstewart/stm/blob/master/R/estimateEffect.R#L294

The benefit of this PR is that the returned quantities are extracted directly from `summary.estimateEffect`, and that they are thus consistent with printed output. It is also much more convenient for users to have this as part of the package rather than have to find and install a distinct package.
Hey, first of all thank you for this great package! It works like a charm and finds frequent use in my projects.

Lately I'm using it on Twitter data, and I am currently trying to find out whether the models can be improved by using hashtags as metadata. The problem I am running into now is, that the stm function(s) don't like their metadata as character vectors, but rather expect single characters (or other variables) per row. While collapsing the vectors is not a problem, it does beg some methodological questions. For one, chances are that a rather large amount of tweets will have unique combinations of hashtags, basically eliminating any prevalence effects. It also makes effect estimations rather tedious since the unique hashtag combinations make for a very large number of observations in the variable. Right now I'm dealing with these issues by reducing the amount of hashtags used in the model by frequency and sorting the hashtags for each document. This, however, still does not seem optimal to me.

So my question is: Is there a way to feed the model character vectors as metadata that I am not seeing? And if not, are there any plans to implement this functionality in the near future?
add a check on the mode location and throw an error when out of bounds
When the covariate values are misspecified (e.g. cov.value1=1 instead of cov.value1=TRUE) there isn't an error message, it just errors.  There should be.
The wordLengths argument is length two but most people probably just want to mess with the lower bound.  Should probably throw an error or append Inf and give a warning when passed a scalar.
There is a strange error that pops up in textProcessor when copying very long metadata fields.  Not entirely sure why or how to stop it.  Looking into it, it might be possible to simplify how metadata is handled by only maintaining a document index in the metadata.

The case we need to test against is one where the entire document is dropped before creating the document term matrix,
Good afternoon,

more than an issue this is a question.
I have tried looking for the optimal K value using the Lee and Mimno algorithm (2014) and the initialization type set to "Spectral". However, when I try to use the K value obtained from the resulting model to generate a new model by explicitly setting up that value as initialization parameter, the outcome is a completely different model (despite the fact that I am still using Spectral as init.type). Therefore, I was wondering if the difference is directly related to the fact that the search K method (Lee and Mimno algorithm) also affects the training stage.

Thanks in advance for the clarification and regards.

C. Bordet pointed out a problem that should be addressed.  tm() is actually not using SMART stopwords by default.  Need to figure out how to resolve as you have to pass "SMART" as the stopword list and we don't have a direct way to do this.  Probably just need to update the directions and give an example.  For anyone who wants to use the SMART stopword list, you can do the following

`txt <- textProcessor(documents, customstopwords = tm::stopwords("SMART"))`
I am currently running `stm` on ~130k documents (from customer chat data) without covariates, but may be running it on 10-20 times that row count with categorical and time covariates in the near future. 

This seems like a good opportunity for an `ngroups` benchmark. Did you have anything in mind that you would want me to include for this?
make.heldout has some pretty rough inefficiencies that arise from large vocabularies.  which() and tabulate() can take a long time because they populate a lot of zeroes unnecessarily.  Need to figure out a work around that only populates the non-zero elements of the table.  This would make which() unnecessary and tabulate's equivalent way quicker.