Hi, I followed the documentation to add nginx lua to an existing server and therefore cannot configure openresty completely on the configuration as it serves an additional purpose. I was just looking for a simple lightweight lua module to handle file uploads. 

I've seen a couple other issues with this but the response always seems to be "its required" and that is it. If it is required why do the docs not reflect that information? 

That being said, I followed the docs and installed everything necessary (as per the docs) and now receive this error. 

nginx: [alert] failed to load the 'resty.core' module (https://github.com/openresty/lua-resty-core); ensure you are using an OpenResty release from https://openresty.org/en/download.html (reason: module 'resty.core' not found:
        no field package.preload['resty.core']
        no file '/usr/local/lib/lua/resty/core.lua'
        no file './resty/core.lua'
        no file '/usr/local/share/luajit-2.1.0-beta3/resty/core.lua'
        no file '/usr/local/share/lua/5.1/resty/core.lua'
        no file '/usr/local/share/lua/5.1/resty/core/init.lua'
        no file '/usr/local/lib/resty/core.so'
        no file './resty/core.so'
        no file '/usr/local/lib/lua/5.1/resty/core.so'
        no file '/usr/local/lib/lua/5.1/loadall.so'
        no file '/usr/local/lib/resty.so'
        no file './resty.so'
        no file '/usr/local/lib/lua/5.1/resty.so'
        no file '/usr/local/lib/lua/5.1/loadall.so') in /usr/local/nginx/conf/nginx.conf:212

Please let me know how I can fix this without configuring openresty on our server setup.

Thanks
Hello all

we think there is some bug in 
https://github.com/openresty/lua-nginx-module#lua_shared_dict

because

> Shared memory zones are always shared by all the Nginx worker processes in the current Nginx server instance.

that means , that after nginx reload all new workers should have the same access to the same shared dict, isn't it?

we have 
CentOS release 6.10 (Final)
nginx/1.16.1
ngx_lua-0.10.15 

and we use 
```
...
lua_shared_dict authdata 32m; 
...
local authdata = ngx.shared.authdata;
authdata:set("stream_sess:"..ssid, 1, 60)
..

```
and after "service nginx reload"  new workers don't have the old sessions. I see 400 code it in our logs. 400 comes from our lua:

```
if args['ss'] ~= nil then
    local key = "stream_sess:"..args['ss']
    local ai_key = "auth_info:"..args['ss']
    local last_key = "last:"..args['ss']
    local ss, err = authdata:get(key)
    local auth_info, err = authdata:get(ai_key)
    if not ss or not auth_info then
        -- session not found
        ngx.exit(400)
    end
```

```
[14/Jan/2020:15:36:51 +0200] "GET /87/live1-141680.ts?ss=d66_ HTTP/1.1" 200 MISS 2112180 0.371 "-" "-" "-" "127.0.0.1:8100" local=x.x.x.x:8200 remote=y.y.y.y:59398
[14/Jan/2020:15:36:57 +0200] "GET /87/live1-141681.ts?ss=d66_ HTTP/1.1" 200 MISS 1951064 0.335 "-" "-" "-" "127.0.0.1:8100" local=x.x.x.x:8200 remote=y.y.y.y:59399
--there was nginx reload--
[14/Jan/2020:15:37:03 +0200] "GET /87/live1-141682.ts?ss=d66_ HTTP/1.1" **400** - 150 0.000 "-" "-" "-" "-" local=x.x.x.x:8200 remote=y.y.y.y:59400
[14/Jan/2020:15:37:03 +0200] "GET /87/live1-141682.ts?ss=d66_ HTTP/1.1" **400** - 150 0.000 "-" "-" "-" "-" local=x.x.x.x:8200 remote=y.y.y.y:59401

```


```
nginx version: nginx/1.16.1
built by gcc 4.4.7 20120313 (Red Hat 4.4.7-11) (GCC)
built with OpenSSL 1.0.1e-fips 11 Feb 2013
TLS SNI support enabled
configure arguments: --prefix=/usr/share/nginx --with-debug --with-cc-opt='-DNGX_LUA_USE_ASSERT -DNGX_LUA_ABORT_AT_PANIC -O2 -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic' --add-module=../ngx_devel_kit-0.3.1rc1 --add-module=../echo-nginx-module-0.61 --add-module=../xss-nginx-module-0.06 --add-module=../ngx_coolkit-0.2 --add-module=../set-misc-nginx-module-0.32 --add-module=../form-input-nginx-module-0.12 --add-module=../encrypted-session-nginx-module-0.08 --add-module=../srcache-nginx-module-0.31 --add-module=../ngx_lua-0.10.15 --add-module=../ngx_lua_upstream-0.07 --add-module=../headers-more-nginx-module-0.33 --add-module=../array-var-nginx-module-0.05 --add-module=../memc-nginx-module-0.19 --add-module=../redis2-nginx-module-0.15 --add-module=../redis-nginx-module-0.3.7 --add-module=../rds-json-nginx-module-0.15 --add-module=../rds-csv-nginx-module-0.09 --add-module=../ngx_stream_lua-0.0.7 --add-module=../nginx-dav-ext-module-2.0.0 --add-module=../ngx_http_bytes_filter_module-0.1 --add-module=../ngx_http_compose_filter_module-0.1 --add-module=../ngx_http_substitutions_filter_module-0.1 --add-module=../ngx_http_file_compose_filter_module-0.1 --add-module=../ngx_hlscombined_module-0.1 --with-ld-opt='-Wl,-rpath,/usr/share/nginx/luajit/lib -Wl,-E' --with-ipv6 --with-pcre-jit --user=nginx --group=nginx --sbin-path=/usr/sbin/nginx --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --http-client-body-temp-path=/var/lib/nginx/tmp/client_body --http-proxy-temp-path=/var/lib/nginx/tmp/proxy --http-fastcgi-temp-path=/var/lib/nginx/tmp/fastcgi --pid-path=/var/run/nginx.pid --lock-path=/var/lock/subsys/nginx --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_ssl_module --with-http_realip_module --with-http_geoip_module --with-http_addition_module --with-http_sub_module --with-http_dav_module --with-http_flv_module --with-http_gzip_static_module --with-http_stub_status_module --with-http_perl_module --with-file-aio --with-http_secure_link_module --with-stream --with-stream_ssl_module --with-stream_ssl_preread_module

```

My nginx config like this:
`
server {
...
access_by_lua_file 'lua/test_access.lua';
location /a/
{
proxy_pass..
   }
location /b/{
        default_type text/plain;
        return 200 "you visit b";
}
}
`
When I visit "/a/"， test_access.lua works while "/b/"， it does not.
It means that the access phase is skipped when 'return' is used?

I hereby granted the copyright of the changes in this pull request
to the authors of this lua-nginx-module project.

Implements #1620 
Hello. In some cases init_worker_by_lua_block cause nginx segfault

```
nginx[31582]: segfault at 8 ip 00007f4a21b29adb sp 00007ffcbbda6720 error 4 in ngx_http_lua_module.so[7f4a21af2000+54000]
```

Nginx core dump:

```
Using host libthread_db library "/lib64/libthread_db.so.1".
Core was generated by `nginx: master process /usr/sbin/nginx -c'.
Program terminated with signal 11, Segmentation fault.
#0  0x00007f0b103fea0b in ngx_http_lua_init_worker (cycle=<optimized out>) at lua-nginx-module-master/src/ngx_http_lua_initworkerby.c:216
216	            if (ngx_modules[i]->index == ngx_http_lua_module.index) {
(gdb) bt
#0  0x00007f0b103fea0b in ngx_http_lua_init_worker (cycle=<optimized out>) at lua-nginx-module-master/src/ngx_http_lua_initworkerby.c:216
#1  0x000055d0534e4cf6 in ngx_worker_process_init (cycle=cycle@entry=0x55d053bb6300, worker=worker@entry=0) at src/os/unix/ngx_process_cycle.c:931
#2  0x000055d0534e5186 in ngx_worker_process_cycle (cycle=cycle@entry=0x55d053bb6300, data=data@entry=0x0) at src/os/unix/ngx_process_cycle.c:735
#3  0x000055d0534e368b in ngx_spawn_process (cycle=cycle@entry=0x55d053bb6300, proc=proc@entry=0x55d0534e5160 <ngx_worker_process_cycle>, data=data@entry=0x0, 
    name=name@entry=0x55d053747d6e "worker process", respawn=respawn@entry=-3) at src/os/unix/ngx_process.c:199
#4  0x000055d0534e4890 in ngx_start_worker_processes (cycle=cycle@entry=0x55d053bb6300, n=1, type=type@entry=-3) at src/os/unix/ngx_process_cycle.c:359
#5  0x000055d0534e5ba3 in ngx_master_process_cycle (cycle=cycle@entry=0x55d053bb6300) at src/os/unix/ngx_process_cycle.c:131
#6  0x000055d0534bcd7f in main (argc=<optimized out>, argv=<optimized out>) at src/core/nginx.c:382
```

I can reproduce it on one of our servers, which was updated to centos 7 and nginx 1.17 a day ago. But I can't reproduce it on freshly installed VM.

* Software versions:
  ```
  CentOS Linux release 7.7.1908 (Core)
  nginx version: nginx/1.17.6
  lua-nginx-module from master
  ```

* A minimal and standalone test case that others can easily run on their side and
reproduce the issue you are seeing:

  I have added init_worker_by_lua_block to my nginx configuration (empty or not - it does not matter). After nginx restart, I looked at /var/log/messages and there are many nginx workers segfaults.

Also, I've investigated file ngx_http_lua_initworkerby.c near line 216 and it seems like a bug:

1. on lines 197-201 **modules** variable was defined based on nginx_version:

https://github.com/openresty/lua-nginx-module/blob/760f7073da87da23bb4f024943089edfad35c72e/src/ngx_http_lua_initworkerby.c#L197-L220

2. but later on line 216 you use **ngx_modules[i]->index** instead of **modules** variable

So I prepared the small patch that fixed issue with nginx segfaults in my case:

```diff
diff --git a/src/ngx_http_lua_initworkerby.c b/src/ngx_http_lua_initworkerby.c
index 5b345280..7e8c3db5 100644
--- a/src/ngx_http_lua_initworkerby.c
+++ b/src/ngx_http_lua_initworkerby.c
@@ -213,7 +213,7 @@ ngx_http_lua_init_worker(ngx_cycle_t *cycle)
                 return NGX_ERROR;
             }
 
-            if (ngx_modules[i]->index == ngx_http_lua_module.index) {
+            if (modules[i]->index == ngx_http_lua_module.index) {
                 ngx_memcpy(cur,
                            conf_ctx->main_conf[ngx_http_lua_module.ctx_index],
                            sizeof(ngx_http_lua_main_conf_t));
```

PS: I'll make PR if you want
Is it a bad idea to append new keys to `ngx.req/resp`? For example, would the following be considered bad?

```lua
if ngx.var.request_method == 'POST' then
  ngx.req.read_body()
  ngx.req.rawBody = ngx.req.get_body_data()
  if ngx.headers['content-type'] == 'application/json' then
    local ok, payload = pcall(cjson.decode, ngx.req.get_body_data())
    if ok then
      ngx.req.body = payload
    end
  end
end
```

So that I can automatically parse the post body in the instantiation of my app, and then just pass around `ngx.req` and `ngx.resp` to all my controllers?

We're getting these segfault error in our servers' `dmesg -T`:

```
[Sun Nov 24 01:11:33 2019] nginx[55337]: segfault at 10 ip 0000000000450278 sp 00007ffe741a7e70 error 4 in nginx[400000+1a5000]
[Sun Nov 24 01:16:58 2019] nginx[62975]: segfault at 10 ip 0000000000450278 sp 00007ffe741a7ed0 error 4 in nginx[400000+1a5000]
[Sun Nov 24 02:10:12 2019] nginx[11441]: segfault at 10 ip 0000000000450278 sp 00007ffe741a7ed0 error 4 in nginx[400000+1a5000]
[Sun Nov 24 03:17:44 2019] nginx[48660]: segfault at 10 ip 0000000000450278 sp 00007ffe741a7ed0 error 4 in nginx[400000+1a5000]
[Sun Nov 24 03:44:28 2019] nginx[45059]: segfault at 10 ip 0000000000450278 sp 00007ffe741a7ed0 error 4 in nginx[400000+1a5000]
[Sun Nov 24 03:53:56 2019] nginx[56675]: segfault at 10 ip 0000000000450278 sp 00007ffe741a7f30 error 4 in nginx[400000+1a5000]
[Sun Nov 24 10:42:01 2019] nginx[45066]: segfault at 10 ip 0000000000450278 sp 00007ffe741a7ed0 error 4 in nginx[400000+1a5000]
[Sun Nov 24 11:35:11 2019] nginx[45049]: segfault at 10 ip 0000000000450278 sp 00007ffe741a7e70 error 4 in nginx[400000+1a5000]
[Sun Nov 24 13:18:27 2019] nginx[48432]: segfault at 10 ip 0000000000450278 sp 00007ffe741a7ed0 error 4 in nginx[400000+1a5000]
[Sun Nov 24 15:11:51 2019] nginx[45046]: segfault at 10 ip 0000000000450278 sp 00007ffe741a7ed0 error 4 in nginx[400000+1a5000]
[Sun Nov 24 17:01:28 2019] nginx[49714]: segfault at 10 ip 0000000000450278 sp 00007ffe741a7ed0 error 4 in nginx[400000+1a5000]
[Sun Nov 24 22:46:25 2019] nginx[45063]: segfault at 10 ip 0000000000450278 sp 00007ffe741a7ed0 error 4 in nginx[400000+1a5000]
[Sun Nov 24 23:15:02 2019] DCCP: Activated CCID 2 (TCP-like)
[Sun Nov 24 23:54:36 2019] nginx[45058]: segfault at 10 ip 0000000000450278 sp 00007ffe741a7ed0 error 4 in nginx[400000+1a5000]
[Mon Nov 25 00:50:16 2019] nginx[11183]: segfault at 10 ip 0000000000450278 sp 00007ffe741a7f30 error 4 in nginx[400000+1a5000]
[Mon Nov 25 09:26:21 2019] nginx[45051]: segfault at 10 ip 0000000000450278 sp 00007ffe741a7ed0 error 4 in nginx[400000+1a5000]
[Mon Nov 25 10:06:42 2019] nginx[45061]: segfault at 10 ip 0000000000450278 sp 00007ffe741a7ed0 error 4 in nginx[400000+1a5000]
[Mon Nov 25 15:28:28 2019] nginx[48523]: segfault at 10 ip 0000000000450278 sp 00007ffe741a7ed0 error 4 in nginx[400000+1a5000]
[Mon Nov 25 17:25:33 2019] nginx[45085]: segfault at 10 ip 0000000000450278 sp 00007ffe741a7ed0 error 4 in nginx[400000+1a5000]
[Tue Nov 26 12:01:32 2019] nginx[26858]: segfault at 10 ip 0000000000450278 sp 00007ffe741a7ed0 error 4 in nginx[400000+1a5000]
[Tue Nov 26 16:27:10 2019] nginx[26852]: segfault at 10 ip 0000000000450278 sp 00007ffe741a7ed0 error 4 in nginx[400000+1a5000]
[Tue Nov 26 16:56:06 2019] nginx[32879]: segfault at 10 ip 0000000000450278 sp 00007ffe741a7ed0 error 4 in nginx[400000+1a5000]
[Tue Nov 26 17:08:48 2019] nginx[27029]: segfault at 10 ip 0000000000450278 sp 00007ffe741a7ed0 error 4 in nginx[400000+1a5000]
```
which results in nginx worker dying.
```
admin@server1:~$ ps aux | grep nginx 
root     17185  0.0  0.0 10966328 76736 ?      Ss   Nov21   0:02 nginx: master process /usr/local/openresty/nginx/sbin/nginx -g daemon off;
admin    19284  0.0  0.0  14428  1016 pts/1    S+   18:11   0:00 grep --color=auto nginx
www-data 24104 53.9  1.1 13112056 2915064 ?    Rl   16:27  56:07 nginx: worker process
www-data 26849 38.3  1.1 13131364 2917532 ?    Rl   Nov25 539:13 nginx: worker process
www-data 26850 40.0  1.1 13119240 2916524 ?    Sl   Nov25 563:26 nginx: worker process
www-data 26851 38.9  1.1 13116688 2915828 ?    Rl   Nov25 548:01 nginx: worker process
www-data 26853 38.6  1.1 13113628 2915744 ?    Sl   Nov25 543:29 nginx: worker process
www-data 26854 37.8  1.1 13165896 2917112 ?    Rl   Nov25 533:00 nginx: worker process
www-data 26855 40.1  1.1 13120868 2913392 ?    Rl   Nov25 564:35 nginx: worker process
www-data 26856 39.7  1.1 13110144 2916520 ?    Rl   Nov25 559:03 nginx: worker process
www-data 26857 38.9  1.1 13148168 2917120 ?    Rl   Nov25 548:08 nginx: worker process
www-data 26859 39.1  1.1 13572304 2915288 ?    Sl   Nov25 550:34 nginx: worker process
www-data 26860 39.3  1.1 13136392 2917824 ?    Sl   Nov25 554:10 nginx: worker process
www-data 26861 36.6  1.1 13121696 2912208 ?    Rl   Nov25 516:31 nginx: worker process
www-data 26862 39.8  1.1 13122880 2915588 ?    Sl   Nov25 560:57 nginx: worker process
www-data 26863 40.0  1.1 13124544 2917608 ?    Rl   Nov25 563:58 nginx: worker process
www-data 26890 38.9  1.1 13118828 2913448 ?    Rl   Nov25 548:29 nginx: worker process
www-data 27125 39.3  1.1 13129416 2916692 ?    Sl   Nov25 553:48 nginx: worker process
www-data 27332 40.1  1.1 13118996 2917948 ?    Rl   Nov25 565:47 nginx: worker process
www-data 27729 34.7  1.1 13123036 2915904 ?    Rl   Nov25 488:57 nginx: worker process
www-data 28471 38.0  1.1 13119220 2915672 ?    Rl   Nov25 536:10 nginx: worker process
www-data 29107 39.9  1.1 13116080 2917152 ?    Rl   Nov25 562:45 nginx: worker process
www-data 29230 38.0  1.1 13119680 2920876 ?    Rl   Nov25 535:43 nginx: worker process
www-data 29343 37.2  1.1 13179464 2918900 ?    Rl   Nov25 523:51 nginx: worker process
www-data 29596 38.4  1.1 13377672 2912988 ?    Rl   Nov25 541:06 nginx: worker process
www-data 30046 39.7  1.1 13130296 2915472 ?    Sl   Nov25 559:25 nginx: worker process
www-data 30343 40.1  1.1 13111124 2915840 ?    Rl   Nov25 564:33 nginx: worker process
www-data 30533 40.7  1.1 13128176 2920924 ?    Sl   Nov25 572:57 nginx: worker process
www-data 31597 38.5  1.1 13115692 2918244 ?    Rl   Nov25 541:58 nginx: worker process
www-data 32360 42.3  1.1 13124084 2920432 ?    Rl   Nov25 595:56 nginx: worker process
www-data 32543 40.5  1.1 13120000 2917596 ?    Sl   Nov25 571:19 nginx: worker process
www-data 33242 39.6  1.1 13118876 2919700 ?    Sl   Nov25 557:54 nginx: worker process
www-data 33319 41.7  1.1 13126744 2918196 ?    Sl   Nov25 587:37 nginx: worker process
www-data 33438 41.5  1.1 13119768 2917208 ?    Sl   Nov25 584:41 nginx: worker process
www-data 33666 39.3  1.1 13152324 2916208 ?    Sl   Nov25 554:31 nginx: worker process
www-data 33761 42.4  1.1 13122924 2920868 ?    Rl   Nov25 596:51 nginx: worker process
www-data 33822 40.0  1.1 13118336 2917828 ?    Rl   Nov25 563:24 nginx: worker process
www-data 33939 44.0  1.1 13123636 2916984 ?    Rl   Nov25 619:38 nginx: worker process
www-data 34749 40.2  1.1 13123592 2918996 ?    Rl   Nov25 567:09 nginx: worker process
www-data 34859  1.8  1.0 10963176 2880508 ?    S    Nov25  26:42 nginx: cache manager process
www-data 38198 54.6  1.1 13127556 2925104 ?    Rl   12:02 202:06 nginx: worker process
www-data 38714 54.5  1.1 13113452 2916536 ?    Sl   16:56  40:54 nginx: worker process
www-data 64889 49.1  1.1 13117332 2913056 ?    Rl   17:09  30:40 nginx: worker process
```

This is the output of `addr2line` which points to `/tmp/openresty-1.15.8.2/build/nginx-1.15.8/src/event/ngx_event_openssl.c:1751` line.
```
admin@server1:/# addr2line -e /usr/local/openresty/nginx/sbin/nginx 0000000000450278
/tmp/openresty-1.15.8.2/build/nginx-1.15.8/src/event/ngx_event_openssl.c:1751
```
We're using the latest version of OpenResty which is `1.15.8.2` with following compile time modules
```
./configure --with-luajit \
                --with-http_secure_link_module \
                --with-http_stub_status_module \
                --with-http_v2_module \
                --with-http_slice_module \
                --with-threads \
                --add-module=/tmp/ngx_http_geoip2_module-3.2
```

i want to use  ngx.req.set_header   to  reset  request  header,use  $upstream_addr  variables, 
but  i try ,then  not effect ,please  tell  me  how to do this? tks
PR to fix #1619 
I have tried to follow the "Installation" part in the README.markdown, and I found two import modules  are not mentioned: "lua-resty-lrucache" and "lua-resty-lrucache". You cannot do the installation without them. So , I think it's very necessary to add them to the doc. 