I have installed both versions, 0.3.10 and 0.2.4, for "umap-learn". Is there a good beginners manual to get me started yet. Thanks
Dave
The basic example in the README hangs when using UMAP from the master or 0.4dev branch.

```python
import umap
from sklearn.datasets import load_digits

digits = load_digits()

embedding = umap.UMAP().fit_transform(digits.data)
```

If I do

```
pip install umap-learn
```

then the code works as expected (taking only 5-10 seconds). However, it hangs when using a version of UMAP built from the 0.4dev branch or master. It looks like it is doing *something* since I see the message

```
/home/gclenden/anaconda3/envs/umap_debug/lib/python3.7/site-packages/umap/spectral.py:243: UserWarning: Embedding a total of 2 separate connected components using meta-embedding (experimental)
  n_components
```

I think the commit where this occurs is https://github.com/lmcinnes/umap/commit/eb6774056aa5fc8a858d466eaba4a1cb35c978ae. It works as expected on the commit prior to that one https://github.com/lmcinnes/umap/commit/c26bc8dac3e8fc71a4221986cf3cf0926b573766. However, I don't know enough about numba/tbb to explain why this might be happening.
Hello,

I'm having the same issue as in #225, except I'm on Ubuntu. This occurred for several numba versions and seems to be fixed in 0.47.0.

Update: the issue may be coming from llvmlite==0.31.0. Downgrading to 0.30.0 fixed the issue. 
I'm trying out the `plot.diagnostic()` function in the `0.4dev` branch of UMAP. I'm using commit `fc59aa7`, with `pynndescent==0.4.2` (if that matters). When I run `umap.plot.diagnostic(embedder, diagnostic_type='local_dim')`, I get this error:
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-300-859233e31c09> in <module>
----> 1 local_dims = umap.plot.diagnostic(o.visual_embedder, diagnostic_type='local_dim')

~/projects/umap/umap/plot.py in diagnostic(umap_object, diagnostic_type, nhood_size, local_variance_threshold, ax, cmap, point_size, background, width, height)
   1001 
   1002     elif diagnostic_type == "local_dim":
-> 1003         highd_indices, highd_dists = _nhood_search(umap_object, umap_object.n_neighbors)
   1004         data = umap_object._raw_data
   1005         local_dim = np.empty(data.shape[0], dtype=np.int64)

~/projects/umap/umap/plot.py in _nhood_search(umap_object, nhood_size)
    151             rng_state,
    152             umap_object._distance_func,
--> 153             umap_object._dist_args,
    154         )
    155 

~/projects/umap/umap/nndescent.py in initialise_search(forest, data, query_points, n_neighbors, rng_state, dist, dist_args)
    276     )
    277     if forest is not None:
--> 278         for tree in forest:
    279             init_from_tree(
    280                 tree, data, query_points, results, rng_state, dist, dist_args

TypeError: 'NNDescent' object is not iterable
``` 
It'd be great to add other correlations as distances, such as Spearman's rho or the new correlation coefficient called [phi_k](https://phik.readthedocs.io/en/latest/). I am aware that it's possible to use any distance metric using either `metric='precomputed'` or implementing a custom function, but it'd be more convenient to pass `metric='spearmanrho'` or `'phik'` directly.
Hi There,

I'm analyzing single-cell rna-seq data using scanpy on a Ubuntu virtual machine with 16 cpus and  the following package versions:

 - umap-learn  0.4.0 (installed from the 0.4dev branch earlier today)
 - pynndescent  0.3.3
-  scanpy 1.4.5.dev175+g64f04d8 (installed from the master branch earlier today)

I'm using the development 0.4 version of umap because it is supposed to have support for parallelized computation of the nearest neighbors. Specifically, scanpy calls:

```
from umap.umap_ import nearest_neighbors

random_state = check_random_state(random_state)

knn_indices, knn_dists, forest = nearest_neighbors(
        X, n_neighbors, random_state=random_state,
        metric=metric, metric_kwds=metric_kwds,
        angular=angular, verbose=verbose,
)
```

the code is working but, empirically is just using a single CPU. It also gives the following warning message:
```
/opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: 
The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.

To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.

File "../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py", line 47:
    @numba.njit(parallel=True)
    def nn_descent(
    ^

  self.func_ir.loc))
```

and when I turn on numba parallel diagnostics, that gives the report below. Any idea what is going on or if parallelized approximate nearest neighbors computations is supposed to be supported? Thanks!

```
 
================================================================================
 Parallel Accelerator Optimizing:  Function make_nn_descent.<locals>.nn_descent,
 /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py (46)
  
================================================================================


Parallel loop listing for  Function make_nn_descent.<locals>.nn_descent, /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py (46) 
------------------------------------------------------------------------------------------|loop #ID
    @numba.njit(parallel=True)                                                            | 
    def nn_descent(                                                                       | 
        data,                                                                             | 
        n_neighbors,                                                                      | 
        rng_state,                                                                        | 
        max_candidates=50,                                                                | 
        n_iters=10,                                                                       | 
        delta=0.001,                                                                      | 
        rho=0.5,                                                                          | 
        rp_tree_init=True,                                                                | 
        leaf_array=None,                                                                  | 
        verbose=False,                                                                    | 
    ):                                                                                    | 
        n_vertices = data.shape[0]                                                        | 
                                                                                          | 
        current_graph = make_heap(data.shape[0], n_neighbors)                             | 
        for i in range(data.shape[0]):                                                    | 
            indices = rejection_sample(n_neighbors, data.shape[0], rng_state)             | 
            for j in range(indices.shape[0]):                                             | 
                d = dist(data[i], data[indices[j]], *dist_args)                           | 
                heap_push(current_graph, i, d, indices[j], 1)                             | 
                heap_push(current_graph, indices[j], d, i, 1)                             | 
                                                                                          | 
        if rp_tree_init:                                                                  | 
            for n in range(leaf_array.shape[0]):                                          | 
                for i in range(leaf_array.shape[1]):                                      | 
                    if leaf_array[n, i] < 0:                                              | 
                        break                                                             | 
                    for j in range(i + 1, leaf_array.shape[1]):                           | 
                        if leaf_array[n, j] < 0:                                          | 
                            break                                                         | 
                        d = dist(                                                         | 
                            data[leaf_array[n, i]], data[leaf_array[n, j]], *dist_args    | 
                        )                                                                 | 
                        heap_push(                                                        | 
                            current_graph, leaf_array[n, i], d, leaf_array[n, j], 1       | 
                        )                                                                 | 
                        heap_push(                                                        | 
                            current_graph, leaf_array[n, j], d, leaf_array[n, i], 1       | 
                        )                                                                 | 
                                                                                          | 
        for n in range(n_iters):                                                          | 
            if verbose:                                                                   | 
                print("\t", n, " / ", n_iters)                                            | 
                                                                                          | 
            candidate_neighbors = build_candidates(                                       | 
                current_graph, n_vertices, n_neighbors, max_candidates, rng_state         | 
            )                                                                             | 
                                                                                          | 
            c = 0                                                                         | 
            for i in range(n_vertices):                                                   | 
                for j in range(max_candidates):                                           | 
                    p = int(candidate_neighbors[0, i, j])                                 | 
                    if p < 0 or tau_rand(rng_state) < rho:                                | 
                        continue                                                          | 
                    for k in range(max_candidates):                                       | 
                        q = int(candidate_neighbors[0, i, k])                             | 
                        if (                                                              | 
                            q < 0                                                         | 
                            or not candidate_neighbors[2, i, j]                           | 
                            and not candidate_neighbors[2, i, k]                          | 
                        ):                                                                | 
                            continue                                                      | 
                                                                                          | 
                        d = dist(data[p], data[q], *dist_args)                            | 
                        c += heap_push(current_graph, p, d, q, 1)                         | 
                        c += heap_push(current_graph, q, d, p, 1)                         | 
                                                                                          | 
            if c <= delta * n_neighbors * data.shape[0]:                                  | 
                break                                                                     | 
                                                                                          | 
        return deheap_sort(current_graph)                                                 | 
--------------------------------- Fusing loops ---------------------------------
Attempting fusion of parallel loops (combines loops with similar properties)...
----------------------------- Before Optimisation ------------------------------
--------------------------------------------------------------------------------
------------------------------ After Optimisation ------------------------------
Parallel structure is already optimal.
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
 
---------------------------Loop invariant code motion---------------------------

Instruction hoisting:
No instruction hoisting found
--------------------------------------------------------------------------------

```

Over the time there are a lot of issues where we have to ask for reproducible code snippets in case of problems. Maybe it would be good to have an issue template that reminds people to add code that allows use to reproduce their problems, as well as write us their used version?

https://help.github.com/en/github/building-a-strong-community/creating-issue-templates-for-your-repository
Hi,

I've run across an error in the transform function when trying to find neighbor samples using the rejection_sample due to my own overlooking of parameter transform_queue_size (by default set to 4). This made the while loop in the rejection_sample function never converge as the number of desired neighbors was bigger than the actual fitted population). Adding a simple size test before starting rejection_sample could help other users make my mistake.

Thanks for your amazing work!
I have two UMAP transform objects that I've "trained" on two separate datasets, one 8000x512 and one 8000x1024 in dimension. Both of these transform objects produce a successful and reasonable embedding in 2 dimensions using the training set. I have an additional 160k rows of data for each of the two feature sizes (512 and 1024), which I am using as "test" data. When I transform the 160k x 512 array into 160k x 2, there is no issue. However, the content of the 160k x 2 array that is produced when I transform the 160k x 1024 input array is **entirely NaNs**. I have already verified that the inputs do not contain any NaNs or infinities. Additionally, taking a smaller subset of the input set, such as 10 rows, also produces a 10x2 output that is full of NaNs. Is there any reason why this might be the case? I'm wondering if perhaps there is an inherent size limit to the number of features for new data that is not a constraint on the original data.
I think Dynamic Time Warping (DTW) distance would be a nice addition when we deal with time series. 

While searching for UMAP + DTW on Google I found this implementation: https://gist.github.com/kylemcdonald/76b6f18fb4026e01196282b59bd31e7e

I didn't try it (I don't know if it correctly works), but I think it could be useful to add DTW to built-in distances in UMAP.