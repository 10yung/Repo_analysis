A port of the Java xDS test client and server, added in https://github.com/grpc/grpc-java/pull/6585. Analogous to the internal `MassageServer` and `MassageClient` for testing LB-related functionality.

It looks like Go is a bit better about mirroring definitions from the grpc-proto repo than Java is; I can update the protos in grpc-proto in a parallel PR if the changes here look agreeable.

First time that I can recall ever writing Go, so apologies in advance for any glaring mistakes. I managed to run `gofmt`, so hopefully it at least is vaguely shaped like real code ;)

cc @menghanl 
When I return io.EOF in a stream handler the client side code sees OUT_OF_RANGE error as the status code of the RPC error.  I understand it is incorrect to return an EOF from a stream handler, and you should return nil, but that aside where is the OUT_OF_RANGE code coming from?

Note: this issue seems fixed on the master branch. Is this tracked by another bug?

### What version of gRPC are you using?
1.9.1
### What version of Go are you using (`go version`)?
go1.13.3
### What operating system (Linux, Windows, …) and version?
Mac OSx 10.14.6
### What did you do?
If possible, provide a recipe for reproducing the error.
Take the route_guide example as is and make the following modifications:

in grpc/examples/route_guide/server/server.go change RouteChat to return different values on EOF. Shown below:
```
 // RouteChat receives a stream of message/location pairs, and responds with a stream of all
 // previous messages at each of those locations.
 func (s *routeGuideServer) RouteChat(stream pb.RouteGuide_RouteChatServer) error {
     for {
         in, err := stream.Recv()
         if err == io.EOF {
             return io.EOF // Client receives: rpc error: code = OutOfRange desc = EOF
             // return errors.New("EOF") // Client recieves: rpc error: code = Unknown desc = EOF
             // return errors.New("foo") // Client recieves: rpc error: code = Unknown desc = foo
             // return nil
         }
         if err != nil {
             return err
         }
         key := serialize(in.Location)

         s.mu.Lock()
         s.routeNotes[key] = append(s.routeNotes[key], in)
         // Note: this copy prevents blocking other clients while serving this one.
         // We don't need to do a deep copy, because elements in the slice are
         // insert-only and never modified.
         rn := make([]*pb.RouteNote, len(s.routeNotes[key]))
         copy(rn, s.routeNotes[key])
         s.mu.Unlock()

         for _, note := range rn {
             if err := stream.Send(note); err != nil {
                 return err
             }
         }
     }
 }
```
In grpc/examples/route_guide/client/client.go:
In runRouteChat() Simply print out the err code:
```
in, err := stream.Recv()
fmt.Printf("%v\n", err)
```
Then run the server and then the client

### What did you expect to see?
I expect to see 
```
rpc error: code = Unknown desc = EOF
```

### What did you see instead?
Instead I see
```
rpc error: code = OutOfRange desc = EOF
```

`conn, err := grpc.Dial("", grpc.WithInsecure())
  if err != nil {
    log.Fatalf("did not connect: %s", err)
  }`

The above code with empty port doesn't throw any error. What is the recommend way to know if the connection has been established or failed?

This PR adds the end to end tests for the advancedtls library.
It can also be used as a reference for users who want to use this library.
The `BalancerName` field, and all the references to it are removed. In tests where it was used to pass the xds server address to the xds client, `bootstrapConfigNew()` is overridden.

<!-- Reviewable:start -->
---
This change is [<img src="https://reviewable.io/review_button.svg" height="34" align="absmiddle" alt="Reviewable"/>](https://reviewable.io/reviews/grpc/grpc-go/3316)
<!-- Reviewable:end -->


Remove decodeState from http_client, and decode the header fields within the client. 

This will fix half of #1457. I can submit a second PR to remove decodeState from http_server, or if it's preferred I can fold it into this PR too.

A data race during proto marshal. Could be related to the buffer reuse.

<details><summary>Full log</summary>

```
==================
WARNING: DATA RACE
Write at 0x00c001ee6ff8 by goroutine 218:
  runtime.slicecopy()
      /home/travis/.gimme/versions/go1.13.6.linux.amd64/src/runtime/slice.go:197 +0x0
  github.com/golang/protobuf/proto.appendBytes3()
      /home/travis/go/pkg/mod/github.com/golang/protobuf@v1.3.2/proto/table_marshal.go:2136 +0x14f
  github.com/golang/protobuf/proto.(*marshalInfo).marshal()
      /home/travis/go/pkg/mod/github.com/golang/protobuf@v1.3.2/proto/table_marshal.go:270 +0x575
  github.com/golang/protobuf/proto.makeMessageMarshaler.func2()
      /home/travis/go/pkg/mod/github.com/golang/protobuf@v1.3.2/proto/table_marshal.go:2234 +0x173
  github.com/golang/protobuf/proto.(*marshalInfo).marshal()
      /home/travis/go/pkg/mod/github.com/golang/protobuf@v1.3.2/proto/table_marshal.go:270 +0x575
  github.com/golang/protobuf/proto.(*InternalMessageInfo).Marshal()
      /home/travis/go/pkg/mod/github.com/golang/protobuf@v1.3.2/proto/table_marshal.go:141 +0xcb
  google.golang.org/grpc/test/grpc_testing.(*SimpleResponse).XXX_Marshal()
      /home/travis/gopath/src/google.golang.org/grpc/test/grpc_testing/test.pb.go:244 +0x9b
  github.com/golang/protobuf/proto.(*Buffer).Marshal()
      /home/travis/go/pkg/mod/github.com/golang/protobuf@v1.3.2/proto/table_marshal.go:2742 +0x190
  google.golang.org/grpc/encoding/proto.marshal()
      /home/travis/gopath/src/google.golang.org/grpc/encoding/proto/proto.go:46 +0x1b5
  google.golang.org/grpc/encoding/proto.codec.Marshal()
      /home/travis/gopath/src/google.golang.org/grpc/encoding/proto/proto.go:60 +0xde
  google.golang.org/grpc/encoding/proto.(*codec).Marshal()
      <autogenerated>:1 +0x62
  google.golang.org/grpc.encode()
      /home/travis/gopath/src/google.golang.org/grpc/rpc_util.go:543 +0x6e
  google.golang.org/grpc.(*Server).sendResponse()
      /home/travis/gopath/src/google.golang.org/grpc/server.go:846 +0x192
  google.golang.org/grpc.(*Server).processUnaryRPC()
      /home/travis/gopath/src/google.golang.org/grpc/server.go:1070 +0xad6
  google.golang.org/grpc.(*Server).handleStream()
      /home/travis/gopath/src/google.golang.org/grpc/server.go:1331 +0x1343
  google.golang.org/grpc.(*Server).serveStreams.func1.1()
      /home/travis/gopath/src/google.golang.org/grpc/server.go:722 +0xc8
Previous read at 0x00c001ee6fff by goroutine 437:
  runtime.slicecopy()
      /home/travis/.gimme/versions/go1.13.6.linux.amd64/src/runtime/slice.go:197 +0x0
  golang.org/x/net/http2.(*Framer).WriteDataPadded()
      /home/travis/go/pkg/mod/golang.org/x/net@v0.0.0-20190311183353-d8887717615a/http2/frame.go:683 +0x34d
  golang.org/x/net/http2.(*writeData).writeFrame()
      /home/travis/go/pkg/mod/golang.org/x/net@v0.0.0-20190311183353-d8887717615a/http2/frame.go:643 +0x11d
  golang.org/x/net/http2.(*serverConn).writeFrameAsync()
      /home/travis/go/pkg/mod/golang.org/x/net@v0.0.0-20190311183353-d8887717615a/http2/server.go:741 +0x58
Goroutine 218 (running) created at:
  google.golang.org/grpc.(*Server).serveStreams.func1()
      /home/travis/gopath/src/google.golang.org/grpc/server.go:720 +0xb8
  google.golang.org/grpc/internal/transport.(*http2Server).operateHeaders()
      /home/travis/gopath/src/google.golang.org/grpc/internal/transport/http2_server.go:447 +0x16a6
  google.golang.org/grpc/internal/transport.(*http2Server).HandleStreams()
      /home/travis/gopath/src/google.golang.org/grpc/internal/transport/http2_server.go:488 +0x459
  google.golang.org/grpc.(*Server).serveStreams()
      /home/travis/gopath/src/google.golang.org/grpc/server.go:718 +0x19a
  google.golang.org/grpc.(*Server).handleRawConn.func1()
      /home/travis/gopath/src/google.golang.org/grpc/server.go:679 +0x4c
Goroutine 437 (finished) created at:
  golang.org/x/net/http2.(*serverConn).startFrameWrite()
      /home/travis/go/pkg/mod/golang.org/x/net@v0.0.0-20190311183353-d8887717615a/http2/server.go:1119 +0x365
  golang.org/x/net/http2.(*serverConn).scheduleFrameWrite()
      /home/travis/go/pkg/mod/golang.org/x/net@v0.0.0-20190311183353-d8887717615a/http2/server.go:1220 +0x371
  golang.org/x/net/http2.(*serverConn).wroteFrame()
      /home/travis/go/pkg/mod/golang.org/x/net@v0.0.0-20190311183353-d8887717615a/http2/server.go:1181 +0x1dd
  golang.org/x/net/http2.(*serverConn).serve()
      /home/travis/go/pkg/mod/golang.org/x/net@v0.0.0-20190311183353-d8887717615a/http2/server.go:834 +0x13ce
  golang.org/x/net/http2.(*Server).ServeConn()
      /home/travis/go/pkg/mod/golang.org/x/net@v0.0.0-20190311183353-d8887717615a/http2/server.go:438 +0xd9d
  golang.org/x/net/http2.ConfigureServer.func1()
      /home/travis/go/pkg/mod/golang.org/x/net@v0.0.0-20190311183353-d8887717615a/http2/server.go:276 +0xb8
  net/http.(*conn).serve()
      /home/travis/.gimme/versions/go1.13.6.linux.amd64/src/net/http/server.go:1800 +0x1d35
==================
E0109 23:39:41.561162    7528 http2_server.go:856] transport: failed to marshal rpc status: code:13 message:"\377\376\375" details:<type_url:"type.googleapis.com/grpc.testing.Empty" > , error: proto: field "google.rpc.Status.Message" contains invalid UTF-8
--- FAIL: Test (99.99s)
    --- FAIL: Test/MaxMsgSizeClientDefault (0.67s)
        end2end_test.go:595: Running test in tcp-clear-v1-balancer environment...
        end2end_test.go:595: Running test in tcp-tls-v1-balancer environment...
        end2end_test.go:595: Running test in tcp-clear environment...
        end2end_test.go:595: Running test in tcp-tls environment...
        end2end_test.go:595: Running test in handler-tls environment...
        end2end_test.go:595: Running test in no-balancer environment...
        testing.go:853: race detected during execution of test
    testing.go:853: race detected during execution of test
```

</details>
I have a production go-grpc server workload that does near-zero heap allocation in the request handler. As I dial up qps, p99 client response times line up with gc duration on the server. The alloc_space profile largely points to http2 meta frame parsing as well as http2Server.operateHeaders() and it's descendants.
```
Fetching profile over HTTP from http://localhost:8081/debug/pprof/heap                                                 
Saved profile in /home/cy/pprof/pprof.quota_server.alloc_objects.alloc_space.inuse_objects.inuse_space.021.pb.gz       
File: quota_server                                         
Type: alloc_space                                          
Time: Jan 9, 2020 at 12:13pm (PST)                         
Entering interactive mode (type "help" for commands, "o" for options)                                                  
(pprof) web                                                
(pprof) top                                                
Showing nodes accounting for 25.35TB, 68.83% of 36.83TB total                                                          
Dropped 408 nodes (cum <= 0.18TB)                          
Showing top 10 nodes out of 68                             
      flat  flat%   sum%        cum   cum%                 
    8.95TB 24.30% 24.30%     8.95TB 24.30%  golang.org/x/net/http2.(*Framer).readMetaFrame.func1                       
    3.90TB 10.59% 34.90%    12.73TB 34.58%  google.golang.org/grpc/internal/transport.(*http2Server).operateHeaders    
    3.33TB  9.04% 43.93%     3.33TB  9.04%  google.golang.org/grpc/internal/transport.(*decodeState).addMetadata       
    2.06TB  5.58% 49.51%     2.06TB  5.58%  google.golang.org/grpc/internal/transport.(*itemList).enqueue              
    1.38TB  3.74% 53.25%     1.38TB  3.74%  google.golang.org/grpc/internal/transport.newRecvBuffer                    
    1.26TB  3.43% 56.68%     1.49TB  4.05%  google.golang.org/grpc/internal/transport.(*http2Server).writeHeaderLocked 
    1.26TB  3.43% 60.11%     1.95TB  5.30%  google.golang.org/grpc/internal/transport.(*http2Server).WriteStatus       
    1.15TB  3.11% 63.22%     4.24TB 11.52%  slack/gen-src/proto_idl/quotaservice._Quota_Ratelimit_Handler              
    1.03TB  2.80% 66.03%     1.03TB  2.80%  context.WithValue                                                          
    1.03TB  2.80% 68.83%     1.03TB  2.80%  google.golang.org/grpc/internal/transport.newWriteQuota  
```
Also see attached image. This could likely be reproduced by writing a 'trivial' server implementation and sending a high request rate to it. Happy to provide more debug info.

![pprof-heap-alloc-space-grpc-server](https://user-images.githubusercontent.com/1189895/72104700-df619980-32e0-11ea-8aca-6cb3ac9eabf1.jpg)

