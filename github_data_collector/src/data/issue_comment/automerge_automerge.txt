I want to _alter_ the contents of an automerge table by using _Table.set(id: UUID, value: T)_. But I'm having an issue, that the connection callback _(sendMsg)_ is not being fired after applying changes to a document. The following part is properly working where I add a new object by using _Table.add(item: T)_. Removing via _Table.remove(id: UUID)_ is also working. What could cause this issue?

**This DOES work**:
```
    let document: Doc<TodoDoc> = Automerge.init<TodoDoc>();
    let docset = this.docSet.setDoc('docset-id-1', this.document);
    let connection: Connection<TodoDoc>;

   constructor() {
     this.connection = new Connection<TodoDoc>(this.docSet, this.sendMessage.bind(this));
     this.connection.open();
   }
   ....
    const changedDoc = Automerge.change(this.document, 'Create todo', (doc: Doc<TodoDoc>) => {
      const newTodo: Todo = {
        id: todo.id,
        title: todo.title,
        complete: todo.complete,
        createdAt: todo.createdAt,
        updatedAt: new Date().toISOString(),
      };

      doc.todos.add(newTodo);
    });

    const currentChanges: Change[] = Automerge.getChanges(this.document, changedDoc);
    this.docSet.applyChanges('docset-id-1', currentChanges);
```

**This DOES NOT work**:
```
    const changedDoc = Automerge.change(this.document, 'Update todo', (doc: Doc<TodoDoc>) => {
      const currentTodoById = this.document.todos.byId(todo.id);

      currentTodoById.title = 'Updated Title';
      currentTodoById.complete = true;
      currentTodoById.updatedAt = new Date().toISOString();

      doc.todos.set(todo.id, currentTodoById)
    });

    const currentChanges: Change[] = Automerge.getChanges(this.document, changedDoc);
    this.docSet.applyChanges('docset-id-1', currentChanges);
```
After initializing a document with `Automerge.from()`, this patch changes the behaviour such that the initialization cannot be undone, as suggested in #219.
Hi folks! I've been following the JSON CRDT awesomeness for a few years now.

From https://github.com/automerge/automerge/issues/212, it sounds like performance for text/ordered lists gets rough for substantial documents, possibly because of the underlying algorithm (tradeoff of tombstones and fast insert/remove).

So first question: what is the current strategy for Text/Ordered Lists? OT? RGA? This code for example:
https://github.com/automerge/automerge/blob/master/frontend/apply_patch.js#L317-L383

Follow up question: I've been using the LSEQ CRDT in collaborative frontend experiments for a few years (like Google Docs). It's a proper CRDT with some amazing performance properties, especially for large documents (keeping memory usage lower than W00T/Logoot or tombstone strategies, but with O(log(n)) for insertion/removal.

The implementation I've been using supports the same insertAt/removeAt operations, so I'm thinking it might be straightforward to experiment with an alternative merge strategy.

What are your thoughts? Any caveats or recommendations on how I could take this for a spin?
I am trying to utilize the libraries undo functionality, however, `Automerge.canUndo` is returning true when called immediately after the initialization of my document.
```
    this.actionDoc = Automerge.from({
      clips: initialClips,
    })
    Automerge.canUndo(this.actionDoc)
```

Displaying the history of the document immediately after initialization only shows the "Initialization" action. Yet, canUndo still returns true.

My question is why it makes sense to be able to undo the populating of the initial state? Is there anyway to change the undo threshold?
## Steps to reproduce

1. Insert some characters into a Text in document 1
2. Sync the changes to document 2
3. Delete the characters in document 2
4. Undo the insert in document 1
5. Undo the delete in document 2
6. Merge the changes

## Expected result
Since the edit that inserted the characters was undone, you would expect the inserted characters to be missing from the end result.

## Actual result
The characters are still present in the end result.

Example code:

```js
// 0. Setup
var base = Automerge.init();

base = Automerge.change(base, 'Base text', doc => {
  doc.text = new Automerge.Text();
  doc.text.insertAt(0, 'B', 'A', 'S', 'E');
});

var doc1 = Automerge.init();
doc1 = Automerge.merge(doc1, base);

var doc2 = Automerge.init();
doc2 = Automerge.merge(doc2, base);

// 1. Insert some characters into a Text in document 1
doc1 = Automerge.change(doc1, 'Insert', doc => {
  doc.text.insertAt(2, 'i', 'n', 's', 'e', 'r', 't');
});

// 2. Sync the changes to document 2
doc2 = Automerge.merge(doc2, doc1);

// 3. Delete the characters in document 2
doc2 = Automerge.change(doc2, 'Delete', doc => {
  doc.text.deleteAt(2, 6);
});

// 4. Undo the insert in document 1
doc1 = Automerge.undo(doc1, 'Undo insert');

// 5. Undo the delete in document 2
doc2 = Automerge.undo(doc2, 'Undo delete');

// 6. Merge the changes
var final = Automerge.merge(doc1, doc2);

// Expected: "BASE". Actual: "BAinsertSE"
console.log(final.text.toString());
```
There's `Automerge.getConflicts(doc, key)`, but this means I already need to know the conflicting key. It would be nice to have for example `Automerge.getConflicts(doc)`, which returns `['x']` or even `{x: {'actor-1': 12}}`, so one can detect those conflicts and surface them to the user.
1. Moves all initialization tests to an 'initialization' `describe` block directly under 'Automerge' 
2. Adds tests for passing different types of values to `Automerge.from`: an array, a string, a number, and a boolean. 
While looking through previous issues and pull requests, I’ve noticed a few mentions of a desire to move from vector clocks towards hash chaining for encoding the dependency graph (#27, #28, #31, #36). After reading through these issues and pull requests, there are a number of questions that come to mind:
* Is this still something that is desired? (If not, then the rest of the questions below become irrelevant!)
* Would it be okay if I started working towards adding this functionality?
    * It seems like adding hash-based integrity checking (#27) would be a good place to start, with full hash chaining to follow. Does this make sense?
    * Are there any backwards-compatibility concerns that would need to be kept in mind?
* What are the expected benefits of using hash chaining over vector clocks? A few potential benefits that jump to mind:
    * Dependency encoding becomes a fixed size for each change, rather than the `O(# of peers)` size that comes with vector clocks (though that’s only observed in the worst case, given the recent change to remove transitive dependencies from changes)
    * I _think_ we wouldn’t need to store the `states` object any more, instead we would only need to store a collection of each hash and its parent(s)
    * I _think_ we wouldn’t need to store `clock` or `deps` objects any more, instead we would only need to store a set of the most recently observed concurrent hashes
    * Any more?
* What are the expected drawbacks of using hash chaining over vector clocks?
  * One of the issues (#31) mentions that we would need to use a multi-round protocol to determine the latest common ancestor between peers
  * Any others?
This is related to #193, where I encounter a need for splitting text segments without making delete, insert route. As I was trying to find a solution I came across to [RGASplit paper](https://pages.lip6.fr/Marc.Shapiro/papers/rgasplit-group2016-11.pdf) that happens to be foundation for [xi editor](https://xi-editor.io/docs/crdt-details.html), [atom's collaborative editing feature](https://github.com/atom/teletype-crdt) and their [next generation code editor](https://github.com/atom/xray#text-is-stored-in-a-copy-on-write-crdt).

I think copy-on-write CRDT approach not only has promising performance improvements, but also might offer some good solution for Rich Text modeling due to introduced split operation. I imagine text blocks could be extended with additional metadata where formatting information could be stored and with that it would be possible to represent Rich Text without resorting to marker tokens as described in #193
This is related to #193 as I attempt to model rich text as series of tokens of characters and markers:

```js
{format:{italic:true}}Hello {format:{bold:true, italic:true}}beautiful{format:null} world\n
```

As I was trying to figure out a best approach to this I noticed that `Automerge.Text` would happily insert objects e.g. `text.insertAt(0, { bold: true })` which than would appear as `objectId`s inside the text. ([related thread on slack](https://automerge.slack.com/archives/C61RJCM9S/p1563228180398800)) It's also worth mentioning that `Automerge.getObjectById(doc, objectId)` would actually return inserted object.

I'm not sure what is the intended behavior here, but I suspect that `text.join('')` including `objectId` is highly unlikely. It seemed like a convenient accident for my use case, but I'm suspicious there might be deeper consequences so I chose to use list (or should I say array) of tokens instead. Either way I would like to propose:

1. To document how `Automerge.Text` outperforms arrays (as suggested by the readme).
2. Either throw if non-text is being inserted or omit `objectId`s in `.join()` and document non textual inserts bit more.