Hi, 

First of all: I am not a researcher, just a programmer who is assigned to do something with CoSMoMVPA.

I have the most recent version of Matlab (R2019a) installed. As mentioned in the title a couple of deprecated SVM functions are removed. This issue is mentioned In the `cosmo_classify_matlabsvm_2class.m` function. But in `cosmo_check_external.m` Matlab is checked for the deprecated versions (see sub-function `get_externals_helper`).

The result is that I am forced to install `libsvm`. We would like to avoid that. And you might want to update your code for Matlab 2019. 

Thanks for looking into this.

cheers

Casper
Strangely enough, as of July 2019 surface files GIFTI files written with cosmo_map2surface cannot be read by AFNI SUMA. Todo: figure out if this is due to changes in AFNI SUMA, the GIFTI toolbox, CoSMoMVPA, or an interaction.
Uses a oneliner instead of a loop
Per https://github.com/CoSMoMVPA/CoSMoMVPA/pull/182

Error was:


failure: zstream.c not compiled - see Makefile
  zstream:29 (/home/travis/build/nno/gifti/@gifti/private/zstream.m)
  save>save_gii:226 (/home/travis/build/nno/gifti/@gifti/save.m)
  save:43 (/home/travis/build/nno/gifti/@gifti/save.m)
  cosmo_map2surface>write_gifti:221 (/home/travis/build/nno/CoSMoMVPA/mvpa/cosmo_map2surface.m)
  cosmo_map2surface:122 (/home/travis/build/nno/CoSMoMVPA/mvpa/cosmo_map2surface.m)
  test_surface_io>save_and_load:155 (/home/travis/build/nno/CoSMoMVPA/tests/test_surface_io.m)
  test_surface_io>test_surface_dataset_gifti:16 (/home/travis/build/nno/CoSMoMVPA/tests/test_surface_io.m)


I have a question on how to load null_data when using the montecarlo algorithm. 

Right now I have 1 actual accuracy map as well as 100 null maps from each participant, and 7 participants in total. If I use the null maps as null data, should I create a 1*100 cell, with each cell containing a 7-volume dataset (one null map from each participant)? The null maps are in .nii.gz format, so I will need to load in the null maps using cosmo_fmri_dataset, and then store the output struct to the null_data cell, right?

Thank you,
Yuqi
I get this error after our IT staff upgraded Matlab to 2018 from 2016. My guess is it is has to do with a change in the Matlab library but not sure.

Error using matlab.graphics.axis.Axes/set
 Error setting property 'CLim' of class 'Axes':
 Value must be a 1x2 vector of numeric type in which the second element is larger than
 the first and may be Inf

Error in imagesc (line 47)
   set(cax,'CLim',clim)

Error in cosmo_plot_slices (line 111)
         imagesc(slice, [mn, mx]);

Error in Stats_LDASVMsearchlight_WB (line 91)
     cosmo_plot_slices(stat_map_z)
  

To avoid code duplication for functionality currently used in ``cosmo_crossvalidate``, it may be good to have a separate function for preprocessing (see #102). Some general ideas and notes of existing functionality:

- Generally preprocessing involves two steps: (1) estimating parameters on a training dataset; (2) applying the parameters to a test dataset. Currently we already have ``cosmo_map_pca`` and ``cosmo_normalize`` as preprocessing functions, which can do these two steps through different call syntax: ``cosmo_map_pca`` uses 'pca_params' as an option to indicate that parameters should be applied to the input dataset; in ``cosmo_normalize`` the type of second argument (string or struct) indicates whether parameters are estimated or applied, respectively.

- Another sort of preprocessing function is ``cosmo_average_samples``, which computes averages over subsets of samples. Currently this function is only used for training datasets. Unless ``cosmo_map_pca`` and ``cosmo_normalize``, ``cosmo_average_samples`` returns an output different in size from the input.

- It would be good to support preprocessing functions to be chain-able, where the output from one function forms the input of the next. This is currently possible with ``cosmo_map_pca and cosmo_normalize``.

- apart from functions currently discussed at PR #102, integration with ``cosmo_naive_bayes_classifier_searchlight`` would also be useful.

- for efficiency reasons it would be good if this function would support numeric matrices with samples directly. It can actually be debated whether it should also support dataset struct input. If yes, then it becomes more of a general purpose function; if no, a low-level helper function.

Questions that may merit discussion are:

- Function signature(s). Some options are:

  * two separate functions, say ``preprocess_train`` and ``preprocess_test``. 
  * one function, distinguished by keyword differences, say ``preprocess(ds_train, 'normalization', 'zscore')`` and ``preprocess(ds_test,'model',model_struct)`` (as in ``cosmo_map_pca``)
  * one function, distinguished by argument order, say ``preprocess(ds_train, 'normalization', 'zscore')`` and ``preprocess(model_struct,ds_test)

  In all cases it seems reasonable that the output should consist of the samples after preprocessing, and as second output the model.

- Once decided about function signature, maybe adjust ``cosmo_map_pca`` and ``cosmo_normalize`` to support such signatures as well? 

- Support multiple preprocessing functions. It would seem reasonable to hard-code the functions to be used for preprocessing, i.e. ``cosmo_map_pca`` and ``cosmo_normalize``. However it would be nice to write the code in such a way that future normalisation options can be added easily. What would be the best way to approach this? 





Classifiers return their decision values too. Add functions (cosmo_measure) to correlate decision values to external vector

As suggested by Brendon Ritchie, it would be useful to provide support for linear discriminant contrast analysis [see Walther et al 2016, "Reliability of dissimilarity measures for multi-voxel pattern analysis", http://www.diedrichsenlab.org/pubs/Walther_Neuroimage_2016.pdf].

I would like to start with a proposal here to add such functionality, and would welcome feedback and comments:
- introduce  a new measure, for example named `cosmo_ldc_measure`.
- as `cosmo_crossvalidation_measure`, it would require an option `partitions` which is used for crossvalidation. Multiple folds are supported.
- There as an `output` option that can be set to rdm_per_fold, rdm, average_rdm, or average_rdm_per_fold. RDMs are represented as a column vector with P_(P-1)/2 values (P=number of classes), with .sa.targets1 and .sa.targets2 set appropriately as in `cosmo_dissimilarity_matrix_measure`. When using ``__per_fold``, then output is produced for each fold and a field .sa.folds is added, otherwise it is averaged over folds. 

Other points for discussion:
- This proposal does currently not involve integration with `cosmo_target_dsm_corr_measure`; such functionality would be nice to add in the future though. 
- It may be useful to have a lightweight `cosmo_ldc_pdist`function that simply computes the pairwise distances between two matrices with patterns. This function can be called by `cosmo_ldc_measure`. This function can be tested separately.
- maybe add an option to output `LDt` values (t-values)

Feedback much appreciated.

don't forget to use annotated or signed tags ;)
