Hi,
I am trying to load redis data of the form (the hash keys don't have a fixed value):
`hset id_1 feature_1 value_1
 hset id_1 feature_2 value_2
 hset id_2 feature_2 value_2
 hset id_3 feature_3 value_3
`
to a dataframe with the schema (id, feature,value) but the method fromRedisHash("*") returns only the hash:
![image](https://user-images.githubusercontent.com/56862768/72358725-b03e9400-36f5-11ea-905a-85baeb2ba163.png)

Is there a way to retrieve the context? assuming I don't want to use the dataframe approach because it will require more memory (i.e hset id_3 feature feature_3 value value_3)
I initialized spark session like this

```scala
val spark = SparkSession
      .builder()
      .appName("ExtractRawTablesFromMySQL")
      .config("spark.redis.host", "my-redis.url.com")
      .config("spark.redis.port", "6379")
      .getOrCreate()
```

Later when I want to save rdd to redis

```scala
sc.toRedisSET(rdd, "tags:1")
```

But it says **"Failed connecting to host localhost:6379"**. 

Looks like it is not honoring this configuration:

```scala
.config("spark.redis.host", "my-redis.url.com")
```

Error stack:

```
redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool
  at redis.clients.jedis.util.Pool.getResource(Pool.java:59)
  at redis.clients.jedis.JedisPool.getResource(JedisPool.java:234)
  at com.redislabs.provider.redis.ConnectionPool$.connect(ConnectionPool.scala:33)
  at com.redislabs.provider.redis.RedisEndpoint.connect(RedisConfig.scala:69)
  at com.redislabs.provider.redis.RedisConfig.clusterEnabled(RedisConfig.scala:182)
  at com.redislabs.provider.redis.RedisConfig.getNodes(RedisConfig.scala:293)
  at com.redislabs.provider.redis.RedisConfig.getHosts(RedisConfig.scala:209)
  at com.redislabs.provider.redis.RedisConfig.<init>(RedisConfig.scala:132)
  at com.redislabs.provider.redis.RedisConfig$.fromSparkConf(RedisConfig.scala:120)
  at com.redislabs.provider.redis.RedisContext.toRedisSET$default$4(redisFunctions.scala:285)
  ... 75 elided
Caused by: redis.clients.jedis.exceptions.JedisConnectionException: Failed connecting to host localhost:6379
  at redis.clients.jedis.Connection.connect(Connection.java:204)
  at redis.clients.jedis.BinaryClient.connect(BinaryClient.java:100)
  at redis.clients.jedis.BinaryJedis.connect(BinaryJedis.java:1862)
  at redis.clients.jedis.JedisFactory.makeObject(JedisFactory.java:117)
  at org.apache.commons.pool2.impl.GenericObjectPool.create(GenericObjectPool.java:819)
  at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:429)
  at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:360)
  at redis.clients.jedis.util.Pool.getResource(Pool.java:50)
  ... 84 more
Caused by: java.net.ConnectException: Connection refused (Connection refused)
  at java.net.PlainSocketImpl.socketConnect(Native Method)
  at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
  at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
  at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
  at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
  at java.net.Socket.connect(Socket.java:589)
  at redis.clients.jedis.Connection.connect(Connection.java:181)
  ... 91 more
```
In the doc, val listRDD = sc.fromRedisList("keyPattern*")  can get Lists in redis, but it get RDD[string] which contains lists only. How to get the corresponding listname of every row of the listRDD?
I cannot understand from issues that were raised if spark-redis supports Sentinel? I couldn't manage to write Dataframe to Redis running with Sentinel. Please confirm that currently lib doesn't provide that functionality.
Hi,

Instead of passing the Redis credential in the spark session creation moment, Is it possible to pass the info at reading or writing time?

Something like:
```
conf = df.write \
            .option("url", self.url) \
            .option("user", self.user) \
            .option("password", self.password).save()
```

I have a PySpark DataFrame with 2 columns [key, bz2], in which "key" is string and "bz2" is bytes (BinaryType). I want to write this dataframe into Redis, then my teammates will read them out using Node API.

I tried ways like
(1) df.write.format("org.apache.spark.sql.redis").option("table", "PP").option("key.column", "key").option("host", REDIS_HOST).option("port", REDIS_PORT).option("auth", REDIS_PW).option("timeout", 100000).mode("append").save()
(2) df.write.format("org.apache.spark.sql.redis").option("table", "PP").option("key.column", "key").option("host", REDIS_HOST).option("port", REDIS_PORT).option("auth", REDIS_PW).option("timeout", 100000).mode("append").option("model", "binary").save()

The 1st method will write only a short and incorrect sequence into Redis, will lose a lot of information.
For the 2nd method, we don't know how to decode the persisted bytes with other languages like Node. The "binary" persistence mode seems only work with Spark-Redis library.

So, is it possible to write a byte array DataFrame column to Redis? How to do that?

Please advise. Thanks a lot.
Spark version: 2.2.1
Spark Redis version: 2.4.0
Spark Streaming: 2.2.1

scalaVersion : 2.11

I have a requirement to use Redis SETNX command in my Spark job. But in RedisContext there is no such a method to support SETNX.
hi,
I am writing a PoC to test spark-redis integration.
Poc Setup
spark-core_2.11:2.4.1
spark-sql_2.11:2.4.1
spark-redis:2.4.0
redis:5.0.5  (standalone)


My poc read data sets from csv files and write to redis. Later, read data set back through spark-redis library. There are 7 data sets write to spark. size varies from 40 to 700k. In total, 1.1 million entries are written to redis.
//Write
ds.write().format("org.apache.spark.sql.redis").option("table", table).option("key.column", keyColumn).mode(SaveMode.Overwrite).save()
//Read
spark.read().format("org.apache.spark.sql.redis").option("table", "FX").option("key.column", "CurrencyID").load()

I found reading from redis is very slow even for a small data sets.  Rewrite above same data sets is also slow. The library spent a lot of time "Computing partition". e.g. it took 9 mins to calculate 3 partitions. 

Can you please suggest if there is way to tune the performance?

thanks
I am using the example provided in the Java docs and running this on a local spark cluster.

```
public void run() throws Exception {
    SparkSession sparkSession = SparkSession.builder()
        .master("local")
        .config("spark.redis.host", redisHost)
        .config("spark.redis.port", redisPort)
        .config("spark.redis.db", dbName)
        .getOrCreate();

    Dataset<Row> df = sparkSession.createDataFrame(Arrays.asList(
        new Person("John", 35),
        new Person("Peter", 40)), Person.class);

    df.write()
        .format("org.apache.spark.sql.redis")
        .option("table", "person")
        .option("key.column", "name")
        .mode(SaveMode.Overwrite)
        .save();

   sparkSession.stop();
}
```

This writes to redis successfully but does not actually complete the spark job and call the shutdown hooks. This stops at:

> 19/09/19 17:21:02 INFO o.a.s.SparkContext: Successfully stopped SparkContext

I am ideally, expecting this job to `complete` so that I can proceed with my DAG, with:

> 19/09/19 17:24:57 INFO o.a.s.u.ShutdownHookManager: Shutdown hook called
> 19/09/19 17:24:57 INFO o.a.s.u.ShutdownHookManager: Deleting directory /private/var/folders/kk/gtt0h1mx46s78vy_wsfdv6r00000gp/T/spark-e4fe17c4-8342-4788-a78a-4fdf73ed2da3

Using Spark-redis version: `2.4`
Hello!

I am using Spark Redis to populate a Redis cluster as follows -- 

  ```
df.sample(sampleRate)
        .write
        .mode(SaveMode.Append)
        .format("org.apache.spark.sql.redis")
        .option("table", table)
        .option("key.column", column)
        .option("ttl", ttl)
        .save()
```

The Redis cluster itself is hosted on the Kubernetes Engine with a load balancer service and the individual nodes can down and come back during the load process.

If one of the nodes goes down, I see an error like -- 
`Caused by: redis.clients.jedis.exceptions.JedisConnectionException: Failed connecting to host 10.4.1.162:6379`

When this happens, the whole job terminates.  Instead, I would like to ignore this exception and try to continue loading. 

Is there a way to achieve that? 

Regards

