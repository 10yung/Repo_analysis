**Describe the bug**
Issue in skipRows with header=true strictColumnHandling: true,maxRows:some_number

**Parsing or Formatting?**

 - [ ] Formatting
 - [Parsing ] Parsing 

**To Reproduce**
Steps to reproduce the behavior:
```js
const csv = require('@fast-csv/parse');
	const rows = [
    'header1,header2\n',
    'col1\n',
    'col2,col2\n',
    'col3\n',
    'col4,col4\n',
    'col5,col5\n',
    'col6,col6\n',
];
const stream = csv
		.parse({ headers: true, skipRows: 2 ,strictColumnHandling: true,maxRows:4
		})
		.on('error', error => console.error(error))
		.on('data-invalid', (row, rowNumber) => {
			console.log(`Invalid [rowNumber=${rowNumber}] [row=${JSON.stringify(row)}]`);
		})
		 .on('data', row => console.log(`valid ${JSON.stringify(row)}`))
		.on('end', rowCount => console.log(`Parsed ${rowCount} rows`));
	rows.forEach(row => stream.write(row));
	stream.end();
```
**Expected behavior**
i am accepting the output like this
Invalid [rowNumber=1] [row=["col3"]]
valid {"header1":"col4","header2":"col4"}
valid {"header1":"col5","header2":"col5"}
valid {"header1":"col6","header2":"col6"}
Parsed 4 rows 

but actual output given is
  Invalid [rowNumber=1] [row=["col1"]]
  Invalid [rowNumber=2] [row=["col3"]]
  valid {"header1":"col4","header2":"col4"}
  valid {"header1":"col5","header2":"col5"}
  Parsed 4 rows

**Desktop (please complete the following information):**
 - OS: [WINDOW 10]
 - OS Version [e.g. Mojave]
 - Node Version [9.3.0]

**Additional context**
i am executing your code of skip_rows.examples.js with some modifications strictColumnHandling: true and maxRows:some_number. i am doing wrong or it's there an issue
v.3.5.0 introduced a breaking change ..
import csv from 'fast-csv';
...
       const parser = csv
        .parseStream(stream, {
            headers : true,
            encoding: 'utf-8',
        })
cannot read property 'parseStream' of undefined
I think your documentation should include that headers are handled **case insensitive** when specifying them as an **array**, however, if not specified as an array, they are handled strictly case sensitive.

This had me confused for a few hours.

Also, when specifying headers as an array, if they are not in the correct order or have columns missing, no error is given and no data is processed. An error only appears when too many headers is present, not too little. I think this should be mentioned in the docs as the only thing mentioned is "You may alternatively pass an array of header names which must match the order of each column in the CSV, otherwise the data columns will not match." , however I think it should read " otherwise the data columns will not match and NO DATA WILL BE IMPORTED".

Let me know if you agree, thx !
I'm exporting some data from an SQL database and, while LibreOffice Calc can import the data just fine, fast-csv fails.

For example, one line is exported as CSV like this

"123";"Item Name";"{"bgColor":"#888888","fgColor":"#000000"}"

and the parser fails as it thinks that the field is `"{"` and expects the next char to be a semi-colon. The parser should continue until another delimiter is matched, then see if all escape chars are even. If the escape chars are odd, then the delimiter is supposed to be part of the field value and the parser keeps on going.

Test cases should ensure that

```
"hello";"world"     => [ "hello", "world" ]
"hel;lo";"world"    => [ "hel;lo", "world" ]     (delimiter within odd escapes)
"hel";"lo";"world"  => [ "hel", "lo" ]
"hel"lo";"world"    => [ "hel\"lo", "world" ]    (next char is not a delimiter)
"hel""lo";"world"   => [ "hel\"\"lo", "world" ]  (...)
```

The condition should be

```
if next char is escape then increment escape count and buffer
else if next char is delimiter then
  if escape count is odd then increment buffer
  else set next field value and reset buffer
else increment buffer
```
