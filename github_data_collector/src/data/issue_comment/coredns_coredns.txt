The acl plugin has metrics (there is a `metrics.go`), but it doesn't describe them in the README.
<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via security@coredns.io
-->

**What happened**:
I keep seeing warning messages although logs is configured for `error` only. Here is an example: 

```
[WARNING] No files matching import glob pattern: custom/*.override
[WARNING] No files matching import glob pattern: custom/*.server
```

**What you expected to happen**:
No warning messages should be shown if the log plugin is in `error` mode. 

**Environment**:

- the version of CoreDNS: coredns:1.6.3
- Corefile: `".:8053 { \n    errors\n    log . {\n        class error\n    }\n    health\n
    \   ready\n    kubernetes cluster.local in-addr.arpa ip6.arpa {\n        pods
    insecure\n        upstream\n        fallthrough in-addr.arpa ip6.arpa\n        ttl
    30\n    }\n    prometheus 0.0.0.0:9153\n    forward . /etc/resolv.conf\n    cache
    30\n    loop\n    reload\n    loadbalance round_robin\n    import custom/*.override\n}\nimport
    custom/*.server"`
- Others: kubernetes version 1.16, 1.15, 1.14 

I have installed a k8s cluster on a physical machine. Bute the log of coredns pod show "Readiness probe failed: Get http://10.233.65.11:8181/ready: dial tcp 10.233.65.11:8181: connect: connection refused".who can help me?
my coredns version is 1.4
and here is my config
```
  Corefile: |
    .:53 {
        errors
        health
        kubernetes cluster.local. in-addr.arpa ip6.arpa {
            pods insecure
            upstream
            fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        proxy . /etc/resolv.conf
        cache 30
        loop
        reload
        loadbalance
    }
```
can anybody tell me the difference between upstream and proxy 

This issue is to keep track of 1.6.7 release. Specifically we want to add the releasing support for mips64le arch https://github.com/coredns/coredns/pull/3589.

<!--
Thank you for contributing to CoreDNS!
Please provide the following information to help us make the most of your pull request:
-->

### 1. Why is this pull request needed and what does it do?
 Although the connection between coredns and apiserver is broken, but the HasSynced () method  of reflector will not report the error.

If this is "connection refused" error, it means that most likely apiserver is not responsive.
It doesn't make sense to re-list all objects because most likely we will be able to restart watch where we ended.
If that's the case wait and resend watch request.

### 2. Which issues (if any) are related?
https://github.com/coredns/coredns/issues/3587
### 3. Which documentation changes (if any) need to be made?
none
### 4. Does this introduce a backward incompatible change or deprecation?
none
**What happened**:
I have 5 coredns pod running about 2 months, then I found 2 of them can not lookup new services created by k8s.
```
~/allen# kubectl -n core-dns-wlcb-prod get svc,po -o wide
NAME           TYPE        CLUSTER-IP   EXTERNAL-IP     PORT(S)                   AGE       SELECTOR
svc/core-dns   ClusterIP   172.29.8.8   192.168.2.184   49153/TCP,53/UDP,53/TCP   88d       name=core-dns

NAME                                 READY     STATUS    RESTARTS   AGE       IP              NODE
po/core-dns-bb32dbc62e244ff-6zzb5    1/1       Running   1          67d       172.16.84.14    192.168.2.41
po/core-dns-bb32dbc62e244ff-89rdx    1/1       Running   1          67d       172.16.117.10   192.168.2.203
po/core-dns-bb32dbc62e244ff-pbfqm    1/1       Running   1          67d       172.16.84.15    192.168.2.41
po/core-dns-bb32dbc62e244ff-rfqbt    1/1       Running   1          67d       172.16.202.12   192.168.2.79
po/core-dns-bb32dbc62e244ff-vx5dx    1/1       Running   1          67d       172.16.83.20    192.168.2.184
```

the problem pod (172.16.83.20) 
```
root@dns-check-e5b984de89ac472-dd98j:/# dig @172.16.83.20 core.titan-dev-open.svc.cluster.local

; <<>> DiG 9.10.3-P4-Ubuntu <<>> @172.16.83.20 core.titan-dev-open.svc.cluster.local
; (1 server found)
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NXDOMAIN, id: 23894
;; flags: qr aa rd; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 1
;; WARNING: recursion requested but not available

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;core.titan-dev-open.svc.cluster.local. IN A

;; AUTHORITY SECTION:
cluster.local.          5       IN      SOA     ns.dns.cluster.local. hostmaster.cluster.local. 1578707575 7200 1800 86400 5

;; Query time: 0 msec
;; SERVER: 172.16.83.20#53(172.16.83.20)
;; WHEN: Sat Jan 11 15:38:37 CST 2020
;; MSG SIZE  rcvd: 159
```

pod(172.16.202.12) works well
```
root@dns-check-e5b984de89ac472-dd98j:/# dig @172.16.202.12 core.titan-dev-open.svc.cluster.local

; <<>> DiG 9.10.3-P4-Ubuntu <<>> @172.16.202.12 core.titan-dev-open.svc.cluster.local
; (1 server found)
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 55436
;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1
;; WARNING: recursion requested but not available

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;core.titan-dev-open.svc.cluster.local. IN A

;; ANSWER SECTION:
core.titan-dev-open.svc.cluster.local. 5 IN A   172.29.82.96

;; Query time: 1 msec
;; SERVER: 172.16.202.12#53(172.16.202.12)
;; WHEN: Sat Jan 11 15:38:57 CST 2020
;; MSG SIZE  rcvd: 119
```

the bad pod (172.16.83.20) is not always bad, it can resolve kubernetes.default and other services created before. However it can not resolve new created k8s service. So, it seems there is some thing wrong for syncing machinism. 

I got so many domain resolve logs but no helpful logs of k8s related.


**What you expected to happen**:
- 1. coredns worker normally, can syncing all changes from k8s
- 2. and I also want to know how I can get coredns logs about k8s resource syncing?

**How to reproduce it (as minimally and precisely as possible)**:
I think restart pod could resolve the problem, but still not find the steps to reproduce

**Anything else we need to know?**:

**Environment**:

- the version of CoreDNS: 1.6.4
- Corefile:
```
.:53 {
        errors
        health
        kubernetes cluster.local in-addr.arpa ip6.arpa {
          endpoint http://xxxx.k8s.com:8080
          pods insecure
          upstream
          fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        forward . 10.1.1.1 {
          max_fails 2
          expire 10s
          policy sequential
          health_check 1s          
        }
        log {
          class denial
          class error
        }
        cache 30
        loop
        reload
        loadbalance
    }

```
- logs, if applicable: 
- OS (e.g: `cat /etc/os-release`): Ubuntu 16.04
- Others:

**What happened**:
`serve_stale` does not update NXDOMAIN status if it gets constantly hammered by requests.

**What you expected to happen**:
CoreDNS updates the record once an upstream DNS starts to return an A record after returning NXDOMAIN.

**How to reproduce it (as minimally and precisely as possible)**:
Begin the exercise by repeatedly hammering a CoreDNS instance with requests to the non-existent domain. It correctly returns NXDOMAIN.
```bash
while sleep 0.5; do dig test.default.svc.cluster.local @169.254.20.10; done
```

Create an appropriate (I've just created a Service in Kubernetes) A record on the upstream DNS. Verify:
```
$ dig test.default.svc.cluster.local. @192.168.0.10 +short
10.10.50.53
```

The aforementioned `while` loop will return NXDOMAIN indefinitely.

Removing the `serve_stale` option alleviates the issue.

**Anything else we need to know?**:

Notice, that these tests are performed not against the primary CoreDNS of a Kubernetes cluster, but against a secondary one that forwards requests to the primary (a node-level caching mechanism).

**Environment**:

- the version of CoreDNS: 1.6.6
- Corefile:
```
.:53 {
  errors {
    consolidate 10s ".* i/o timeout$"
    consolidate 10s ".* write: operation not permitted$"
  }
  cache {
    success 39936
    denial 9984
    prefetch 10 1m 25%
    serve_stale
  }
  reload 2s
  loop
  bind 192.168.0.10 169.254.20.10
  forward . 192.168.0.10 192.168.0.10 192.168.0.10 {
    max_fails 0
  }
  prometheus 127.0.0.1:9254
  health 127.0.0.1:9225
}
```
- logs, if applicable:
- OS (e.g: `cat /etc/os-release`):
```
NAME="Ubuntu"
VERSION="18.04.3 LTS (Bionic Beaver)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 18.04.3 LTS"
VERSION_ID="18.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic
```
- Others:

Signed-off-by: zouyee <zounyee1989@gmail.com>

<!--
Thank you for contributing to CoreDNS!
Please provide the following information to help us make the most of your pull request:
-->

### 1. Why is this pull request needed and what does it do?
To optimize the monitoring test, we should not simply confirm the existence of metrics, but compare the results of metrics deeply.

### 2. Which issues (if any) are related?
https://github.com/coredns/coredns/issues/3579

### 3. Which documentation changes (if any) need to be made?
NONE

### 4. Does this introduce a backward incompatible change or deprecation?
NONE
<!-- Please only use this template for submitting enhancement requests -->

**What would you like to be added**:

 compare the results of metrics deeply.

**Why is this needed**:
To optimize the monitoring test, we should not simply confirm the existence of metrics, but compare the results of metrics deeply.
