How we can grab the hostname of the server on which the container is runnig and send this hostname to the logs agregator: syslog-ng, logstash etc.?

Something we got when we use:
`docker service logs SERVICE|TASK`
And the output is:
```
node_app_1.7.zyp3bh58k6dn@SRVNAME     | app listening on port 80
node_app_1.7.zyp3bh58k6dn@SRVNAME     | queryDb took 35 ms ("select * from visits limit 1")
node_app_1.7.zyp3bh58k6dn@SRVNAME     | db error Error: read ECONNRESET
node_app_1.7.zyp3bh58k6dn@SRVNAME     |     at TCP.onStreamRead (internal/stream_base_commons.js:201:27) {
node_app_1.7.zyp3bh58k6dn@SRVNAME     |   errno: 'ECONNRESET',
node_app_1.7.zyp3bh58k6dn@SRVNAME     |   code: 'ECONNRESET',
node_app_1.7.zyp3bh58k6dn@SRVNAME     |   syscall: 'read',
node_app_1.7.zyp3bh58k6dn@SRVNAME     |   fatal: true
node_app_1.7.zyp3bh58k6dn@SRVNAME     | }
node_app_1.7.zyp3bh58k6dn@SRVNAME     | /usr/src/app/src/db-mysql.js:139
node_app_1.7.zyp3bh58k6dn@SRVNAME     |         throw err;
node_app_1.7.zyp3bh58k6dn@SRVNAME     |         ^
node_app_1.7.zyp3bh58k6dn@SRVNAME     | 
node_app_1.7.zyp3bh58k6dn@SRVNAME     | Error: read ECONNRESET
node_app_1.7.zyp3bh58k6dn@SRVNAME     |     at TCP.onStreamRead (internal/stream_base_commons.js:201:27) {
node_app_1.7.zyp3bh58k6dn@SRVNAME     |   errno: 'ECONNRESET',
node_app_1.7.zyp3bh58k6dn@SRVNAME     |   code: 'ECONNRESET',
node_app_1.7.zyp3bh58k6dn@SRVNAME     |   syscall: 'read',
node_app_1.7.zyp3bh58k6dn@SRVNAME     |   fatal: true
node_app_1.7.zyp3bh58k6dn@SRVNAME     | }
```
Hi,

I have recently set up a logspout container:

<pre>
version: '2.3'
services:
  logspout:
    image: gliderlabs/logspout/v3.2.6
    restart: unless-stopped
    command: 'udp://10.132.15.196:5514'
    mem_limit: 64m
    memswap_limit: 64m
    cpus: 0.5
    environment:
      - "RAW_FORMAT={\"type\" : \"docker\", \"hostname\" : \"fqdn\", \"containerName\" : {{toJSON .Container.Name}}, \"labels\": {{ toJSON .Container.Config.Labels }}, \"message\" : {{ toJSON .Data }}  }" 
      - BACKLOG=false
      - ROUTE_URIS=udp://10.132.15.196:5514
    volumes:
      - '/var/run/docker.sock:/tmp/docker.sock'

</pre>

This works perfectly, unless you configure disk block limits on another docker(-compose) container.
https://docs.docker.com/compose/compose-file/#blkio_config

<pre>
    blkio_config:
      device_read_bps:
      - path: /dev/sdb
        rate: '1mb'
      device_read_iops:
      - path: /dev/sdb
        rate: 50
      device_write_bps:
      - path: /dev/sdb
        rate: '1mb'
      device_write_iops:
      - path: /dev/sdb
        rate: 50
</pre>

If you start logspout, something like this happens:

<pre>

logspout_1  | 2020/01/10 12:30:58 # logspout v3.2.6 by gliderlabs
logspout_1  | 2020/01/10 12:30:58 # adapters: udp multiline raw syslog tcp tls
logspout_1  | 2020/01/10 12:30:58 # options : 
logspout_1  | 2020/01/10 12:30:58 backlog:false
logspout_1  | 2020/01/10 12:30:58 persist:/mnt/routes
logspout_1  | 2020/01/10 12:30:58 # jobs    : pump routes http[health,logs,routes]:80
logspout_1  | 2020/01/10 12:30:58 # routes  :
logspout_1  | #   ADAPTER	ADDRESS			CONTAINERS	SOURCES	OPTIONS
logspout_1  | #   raw+udp	10.132.15.196:5514				map[]
logspout_1  | 2020/01/10 12:31:00 pump: json: cannot unmarshal number into Go struct field BlockLimit.Rate of type string

</pre>

Thanks in advance, much appreciated.

Kind Regards,

M.


Hi, I've commented to related PR chat but couldn't get an answer so I'm asking from there.
https://github.com/gliderlabs/logspout/pull/401#issuecomment-544856706

Since log messages doesn't includes miliseconds in timestamp fields, observing exceptions is so hard. Is there a parameter or something for getting timestamps from docker logs. It really hurts us and feels weird that only a few person talks about this issue, am I missing something?
If a container was started by a swarm service, this additionally checks the service labels in the MatchContainer function. It also looks at service labels when deciding whether to ignore a container based on labels. This approach doesn't require any special configuration, it should just work seamlessly whether containers are started individually or as part of a service or stack.
I've been evaluating logspout and it has been meeting all my needs until now. I'm able to use filter.labels to only include logs from containers with certain labels. My next step was to try this out in swarm mode. It seems that labels in swarm mode apply to the service, not individual containers. I can inspect containers created by the `docker stack deploy` command, and they don't have the labels defined in my yml file. These are the labels I had been using in the filter.labels parameter. Those labels DO exist when I inspect the service created by `docker stack deploy`. Since my individual containers don't have these labels, I get not logs through logspout. If I remove the filter.labels parameter, I get logs from my containers, but the log entries don't include my custom labels either, only the labels the stack applies to the container.

Labels for a container running as part of a stack:

```json
"Labels": {
  "com.docker.stack.namespace": "logspout",
  "com.docker.swarm.node.id": "r01ryq9ozir0ngzezpzyib4tm",
  "com.docker.swarm.service.id": "jvbmw62dkxk6oclt343aeg9ro",
  "com.docker.swarm.service.name": "logspout_logspout",
  "com.docker.swarm.task": "",
  "com.docker.swarm.task.id": "pg18eb97smkts4uaaozqkfq59",
  "com.docker.swarm.task.name": "logspout_logspout.1.pg18eb97smkts4uaaozqkfq59"
}
```

And labels for the service, which has the id in the `com.docker.swarm.service.id` label from above:

```json
[
    {
        "ID": "jvbmw62dkxk6oclt343aeg9ro",
        "Version": {
            "Index": 1378
        },
        "CreatedAt": "2019-08-14T02:58:29.023517774Z",
        "UpdatedAt": "2019-08-14T02:58:29.027497629Z",
        "Spec": {
            "Name": "logspout_logspout",
            "Labels": {
                "com.docker.stack.image": "logspout-file:latest",
                "com.docker.stack.namespace": "logspout",
                "mylabel": "works",
                "myotherlabel": "really"
            }
...
```

A log entry from a (different) container showing just the `com.docker.stack` and `com.docker.swarm` labels:

```json
{
  "container": "/log-test_echo.1.w78hfexf4l4g3lqmas90lrheo",
  "labels": {
    "com.docker.stack.namespace": "log-test",
    "com.docker.swarm.node.id": "r01ryq9ozir0ngzezpzyib4tm",
    "com.docker.swarm.service.id": "b5giiu2exm91gb2bclro3udle",
    "com.docker.swarm.service.name": "log-test_echo",
    "com.docker.swarm.task": "",
    "com.docker.swarm.task.id": "w78hfexf4l4g3lqmas90lrheo",
    "com.docker.swarm.task.name": "log-test_echo.1.w78hfexf4l4g3lqmas90lrheo"
  },
  "timestamp": "2019-08-14T03:46:18Z",
  "source": "stdout",
  "line": "log"
}
```

Is it possible to check labels for a given service, and filter container logs accordingly? I know that traefik relies heavily on labels, and it's able to detect when a service starts up with given labels. I know it's a different project, and different authors, but it's a similar situation in terms of dealing with swarm mode. Perhaps specifically telling logspout it is operating in a swarm so it knows to check for services instead of just containers? I would take a crack at it but I haven't wrapped my head around the code enough to know where to start.
The documentation actually is amazing, but for me (maybe for someone else) it's not clear: can I use the rotating logs setup with the logspout?

For example, I have the following command for starting some docker container:

```
docker run --log-opt max-size=10m --log-opt max-file=5 ...
```

And the Logstash command is:

```
docker run -d --name=logspout\
 --volume=/var/run/docker.sock:/var/run/docker.sock\
 gliderlabs/logspout\
 tcp://address:ip
```

As I know, the docker rotating logs mechanism will remove the "old-logs", so the total size of all logfiles will remain constant. Is this not a problem for Logspout? Can logspout correctly process multiple log files?
the docker template tags for swarm and compose are fairly different. it took my quite a while to figure out.

the source documentation is here - https://docs.docker.com/engine/reference/commandline/service_create/#create-services-using-templates

a working, reasonable logspout configuration for swarm is 
```
  logspout:
    image: gliderlabs/logspout:latest
    networks:
      - logging
    volumes:
      - /etc/hostname:/etc/host_hostname:ro
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - SYSLOG_TAG="{{index .Service.Name}}"
      - SYSLOG_HOSTNAME="{{index .Node.Hostname}}"
      - INACTIVITY_TIMEOUT=1m
      - BACKLOG=false
      - SYSLOG_STRUCTURED_DATA="logdna@48950 key=\"12345\" tag=\"swarm,stacks\""
      - LOGSPOUT=ignore
    command:
      multiline+syslog+tls://syslog-a.logdna.com:44533
    deploy:
      mode: global
      resources:
        limits:
          cpus: '0.20'
          memory: 50M
        reservations:
          cpus: '0.10'
          memory: 50M```
Please update the README.md to include ROUTE_URIS in the environment variables and an invocation example.

I only came across this with a search and found #217 and #90  my use case is logspout in a docker stack.
Documentation states: 
```
Logspout relies on the Docker API to retrieve container logs. A failure in the API may cause a log 
stream to hang. Logspout can detect and restart inactive Docker log streams. Use the environment 
variable INACTIVITY_TIMEOUT to enable this feature. E.g.: INACTIVITY_TIMEOUT=1m for a 1-minute 
threshold.

```

- how does exactly detect the stream to hang? 
- how does this interact with docker logging option "mode: non-blocking" ? 
- what exactly restart? `can detect and restart inactive Docker log streams`