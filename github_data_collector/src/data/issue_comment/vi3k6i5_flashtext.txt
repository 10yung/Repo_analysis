Any possibility to dump the KeywordProcessor object??
Or are pickle or dill modules enough for the job?

Thanks!
In order to use in languages other than English (Chinese or Japanese with tokenized), it is now possible to select words to be separated by word boundaries instead of non-wordboundary when separating words .
Hi,

 I would like to request for a simple handy feature that will extract the nearby context of the word extracted .

Suppose, 


```
keyword_processor.add_keyword('Python')
keyword_processor.extract_keywords(" Respected sir, My Name is Akhil and I love doing python programming. I am from India completed my masters degree in CSE")
>>  [Akhil , Python]  or [My Name is Akhil and I love doing python programming]
```
hi i am from belong we have one use case that is we need to store additional meta data along with 
 sibling.
{"java": [{ "name": "xyz",
                "meta": {
                               "count":10,
                               "score":0.24
                            }
              },
             { "name : "abc",
                "meta": {
                             "count :12,
                             "score": 0.5
                             }
            }]

so now if sentence is like this "i know xyz"
it should return  - ["java": { "meta": {
                                                   "count: 10,
                                                   "score":0.24
                                                }
                                  }]

so if this look like generic one can we implement to the flash text library.


Thanks

Regards
Krishna


for example i have a string : "todayIgotEmailreport"
how do i get email keyword from this string ??
`if i use str.contains('report,False,regex=True)`
this will return this string.
how can we do it with flashtext? 

Hi,

This is not an issue. Just a question to understand if the package is feasible to solve the issue I'm facing. 

There are 40k words (which has unique id's) and 40k sentences. If any of these words are found in any of the sentences, I have to replace the word with the word and its id. The catch is we need to retain the same case as in the sentence.

For e.g. Word is "Information Security"
The sentence is "information security is also called InfoSec."

The expected output is "information security/1 is also called InfoSec."

Can this scenario be handled with the help of this package? By looping through all the sentences?

Thanks!

It would be good to have a count getter to obtain the number of keywords processed by. 

Using the processor to identify the presence of the keywords, but still in need to split the source and remove any delimiter doesn't sound efficient to me.

Looking at the class methods, I couldn't get anything and the count method returns the count of occurrences found.
Hi,

I´m loading 4,6 million keywords + their replacements into flashtext.

The raw data in a pandas dataframe consumes approx. 1 GB RAM, profiled with pd.DataFrame.memory_usage(True, True) and guppy).

When I load this data to flashtext, the algorithm consumes 70 GB RAM.

My question is if this in intentionally and why the algorithm uses that much ram for a small dataset?
Did I do something wrong or is this correct?

I´m also curious if there is a "Cloud native" way to use this algorithm? (Having the Data  of the algorithm swapped out to a fast in-memory Database) to have a stateless algorithm where kubernets pods can be transfered/restarted withou a long waiting period or a huge memory footprint.

Thanks in advance.

Best regards

Florian