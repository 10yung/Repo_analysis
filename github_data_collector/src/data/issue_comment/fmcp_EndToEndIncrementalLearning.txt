@fmcp I was going through your paper and am a little bit confused with the idea of distillation loss. More specifically, I am assuming that the ground-truth probability distribution of each sample is a one-hot vector. This means that the distillation loss effectively contains only one of the classes since all other classes would have p<sub>i</sub>=0. So, I don't see any point of raising the probabilities by **1/T**, since it effectively only scales the loss term by a factor of **T** linearly (loss = -log q<sub>i</sub><sup>1/T</sup> = -1/T*log q<sub>i</sub>, where i the index of the ground truth class).

Also, coming to q<sub>i</sub>, I am assuming that by logits you mean the normalized softmax probabilities over the total classes (old + new).
When I try running the script 'experiment_imagenet.m' with all the requiremetns satisfied it throws the following error. I don't see the file in the repo as well. Maybe the authors **forgot to include those in the repo.**. Thanks in advance for the help.

> Undefined function or variable 'fc_buildExemplarsSetImagenet'.
> 
> Error in experiment_imagenet (line 66)
>                     exemplars = fc_buildExemplarsSetImagenet([], exemplars, opts);
Hi, there. After generating the imdbs using buildImdbsImageNet.m, I tried to use the "res_imagenet" function in ResNet-Matconvnet to train the first model on ImageNet, but I had trouble loading the first imdb using this function. "opts.imdb" seems the only related parameter I can change to point to the path of the first imdb (res_imagenet.m:14), but it doesn't work when I set it. Could you help me with it?
![](https://ask.qcloudimg.com/draft/1215004/pzvms5gfpi.png)
After read your paper, I still don't understand what the classification layers mean? Do they mean different fully connect layers?
Thank you for sharing the code. I wonder the detail of obtaining the first (ResNet) model for the first 10 classes in CIFAR100. I tried to reimplement your code with Pytorch. However, even at the first step, the performance is only 85% (averaged on many random splits, with your augmentation strategy), but yours is nearly 90%. It is said that the first model is trained using ResNet-Matconvnet. Do you used any pretrained model (by finetuning) when train the first model?
@fmcp  Thanks for the help before on the cifar dataset.

Now I am trying to use the code to reproduce the imagenet results. I notice that in the readme.md, it mentioned that:
```
For ImageNet, you only have to change the number of classes and relative paths to the dataset.
```

According to the  4 steps under the USAGE, I guess the first step I have to split the data first and then train a first model using the ResNet-Matconvnet (https://github.com/zhanghang1989/ResNet-Matconvnet).

In cifar, the `build_imdbs.m` loads all data into memory and then splits the data. I am wondering what's gonna to be load for the ImageNet data. 

Thanks for the help and looking forward to the reply. 


Can you provide the learning rate of training the first net model of each incremental step on cifar-100？

can you provide some source code for the step of getting the first model for each step size and iteration？

