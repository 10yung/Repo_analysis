## Description
Remove usage of GORPOXY let 'go' decide the defaults

## Motivation and Context
custom envs are not needed

## How to test this PR?
nothing special to test

## Types of changes
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)

## Checklist:
- [ ] Fixes a regression (If yes, please add `commit-id` or `PR #` here)
- [ ] Documentation needed
- [ ] Unit tests needed
- [ ] Functional tests needed (If yes, add [mint](https://github.com/minio/mint) PR # here: )

<!--- Provide a general summary of the issue in the Title above -->

## Expected Behavior
<!--- If you're describing a bug, tell us what should happen -->
<!--- If you're suggesting a change/improvement, tell us how it should work -->
Prometeus  monitoring stats of disk usage stats show ```/opt/data/51``` disk usage, not ```/```
```disk_storage_used{disk="/opt/data/51"} 1.755652096e+09```
```
[root@51 ~]# du -hs /opt/data/51
108K	/opt/data/51
```
## Current Behavior
<!--- If describing a bug, tell us what happens instead of the expected behavior -->
<!--- If suggesting a change/improvement, explain the difference from current behavior -->
Wrong monitoring stats of disk usage. Prometeus monitoring stats show ```/``` disk usage
```disk_storage_used{disk="/opt/data/51"} 1.755652096e+09```
```
[root@51 ~]# df -h
Файловая система        Размер Использовано  Дост Использовано% Cмонтировано в
devtmpfs                  223M            0  223M            0% /dev
tmpfs                     235M            0  235M            0% /dev/shm
tmpfs                     235M         9,6M  225M            5% /run
tmpfs                     235M            0  235M            0% /sys/fs/cgroup
/dev/mapper/centos-root    47G         1,7G   46G            4% /
/dev/sda1                1014M         197M  818M           20% /boot
tmpfs                      47M            0   47M            0% /run/user/0
```
## Possible Solution
<!--- Not obligatory, but suggest a fix/reason for the bug, -->
<!--- or ideas how to implement the addition or change -->
Change code to display  stats shown in paramert ```disk_storage_used{disk="/opt/data/51"} 1.755652096e+09``` not ```/```
## Steps to Reproduce (for bugs)
<!--- Provide a link to a live example, or an unambiguous set of steps to -->
<!--- reproduce this bug. Include code to reproduce, if relevant -->
1. Run minio distributed server via reverse proxy nginx  [install docs](https://docs.min.io/docs/setup-nginx-proxy-with-minio.html)
2. Take a look ``` df -h``` and  ``` du -hs /opt/data/51 ```
3. Take a look  prometeus stats ```disk_storage_used{disk="/opt/data/51"} 1.755652096e+09```
4.

## Context
<!--- How has this issue affected you? What are you trying to accomplish? -->
<!--- Providing context helps us come up with a solution that is most useful in the real world -->

## Regression
<!-- Is this issue a regression? (Yes / No) -->
<!-- If Yes, optionally please include minio version or commit id or PR# that caused this regression, if you have these details. -->

## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in -->
* Version used (`minio version`): RELEASE.2020-01-13T22-49-03Z
* Environment name and version (e.g. nginx 1.9.1): nginx version: openresty/1.15.8.2
* Server type and version: VM, Centos 7
* Operating System and version (`uname -a`): 3.10.0-1062.7.1.el7.x86_64
* Link to your project:

## Description

For 'snapshot' type profiles, record a 'before' profile that can be used as `go tool pprof -base=before ...` to compare before and after.

"Before" profiles are included in the zipped package.

[`runtime.MemProfileRate`](https://golang.org/pkg/runtime/#pkg-variables) should not be updated while the application is running, so we set it at startup.

Adds `threadcreate` as `threads` profiling as well.

## Motivation and Context

Reliable benchmarking tools.


## How to test this PR?

```
$ warp mixed -obj.randsize -serverprof=mem,cpu,block,mutex -duration=1m
[...]
warp: Profile data successfully downloaded as warp-mixed-2020-01-18[134149]-HCA2.profiles.zip
[...]

λ unzip warp-mixed-2020-01-18[134149]-HCA2.profiles.zip
Archive:  warp-mixed-2020-01-18[134149]-HCA2.profiles.zip
 extracting: profiling-127.0.0.1_9000-mutex-before.pprof
 extracting: profiling-127.0.0.1_9000-mem.pprof
 extracting: profiling-127.0.0.1_9000-mem-before.pprof
 extracting: profiling-127.0.0.1_9000-block.pprof
 extracting: profiling-127.0.0.1_9000-block-before.pprof
 extracting: profiling-127.0.0.1_9000-cpu.pprof
 extracting: profiling-127.0.0.1_9000-mutex.pprof

λ go tool pprof -base=profiling-127.0.0.1_9000-mutex-before.pprof profiling-127.0.0.1_9000-mutex.pprof
Type: delay
Time: Jan 18, 2020 at 1:41pm (CET)
Entering interactive mode (type "help" for commands, "o" for options)
(pprof) web

```

## Types of changes
- [x] New feature (non-breaking change which adds functionality)

## Description
fix: close and drain the response body always

## Motivation and Context
Fix close and drain of response body across the codebase.

## How to test this PR?
Allows for proper keep-alive behavior in net/http clients, observed with
liveness checks hitting gateway backends

## Types of changes
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)

## Checklist:
- [ ] Fixes a regression (If yes, please add `commit-id` or `PR #` here)
- [ ] Documentation needed
- [ ] Unit tests needed
- [ ] Functional tests needed (If yes, add [mint](https://github.com/minio/mint) PR # here: )

I'm running minio (in server mode) on an edge server for local object storage.
Every 12h when disk usage is crawled this results in a peak of CPU/memory usage.
Since there are quite a few other things running on this edge node, this breaks my use case.

It was working fine with version RELEASE.2019-09-26T19-42-35Z

Related:
* documentation about this: #8722
* rss increase: #8664

## Expected Behavior
<!--- If you're describing a bug, tell us what should happen -->
<!--- If you're suggesting a change/improvement, tell us how it should work -->
Disk usage calculation does NOT create high peaks in CPU/memory usage.

## Current Behavior
<!--- If describing a bug, tell us what happens instead of the expected behavior -->
<!--- If suggesting a change/improvement, explain the difference from current behavior -->
Disk usage calculation creates high peaks in CPU/memory usage.

## Possible Solution
<!--- Not obligatory, but suggest a fix/reason for the bug, -->
<!--- or ideas how to implement the addition or change -->
Disk usage calculation should either run non-aggressively in the background (probably preferred) or an option provided to turn this feature off.

## Steps to Reproduce (for bugs)
<!--- Provide a link to a live example, or an unambiguous set of steps to -->
<!--- reproduce this bug. Include code to reproduce, if relevant -->
1. run minio server
2. observe the high CPU/memory usage every 12h

## Context
<!--- How has this issue affected you? What are you trying to accomplish? -->
<!--- Providing context helps us come up with a solution that is most useful in the real world -->
This prevents me from using minio for local object storage on our edge nodes as these high peaks degrade the performance of the other processes running on the same edge node too much.

## Regression
<!-- Is this issue a regression? (Yes / No) -->
<!-- If Yes, optionally please include minio version or commit id or PR# that caused this regression, if you have these details. -->
For my use-case this is a clear regression.

## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in -->
* Version used (`minio version`): `RELEASE.2020-01-16T22-40-29Z`
* Environment name and version (e.g. nginx 1.9.1):
* Server type and version: various smaller x86 edge nodes
* Operating System and version (`uname -a`): Linux
* Link to your project:

If i remove a user from a group, the user is not removed at all nodes in the cluster.
i remove the user with the command "mc admin group remove" from a windows 10 client. the server runs under ubuntu 18.04

command to remove
mc admin group remove minio mygroup myuser

command to check
mc admin user info minio myuser

Here an example out put:

```
D:\MinIo>mc admin user info exanic developer
AccessKey: developer
Status: enabled
PolicyName:
MemberOf: heidak-dev-write,newhome-build-write,newhome-dev-write,newhome-integration-write

D:\MinIo>mc admin user info exanic developer
AccessKey: developer
Status: enabled
PolicyName:
MemberOf: heidak-dev-write,newhome-build-write,newhome-dev-write

D:\MinIo>mc admin user info exanic developer
AccessKey: developer
Status: enabled
PolicyName:
MemberOf: heidak-dev-write,newhome-build-write,newhome-dev-write,newhome-integration-write

D:\MinIo>mc admin user info exanic developer
AccessKey: developer
Status: enabled
PolicyName:
MemberOf: heidak-dev-write,newhome-build-write,newhome-dev-write,newhome-integration-write

D:\MinIo>mc admin user info exanic developer
AccessKey: developer
Status: enabled
PolicyName:
MemberOf: heidak-dev-write,newhome-dev-write,newhome-integration-write,newhome-staging-write

D:\MinIo>mc admin user info exanic developer
AccessKey: developer
Status: enabled
PolicyName:
MemberOf: heidak-dev-write,newhome-build-write,newhome-dev-write,newhome-integration-write,newhome-staging-write
```
The commands were carried out one after the other

EDIT:  The Cluster is behind a loadbalancer ... So you can see many different nodes

## Expected Behavior
the user should removed from the group at all nodes inside the cluster

## Current Behavior
it looks like, that the user is only remove at the node it the command executed. In the logs at the server i cant found any exception

## Steps to Reproduce (for bugs)
1. Create a user and a group
2. Add the user to the group
3. Remove the user from the group
4. Check if the user is removed

## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in -->
* Version used (`minio version`): 2020-01-16T22:40:29Z
* Environment name and version (e.g. nginx 1.9.1): 
* Server type and version: ubuntu 18.04 lts
* Operating System and version (`uname -a`): Linux vstorage01 4.15.0-62-generic #69-Ubuntu SMP Wed Sep 4 20:55:53 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux


For the building my Minio cluster (Minio version RELEASE.2020-01-03T19-12-21Z) I used 2 servers - **server1** and **server2** (Debian stretch)
On each server, I have 2 instances of Mino (in the docker containers. Docker version 5:19.03) every instance has one HDD disk (7200RPM) (`/dev/sdc` and `/dev/sdd` && `xfs fielsystem`)
i.e.
**server1:**
_instance1_ (mounted to /home/data1) && _instance2_ (mounted to /home/data2)
**server2:**
_instance1_ (mounted to /home/data1) && _instance2_ (mounted to /home/data2)

I had a fat directory (~100GB+) with millions of files inside which I decided to delete (from the Minio Web interface).
- After deleting I looked at the graphics and found that disks utilization was 100% on both servers (**server1**, **server2**)
- So I decided to turn off my cluster (each node), but after turning back (turn on) my cluster I saw that my disks operations didn't normalize.
- Then I decided to turn off one of my instances (**server1** _instance1_) for minimizing disks operations and CPU WA, and it a little bit helped.
- I waited for a while and when I look that I/O operations for working instance come to normal working I tuned on my second instance and I got the same behavior as in the beginning both disks were eaten.
- Now I turn off my **server1** _instance1_ and enable **server1** _instance2_ (just for test and what I saw - the disk was eaten =) ) (on the graphics, you can see two disks blue and purple and how the load is growing when one or another instance is turning on or off)

I illustrate the graphics of one of the server **server1:**
![disks_operations](https://user-images.githubusercontent.com/11527999/72549289-790ce600-38a1-11ea-9428-20005ffab06b.png)

![cpu_usage](https://user-images.githubusercontent.com/11527999/72549299-7ca06d00-38a1-11ea-9e7b-fe6f7dc495cc.png)

- For the debugging, I dug `strace` output for each instance. 
I found the name of the biggest directory that I deleted before (the directory with millions of files) it has the name `pr.ru`
Then I count for every process `openat` files from the trace command. The `strace` worked 15 seconds for every instance.
_I disabled one of the instances for the comparing._

`server1 _instance1_`
```
DISABLED (for the test)
```

`server1 _instance2_`
```
# strace -f -p 29056 2>&1 |grep openat |grep pr.ru > /tmp/pr1.txt
# cat /tmp/pr1.txt |wc -l
3131

```
`server2 _instance1_`
```
# strace -f -p 27881 2>&1 |grep openat |grep pr.ru > /tmp/pr1.txt
#  cat /tmp/pr1.txt |wc -l
2257

```

`server2 _instance2_`
```
# strace -f -p 26817 2>&1 |grep openat |grep pr.ru > /tmp/pr2.txt
# cat /tmp/pr2.txt |wc -l
1113
```
The questions are:
- Is it normal behavior for the Minio cluster?
- Why did it happen just after deleting, but before it works perfectly (some kind of indexing? But why for all instances and so aggressive?)
- Can I minimize the IO operations?
## Expected Behavior
When I use following YAMLs in our OpenShift cluster, I would expect that I get a working minio cluster:
https://github.com/minio/minio/blob/master/docs/orchestration/kubernetes/minio-distributed-headless-service.yaml?raw=true
https://github.com/minio/minio/blob/master/docs/orchestration/kubernetes/minio-distributed-statefulset.yaml?raw=true
https://github.com/minio/minio/blob/master/docs/orchestration/kubernetes/minio-distributed-service.yaml?raw=true

## Current Behavior
On start I get following error:

ERROR Invalid command line arguments: path '/data' can not be served by different port on same address
    HINT:
        For more information, please refer to https://docs.min.io/docs/minio-erasure-code-quickstart-guide



## Steps to Reproduce (for bugs)
Use the 3 yamls mentioned above on OpenShift v3.10


## Your Environment
* Created my own minio Dockerimage, based on rhel:7

## Description
This PR introduces a form of caching for `b2_list_buckets` calls, clearing the cache and retrying once on any errors thrown, removing the need for this to happen on every request. In the event that the cached bucket information is invalid, the next request will throw an error, which can be caught silently, the bucket cache updated, and the request repeated once.

This is an non-breaking change and should not affect users in any way other than saving costs and increasing performance.

Resolves  #8739 and #6806.

## Motivation and Context
The constant bucket listing calls slow down B2 integration and cost users. Largely eliminating these requests is anticipated to save significant amounts of money and time for users making many requests.

## How to test this PR?
Use the gateway and monitor the number of `b2_list_buckets` calls during testing, and benchmark the performance. Use a number of random requests against random objects to encourage invalid bucket errors and ensure the expected silent recovery takes place.

## Types of changes
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)

## Checklist:
- [ ] Fixes a regression (If yes, please add `commit-id` or `PR #` here)
- [ ] Documentation needed
- [ ] Unit tests needed
- [ ] Functional tests needed (If yes, add [mint](https://github.com/minio/mint) PR # here: )

I'm trying to develop an app that needs to have an object store, and am using minio inside minikube to help me develop it. Uploading files to minio seems to work with port-forward, but the upload gets stuck after transferring 100% of the files when using an ingress

## Expected Behavior
After starting minikube, creating a pod for minio, and creating an ingress, logging in through a browser, creating a bucket, and adding a file to that bucket will work.

## Current Behavior
When adding a file, the transfer seems to succeed and reaches "100%". However, the blue "uploading" message remains; closing it will create a warning that this will abort the upload, and aborting it shows that no objects are created.

## Steps to Reproduce (for bugs)
1. Install minikube
2. Run `minikube start`
3. After that finished, create the following YAML file, and apply it with `kubectl apply -f`:

```yaml
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: minio
spec:
  replicas: 1
  selector:
    matchLabels:
      app: minio
  serviceName: "minio"
  template:
    metadata:
      labels:
        app: minio
    spec:
      containers:
      - name: minio
        image: minio/minio
        ports:
        - containerPort: 9000
          name: minio
        args:
        - server
        - "/data"
        env:
        - name: MINIO_ACCESS_KEY
          value: minio
        - name: MINIO_SECRET_KEY
          value: foobar
---
apiVersion: v1
kind: Service
metadata:
  name: minio
  labels:
    app: minio
spec:
  ports:
  - port: 9000
    name: minio
  clusterIP: None
  selector:
    app: minio
---
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: minio
spec:
  rules:
  - host: minio.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: minio
          servicePort: 9000
```
4. Create a `/etc/hosts` entry for `minio.example.com` that points to whatever `minikube ip` points to
5. Open web browser, and enter `minio.example.com` as URL
6. Log in, add a bucket
7. Try to upload a file to that bucket (fails)
8. Run `kubectl port-forward svc/minio 9000`
9. Go back to browser, enter `localhost:9000` as URL
10. Log in, add a file to the bucket created in step 6 (succeeds)

(note: yaml file does not create any volumes for brevity, but it fails with a volume too)

## Your Environment
* Version used (`minio version`): `2020-01-03T19:12:21Z`; runtime `Version: go1.13.5 | CPUs: 4`
* Environment name and version (e.g. nginx 1.9.1): docker file, inside kubernetes
