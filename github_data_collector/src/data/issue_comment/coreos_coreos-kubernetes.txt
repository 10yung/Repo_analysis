I tried to spin up a new single-node cluster but it fails prematurely within the cloudinit unit with the error:

```
● coreos-cloudinit-791973373.service - Unit generated and executed by coreos-cloudinit on behalf of user
   Loaded: loaded (/run/systemd/transient/coreos-cloudinit-791973373.service; transient)
Transient: yes
   Active: failed (Result: exit-code) since Tue 2019-10-22 17:29:20 UTC; 14min ago
  Process: 1318 ExecStart=/bin/bash /var/lib/coreos-cloudinit/scripts/791973373 (code=exited, status=1/FAILURE)
 Main PID: 1318 (code=exited, status=1/FAILURE)

Oct 22 17:29:20 localhost systemd[1]: Started Unit generated and executed by coreos-cloudinit on behalf of user.
Oct 22 17:29:20 localhost bash[1318]: Failed to enable unit: Unit file etcd2.service does not exist.
Oct 22 17:29:20 localhost systemd[1]: coreos-cloudinit-791973373.service: Main process exited, code=exited, status=1/FAILURE
Oct 22 17:29:20 localhost systemd[1]: coreos-cloudinit-791973373.service: Failed with result 'exit-code'.
```

I followed the instructions at: https://coreos.com/kubernetes/docs/1.6.1/kubernetes-on-vagrant-single.html

And also the multi-node seam to have the same problem:
e1:
```
● user-cloudinit@var-lib-coreos\x2dvagrant-vagrantfile\x2duser\x2ddata.service - Load cloud-config from /var/lib/coreos-vagrant/vagrantfile-user-data
   Loaded: loaded (/usr/lib/systemd/system/user-cloudinit@.service; static; vendor preset: disabled)
   Active: failed (Result: exit-code) since Tue 2019-10-22 18:05:00 UTC; 11min ago
  Process: 1112 ExecStart=/usr/bin/coreos-cloudinit --from-file=/var/lib/coreos-vagrant/vagrantfile-user-data (code=exited, status=1/FAILURE)
 Main PID: 1112 (code=exited, status=1/FAILURE)

Oct 22 18:04:59 e1 coreos-cloudinit[1112]: 2019/10/22 18:04:59 Wrote drop-in unit "20-cloudinit.conf"
Oct 22 18:04:59 e1 coreos-cloudinit[1112]: 2019/10/22 18:04:59 Ensuring runtime unit file "etcd2.service" is unmasked
Oct 22 18:04:59 e1 coreos-cloudinit[1112]: 2019/10/22 18:04:59 Ensuring runtime unit file "fleet.service" is unmasked
Oct 22 18:04:59 e1 coreos-cloudinit[1112]: 2019/10/22 18:04:59 Ensuring runtime unit file "locksmithd.service" is unmasked
Oct 22 18:04:59 e1 coreos-cloudinit[1112]: 2019/10/22 18:04:59 Masking unit file "locksmithd.service"
Oct 22 18:05:00 e1 coreos-cloudinit[1112]: 2019/10/22 18:05:00 Calling unit command "start" on "etcd2.service"
Oct 22 18:05:00 e1 coreos-cloudinit[1112]: 2019/10/22 18:05:00 Failed to apply cloud-config: Unit etcd2.service not found.
Oct 22 18:05:00 e1 systemd[1]: user-cloudinit@var-lib-coreos\x2dvagrant-vagrantfile\x2duser\x2ddata.service: Main process exited, code=exited, status=1/FAILURE
Oct 22 18:05:00 e1 systemd[1]: user-cloudinit@var-lib-coreos\x2dvagrant-vagrantfile\x2duser\x2ddata.service: Failed with result 'exit-code'.
Oct 22 18:05:00 e1 systemd[1]: Failed to start Load cloud-config from /var/lib/coreos-vagrant/vagrantfile-user-data.
```

And all worker nodes:
```
● coreos-cloudinit-362193141.service - Unit generated and executed by coreos-cloudinit on behalf of user
   Loaded: loaded (/run/systemd/transient/coreos-cloudinit-362193141.service; transient)
Transient: yes
   Active: failed (Result: exit-code) since Tue 2019-10-22 18:11:19 UTC; 7min ago
  Process: 1308 ExecStart=/bin/bash /var/lib/coreos-cloudinit/scripts/362193141 (code=exited, status=1/FAILURE)
 Main PID: 1308 (code=exited, status=1/FAILURE)

Oct 22 18:06:22 w1 bash[1308]: TEMPLATE: /etc/systemd/system/flanneld.service.d/40-ExecStartPre-symlink.conf.conf
Oct 22 18:06:22 w1 bash[1308]: TEMPLATE: /etc/systemd/system/docker.service.d/40-flannel.conf
Oct 22 18:06:22 w1 bash[1308]: TEMPLATE: /etc/kubernetes/cni/docker_opts_cni.env
Oct 22 18:06:22 w1 bash[1308]: TEMPLATE: /etc/kubernetes/cni/net.d/10-flannel.conf
Oct 22 18:06:22 w1 bash[1308]: Created symlink /etc/systemd/system/update-engine.service → /dev/null.
Oct 22 18:06:23 w1 bash[1308]: Created symlink /etc/systemd/system/multi-user.target.wants/flanneld.service → /usr/lib/systemd/system/flanneld.service.
Oct 22 18:11:19 w1 bash[1308]: Job for flanneld.service failed because a timeout was exceeded.
Oct 22 18:11:19 w1 bash[1308]: See "systemctl status flanneld.service" and "journalctl -xe" for details.
Oct 22 18:11:19 w1 systemd[1]: coreos-cloudinit-362193141.service: Main process exited, code=exited, status=1/FAILURE
Oct 22 18:11:19 w1 systemd[1]: coreos-cloudinit-362193141.service: Failed with result 'exit-code'.

● update-engine.service
   Loaded: masked (Reason: Unit update-engine.service is masked.)
   Active: failed (Result: exit-code) since Tue 2019-10-22 18:06:22 UTC; 11min ago
 Main PID: 699 (code=exited, status=1/FAILURE)

Oct 22 18:06:12 localhost systemd[1]: Starting Update Engine...
Oct 22 18:06:13 localhost update_engine[699]: I1022 18:06:13.402832   699 main.cc:89] CoreOS Update Engine starting
Oct 22 18:06:13 localhost systemd[1]: Started Update Engine.
Oct 22 18:06:13 localhost update_engine[699]: I1022 18:06:13.429944   699 update_check_scheduler.cc:74] Next update check in 6m10s
Oct 22 18:06:22 w1 systemd[1]: Stopping Update Engine...
Oct 22 18:06:22 w1 systemd[1]: update-engine.service: Main process exited, code=exited, status=1/FAILURE
Oct 22 18:06:22 w1 systemd[1]: update-engine.service: Failed with result 'exit-code'.
Oct 22 18:06:22 w1 systemd[1]: Stopped Update Engine.
```
Not sure where to put this issue, but the latest stable version did seem to have some changes as our network devices were renamed which broke BGP networking.

https://github.com/coreos/fedora-coreos-tracker/issues/173


Will there be a new hyperkube 1.10.11 version image build? the last version is 1.10.5.

Hello,

I'm mounting a host volume into the kubelet rkt container via

```
  --volume srv,kind=host,source=/srv \
  --mount volume=srv,target=/srv \
```

Below `/srv` I mount some volumes on the host via systemd units, e.g.

```
j1900 core # systemctl cat srv-bitbucketPGData.mount
# /etc/systemd/system/srv-bitbucketPGData.mount
[Unit]
Before=local-fs.target
[Mount]
What=/dev/data/lvol2
Where=/srv/bitbucketPGData
Type=xfs
[Install]
WantedBy=local-fs.target
```

This worked fine up until a reboot, since then the mounts below `/srv` are not visible inside the rkt container anymore:

```
j1900 core # rkt enter c0 /bin/mount|grep /srv/bitbucketPGData
```

vs
```
j1900 core # mount|grep /srv/bitbucketPGData
/dev/mapper/data-lvol2 on /srv/bitbucketPGData type xfs (rw,relatime,seclabel,attr2,inode64,noquota)
```

Neither restarting the node again nor restarting the kubelet has solved the issue. The only workaround I've found so far was to remount the respective mounts after the kubelet was started which IMHO is a terrible "solution" as this means the Kubelet already has started the containers that use those mounts
I try to access to my deployment but can't reach NodePort net.

kubectl get ep

```
NAME            ENDPOINTS                          AGE
dark-room-dep   172.17.0.10:8085,172.17.0.9:8085   19h
kubernetes      10.66.222.223:6443                 8d
```

kubectl get svc

```
NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
dark-room-dep   NodePort    10.99.12.214   <none>        8085:30991/TCP   19h
kubernetes      ClusterIP   10.96.0.1      <none>        443/TCP          8d
```

kubectl cluster-info

```
Kubernetes master is running at https://10.66.222.223:6443
Heapster is running at https://10.66.222.223:6443/api/v1/namespaces/kube-system/services/heapster/proxy
KubeDNS is running at https://10.66.222.223:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
monitoring-grafana is running at https://10.66.222.223:6443/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy
monitoring-influxdb is running at https://10.66.222.223:6443/api/v1/namespaces/kube-system/services/monitoring-influxdb/proxy

```
kubectl get deployment

```
NAME            DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
dark-room-dep   2         2         2            2           20h
```

kubectl get pods --all-namespaces

```
NAMESPACE     NAME                                            READY     STATUS    RESTARTS   AGE
default       dark-room-dep-577bf64bb8-9n5p7                  1/1       Running   0          20h
default       dark-room-dep-577bf64bb8-jmppg                  1/1       Running   0          20h
kube-system   etcd-localhost.localdomain                      1/1       Running   6          8d
kube-system   heapster-69b5d4974d-qvtrj                       1/1       Running   0          1d
kube-system   kube-apiserver-localhost.localdomain            1/1       Running   5          8d
kube-system   kube-controller-manager-localhost.localdomain   1/1       Running   4          8d
kube-system   kube-dns-86f4d74b45-njzj9                       3/3       Running   0          1d
kube-system   kube-flannel-ds-h9c2m                           1/1       Running   3          6d
kube-system   kube-flannel-ds-tcbd7                           1/1       Running   5          8d
kube-system   kube-proxy-7v6mf                                1/1       Running   3          6d
kube-system   kube-proxy-hwbwl                                1/1       Running   4          8d
kube-system   kube-scheduler-localhost.localdomain            1/1       Running   6          8d
kube-system   kubernetes-dashboard-7d5dcdb6d9-q42q5           1/1       Running   0          1d
kube-system   monitoring-grafana-69df66f668-zf2kc             1/1       Running   0          1d
kube-system   monitoring-influxdb-78d4c6f5b6-nhdbx            1/1       Running   0          1d
```


curl 10.99.12.214:30991

`curl: (7) Failed connect to 10.99.12.214:30991; Aucun chemin d'accès pour atteindre l'hôte cible`


Hi,
I am following this [guide ](https://coreos.com/kubernetes/docs/1.6.1/deploy-master.html#create-the-kubelet-unit) to deploy a kubernetes cluster on CoreOS with 1 master and 3 worker nodes. I have 2 issues:

1) I wanted to assign host names to my k8s Nodes instead on IPs as name so I assigned the [--hostname-override](https://coreos.com/kubernetes/docs/1.6.1/deploy-master.html#create-the-kubelet-unit) to string names like "k8s-master-1", "k8s-worker1" etc. 
Everything worked fine but when I deployed the calico.yaml, the calico-node containers showed the error of not resolve adress for https://k8s-master-1:10250/containerLogs/kube-system/calico-node-jsfj6/calico-node , because it started to use the hostname instead of IP. What could be the solution for this, I mean can I specify it to use IP instead of hostname or does it only uses the hostname?

2) The calico-node containers on worker nodes were in CrashLoopBackOff state so I thought that this is the issue and I changed back the --hostname-override to IP address, and restarted. But the  calico-node containers on worker nodes are stll in CrashLoopBackOff state. The calico-node container on master node is working fine. 
The logs of calico-node containers on worker nodes:

```
$ kubectl logs calico-node-bvzf7 -n kube-system calico-node
Waiting for etcd connection...
No IP provided. Using detected IP: 10.0.2.15
ERROR: IP address 10.0.2.15 is already in use by host w1k8s. Calico requires each compute host to have a unique IP. If this is your first time running the Calico node on this host, ensure that another host is not already using the same IP address.
Calico node failed to start

$ kubectl logs calico-node-fxxsz -n kube-system calico-node
Waiting for etcd connection...
No IP provided. Using detected IP: 10.0.2.15
ERROR: IP address 10.0.2.15 is already in use by host w1k8s. Calico requires each compute host to have a unique IP. If this is your first time running the Calico node on this host, ensure that another host is not already using the same IP address.
Calico node failed to start
```
Kindly tell me what could be the issue or how can I debug it to find the root cause?
Environment:
coreos iso beta (1662.2.0)
```bash
Container Linux by CoreOS beta (1662.2.0)
core@k8s-master ~ $ /usr/bin/kubelet
-bash: /usr/bin/kubelet: No such file or directory
core@k8s-master ~ $ 
```
I don't know why i don't have access to kubelet.

Is it possible to have some advice or help ?

Thanks
The owner of github.com/jteeuwen/go-bindata recently deleted their account, and someone else took it over.

https://twitter.com/francesc/status/961249107020001280

We probably want to re-evaluate how we're using that tool and if we should switch to something else.
Hi,

is there a wrapper to install CRI-O and use it with kubernetes on Containler Linux ?
None