Mish is a novel activation function proposed in this [paper](https://arxiv.org/abs/1908.08681).
It has shown promising results so far and has been adopted in several packages including:

|     |     |  |
| :---: | :---: |  :---: |
|[TensorFlow-Addons](https://github.com/tensorflow/addons/tree/master/tensorflow_addons/activations)|[SpaCy (Tok2Vec Layer)](https://github.com/explosion/spaCy)|[Thinc - SpaCy's official NLP based ML library](https://github.com/explosion/thinc/releases/tag/v7.3.0)|
|[Eclipse's deeplearning4j](https://github.com/eclipse/deeplearning4j/issues/8417)|[Hasktorch](https://github.com/hasktorch/hasktorch/blob/e5d4fb1b11663de3e032941f78b9d25c5da06f6d/hasktorch/src/Torch/Typed/Native.hs#L314)|[Echo AI](https://github.com/digantamisra98/Echo)|
|[CNTKX - Extension of Microsoft's CNTK](https://github.com/delzac/cntkx)|[FastAI-Dev](https://github.com/fastai/fastai_dev/blob/0f613ba3205990c83de9dba0c8798a9eec5452ce/dev/local/layers.py#L441)|[Darknet](https://github.com/AlexeyAB/darknet/commit/bf8ea4183dc265ac17f7c9d939dc815269f0a213)|
|[Yolov3](https://github.com/ultralytics/yolov3/commit/444a9f7099d4ff1aef12783704e3df9a8c3aa4b3)|[BeeDNN - Library in C++](https://github.com/edeforas/BeeDNN)|[Gen-EfficientNet-PyTorch](https://github.com/rwightman/gen-efficientnet-pytorch)|
|[dnet](https://github.com/umangjpatel/dnet)|[ruby-dnn](https://github.com/unagiootoro/ruby-dnn#implemented)|[blackcat-tensors](https://github.com/josephjaspers/blackcat_tensors/commit/87fa2f427d044a120816f2bc83176bdb455ceccd)|
|[DL4S](https://palle-k.github.io/DL4S/)|[HuggingFace Transformers](https://github.com/huggingface/transformers/pull/1753)|[PAGI](https://github.com/ProjectAGI/pagi)|
|[OpenCV](https://github.com/opencv/opencv/pull/15808)|[Odin-AI](https://github.com/imito/odin-ai)|[Mini DNN](https://github.com/yixuan/MiniDNN/commit/a405bd082183e685222472fa5851fd5024b1a7e6)|
|[Efficient Segmentation Networks](https://github.com/xiaoyufenfei/Efficient-Segmentation-Networks)|[TF Semantic Segmentation](https://pypi.org/project/tf-semantic-segmentation/0.1.0/)|[Dynastes](https://github.com/veqtor/dynastes)|
|[DLib](https://github.com/davisking/dlib/commit/edff12d2e17f65a2ded6ea71c6ba402055cea896)|[Copernicus](https://github.com/dmbernaal/copernicus)|[AllenNLP](https://github.com/allenai/allennlp/pull/3466#event-2951976800)|
|[PyWick](https://github.com/achaiah/pywick)|||

All benchmarks, analysis and links to official package implementations can be found in this [repository](https://github.com/digantamisra98/Mish) 

Mish also was recently used for a submission on the [Stanford DAWN Cifar-10 Training Time Benchmark](https://dawn.cs.stanford.edu/benchmark/CIFAR10/train.html) where it obtained 94% accuracy in just 10.7 seconds which is the current best score on 4 GPU and second fastest overall. Additionally, Mish has shown to improve convergence rate by requiring less epochs. Reference - 

![0 (2)](https://user-images.githubusercontent.com/34192716/72232207-9c0b7180-35e5-11ea-920f-295a7c873216.jpg)

Mish also has shown consistent improved ImageNet scores and is more robust. Reference - 

![0](https://user-images.githubusercontent.com/34192716/72232233-b9d8d680-35e5-11ea-97bb-fdb715a6f276.jpg)

Additional ImageNet benchmarks along with Network architectures and weights are avilable on my [repository](https://github.com/digantamisra98/Mish).

Summary of Vision related results: 

![Capture](https://user-images.githubusercontent.com/34192716/72232240-c78e5c00-35e5-11ea-9d1e-dd69f1ef33a7.PNG)

It would be nice to have Mish as an option within the activation function group.

This is the comparison of Mish with other conventional activation functions in a SEResNet-50 for CIFAR-10: 
![se50_1](https://user-images.githubusercontent.com/34192716/69002745-0de37980-091b-11ea-87da-ac8d17c79e07.png)
I get the error message from the title of this issue. It's not very informative what exactly the problem is or how i can fix this. I don't know if this is a problem in my code or in the library code.

Code: https://gist.github.com/flip111/af93e5597df5b35ca45a69d0e3571290

Terminal:
```
Â» RUST_BACKTRACE=1 cargo run
   Compiling rust v0.1.0 (/home/flip111/titanic/linear regression/rust)
    Finished dev [unoptimized + debuginfo] target(s) in 2.23s
     Running `target/debug/rust`
thread 'main' panicked at 'assertion failed: `(left == right)`
  left: `891`,
 right: `714`', /home/flip111/.cargo/registry/src/github.com-1ecc6299db9ec823/rulinalg-0.3.7/src/vector.rs:314:9
stack backtrace:
   0: backtrace::backtrace::libunwind::trace
             at /cargo/registry/src/github.com-1ecc6299db9ec823/backtrace-0.3.37/src/backtrace/libunwind.rs:88
   1: backtrace::backtrace::trace_unsynchronized
             at /cargo/registry/src/github.com-1ecc6299db9ec823/backtrace-0.3.37/src/backtrace/mod.rs:66
   2: std::sys_common::backtrace::_print_fmt
             at src/libstd/sys_common/backtrace.rs:76
   3: <std::sys_common::backtrace::_print::DisplayBacktrace as core::fmt::Display>::fmt
             at src/libstd/sys_common/backtrace.rs:60
   4: core::fmt::write
             at src/libcore/fmt/mod.rs:1030
   5: std::io::Write::write_fmt
             at src/libstd/io/mod.rs:1412
   6: std::sys_common::backtrace::_print
             at src/libstd/sys_common/backtrace.rs:64
   7: std::sys_common::backtrace::print
             at src/libstd/sys_common/backtrace.rs:49
   8: std::panicking::default_hook::{{closure}}
             at src/libstd/panicking.rs:196
   9: std::panicking::default_hook
             at src/libstd/panicking.rs:210
  10: std::panicking::rust_panic_with_hook
             at src/libstd/panicking.rs:473
  11: std::panicking::continue_panic_fmt
             at src/libstd/panicking.rs:380
  12: std::panicking::begin_panic_fmt
             at src/libstd/panicking.rs:335
  13: rulinalg::vector::Vector<T>::elemul
             at /home/flip111/.cargo/registry/src/github.com-1ecc6299db9ec823/rusty-machine-0.5.4/<::std::macros::panic macros>:9
  14: <rusty_machine::learning::toolkit::cost_fn::CrossEntropyError as rusty_machine::learning::toolkit::cost_fn::CostFunc<rulinalg::vector::Vector<f64>>>::cost
             at /home/flip111/.cargo/registry/src/github.com-1ecc6299db9ec823/rusty-machine-0.5.4/src/learning/toolkit/cost_fn.rs:87
  15: <rusty_machine::learning::logistic_reg::BaseLogisticRegressor as rusty_machine::learning::optim::Optimizable>::compute_grad
             at /home/flip111/.cargo/registry/src/github.com-1ecc6299db9ec823/rusty-machine-0.5.4/src/learning/logistic_reg.rs:190
  16: <rusty_machine::learning::optim::grad_desc::GradientDesc as rusty_machine::learning::optim::OptimAlgorithm<M>>::optimize
             at /home/flip111/.cargo/registry/src/github.com-1ecc6299db9ec823/rusty-machine-0.5.4/src/learning/optim/grad_desc.rs:83
  17: <rusty_machine::learning::logistic_reg::LogisticRegressor<A> as rusty_machine::learning::SupModel<rulinalg::matrix::Matrix<f64>,rulinalg::vector::Vector<f64>>>::train
             at /home/flip111/.cargo/registry/src/github.com-1ecc6299db9ec823/rusty-machine-0.5.4/src/learning/logistic_reg.rs:122
  18: rust::run
             at src/main.rs:167
  19: rust::main
             at src/main.rs:178
  20: std::rt::lang_start::{{closure}}
             at /rustc/4560ea788cb760f0a34127156c78e2552949f734/src/libstd/rt.rs:64
  21: std::rt::lang_start_internal::{{closure}}
             at src/libstd/rt.rs:49
  22: std::panicking::try::do_call
             at src/libstd/panicking.rs:292
  23: __rust_maybe_catch_panic
             at src/libpanic_unwind/lib.rs:80
  24: std::panicking::try
             at src/libstd/panicking.rs:271
  25: std::panic::catch_unwind
             at src/libstd/panic.rs:394
  26: std::rt::lang_start_internal
             at src/libstd/rt.rs:48
  27: std::rt::lang_start
             at /rustc/4560ea788cb760f0a34127156c78e2552949f734/src/libstd/rt.rs:64
  28: main
  29: __libc_start_main
  30: _start
```
I wanted to try rust for some data analysis after getting started with python. In scikit there saw there is a [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) which is useful for categorical data. Could such encoder be considered for rusty-machine? I would avoid having to write a bunch of boilerplate code.

By the way, i saw that the rust bindings for tensorflow have an open issue for the same thing https://github.com/tensorflow/rust

The scikit OneHotEncoder is not "the perfect design" in my opinion, but still it's nice to have one :) Perhaps a different design would be more suitable for rusty-machine
Use `slice::iter` instead of `into_iter` to avoid future breakage

`an_array.into_iter()` currently just works because of the autoref
feature, which then calls `<[T] as IntoIterator>::into_iter`. But
in the future, arrays will implement `IntoIterator`, too. In order
to avoid problems in the future, the call is replaced by `iter()`
which is shorter and more explicit.

A crater run showed that your crate is affected by a potential future 
change. See https://github.com/rust-lang/rust/pull/65819 for more information.
I am using the logit model

```
use rusty_machine::learning::logistic_reg::LogisticRegressor;
```

and I would like to get corresponding pvalues and error rates for the coeffs

my model looks like:

```
LogisticRegressor {
    base: BaseLogisticRegressor {
        parameters: Some(
            Vector {
                size: 8,
                data: [
                    -0.7278923401926283,
                    0.06431233971308888,
                    0.0030272254896660214,
                    0.016991843894604487,
                    0.010915105151989409,
                    -0.012891552442079453,
                    0.006423837227273267,
                    0.007800680523993605,
                ],
            },
        ),
    },
    alg: GradientDesc {
        alpha: 0.2,
        iters: 200,
    },
}
```

how do I know which of theses parameters are statistically significant? 
Prevent recursion stack overflow and make fewer allocations on large
dbscan clusters.
This enables the use of rusty-machine inside of WASM.

Thanks!
I can see that Stochastic Gradient Descent has already been implemented. But linear regression works using simple gradient descent. What are the challenges to implementing SGD for Linear Regression.

SGD implementation: https://github.com/AtheMathmo/rusty-machine/blob/master/src/learning/optim/grad_desc.rs#L159

https://github.com/AtheMathmo/rusty-machine/blob/master/src/learning/lin_reg.rs#L157
I get a sigkill when training on a 219000x100 matrix. 
Trying to run pca gives the following error

```
17 | use rusty_machine::learning::pca::PCA;
   |                              ^^^ could not find `pca` in `learning`
```

In case we are yet working on implementing pca, the docs should say that.