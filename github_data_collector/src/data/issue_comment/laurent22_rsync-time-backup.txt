Fixes #137 

Tested on FreeBSD 12.0, $OSTYPE is reported as freebsd12.0, script expects FreeBSD12.0
Super newb here so I hope I am not missing something super obvious.

When I run your script each file is being written again despite there being an existing copy of the file from a previous backup. I also ran out of space on my destination so it appears that auto deletion is also not functioning. 

I tested this out with simple test source directory of 27MB on the desktop and the destination grew by 27MB exactly each time I ran it. So after three runs the destination folder was ~81MB. 

I havn't customized the default behavior in anyway, when I get flags on the script I get the following:

-D --numeric-ids --links --hard-links --one-file-system --itemize-changes --times --recursive --perms --owner --group --stats --human-readable

What am I doing wrong?


advantages of temporary backup name:
 * script knows easy which backups are complete
    => no danger to try to resume a completed backup
    => lock can be set early without problems
 * user sees easy which backups are complete

setting the lock early prevents concurrent backups more reliable
The "inprogress" mark is set after purging old backups.
Problem: When deleting old backups takes long (many backups to purge, slow fs because remote mount, ...), a new backup can start successfully while the 1st is still purging backups.

suggestion: create "inprogress" just after checking if it exists.
This may have side effect on trying to resume an aborted backup. to improve this, the backup could get a temporary name ("_" preppended?) and be renamed to the final name when finished.

see in code:
 1. check if inprogress exists: [$(fn_find "$INPROGRESS_FILE") ... , L450](https://github.com/laurent22/rsync-time-backup/blob/88db869/rsync_tmbackup.sh#L450)
1. expire backups: [fn_expire_backups  ..., L529](https://github.com/laurent22/rsync-time-backup/blob/88db869/rsync_tmbackup.sh#L529)
1. create inprogress file: [echo $MYPID > $INPROGRESS_FILE , L566](https://github.com/laurent22/rsync-time-backup/blob/88db869/rsync_tmbackup.sh#L566)

note: also order related: defining link dest before purging backups #162
side note: DEST="$DEST_FOLDER/$NOW" is already done before purging, so the stamp in the folder name may have almost noting to do with the time of its content.
When doing a weekly/monthly backup with `rsync_tmbackup.sh` in a directory that contains approximately 30 backups, I've noticed that it can take upwards of 10 minutes before the backup actually begins with little to no information written to the console.

After enabling `+x` for the bash script and removing some redirection, I think I traced the problem to this line (trace upon request): https://github.com/laurent22/rsync-time-backup/blob/master/rsync_tmbackup.sh#L518, which is:

```
fn_find "$DEST -type d" 2>/dev/null`
```

Should the command on 518 in fact have a `-maxdepth` parameter? e.g.

```
fn_find "$DEST -maxdepth 1 -type d" 2>/dev/null
```

Right now the default behavior is to recursively look into each backup folder in the backup directory to look for whether the directory we want to create for the backup exists. Is there any particular reason to do this when the directory will be created _aside_ any of the directories traversed by this command?
updates #170, by introducing cases for different OSes. Also includes fixes for #172 and part of #181.

Thanks @kapitainsky.
Hello and thanks for this very useful tool!

Regarding #181, I made a couple changes that under circumstances always wrote to STDERR. This is a problem with cron where you usually redirect STDOUT to /dev/null but want to keep STDERR as is so that you get an email notification when a warning or error occurs. 

The first change was with the `df` check on SRC_FOLDER right after the trailling / is stripped. In the case of `SRC_FOLDER == /` , the / was stripped and df wrote to STDERR because FILE was the empty string.

The second change was the `-o UserKnownHostsFile=/dev/null` fixed ssh argument. This lead to ssh always writing to STDERR. This is part of #128. I would expect people to first setup the ssh access and then use rsync_tmbackup.sh. I cant' see a reason in bypassing the local configuration by default. There is always the `-e` option of rsync which can be used to set ssh options.

Best regards,

Panos
I want to back up a folder `/home/ec2-user/rsync_folder` from one machine to another machine.

Here's the content of the folder

```
.
|-- excludeme
|   `-- backupFile.txt
|-- file2.txt
|-- hello.txt
`-- mylink.txt -> hello.txt
```

The `excludeme` folder should not be copied to the destination machine.

Based on this [tutorial](https://sites.google.com/site/rsync2u/home/rsync-tutorial/the-exclude-from-option), I created a exclude file `exclude_list.txt`. Here's the content of it.

```
- excludeme/
```

After running `./rsync_tmbackup.sh -i ~/.ssh/my-key.pem -p 22 /home/ec2-user/rsync_folder ec2-user@123.123.123.123:/home/ec2-user exclude_list.txt` command. The folder `excludeme` appears on the other machine?

Why does this happen?

Is there anything wrong with the exclude file?

I've also tried `- /excludeme/`, `- /excludeme/*`, and `- ./excludeme/`. But all of them failed to prevent from copying `excludeme` folder to another machine.
Hello and thanks for this very useful tool!

When you backup /, the SRC_FOLDER var is stripped of the last / (line 378), and the df call (line 411) writes "df: '': No such file or directory" to STDERR. The script continues with the backup but it is not cron friendly since it always writes to STDERR and you cannot do:

MAILTO=panos@example.com
 0 */1 * * * rsync_tmbackup.sh / /mnt/backup >/dev/null

and only expect emails when something unexpected happens. It will always send an email due to the df error.

Best regards,

Panos
This is mostly useful for smaller embedded Linux systems that may only have `dropbear`/`dbclient` installed, without needing to do a `sed` replace!