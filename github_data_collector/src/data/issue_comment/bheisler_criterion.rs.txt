I discovered and fixed a panic caused by someone reducing nresamples to a very small number. After fixing the panic, I added a warning about reducing nresamples to such a low value. That warning should become an error in a future breaking change release.

Alternately, should the user even be allowed to configure nresamples? I can't think of a use case for it; it doesn't meaningfully affect the benchmark time. It might speed up analysis on underpowered machines I guess, but in that situation the benchmark itself is still likely to dominate. One downside of leaving the `nresamples` function is that it can confuse people who are looking for the `sample_size` function.
I wanted to write a benchmark function generic over the measurement such that I can easily benchmark once using cycles and once using time (see #355). However, there was no way to generically perform the measurements, which is why I added this function. The clone might not necessarily be required, but otherwise lifetime foo would become complicated, and usually it's a unit-struct anyway.

See the example in the doc-comment for an example use-case.
Adds a new application argument:

`--load-baseline <name>`

This restores raw data from the implementation defined raw dumps and
subsequently treats it as if it had just been sampled. The dumps are
created for all runs early after the sampling and analysis process.
Note that this requires all benchmarks that are analyzed to have the
named baseline previously saved with --save-baseline (except the last
run which can be loaded under the name 'new').

The expose command line option requires a comparison baseline to be
specified as the option otherwise offers little advantage over the
already available csv exporter.

Open to discussion of the approach and alternatives. Also, the error 
behaviour could be adjusted, it does not feel correct yet. A mechanism to
actually import/export sample dumps can be discussed in future issues and 
PRs or implemented by external tools as filesystem operations.

Closes: #357
currently benchmarks within the same benchmark group may use different time units for displaying, this makes it a bit harder to compare at a glance. for example one may use nano-seconds and one micro-seconds.

they should instead use one unit for the whole group.
Now that `cargo install` automatically updates the version if there is a new one available (see rust-lang/cargo/pull/7560), we no longer need to `--force` the installation of mdbook in the documentation CI build. Remove that once that change lands in a stable release.
## Motivation

Criterion's elaborate statistical analysis can only be run on a *current, newly performed* benchmark against some previously established baseline measurement. It would also be useful to be able to do a comparison without having to do a new expensive benchmark run and instead utilize two saved measurements/baselines.

## Use cases

Two main use cases come to mind:

1. Compare one state of a program against multiple other previous runs such as against the integration branch, against master, and against the last release. But this may also apply to experimentation where several competing ideas should be compared against each other instead of the linear fashion that `baseline` suggests.

2. Allow the analysis to occur on a separate (virtual) machine than the performance measurement. For example, when running the performance capture in a separate machine to ensure the better control of the hardware and the runtime environment. But the output is persisted into the `target` directory which is typically wiped in CI, and the comparison analysis is not saved at all. A current workaround is to capture a the full analysis run but this restricts it to comparing the exact chosen baseline against that CI run.

Currently, I've only found ways to perform a single measurement at the same time. However, I'd like to measure both the cpu cycles (`rdtscp`) and time (`CLOCK_THREAD_CPUTIME_ID`) at the same time.
I could have two benchmarks, one with cycles and one with time, but I don't want to run the benchmarks twice as they already take in the range of 10 minutes.

I tried implementing a custom `Measurement`, which uses a tuple of `(cycles, time)` as `Value`. However, it appears that the measurement value will be converted to an `f64` for internal analysis and is pass to the `ValueFormatter` as an f64 as well.

I could think of the following two solutions:
* Let the formatter format the output by passing the original `Measurement::Value` types.
* Add a function `Criterion::with_measurements(&[&dyn Measurement])`, which allows performing multiple measurements during a single run.
I have a cargo workspace project with a child project that contains benchmarks. When I run `cargo +nightly bench` it performs the benchmarking and generates an `index.html` file and various json and csv files. However, there are no plots or tables or links in the `index.html` report, and no other files in that report directory.

I have gnuplot installed, and can run it from the command-line. I've copy-pasted the fib benchmarks and most of their cargo.

```
<body>
   <div class="body">
   <h2>Criterion.rs Benchmark Index</h2>
   See individual benchmark pages below for more details.
   <ul>
   <li>Fibonacci-Custom</li>
   <li>Fibonacci-Custom-copy</li>
   <li>Fibonacci-Simple</li>
   <li>Fibonacci-Simple-copy</li>
   </ul>
   </div>
   <div id="footer">
   <p>This report was generated by
   <a href="https://github.com/bheisler/criterion.rs">Criterion.rs</a>, a statistics-driven benchmarking
   library in Rust.</p>
   </div>
</body>
```


Hi,

I am using latest rayon from git. By adding the `criterion` to my dependencies, cargo is not building the project. Here is the error,

```
|  DESKTOP DESKTOP=> cargo test
    Updating crates.io index
error: multiple packages link to native library `rayon-core`, but a native library can be linked only once

package `rayon-core v1.6.0 (/home/dinesh/phd/rusting/rayon/rayon-core)`
    ... which is depended on by `rayon v1.2.0 (/home/dinesh/phd/rusting/rayon)`
    ... which is depended on by `continuous_dem v0.1.0 (/home/dinesh/phd/papers/tpb_sand_stone/src/continuous_dem)`
links to native library `rayon-core`

package `rayon-core v1.4.0`
    ... which is depended on by `rayon v1.0.3`
    ... which is depended on by `criterion v0.2.10`
    ... which is depended on by `continuous_dem v0.1.0 (/home/dinesh/phd/papers/tpb_sand_stone/src/continuous_dem)`
also links to native library `rayon-core`
 
 
```
I tried to benching a system that has several threads communicating by channels.  And after I started `iter_batched`, the `Criterion` givens me some info:
```
Benchmarking RawNode::cluster/1: Warming up for 500.00 ms
Warning: Unable to complete 100 samples in 10.0s. You may wish to increase target time to 5265.0s or reduce sample count to 10
Benchmarking RawNode::cluster/1: Collecting 100 samples in estimated 5265.0 s (5050 iterations)
```
