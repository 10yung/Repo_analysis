
Allow for different fitting procedures following caret::trainControl()
Hi,

I was checking your work, and found a bug in `predict.FFForest`, that results in the error:
```
Error in matrix(NA, nrow = nrow(data), ncol = n.trees) : 
  non-numeric matrix extent
```
After checking the lastest source file (`predictFFForest_function.R`) I noticed that it tries to access the list key  `tree.sim` to iterate the various trees (lines 17 and 24). However the key with the simulations in the list returned by `FFForest` is named `fft.sim`. 
I tested by renaming the key in the list returned by `FFForest`, and the predict function works without error. 
Perhaps it could be changed in `predict.FFForest`.

Also, great work, these trees provide great explainability that allows to integrate non-technical users with extensive domain knowledge into the modeling decision processes.
There appears to be an overlooked condition in `inwords()` in that a predictive covariate of type `"l"` fails to result in an assignment of `sentence.i`, and thus this line errors:

https://github.com/ndphillips/FFTrees/blob/ac259886a85975135cab24a297eed4028d9f37c6/R/inwords.R#L141

I can't tell if this is supposed to be a restriction on the input data (no logical columns) or just a bug, but at first glance it seems like the latter.

Minimal reprexes:

``` r
suppressPackageStartupMessages(library(FFTrees))

## create a perfectly dependent column which is logical
heartdisease2 <- heartdisease
heartdisease2$lgl <- heartdisease2$diagnosis == 1
heart.fft <- FFTrees(formula = diagnosis ~., 
                     data = heartdisease2, 
                     progress = FALSE)
#> Error: object 'sentence.i' not found
```

With an independent logical column, the issue is bypassed

```r
heartdisease2 <- heartdisease
set.seed(1)
heartdisease2$lgl <- sample(c(TRUE, FALSE), nrow(heartdisease), replace = TRUE)
heart.fft <- FFTrees(formula = diagnosis ~., 
                     data = heartdisease2, 
                     progress = FALSE)
heart.fft$inwords
#> [1] "If thal = {rd,fd}, predict True"                   
#> [2] "If cp != {a}, predict False"                       
#> [3] "If ca <= 0, predict False, otherwise, predict True"
```

With a perfectly dependent *integer* column, the issue is bypassed

```r
heartdisease2 <- heartdisease
heartdisease2$lgl <- as.integer(heartdisease2$diagnosis == 1)
heart.fft <- FFTrees(formula = diagnosis ~., 
                     data = heartdisease2, 
                     progress = FALSE)
heart.fft$inwords
#> [1] "If lgl > 0, predict True"                            
#> [2] "If thal != {rd,fd}, predict False"                   
#> [3] "If cp != {a}, predict False, otherwise, predict True"
```
Goal: Create an FFTrees building algorithm that allows for classifying more than 2 classes.

*Method A*

The basic idea is to, for each cue / class combination, calculate a decision threshold that maximises a *joint* function of classification accuracy and percentage of cases classified. For example, a threshold that classifies 80% of class A with an accuracy of 90%, might be preferred to a threshold that classifies 70% of class A with an accuracy of 95%. 

For example, in the plot below, I show the relationship between classification proportions, and classification accuracy, for many different thresholds. The best value should be one that has a relatively high classification proportion, and accuracy.

In this case, threshold #33 might a good choice, because it has a relatively high accuracy around 90%, where 10% of cases are classified.

![image](https://user-images.githubusercontent.com/8480980/33139101-9a40f3bc-cfac-11e7-934c-f6f9ae3fabe2.png)




It's an important milestone that the latest version allows for **missing values**.  

Nonetheless, some related issues remain: 

1. How are missing values treated when **evaluating (individual) cue validities**?  
The current version simply ignores missing values.  However, this could yield undeserved high value to cues with many missing values. Thus, a more cautious alternative could classify cases according to the criterion's baseline probabilities when classifying cases with missing values.  An imaginary cue with ALL values missing would then perform at baseline level â€” and any improvement beyond this would be due to actual cue information. 

2. How are missing values treated when **applying an FFT** to data?
In the current version, nodes with missing data are simply skipped.  This seems fine, but what when the cue data of a final node is missing?  Again, classification according to the baseline probabilities seems an obvious choice (as is what presently happens). 

Another (and complementary) way to deal with many missing values is to build larger FFTs.  As long as the cues at each node perform better than baseline, this should increase the likelihood that a case can be classified at some node.

3. More **advanced issues** include:

- What happens when _new_ variables or levels of variables/factors appear during testing that have not been encountered before (during training)?
- What about _imputing_ missing values from existing data?




in plot.FFTrees(), add options to only show ROC curve, and main tree with icons
Re-write plot.FFTrees() in grid()