This pull request introduces a stable version of an `export.socrata()` function as outlined in #126. This allows users to download the contents of a data portal to a local directory. This function will download CSVs (compressed), PDFs, Word, Excel, PowerPoints, GeoJSON, Shapefiles, plain text documents (uncompressed), etc. It will not download HTML pages. As part of the process, the function also copies the [`data.json` file](https://resources.data.gov/schemas/dcat-us/v1.1/) to act as an index for other downloaded files.

I've proposed the version as 1.8.0.

## Testing portal export

To test this function, I used the [City of Norfolk, VA](https://data.norfolk.gov/) to export all of the data sets. Looking at their [data.json file](https://data.norfolk.gov/data.json), I counted 32 data sets that were _not_ HTML pages or did not have a downloadable file. Executing `export.socrata("https://data.norfolk.gov)` resulted in 32 downloaded files plus the copy of the `data.json` file. Thus, the expected number of files match the actual number of downloaded files.

### Testing non-CSV documents

All of the testing for Norfolk resulted in compressed CSV files, however, also needed to test the ability to download non-CSV files. [Kansas City, Missouri's data portal](https://data.kcmo.org/) has an unusually large number of non-CSV data sets on their portal, such as PDFs, word documents, Excel documents, etc. 

I tested the function on downloading files from their data portal. The function downloaded PDFs, Words, Excel, and other non-CSV files along with CSV files.

However, I did encounter frequent network timeouts after approximately 80 items were downloaded. I believe this is limited to the network and not an issue with the function itself. While this may not be a bug, it may be a limitation on the ability to export files from Socrata.

## Unit Testing

I have not written a unit test. I think any unit test will take too much time and space for typical unit testing. The smallest portal download, Norfolk, elapsed over 30 minutes to complete all downloads.

In general, a recommended method for testing is to choose a reasonably small portal and do the following:

  1. Export all files from the portal.
  2. When finished, open the data.json file and count all of the entries with the following exceptions:
    * `distribution/mediaType` is blank
    * `distribution/mediaType` is `text/html`
    * `distribution/downloadURL` is blank
  3. Compare the counts of download files (except the data.json file) and the count from step (2).

Ideally, the portal being used to test contains CSV files as well as non-CSV files.
Enhancement:
The Socrata Data Management API (https://socratapublishing.docs.apiary.io/#) enables Socrata publishing features like dataset drafts and on-platform data transformations. It would be ideal if RSocrata offered data publishers a convenient method for using the Data Management API codepath when sending data to Socrata, in addition to the current SODA endpoints.  
The test for this started failing recently.

The section of code:
```
test_that("Warn instead of fail if X-SODA2-* headers are missing", {
  expect_warning(dfCsv <- read.socrata("https://data.healthcare.gov/resource/enx3-h2qp.csv?$limit=1000"),
                info="https://github.com/Chicago/RSocrata/issues/118")
  expect_warning(dfJson <- read.socrata("https://data.healthcare.gov/resource/enx3-h2qp.json?$limit=1000"),
                info="https://github.com/Chicago/RSocrata/issues/118")
  expect_silent(df <- read.socrata("https://odn.data.socrata.com/resource/pvug-y23y.csv"))
  expect_silent(df <- read.socrata("https://odn.data.socrata.com/resource/pvug-y23y.json"))
  expect_equal("data.frame", class(dfCsv), label="class", info="https://github.com/Chicago/RSocrata/issues/118")
  expect_equal("data.frame", class(dfJson), label="class", info="https://github.com/Chicago/RSocrata/issues/118")
  expect_equal(150, ncol(dfCsv), label="columns", info="https://github.com/Chicago/RSocrata/issues/118")
  expect_equal(140, ncol(dfJson), label="columns", info="https://github.com/Chicago/RSocrata/issues/118")
})
```

The actual failing test message:
```
>   expect_equal(150, ncol(dfCsv), label="columns", info="https://github.com/Chicago/RSocrata/issues/118")
Error: columns not equal to ncol(dfCsv).
1/1 mismatches
[1] 150 - 146 == 4
https://github.com/Chicago/RSocrata/issues/118
```

I thought this might be useful, but it didn't help me:
```
> setdiff(colnames(dfJson), colnames(dfCsv))
[1] "url"   "url.1" "url.2" "url.3"
> setdiff(colnames(dfCsv), colnames(dfJson))
 [1] "network_url"                                          
 [2] "plan_brochure_url"                                    
 [3] "summary_of_benefits_url"                              
 [4] "drug_formulary_url"                                   
 [5] "adult_dental"                                         
 [6] "premium_scenarios"                                    
 [7] "standard_plan_cost_sharing"                           
 [8] "X_73_percent_actuarial_value_silver_plan_cost_sharing"
 [9] "X_87_percent_actuarial_value_silver_plan_cost_sharing"
[10] "X_94_percent_actuarial_value_silver_plan_cost_sharing"
```

It looks like the URL columns are different in name only, but the other six columns are missing in the JSON.  Not sure if this is related to this issue, or if this is something else?

_Originally posted by @geneorama in https://github.com/Chicago/RSocrata/issues/118#issuecomment-543835796_
Adding a note describing differences in writing to Socrata datasets from RSocrata/SODA compared to the Data Management API. This is to help prevent confusion when automating data updates to Socrata that should make use of transforms applied on the platform during ingress.
The `major.minor.patch` concept is clear, but what does the part after the dash indicate?

I believe it's one of these: 
 - major.minor.patch-commit
 - major.minor.patch-buildnumber

It's to me what convention @tomschenkjr was using. I see some big numbers which would seem like commits, but I know that the CRAN submission process can be challenging and it could be actual builds.

Either way, I'd like to make a decision and clarify it in the wiki. 
Socrata's Spatial Lens (most prominently used in Data Lens geographic region cards) determines the geographic regions (e.g., Chicago Community Area) in which a point record falls. These regions are not presented in an easily understood way in either the grid view of a dataset or the /resource API but can be determined through a multi-step process. It would be great service if RSocrata could perform this process and return the calculated regions, if present.

As an example, see https://data.cityofchicago.org/Buildings/Building-Violations/22u3-xenr.

The [/resource API](https://data.cityofchicago.org/resource/22u3-xenr.json?$limit=1) shows an example record:

```
[
{
"id": "6274501",
"violation_last_modified_date": "2019-06-13T06:50:00.000",
"violation_date": "2019-06-13T00:00:00.000",
"violation_code": "CN193029",
"violation_status": "OPEN",
"violation_description": "WATCHMAN",
"violation_ordinance": "Maintain watchman from 4:00 PM to 8:00 AM for vacant and dangerous residential premises. (13-12-140)",
"inspector_id": "BL00943",
"inspection_number": "12953099",
"inspection_status": "CLOSED",
"inspection_waived": "N",
"inspection_category": "COMPLAINT",
"department_bureau": "DEMOLITION",
"address": "5301 S JUSTINE ST",
"street_number": "5301",
"street_direction": "S",
"street_name": "JUSTINE",
"street_type": "ST",
"property_group": "331440",
"latitude": "41.797602233",
"longitude": "-87.663320286",
"location": {
"latitude": "41.797602233217454",
"longitude": "-87.6633202858523",
"human_address": "{\"address\": \"\", \"city\": \"\", \"state\": \"\", \"zip\": \"\"}"
},
":@computed_region_vrxf_vc4k": "59",
":@computed_region_6mkv_f3dw": "14924",
":@computed_region_rpca_8um6": "37",
":@computed_region_bdys_3d7i": "790",
":@computed_region_43wa_7qmu": "2",
":@computed_region_awaf_s7ux": "19"
}
]
```

Note in particular:

`":@computed_region_vrxf_vc4k": "59"`

The [/views API](https://data.cityofchicago.org/views/22u3-xenr) shows us under columns:

```
{
    "id" : 342479787,
    "name" : "Community Areas",
    "dataTypeName" : "number",
    "fieldName" : ":@computed_region_vrxf_vc4k",
    "position" : 31,
    "renderTypeName" : "number",
    "tableColumnId" : 60501607,
    "computationStrategy" : {
      "source_columns" : [ "location" ],
      "type" : "georegion_match_on_point",
      "parameters" : {
        "region" : "_vrxf-vc4k",
        "primary_key" : "_feature_id"
      }
```

So, that value is the Community Area but the value is not, as it might appear, Community Area 59. Instead, examine https://data.cityofchicago.org/dataset/Community-Areas/vrxf-vc4k. (Note the conversion of the underscore from the `computed_region` value to a hyphen.) The 59 refers to the record in this dataset with `_feature_id` 59, which turns out to be Community Area 61. (As I discovered in working through this example, the Feature IDs and Community Areas do match in many cases, which could lead people to think, incorrectly, that the `:@computed_region_vrxf_vc4k` is the Community Area number, itself.)

The final step is determining which column in this dataset shows the relevant value (Community Area, in this case). It should be fairly apparent to a person which column to use so, given the fairly small number of computed regions (types of regions, not the individual regions) likely used on a domain, it might be feasible to leverage that in some manner. However, there is an API. For the record, Socrata gave me the following warning, which I wish to record here:

> Please note: Engineering emphasized that this is **not** an official API, so you are welcome to consult it but just know it's not officially supported as a source of truth for automated processes.

That said, if we consult https://data.cityofchicago.org/api/curated_regions and search for `vrxf-vc4k`, we see:

```
{
"id": 261,
"name": "Community Areas",
"createdAt": 1445869668,
"defaultFlag": true,
"enabledFlag": true,
"featurePk": "_feature_id",
"geometryLabel": "community",
"uid": "vrxf-vc4k",
"view": {
"id": "vrxf-vc4k",
"name": "Community Areas",
"averageRating": 0,
"createdAt": 1424310233,
"displayType": "table",
"downloadCount": 4,
"hideFromCatalog": true,
"hideFromDataJson": true,
"indexUpdatedAt": 1494641500,
"newBackend": true,
"numberOfComments": 0,
"oid": 10269628,
"provenance": "official",
"publicationAppendEnabled": false,
"publicationDate": 1424310240,
"publicationGroup": 2273835,
"publicationStage": "published",
"tableId": 2273835,
"totalTimesRated": 0,
"viewCount": 64,
"viewLastModified": 1494640995,
"viewType": "tabular",
"grants": [
{
"inherited": false,
"type": "viewer",
"flags": [
"public"
]
}
```

The item of interest is:

`"geometryLabel": "community"`

That is, in fact, the API field name from https://data.cityofchicago.org/dataset/Community-Areas/vrxf-vc4k indicating the Community Area, although it is worth noting that the value in this column for the above example is not `61` but `NEW CITY`, the name of the Community Area, rather than the number.
```
q <- paste0("https://data.cityofchicago.org/resource/m6dm-c72p.json?",
            "$select=date_extract_y(trip_start_timestamp),",
            "date_extract_m(trip_start_timestamp),",
            "date_extract_d(trip_start_timestamp),",
            "count(trip_start_timestamp)&",
            "$group=date_extract_y(trip_start_timestamp),",
            "date_extract_m(trip_start_timestamp),",
            "date_extract_d(trip_start_timestamp)")
tab <- RSocrata::read.socrata(q)
```

results in 

```
> tab <- RSocrata::read.socrata(q)
2019-05-23 09:15:00.397 getResponse: Error in httr GET: 400  https://data.cityofchicago.org/resource/m6dm-c72p.json?%24select=date_extract_y%28trip_start_timestamp%29%2Cdate_extract_m%28trip_start_timestamp%29%2Cdate_extract_d%28trip_start_timestamp%29%2Ccount%28trip_start_timestamp%29&%24group=date_extract_y%28trip_start_timestamp%29%2Cdate_extract_m%28trip_start_timestamp%29%2Cdate_extract_d%28trip_start_timestamp%29&$order=:id
Error in getResponse(validUrl, email, password) : Bad Request (HTTP 400).
```

This is my first time noticing the end of the query (the `order=:id` part), this is probably the issue. (before it was off the screen and I didn't see it!)

For years I have been using The Chicago Police Department "Crimes 2001 to Present" data set by direct tsv for excel downloads. 'https://data.cityofchicago.org/resource/6zsd-86xi.csv'

Just started using RSocrata for access. Finding that the output is completely different.
adding completely new columns and changing capitalization on others.

What's up with this. 

Thanks for some insight.
From issue #158 

> The TravisCI build for release R is working. The AppVeyor build is failing while installing R packages, specifically openssl. It looks like it is building openssl from source and is hitting a bug or configuration issue, because it cannot find gcc to compile the package.
> 
> I believe that this commit would pass if not for this issue, and the CRAN builds do not show the openssl error.
> 
> One of the release R builds fails on an error that looks to be unrelated to our code, and I noticed some other packages on CRAN are getting the same failure.
> 
> I'm going to move forward in the interest of time.

Building off of #154, and also a prior PR #127, this change allows users to pass offsets as the queries. This skips paging and gives a message that the query has been offset.

I'll keep working on this if it breaks any of the current tests.