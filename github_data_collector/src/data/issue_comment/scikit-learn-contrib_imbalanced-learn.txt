Installing and calling pep8 as directed now leads to this warning

```
pep8 has been renamed to pycodestyle (GitHub issue #466)
Use of the pep8 tool will be removed in a future release.
Please install and use `pycodestyle` instead.

$ pip install pycodestyle
$ pycodestyle ...
```

This PR updates the contribution guide to reflect the name change.
I am trying to balance my data by reducing the number of samples of the class 0 but every time I specify the ratio ( via the sampling_strategy parameter, I get the following error :

#### Code
import matplotlib.pyplot as plt
import numpy as np

from sklearn.decomposition import PCA
from imblearn.under_sampling import (CondensedNearestNeighbour,
                                     EditedNearestNeighbours, AllKNN)

print(__doc__)

pca = PCA(n_components=2)

sampling_strategy = {0: 3500, 1:171}
cnn = CondensedNearestNeighbour(sampling_strategy)
X_resampled, y_resampled = cnn.fit_sample(X_train, Y_train)

ValueError: 'sampling_strategy' as a dict for cleaning methods is not supported. Please give a list of the classes to be targeted by the sampling. 

#### Remark
If the sampling strategy parameter is indeed removed as mentioned in the documentation then I would like to know if there is another way to specify the ratio.

#### Versions

Linux-4.9.119-44.140.amzn1.x86_64-x86_64-with-glibc2.9
Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19) 
[GCC 7.2.0]
NumPy 1.17.0
SciPy 1.3.1
Scikit-Learn 0.22.1
Imbalanced-Learn 0.6.1


<!-- Thanks for contributing! -->

Fixes #662

#### What does this implement/fix? Explain your changes.
If the median standard deviation is 0, the SMOTENC class will now store the categorical features before multiplying the 1's by the median standard deviation. This way, information about the most common categorical labels can still be used in _get_samples.

Checklist:
- [ ] Write tests

#### Example:
```
import numpy as np
from imblearn.over_sampling import SMOTENC
from sklearn.datasets import make_classification

np.random.seed(2)

# Original data
X = np.array([[1, 2, 4, 2], #minority class
              [1, 2, 5, 2], #minority class
              [1, 2, 1, 0], 
              [2, 1, 2, 0], 
              [1, 2, 3, 1]])
y = np.array(['A', 'A', 'B', 'B', 'B'])

# Construct SMOTENC with masks
smotenc = SMOTENC(
    [False, False, False, True], 
    sampling_strategy = "not majority",
    k_neighbors=1
)

# Resample
X_resampled, y_resampled = smotenc.fit_resample(X, y)
print(X_resampled)
print(y_resampled)
```
*Output on master*:
```
[[1.         2.         4.         2.        ]
 [1.         2.         5.         2.        ]
 [1.         2.         1.         0.        ]
 [2.         1.         2.         0.        ]
 [1.         2.         3.         1.        ]
 [1.         2.         4.18508208 1.        ]]
['A' 'A' 'B' 'B' 'B' 'A']
```
Only the last row is new. It has the category 1 in the fourth column, even though all rows from the minority class have the category 2 in the fourth column. This is incorrect.

*Output on this fork*:
```
[[1.         2.         4.         2.        ]
 [1.         2.         5.         2.        ]
 [1.         2.         1.         0.        ]
 [2.         1.         2.         0.        ]
 [1.         2.         3.         1.        ]
 [1.         2.         4.18508208 2.        ]]
['A' 'A' 'B' 'B' 'B' 'A']
```
Here, the resampled row correctly has the category 2 in the fourth column.

#### Reference Issue
Fixes #671


#### What does this implement/fix? Explain your changes.

Converts the target vector in the binary or multiclass case

#### Any other comments?
#666 (OMG what an evil number) is related but it will be solved probably in a following PR because there is an extra check of the name in the `y` which I think that is redundant.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.

Thanks for contributing!
-->

In tests it could be ok to have copy/paste code but I believe that in [common estimator checks](https://github.com/scikit-learn-contrib/imbalanced-learn/blob/master/imblearn/utils/estimator_checks.py) we could remove the boilerplate of datasets generation for each test by creating a common fixture. Apart from that we could reduce the number of instances of the dataset(s) in order to speed up the execution of the commont tests. The later could be measured, though.
#### Description
When running the method `fit()` on the following Pipeline the code fails with `TypeError`. If I remove the `RandomOverSampler` from the Pipeline then I face no error. Is it a bug or a wrong initialization?

#### Steps/Code to Reproduce
```python
from imblearn.pipeline import Pipeline
from sklearn.feature_selection import VarianceThreshold
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import RandomOverSampler
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier
from sklearn.utils import shuffle
import numpy as np
import pandas as pd

mnist_train = pd.read_csv("https://www.python-course.eu/data/mnist/mnist_train.csv", header=None).values
mnist_test = pd.read_csv("https://www.python-course.eu/data/mnist/mnist_test.csv", header=None).values
mnist = np.concatenate((mnist_train, mnist_test), axis=0)

xfull = mnist[:, 1:]
yfull = mnist[:, :1]
sdata, starget = shuffle(xfull, yfull, random_state=36)
samples = 1000
X = sdata[0:samples-1,:]
y = starget[0:samples-1]
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.33, random_state=20176)
selector = VarianceThreshold()
scaler = StandardScaler()
ros = RandomOverSampler()
pca = PCA()
clf = KNeighborsClassifier(n_jobs=-1)
pipe = Pipeline(
    steps=[
           ('selector', selector), ('scaler', scaler), ('sampler', ros),
           ('pca', pca), ('kNN', clf)])
# X_train.shape is (669, 784)
# Y_train.shape is (669, 1)
pipe.fit(X_train, y_train)
```

#### Expected Results
No error is thrown.

#### Actual Results
```
TypeError                                 Traceback (most recent call last)
<ipython-input-51-da2a32d7cb52> in <module>()
----> pipe.fit(X_train, y_train)

10 frames
/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py in fit(self, X, y, groups, **fit_params)
    737             refit_start_time = time.time()
    738             if y is not None:
--> 739                 self.best_estimator_.fit(X, y, **fit_params)
    740             else:
    741                 self.best_estimator_.fit(X, **fit_params)

/usr/local/lib/python3.6/dist-packages/imblearn/pipeline.py in fit(self, X, y, **fit_params)
    285 
    286         """
--> 287         Xt, yt, fit_params = self._fit(X, y, **fit_params)
    288         with _print_elapsed_time('Pipeline',
    289                                  self._log_message(len(self.steps) - 1)):

/usr/local/lib/python3.6/dist-packages/imblearn/pipeline.py in _fit(self, X, y, **fit_params)
    247                     message_clsname='Pipeline',
    248                     message=self._log_message(step_idx),
--> 249                     **fit_params_steps[name]
    250                 )
    251             # Replace the transformer of the step with the fitted

/usr/local/lib/python3.6/dist-packages/joblib/memory.py in __call__(self, *args, **kwargs)
    566 
    567     def __call__(self, *args, **kwargs):
--> 568         return self._cached_call(args, kwargs)[0]
    569 
    570     def __getstate__(self):

/usr/local/lib/python3.6/dist-packages/joblib/memory.py in _cached_call(self, args, kwargs, shelving)
    532 
    533         if must_call:
--> 534             out, metadata = self.call(*args, **kwargs)
    535             if self.mmap_mode is not None:
    536                 # Memmap the output at the first call to be consistent with

/usr/local/lib/python3.6/dist-packages/joblib/memory.py in call(self, *args, **kwargs)
    732         if self._verbose > 0:
    733             print(format_call(self.func, args, kwargs))
--> 734         output = self.func(*args, **kwargs)
    735         self.store_backend.dump_item(
    736             [func_id, args_id], output, verbose=self._verbose)

/usr/local/lib/python3.6/dist-packages/imblearn/pipeline.py in _fit_resample_one(sampler, X, y, message_clsname, message, **fit_params)
    412                       **fit_params):
    413     with _print_elapsed_time(message_clsname, message):
--> 414         X_res, y_res = sampler.fit_resample(X, y, **fit_params)
    415 
    416         return X_res, y_res, sampler

/usr/local/lib/python3.6/dist-packages/imblearn/base.py in fit_resample(self, X, y)
     79         )
     80 
---> 81         output = self._fit_resample(X, y)
     82 
     83         if self._X_columns is not None or self._y_name is not None:

/usr/local/lib/python3.6/dist-packages/imblearn/over_sampling/_random_over_sampler.py in _fit_resample(self, X, y)
    102     def _fit_resample(self, X, y):
    103         random_state = check_random_state(self.random_state)
--> 104         target_stats = Counter(y)
    105 
    106         sample_indices = range(X.shape[0])

/usr/lib/python3.6/collections/__init__.py in __init__(*args, **kwds)
    533             raise TypeError('expected at most 1 arguments, got %d' % len(args))
    534         super(Counter, self).__init__()
--> 535         self.update(*args, **kwds)
    536 
    537     def __missing__(self, key):

/usr/lib/python3.6/collections/__init__.py in update(*args, **kwds)
    620                     super(Counter, self).update(iterable) # fast path when counter is empty
    621             else:
--> 622                 _count_elements(self, iterable)
    623         if kwds:
    624             self.update(kwds)

TypeError: unhashable type: 'numpy.ndarray'
```

#### Versions
Linux-4.14.137+-x86_64-with-Ubuntu-18.04-bionic
Python 3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
NumPy 1.18.0
SciPy 1.3.3
Scikit-Learn 0.22.1
Imbalanced-Learn 0.6.1


dear sir,  
           presently I am working with imbalanced datasets having dimension 20 billion instances and 1450 features. I am using Dask.dataframe  as inputing library beacuse my data does not fit into my ram. and willing to incremental machine learning model. while using make_pipeline from pipeline library, I am getting am error that make_pipeline does not  work with partial_fit function used for big data. please suggest me way out to use imbalance learning algorithms with these datasets.


```pytb
AttributeError                            Traceback (most recent call last)
<ipython-input-32-8d38637b6cc6> in <module>
      6 
      7 oversampler=SMOTE(random_state=42)
----> 8 smote_train, smote_target = oversampler.fit_resample(X,y)
      9 
     10 print("Before OverSampling, counts of label '0', '1':", smote_target['label'].value_counts())

~\Anaconda3\lib\site-packages\imblearn\base.py in fit_resample(self, X, y)
     73         """
     74         check_classification_targets(y)
---> 75         X, y, binarize_y = self._check_X_y(X, y)
     76 
     77         self.sampling_strategy_ = check_sampling_strategy(

~\Anaconda3\lib\site-packages\imblearn\base.py in _check_X_y(self, X, y, accept_sparse)
    148         if hasattr(y, "loc"):
    149             # store information to build a series
--> 150             self._y_name = y.name
    151             self._y_dtype = y.dtype
    152         else:

~\Anaconda3\lib\site-packages\pandas\core\generic.py in __getattr__(self, name)
   5065             if self._info_axis._can_hold_identifiers_and_holds_name(name):
   5066                 return self[name]
-> 5067             return object.__getattribute__(self, name)
   5068 
   5069     def __setattr__(self, name, value):

AttributeError: 'DataFrame' object has no attribute 'name'
```
#### Description:
When using `'not majority'` method and when self.median_std_ == 0.0 (based on the minority class standard deviations), the new over-sampled categorical field gets categories which don't belong to the given class, but rather are any of the categories of the WHOLE categorical field, across classes. This is a violation of the main imputation logic in the original paper, where the K nearest neighbours, and their fields distributions, are calculated and taken within the same-class vectors.

This issue occurs through the following two code segments:
1. in `_smote.py`, class `SMOTENC`, method `_fit_resample`:

```
def _fit_resample(self, X, y):
...
        # we can replace the 1 entries of the categorical features with the
        # median of the standard deviation. It will ensure that whenever
        # distance is computed between 2 samples, the difference will be equal
        # to the median of the standard deviation as in the original paper.
        X_ohe.data = (np.ones_like(X_ohe.data, dtype=X_ohe.dtype) *
                      self.median_std_ / 2)
...
```
 The problem occurs when the median std of the minority class is zero (`self.median_std_ == 0.0`): the one-hot vectors are multiplied by 0.0, then we get zero-vectors and all the information of the categories (i.e. which category is represented) is lost.

2. in `_smote.py`, class `SMOTENC`, method `_generate_sample`:
```
def _generate_sample(self, X, nn_data, nn_num, row, col, step):
...
        for start_idx, end_idx in zip(np.cumsum(categories_size)[:-1],
                                      np.cumsum(categories_size)[1:]):
            col_max = all_neighbors[:, start_idx:end_idx].sum(axis=0)
            # tie breaking argmax
            col_sel = rng.choice(np.flatnonzero(
                np.isclose(col_max, col_max.max())))
...
```
Then, `col_max` gets the zero-vectors from the previous step and `col_sel` gets a random tie-breaking choice from the WHOLE categories range, including categories which don't belong to the resampled minority class. This happens because no information was kept regarding the categories of the K nearest neighbours and thus there's no maximum value of any category across the vectors to determine the true value.

### Example: 
```
### First three columns are continuous and the fourth is categorical
data = np.array([[1, 2, 1, 'A'], [2, 1, 2, 'A'], [1, 2, 3, 'B'], [1, 2, 4, 'C'], [1, 2, 5, 'C']])
labels = np.array(['class_1', 'class_1', 'class_1', 'class_2', 'class_2'])
print(data)
array([['1', '2', '1', 'A'],
       ['2', '1', '2', 'A'],
       ['1', '2', '3', 'B'],
       ['1', '2', '4', 'C'],
       ['1', '2', '5', 'C']], dtype='<U21')
print(labels)
array(['class_1', 'class_1', 'class_1', 'class_2', 'class_2'], dtype='<U7')

```
#### Expected Results:
```
print(resampled_data)
array([['1', '2', '1', 'A'],
       ['2', '1', '2', 'A'],
       ['1', '2', '3', 'B'],
       ['1', '2', '4', 'C'],
       ['1', '2', '5', 'C'],
       ['1', '2', '4.72', 'C']], dtype='<U21')

print(resampled_labels)
array(['class_1', 'class_1', 'class_1', 'class_2', 'class_2', 'class_2'],
      dtype='<U7')
```
#### Actual Results:
```
print(resampled_data)
array([['1', '2', '1', 'A'],
       ['2', '1', '2', 'A'],
       ['1', '2', '3', 'B'],
       ['1', '2', '4', 'C'],
       ['1', '2', '5', 'C'],
       ['1', '2', '4.72', 'A']], dtype='<U21')

print(resampled_labels)
array(['class_1', 'class_1', 'class_1', 'class_2', 'class_2', 'class_2'],
      dtype='<U7')
```

#### Versions
Darwin-19.0.0-x86_64-i386-64bit
Python 3.5.4 (v3.5.4:3f56838976, Aug  7 2017, 12:56:33) 
[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)]
NumPy 1.16.3
SciPy 1.3.0
Scikit-Learn 0.21.2
Imbalanced-Learn 0.5.0
