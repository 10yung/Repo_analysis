Description
===========
I am running a Python Producer service in AWS Lambda using the `confluent-kafka` library which makes use of librdkafka, connecting to a kafka service that requires SSL. I have found that the only way I can authenticate to kafka is to include a physical CA PEM file in the 'ssl.ca.location' setting. This is deemed as a security risk, because I'd have to store the PEM file in my repo in order to deploy it with my Lambda code. This feels like I'm missing a better alternative, but in my searches through the documentation, I haven't seen one. Is there a better way to do this?

Checklist
=========
 - [x] librdkafka version (release number or git tag): `confluent-kafka 1.3.0`
 - [x] Apache Kafka version: Confluent 5.1
 - [x] librdkafka client configuration: `acks=1,bootstrap.servers=KAFKA_BROKER,security.protocol=SASL_SSL,sasl.mechanism=SCRAM-SHA-256,sasl.username=SASL_USERNAME,sasl.password=SASL_PASSWORD,ssl.ca.location=<path to CA pem>`
 - [x] Operating system: `Python 3.6`
 - [x] Provide logs: There are no logs; without the CA, `producer.list_topics()` and `producer.flush()` just hang until timeout
 - [x] Provide broker log excerpts: No logs
 - [x] Critical issue: No
Another minor tweak to a type:
For proper atomicity (with respect to signal handler), the type of variables modified in signal handlers should be 'sig_atomic_t' with 'volatile' qualifier (modifying other types such as a plain 'int' is undefined behaviour even if it works as expected on most platforms). This is atomicity between main code & signal handler i.e. required even in single-threaded program.

In C++ files, used 0 for false and 1 for true respectively just to avoid any confusion and implicit conversions (which is otherwise fine).

sig_atomic_t type is available since C89, so should be reasonably portable.
Read the FAQ first: https://github.com/edenhill/librdkafka/wiki/FAQ

Description
===========
Hi , I am trying to create a producer that connects to a KAFKA broker. This is a SSL connection with all the certs supplied.

RDKAFKA-7-BROKERFAIL: rdkafka#producer-3: [thrd:ssl://10.0.0.1:9092/bootstrap]: ssl://10.0.0.1:9092/bootstrap: failed: err: Local: Host resolution failure: (errno: Bad address)
nfm_trace_dump.txt:[2020 Jan 15 18:54:32.295180265:7096:P:logger:1486] RDKAFKA-3-FAIL: rdkafka#producer-3: [thrd:ssl://10.0.0.1:9092/bootstrap]: ssl://10.0.0.1:9092/bootstrap: Failed to resolve '10.0.0.1:9092': invalid flags value


How to reproduce
================
1. Created a conf and updated the parameters
//Setup the dcos_context
snprintf(vrf_str, sizeof(vrf_str), "%d", 4);
if (setenv("DCOS_CONTEXT", vrf_str, 1) == -1)

rd_kafka_conf_set(conf, "bootstrap.servers","10.0.0.1",
rd_kafka_conf_set(conf, "security.protocol", "SSL", errstr, sizeof(errstr)) 
rd_kafka_conf_set(conf,"ssl.certificate.location", "/client/kafka/KafkaClient.crt",errstr, sizeof(errstr))
rd_kafka_conf_set(conf, "ssl.key.location", "/client/kafka/KafkaClient.key",errstr, sizeof(errstr))
rd_kafka_conf_set(conf, "ssl.ca.location", "/client/kafka/Ca.crt"errstr, sizeof(errstr))

2. rd_conf_new with above configs were successful 
3. Created a topic using rd_kafka_topic_new which was successful

rkt = rd_kafka_topic_new(rk, "topic_test", NULL);


4. Started the rd_kafka poll after the creation of the producer
rd_kafka_poll(rk, 1000/*block for max 1000ms*/)

5. Got the following logs 
RDKAFKA-7-SSL: rdkafka#producer-1: [thrd:app]: Loading CA certificate(s) from file /client/kaf
ka/Ca.crt
RDKAFKA-7-SSL: rdkafka#producer-1: [thrd:app]: Loading certificate from file /client/kafka/KafkaClient.crt
 RDKAFKA-7-SSL: rdkafka#producer-1: [thrd:app]: Loading private key file from /client/kafka/KafkaClient.key
 RDKAFKA-7-BRKMAIN: rdkafka#producer-1: [thrd::0/internal]: :0/internal: Enter main broke
r thread
RDKAFKA-7-STATE: rdkafka#producer-1: [thrd::0/internal]: :0/internal: Broker changed sta
te INIT -> UP
 RDKAFKA-7-WAKEUPFD: rdkafka#producer-1: [thrd:app]: ssl://10.0.0.1:9092/bootstrap: Enabl
ed low-latency partition queue wake-ups
RDKAFKA-7-BROADCAST: rdkafka#producer-1: [thrd::0/internal]: Broadcasting state change
RDKAFKA-7-WAKEUPFD: rdkafka#producer-1: [thrd:app]: ssl://10.0.0.1:9092/bootstrap: Enabl
ed low-latency ops queue wake-ups
RDKAFKA-7-BROKER: rdkafka#producer-1: [thrd:app]: ssl://10.0.0.1:9092/bootstrap: Added n
ew broker with NodeId -1
 [KAFKA] Created kafka producer
 RDKAFKA-7-TOPIC: rdkafka#producer-1: [thrd:app]: New local topic: topic_test
RDKAFKA-7-BRKMAIN: rdkafka#producer-1: [thrd:ssl://10.0.0.1:9092/bootstrap]: ssl://10.0.
0.1:9092/bootstrap: Enter main broker thread
RDKAFKA-7-CONNECT: rdkafka#producer-1: [thrd:ssl://10.0.0.1:9092/bootstrap]: ssl://10.0.
0.1:9092/bootstrap: broker in state INIT connecting
 RDKAFKA-7-TOPPARNEW: rdkafka#producer-1: [thrd:app]: NEW topic_test [-1] 0x1183cdbc (at rd_ka
fka_topic_new0:282)
RDKAFKA-7-METADATA: rdkafka#producer-1: [thrd:app]: Skipping metadata refresh of 1 topic
(s): no usable brokers
Created kafka topic
Created producer
 RDKAFKA-7-BROKERFAIL: rdkafka#producer-1: [thrd:ssl://10.0.0.1:9092/bootstrap]: ssl://10
.0.0.1:9092/bootstrap: failed: err: Local: Host resolution failure: (errno: Bad address)
RDKAFKA-3-FAIL: rdkafka#producer-1: [thrd:ssl://10.0.0.1:9092/bootstrap]: ssl://10.0.0.1
:9092/bootstrap: Failed to resolve '10.0.0.1:9092': invalid flags value
 RDKAFKA-3-ERROR: rdkafka#producer-1: [thrd:ssl://10.0.0.1:9092/bootstrap]: ssl://10.0.0.
1:9092/bootstrap: Failed to resolve '10.0.0.1:9092': invalid flags value

6. The certs are all present and the broker is accessible via openssl
 DCOS_CONTEXT=4 openssl s_client -CAfile /client/kafka/Ca.crt -connect 10.0.0.1:9092 -prexit -cert /client/kafka/KafkaClient.crt -key /client/kafka/KafkaClient8.key -debug -state -tls1 -msg

SSL handshake has read 2493 bytes and written 2487 bytes

**IMPORTANT**: Always try to reproduce the issue on the latest released version (see https://github.com/edenhill/librdkafka/releases), if it can't be reproduced on the latest version the issue has been fixed.


Checklist
=========

**IMPORTANT**: We will close issues where the checklist has not been completed.

Please provide the following information:

 - [x] librdkafka version (release number or git tag): `<REPLACE with e.g., v0.10.5 or a git sha. NOT "latest" or "current">`Version: 0.9.1-1pre1
 - [] Apache Kafka version: 
 - [x ] librdkafka client configuration: `<REPLACE with e.g., message.timeout.ms=123, auto.reset.offset=earliest, ..>`
 - [x ] Operating system: `<REPLACE with e.g., Centos 5 (x64)>NXOS`
 - [ x] Provide logs (with `debug=..` as necessary) from librdkafka
 - [ ] Provide broker log excerpts
 - [x ] Critical issue


 size_t being an unsigned type, '%zu' is the correct format specifier.
 Using '%zd' is technically undefined behaviour (due to sign mismatch).
recently I used fluentd to send logs to kafka, using the rdkafka2 type in fluent-plugin-kafka. Rdkafka2 will use rdkafka-ruby as the client, and rdkafka-ruby will call librdkafka as the actual worker.
I did some tests for performance, but the results were very unsatisfactory.

Description
===========
The producer and kafka server are on the same network, so there is no network delay,
The test loads 400,000 pieces of data from a file, each piece of data is about 500 bytes, the average transmission time of the test results is 35s, and only more than 10,000 pieces of data are processed per second, and I found from the trace log print that most of the time is used for data After processing, it will take very little time to send to Kafka after packaging into a MessageSet. Does anyone have experience in this area, please help


How to reproduce
================
Requires a complete fluentd environment

**IMPORTANT**: Always try to reproduce the issue on the latest released version (see https://github.com/edenhill/librdkafka/releases), if it can't be reproduced on the latest version the issue has been fixed.


Checklist
=========

**IMPORTANT**: We will close issues where the checklist has not been completed.

Please provide the following information:

 - [x] librdkafka version (release number or git tag): `<1.2.0 >`
 - [x]  Apache Kafka version: `<2.0.0>`
 - [x] librdkafka client configuration:
 `<"batch.num.messages": 1000000,
"queue.buffering.max.ms": 15000,
"request.required.acks": 1,
"queue.buffering.max.messages":10000000,
"queue.buffering.max.kbytes": 4000000,
"socket.keepalive.enable": true,
"socket.send.buffer.bytes": 0,
"socket.blocking.max.ms": 2,
"message.max.bytes": 1000000000>`
 - [x] Operating system: `<Alpine Linux v3.9>`
 - [x] Provide logs (with `debug=..` as necessary) from librdkafka
https://user-images.githubusercontent.com/7601142/72117955-7d5d6e80-338a-11ea-9bf1-58dc47d64737.png
 - [ ] Provide broker log excerpts
 - [ ] Critical issue


Additionally, i think it's misleading to use `RD_KAFKA_RESP_ERR_MSG_SIZE_TOO_LARGE` in the error op below, as it's a broker error implying it originated from the broker. This had me confused for some time. I'll let you work out what to do with that.
Read the FAQ first: https://github.com/edenhill/librdkafka/wiki/FAQ



Description
===========
Every time TopicImpl::name() is called, a new std::string is created via copy form C string. That will waste CPU and create extra allocations/deallocations if someone decides to do things like log the topic name from a Topic instance. Topic::name() should return a std::string&, like TopicPartition::topic() does. TopicImpl::name() should create the string lazily and store it as a member for reuse.


How to reproduce
================
In code.


**IMPORTANT**: Always try to reproduce the issue on the latest released version (see https://github.com/edenhill/librdkafka/releases), if it can't be reproduced on the latest version the issue has been fixed.


Checklist
=========

**IMPORTANT**: We will close issues where the checklist has not been completed.

Please provide the following information:

 - [x] librdkafka version (release number or git tag): master branch
 - [ ] Apache Kafka version: n/a
 - [ ] librdkafka client configuration: n/a
 - [ ] Operating system: n/a
 - [ ] Provide logs (with `debug=..` as necessary) from librdkafka
 - [ ] Provide broker log excerpts
 - [ ] Critical issue


Description
===========
Back Trace

#0  0x00007ffff7833340 in thrd_create () from /usr/lib64/libpthread.so.0
#1  0x00007ffff2193afe in rd_kafka_new (type=RD_KAFKA_CONSUMER, app_conf=0x894840, errstr=0x88c438 "",
    errstr_size=512) at rdkafka.c:2179

[New Thread 0x7ffff0ca3700 (LWP 31923)]
rd_kafka_new (type=RD_KAFKA_CONSUMER, app_conf=0x894840, errstr=0x88c438 "", errstr_size=512) at rdkafka.c:2181
2181                    rk->rk_init_wait_cnt--;
(gdb) n
2182                    ret_err = RD_KAFKA_RESP_ERR__CRIT_SYS_RESOURCE;
(gdb) n
2183                    ret_errno = errno;
(gdb) n
2184                    if (errstr)
(gdb) n
2185                            rd_snprintf(errstr, errstr_size,
(gdb) n
2188                    rd_kafka_wrunlock(rk);
(gdb) p errstr
$1 = 0x88c438 "Failed to create thread: No such file or directory (2)"
(gdb)

Librdkafka is built with the following options
./configure --enable-gssapi --enable-ssl --disable-optimization
.........................
.........................
checking for libpthread (by pkg-config)... failed
checking for libpthread (by compile)... ok (cached)
checking for c11threads (by pkg-config)... failed
checking for c11threads (by compile)... failed (disable)
..................................
..................................


Checklist
=========

**IMPORTANT**: We will close issues where the checklist has not been completed.

Please provide the following information:

 - [x] librdkafka version (release number or git tag): `<latest>`
 - [ ] Operating system: `<RHEL 8>`
 - [ ] Critical issue


update the usage with the compression codec of lz4 and zstd
    When building without the fix, you get an error:

    1>------ Build started: Project: rdkafka_example, Configuration: Release x64 ------
    1>C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\MSBuild\Microsoft\VC\v160\Microsoft.CppBuild.targets(379,5): error MSB8020: The build tools for v142 (Platform Toolset = 'v142') cannot be found. To build using the v142 build tools, please install v142 build tools.  Alternatively, you may upgrade to the current Visual Studio tools by selecting the Project menu or right-click the solution, and then selecting "Retarget solution".
    1>Done building project "rdkafka_example.vcxproj" -- FAILED.

    The error is NOT fixed, when re-targeting the solution.
    The file common.vcxproj needs to contain the relevant platform toolset definitions, and this is what this commit does.