Bumps xstream from 1.4.2 to 1.4.6.

[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.thoughtworks.xstream:xstream&package-manager=maven&previous-version=1.4.2&new-version=1.4.6)](https://help.github.com/articles/configuring-automated-security-fixes)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot ignore this [patch|minor|major] version` will close this PR and stop Dependabot creating any more for this minor/major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/NICTA/scoobi/network/alerts).

</details>
Scoobi has no mechanism for notifying users on various steps of workflow. I believe having a notification mechanism greatly increases monitoring ability granting greater control. 

A Webhooks API is  a system of automated notifications sent to your server based on certain triggers so you always have the most accurate info about your customers. Every notification is a POST request sent to a URL of your choice. Having such a mechanism would also help us to create better visualizations of workflow.

I run a ScoobiApp locally using scoobi 0.8.4-cdh5 with the LocalJobRunner. The overwrite option is set to true. At the last step, I get the following error:

[ERROR] OutputChannel - can not move 
 file:/tmp/scoobi-user/SampleApp$-1104-100253-2062082297/tmp-out-step_4_of_4/ch303-128 to 
 data/output

ch303-128 contains a file with the expected results. 

This error only occurs if the data/output folder contains only a _SUCCESS or an hidden file before I run the application.

The workaround is to remove the output directory or its content.

1) [minor] Rename parameters to be more consistent (no function change)
2) Add parallelization to HDFS->S3 move operation for speedup.
PTAL @etorreborre 

<!-- Reviewable:start -->

[<img src="https://reviewable.io/review_button.png" height=40 alt="Review on Reviewable"/>](https://reviewable.io/reviews/nicta/scoobi/348)

<!-- Reviewable:end -->

If the cluster is under a lot of load, client calls which directly interact with it can fail in weird ways. I have seen `LeaseExpiredException`, `ConnectException: Connection refused`, and others which all point to a cluster under load not able to respond to requests.

Scoobi should retry these client calls a number of times so the app doesn't die on the first exception from a hadoop call.

Examples:
- [ ] Counters
- [ ] HDFS interaction
- [ ] Other?

The double values are noted as floats in the logs, which is often confusing and sometimes misleading. I have created a pull request to fix this issue : 
https://github.com/NICTA/scoobi/pull/342

It always gets written to `java.io.tmpdir`

Apache tez[1] enables executing DAG on top of YARN. It has significant performance gains over plain map-reduce. Hive 0.13[2] has seen magnitude of performance improvement by using Tez. Is there any plan of using Tez in scoobi as one of the underlying execution framework?
1. http://tez.apache.org/
2. http://www.slideshare.net/hortonworks/apache-hive-013-performance-benchmarks

Attn: @t3rmin4t0r


See the discussion [here](https://groups.google.com/forum/#!topic/scoobi-users/ga2N0WDheHw).
