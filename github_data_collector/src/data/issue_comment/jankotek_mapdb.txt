I get NPE periodically, but it always happens _while trying to read when a large number of entries are being written_ .

Attached the stacktrace below.

![image](https://user-images.githubusercontent.com/34095740/72614534-86d17280-3958-11ea-897c-be59e6c8f60d.png)

**Details:**
MapDB version: 3.0.7
MapDB type: transaction enabled fileDB
Is it Reproducing: Always

Hi,

I am getting an NPE in HTreeMap.get, the stack trace is below.

The map is created using this code:
```
_mapdb = DBMaker.fileDB(filePath.toFile()).
            allocateStartSize(ALLOCATE_START_SIZE).
            allocateIncrement(ALLOCATE_INCREMENT).
            closeOnJvmShutdown().
            transactionEnable().
            fileMmapEnableIfSupported().
            make();
        _metricsMap = _mapdb.
            hashMap("metrics", Serializer.STRING, new JacksonSerializer<>(MetricDefinition.class)).
            createOrOpen();
```

The key supplied to get() is non null, and it doesn't exist in the map. This is during load tests and there are many concurrent calls to
```
map.get(String key)
map.put(key, value);
map.values().stream()...
map.size()```

The stack trace:
```
java.lang.NullPointerException: null
	at org.mapdb.volume.ByteBufferVol.getSlice(ByteBufferVol.java:43)
	at org.mapdb.volume.ByteBufferVol.getLong(ByteBufferVol.java:122)
	at org.mapdb.volume.ReadOnlyVolume.getLong(ReadOnlyVolume.java:62)
	at org.mapdb.StoreWAL.getIndexVal(StoreWAL.kt:167)
	at org.mapdb.StoreWAL.get(StoreWAL.kt:499)
	at org.mapdb.IndexTreeListJava.treeGetNonBinary(IndexTreeListJava.java:254)
	at org.mapdb.IndexTreeListJava.treeGet(IndexTreeListJava.java:189)
	at org.mapdb.IndexTreeLongLongMap.get(IndexTreeLongLongMap.kt:82)
	at org.mapdb.HTreeMap.getprotected(HTreeMap.kt:629)
	at org.mapdb.HTreeMap.get(HTreeMap.kt:603)```
Use `createOrOpen()` instead of deprecated `make()` method
I've noticed even using regular value serializer SerializerArray would cause multiple copies of underlying raw bytes. Ideally I'd like to get only 1 copy for thread safety.

What's the simplest way to get `byte[]/ByteBuffer` for key with minimum number (1) memory copy operations from `DirectStore`?

I've done some code checks and looks like it's doable by creating custom deserializer.
Is it safe to assume, what `DataInput2.internalByteBuffer` will have copy already ready if it's retrieved from `DirectStore` or there are more copies of memory one should be aware of?

Thanks!
Cannot compile on Android. This is the error I get:

`Error: Method name '%%%verify$mapdb' in class 'org.mapdb.DB$Maker' cannot be represented in dex format.`

Any idea?
because I am using coroutines 1.3.1 in my project, I get error when I try to depend on mapdb.

    Error:Kotlin: Supertypes of the following classes cannot be resolved. Please make sure you have the required dependencies in the classpath:
    class kotlinx.coroutines.CoroutineExceptionHandler.Key, unresolved supertypes: kotlin.coroutines.CoroutineContext.Key

Can you please upgrade mapdb to use 1.3.1?
Please add updte batch future as in RocksDB https://github.com/facebook/rocksdb/wiki/Transactions

I need WriteBatchWithIndex:  
 Similar to {@link org.rocksdb.WriteBatch} but with a binary searchable  index built for all the keys inserted.  

 Calling put, merge, remove or putLogData calls the same function
  as with {@link org.rocksdb.WriteBatch} whilst also building an index.
 
  A user can call {@link org.rocksdb.WriteBatchWithIndex#newIterator()} to
  create an iterator over the write batch or
  {@link org.rocksdb.WriteBatchWithIndex#newIteratorWithBase(org.rocksdb.RocksIterator)}
  to get an iterator for the database with Read-Your-Own-Writes like capability
 
public class WriteBatchWithIndex extends AbstractWriteBatch

with methods:  

+ getFromBatchAndDB
+ newIteratorWithBase
+ containsBatchAndDB
+ db.write(writeBatch)


Release example as db.fork() here:
https://github.com/Qoracoin/Qora/blob/master/Qora/src/database/DBMap.java
 - see with Patent
```
// Copyright (c) 2011-present, Facebook, Inc.  All rights reserved.
//  This source code is licensed under both the GPLv2 (found in the
//  COPYING file in the root directory) and Apache 2.0 License
//  (found in the LICENSE.Apache file in the root directory).

package org.rocksdb;

/**
 * Similar to {@link org.rocksdb.WriteBatch} but with a binary searchable
 * index built for all the keys inserted.
 *
 * Calling put, merge, remove or putLogData calls the same function
 * as with {@link org.rocksdb.WriteBatch} whilst also building an index.
 *
 * A user can call {@link org.rocksdb.WriteBatchWithIndex#newIterator()} to
 * create an iterator over the write batch or
 * {@link org.rocksdb.WriteBatchWithIndex#newIteratorWithBase(org.rocksdb.RocksIterator)}
 * to get an iterator for the database with Read-Your-Own-Writes like capability
 */
public class WriteBatchWithIndex extends AbstractWriteBatch {
  /**
   * Creates a WriteBatchWithIndex where no bytes
   * are reserved up-front, bytewise comparison is
   * used for fallback key comparisons,
   * and duplicate keys operations are retained
   */
  public WriteBatchWithIndex() {
    super(newWriteBatchWithIndex());
  }


  /**
   * Creates a WriteBatchWithIndex where no bytes
   * are reserved up-front, bytewise comparison is
   * used for fallback key comparisons, and duplicate key
   * assignment is determined by the constructor argument
   *
   * @param overwriteKey if true, overwrite the key in the index when
   *   inserting a duplicate key, in this way an iterator will never
   *   show two entries with the same key.
   */
  public WriteBatchWithIndex(final boolean overwriteKey) {
    super(newWriteBatchWithIndex(overwriteKey));
  }

  /**
   * Creates a WriteBatchWithIndex
   *
   * @param fallbackIndexComparator We fallback to this comparator
   *  to compare keys within a column family if we cannot determine
   *  the column family and so look up it's comparator.
   *
   * @param reservedBytes reserved bytes in underlying WriteBatch
   *
   * @param overwriteKey if true, overwrite the key in the index when
   *   inserting a duplicate key, in this way an iterator will never
   *   show two entries with the same key.
   */
  public WriteBatchWithIndex(
      final AbstractComparator<? extends AbstractSlice<?>>
          fallbackIndexComparator, final int reservedBytes,
      final boolean overwriteKey) {
    super(newWriteBatchWithIndex(fallbackIndexComparator.nativeHandle_,
        fallbackIndexComparator.getComparatorType().getValue(), reservedBytes,
        overwriteKey));
  }

  /**
   * <p>Private WriteBatchWithIndex constructor which is used to construct
   * WriteBatchWithIndex instances from C++ side. As the reference to this
   * object is also managed from C++ side the handle will be disowned.</p>
   *
   * @param nativeHandle address of native instance.
   */
  WriteBatchWithIndex(final long nativeHandle) {
    super(nativeHandle);
    disOwnNativeHandle();
  }

  /**
   * Create an iterator of a column family. User can call
   * {@link org.rocksdb.RocksIteratorInterface#seek(byte[])} to
   * search to the next entry of or after a key. Keys will be iterated in the
   * order given by index_comparator. For multiple updates on the same key,
   * each update will be returned as a separate entry, in the order of update
   * time.
   *
   * @param columnFamilyHandle The column family to iterate over
   * @return An iterator for the Write Batch contents, restricted to the column
   * family
   */
  public WBWIRocksIterator newIterator(
      final ColumnFamilyHandle columnFamilyHandle) {
    return new WBWIRocksIterator(this, iterator1(nativeHandle_,
            columnFamilyHandle.nativeHandle_));
  }

  /**
   * Create an iterator of the default column family. User can call
   * {@link org.rocksdb.RocksIteratorInterface#seek(byte[])} to
   * search to the next entry of or after a key. Keys will be iterated in the
   * order given by index_comparator. For multiple updates on the same key,
   * each update will be returned as a separate entry, in the order of update
   * time.
   *
   * @return An iterator for the Write Batch contents
   */
  public WBWIRocksIterator newIterator() {
    return new WBWIRocksIterator(this, iterator0(nativeHandle_));
  }

  /**
   * Provides Read-Your-Own-Writes like functionality by
   * creating a new Iterator that will use {@link org.rocksdb.WBWIRocksIterator}
   * as a delta and baseIterator as a base
   *
   * Updating write batch with the current key of the iterator is not safe.
   * We strongly recommand users not to do it. It will invalidate the current
   * key() and value() of the iterator. This invalidation happens even before
   * the write batch update finishes. The state may recover after Next() is
   * called.
   *
   * @param columnFamilyHandle The column family to iterate over
   * @param baseIterator The base iterator,
   *   e.g. {@link org.rocksdb.RocksDB#newIterator()}
   * @return An iterator which shows a view comprised of both the database
   * point-in-time from baseIterator and modifications made in this write batch.
   */
  public RocksIterator newIteratorWithBase(
      final ColumnFamilyHandle columnFamilyHandle,
      final RocksIterator baseIterator) {
    RocksIterator iterator = new RocksIterator(baseIterator.parent_,
        iteratorWithBase(
            nativeHandle_, columnFamilyHandle.nativeHandle_, baseIterator.nativeHandle_));
    // when the iterator is deleted it will also delete the baseIterator
    baseIterator.disOwnNativeHandle();
    return iterator;
  }

  /**
   * Provides Read-Your-Own-Writes like functionality by
   * creating a new Iterator that will use {@link org.rocksdb.WBWIRocksIterator}
   * as a delta and baseIterator as a base. Operates on the default column
   * family.
   *
   * @param baseIterator The base iterator,
   *   e.g. {@link org.rocksdb.RocksDB#newIterator()}
   * @return An iterator which shows a view comprised of both the database
   * point-in-timefrom baseIterator and modifications made in this write batch.
   */
  public RocksIterator newIteratorWithBase(final RocksIterator baseIterator) {
    return newIteratorWithBase(baseIterator.parent_.getDefaultColumnFamily(), baseIterator);
  }

  /**
   * Similar to {@link RocksDB#get(ColumnFamilyHandle, byte[])} but will only
   * read the key from this batch.
   *
   * @param columnFamilyHandle The column family to retrieve the value from
   * @param options The database options to use
   * @param key The key to read the value for
   *
   * @return a byte array storing the value associated with the input key if
   *     any. null if it does not find the specified key.
   *
   * @throws RocksDBException if the batch does not have enough data to resolve
   * Merge operations, MergeInProgress status may be returned.
   */
  public byte[] getFromBatch(final ColumnFamilyHandle columnFamilyHandle,
      final DBOptions options, final byte[] key) throws RocksDBException {
    return getFromBatch(nativeHandle_, options.nativeHandle_,
        key, key.length, columnFamilyHandle.nativeHandle_);
  }

  /**
   * Similar to {@link RocksDB#get(byte[])} but will only
   * read the key from this batch.
   *
   * @param options The database options to use
   * @param key The key to read the value for
   *
   * @return a byte array storing the value associated with the input key if
   *     any. null if it does not find the specified key.
   *
   * @throws RocksDBException if the batch does not have enough data to resolve
   * Merge operations, MergeInProgress status may be returned.
   */
  public byte[] getFromBatch(final DBOptions options, final byte[] key)
      throws RocksDBException {
    return getFromBatch(nativeHandle_, options.nativeHandle_, key, key.length);
  }

  /**
   * Similar to {@link RocksDB#get(ColumnFamilyHandle, byte[])} but will also
   * read writes from this batch.
   *
   * This function will query both this batch and the DB and then merge
   * the results using the DB's merge operator (if the batch contains any
   * merge requests).
   *
   * Setting {@link ReadOptions#setSnapshot(long, long)} will affect what is
   * read from the DB but will NOT change which keys are read from the batch
   * (the keys in this batch do not yet belong to any snapshot and will be
   * fetched regardless).
   *
   * @param db The Rocks database
   * @param columnFamilyHandle The column family to retrieve the value from
   * @param options The read options to use
   * @param key The key to read the value for
   *
   * @return a byte array storing the value associated with the input key if
   *     any. null if it does not find the specified key.
   *
   * @throws RocksDBException if the value for the key cannot be read
   */
  public byte[] getFromBatchAndDB(final RocksDB db, final ColumnFamilyHandle columnFamilyHandle,
      final ReadOptions options, final byte[] key) throws RocksDBException {
    return getFromBatchAndDB(nativeHandle_, db.nativeHandle_,
        options.nativeHandle_, key, key.length,
        columnFamilyHandle.nativeHandle_);
  }

  /**
   * Similar to {@link RocksDB#get(byte[])} but will also
   * read writes from this batch.
   *
   * This function will query both this batch and the DB and then merge
   * the results using the DB's merge operator (if the batch contains any
   * merge requests).
   *
   * Setting {@link ReadOptions#setSnapshot(long, long)} will affect what is
   * read from the DB but will NOT change which keys are read from the batch
   * (the keys in this batch do not yet belong to any snapshot and will be
   * fetched regardless).
   *
   * @param db The Rocks database
   * @param options The read options to use
   * @param key The key to read the value for
   *
   * @return a byte array storing the value associated with the input key if
   *     any. null if it does not find the specified key.
   *
   * @throws RocksDBException if the value for the key cannot be read
   */
  public byte[] getFromBatchAndDB(final RocksDB db, final ReadOptions options,
      final byte[] key) throws RocksDBException {
    return getFromBatchAndDB(nativeHandle_, db.nativeHandle_,
        options.nativeHandle_, key, key.length);
  }

  @Override protected final native void disposeInternal(final long handle);
  @Override final native int count0(final long handle);
  @Override final native void put(final long handle, final byte[] key,
      final int keyLen, final byte[] value, final int valueLen);
  @Override final native void put(final long handle, final byte[] key,
      final int keyLen, final byte[] value, final int valueLen,
      final long cfHandle);
  @Override final native void merge(final long handle, final byte[] key,
      final int keyLen, final byte[] value, final int valueLen);
  @Override final native void merge(final long handle, final byte[] key,
      final int keyLen, final byte[] value, final int valueLen,
      final long cfHandle);
  @Override final native void delete(final long handle, final byte[] key,
      final int keyLen) throws RocksDBException;
  @Override final native void delete(final long handle, final byte[] key,
      final int keyLen, final long cfHandle) throws RocksDBException;
  @Override final native void singleDelete(final long handle, final byte[] key,
      final int keyLen) throws RocksDBException;
  @Override final native void singleDelete(final long handle, final byte[] key,
      final int keyLen, final long cfHandle) throws RocksDBException;
  @Override
  final native void deleteRange(final long handle, final byte[] beginKey, final int beginKeyLen,
      final byte[] endKey, final int endKeyLen);
  @Override
  final native void deleteRange(final long handle, final byte[] beginKey, final int beginKeyLen,
      final byte[] endKey, final int endKeyLen, final long cfHandle);
  @Override final native void putLogData(final long handle, final byte[] blob,
      final int blobLen) throws RocksDBException;
  @Override final native void clear0(final long handle);
  @Override final native void setSavePoint0(final long handle);
  @Override final native void rollbackToSavePoint0(final long handle);
  @Override final native void popSavePoint(final long handle) throws RocksDBException;
  @Override final native void setMaxBytes(final long nativeHandle,
      final long maxBytes);
  @Override final native WriteBatch getWriteBatch(final long handle);

  private native static long newWriteBatchWithIndex();
  private native static long newWriteBatchWithIndex(final boolean overwriteKey);
  private native static long newWriteBatchWithIndex(
      final long fallbackIndexComparatorHandle,
      final byte comparatorType, final int reservedBytes,
      final boolean overwriteKey);
  private native long iterator0(final long handle);
  private native long iterator1(final long handle, final long cfHandle);
  private native long iteratorWithBase(
      final long handle, final long baseIteratorHandle, final long cfHandle);
  private native byte[] getFromBatch(final long handle, final long optHandle,
      final byte[] key, final int keyLen);
  private native byte[] getFromBatch(final long handle, final long optHandle,
      final byte[] key, final int keyLen, final long cfHandle);
  private native byte[] getFromBatchAndDB(final long handle,
      final long dbHandle,  final long readOptHandle, final byte[] key,
      final int keyLen);
  private native byte[] getFromBatchAndDB(final long handle,
      final long dbHandle, final long readOptHandle, final byte[] key,
      final int keyLen, final long cfHandle);
}

```
The secondary title of Mapdb states:

> MapDB provides concurrent Maps, Sets and Queues...

But I can't find any way to create a queue

When I searched I found in the tests here a QueueMaker, but I can't use it in my code (maybe it doesn't exist in v3.XX ? )

I am using latest stable (v3.0.7)


How can I create a simple queue ?
I've analysed your codebase and noticed that `org.mapdb.io.DataIO` is not fully tested.
I've written some tests for the methods in this class with the help of [Diffblue Cover](https://www.diffblue.com/opensource).

Hopefully, these tests will help you detect any regressions caused by future code changes. If you would find it useful to have additional tests written for this repository, I would be more than happy to look at other classes that you consider important in a subsequent PR.
Including MapDB dependency triggers sonatype repository scanning from time to time:

    Downloading from sonatype-nexus-snapshots: https://oss.sonatype.org/content/repositories/snapshots/org/eclipse/collections/eclipse-collections-api/maven-metadata.xml
    Downloading from central: https://repo.maven.apache.org/maven2/org/eclipse/collections/eclipse-collections-api/maven-metadata.xml
    Downloaded from central: https://repo.maven.apache.org/maven2/org/eclipse/collections/eclipse-collections-api/maven-metadata.xml (1.2 kB at 956 B/s)

And as far as I can tell, MapDB depends on both the snapshot and released versions of eclipse-collections.