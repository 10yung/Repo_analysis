I find Hpatches datasets already,but Brown datasets is hard to find.Where I can download this datasets?
Thanks!
Hi, your work is great and I was inspired, but I have a little problem when I read the paper,Why is gkk the smallest in the third term of the loss function?Shouldn't the inner product between the features of the matching block be the largest?
1.What is the Browndataset URL?
2.Since I have not been in touch with matlab to realize deep learning, please tell me the specific operation instruction of this project.
thanks.
there are some statements needed to make sure:

1. while the weighting and bias are fixed at 1 and 0 respectfully,###  the mean and var in it should be learned in the training process. Right?

2.the author state in the paper that the bn layer before DIF is not learned. Then it is the mean and var are calculated every time for different input?
I am really interesting in your L2-NET work, and want to reproduce the training process.
Is the training code available or can I get access to it?

Thanks a lot.
Hi @yuruntian, I read your paper and found it very interseting.
I'm trying to reproduce your results using tensorflow.
specifically,I'm trying to take the model trained on HPatches(with augmentation) and test it  on Brown dataset.
I ported the weights from matconvnet into tensorflow,and followed the exact architecture.
The tensorflow descriptor works quite well for feature matching tasks, so I'm guessing I plugged in the weights correctly.
I also followed the Brown evaluation method and report FPN @ recall=0.95
in this case, however, I'm getting quite different results:
20% FPN @ recall=0.95 on liberty(vs. 3.2% you reported in the paper)
So I guess I must be doing something bad in the evaluation code.
Can you share or point me to the evaluation code you were using?
Also, can you elaborate more on how you measured yourself on brown dataset (patch size, special tweaks you had to do, etc..)?

Thanks,
Guy
