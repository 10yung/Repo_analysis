Description
===========
Hi, I am trying to use connect to the broker via SASL_SSL on a Window Machine.

But I got error saying InitializeSecurityContext failed: Target unknown

Note: I am able to connect to the broker via SSL.


How to reproduce
================
Operating System: Windows 7
Confluent Kafka Dotnet version: 1.3.0
Apache Kafka version: Confluent 5.2.2 (Apache Kafka 2.2.1)

Producer configuration:

                BootstrapServers ="broker:6668",
                SaslMechanism = SaslMechanism.Gssapi,
                SecurityProtocol = SecurityProtocol.SaslSsl,
                SslCaLocation = "./cert.pem",
                SslCertificateLocation = "./cert.pem",
                SslKeyLocation = "./cert.key",
                SaslKerberosServiceName = "service",
                SaslKerberosPrincipal = "service/principle@domain.com",
                ApiVersionRequest = false,
                Debug = "all"
Log
=========

%7|1579192615.371|SEND|rdkafka#producer-1| [thrd:sasl_ssl://broker:6668/bootstrap]: sasl_ssl://broker:6668/bootstrap: Sent SaslHandshakeRequest (v1, 29 bytes @ 0, CorrId 10)
%7|1579192615.390|RECV|rdkafka#producer-1| [thrd:sasl_ssl://broker:6668/bootstrap]: sasl_ssl://broker:6668/bootstrap: Received SaslHandshakeResponse (v1, 14 bytes, CorrId 10, rtt 18.89ms)
%7|1579192615.392|SASLMECHS|rdkafka#producer-1| [thrd:sasl_ssl://broker:6668/bootstrap]: sasl_ssl://broker:6668/bootstrap: Broker supported SASL mechanisms: GSSAPI
%7|1579192615.394|AUTH|rdkafka#producer-1| [thrd:sasl_ssl://broker:6668/bootstrap]: sasl_ssl://broker:6668/bootstrap: Auth in state AUTH_HANDSHAKE (handshake supported)
%7|1579192615.395|STATE|rdkafka#producer-1| [thrd:sasl_ssl://broker:6668/bootstrap]: sasl_ssl://broker:6668/bootstrap: Broker changed state AUTH_HANDSHAKE -> AUTH_REQ
%7|1579192615.398|BROADCAST|rdkafka#producer-1| [thrd:sasl_ssl://broker:6668/bootstrap]: Broadcasting state change
%7|1579192615.398|SASL|rdkafka#producer-1| [thrd:sasl_ssl://broker:6668/bootstrap]: sasl_ssl://broker:6668/bootstrap: Initializing SASL client: service name esaas, hostname broker, mechanisms GSSAPI, provider Win32 SSPI

%7|1579192615.401|SASL|rdkafka#producer-1| [thrd:sasl_ssl://broker:6668/bootstrap]: sasl_ssl://broker:6668/bootstrap: Acquired Kerberos credentials handle (expiry in 2147483404.-322922497s)
%7|1579192615.402|BROKERFAIL|rdkafka#producer-1| [thrd:sasl_ssl://broker:6668/bootstrap]: sasl_ssl://broker:6668/bootstrap: failed: err: Local: Authentication failure: (errno: Invalid argument)
%3|1579192615.404|FAIL|rdkafka#producer-1| [thrd:sasl_ssl://broker:6668/bootstrap]: sasl_ssl://broker:6668/bootstrap: Failed to initialize SASL authentication: InitializeSecurityContext failed: Target unknown (0x80090303) (after 6ms in state AUTH
_REQ)
%3|1579192615.407|ERROR|rdkafka#producer-1| [thrd:sasl_ssl://broker:6668/bootstrap]: sasl_ssl://broker:6668/bootstrap: Failed to initialize SASL authentication: InitializeSecurityContext failed: Target unknown (0x80090303) (after 6ms in state AUTH_REQ)

Checklist
=========

Please provide the following information:
 - [ ] A complete (i.e. we can run it), minimal program demonstrating the problem. No need to supply a project file.
 - [x] Confluent.Kafka nuget version.
 - [x] Apache Kafka version.
 - [x] Client configuration.
 - [x] Operating system.
 - [x] Provide logs (with "debug" : "..." as necessary in configuration).
 - [ ] Provide broker log excerpts.
 - [x] Critical issue.

Question,
Is it possible to read multiple messages/stream of bytes from kafka topic ?

Right know I can't find any information regardless consume bytes of array/ multiple messages at once

Since consuming each message individually takes a lot of time.

If such case is impossible, what's the best solution would be to consume a lot of data (50gb) each day

related to #1160

This version just exposes the `Consume(int)` method.

Unfortunately moving the CancellationToken overload to an extension method does not work as it needs access to `cancellationDelayMaxMs` which is pulled from config in the `Consumer` impl. So not available to the extension in a "nice" way.
I did a search in github for  "dotnet.cancellation.delay.max.ms". Only a single hit outside Confluent.Kafka. I haven't seen anyone in "Corp world" use it either. 100ms seems like a reasonable value. We could just expose it as a default param on the method? And remove the config option, leave it up to the dev?

Also the `Consume(TimeStamp)` overload is referred to all over the place in xmldoc. Mostly codegen so probably not that big of a deal.

I do like extensions for this kind of thing. But not as nice as I'd thought in this case.
Keeping it simple for now. I may do another PR with the above changes, just to show what it would look like
Hi everyone,

Recently i got some issue when i was facing a disconnection or a coordination load in progress.

> failed: No connection could be made because the target machine actively refused
Error: GroupCoordinator: Connect to ipv4#XXXXXXXXXXX:9092 failed: No connection could be made because the target machine actively refused it... (after 2001ms in state CONNECT)
Revoking assignment: XXXXXXX@16829]
Assigned partitions: [XXXXXX [[0]]

After that, messages are not consumed anymore and just my program is waiting.

Sometimes also : 

> Revoking partition
> Coordinator load in progress
> Assigning partition

Is it a know issue of the version (I recently updated from 1.0.0 or 1.0.1) ? Did i do something wrong or do i have to do something more ?

Thank you for your kind help !

- Confluent.Kafka nuget version : 1.3.0
- Client configuration : 
```
        private ConsumerConfig _configKafka => new ConsumerConfig
        {
            BootstrapServers = ConfigurationManager.AppSettings["BrokerList"],
            GroupId = "app", // DO NOT CHANGE THIS
            EnableAutoCommit = false,
            SessionTimeoutMs = 20000,
            MaxPollIntervalMs = 600000,
            AutoOffsetReset = AutoOffsetReset.Earliest
        };
```
- Operating system : Windows server
Description
===========
Hi!

I found a way to get a consumer's topic lag by reading the internal property _OutQueueLength_ found in the internal _LibrdkafkaHandle_ _Handle_ property of the consumer. Is this a "safe" hack?
@mhowlett, can you please explain me why this property was not publicly exposed in the library?
If this version is not safe is there another way to accomplish this?
Thank you, 
Alex

Code:
```
var propertyLevel1 = _consumer.Handle.GetType().GetProperty("LibrdkafkaHandle", BindingFlags.NonPublic | BindingFlags.Instance);
var valueLevel1 = propertyLevel1.GetValue(_consumer.Handle);

var propertyLevel2 = valueLevel1.GetType().GetProperty("OutQueueLength", BindingFlags.NonPublic | BindingFlags.Instance);
var valueLevel2 = propertyLevel2.GetValue(valueLevel1);
                        
topicLag = Convert.ToInt64(valueLevel2);
```



Checklist
=========

Please provide the following information:
 - [ ] A complete (i.e. we can run it), minimal program demonstrating the problem. No need to supply a project file.
 - [x] Confluent.Kafka nuget version: 1.3.0 
 - [x] Apache Kafka version: 2.x
 - [ ] Client configuration.
 - [x] Operating system : Linux 
 - [ ] Provide logs (with "debug" : "..." as necessary in configuration).
 - [ ] Provide broker log excerpts.
 - [ ] Critical issue.

Description
===========

So we are trying to port our application to .net core 3.1, but when we generate the docker image with .net core 3 as base image it fails because of Confluent.Kafka do not support .net core 3.

Can someone give me some information if the team is working on this or if there is any plan for soon to give support to .net core 3 .


Checklist
=========

Please provide the following information:
 - [ ] A complete (i.e. we can run it), minimal program demonstrating the problem. No need to supply a project file.
 - [X] Confluent.Kafka nuget version.
 - [ ] Apache Kafka version.
 - [ ] Client configuration.
 - [ ] Operating system.
 - [ ] Provide logs (with "debug" : "..." as necessary in configuration).
 - [ ] Provide broker log excerpts.
 - [] Critical issue.
How to Fetch the message value using Key value used when producing 
Description
===========

Some time my consumer throws ConsumeException:

> Confluent.Kafka.ConsumeException: Local: Maximum application poll interval (max.poll.interval.ms) exceeded

 >  at Confluent.Kafka.Consumer`2.ConsumeImpl[K,V](Int32 millisecondsTimeout, IDeserializer`1 keyDeserializer, IDeserializer`1 valueDeserializer)
   at Confluent.Kafka.Consumer`2.Consume(TimeSpan timeout)

The log informs:

> Application maximum poll interval (300000ms) exceeded by 2134298747ms (adjust max.poll.interval.ms for long-running message processing): leaving group 


and the consumer stops receiving new messages (consume() returns null).

Is there are a way of detect when max.poll.interval.ms occours?

Is there another way besides try to check the exception message?

I was searching and found [931](https://github.com/confluentinc/confluent-kafka-dotnet/issues/931).

[2316](https://github.com/edenhill/librdkafka/pull/2316) has been released?


Lib Version: 1.2.2.0




resolves #1154 
https://github.com/confluentinc/confluent-kafka-dotnet/blob/2115c9d5d7d074b01ea9f3f79c5e773c99161bb3/src/Confluent.Kafka/Impl/SafeKafkaHandle.cs#L1026


=>        internal List<GroupInfo> ListGroups(int millisecondsTimeout)