Hi!
Can you please look into adding Log Rotation into StatsD?
Perhaps configurable by file size. Even better, add something a-la MongoDB, where sending a SIGUSR1 command rotates the log file.

Thank you!

We have an application reporting the current number of in-flight requests as a gauge, and the value is reported every time a request is added to the queue or removed from it. This generates a lot of data, but statsd only reports the last value it has seen before the flush. What we'd like to see is an average of all values received since the last flush, so that if a spike occurs _during_ an interval, we can spot it on the graphs.

We've worked around this for now by (ab)using timers, but it would be nice to have some sort of an official, non-hacky way to support this use case.

With the console backend enabled on, with the reference (example) config, I'm seeing a double-flush every 30 seconds (every third flush on a 20-second interval).

```
Flushing stats at  Fri Sep 27 2019 23:29:46 GMT+0000 (Coordinated Universal Time)
{ counters:
   { 'statsd.bad_lines_seen': 0,
     'statsd.packets_received': 0,
     'statsd.metrics_received': 0 },
  timers: {},
  gauges: { 'statsd.timestamp_lag': 0 },
  timer_data: {},
  counter_rates:
   { 'statsd.bad_lines_seen': 0,
     'statsd.packets_received': 0,
     'statsd.metrics_received': 0 },
  sets: {},
  pctThreshold: [ 90 ] }
Flushing stats at  Fri Sep 27 2019 23:29:46 GMT+0000 (Coordinated Universal Time)
{ counters:
   { 'statsd.bad_lines_seen': 0,
     'statsd.packets_received': 0,
     'statsd.metrics_received': 0 },
  timers: {},
  gauges: { 'statsd.timestamp_lag': -10 },
  timer_data: {},
  counter_rates:
   { 'statsd.bad_lines_seen': 0,
     'statsd.packets_received': 0,
     'statsd.metrics_received': 0 },
  sets: {},
  pctThreshold: [ 90 ] }
```

See the related issue on the Graphite project: https://github.com/graphite-project/docker-graphite-statsd/issues/52
Hello,

we would like to add port 8125 into SELinux, however it's not officially assigned by IANA. While this is not required for port to be added into the base SELinux port policy, I would like to ask the community to consider filing application form at https://www.iana.org/form/ports-services

The port is currently free to use and the workflow is relatively fast and easy. It can prevent from port conflicts in the future. Thanks. Cheers!
I have a setup where applications are sending metrics to 2 different statsd. Both Statsd are sending to a graphite backend. 
I decide to test repeater backend on one of 2 statsd in order to copy all metrics to statsd-exporter and expose some metrics in prometheus format. The scenario is the following using statsd version v0.8.5, STATSD1 and STATSD2 are running on different servers. Everything is running in a docker.
			
```
                 |--UDP--> STATSD1 -UDP-> GRAPHITE-BACKEND-1
                 |
APPLICATION -----|
                 |					 
                 |                  |-TCP--> REPEATER-BACKEND --> STATSD-EXPORTER
                 |--UDP---> STATSD2-|
                                    |-UDP--> GRAPHITE-BACKEND-2
```

What is happening here is that, when I activate "REPEATER-BACKEND" on STATSD2, the data sent to GRAPHITE-BACKEND-2 are different from the data sent to GRAPHITE-BACKEND-1 and also seems some data are lost. If I disable REPEATER-BACKEND, GRAPHITE-BACKEND-1 and GRAPHITE-BACKEND-2 receive same data and everything is working as expected. 
Here the configuration that I used on STATSD2:

STATSD2
```
{
  port: 8125,
  mgmt_port: 8126,

  percentThreshold: [ 50, 75, 90, 95, 98, 99, 99.9, 99.99, 99.999],

  graphitePort: 2003,
  graphiteHost: "relay",
  flushInterval: 10000,
  deleteIdleStats: true,
  debug: false,

  backends: [ "./backends/repeater", "./backends/graphite" ],
  repeater: [ { host: 'statsd-exporter', port: 9125 } ],
  repeaterProtocol: "tcp",

  graphite: {
    legacyNamespace: false
  }
}

```
 
 I tried also with another scenario as following: 

 ```
                 |--UDP--> STATSD1 -UDP-> GRAPHITE-BACKEND-1
                 |
APPLICATION ----|
                 |
                 | 
                 |                  |-TCP--> REPEATER-BACKEND-1 --> STATSD-EXPORTER  
                 |--UDP--> STATSD2 -| 
                                    |-TCP--> REPEATER-BACKEND-2 --> STATSD-GRAPHITE-1 -UDP-> GRAPHITE-BACKEND-2
```

With following configurations:

STATSD2
```
{
  port: 8125,
  mgmt_port: 8126,

  percentThreshold: [ 50, 75, 90, 95, 98, 99, 99.9, 99.99, 99.999],

  flushInterval: 10000,
  deleteIdleStats: true,
  debug: false,

  backends: [ "./backends/repeater" ],
  repeater: [ { host: 'statsd-exporter', port: 9125 }, { host: 'statsd-graphite', port: 10125 } ],
  repeaterProtocol: "tcp",

  graphite: {
    legacyNamespace: false
  }
}
```

STATSD-GRAPHITE-1
```
{
  port: "10125",
  mgmt_port: 10126,

  percentThreshold: [ 50, 75, 90, 95, 98, 99, 99.9, 99.99, 99.999],

  graphitePort: 2003,
  graphiteHost: "relay",
  flushInterval: 10000,
  deleteIdleStats: true,
  debug: false,
  
  server: "./servers/tcp",

  backends: [ "./backends/graphite" ],

  graphite: {
    legacyNamespace: false
  }
}
```

But the values sent are different. Here you can find screenshot of data read from 

GRAPHITE-BACKEND-1

![Screenshot 2019-07-29 at 11 03 05](https://user-images.githubusercontent.com/1786652/62036882-f7da6d80-b1f2-11e9-9243-59faf4af954c.png)

GRAPHITE-BACKEND-2

![Screenshot 2019-07-29 at 11 02 59](https://user-images.githubusercontent.com/1786652/62036898-02950280-b1f3-11e9-9694-5bd6138eb173.png)

Do you have any idea why this is happening or have any suggestions on ways to troubleshoot that ?

Thank you.
Right now we support stdout and local system syslog (as opposed to a remote syslog). I'd like to propose a plugable logging system where you'd pass in a logger object that you create in the config file. (including examples)

The would work around the issue of needing optionalDeps at all and would remove `modern-syslog` from the build allowing people to use any logging system they want (remote syslog, winston, pino, etc)

I'm volunteering to do this, but It's a major breaking change so I wanted to solicit feedback first.
Currently there are over a dozen metrics that get sent out for every timer stat that comes in. Being able to filter on the specific aggregated metrics you want for a timer at the config level will allow for drastic reduction in the data sent out from statsd and stored.

The config that I've added is `calculated_timer_metrics` which by default will send all metrics, however once any other value(s) are added, it will only send those specified. This also allows for the not sending percentile metrics as well.

Currently running these changes in production has reduced our Carbon/whisper load by ~20%.

Related to this issue:
https://github.com/statsd/statsd/issues/235
This adds a configuration option for skipping the calculation of counter rates. This can save a lot of metric volume if the user uses a consistent flushInterval and would rather calculate rate at query time.
This is the followup to https://github.com/statsd/statsd/issues/649. The transfer is now done, but we still have an open call for maintainers. If you are genuinely interested in becoming a maintainer, start contributing now by e.g. commenting on issues, suggesting labels, testing PRs, writing PRs. This will help with becoming a maintainer as it

* Provides a history of contributing to the project.
* Gives you an idea of what is involved to be a maintainer, so you will know if it's something you actually are still interested in.

Then follow the steps in the [maintainers doc](https://github.com/statsd/statsd/blob/master/MAINTAINERS.md).