- Flink to 1.2.1
- Tensorflow to 1.2.0-rc1
- Added Installation instructions in SBT and Maven in readme.
Add saved model for MNIST_dense that includes a prediction signature.
Create a PredictionMethod to use the prediction signature.
Create a test for the saved model and prediction signature.
Implement out-of-box interoperability with TF Serving models.   Given a model that works in TF Serving, make it easy to use directly in a Flink program, e.g.:
- implementations of "predict" and "classification" signature
- MapFunction implementations w/ appropriate types (`tf.Example`, `Array[Byte]`)
- Example code

Some snippets to look at:
- [Serving a TensorFlow Model](https://github.com/tensorflow/serving/blob/0.5.1/tensorflow_serving/g3doc/serving_basic.md)
- [mnist_saved_model.py](https://github.com/tensorflow/serving/blob/0.5.1/tensorflow_serving/example/mnist_saved_model.py#L97)
- [API definitions](https://github.com/tensorflow/serving/tree/0.5.1/tensorflow_serving/apis)
Make it possible to define TF graphs from scratch, in addition to existing support for using externally-developed models.  

Because the object model of TensorFlow graphs is extensive and ever-growing, we should hesitate to introduce a Flink-specific DSL.   We should consider creating or incorporating a pure Scala TensorFlow library where such a DSL could be developed.


The library today has numerous layers of functionality that could benefit from refactoring, to reduce dependencies and clarify concepts.   The conceptual layering (from bottom up):

1. Scala language bindings
a. focus on using the TensorFlow library in Scala (not Flink specific)
2. Core Flink integration code
a. graph/model loaders
b. tensors in the Flink type system (e.g. `TensorValue`, type tags)
c. core lifecycle classes (e.g. `ModelAwareFunction`)
d. (future) integration of graph state with Flink managed state
3. ML specializations
a. representations of specific ML signatures ('classify', 'regress', 'predict')
b. TF Serving interoperability
c. FlinkML integration
d. converters for specific datatypes (e.g. `LabeledVector`, `Example`)

Ideally the core module (2) would shed its dependency on 'twitter-bijection', though it may still be useful in (3).
Let's prove that serving TF models in-process has a performance advantage over making RPC calls to TF Serving.
Make it possible to use TensorFlow models as elements of a FlinkML pipeline (as described [here](https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/libs/ml/pipelines.html)).   Possible features include:

- Use TF models as FlinkML transformers and predictors
- Support FlinkML datatypes
- Train new models to be used for inference and/or exported
- Integrate graph state into Flink operator state
- Support checkpointing
- Support keyed streams