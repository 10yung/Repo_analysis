Would it be possible to add a notice at the beginning of the readme to warn that this datasource is merged into spark 2.4 and all users of spark 2.4+ should not use it?
When I have a schema which has a reference to itself, it causes an infinite recursion and thus a StackOverflowError, SchemaConverters should have some sort of bail-out if it reads a class more than once.
We are using Spark 1.6 in our clusters and want to use this library to read avro files. As part of reading the avro files, we want to able to supply a different read schema so that we can handle schema evolutions. The version suggested for Spark 1.6 (version=2.0.1) does not support this.

Is there a version which works with Spark 1.6 and also supports this feature ?
I've created a PR to support few logical types and was very surprised that Java 7 is still being targeted by the tests.
I think it is time leave Java 7 legacy behind.
For databricks 2.0.1 with spark 1.6, While Reading a Complex AVRO with below Code 

    **SparkConf conf = new SparkConf().setAppName("SearchAVRO").setMaster("local[*]");
    JavaSparkContext sc = new JavaSparkContext(conf);      
    SQLContext sqlContext = new SQLContext(sc);

    // Creates a DataFrame from a specified file
    DataFrame df = sqlContext.read().format("com.databricks.spark.avro").load(inputPath);**


We get an exception:

**Exception in thread "main" java.lang.UnsupportedOperationException: This mix of union types is not supported (see README): ArrayBuffer(RECORD) at com.databricks.spark.avro.SchemaConverters$.toSqlType(SchemaConverters.scala:88) at com.databricks.spark.avro.SchemaConverters$.toSqlType(SchemaConverters.scala:63) at com.databricks.spark.avro.SchemaConverters$.toSqlType(SchemaConverters.scala:79) at com.databricks.spark.avro.SchemaConverters$$anonfun$1.apply(SchemaConverters.scala:56)**

==========================================================
Similar code works properly for databricks 4.0.0 with Spark 2.2

Can you please add support for the same in databricks 2.0.1??
When I read simple avro file with all fileds non-nullable, resulted dataframe schema has all fields nullable.

avro file schema:
```
{
  "type" : "record",
  "name" : "RobotDetection",
  "namespace" : "cz.search.robotdetection",
  "fields" : [ {
    "name" : "sessionId",
    "type" : "string"
  }, {
    "name" : "robotDetectionResult",
    "type" : "int"
  } ]
}
```
dataframe schema:
```
val a = spark.read.format("com.databricks.spark.avro").load("avrofile.avro")
a.schema
res1: org.apache.spark.sql.types.StructType = StructType(StructField(sessionId,StringType,true), StructField(robotDetectionResult,IntegerType,true))
```
I was taking a quick look through [DefaultSource.scala](https://github.com/databricks/spark-avro/blob/branch-4.0/src/main/scala/com/databricks/spark/avro/DefaultSource.scala#L123-L143) and it seems there is only support for snappy and deflate. I believe Avro has support for xz and bzip2 as well.
Hello,

`Caused by: org.apache.avro.AvroRuntimeException: Unknown datum type [Ljava.lang.Object;: [Ljava.lang.Object;@616f81b5
	at org.apache.avro.generic.GenericData.getSchemaName(GenericData.java:636)
	at org.apache.avro.specific.SpecificData.getSchemaName(SpecificData.java:265)
	at org.apache.avro.generic.GenericData.resolveUnion(GenericData.java:601)
	at org.apache.avro.generic.GenericDatumWriter.resolveUnion(GenericDatumWriter.java:151)
	at org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:71)
	at org.apache.avro.generic.GenericDatumWriter.writeField(GenericDatumWriter.java:114)
	at org.apache.avro.generic.GenericDatumWriter.writeRecord(GenericDatumWriter.java:104)
	at org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:66)
	at org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:58)`

The above exception is triggered in the following scenario:
`case class Model(params: Option[List[String]])`

The above case class generates the following schema:
`{"type":"record","name":"Model","namespace":"Test","fields":[{"name":"params","type":[{"type":"array","items":["string","null"]},"null"]}]}`

Now, when I create my converterToAvro:
`val structType: StructType = Encoders.product[Model].schema`
`val converter = createConverterToAvro(structType, recordName, recordNamespace)`

...and try to generate my genericRecord:
`val record: GenericRecord = converter(item).asInstanceOf[GenericRecord]`

...I get the above exception!

This happens because in the implementation of `AvroOutputWriter.createConverterToAvro`, in the `case ArrayType`, we have the following:
`val targetArray = new Array[Any](sourceArraySize)`

...and `GenericData.getSchemaName` does this check:
`if (isArray(datum)) return Type.ARRAY.getName();`
`protected boolean isArray(Object datum) { return datum instanceof Collection;}`

Now `scala.Array` is not an instance of `Collection` and it will fail gracefully. 
In order to fix this, we can use `java.util.ArrayList`!!!
This is not an issue. It is a request to improve README documentation. I am not sure where do I put it.

[spark-csv](https://github.com/databricks/spark-csv) [#Features](https://github.com/databricks/spark-csv#features) says : API accepts several options. A list of accepted options is followed with brief description. This is very intuitive to understand supported options.

Can we please have such a list for spark-avro as well. I know it is mentioned in README but it looks scattered and is not like a list.
This pr saves on string conversions by explicitly making the generic datum reader read strings instead of utf-8. 
Also, the converter copies every object from the record read by the reader but the iterator never actually reused any record instance. Now record is reused per iterator for efficient memory reuse