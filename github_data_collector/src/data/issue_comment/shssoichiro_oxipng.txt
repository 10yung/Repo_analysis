When tested on a larger number of images, level 4 has a lower average compression ratio than level 3. This really shouldn't happen, but I am not quite sure why it does (perhaps something wrong with the evaluation code or alpha reduction itself). In general it might be a good idea to collect numbers on how each level performs to make it easy to compare performance and compression ratio numbers.

Given that my free time is limited and split between many things, and that oxipng has grown quite popular, I'm looking for 1-2 volunteers to help maintain the repository--preferably individuals who have been involved in oxipng development. This would primarily involve triaging issues and reviewing PRs, and may also involve publishing releases.
This improves runtime by 8% on my benchmark. Surprisingly, it only causes output files to be a fraction of a percent larger. I feel like this compression loss definitely _should_ be larger. It might just be an issue with the images I use for testing, but it seems to me that something is going wrong in the evaluation code. I also think that evaluation is not that impactful on many images since several reductions are rarely not worth doing.

Note that I have not refactored this code to fix warnings since it is a significant change and I want to hear your opinion first.
The filter strategy is evaluated heuristically by trying all filters at compression level 1 and only using full compression with the best result. In this patch, it is enabled by default on level 2. While compression is a little bit worse since the heuristic is sometimes wrong, performance improves by around a third in my tests. Alternatively, all filters could be tested heuristically, which is only slightly slower and produces better filtering than the current approach for level 2.

This represents a notable deviation from what optipng does. If you think that oxipng's behaviour should mirror optipng, it's probably best to only enable this if requested through the command line.
The same approach can be used to determine the best strategy heuristically (possibly with a higher compression level than 1, which helped with choosing the right one in my tests).
There are a number of different filter heuristics that perform better than the one used in libpng documented [here](http://bjoern.hoehrmann.de/pngwolf/).

I have implemented two additional strategies on the [better-filters](https://github.com/shssoichiro/oxipng/compare/master...fhanau:better-filters) branch. There are a number of caveats with these that need to be discussed, so I am opening this as an issue instead of a pull request.


- The bigrams strategy is more expensive than minsum (the current heuristic), but on average delivers better results than minsum. Trying both strategies yields the smallest size.
- The distinct bytes strategy rarely delivers the best result in my tests and on average performs worse than minsum, so it's questionable whether it is worth including.
- Enabling these strategies by default inevitably increases runtime, so I am not sure what the best way to integrate them is. As a possible solution, I have written some code that will add the option to use heuristics to choose the color type, zlib strategy and filter strategy at a smaller compression level to decrease the time spent deflating. I plan to open a seperate pull request for those changes.
- For more aggressive optimizations, there are some additional filter strategies based on the size of the deflated scanline or the shannon entropy as implemented in [zopflipng](https://github.com/lvandeve/lodepng/blob/f6155a4206046a31a9532d70caf517845af64c83/lodepng.cpp#L5002)
- I also added an experimental bias factor to the bigrams strategy, which slightly improves compression on average, but needs more work.
The recursive option doesn't work and says failed to open file for reading.
Version is oxipng 2.2.0
This is the output
```
konack@DESKTOP:/mnt/c/users/accounting2/pictures/test$ ls Screenshots
desktop.ini  s.png
konack@DESKTOP:/mnt/c/users/accounting2/pictures/test$ oxipng *.png --opt 4 --recursive
Processing: *.png
Failed to open file for reading
```
Oxipng only ever uses 18% of my CPU power, or about 6 threads.

My command line is:
```
        /opt/oxipng/target/release/oxipng \
                --opt 6 \
                --zw 32k \
                --zopfli \
                --threads 24 \
                *.png
```

I've tried with and without the `--threads 24` option, it doesn't seem to change anything. Are there only 6 parameter combinations at those settings, is it using a limited thread pool or misdetecting my cpu count (default is supposedly 1.5x cpu cores, so if it somehow couldn't detect and fell back to a fixed value of 4...)

This is on Gentoo Linux, Kernel 4.19.27, system with two Xeon E5-2680 CPUs.
I'd like to start off by saying I highly appreciate what this software offers, but there is this one feature I miss.

In jpegoptim you can specify [--preserve](https://www.mankier.com/1/jpegoptim#--preserve) to not change the modification time of the file being optimized.
Could a similar feature be implemented into oxipng? Would make at least one user happy. ^^
I'm working on implementing #74 

The problem is that the main loop which tries different compression modes/filters assumes there's a single PNG file, and only IDAT differs.

However, palette sorting makes palette + IDAT dependent, so the compression trials can't just pick a smaller IDAT, they have to also set the right palette to match.

The alpha filtering trials get away with this, because they don't change the image mode, but I'm still not entirely happy with alpha trials running compression trials "internally" as part of reduction, rather than being integrated with the main loop of trials (e.g. the side effect is that verbose mode doesn't show alpha mode compressions tested, and the winning IDAT is recompressed later with the same settings).

So I think some refactoring is needed. That's also related to #145

I could just compress with default settings as part of the palette reduction, same as alpha reductions do it, but that feels like copying&pasting a lot of code, and will perform redundant compressions.

So I'm thinking about introducing an object that will explicitly manage compression trials, which could be given to alpha and palette reductions to add their trials to the queue, so that the progress is properly displayed and the best IDAT is saved.

WDYT?
`PngData` has both `idat_data` and `raw_data`, which are related, but used in different contexts and handled as independent fields.

This is problematic, because:

* It's possible to modify `raw_data` without invalidating `idat_data`. Currently the code has to keep track of that "manually" through a flag passed around. That feels fragile.

* Copying of `PngData` copies both of them, even when it's just for trials/reductions which are going to use only one of them.

* Alpha filter trials get `filtered_image` version of the idat that needs to be recompressed better later, but there's no way to store the filtered version, so later recompression does filtering again (that's very minor though).

I'm not entirely sure how this should be handled, but there must be a better way. Some ideas:

* Have separate CompressedPng and UncompressedPng objects, with only either raw or idat, and try to use them throughout the code accordingly.
* Have one body as enum `Compressed`/`Uncompressed`/`Both`
* Have getters/setters on PngData that clear `idat_data` when `raw_data` is set or borrowed mutably.



