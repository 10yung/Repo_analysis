Resolves the error "AttributeError: module 'tornado.web' has no attribute 'asynchronous'", which is breaking the Travis build.
Users can have the nice blue bar with multiple spark applications with the following code change: https://github.com/birdsarah/jupyter-spark/commit/e0418883347b961db45a0399cec5ae2db45b365d

This is currently very crude and requires the user to manually know their spark ui port (e.g. by running `spark.conf.get('spark.driver.appUIAddress')`) and then hitting the url `http://localhost:11889/spark-configure?spark_ui_port=<port>`.

But it does work and opens up some nice new possibilities for cluster sharing.

Tornado 6.0 appears to have removed the tornado.web.asynchronous method, which handlers.py relies on:

```
[W 16:15:22.590 NotebookApp] Error loading server extension jupyter_spark
    Traceback (most recent call last):
      File "/usr/lib/python3.6/site-packages/notebook/notebookapp.py", line 1575, in init_server_extensions
        func(self)
      File "/usr/lib/python3.6/site-packages/jupyter_spark/__init__.py", line 30, in load_jupyter_server_extension
        from .handlers import SparkHandler
      File "/usr/lib/python3.6/site-packages/jupyter_spark/handlers.py", line 8, in <module>
        class SparkHandler(IPythonHandler):
      File "/usr/lib/python3.6/site-packages/jupyter_spark/handlers.py", line 13, in SparkHandler
        @tornado.web.asynchronous
    AttributeError: module 'tornado.web' has no attribute 'asynchronous'
```

This has been noted in jupyter/notebook#4311 which may have a path to a fix.

`pip freeze` of the environment this was run on under Python 3.6.8:

```
atomicwrites==1.3.0
attrs==19.1.0
backcall==0.1.0
beautifulsoup4==4.7.1
bleach==3.1.0
certifi==2019.3.9
chardet==3.0.4
decorator==4.4.0
defusedxml==0.5.0
docutils==0.14
entrypoints==0.3
idna==2.8
ipykernel==5.1.0
ipython==7.4.0
ipython-genutils==0.2.0
ipywidgets==7.4.2
jedi==0.13.3
Jinja2==2.10
jsonschema==3.0.1
jupyter==1.0.0
jupyter-client==5.2.4
jupyter-console==6.0.0
jupyter-core==4.4.0
jupyter-spark==0.4.0
MarkupSafe==1.1.1
mistune==0.8.4
more-itertools==7.0.0
nbconvert==5.4.1
nbformat==4.4.0
notebook==5.7.8
pandocfilters==1.4.2
parso==0.3.4
pexpect==4.6.0
pickleshare==0.7.5
pkginfo==1.5.0.1
pluggy==0.6.0
prometheus-client==0.6.0
prompt-toolkit==2.0.9
ptyprocess==0.6.0
py==1.8.0
Pygments==2.3.1
pyrsistent==0.14.11
pyspark==2.4.0
pytest==3.6.0
python-dateutil==2.8.0
pyzmq==18.0.1
qtconsole==4.4.3
readme-renderer==24.0
requests==2.21.0
requests-toolbelt==0.9.1
Send2Trash==1.5.0
six==1.12.0
soupsieve==1.9
terminado==0.8.2
testpath==0.4.2
tornado==6.0.2
tqdm==4.31.1
traitlets==4.3.2
twine==1.13.0
urllib3==1.24.1
wcwidth==0.1.7
webencodings==0.5.1
widgetsnbextension==3.4.2
```
Would be great to support Livy connections too (through %sparkmagic extension )

Thanks!


See releases has 0.4.0, but https://github.com/mozilla/jupyter-spark#changelog does not
Hi your tools looks really promising.

currently i get:
`{"error": "SPARK_NOT_RUNNING"}`
running /spark in my setup.

How can i configure the extension to work in an environment like this:
`pyspark2 --deploy-mode client --master yarn`

Could you give me a hint how to configure the setting in this case?
`jupyter notebook --Spark.url="http://localhost:4040"`

Thank you.
I have a case where my whole job seems to have 2 jobs running in parallel.
The progress bar at the cell level shows me only 1 job's progress bar (it shows the last job not even the job where progress is going on right now)

Love this extension btw!

Fix #39.

This adds a Spark status window to the side pane in Jupyter Lab.  I played with making a modal dialog like the old extension, but it feels like the side pane is more in keeping with the "JupyterLab way".

It looks like:

![image](https://user-images.githubusercontent.com/38294/39948413-dfc91d84-5543-11e8-9f73-f6e2b5d6813c.png)

@teonbrooks, @jezdez
Proposal for [Issue 22](https://github.com/mozilla/jupyter-spark/issues/22):

In the Jupyter notebook a Jupyter Comm target gets opened to listen for messages from a python kernel. A new Jupyter Magic uses this comm target to forward the Spark API URL to the notebook: 

`%spark_progress spark`

where `spark` is the variable holding the Spark Session, so the magic can use `globals()["spark"].sparkContext.uiWebUrl` to get the actual Spark API Url.

Each call from the javascript notebook then forwards the Spark API Url as a query parameter `spark_url` to the backend handler which uses it to create the backend_url.

This allows for multiple SparkContexts in different tabs and even for `spark.ui.port=0` setting.
ATMO now has support for jupyter lab. Is there a way to enable the progress bar within jupyter lab?