![grafik](https://user-images.githubusercontent.com/35228356/72205882-32c21c00-3488-11ea-8aea-be784f0dc9c7.png)
In this graphic what does the 0.26 actually mean in odor=foul? I assume it's a probability. How do the individual values contribute to the final explanation? If edible is 0.00, how can there be gill-size=broad of 0.13 which should increase the probability of edible?

Thank you!
error code 
```
E:\Soft\anaconda\envs\retinanet\lib\site-packages\lime\lime_image.py in data_labels(self, image, fudged_image, segments, classifier_fn, num_samples, batch_size)
    263                 labels.extend(preds)
    264                 imgs = []
--> 265             pbar.currval += 1
    266             pbar.update()
    267         pbar.finish()

AttributeError: 'ProgressBar' object has no attribute 'currval'
```

Package version
```
lime:  0.1.1.37
progressbar:  2.5
progressbar2:  3.42.0
```

Dear Marco, 
I have read your paper and use LIME to explain my model in Kaggle Default Risk competion data, and encounter some struggle as following: 
1. How can LIME choose or calculate  πx(z)? Please refer your source code that is related to this part?
2. How can LIME divide group of each variables in its explanation?
Please help me address these issue, thank you in advance
Hi there,

I´m trying to use the table explainer on categorical data and I have a highly dimensional dataset, so I have to take a sample out of the dataset. I fit the Label encoder and the One Hot encoder on the entire dataset, and try to fit it, but I´m getting the error

```
cat_feat = ['feat_1', 'feat_2', 'feat_3']
df_sample = df_m.sample(1000)
labels = df_sample.discreas
df_cat = df_sample[cat_feat].copy()
df_cat.fillna('NA', inplace=True)

categorical_names = {}
for feature in cat_feat:
    le = LabelEncoder().fit(df_m[feature].fillna('NA'))
    df_cat[feature] = le.transform(df_cat[feature])
    categorical_names[feature] = le.classes_

df_cat = df_cat.astype(float)

one_hot_encoder = OneHotEncoder(categories='auto').fit(df_m[cat_feat].fillna('NA'))
X_train, X_test, y_train, y_test = train_test_split(df_cat, labels)


encoded_train = one_hot_encoder.transform(X_train)

mdl_c = RandomForestClassifier(n_jobs=-1, n_estimators=200)
mdl_c.fit(encoded_train, y_train)

predict_fn = lambda x: mdl_c.predict_proba(one_hot_encoder.transform(x))

explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values ,
                                                   #class_names=labels, 
                                                   feature_names = cat_feat,
                                                   categorical_features = len(df_cat),
                                                   categorical_names=categorical_names
                                                  )
exp = explainer.explain_instance(X_test.values[0], predict_fn)
exp.show_in_notebook()

ValueError: Found unknown categories [0.0, 1.0, 9.0, 12.0, 15.0, 17.0, 26.0, 27.0, 28.0, 29.0, 37.0, 48.0, 49.0, 51.0, 52.0, 53.0, 55.0, 56.0, 57.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0] in column 0 during transform

```

Any idea on what´s going on? Thanks
Hello,
The only class I'm using from lime is LimeTextExplainer. I'm trying to fit it into AWS lambda limitation which is 250 mb. Lime alone with all dependencies has this size. Do you know maybe a way how to shrink it?
Hello,
First I would like to thank you for a very nice tool and for very efficient support and issues handling. 
I am trying to use LimeTextExplainer with a bert  model fine-tuned for YelpReviewsPolarity.
I was using the structure i found on https://github.com/ThilinaRajapakse/pytorch-transformers-classification .
The thing is, that when evaluating a transformer-based classifier (or any DNN for that matter), the last layer is not softmaxed, but rather argmaxed by default.  
in-order to use LimeTextExplainer, I wrote an additional method called predict_proba(str_list)--> np.array , that return the desired np array of "probabilities".
However, since softmax is a rather arbitrary way to normalize the logits into the [0 1] interval, I played a little with the softmax temperature. 
the result was a bit surprising - first of all, the "word importance" order seems to change, while i was assuming it should be invariant to monotonous transformation.
second, for some of the temperatures, the "values" each word got became very small, making it hard to display on default settings.  (see attached screenshots)
In order to check this issue, I also ran LimeTextExplainer on a default params RandomForest, with an additional softmax-temperature at the end, and got similar behavior. 

can you explain these results? 
could you elaborate on the best practices to implement the necessary predict_proba method? is there a better way to normalize (aside from softmax?)
what causes the word coefficients to vanish? 

thanks a lot,

Carmel

code:
       `

    def predict_proba(self, str_list):
        """ This function is fed to LimeTextExplainer.explain_instance() as the
        'classifier_fn'. it gets a list of strings, and gives back an np.array
        of 'probabilities', that LIME needs to fit the explanation's decision
        boundery.
        
        Arguments:
            str_list {list(str)} -- list of strings; if a single text is given,
                it should be wrapped in len=1 list.

        Returns:
            [np.array] -- an array of dim=(str_list.len, num_of_labels) of
                        the 'probabilities' for each label
        """

        dataset = self.convert_str_list_to_dataset(str_list)
        eval_sampler = SequentialSampler(dataset)
        dataloader = DataLoader(
            dataset, sampler=eval_sampler, batch_size=self.batch_size)

        preds = None
        out_label_ids = None    # might be important for other transformers

        for batch in dataloader:
            self.model.eval()
            batch = tuple(t.to(self.device) for t in batch)

            with torch.no_grad():
                inputs = {'input_ids':      batch[0],
                          'attention_mask': batch[1]
                          if self.model_type in self.transformers_models else None,
                          'token_type_ids': batch[2]
                          # XLM don't use segment_ids
                          if self.model_type in ['bert', 'xlnet'] else None}

                outputs = self.model(**inputs)
                logits = outputs[0]
                # in case model.forward gives more than one output then logits
                # should be the first
                logits = logits.detach().cpu().numpy()
                # Lime collects the logits from ALL examples, so it needs to
                # detach each batch's logits form device back to cpu.
                if preds is None:
                    preds = logits
                    # out_label_ids = inputs["labels"].detach().cpu().numpy()
                else:
                    preds = np.append(preds, logits, axis=0)
                    # out_label_ids = np.append(out_label_ids,
                    # inputs["labels"].detach().cpu().numpy(),
                    # axis=0)

        if self.softmax_T is not None:
            proba = softmaxT(preds, theta=self.softmax_T, axis=1)
            return proba
        else:
            return preds
`
screenshots:

![image](https://user-images.githubusercontent.com/58820263/71905249-71966000-3170-11ea-8fcb-96ce8f6f4ab9.png)
![image](https://user-images.githubusercontent.com/58820263/71905443-d0f47000-3170-11ea-9810-8b654ae322c9.png)
![image](https://user-images.githubusercontent.com/58820263/71905552-0c8f3a00-3171-11ea-8565-52b452f9e35c.png)

![image](https://user-images.githubusercontent.com/58820263/71906396-aacfcf80-3172-11ea-9f8f-4fd4a1cf9537.png)


Hello everyone,
I have trained my dataset with XGBoost classifier and used Pandas dataframe directly as Input data. 
the Code is as follows:

    #Read csv file and prepare Input and Target columns
    df = pd.read_csv('Broken.csv', encoding='ISO-8859-1', sep=',') 
    X = df5.drop(['C'], axis=1)  #Input column, dropping the target column
    y = df5['C']  #Target column

    #split test and Train dataset
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)
    
    #Train using XGBoost classifier
    from xgboost import XGBClassifier
    xg = XGBClassifier()
    xg.fit(X_train, y_train)

after Training i am trying to use LIME to slice through the dataframe. I am using the iloc command but still i get an error : 

    TypeError: '(slice(None, None, None), 0)' is an invalid key

My Code is as follows:

    prepare feature names for LIME Tabular explainer
    columnsNamesArr = (X).columns.values   #get column names from df
    listOfColumnNames = list(columnsNamesArr) #convert it to a list

    #Column used as target
    breakage = df5['C'].values   # get target values to be used in LIME tabular explainer

    #LIME explainer  
    explainer =lime.lime_tabular.LimeTabularExplainer(X_train.iloc[:,:], feature_names=listOfColumnNames, class_names=breakage,verbose=True, mode='classification')

which gives the error : 
 
    TypeError: '(slice(None, None, None), 0)' is an invalid key

Can anyone suggest where i am going wrong ?
I just add a filter before selection features.
When I use LIME to get an explanation with a model, I wouldn't get some features are come out in explanation result.
So I implement a filter before the feature selection.
That also can decrease the feature selection's search space to speed up the feature selection.
Hi, I've used Lime on a pytorch CNN model trained on the MNIST dataset.
The problem I'm having is that it works fine for most instances where it properly displays the boundaries but on certain predictions the following code:

temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=False)
img_boundry1 = mark_boundaries(temp/255.0, mask)
plt.imshow(img_boundry1)

would just show the base image and setting positive_only to False makes the entire image green with no boundaries.

Am I doing something wrong? It works for some images and others have this issue


I have an error as below. Please help me....

explainer_Lime_one = lime.lime_tabular.LimeTabularExplainer(full_train_npary, mode="regression", feature_names = None, feature_selection = "lasso_path")
Lime_one_model = explainer_Lime_one.explain_instance(full_test_npary[0], model.predict, num_features=5)

---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
C:\ProgramData\Anaconda3\lib\site-packages\lime\lime_tabular.py in explain_instance(self, data_row, predict_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)
    288             try:
--> 289                 assert isinstance(yss, np.ndarray) and len(yss.shape) == 1
    290             except AssertionError:

AssertionError: 

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-286-cb5b728c4b93> in <module>
----> 1 Lime_one_model = explainer_Lime_one.explain_instance(full_test_npary[0], model.predict, num_features=5)

C:\ProgramData\Anaconda3\lib\site-packages\lime\lime_tabular.py in explain_instance(self, data_row, predict_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)
    290             except AssertionError:
    291                 raise ValueError("Your model needs to output single-dimensional \
--> 292                     numpyarrays, not arrays of {} dimensions".format(yss.shape))
    293 
    294             predicted_value = yss[0]

ValueError: Your model needs to output single-dimensional                     numpyarrays, not arrays of (5000, 1) dimensions


FYI, 
full_test_npary[0].shape
(192,)

type(full_test_npary)
numpy.ndarray

model.summary()
Layer (type)                 Output Shape              Param #   
=================================================================
dense_45 (Dense)             (None, 128)               24704     
_________________________________________________________________
dense_46 (Dense)             (None, 64)                8256      
_________________________________________________________________
dense_47 (Dense)             (None, 32)                2080      
_________________________________________________________________
dense_48 (Dense)             (None, 16)                528       
_________________________________________________________________
dense_49 (Dense)             (None, 1)                 17        