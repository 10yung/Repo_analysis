``` r
library(tidymodels)
#> Registered S3 method overwritten by 'xts':
#>   method     from
#>   as.zoo.xts zoo
#> ── Attaching packages ─────────────────────────────────────────────────────────── tidymodels 0.0.4 ──
#> ✓ broom     0.5.2          ✓ recipes   0.1.9     
#> ✓ dials     0.0.4          ✓ rsample   0.0.5     
#> ✓ dplyr     0.8.3          ✓ tibble    2.1.3     
#> ✓ ggplot2   3.2.1          ✓ tune      0.0.1     
#> ✓ infer     0.5.1          ✓ workflows 0.1.0     
#> ✓ parsnip   0.0.4.9000     ✓ yardstick 0.0.4     
#> ✓ purrr     0.3.3
#> ── Conflicts ────────────────────────────────────────────────────────────── tidymodels_conflicts() ──
#> x purrr::discard()    masks scales::discard()
#> x dplyr::filter()     masks stats::filter()
#> x dplyr::lag()        masks stats::lag()
#> x ggplot2::margin()   masks dials::margin()
#> x recipes::step()     masks stats::step()
#> x recipes::yj_trans() masks scales::yj_trans()
data(two_class_example)
two_class_example %>% 
  dplyr::filter(truth == "Class1") %>% 
  roc_auc(truth, Class1)
#> Error in roc.default(response, predictor, auc = TRUE, ...): No control observation.
```

<sup>Created on 2020-01-18 by the [reprex package](https://reprex.tidyverse.org) (v0.3.0)</sup>

It would be better to return `NA` with a warning.
The plot needs predictions along the `y` axis, and truth along the `x` axis

<img width="1203" alt="Screen Shot 2020-01-14 at 2 32 01 PM" src="https://user-images.githubusercontent.com/19150088/72391577-85573e80-36fb-11ea-8214-bd1476e79fbf.png">
<img width="1203" alt="Screen Shot 2020-01-14 at 2 32 05 PM" src="https://user-images.githubusercontent.com/19150088/72391578-85573e80-36fb-11ea-95d3-54376d93d546.png">

I think that for `cm_heat()` this isn't too bad. We can swap out for these lines:

```r
    dplyr::mutate(Prediction = factor(Prediction, levels = rev(levels(Prediction)))) %>%
    ggplot2::ggplot(ggplot2::aes(Truth, Prediction, fill = Freq)) %+%
```

For `cm_mosaic()` I think this gets us close, but the labels don't look like they are placed quite right

```r
  ggplot2::ggplot(full_data) %+%
    ggplot2::geom_rect(ggplot2::aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax)) %+%
    ggplot2::scale_x_reverse(
      breaks = (x_data$xmin + x_data$xmax) / 2,
      labels = tick_labels
    ) %+%
    ggplot2::scale_y_reverse(
      breaks = (y1_data$ymin + y1_data$ymax) / 2,
      labels = tick_labels
    ) %+%
    ggplot2::labs(x = "Predicted", y = "Truth") %+%
    ggplot2::theme(panel.background = ggplot2::element_blank()) %+%
    ggplot2::coord_flip()
```
This collision with `readr::spec` could be improved if `other (spec)` showed where `spec` was found

```
> class_metrics <- yardstick::metric_set(accuracy, ppv, sens, spec)
Error: 
The combination of metric functions must be:
- only numeric metrics
- a mix of class metrics and class probability metrics
The following metric function types are being mixed:
- class (accuracy, ppv, sens)
- other (spec)
Call `rlang::last_error()` to see a backtrace.
```

becomes

```
> class_metrics <- yardstick::metric_set(accuracy, ppv, sens, spec)
Error: 
The combination of metric functions must be:
- only numeric metrics
- a mix of class metrics and class probability metrics
The following metric function types are being mixed:
- class (accuracy, ppv, sens)
- other
   - spec - from environment: "namespace:readr"
Call `rlang::last_error()` to see a backtrace.
```

We can use `rlang::fn_env()` and `rlang::env_name()` to get a decent name

``` r
rlang::env_name(rlang::fn_env(function(x) x))
#> [1] "global"
```

``` r
library(yardstick)
#> For binary classification, the first factor level is assumed to be the event.
#> Set the global option `yardstick.event_first` to `FALSE` to change this.
library(readr)
#> 
#> Attaching package: 'readr'
#> The following object is masked from 'package:yardstick':
#> 
#>     spec
library(rlang)

env_name(fn_env(spec))
#> [1] "namespace:readr"
```
For `spec()` and `sens()`. This would help avoid a collision with `readr::spec()` that looks like:

```
> class_metrics <- yardstick::metric_set(accuracy, ppv, sens, spec)
Error: 
The combination of metric functions must be:
- only numeric metrics
- a mix of class metrics and class probability metrics
The following metric function types are being mixed:
- class (accuracy, ppv, sens)
- other (spec)
Call `rlang::last_error()` to see a backtrace.
```
instead of `geom_line` and use the option `direction = "vh"`
I'll likely add more tests as I dive in, but I got the sklearn comparison set up.
The print method for `metric_set()` can probably be improved by unclassing it and removing all of the attributes before printing, so it just prints like a function. The most important thing you'd do when printing it out is check the function signature, so its a bit confusing that it also currently prints out all the attributes, which includes the `metrics` attribute holding all of the individual functions in the metric set.
Why is the AUC result from `yardstick` different from that of `pROC` and the `HandTill2001` packages? Something to do with the pattern of predicted probabilities?

It also seems like `yardstick` cannot handle the case when there is one true class with 0 predicted classes.
``` r
x <- structure(c(3L, 3L, 3L, 4L, 4L, 1L, 4L, 3L, 1L, 1L, 4L, 2L, 4L, 
                 1L, 4L, 1L, 4L, 4L), .Label = c("a", "b", "c", "d"), class = "factor")
probs <- structure(c(0.5, 0.4, 0.8, 0.5, 0.8, 1, 0.8, 0, 0.5, 0.666666666666667, 
                     0, 1, 0.4, 0.166666666666667, 0.4, 0.8, 0.5, 0.6, 0, 0.2, 0, 
                     0, 0, 0, 0, 0.6, 0, 0.333333333333333, 0, 0, 0, 0.833333333333333, 
                     0.6, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 
                     0, 0, 0, 0.5, 0.2, 0.2, 0.5, 0.2, 0, 0.2, 0.2, 0.5, 0, 1, 0, 
                     0.6, 0, 0, 0.2, 0.5, 0.4), .Dim = c(18L, 4L), .Dimnames = list(
                       NULL, c("a", "b", "c", "d")))

auc_handtill <- function (x, probs) {
  mcap.construct <-
    suppressWarnings(HandTill2001::multcap(response = x,
                                           predicted = as.matrix(probs)))
  HandTill2001::auc(mcap.construct)
}

auc_handtill(x, probs)
#> [1] 0.5890625
pROC::multiclass.roc(x, probs)[["auc"]]
#> Multi-class area under the curve: 0.5891
yardstick::roc_auc_vec(x, probs)
#> [1] 0.7411458

# Single case for level "b"
table(x)
#> x
#> a b c d 
#> 5 1 4 8
which(x == "b")
#> [1] 12

# Taking out "b"
auc_handtill(x[-12], probs[-12, ])
#> [1] 0.715625
pROC::multiclass.roc(x[-12], probs[-12, ])[["auc"]]
#> Warning in multiclass.roc.multivariate(response, predictor, levels,
#> percent, : No observation for response level(s): b
#> Warning in multiclass.roc.multivariate(response, predictor, levels,
#> percent, : The following classes were not found in 'response': b.
#> Multi-class area under the curve: 0.7156
yardstick::roc_auc_vec(x[-12], probs[-12, ])
#> Error in roc.default(response, predictor, auc = TRUE, ...): No control observation.
```

<sup>Created on 2019-09-06 by the [reprex package](https://reprex.tidyverse.org) (v0.3.0)</sup>
I saw an interesting presentation where they optimized the lift statistic but for a fixed percentage of samples that are tested. 

So for the example data, if someone used `gain_point(two_class_example, pct = 1/1, truth, Class1)` (or whatever we call it), they would get back an estimate of 84.5:

``` r
library(tidymodels)
#> Registered S3 methods overwritten by 'ggplot2':
#>   method         from 
#>   [.quosures     rlang
#>   c.quosures     rlang
#>   print.quosures rlang
#> Registered S3 method overwritten by 'xts':
#>   method     from
#>   as.zoo.xts zoo
#> ── Attaching packages ───────────────────────────────────────────────────────────────────────────────────── tidymodels 0.0.2 ──
#> ✔ broom     0.5.1       ✔ purrr     0.3.2  
#> ✔ dials     0.0.2       ✔ recipes   0.1.5  
#> ✔ dplyr     0.8.0.1     ✔ rsample   0.0.4  
#> ✔ ggplot2   3.1.1       ✔ tibble    2.1.1  
#> ✔ infer     0.4.0       ✔ yardstick 0.0.2  
#> ✔ parsnip   0.0.2
#> ── Conflicts ──────────────────────────────────────────────────────────────────────────────────────── tidymodels_conflicts() ──
#> ✖ purrr::discard() masks scales::discard()
#> ✖ dplyr::filter()  masks stats::filter()
#> ✖ dplyr::lag()     masks stats::lag()
#> ✖ recipes::step()  masks stats::step()

gain_curve(two_class_example, truth, Class1) %>% slice(251)
#> # A tibble: 1 x 4
#>      .n .n_events .percent_tested .percent_found
#>   <dbl>     <dbl>           <dbl>          <dbl>
#> 1   250       218              50           84.5
```

<sup>Created on 2019-05-13 by the [reprex package](https://reprex.tidyverse.org) (v0.2.1)</sup>

We can interpolate between points if we don't have the exact percentage. 

`lift_point()` would return the ratio of the observed percent over the baseline rate (e.g. prevalence)
Consider the following example:

```
library(yardstick)
library(tidyverse)

df = tibble(
    truth = FALSE, 
    est = TRUE
) %>% mutate_all(factor, levels=c("TRUE", "FALSE"))

df
# A tibble: 1 x 2
  truth est  
  <fct> <fct>
1 FALSE TRUE 

df %>% ppv(truth, est)
# A tibble: 1 x 3
  .metric .estimator .estimate
  <chr>   <chr>          <dbl>
1 ppv     binary            NA
```

The ppv comes out as `NA` when it should clearly be 0/1 = 0. The problem is that a formula involving specificity and sensitivity is used in the code to calculate ppv. In this example, the sensitivity is `NA`, which corrupts the calculation. 

The formula that is used is useful in settings with biased case-control sampling, but otherwise should not be used. If no prevalence is provided, the calculation should be something like `mean(truth[est=="TRUE"]=="TRUE")`. 