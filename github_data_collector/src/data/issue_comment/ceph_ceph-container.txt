See https://tracker.ceph.com/issues/43609

The container image has the systemd-239-18.el8_1.1.x86_64 package installed, which contains a few dozen files in /usr/lib/systemd, but that directory is missing.

The end result is that the 'udevadm' command fails:

```
[ceph: root@gnit /]# udevadm
udevadm: error while loading shared libraries: libsystemd-shared-239.so: cannot open shared object file: No such file or directory
```
After a reboot, the encrypted partitions aren't opened so the ceph-volume
simple scan command won't find the block partition (not mapped, broken
symlink) and the simple activate will fail.
```console
$ ceph-volume simple scan /dev/sdb1 --force
(...)
--> broken symlink found /tmp/tmpept0ox2j/block -> /dev/mapper/457d7196-4015-40fe-aa83-a160477450f7
```
```console
$ ceph-volume.log
(...)
[ceph_volume.util.system][INFO  ] /dev/sdb1 was not found as mounted
[ceph_volume.util.encryption][WARNING] failed to detect device mapper information
[ceph_volume.util.encryption][WARNING] failed to detect device mapper information
[ceph_volume.devices.simple.scan][WARNING] broken symlink found /tmp/tmpept0ox2j/block -> /dev/mapper/457d7196-4015-40fe-aa83-a160477450f7
[ceph_volume.devices.simple.scan][ERROR ] skipping due to IOError on file: /tmp/tmpept0ox2j/block
Traceback (most recent call last):
  File "/usr/lib/python3.6/site-packages/ceph_volume/devices/simple/scan.py", line 118, in scan_directory
    if system.is_binary(file_path):
  File "/usr/lib/python3.6/site-packages/ceph_volume/util/system.py", line 116, in is_binary
    with open(path, 'rb') as fp:
FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpept0ox2j/block'
[ceph_volume.util.encryption][WARNING] failed to detect device mapper information
[ceph_volume.devices.simple.scan][INFO  ] skipping binary file: /tmp/tmpept0ox2j/block_dmcrypt
```
If the OSD has already been scanned then we shouldn't update the
associated json file on each start.

Closes: https://bugzilla.redhat.com/show_bug.cgi?id=1792122

Signed-off-by: Dimitri Savineau <dsavinea@redhat.com>
The Sree interface wasn't compatible with python3. Thoses changes allow
to run sree on both python 2 and 3.
This was breaking ceph nano (cn) with the CentOS 8 based ceph container
images (like master/octopus).

* Remove pycurl/StringIO requirements and switch to requests [1]
* Change the exception syntax in xmlparse.py [2]
* Try to import urlparse (py2) then urllib.parse (py3)
* CORSConfiguration encode/decode for md5 content

[1] https://github.com/cannium/Sree/commit/3b6cc63
[2] https://github.com/cannium/Sree/commit/28b89a4

Signed-off-by: Dimitri Savineau <dsavinea@redhat.com>
Is this a bug report or feature request?
* Bug Report

**Bug Report**

What happened:
When specified 'BASEOS_TAG', build image failed.

make FLAVORS="luminous-12.2.11,centos,7" RELEASE=12.2.11-1 TAG_REGISTRY=registry.icp.com:5000/library/ceph BASEOS_REGISTRY=registry.icp.com:5000/library/os BASEOS_REPO=inspur-centos-7 BASEOS_TAG=5.0.0 DAEMON_TAG=ceph:12.2.11-1 IMAGES_TO_BUILD=daemon DAEMON_BASE_TAG=ceph-base:12.2.11-1 build

can not get correct ceph-iscsi.repo cause that wrong url 'https://download.ceph.com/ceph-iscsi/2/rpm/el12.2.11-1/ceph-iscsi.repo'

What you expected to happen:
Url should be https://download.ceph.com/ceph-iscsi/2/rpm/el7/ceph-iscsi.repo

How to reproduce it (minimal and precise):
Specify BASEOS_TAG=5.0.0

**Environment**:
* OS (e.g. from /etc/os-release):
cat /etc/os-release 
NAME="CentOS Linux"
VERSION="7 (Core)"
ID="centos"
ID_LIKE="rhel fedora"
VERSION_ID="7"
PRETTY_NAME="CentOS Linux 7 (Core)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:centos:centos:7"
HOME_URL="https://www.centos.org/"
BUG_REPORT_URL="https://bugs.centos.org/"

CENTOS_MANTISBT_PROJECT="CentOS-7"
CENTOS_MANTISBT_PROJECT_VERSION="7"
REDHAT_SUPPORT_PRODUCT="centos"
REDHAT_SUPPORT_PRODUCT_VERSION="7"

* Kernel (e.g. `uname -a`):
Linux xiett.novalocal 3.10.0-957.el7.x86_64 #1 SMP Thu Nov 8 23:39:32 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
* Docker version (e.g. `docker version`):
Docker version 18.09.7, build 2d0083d
* Ceph version (e.g. `ceph -v`):
12.2.11

In a container, there's no need to install documentation.
Pass the relevant option for yum to skip documentation installation

Signed-off-by: Yaniv Kaul <ykaul@redhat.com>
Hi, all:
    As we know, we can easily run any components of ceph cluster with one docker image(ceph/daemon:latest-luminous, e.g.), docker run -d --net=host --name=xxx -v /etc/localtime:/etc/localtime -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -e MON_IP=xxx.xxx.xxx.xxx -e CEPH_PUBLIC_NETWORK=xxx.xxx.xxx.xxx/24 ceph/daemon:latest-luminous {mon,osd,mds}. And we are able to check logs by command "docker logs -f xxx". 
    But as time flies, 1 month, for example, sizes of these stdout may take to much under directory /var/lib/docker/containers/CONTAINER ID/CONTAINER ID-json.log, may reach 10G+. It is not so easy to control the size of them.  
    I know there is one ENTRYPOINT in Dockerfile: [/opt/ceph-container/bin/entrypoint.sh], and when running docker, we can add CMD like 'mon','osd','mgr' .etc. To setup different componets and check log. 
    So are there any ways(bring in new Env parameter or bring in new option following behind the scripts entrypoint.sh) to define one switch which controls or restrains stdout log and redirect the stdout log to one directory inside the container, so that we can mount the directory outside to the host directory, and handle the logs easily? 

BR


We need to build development containers from git with branch names like ``master``, ``nautilus``, ``fix-12345``, etc., since not all branches begin with ``wip-*``.
Right now the labels (as reported by podman inspect) look like:
```
            "Labels": {
                "CEPH_POINT_RELEASE": "",
                "GIT_BRANCH": "HEAD",
                "GIT_CLEAN": "True",
                "GIT_COMMIT": "3e4c74ab4205876cce4afd9c4051ac710a82973b",
                "GIT_REPO": "git@github.com:ceph/ceph-container.git",
                "RELEASE": "wip-cd-df40a49",
                "maintainer": "Dimitri Savineau \u003cdsavinea@redhat.com\u003e",
                "org.label-schema.build-date": "20190801",
                "org.label-schema.license": "GPLv2",
                "org.label-schema.name": "CentOS Base Image",
                "org.label-schema.schema-version": "1.0",
                "org.label-schema.vendor": "CentOS"
            },
```
For a ``ceph -v`` of ``ceph version 15.0.0-6884-gf0458b188a (d76dc026e8194928712f229afa002cf4005bb5df) octopus (dev)``, I'd like to add labels like the following:
- ``CEPH_VERSION`` - ``15.0.0-6884-gf0458b188a``
- ``CEPH_RELEASE`` - ``octopus``
- ``CEPH_RELEASE_TYPE` -- ``dev``  (or ``rc`` or ``stable``)

Currently ceph-daemon is executing ``ceph -v`` inside the container to get this information, but simply extracting it from the container image metadata would be much faster!


<!-- **Are you in the right place?**
1. For issues or feature requests, please create an issue in this repository.
2. Did you already search the existing open issues for anything similar? -->

Is this a bug report or feature request?
* Feature Request
Connecting the ceph project and ceph-container projects together so that we can build and package from ceph source code to container images 

Currently I'm able to use the ceph project to compile all the source code and package them into a dozen of RPMs.  How can I use these local RPMs as an input to the ceph-container project in order to further package them into container images?

**Feature Request**

Are there any similar features already existing:

What should the feature do:

What would be solved through this feature:

Does this have an impact on existing features:

**Environment**:
* OS (e.g. from /etc/os-release):
* Kernel (e.g. `uname -a`):
* Docker version (e.g. `docker version`):
* Ceph version (e.g. `ceph -v`):

Hello folks!

I dont understand this moment, can u help me , plz?

Where Do i need to put ip-address?

`#This only works if you have skyDNS resolveable from the kubernetes node. Otherwise you must manually put in one or more mon pod ips.`