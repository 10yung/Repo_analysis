There's been some uncertainty about what `n_min` means, and I believe just a little more detail would be helpful. Addresses juliasilge/tidytext#148.
As the title says does tokenize_tweets not separate emojis that don't have a space between them. furthermore anything following a emoji (without a space) is grouped together with the emoji code

```
tokenizers::tokenize_tweets(c("test string \U0001f4e6\U0001f47e",
                              "\U0001f4e6Don't match",
                              "\U0001f4e6\U0001f47e",
                              "\U0001f4e6#hashtag",
                              "\U0001f4e6@User"))

[[1]]
[1] "test"                 "string"               "\U0001f4e6\U0001f47e"

[[2]]
[1] "\U0001f4e6dont" "match"         

[[3]]
[1] "\U0001f4e6\U0001f47e"

[[4]]
[1] "\U0001f4e6hashtag"

[[5]]
[1] "\U0001f4e6user"
```

Is it something that can be handled or is it simply a limitation of the algorithm? :)  
Just as in quanteda, the functions which call Rcpp versions should be parallelized with RcppParallel. They should all have an argument that sets the number of cores: `cores = getOption("mc.cores")` probably defaulting to 2 if that option is not set.
gensim has a lemmatizing tokenizer, which, instead of stemming words, converts them to their lemma. For instance, "was," "being," "am" would tokenize to "be."

https://radimrehurek.com/gensim/utils.html
