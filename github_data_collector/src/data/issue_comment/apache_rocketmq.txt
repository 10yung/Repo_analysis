代码在DefaultMessageStore的DefaultMessageStore的558行
`boolean isInDisk = checkInDiskByCommitOffset(offsetPy, maxOffsetPy);`
![Image 2](https://user-images.githubusercontent.com/22953986/72597472-d4de7a00-3948-11ea-85a3-61cf670d1baf.png)


should be "SLAVE", not "SLVAE"
when broker receives a put message request, after putting the message onto disk, it directly 
calls `AbstractSendMessageProcessor#doResponse` 
in  `SendMessageProcessor#handlePutMessageResult`, 
rather than return a normal response, and send response in `NettyRemotingAbstract#processRequestCommand`
why?
The issue tracker is **ONLY** used for bug report(feature request need to follow [RIP process](https://github.com/apache/rocketmq/wiki/RocketMQ-Improvement-Proposal)). Keep in mind, please check whether there is an existing same report before your raise a new one.

Alternately (especially if your communication is not a bug report), you can send mail to our [mailing lists](http://rocketmq.apache.org/about/contact/). We welcome any friendly suggestions, bug fixes, collaboration and other improvements.

Please ensure that your bug report is clear and that it is complete. Otherwise, we may be unable to understand it or to reproduce it, either of which would prevent us from fixing the bug. We strongly recommend the report(bug report or feature request) could include some hints as the following:

**BUG REPORT**

1. Please describe the issue you observed:
ReqResp timeout execute N time
- What did you do (The steps to reproduce)?
1. producer start request(x,callback,3000ttl)
2. timeout callback on exception execute 2 time

- What did you expect to see?

- What did you see instead?

2. Please tell us about your environment:
4.6.0
3. Other information (e.g. detailed explanation, logs, related issues, suggestions how to fix, etc):

**FEATURE REQUEST**

1. Please describe the feature you are requesting.

2. Provide any additional detail on your proposed use case for this feature.
DefaultMQProducerImpl.start(){
...
this.timer.scheduleAtFixedRate(new TimerTask() {
            @Override
            public void run() {
                try {
                    RequestFutureTable.scanExpiredRequest();
                } catch (Throwable e) {
                    log.error("scan RequestFutureTable exception", e);
                }
            }
        }, 1000 * 3, 1000);
...
remove this code,
change to
DefaultMQProducerImpl
static {
private static final Timer timer = new Timer("RequestHouseKeepingService", true);
timer.scheduleAtFixedRate(new TimerTask() {
            @Override
            public void run() {
                try {
                    RequestFutureTable.scanExpiredRequest();
                } catch (Throwable e) {
                    log.error("scan RequestFutureTable exception", e);
                }
            }
        }, 1000 * 3, 1000);
}
}
2. Indicate the importance of this issue to you (blocker, must-have, should-have, nice-to-have). Are you currently using any workarounds to address this issue?

4. If there are some sub-tasks using -[] for each subtask and create a corresponding issue to map to the sub task:

- [sub-task1-issue-number](example_sub_issue1_link_here): sub-task1 description here, 
- [sub-task2-issue-number](example_sub_issue2_link_here): sub-task2 description here,
- ...

When acl is enabled, I defined only one topic can be subscribed as follow :
``` 
globalWhiteRemoteAddresses:
accounts:
- accessKey: accessId123
  secretKey: accessKey123
  whiteRemoteAddress:
  admin: false
  defaultTopicPerm: DENY
  defaultGroupPerm: SUB
  topicPerms:
  - TopicA=SUB
  groupPerms:
```
Then I defined two consumers named ConsumerA that subscribed TopicA and ConsumerB that subscribed TopicB. Obviously ConsumerB dont have access to subscribe TopicB.
if I started ConsumerA before ConsumerB  :
```
//will subscribe TopicA
Consumer consumerA = new ConsumerA();
//will subscribe TopicB
Consumer consumerB = new ConsumerB();
consumerA.start();
consumerB.start();
```  
Though there would be an AclException in the remoting.log that shows no default permission for topicB, consumerA could start successfully.
However if I started ConsumerB before ConsumerA:
```
//will subscribe TopicA
Consumer consumerA = new ConsumerA();
//will subscribe TopicB
Consumer consumerB = new ConsumerB();
//consumerB starts first
consumerB.start();
consumerA.start();
``` 
There also would be an AclExcpetion in the remoting.log that shows no default permission for topicB, **consumerA can not be started either**.
Even though I can see the linux connect was established by command netstat,  consumer was offline and can not consume any message. 
Is this a bug?

如果我只在 plain_acl.yml中只定义了TopicA的订阅权限,
然后我封装了两个消费者类,ConsumerA和ConsumerB,他们分别订阅TopicA和TopicB.
很明显ConsumerB将无权订阅TopicB. 当我先启动ConsumerA,后启动ConsumerB时, ConsumerA启动正常,并可以消费消息.
但是如果我先启动ConsumerB,后启动ConsumerA, 那么不仅ConsumerB无法消费消息,ConsumerA也没法消费消息,
并且这时候,如果我用netstat看的话,ConsumerA是有连接建立的, 但是我通过mqadmin的consumeProgress命令,显示的是消费者处于OffLine状态, 通过rocketmq-console也看不到消费者在线.
这是个bug吗
Now, the jvm Namesrv running on uses CMS, while the jvm Broker running on uses G1.
Maybe we could switch from CMS to G1, reasons are as follows:

1. Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
2. As #1688 mentioned, Namesrv cannot run on JDK 11, the reason is some options about CMS has been removed. But G1 was introduced in Java 7, and becomes default GC in the higher version JDKs(after version JDK 9).
3. The G1 can collect both the young and the old generation.
...

Undeniably, the G1 doesn’t perform well with small heaps. However, the heap memory size Namesrv uses is 4G, it may not be small. Also, full GCs can be triggered when humongous objects (objects over 50% of the size of a region) can’t be allocated due to a lack of space(no continuous regions available). 
However, 4G heap Memory for Namesrv as an example, the G1 can still perform well. According to best tuning practices( heap memory is divided into about 2000 regions), we can set one region 2M. Use jmap tool to get heap dump file of Namesrv in the test environment(2m-2s-async brokers, 2 namesrv), use visualvm tool to analysis the file, the 10 biggest objects are as follows:

Class Name|Retained Heap
------|-------------------
sun.misc.Launcher$ExtClassLoader#1 | 1,223,336
java.util.Vector#45|658,138
java.lang.Object[]#59741|658,102
java.util.concurrent.ConcurrentHashMap#75809|369,344
java.util.concurrent.ConcurrentHashMap$Node[]#18955|369,244
class io.netty.buffer.ByteBufUtil$HexUtil|358,480
io.netty.buffer.PoolThreadCache#1|339,839
io.netty.buffer.PoolThreadCache#16|339,758
io.netty.buffer.PoolThreadCache#2|339,434
io.netty.buffer.PoolThreadCache#6|339,434

In summary, there is no performance penalty for switching from CMS to G1.

code like this:
```
public IndexFile getAndCreateLastIndexFile() {
        IndexFile indexFile = null;
        IndexFile prevIndexFile = null;
        long lastUpdateEndPhyOffset = 0;
        long lastUpdateIndexTimestamp = 0;

        {
            this.readWriteLock.readLock().lock();
            if (!this.indexFileList.isEmpty()) {
                IndexFile tmp = this.indexFileList.get(this.indexFileList.size() - 1);
                if (!tmp.isWriteFull()) {
                    indexFile = tmp;
                } else {
                    lastUpdateEndPhyOffset = tmp.getEndPhyOffset();
                    lastUpdateIndexTimestamp = tmp.getEndTimestamp();
                    prevIndexFile = tmp;
                }
            }

            this.readWriteLock.readLock().unlock();
        }

        if (indexFile == null) {
            try {
                String fileName =
                    this.storePath + File.separator
                        + UtilAll.timeMillisToHumanString(System.currentTimeMillis());
                indexFile =
                    new IndexFile(fileName, this.hashSlotNum, this.indexNum, lastUpdateEndPhyOffset,
                        lastUpdateIndexTimestamp);
                this.readWriteLock.writeLock().lock();
                this.indexFileList.add(indexFile);
            } catch (Exception e) {
                log.error("getLastIndexFile exception ", e);
            } finally {
                this.readWriteLock.writeLock().unlock();
            }

            if (indexFile != null) {
                final IndexFile flushThisFile = prevIndexFile;
                Thread flushThread = new Thread(new Runnable() {
                    @Override
                    public void run() {
                        IndexService.this.flush(flushThisFile);
                    }
                }, "FlushIndexFileThread");

                flushThread.setDaemon(true);
                flushThread.start();
            }
        }

        return indexFile;
    }
```
**version:4.6.0 release**

**broker-a for master-config:**
storePathRootDir=/data/rocketmq/store
storePathCommitLog=/data/rocketmq/store/commitlog
storePathConsumeQueue=/data/rocketmq/store/consumequeue
storePathConsumeQueue=/data/rocketmq/store/consumequeue
storePathIndex=/data/rocketmq/store/index
storeCheckpoint=/data/rocketmq/store/checkpoint

**broker start detail message:**
2020-01-11 19:54:12 INFO main - dLegerGroup=
2020-01-11 19:54:12 INFO main - dLegerPeers=
2020-01-11 19:54:12 INFO main - dLegerSelfId=
2020-01-11 19:54:13 INFO main - load /data/rocketmq/store/config/consumerOffset.json OK
2020-01-11 19:54:13 INFO main - load /data/rocketmq/store/config/consumerFilter.json OK
2020-01-11 19:54:13 INFO main - Try to start service thread:AllocateMappedFileService started:false lastThread:null
2020-01-11 19:54:13 INFO main - Try to shutdown service thread:AllocateMappedFileService started:true lastThread:Thread[AllocateMappedFileService,5,main]
2020-01-11 19:54:13 INFO main - shutdown thread AllocateMappedFileService interrupt true
2020-01-11 19:54:13 INFO main - join thread AllocateMappedFileService elapsed time(ms) 0 90000
2020-01-11 19:54:13 INFO main - Try to shutdown service thread:PullRequestHoldService started:false lastThread:null
The code in MappedFile.class like this: 

```
public boolean appendMessage(final byte[] data) {
        int currentPos = this.wrotePosition.get();

        if ((currentPos + data.length) <= this.fileSize) {
            try {
                this.fileChannel.position(currentPos);
                this.fileChannel.write(ByteBuffer.wrap(data));
            } catch (Throwable e) {
                log.error("Error occurred when append message to mappedFile.", e);
            }
            this.wrotePosition.addAndGet(data.length);
            return true;
        }

        return false;
    }
```