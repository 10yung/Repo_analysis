
@Linzaer 

I am trying to convert your existing slim model -> onnx->caffe->nncase for use in Kendryte chip k210, so that I have a full working flow before I use your model for training.

I am new to pytorch.

I installed pytorch without CUDA as I only have an AMD machine.

When I run` "python3 convert_to_onnx.py slim"`

I get AssertionError: Torch not compiled with CUDA enabled

Is there a way to run the conversion without CUDA?


作者你好，我在用detect_imgs_onnx.py去运行以上的两个onnx模型的时候，发现version-RFB-320_simplified.onnx的结果是乱七八糟的(错误的结果)。然后我用onnx-simplifier转version-RFB-320.onnx之后，结果才是正确的，是上传错了吗 :P
![20200115163330](https://user-images.githubusercontent.com/50159625/72418154-45ed2880-37b5-11ea-9f8d-689b6ee53ec8.png)





Thank you for sharing such a great project. I encountered this error when convert simplified onnx model to caffe model. can you help me to solve it? thank you very much.
Traceback (most recent call last):
  File "convertCaffe.py", line 110, in <module>
    convertToCaffe(graph, prototxt_path, caffemodel_path)
  File "convertCaffe.py", line 52, in convertToCaffe
    err.unsupported_op(node)
  File "Ultra-Light-Fast-Generic-Face-Detector-1MB/caffe/onnx2caffe/_error_utils.py", line 36, in unsupported_op
    "ONNX node of type {} is not supported.\n".format(node.op_type,)
TypeError: ONNX node of type Slice is not supported.
Hi @Linzaer  
awesome work once again! 

I am unable to comprehend the below issue. Shouldn't an INT8 quantized model run faster than the unquantized one  (for the same image file , system settings) ?

**1.1MB model : 0.005 secs
300kB model : 0.013 secs**

```
 ./Ultra-face-mnn ../model/version-RFB/RFB-320.mnn ../imgs/1.jpg 
Processing ../imgs/1.jpg
inference time:0.00524949 s
all time: 0.00826154 s

./Ultra-face-mnn ../model/version-RFB/RFB-320-quant-KL-5792.mnn ../imgs/1.jpg 
Processing ../imgs/1.jpg
inference time:0.0131211 s
all time: 0.0145813 s
```

First, thanks for the great work, the onnx models on python works just like expected.

Your models seem to be perfectly fitted for a javascript implementation, light and fast, but when I tried them on onnx.js in a browser, it returns this error :

`cannot resolve operator 'Shape' with opsets: ai.onnx v9`

Is there a way to get around this error, or should I investigate other solutions like :
_ onnx > caffe > webassembly
_ onnx > tensorflow > tensroflow.js 

Thanks.
Is it possible to increase the resolution beyond the 640x480, which is currently the best available resolution? 
与训练模型加载,为什么提示AttributeError: 'DataParallel' object has no attribute 'init_from_base_net'呢?