Added an example which would publish the odometry message information from openvslam. Seen some issues created looking for this. I needed it as well, so I thought it would be good to have an example for others as well. 

This is a very rudimentary implementation, and there might even be some issues, but maybe someone will improve upon it if need be and perhaps add some more features etc.

This branch is checked out from ros2 latest commit, so this will hopefully pass all tests etc as soon as the main ros2 PR is merged.

edit: of course all commits are here as well, but I still think the other one should be a separate merge, and if needed I'll just merge master into this one after that PR merge.
![vslam](https://user-images.githubusercontent.com/12890191/72126201-3c735300-33a6-11ea-8ddd-fd87bbe811d7.png)

I am able to build and run the sample Run_video_slam.exe, but after run a few frames a debug assertion warning message popup. The warning message is attached with this issue. 
Is there any plan to include non-maximal suppression (NMS) algorithms to homogenously distribute detected keypoints in the image? 
Hello, 
How can I specify a real world measurement in order to provide a reference to resolve scale and coordinates frame ? (Feeding a stereo frame would do the trick but I'm doing monocular)

For example in Ucoslam one can set aruco_markerSize so that we provide an object size reference to solve the scale issue. 

Can something like that be implemented in openvslam, as it will also help fight scale drift.

For the coordinate position & orientation reference, we can always work in the frame reference of the first Keyframe, but it would be great if we could specify the position and orientation of a KeyFrame and freeze it as a reference, but it will probably induce some numerical instabilities to have a hard constraint.

To fight orientation and position drift, it would be great to be able to add compass from IMU, but it would be great to have some way to manually constrain some orientation of a KeyFrame, and position of a KeyFrame.

Thanks
It seems that in order to build OpenVSLAM using Visual Studio, people have to keep adding pretty much these same changes over and over again (see e.g. [here](https://qiita.com/na0ki_ikeda/items/2310dc8dd70713de98e1#openvslamcmakeliststxt%E3%82%92%E7%B7%A8%E9%9B%86) and [here](https://github.com/xdspacelab/openvslam/issues/108#issuecomment-543720154)).

As these minor additions should cause no harm to Linux or macOS users, would suggest incorporating them in the official codebase.
I ran the sample tutorial with the video sequence provided in Google drive. After the vocabulary, *.mp4 and *.yaml files are loaded, the program prompts a invalid node error. I have tried to figure out the reason by running the program with --debug option, here is the output:
(base) ubuntu@ubuntu-Macmini:~/openvslam-master/build$ ./run_video_slam -v orb_vocab/orb_vocab.dbow2 -c aist_entrance_hall_1/config.yaml -m aist_entrance_hall_2/video.mp4 --frame-skip 3 --map-db aist_entrance_hall_1_map.msg --debug
[2019-12-19 15:37:14.769] [D] CONSTRUCT: config
[2019-12-19 15:37:14.769] [I] config file loaded: aist_entrance_hall_1/config.yaml
[2019-12-19 15:37:14.769] [D] load camera model type
invalid node; this may result from using a map iterator as a sequence iterator, or vice-versa

I have checked the files and they are all right. Can someone help?
Hi all,

Thanks for the repo. 
And I wonder what are the options for the camera model? I see perspective, fisheye, and equirectangular. What should I put into the camera config file if I am using a common stereo camera?

Thanks in advance.

Duncan
hi, I recently read the code of openvslam, well done!
But, when i run into the `tracking_module::track_monocular_image` , i have being confused by the method you used to initialize the first frame and second frame. Then I give my question.

We run into the `initializer::initialize()`, firstly, use first frame in create_initializer (). Secondly, use  second frame combine with first frame to do initilization job for mobocular tracking. Where the code makes me confused is ` area::match_in_consistent_area()` called by `initializer::try_initialize_for_monocular` :
       https://github.com/xdspacelab/openvslam/blob/dcc96a949673254ab3a18c9192ea1067b8b447a2/src/openvslam/match/area.cc#L19-L27

Why we omit keypoints extracted on other scale? I donnot think it's reasonable. I am looking forward for your answer, thanks in advance!
@kensakurada  @shinsumicco @MikiyaShibuya 
Just confused. The case for Perspective camera has the same code as the case for Fisheye

https://github.com/xdspacelab/openvslam/blob/master/src/openvslam/data/frame.cc#L205

Is fisheye meant to behave differently? Is this a mistake or oversight (or just convenient)?
I am running the system for my own video. I already calibrated the camera accordingly. However, the system has a weird behavior: at the beginning, it works fine when the tracking is performed in a straight line, but when it turns left or right, the tracking is eventually lost. At other times, after turning, the viewer shows the current keyframe (green square) spinning around repeatedly although the video shows a following up straight path. Has somebody an idea what is happening? Can it be due to the fact that the video shows very close objects (less than one meter close)?

