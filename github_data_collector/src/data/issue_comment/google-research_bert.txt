
Hi, I have a few questions about my development.

1. **What's the primary difference between pre-training from pre-trained checkpoints and fine-tuning the models?** 
2. **How much data do I need to pre-train the model?**
3. What will be the format of the custom corpus, will it be a **text file** containing sequences per line?
4. Could you suggest any other way how can I fine-tune the BERT with the unsupervised dataset (no label dataset, an only text file containing sequences)?
5. What **`train_batch_size`** do I need to set while pre-training when I am setting **`max_seq_length = 512`**?
I have been using bert embeddings from the pretrained bert-base-uncased for various downstream tasks.
When i try to understand the theory of how it is contextual, how the other tokens in a sentence play a role in determining the meaning or context of every individual token, the learning is quite clear.
But when I think of it as a single token (A sentence of a single token), Questions like "what has the model really learnt when it looks at it standalone" arise.
I have been testing it and the results are quite misleading.

I don't want to use different techniques for same task but at different granularity level (i.e single term, n-grams, sentences)

Coding is more similar to killing than it is to programming.
Can one help me understand this discrepancy and is it expected?
@jacobdevlin-google @hsm207 
<img width="850" alt="Screenshot 2020-01-13 at 11 21 38 AM" src="https://user-images.githubusercontent.com/25073753/72235241-109adc00-35f7-11ea-9dc8-aa503b3f90d3.png">

train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example
                                                                   text_a = x[DATA_COLUMN],
                                                                   text_b = None,
                                                                   label = x[LABEL_COLUMN]), axis = 1)



I get the below error:
train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example
AttributeError: ("module 'bert' has no attribute 'run_classifier'", 'occurred at index 17264')
why not train all tokens ? can someone give a straight answer or point me another pdf where it came from at first place ?
I want to pre-train BERT on Urdu. There are a few things which I would like to know. 
BERT multilingual model does not contain all the tokens(tokenized words) in vocab.txt. So if I want to pre-train, do I need to enhance the vocabulary ? If so
Then how do I enhance the vocabulary ?
Replace other language words with my language words or some other technique ? Please explain. 
One more thing if I run pretraining on a large corpus of data without changing the vocab ? Will my model perform any better ?
I am using flask to call BERT model to extract features for my sentences and prediction on top of that embeddings for simple classification. I am using threaded-True in flask but still for multi-user-request (say 10) its taking almost 10 minutes for 600 sentences on 8GB windows CPU. It can be flask server issue as well as it may or may not support multi threading. 
Is there any thing possible from BERT end to allow parallelism to make response faster on Windows CPU? 
I am using BERT embeddings and using them to classify my basic Keras model. But every time I pass in my sentences, its loading the entire BERT graph which is making it too slow for for my prediction service. Is there any circumvention to this problem?

Time taken to extract 600 sentences embedding: 3 minutes Approx. (Arch: 8 GB RAM Windows-CPU)  
I read the code, found that in the comments, 
token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])

But I think token_type_ids can only be 1 or 0 , am  I wrong
CONTRIBUTING.md
